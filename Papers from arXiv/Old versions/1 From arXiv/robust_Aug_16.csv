Title,Authors,Abstract,arXiv ID,PDF_Link
"A graph neural network-based model with Out-of-Distribution Robustness
  for enhancing Antiretroviral Therapy Outcome Prediction for HIV-1","Giulia Di Teodoro, Federico Siciliano, Valerio Guarrasi, Anne-Mieke Vandamme, Valeria Ghisetti, Anders Sönnerborg, Maurizio Zazzi, Fabrizio Silvestri, Laura Palagi","Predicting the outcome of antiretroviral therapies for HIV-1 is a pressing
clinical challenge, especially when the treatment regimen includes drugs for
which limited effectiveness data is available. This scarcity of data can arise
either due to the introduction of a new drug to the market or due to limited
use in clinical settings. To tackle this issue, we introduce a novel joint
fusion model, which combines features from a Fully Connected (FC) Neural
Network and a Graph Neural Network (GNN). The FC network employs tabular data
with a feature vector made up of viral mutations identified in the most recent
genotypic resistance test, along with the drugs used in therapy. Conversely,
the GNN leverages knowledge derived from Stanford drug-resistance mutation
tables, which serve as benchmark references for deducing in-vivo treatment
efficacy based on the viral genetic sequence, to build informative graphs. We
evaluated these models' robustness against Out-of-Distribution drugs in the
test set, with a specific focus on the GNN's role in handling such scenarios.
Our comprehensive analysis demonstrates that the proposed model consistently
outperforms the FC model, especially when considering Out-of-Distribution
drugs. These results underscore the advantage of integrating Stanford scores in
the model, thereby enhancing its generalizability and robustness, but also
extending its utility in real-world applications with limited data
availability. This research highlights the potential of our approach to inform
antiretroviral therapy outcome prediction and contribute to more informed
clinical decisions.",2312.17506v1,https://arxiv.org/pdf/2312.17506v1
"Self-supervised Pretraining for Robust Personalized Voice Activity
  Detection in Adverse Conditions","Holger Severin Bovbjerg, Jesper Jensen, Jan Østergaard, Zheng-Hua Tan","In this paper, we propose the use of self-supervised pretraining on a large
unlabelled data set to improve the performance of a personalized voice activity
detection (VAD) model in adverse conditions. We pretrain a long short-term
memory (LSTM)-encoder using the autoregressive predictive coding (APC)
framework and fine-tune it for personalized VAD. We also propose a denoising
variant of APC, with the goal of improving the robustness of personalized VAD.
The trained models are systematically evaluated on both clean speech and speech
contaminated by various types of noise at different SNR-levels and compared to
a purely supervised model. Our experiments show that self-supervised
pretraining not only improves performance in clean conditions, but also yields
models which are more robust to adverse conditions compared to purely
supervised learning.",2312.16613v2,https://arxiv.org/pdf/2312.16613v2
"An Integrated Imitation and Reinforcement Learning Methodology for
  Robust Agile Aircraft Control with Limited Pilot Demonstration Data","Gulay Goktas Sever, Umut Demir, Abdullah Sadik Satir, Mustafa Cagatay Sahin, Nazim Kemal Ure","In this paper, we present a methodology for constructing data-driven maneuver
generation models for agile aircraft that can generalize across a wide range of
trim conditions and aircraft model parameters. Maneuver generation models play
a crucial role in the testing and evaluation of aircraft prototypes, providing
insights into the maneuverability and agility of the aircraft. However,
constructing the models typically requires extensive amounts of real pilot
data, which can be time-consuming and costly to obtain. Moreover, models built
with limited data often struggle to generalize beyond the specific flight
conditions covered in the original dataset. To address these challenges, we
propose a hybrid architecture that leverages a simulation model, referred to as
the source model. This open-source agile aircraft simulator shares similar
dynamics with the target aircraft and allows us to generate unlimited data for
building a proxy maneuver generation model. We then fine-tune this model to the
target aircraft using a limited amount of real pilot data. Our approach
combines techniques from imitation learning, transfer learning, and
reinforcement learning to achieve this objective. To validate our methodology,
we utilize real agile pilot data provided by Turkish Aerospace Industries
(TAI). By employing the F-16 as the source model, we demonstrate that it is
possible to construct a maneuver generation model that generalizes across
various trim conditions and aircraft parameters without requiring any
additional real pilot data. Our results showcase the effectiveness of our
approach in developing robust and adaptable models for agile aircraft.",2401.08663v1,https://arxiv.org/pdf/2401.08663v1
How Robust are LLMs to In-Context Majority Label Bias?,"Karan Gupta, Sumegh Roychowdhury, Siva Rajesh Kasa, Santhosh Kumar Kasa, Anish Bhanushali, Nikhil Pattisapu, Prasanna Srinivasa Murthy","In the In-Context Learning (ICL) setup, various forms of label biases can
manifest. One such manifestation is majority label bias, which arises when the
distribution of labeled examples in the in-context samples is skewed towards
one or more specific classes making Large Language Models (LLMs) more prone to
predict those labels. Such discrepancies can arise from various factors,
including logistical constraints, inherent biases in data collection methods,
limited access to diverse data sources, etc. which are unavoidable in a
real-world industry setup. In this work, we study the robustness of in-context
learning in LLMs to shifts that occur due to majority label bias within the
purview of text classification tasks. Prior works have shown that in-context
learning with LLMs is susceptible to such biases. In our study, we go one level
deeper and show that the robustness boundary varies widely for different models
and tasks, with certain LLMs being highly robust (~90%) to majority label bias.
Additionally, our findings also highlight the impact of model size and the
richness of instructional prompts contributing towards model robustness. We
restrict our study to only publicly available open-source models to ensure
transparency and reproducibility.",2312.16549v1,https://arxiv.org/pdf/2312.16549v1
"Visual Spatial Attention and Proprioceptive Data-Driven Reinforcement
  Learning for Robust Peg-in-Hole Task Under Variable Conditions","André Yuji Yasutomi, Hideyuki Ichiwara, Hiroshi Ito, Hiroki Mori, Tetsuya Ogata","Anchor-bolt insertion is a peg-in-hole task performed in the construction
field for holes in concrete. Efforts have been made to automate this task, but
the variable lighting and hole surface conditions, as well as the requirements
for short setup and task execution time make the automation challenging. In
this study, we introduce a vision and proprioceptive data-driven robot control
model for this task that is robust to challenging lighting and hole surface
conditions. This model consists of a spatial attention point network (SAP) and
a deep reinforcement learning (DRL) policy that are trained jointly end-to-end
to control the robot. The model is trained in an offline manner, with a
sample-efficient framework designed to reduce training time and minimize the
reality gap when transferring the model to the physical world. Through
evaluations with an industrial robot performing the task in 12 unknown holes,
starting from 16 different initial positions, and under three different
lighting conditions (two with misleading shadows), we demonstrate that SAP can
generate relevant attention points of the image even in challenging lighting
conditions. We also show that the proposed model enables task execution with
higher success rate and shorter task completion time than various baselines.
Due to the proposed model's high effectiveness even in severe lighting, initial
positions, and hole conditions, and the offline training framework's high
sample-efficiency and short training time, this approach can be easily applied
to construction.",2312.16438v2,https://arxiv.org/pdf/2312.16438v2
"Refining Latent Homophilic Structures over Heterophilic Graphs for
  Robust Graph Convolution Networks","Chenyang Qiu, Guoshun Nan, Tianyu Xiong, Wendi Deng, Di Wang, Zhiyang Teng, Lijuan Sun, Qimei Cui, Xiaofeng Tao","Graph convolution networks (GCNs) are extensively utilized in various graph
tasks to mine knowledge from spatial data. Our study marks the pioneering
attempt to quantitatively investigate the GCN robustness over omnipresent
heterophilic graphs for node classification. We uncover that the predominant
vulnerability is caused by the structural out-of-distribution (OOD) issue. This
finding motivates us to present a novel method that aims to harden GCNs by
automatically learning Latent Homophilic Structures over heterophilic graphs.
We term such a methodology as LHS. To elaborate, our initial step involves
learning a latent structure by employing a novel self-expressive technique
based on multi-node interactions. Subsequently, the structure is refined using
a pairwisely constrained dual-view contrastive learning approach. We
iteratively perform the above procedure, enabling a GCN model to aggregate
information in a homophilic way on heterophilic graphs. Armed with such an
adaptable structure, we can properly mitigate the structural OOD threats over
heterophilic graphs. Experiments on various benchmarks show the effectiveness
of the proposed LHS approach for robust GCNs.",2312.16418v1,https://arxiv.org/pdf/2312.16418v1
"Dynamic Sub-graph Distillation for Robust Semi-supervised Continual
  Learning","Yan Fan, Yu Wang, Pengfei Zhu, Qinghua Hu","Continual learning (CL) has shown promising results and comparable
performance to learning at once in a fully supervised manner. However, CL
strategies typically require a large number of labeled samples, making their
real-life deployment challenging. In this work, we focus on semi-supervised
continual learning (SSCL), where the model progressively learns from partially
labeled data with unknown categories. We provide a comprehensive analysis of
SSCL and demonstrate that unreliable distributions of unlabeled data lead to
unstable training and refinement of the progressing stages. This problem
severely impacts the performance of SSCL. To address the limitations, we
propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for
semi-supervised continual learning, which leverages both semantic and
structural information to achieve more stable knowledge distillation on
unlabeled data and exhibit robustness against distribution bias. Firstly, we
formalize a general model of structural distillation and design a dynamic graph
construction for the continual learning progress. Next, we define a structure
distillation vector and design a dynamic sub-graph distillation algorithm,
which enables end-to-end training and adaptability to scale up tasks. The
entire proposed method is adaptable to various CL methods and supervision
settings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100,
and ImageNet-100, with varying supervision ratios, demonstrate the
effectiveness of our proposed approach in mitigating the catastrophic
forgetting problem in semi-supervised continual learning scenarios.",2312.16409v1,https://arxiv.org/pdf/2312.16409v1
"Robustness Verification for Knowledge-Based Logic of Risky Driving
  Scenes","Xia Wang, Anda Liang, Jonathan Sprinkle, Taylor T. Johnson","Many decision-making scenarios in modern life benefit from the decision
support of artificial intelligence algorithms, which focus on a data-driven
philosophy and automated programs or systems. However, crucial decision issues
related to security, fairness, and privacy should consider more human knowledge
and principles to supervise such AI algorithms to reach more proper solutions
and to benefit society more effectively. In this work, we extract
knowledge-based logic that defines risky driving formats learned from public
transportation accident datasets, which haven't been analyzed in detail to the
best of our knowledge. More importantly, this knowledge is critical for
recognizing traffic hazards and could supervise and improve AI models in
safety-critical systems. Then we use automated verification methods to verify
the robustness of such logic. More specifically, we gather 72 accident datasets
from Data.gov and organize them by state. Further, we train Decision Tree and
XGBoost models on each state's dataset, deriving accident judgment logic.
Finally, we deploy robustness verification on these tree-based models under
multiple parameter combinations.",2312.16364v1,https://arxiv.org/pdf/2312.16364v1
"Robust Neural Pruning with Gradient Sampling Optimization for Residual
  Neural Networks",Juyoung Yun,"This research embarks on pioneering the integration of gradient sampling
optimization techniques, particularly StochGradAdam, into the pruning process
of neural networks. Our main objective is to address the significant challenge
of maintaining accuracy in pruned neural models, critical in
resource-constrained scenarios. Through extensive experimentation, we
demonstrate that gradient sampling significantly preserves accuracy during and
after the pruning process compared to traditional optimization methods. Our
study highlights the pivotal role of gradient sampling in robust learning and
maintaining crucial information post substantial model simplification. The
results across CIFAR-10 datasets and residual neural architectures validate the
versatility and effectiveness of our approach. This work presents a promising
direction for developing efficient neural networks without compromising
performance, even in environments with limited computational resources.",2312.16020v3,https://arxiv.org/pdf/2312.16020v3
Robust Survival Analysis with Adversarial Regularization,"Michael Potter, Stefano Maxenti, Michael Everett","Survival Analysis (SA) models the time until an event occurs, with
applications in fields like medicine, defense, finance, and aerospace. Recent
work shows that Neural Networks (NNs) can capture complex relationships in SA.
However, dataset uncertainties (e.g., noisy measurements, human error) can
degrade model performance. To address this, we leverage NN verification
advances to create algorithms for robust, fully-parametric survival models. We
introduce a robust loss function and use CROWN-IBP regularization to handle
computational challenges in the Min-Max problem. Evaluating our approach on
SurvSet datasets, we find that our Survival Analysis with Adversarial
Regularization (SAWAR) method consistently outperforms baselines under various
perturbations with respect to Negative Log Likelihood (NegLL), Integrated Brier
Score (IBS), and Concordance Index (CI). This demonstrates that adversarial
regularization enhances SA performance and calibration, mitigating data
uncertainty and improving generalization across diverse datasets up to 150%
across all perturbation magnitudes.",2312.16019v3,https://arxiv.org/pdf/2312.16019v3
"Modality-Collaborative Transformer with Hybrid Feature Reconstruction
  for Robust Emotion Recognition","Chengxin Chen, Pengyuan Zhang","As a vital aspect of affective computing, Multimodal Emotion Recognition has
been an active research area in the multimedia community. Despite recent
progress, this field still confronts two major challenges in real-world
applications: 1) improving the efficiency of constructing joint representations
from unaligned multimodal features, and 2) relieving the performance decline
caused by random modality feature missing. In this paper, we propose a unified
framework, Modality-Collaborative Transformer with Hybrid Feature
Reconstruction (MCT-HFR), to address these issues. The crucial component of MCT
is a novel attention-based encoder which concurrently extracts and dynamically
balances the intra- and inter-modality relations for all associated modalities.
With additional modality-wise parameter sharing, a more compact representation
can be encoded with less time and space complexity. To improve the robustness
of MCT, we further introduce HFR which consists of two modules: Local Feature
Imagination (LFI) and Global Feature Alignment (GFA). During model training,
LFI leverages complete features as supervisory signals to recover local missing
features, while GFA is designed to reduce the global semantic gap between
pairwise complete and incomplete representations. Experimental evaluations on
two popular benchmark datasets demonstrate that our proposed method
consistently outperforms advanced baselines in both complete and incomplete
data scenarios.",2312.15848v1,https://arxiv.org/pdf/2312.15848v1
Robust Stochastically-Descending Unrolled Networks,"Samar Hadou, Navid NaderiAlizadeh, Alejandro Ribeiro","Deep unrolling, or unfolding, is an emerging learning-to-optimize method that
unrolls a truncated iterative algorithm in the layers of a trainable neural
network. However, the convergence guarantees and generalizability of the
unrolled networks are still open theoretical problems. To tackle these
problems, we provide deep unrolled architectures with a stochastic descent
nature by imposing descending constraints during training. The descending
constraints are forced layer by layer to ensure that each unrolled layer takes,
on average, a descent step toward the optimum during training. We theoretically
prove that the sequence constructed by the outputs of the unrolled layers is
then guaranteed to converge for unseen problems, assuming no distribution shift
between training and test problems. We also show that standard unrolling is
brittle to perturbations, and our imposed constraints provide the unrolled
networks with robustness to additive noise and perturbations. We numerically
assess unrolled architectures trained under the proposed constraints in two
different applications, including the sparse coding using learnable iterative
shrinkage and thresholding algorithm (LISTA) and image inpainting using
proximal generative flow (GLOW-Prox), and demonstrate the performance and
robustness benefits of the proposed method.",2312.15788v1,https://arxiv.org/pdf/2312.15788v1
On Robust Wasserstein Barycenter: The Model and Algorithm,"Xu Wang, Jiawei Huang, Qingyuan Yang, Jinpeng Zhang","The Wasserstein barycenter problem is to compute the average of $m$ given
probability measures, which has been widely studied in many different areas;
however, real-world data sets are often noisy and huge, which impedes its
applications in practice. Hence, in this paper, we focus on improving the
computational efficiency of two types of robust Wasserstein barycenter problem
(RWB): fixed-support RWB (fixed-RWB) and free-support RWB (free-RWB); actually,
the former is a subroutine of the latter. Firstly, we improve efficiency
through model reducing; we reduce RWB as an augmented Wasserstein barycenter
problem, which works for both fixed-RWB and free-RWB. Especially, fixed-RWB can
be computed within $\widetilde{O}(\frac{mn^2}{\epsilon_+})$ time by using an
off-the-shelf solver, where $\epsilon_+$ is the pre-specified additive error
and $n$ is the size of locations of input measures. Then, for free-RWB, we
leverage a quality guaranteed data compression technique, coreset, to
accelerate computation by reducing the data set size $m$. It shows that running
algorithms on the coreset is enough instead of on the original data set. Next,
by combining the model reducing and coreset techniques above, we propose an
algorithm for free-RWB by updating the weights and locations alternatively.
Finally, our experiments demonstrate the efficiency of our techniques.",2312.15762v1,https://arxiv.org/pdf/2312.15762v1
"Robust Sclera Segmentation for Skin-tone Agnostic Face Image Quality
  Assessment","Wassim Kabbani, Christoph Busch, Kiran Raja","Face image quality assessment (FIQA) is crucial for obtaining good face
recognition performance. FIQA algorithms should be robust and insensitive to
demographic factors. The eye sclera has a consistent whitish color in all
humans regardless of their age, ethnicity and skin-tone. This work proposes a
robust sclera segmentation method that is suitable for face images in the
enrolment and the border control face recognition scenarios. It shows how the
statistical analysis of the sclera pixels produces features that are invariant
to skin-tone, age and ethnicity and thus can be incorporated into FIQA
algorithms to make them agnostic to demographic factors.",2312.15102v1,https://arxiv.org/pdf/2312.15102v1
"Robust Knowledge Extraction from Large Language Models using Social
  Choice Theory","Nico Potyka, Yuqicheng Zhu, Yunjie He, Evgeny Kharlamov, Steffen Staab","Large-language models (LLMs) can support a wide range of applications like
conversational agents, creative writing or general query answering. However,
they are ill-suited for query answering in high-stake domains like medicine
because they are typically not robust - even the same query can result in
different answers when prompted multiple times. In order to improve the
robustness of LLM queries, we propose using ranking queries repeatedly and to
aggregate the queries using methods from social choice theory. We study ranking
queries in diagnostic settings like medical and fault diagnosis and discuss how
the Partial Borda Choice function from the literature can be applied to merge
multiple query results. We discuss some additional interesting properties in
our setting and evaluate the robustness of our approach empirically.",2312.14877v2,https://arxiv.org/pdf/2312.14877v2
"Robustness, Efficiency, or Privacy: Pick Two in Machine Learning","Youssef Allouah, Rachid Guerraoui, John Stephan","The success of machine learning (ML) applications relies on vast datasets and
distributed architectures which, as they grow, present major challenges. In
real-world scenarios, where data often contains sensitive information, issues
like data poisoning and hardware failures are common. Ensuring privacy and
robustness is vital for the broad adoption of ML in public life. This paper
examines the costs associated with achieving these objectives in distributed ML
architectures, from both theoretical and empirical perspectives. We overview
the meanings of privacy and robustness in distributed ML, and clarify how they
can be achieved efficiently in isolation. However, we contend that the
integration of these two objectives entails a notable compromise in
computational efficiency. In short, traditional noise injection hurts accuracy
by concealing poisoned inputs, while cryptographic methods clash with poisoning
defenses due to their non-linear nature. However, we outline future research
directions aimed at reconciling this compromise with efficiency by considering
weaker threat models.",2312.14712v2,https://arxiv.org/pdf/2312.14712v2
"Balancing Energy Efficiency and Distributional Robustness in
  Over-the-Air Federated Learning","Mohamed Badi, Chaouki Ben Issaid, Anis Elgabli, Mehdi Bennis","The growing number of wireless edge devices has magnified challenges
concerning energy, bandwidth, latency, and data heterogeneity. These challenges
have become bottlenecks for distributed learning. To address these issues, this
paper presents a novel approach that ensures energy efficiency for
distributionally robust federated learning (FL) with over air computation
(AirComp). In this context, to effectively balance robustness with energy
efficiency, we introduce a novel client selection method that integrates two
complementary insights: a deterministic one that is designed for energy
efficiency, and a probabilistic one designed for distributional robustness.
Simulation results underscore the efficacy of the proposed algorithm, revealing
its superior performance compared to baselines from both robustness and energy
efficiency perspectives, achieving more than 3-fold energy savings compared to
the considered baselines.",2312.14638v1,https://arxiv.org/pdf/2312.14638v1
Attacking Byzantine Robust Aggregation in High Dimensions,"Sarthak Choudhary, Aashish Kolluri, Prateek Saxena","Training modern neural networks or models typically requires averaging over a
sample of high-dimensional vectors. Poisoning attacks can skew or bias the
average vectors used to train the model, forcing the model to learn specific
patterns or avoid learning anything useful. Byzantine robust aggregation is a
principled algorithmic defense against such biasing. Robust aggregators can
bound the maximum bias in computing centrality statistics, such as mean, even
when some fraction of inputs are arbitrarily corrupted. Designing such
aggregators is challenging when dealing with high dimensions. However, the
first polynomial-time algorithms with strong theoretical bounds on the bias
have recently been proposed. Their bounds are independent of the number of
dimensions, promising a conceptual limit on the power of poisoning attacks in
their ongoing arms race against defenses.
  In this paper, we show a new attack called HIDRA on practical realization of
strong defenses which subverts their claim of dimension-independent bias. HIDRA
highlights a novel computational bottleneck that has not been a concern of
prior information-theoretic analysis. Our experimental evaluation shows that
our attacks almost completely destroy the model performance, whereas existing
attacks with the same goal fail to have much effect. Our findings leave the
arms race between poisoning attacks and provable defenses wide open.",2312.14461v2,https://arxiv.org/pdf/2312.14461v2
Solving Long-run Average Reward Robust MDPs via Stochastic Games,"Krishnendu Chatterjee, Ehsan Kafshdar Goharshady, Mehrdad Karrabi, Petr Novotný, Đorđe Žikelić","Markov decision processes (MDPs) provide a standard framework for sequential
decision making under uncertainty. However, MDPs do not take uncertainty in
transition probabilities into account. Robust Markov decision processes (RMDPs)
address this shortcoming of MDPs by assigning to each transition an uncertainty
set rather than a single probability value. In this work, we consider polytopic
RMDPs in which all uncertainty sets are polytopes and study the problem of
solving long-run average reward polytopic RMDPs. We present a novel perspective
on this problem and show that it can be reduced to solving long-run average
reward turn-based stochastic games with finite state and action spaces. This
reduction allows us to derive several important consequences that were hitherto
not known to hold for polytopic RMDPs. First, we derive new computational
complexity bounds for solving long-run average reward polytopic RMDPs, showing
for the first time that the threshold decision problem for them is in $NP \cap
coNP$ and that they admit a randomized algorithm with sub-exponential expected
runtime. Second, we present Robust Polytopic Policy Iteration (RPPI), a novel
policy iteration algorithm for solving long-run average reward polytopic RMDPs.
Our experimental evaluation shows that RPPI is much more efficient in solving
long-run average reward polytopic RMDPs compared to state-of-the-art methods
based on value iteration.",2312.13912v2,https://arxiv.org/pdf/2312.13912v2
"ARBiBench: Benchmarking Adversarial Robustness of Binarized Neural
  Networks","Peng Zhao, Jiehua Zhang, Bowen Peng, Longguang Wang, YingMei Wei, Yu Liu, Li Liu","Network binarization exhibits great potential for deployment on
resource-constrained devices due to its low computational cost. Despite the
critical importance, the security of binarized neural networks (BNNs) is rarely
investigated. In this paper, we present ARBiBench, a comprehensive benchmark to
evaluate the robustness of BNNs against adversarial perturbations on CIFAR-10
and ImageNet. We first evaluate the robustness of seven influential BNNs on
various white-box and black-box attacks. The results reveal that 1) The
adversarial robustness of BNNs exhibits a completely opposite performance on
the two datasets under white-box attacks. 2) BNNs consistently exhibit better
adversarial robustness under black-box attacks. 3) Different BNNs exhibit
certain similarities in their robustness performance. Then, we conduct
experiments to analyze the adversarial robustness of BNNs based on these
insights. Our research contributes to inspiring future research on enhancing
the robustness of BNNs and advancing their application in real-world scenarios.",2312.13575v1,https://arxiv.org/pdf/2312.13575v1
Scaling Compute Is Not All You Need for Adversarial Robustness,"Edoardo Debenedetti, Zishen Wan, Maksym Andriushchenko, Vikash Sehwag, Kshitij Bhardwaj, Bhavya Kailkhura","The last six years have witnessed significant progress in adversarially
robust deep learning. As evidenced by the CIFAR-10 dataset category in
RobustBench benchmark, the accuracy under $\ell_\infty$ adversarial
perturbations improved from 44\% in \citet{Madry2018Towards} to 71\% in
\citet{peng2023robust}. Although impressive, existing state-of-the-art is still
far from satisfactory. It is further observed that best-performing models are
often very large models adversarially trained by industrial labs with
significant computational budgets. In this paper, we aim to understand: ``how
much longer can computing power drive adversarial robustness advances?"" To
answer this question, we derive \emph{scaling laws for adversarial robustness}
which can be extrapolated in the future to provide an estimate of how much cost
we would need to pay to reach a desired level of robustness. We show that
increasing the FLOPs needed for adversarial training does not bring as much
advantage as it does for standard training in terms of performance
improvements. Moreover, we find that some of the top-performing techniques are
difficult to exactly reproduce, suggesting that they are not robust enough for
minor changes in the training setup. Our analysis also uncovers potentially
worthwhile directions to pursue in future research. Finally, we make our
benchmarking framework (built on top of \texttt{timm}~\citep{rw2019timm})
publicly available to facilitate future analysis in efficient robust deep
learning.",2312.13131v1,https://arxiv.org/pdf/2312.13131v1
Robust Loss Functions for Training Decision Trees with Noisy Labels,"Jonathan Wilton, Nan Ye","We consider training decision trees using noisily labeled data, focusing on
loss functions that can lead to robust learning algorithms. Our contributions
are threefold. First, we offer novel theoretical insights on the robustness of
many existing loss functions in the context of decision tree learning. We show
that some of the losses belong to a class of what we call conservative losses,
and the conservative losses lead to an early stopping behavior during training
and noise-tolerant predictions during testing. Second, we introduce a framework
for constructing robust loss functions, called distribution losses. These
losses apply percentile-based penalties based on an assumed margin
distribution, and they naturally allow adapting to different noise rates via a
robustness parameter. In particular, we introduce a new loss called the
negative exponential loss, which leads to an efficient greedy
impurity-reduction learning algorithm. Lastly, our experiments on multiple
datasets and noise settings validate our theoretical insight and the
effectiveness of our adaptive negative exponential loss.",2312.12937v2,https://arxiv.org/pdf/2312.12937v2
"Robustly Improving Bandit Algorithms with Confounded and Selection
  Biased Offline Data: A Causal Approach","Wen Huang, Xintao Wu","This paper studies bandit problems where an agent has access to offline data
that might be utilized to potentially improve the estimation of each arm's
reward distribution. A major obstacle in this setting is the existence of
compound biases from the observational data. Ignoring these biases and blindly
fitting a model with the biased data could even negatively affect the online
learning phase. In this work, we formulate this problem from a causal
perspective. First, we categorize the biases into confounding bias and
selection bias based on the causal structure they imply. Next, we extract the
causal bound for each arm that is robust towards compound biases from biased
observational data. The derived bounds contain the ground truth mean reward and
can effectively guide the bandit agent to learn a nearly-optimal decision
policy. We also conduct regret analysis in both contextual and non-contextual
bandit settings and show that prior causal bounds could help consistently
reduce the asymptotic regret.",2312.12731v1,https://arxiv.org/pdf/2312.12731v1
Robust Point Matching with Distance Profiles,"YoonHaeng Hur, Yuehaw Khoo","While matching procedures based on pairwise distances are conceptually
appealing and thus favored in practice, theoretical guarantees for such
procedures are rarely found in the literature. We propose and analyze matching
procedures based on distance profiles that are easily implementable in
practice, showing these procedures are robust to outliers and noise. We
demonstrate the performance of the proposed method using a real data example
and provide simulation studies to complement the theoretical findings.",2312.12641v2,https://arxiv.org/pdf/2312.12641v2
"Robust Machine Learning by Transforming and Augmenting Imperfect
  Training Data",Elliot Creager,"Machine Learning (ML) is an expressive framework for turning data into
computer programs. Across many problem domains -- both in industry and policy
settings -- the types of computer programs needed for accurate prediction or
optimal control are difficult to write by hand. On the other hand, collecting
instances of desired system behavior may be relatively more feasible. This
makes ML broadly appealing, but also induces data sensitivities that often
manifest as unexpected failure modes during deployment. In this sense, the
training data available tend to be imperfect for the task at hand. This thesis
explores several data sensitivities of modern machine learning and how to
address them. We begin by discussing how to prevent ML from codifying prior
human discrimination measured in the training data, where we take a fair
representation learning approach. We then discuss the problem of learning from
data containing spurious features, which provide predictive fidelity during
training but are unreliable upon deployment. Here we observe that insofar as
standard training methods tend to learn such features, this propensity can be
leveraged to search for partitions of training data that expose this
inconsistency, ultimately promoting learning algorithms invariant to spurious
features. Finally, we turn our attention to reinforcement learning from data
with insufficient coverage over all possible states and actions. To address the
coverage issue, we discuss how causal priors can be used to model the
single-step dynamics of the setting where data are collected. This enables a
new type of data augmentation where observed trajectories are stitched together
to produce new but plausible counterfactual trajectories.",2312.12597v1,https://arxiv.org/pdf/2312.12597v1
Learning Deterministic Surrogates for Robust Convex QCQPs,"Egon Peršak, Miguel F. Anjos","Decision-focused learning is a promising development for contextual
optimisation. It enables us to train prediction models that reflect the
contextual sensitivity structure of the problem. However, there have been
limited attempts to extend this paradigm to robust optimisation. We propose a
double implicit layer model for training prediction models with respect to
robust decision loss in uncertain convex quadratically constrained quadratic
programs (QCQP). The first layer solves a deterministic version of the problem,
the second layer evaluates the worst case realisation for an uncertainty set
centred on the observation given the decisions obtained from the first layer.
This enables us to learn model parameterisations that lead to robust decisions
while only solving a simpler deterministic problem at test time. Additionally,
instead of having to solve a robust counterpart we solve two smaller and
potentially easier problems in training. The second layer (worst case problem)
can be seen as a regularisation approach for predict-and-optimise by fitting to
a neighbourhood of problems instead of just a point observation. We motivate
relaxations of the worst-case problem in cases of uncertainty sets that would
otherwise lead to trust region problems, and leverage various relaxations to
deal with uncertain constraints. Both layers are typically strictly convex in
this problem setting and thus have meaningful gradients almost everywhere. We
demonstrate an application of this model on simulated experiments. The method
is an effective regularisation tool for decision-focused learning for uncertain
convex QCQPs.",2312.12485v1,https://arxiv.org/pdf/2312.12485v1
"SkyMask: Attack-agnostic Robust Federated Learning with Fine-grained
  Learnable Masks","Peishen Yan, Hao Wang, Tao Song, Yang Hua, Ruhui Ma, Ningxin Hu, Mohammad R. Haghighat, Haibing Guan","Federated Learning (FL) is becoming a popular paradigm for leveraging
distributed data and preserving data privacy. However, due to the distributed
characteristic, FL systems are vulnerable to Byzantine attacks that compromised
clients attack the global model by uploading malicious model updates. With the
development of layer-level and parameter-level fine-grained attacks, the
attacks' stealthiness and effectiveness have been significantly improved. The
existing defense mechanisms solely analyze the model-level statistics of
individual model updates uploaded by clients to mitigate Byzantine attacks,
which are ineffective against fine-grained attacks due to unawareness or
overreaction. To address this problem, we propose SkyMask, a new
attack-agnostic robust FL system that firstly leverages fine-grained learnable
masks to identify malicious model updates at the parameter level. Specifically,
the FL server freezes and multiplies the model updates uploaded by clients with
the parameter-level masks, and trains the masks over a small clean dataset
(i.e., root dataset) to learn the subtle difference between benign and
malicious model updates in a high-dimension space. Our extensive experiments
involve different models on three public datasets under state-of-the-art (SOTA)
attacks, where the results show that SkyMask achieves up to 14% higher testing
accuracy compared with SOTA defense strategies under the same attacks and
successfully defends against attacks with malicious clients of a high fraction
up to 80%. Code is available at https://github.com/KoalaYan/SkyMask.",2312.12484v2,https://arxiv.org/pdf/2312.12484v2
Robust Stochastic Graph Generator for Counterfactual Explanations,"Mario Alfonso Prado-Romero, Bardh Prenkaj, Giovanni Stilo","Counterfactual Explanation (CE) techniques have garnered attention as a means
to provide insights to the users engaging with AI systems. While extensively
researched in domains such as medical imaging and autonomous vehicles, Graph
Counterfactual Explanation (GCE) methods have been comparatively
under-explored. GCEs generate a new graph similar to the original one, with a
different outcome grounded on the underlying predictive model. Among these GCE
techniques, those rooted in generative mechanisms have received relatively
limited investigation despite demonstrating impressive accomplishments in other
domains, such as artistic styles and natural language modelling. The preference
for generative explainers stems from their capacity to generate counterfactual
instances during inference, leveraging autonomously acquired perturbations of
the input graph. Motivated by the rationales above, our study introduces
RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual
Explanations able to produce counterfactual examples from the learned latent
space considering a partially ordered generation sequence. Furthermore, we
undertake quantitative and qualitative analyses to compare RSGG-CE's
performance against SoA generative explainers, highlighting its increased
ability to engendering plausible counterfactual candidates.",2312.11747v2,https://arxiv.org/pdf/2312.11747v2
DeRDaVa: Deletion-Robust Data Valuation for Machine Learning,"Xiao Tian, Rachael Hwee Ling Sim, Jue Fan, Bryan Kian Hsiang Low","Data valuation is concerned with determining a fair valuation of data from
data sources to compensate them or to identify training examples that are the
most or least useful for predictions. With the rising interest in personal data
ownership and data protection regulations, model owners will likely have to
fulfil more data deletion requests. This raises issues that have not been
addressed by existing works: Are the data valuation scores still fair with
deletions? Must the scores be expensively recomputed? The answer is no. To
avoid recomputations, we propose using our data valuation framework DeRDaVa
upfront for valuing each data source's contribution to preserving robust model
performance after anticipated data deletions. DeRDaVa can be efficiently
approximated and will assign higher values to data that are more useful or less
likely to be deleted. We further generalize DeRDaVa to Risk-DeRDaVa to cater to
risk-averse/seeking model owners who are concerned with the worst/best-cases
model utility. We also empirically demonstrate the practicality of our
solutions.",2312.11413v2,https://arxiv.org/pdf/2312.11413v2
Robust Active Measuring under Model Uncertainty,"Merlijn Krale, Thiago D. Simão, Jana Tumova, Nils Jansen","Partial observability and uncertainty are common problems in sequential
decision-making that particularly impede the use of formal models such as
Markov decision processes (MDPs). However, in practice, agents may be able to
employ costly sensors to measure their environment and resolve partial
observability by gathering information. Moreover, imprecise transition
functions can capture model uncertainty. We combine these concepts and extend
MDPs to robust active-measuring MDPs (RAM-MDPs). We present an active-measure
heuristic to solve RAM-MDPs efficiently and show that model uncertainty can,
counterintuitively, let agents take fewer measurements. We propose a method to
counteract this behavior while only incurring a bounded additional cost. We
empirically compare our methods to several baselines and show their superior
scalability and performance.",2312.11227v1,https://arxiv.org/pdf/2312.11227v1
"TDeLTA: A Light-weight and Robust Table Detection Method based on
  Learning Text Arrangement","Yang Fan, Xiangping Wu, Qingcai Chen, Heng Li, Yan Huang, Zhixiang Cai, Qitian Wu","The diversity of tables makes table detection a great challenge, leading to
existing models becoming more tedious and complex. Despite achieving high
performance, they often overfit to the table style in training set, and suffer
from significant performance degradation when encountering out-of-distribution
tables in other domains. To tackle this problem, we start from the essence of
the table, which is a set of text arranged in rows and columns. Based on this,
we propose a novel, light-weighted and robust Table Detection method based on
Learning Text Arrangement, namely TDeLTA. TDeLTA takes the text blocks as
input, and then models the arrangement of them with a sequential encoder and an
attention module. To locate the tables precisely, we design a
text-classification task, classifying the text blocks into 4 categories
according to their semantic roles in the tables. Experiments are conducted on
both the text blocks parsed from PDF and extracted by open-source OCR tools,
respectively. Compared to several state-of-the-art methods, TDeLTA achieves
competitive results with only 3.1M model parameters on the large-scale public
datasets. Moreover, when faced with the cross-domain data under the 0-shot
setting, TDeLTA outperforms baselines by a large margin of nearly 7%, which
shows the strong robustness and transferability of the proposed model.",2312.11043v1,https://arxiv.org/pdf/2312.11043v1
The Pros and Cons of Adversarial Robustness,"Yacine Izza, Joao Marques-Silva","Robustness is widely regarded as a fundamental problem in the analysis of
machine learning (ML) models. Most often robustness equates with deciding the
non-existence of adversarial examples, where adversarial examples denote
situations where small changes on some inputs cause a change in the prediction.
The perceived importance of ML model robustness explains the continued progress
observed for most of the last decade. Whereas robustness is often assessed
locally, i.e. given some target point in feature space, robustness can also be
defined globally, i.e. where any point in feature space can be considered. The
importance of ML model robustness is illustrated for example by the existence
of competitions evaluating the progress of robustness tools, namely in the case
of neural networks (NNs) but also by efforts towards robustness certification.
More recently, robustness tools have also been used for computing rigorous
explanations of ML models. In contrast with the observed successes of
robustness, this paper uncovers some limitations with existing definitions of
robustness, both global and local, but also with efforts towards robustness
certification. The paper also investigates uses of adversarial examples besides
those related with robustness.",2312.10911v1,https://arxiv.org/pdf/2312.10911v1
"Robust Node Representation Learning via Graph Variational Diffusion
  Networks","Jun Zhuang, Mohammad Al Hasan","Node representation learning by using Graph Neural Networks (GNNs) has been
widely explored. However, in recent years, compelling evidence has revealed
that GNN-based node representation learning can be substantially deteriorated
by delicately-crafted perturbations in a graph structure. To learn robust node
representation in the presence of perturbations, various works have been
proposed to safeguard GNNs. Within these existing works, Bayesian label
transition has been proven to be more effective, but this method is extensively
reliant on a well-built prior distribution. The variational inference could
address this limitation by sampling the latent node embedding from a Gaussian
prior distribution. Besides, leveraging the Gaussian distribution (noise) in
hidden layers is an appealing strategy to strengthen the robustness of GNNs.
However, our experiments indicate that such a strategy can cause over-smoothing
issues during node aggregation. In this work, we propose the Graph Variational
Diffusion Network (GVDN), a new node encoder that effectively manipulates
Gaussian noise to safeguard robustness on perturbed graphs while alleviating
over-smoothing issues through two mechanisms: Gaussian diffusion and node
embedding propagation. Thanks to these two mechanisms, our model can generate
robust node embeddings for recovery. Specifically, we design a retraining
mechanism using the generated node embedding to recover the performance of node
classifications in the presence of perturbations. The experiments verify the
effectiveness of our proposed model across six public datasets.",2312.10903v1,https://arxiv.org/pdf/2312.10903v1
"Exploring Sound vs Vibration for Robust Fault Detection on Rotating
  Machinery","Serkan Kiranyaz, Ozer Can Devecioglu, Amir Alhams, Sadok Sassi, Turker Ince, Onur Avci, Moncef Gabbouj","Robust and real-time detection of faults on rotating machinery has become an
ultimate objective for predictive maintenance in various industries.
Vibration-based Deep Learning (DL) methodologies have become the de facto
standard for bearing fault detection as they can produce state-of-the-art
detection performances under certain conditions. Despite such particular focus
on the vibration signal, the utilization of sound, on the other hand, has been
neglected whilst only a few studies have been proposed during the last two
decades, all of which were based on a conventional ML approach. One major
reason is the lack of a benchmark dataset providing a large volume of both
vibration and sound data over several working conditions for different machines
and sensor locations. In this study, we address this need by presenting the new
benchmark Qatar University Dual-Machine Bearing Fault Benchmark dataset
(QU-DMBF), which encapsulates sound and vibration data from two different
motors operating under 1080 working conditions overall. Then we draw the focus
on the major limitations and drawbacks of vibration-based fault detection due
to numerous installation and operational conditions. Finally, we propose the
first DL approach for sound-based fault detection and perform comparative
evaluations between the sound and vibration over the QU-DMBF dataset. A wide
range of experimental results shows that the sound-based fault detection method
is significantly more robust than its vibration-based counterpart, as it is
entirely independent of the sensor location, cost-effective (requiring no
sensor and sensor maintenance), and can achieve the same level of the best
detection performance by its vibration-based counterpart. With this study, the
QU-DMBF dataset, the optimized source codes in PyTorch, and comparative
evaluations are now publicly shared.",2312.10742v1,https://arxiv.org/pdf/2312.10742v1
Cross-Domain Robustness of Transformer-based Keyphrase Generation,"Anna Glazkova, Dmitry Morozov","Modern models for text generation show state-of-the-art results in many
natural language processing tasks. In this work, we explore the effectiveness
of abstractive text summarization models for keyphrase selection. A list of
keyphrases is an important element of a text in databases and repositories of
electronic documents. In our experiments, abstractive text summarization models
fine-tuned for keyphrase generation show quite high results for a target text
corpus. However, in most cases, the zero-shot performance on other corpora and
domains is significantly lower. We investigate cross-domain limitations of
abstractive text summarization models for keyphrase generation. We present an
evaluation of the fine-tuned BART models for the keyphrase selection task
across six benchmark corpora for keyphrase extraction including scientific
texts from two domains and news texts. We explore the role of transfer learning
between different domains to improve the BART model performance on small text
corpora. Our experiments show that preliminary fine-tuning on out-of-domain
corpora can be effective under conditions of a limited number of samples.",2312.10700v1,https://arxiv.org/pdf/2312.10700v1
"Improving Environment Robustness of Deep Reinforcement Learning
  Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum
  Learning","Rohan Banerjee, Prishita Ray, Mark Campbell","Deep reinforcement learning (RL) approaches have been broadly applied to a
large number of robotics tasks, such as robot manipulation and autonomous
driving. However, an open problem in deep RL is learning policies that are
robust to variations in the environment, which is an important condition for
such systems to be deployed into real-world, unstructured settings. Curriculum
learning is one approach that has been applied to improve generalization
performance in both supervised and reinforcement learning domains, but
selecting the appropriate curriculum to achieve robustness can be a
user-intensive process. In our work, we show that performing probabilistic
inference of the underlying curriculum-reward function using Bayesian
Optimization can be a promising technique for finding a robust curriculum. We
demonstrate that a curriculum found with Bayesian optimization can outperform a
vanilla deep RL agent and a hand-engineered curriculum in the domain of
autonomous racing with obstacle avoidance. Our code is available at
https://github.com/PRISHIta123/Curriculum_RL_for_Driving.",2312.10557v1,https://arxiv.org/pdf/2312.10557v1
Rethinking Robustness of Model Attributions,"Sandesh Kamath, Sankalp Mittal, Amit Deshpande, Vineeth N Balasubramanian","For machine learning models to be reliable and trustworthy, their decisions
must be interpretable. As these models find increasing use in safety-critical
applications, it is important that not just the model predictions but also
their explanations (as feature attributions) be robust to small
human-imperceptible input perturbations. Recent works have shown that many
attribution methods are fragile and have proposed improvements in either these
methods or the model training. We observe two main causes for fragile
attributions: first, the existing metrics of robustness (e.g., top-k
intersection) over-penalize even reasonable local shifts in attribution,
thereby making random perturbations to appear as a strong attack, and second,
the attribution can be concentrated in a small region even when there are
multiple important parts in an image. To rectify this, we propose simple ways
to strengthen existing metrics and attribution methods that incorporate
locality of pixels in robustness metrics and diversity of pixel locations in
attributions. Towards the role of model training in attributional robustness,
we empirically observe that adversarially trained models have more robust
attributions on smaller datasets, however, this advantage disappears in larger
datasets. Code is available at https://github.com/ksandeshk/LENS.",2312.10534v1,https://arxiv.org/pdf/2312.10534v1
"A new method color MS-BSIF Features learning for the robust kinship
  verification","Rachid Aliradi, Abdealmalik Ouamane, Abdeslam Amrane","the paper presents a new method color MS-BSIF learning and MS-LBP for the
kinship verification is the machine's ability to identify the genetic and blood
the relationship and its degree between the facial images of humans. Facial
verification of kinship refers to the task of training a machine to recognize
the blood relationship between a pair of faces parent and non-parent
(verification) based on features extracted from facial images, and determining
the exact type or degree of this genetic relationship. We use the LBP and color
BSIF learning features for the comparison and the TXQDA method for
dimensionality reduction and data classification. We let's test the kinship
facial verification application is namely the kinface Cornell database. This
system improves the robustness of learning while controlling efficiency. The
experimental results obtained and compared to other methods have proven the
reliability of our framework and surpass the performance of other
state-of-the-art techniques.",2312.10482v1,https://arxiv.org/pdf/2312.10482v1
"ResoNet: Robust and Explainable ENSO Forecasts with Hybrid Convolution
  and Transformer Networks","Pumeng Lyu, Tao Tang, Fenghua Ling, Jing-Jia Luo, Niklas Boers, Wanli Ouyang, Lei Bai","Recent studies have shown that deep learning (DL) models can skillfully
predict the El Ni\~no-Southern Oscillation (ENSO) forecasts over 1.5 years
ahead. However, concerns regarding the reliability of predictions made by DL
methods persist, including potential overfitting issues and lack of
interpretability. Here, we propose ResoNet, a DL model that combines
convolutional neural network (CNN) and Transformer architectures. This hybrid
architecture design enables our model to adequately capture local SSTA as well
as long-range inter-basin interactions across oceans. We show that ResoNet can
robustly predict ESNO at lead times between 19 and 26 months, thus
outperforming existing approaches in terms of the forecast horizon. According
to an explainability method applied to ResoNet predictions of El Ni\~no and La
Ni\~na events from 1- to 18-month lead, we find that it predicts the Ni\~no3.4
index based on multiple physically reasonable mechanisms, such as the Recharge
Oscillator concept, Seasonal Footprint Mechanism, and Indian Ocean capacitor
effect. Moreover, we demonstrate that for the first time, the asymmetry between
El Ni\~no and La Ni\~na development can be captured by ResoNet. Our results
could help alleviate skepticism about applying DL models for ENSO prediction
and encourage more attempts to discover and predict climate phenomena using AI
methods.",2312.10429v1,https://arxiv.org/pdf/2312.10429v1
"Robust Communicative Multi-Agent Reinforcement Learning with Active
  Defense","Lebin Yu, Yunbo Qiu, Quanming Yao, Yuan Shen, Xudong Zhang, Jian Wang","Communication in multi-agent reinforcement learning (MARL) has been proven to
effectively promote cooperation among agents recently. Since communication in
real-world scenarios is vulnerable to noises and adversarial attacks, it is
crucial to develop robust communicative MARL technique. However, existing
research in this domain has predominantly focused on passive defense
strategies, where agents receive all messages equally, making it hard to
balance performance and robustness. We propose an active defense strategy,
where agents automatically reduce the impact of potentially harmful messages on
the final decision. There are two challenges to implement this strategy, that
are defining unreliable messages and adjusting the unreliable messages' impact
on the final decision properly. To address them, we design an Active Defense
Multi-Agent Communication framework (ADMAC), which estimates the reliability of
received messages and adjusts their impact on the final decision accordingly
with the help of a decomposable decision structure. The superiority of ADMAC
over existing methods is validated by experiments in three
communication-critical tasks under four types of attacks.",2312.11545v1,https://arxiv.org/pdf/2312.11545v1
"Self-Supervised Disentangled Representation Learning for Robust Target
  Speech Extraction","Zhaoxi Mu, Xinyu Yang, Sining Sun, Qing Yang","Speech signals are inherently complex as they encompass both global acoustic
characteristics and local semantic information. However, in the task of target
speech extraction, certain elements of global and local semantic information in
the reference speech, which are irrelevant to speaker identity, can lead to
speaker confusion within the speech extraction network. To overcome this
challenge, we propose a self-supervised disentangled representation learning
method. Our approach tackles this issue through a two-phase process, utilizing
a reference speech encoding network and a global information disentanglement
network to gradually disentangle the speaker identity information from other
irrelevant factors. We exclusively employ the disentangled speaker identity
information to guide the speech extraction network. Moreover, we introduce the
adaptive modulation Transformer to ensure that the acoustic representation of
the mixed signal remains undisturbed by the speaker embeddings. This component
incorporates speaker embeddings as conditional information, facilitating
natural and efficient guidance for the speech extraction network. Experimental
results substantiate the effectiveness of our meticulously crafted approach,
showcasing a substantial reduction in the likelihood of speaker confusion.",2312.10305v2,https://arxiv.org/pdf/2312.10305v2
Active Reinforcement Learning for Robust Building Control,"Doseok Jang, Larry Yan, Lucas Spangher, Costas Spanos","Reinforcement learning (RL) is a powerful tool for optimal control that has
found great success in Atari games, the game of Go, robotic control, and
building optimization. RL is also very brittle; agents often overfit to their
training environment and fail to generalize to new settings. Unsupervised
environment design (UED) has been proposed as a solution to this problem, in
which the agent trains in environments that have been specially selected to
help it learn. Previous UED algorithms focus on trying to train an RL agent
that generalizes across a large distribution of environments. This is not
necessarily desirable when we wish to prioritize performance in one environment
over others. In this work, we will be examining the setting of robust RL
building control, where we wish to train an RL agent that prioritizes
performing well in normal weather while still being robust to extreme weather
conditions. We demonstrate a novel UED algorithm, ActivePLR, that uses
uncertainty-aware neural network architectures to generate new training
environments at the limit of the RL agent's ability while being able to
prioritize performance in a desired base environment. We show that ActivePLR is
able to outperform state-of-the-art UED algorithms in minimizing energy usage
while maximizing occupant comfort in the setting of building control.",2312.10289v1,https://arxiv.org/pdf/2312.10289v1
"Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse
  Training Data","Kang Lin, Reinhard Heckel","Deep learning based methods for image reconstruction are state-of-the-art for
a variety of imaging tasks. However, neural networks often perform worse if the
training data differs significantly from the data they are applied to. For
example, a model trained for accelerated magnetic resonance imaging (MRI) on
one scanner performs worse on another scanner. In this work, we investigate the
impact of the training data on a model's performance and robustness for
accelerated MRI. We find that models trained on the combination of various data
distributions, such as those obtained from different MRI scanners and
anatomies, exhibit robustness equal or superior to models trained on the best
single distribution for a specific target distribution. Thus training on such
diverse data tends to improve robustness. Furthermore, training on such a
diverse dataset does not compromise in-distribution performance, i.e., a model
trained on diverse data yields in-distribution performance at least as good as
models trained on the more narrow individual distributions. Our results suggest
that training a model for imaging on a variety of distributions tends to yield
a more effective and robust model than maintaining separate models for
individual distributions.",2312.10271v2,https://arxiv.org/pdf/2312.10271v2
"Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against
  Query-Based Attacks","Pascal Zimmer, Sébastien Andreina, Giorgia Azzurra Marson, Ghassan Karame","Although promising, existing defenses against query-based attacks share a
common limitation: they offer increased robustness against attacks at the price
of a considerable accuracy drop on clean samples. In this work, we show how to
efficiently establish, at test-time, a solid tradeoff between robustness and
accuracy when mitigating query-based attacks. Given that these attacks
necessarily explore low-confidence regions, our insight is that activating
dedicated defenses, such as random noise defense and random image
transformations, only for low-confidence inputs is sufficient to prevent them.
Our approach is independent of training and supported by theory. We verify the
effectiveness of our approach for various existing defenses by conducting
extensive experiments on CIFAR-10, CIFAR-100, and ImageNet. Our results confirm
that our proposal can indeed enhance these defenses by providing better
tradeoffs between robustness and accuracy when compared to state-of-the-art
approaches while being completely training-free.",2312.10132v2,https://arxiv.org/pdf/2312.10132v2
Sketch and shift: a robust decoder for compressive clustering,"Ayoub Belhadji, Rémi Gribonval","Compressive learning is an emerging approach to drastically reduce the memory
footprint of large-scale learning, by first summarizing a large dataset into a
low-dimensional sketch vector, and then decoding from this sketch the latent
information needed for learning. In light of recent progress on information
preservation guarantees for sketches based on random features, a major
objective is to design easy-to-tune algorithms (called decoders) to robustly
and efficiently extract this information. To address the underlying non-convex
optimization problems, various heuristics have been proposed. In the case of
compressive clustering, the standard heuristic is CL-OMPR, a variant of sliding
Frank-Wolfe. Yet, CL-OMPR is hard to tune, and the examination of its
robustness was overlooked. In this work, we undertake a scrutinized examination
of CL-OMPR to circumvent its limitations. In particular, we show how this
algorithm can fail to recover the clusters even in advantageous scenarios. To
gain insight, we show how the deficiencies of this algorithm can be attributed
to optimization difficulties related to the structure of a correlation function
appearing at core steps of the algorithm. To address these limitations, we
propose an alternative decoder offering substantial improvements over CL-OMPR.
Its design is notably inspired from the mean shift algorithm, a classic
approach to detect the local maxima of kernel density estimators. The proposed
algorithm can extract clustering information from a sketch of the MNIST dataset
that is 10 times smaller than previously.",2312.09940v2,https://arxiv.org/pdf/2312.09940v2
"Fragility, Robustness and Antifragility in Deep Learning","Chandresh Pravin, Ivan Martino, Giuseppe Nicosia, Varun Ojha","We propose a systematic analysis of deep neural networks (DNNs) based on a
signal processing technique for network parameter removal, in the form of
synaptic filters that identifies the fragility, robustness and antifragility
characteristics of DNN parameters. Our proposed analysis investigates if the
DNN performance is impacted negatively, invariantly, or positively on both
clean and adversarially perturbed test datasets when the DNN undergoes synaptic
filtering. We define three \textit{filtering scores} for quantifying the
fragility, robustness and antifragility characteristics of DNN parameters based
on the performances for (i) clean dataset, (ii) adversarial dataset, and (iii)
the difference in performances of clean and adversarial datasets. We validate
the proposed systematic analysis on ResNet-18, ResNet-50, SqueezeNet-v1.1 and
ShuffleNet V2 x1.0 network architectures for MNIST, CIFAR10 and Tiny ImageNet
datasets. The filtering scores, for a given network architecture, identify
network parameters that are invariant in characteristics across different
datasets over learning epochs. Vice-versa, for a given dataset, the filtering
scores identify the parameters that are invariant in characteristics across
different network architectures. We show that our synaptic filtering method
improves the test accuracy of ResNet and ShuffleNet models on adversarial
datasets when only the robust and antifragile parameters are selectively
retrained at any given epoch, thus demonstrating applications of the proposed
strategy in improving model robustness.",2312.09821v2,https://arxiv.org/pdf/2312.09821v2
"VNN: Verification-Friendly Neural Networks with Hard Robustness
  Guarantees","Anahita Baninajjar, Ahmed Rezine, Amir Aminifar","Machine learning techniques often lack formal correctness guarantees,
evidenced by the widespread adversarial examples that plague most deep-learning
applications. This lack of formal guarantees resulted in several research
efforts that aim at verifying Deep Neural Networks (DNNs), with a particular
focus on safety-critical applications. However, formal verification techniques
still face major scalability and precision challenges. The over-approximation
introduced during the formal verification process to tackle the scalability
challenge often results in inconclusive analysis. To address this challenge, we
propose a novel framework to generate Verification-Friendly Neural Networks
(VNNs). We present a post-training optimization framework to achieve a balance
between preserving prediction performance and verification-friendliness. Our
proposed framework results in VNNs that are comparable to the original DNNs in
terms of prediction performance, while amenable to formal verification
techniques. This essentially enables us to establish robustness for more VNNs
than their DNN counterparts, in a time-efficient manner.",2312.09748v2,https://arxiv.org/pdf/2312.09748v2
"Robustness Verification of Deep Reinforcement Learning Based Control
  Systems using Reward Martingales","Dapeng Zhi, Peixin Wang, Cheng Chen, Min Zhang","Deep Reinforcement Learning (DRL) has gained prominence as an effective
approach for control systems. However, its practical deployment is impeded by
state perturbations that can severely impact system performance. Addressing
this critical challenge requires robustness verification about system
performance, which involves tackling two quantitative questions: (i) how to
establish guaranteed bounds for expected cumulative rewards, and (ii) how to
determine tail bounds for cumulative rewards. In this work, we present the
first approach for robustness verification of DRL-based control systems by
introducing reward martingales, which offer a rigorous mathematical foundation
to characterize the impact of state perturbations on system performance in
terms of cumulative rewards. Our verified results provide provably quantitative
certificates for the two questions. We then show that reward martingales can be
implemented and trained via neural networks, against different types of control
policies. Experimental results demonstrate that our certified bounds tightly
enclose simulation outcomes on various DRL-based control systems, indicating
the effectiveness and generality of the proposed approach.",2312.09695v1,https://arxiv.org/pdf/2312.09695v1
Quilt: Robust Data Segment Selection against Concept Drifts,"Minsu Kim, Seong-Hyeon Hwang, Steven Euijong Whang","Continuous machine learning pipelines are common in industrial settings where
models are periodically trained on data streams. Unfortunately, concept drifts
may occur in data streams where the joint distribution of the data X and label
y, P(X, y), changes over time and possibly degrade model accuracy. Existing
concept drift adaptation approaches mostly focus on updating the model to the
new data possibly using ensemble techniques of previous models and tend to
discard the drifted historical data. However, we contend that explicitly
utilizing the drifted data together leads to much better model accuracy and
propose Quilt, a data-centric framework for identifying and selecting data
segments that maximize model accuracy. To address the potential downside of
efficiency, Quilt extends existing data subset selection techniques, which can
be used to reduce the training data without compromising model accuracy. These
techniques cannot be used as is because they only assume virtual drifts where
the posterior probabilities P(y|X) are assumed not to change. In contrast, a
key challenge in our setup is to also discard undesirable data segments with
concept drifts. Quilt thus discards drifted data segments and selects data
segment subsets holistically for accurate and efficient model training. The two
operations use gradient-based scores, which have little computation overhead.
In our experiments, we show that Quilt outperforms state-of-the-art drift
adaptation and data selection baselines on synthetic and real datasets.",2312.09691v1,https://arxiv.org/pdf/2312.09691v1
Adversarial Robustness on Image Classification with $k$-means,"Rollin Omari, Junae Kim, Paul Montague","In this paper we explore the challenges and strategies for enhancing the
robustness of $k$-means clustering algorithms against adversarial
manipulations. We evaluate the vulnerability of clustering algorithms to
adversarial attacks, emphasising the associated security risks. Our study
investigates the impact of incremental attack strength on training, introduces
the concept of transferability between supervised and unsupervised models, and
highlights the sensitivity of unsupervised models to sample distributions. We
additionally introduce and evaluate an adversarial training method that
improves testing performance in adversarial scenarios, and we highlight the
importance of various parameters in the proposed training method, such as
continuous learning, centroid initialisation, and adversarial step-count.",2312.09533v2,https://arxiv.org/pdf/2312.09533v2
Robust Estimation of Causal Heteroscedastic Noise Models,"Quang-Duy Tran, Bao Duong, Phuoc Nguyen, Thin Nguyen","Distinguishing the cause and effect from bivariate observational data is the
foundational problem that finds applications in many scientific disciplines.
One solution to this problem is assuming that cause and effect are generated
from a structural causal model, enabling identification of the causal direction
after estimating the model in each direction. The heteroscedastic noise model
is a type of structural causal model where the cause can contribute to both the
mean and variance of the noise. Current methods for estimating heteroscedastic
noise models choose the Gaussian likelihood as the optimization objective which
can be suboptimal and unstable when the data has a non-Gaussian distribution.
To address this limitation, we propose a novel approach to estimating this
model with Student's $t$-distribution, which is known for its robustness in
accounting for sampling variability with smaller sample sizes and extreme
values without significantly altering the overall distribution shape. This
adaptability is beneficial for capturing the parameters of the noise
distribution in heteroscedastic noise models. Our empirical evaluations
demonstrate that our estimators are more robust and achieve better overall
performance across synthetic and real benchmarks.",2312.10102v1,https://arxiv.org/pdf/2312.10102v1
"Coevolutionary Algorithm for Building Robust Decision Trees under
  Minimax Regret","Adam Żychowski, Andrew Perrault, Jacek Mańdziuk","In recent years, there has been growing interest in developing robust machine
learning (ML) models that can withstand adversarial attacks, including one of
the most widely adopted, efficient, and interpretable ML algorithms-decision
trees (DTs). This paper proposes a novel coevolutionary algorithm (CoEvoRDT)
designed to create robust DTs capable of handling noisy high-dimensional data
in adversarial contexts. Motivated by the limitations of traditional DT
algorithms, we leverage adaptive coevolution to allow DTs to evolve and learn
from interactions with perturbed input data. CoEvoRDT alternately evolves
competing populations of DTs and perturbed features, enabling construction of
DTs with desired properties. CoEvoRDT is easily adaptable to various target
metrics, allowing the use of tailored robustness criteria such as minimax
regret. Furthermore, CoEvoRDT has potential to improve the results of other
state-of-the-art methods by incorporating their outcomes (DTs they produce)
into the initial population and optimize them in the process of coevolution.
Inspired by the game theory, CoEvoRDT utilizes mixed Nash equilibrium to
enhance convergence. The method is tested on 20 popular datasets and shows
superior performance compared to 4 state-of-the-art algorithms. It outperformed
all competing methods on 13 datasets with adversarial accuracy metrics, and on
all 20 considered datasets with minimax regret. Strong experimental results and
flexibility in choosing the error measure make CoEvoRDT a promising approach
for constructing robust DTs in real-world applications.",2312.09078v1,https://arxiv.org/pdf/2312.09078v1
"Improve Robustness of Reinforcement Learning against Observation
  Perturbations via $l_\infty$ Lipschitz Policy Networks","Buqing Nie, Jingtian Ji, Yangqing Fu, Yue Gao","Deep Reinforcement Learning (DRL) has achieved remarkable advances in
sequential decision tasks. However, recent works have revealed that DRL agents
are susceptible to slight perturbations in observations. This vulnerability
raises concerns regarding the effectiveness and robustness of deploying such
agents in real-world applications. In this work, we propose a novel robust
reinforcement learning method called SortRL, which improves the robustness of
DRL policies against observation perturbations from the perspective of the
network architecture. We employ a novel architecture for the policy network
that incorporates global $l_\infty$ Lipschitz continuity and provide a
convenient method to enhance policy robustness based on the output margin.
Besides, a training framework is designed for SortRL, which solves given tasks
while maintaining robustness against $l_\infty$ bounded perturbations on the
observations. Several experiments are conducted to evaluate the effectiveness
of our method, including classic control tasks and video games. The results
demonstrate that SortRL achieves state-of-the-art robustness performance
against different perturbation strength.",2312.08751v1,https://arxiv.org/pdf/2312.08751v1
"Towards Inductive Robustness: Distilling and Fostering Wave-induced
  Resonance in Transductive GCNs Against Graph Adversarial Attacks","Ao Liu, Wenshan Li, Tao Li, Beibei Li, Hanyuan Huang, Pan Zhou","Graph neural networks (GNNs) have recently been shown to be vulnerable to
adversarial attacks, where slight perturbations in the graph structure can lead
to erroneous predictions. However, current robust models for defending against
such attacks inherit the transductive limitations of graph convolutional
networks (GCNs). As a result, they are constrained by fixed structures and do
not naturally generalize to unseen nodes. Here, we discover that transductive
GCNs inherently possess a distillable robustness, achieved through a
wave-induced resonance process. Based on this, we foster this resonance to
facilitate inductive and robust learning. Specifically, we first prove that the
signal formed by GCN-driven message passing (MP) is equivalent to the
edge-based Laplacian wave, where, within a wave system, resonance can naturally
emerge between the signal and its transmitting medium. This resonance provides
inherent resistance to malicious perturbations inflicted on the signal system.
We then prove that merely three MP iterations within GCNs can induce signal
resonance between nodes and edges, manifesting as a coupling between nodes and
their distillable surrounding local subgraph. Consequently, we present Graph
Resonance-fostering Network (GRN) to foster this resonance via learning node
representations from their distilled resonating subgraphs. By capturing the
edge-transmitted signals within this subgraph and integrating them with the
node signal, GRN embeds these combined signals into the central node's
representation. This node-wise embedding approach allows for generalization to
unseen nodes. We validate our theoretical findings with experiments, and
demonstrate that GRN generalizes robustness to unseen nodes, whilst maintaining
state-of-the-art classification accuracy on perturbed graphs.",2312.08651v1,https://arxiv.org/pdf/2312.08651v1
"Universal Adversarial Framework to Improve Adversarial Robustness for
  Diabetic Retinopathy Detection","Samrat Mukherjee, Dibyanayan Bandyopadhyay, Baban Gain, Asif Ekbal","Diabetic Retinopathy (DR) is a prevalent illness associated with Diabetes
which, if left untreated, can result in irreversible blindness. Deep Learning
based systems are gradually being introduced as automated support for clinical
diagnosis. Since healthcare has always been an extremely important domain
demanding error-free performance, any adversaries could pose a big threat to
the applicability of such systems. In this work, we use Universal Adversarial
Perturbations (UAPs) to quantify the vulnerability of Medical Deep Neural
Networks (DNNs) for detecting DR. To the best of our knowledge, this is the
very first attempt that works on attacking complete fine-grained classification
of DR images using various UAPs. Also, as a part of this work, we use UAPs to
fine-tune the trained models to defend against adversarial samples. We
experiment on several models and observe that the performance of such models
towards unseen adversarial attacks gets boosted on average by $3.41$
Cohen-kappa value and maximum by $31.92$ Cohen-kappa value. The performance
degradation on normal data upon ensembling the fine-tuned models was found to
be statistically insignificant using t-test, highlighting the benefits of
UAP-based adversarial fine-tuning.",2312.08193v1,https://arxiv.org/pdf/2312.08193v1
"Robust Few-Shot Named Entity Recognition with Boundary Discrimination
  and Correlation Purification","Xiaojun Xue, Chunxia Zhang, Tianxiang Xu, Zhendong Niu","Few-shot named entity recognition (NER) aims to recognize novel named
entities in low-resource domains utilizing existing knowledge. However, the
present few-shot NER models assume that the labeled data are all clean without
noise or outliers, and there are few works focusing on the robustness of the
cross-domain transfer learning ability to textual adversarial attacks in
Few-shot NER. In this work, we comprehensively explore and assess the
robustness of few-shot NER models under textual adversarial attack scenario,
and found the vulnerability of existing few-shot NER models. Furthermore, we
propose a robust two-stage few-shot NER method with Boundary Discrimination and
Correlation Purification (BDCP). Specifically, in the span detection stage, the
entity boundary discriminative module is introduced to provide a highly
distinguishing boundary representation space to detect entity spans. In the
entity typing stage, the correlations between entities and contexts are
purified by minimizing the interference information and facilitating
correlation generalization to alleviate the perturbations caused by textual
adversarial attacks. In addition, we construct adversarial examples for
few-shot NER based on public datasets Few-NERD and Cross-Dataset. Comprehensive
evaluations on those two groups of few-shot NER datasets containing adversarial
examples demonstrate the robustness and superiority of the proposed method.",2312.07961v1,https://arxiv.org/pdf/2312.07961v1
"Robust and Performance Incentivizing Algorithms for Multi-Armed Bandits
  with Strategic Agents","Seyed A. Esmaeili, Suho Shin, Aleksandrs Slivkins","We consider a variant of the stochastic multi-armed bandit problem.
Specifically, the arms are strategic agents who can improve their rewards or
absorb them. The utility of an agent increases if she is pulled more or absorbs
more of her rewards but decreases if she spends more effort improving her
rewards. Agents have heterogeneous properties, specifically having different
means and able to improve their rewards up to different levels. Further, a
non-empty subset of agents are ''honest'' and in the worst case always give
their rewards without absorbing any part. The principal wishes to obtain a high
revenue (cumulative reward) by designing a mechanism that incentives top level
performance at equilibrium. At the same time, the principal wishes to be robust
and obtain revenue at least at the level of the honest agent with the highest
mean in case of non-equilibrium behaviour. We identify a class of MAB
algorithms which we call performance incentivizing which satisfy a collection
of properties and show that they lead to mechanisms that incentivize top level
performance at equilibrium and are robust under any strategy profile.
Interestingly, we show that UCB is an example of such a MAB algorithm. Further,
in the case where the top performance level is unknown we show that combining
second price auction ideas with performance incentivizing algorithms achieves
performance at least at the second top level while also being robust.",2312.07929v1,https://arxiv.org/pdf/2312.07929v1
On Robustness to Missing Video for Audiovisual Speech Recognition,"Oscar Chang, Otavio Braga, Hank Liao, Dmitriy Serdyuk, Olivier Siohan","It has been shown that learning audiovisual features can lead to improved
speech recognition performance over audio-only features, especially for noisy
speech. However, in many common applications, the visual features are partially
or entirely missing, e.g.~the speaker might move off screen. Multi-modal models
need to be robust: missing video frames should not degrade the performance of
an audiovisual model to be worse than that of a single-modality audio-only
model. While there have been many attempts at building robust models, there is
little consensus on how robustness should be evaluated. To address this, we
introduce a framework that allows claims about robustness to be evaluated in a
precise and testable way. We also conduct a systematic empirical study of the
robustness of common audiovisual speech recognition architectures on a range of
acoustic noise conditions and test suites. Finally, we show that an
architecture-agnostic solution based on cascades can consistently achieve
robustness to missing video, even in settings where existing techniques for
robustness like dropout fall short.",2312.10088v2,https://arxiv.org/pdf/2312.10088v2
"Video Dynamics Prior: An Internal Learning Approach for Robust Video
  Enhancements","Gaurav Shrivastava, Ser-Nam Lim, Abhinav Shrivastava","In this paper, we present a novel robust framework for low-level vision
tasks, including denoising, object removal, frame interpolation, and
super-resolution, that does not require any external training data corpus. Our
proposed approach directly learns the weights of neural modules by optimizing
over the corrupted test sequence, leveraging the spatio-temporal coherence and
internal statistics of videos. Furthermore, we introduce a novel spatial
pyramid loss that leverages the property of spatio-temporal patch recurrence in
a video across the different scales of the video. This loss enhances robustness
to unstructured noise in both the spatial and temporal domains. This further
results in our framework being highly robust to degradation in input frames and
yields state-of-the-art results on downstream tasks such as denoising, object
removal, and frame interpolation. To validate the effectiveness of our
approach, we conduct qualitative and quantitative evaluations on standard video
datasets such as DAVIS, UCF-101, and VIMEO90K-T.",2312.07835v1,https://arxiv.org/pdf/2312.07835v1
"Radio Signal Classification by Adversarially Robust Quantum Machine
  Learning","Yanqiu Wu, Eromanga Adermann, Chandra Thapa, Seyit Camtepe, Hajime Suzuki, Muhammad Usman","Radio signal classification plays a pivotal role in identifying the
modulation scheme used in received radio signals, which is essential for
demodulation and proper interpretation of the transmitted information.
Researchers have underscored the high susceptibility of ML algorithms for radio
signal classification to adversarial attacks. Such vulnerability could result
in severe consequences, including misinterpretation of critical messages,
interception of classified information, or disruption of communication
channels. Recent advancements in quantum computing have revolutionized theories
and implementations of computation, bringing the unprecedented development of
Quantum Machine Learning (QML). It is shown that quantum variational
classifiers (QVCs) provide notably enhanced robustness against classical
adversarial attacks in image classification. However, no research has yet
explored whether QML can similarly mitigate adversarial threats in the context
of radio signal classification. This work applies QVCs to radio signal
classification and studies their robustness to various adversarial attacks. We
also propose the novel application of the approximate amplitude encoding (AAE)
technique to encode radio signal data efficiently. Our extensive simulation
results present that attacks generated on QVCs transfer well to CNN models,
indicating that these adversarial examples can fool neural networks that they
are not explicitly designed to attack. However, the converse is not true. QVCs
primarily resist the attacks generated on CNNs. Overall, with comprehensive
simulations, our results shed new light on the growing field of QML by bridging
knowledge gaps in QAML in radio signal classification and uncovering the
advantages of applying QML methods in practical applications.",2312.07821v1,https://arxiv.org/pdf/2312.07821v1
Robust MRI Reconstruction by Smoothed Unrolling (SMUG),"Shijun Liang, Van Hoang Minh Nguyen, Jinghan Jia, Ismail Alkhouri, Sijia Liu, Saiprasad Ravishankar","As the popularity of deep learning (DL) in the field of magnetic resonance
imaging (MRI) continues to rise, recent research has indicated that DL-based
MRI reconstruction models might be excessively sensitive to minor input
disturbances, including worst-case additive perturbations. This sensitivity
often leads to unstable, aliased images. This raises the question of how to
devise DL techniques for MRI reconstruction that can be robust to train-test
variations. To address this problem, we propose a novel image reconstruction
framework, termed Smoothed Unrolling (SMUG), which advances a deep
unrolling-based MRI reconstruction model using a randomized smoothing
(RS)-based robust learning approach. RS, which improves the tolerance of a
model against input noises, has been widely used in the design of adversarial
defense approaches for image classification tasks. Yet, we find that the
conventional design that applies RS to the entire DL-based MRI model is
ineffective. In this paper, we show that SMUG and its variants address the
above issue by customizing the RS process based on the unrolling architecture
of a DL-based MRI reconstruction model. Compared to the vanilla RS approach, we
show that SMUG improves the robustness of MRI reconstruction with respect to a
diverse set of instability sources, including worst-case and random noise
perturbations to input measurements, varying measurement sampling rates, and
different numbers of unrolling steps. Furthermore, we theoretically analyze the
robustness of our method in the presence of perturbations.",2312.07784v1,https://arxiv.org/pdf/2312.07784v1
"ReRoGCRL: Representation-based Robustness in Goal-Conditioned
  Reinforcement Learning","Xiangyu Yin, Sihao Wu, Jiaxu Liu, Meng Fang, Xingyu Zhao, Xiaowei Huang, Wenjie Ruan","While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention,
its algorithmic robustness against adversarial perturbations remains
unexplored. The attacks and robust representation training methods that are
designed for traditional RL become less effective when applied to GCRL. To
address this challenge, we first propose the Semi-Contrastive Representation
attack, a novel approach inspired by the adversarial contrastive attack. Unlike
existing attacks in RL, it only necessitates information from the policy
function and can be seamlessly implemented during deployment. Then, to mitigate
the vulnerability of existing GCRL algorithms, we introduce Adversarial
Representation Tactics, which combines Semi-Contrastive Adversarial
Augmentation with Sensitivity-Aware Regularizer to improve the adversarial
robustness of the underlying RL agent against various types of perturbations.
Extensive experiments validate the superior performance of our attack and
defence methods across multiple state-of-the-art GCRL algorithms. Our tool
ReRoGCRL is available at https://github.com/TrustAI/ReRoGCRL.",2312.07392v3,https://arxiv.org/pdf/2312.07392v3
Analyze the Robustness of Classifiers under Label Noise,"Cheng Zeng, Yixuan Xu, Jiaqi Tian","This study explores the robustness of label noise classifiers, aiming to
enhance model resilience against noisy data in complex real-world scenarios.
Label noise in supervised learning, characterized by erroneous or imprecise
labels, significantly impairs model performance. This research focuses on the
increasingly pertinent issue of label noise's impact on practical applications.
Addressing the prevalent challenge of inaccurate training data labels, we
integrate adversarial machine learning (AML) and importance reweighting
techniques. Our approach involves employing convolutional neural networks (CNN)
as the foundational model, with an emphasis on parameter adjustment for
individual training samples. This strategy is designed to heighten the model's
focus on samples critically influencing performance.",2312.07271v1,https://arxiv.org/pdf/2312.07271v1
"Toward Robustness in Multi-label Classification: A Data Augmentation
  Strategy against Imbalance and Noise","Hwanjun Song, Minseok Kim, Jae-Gil Lee","Multi-label classification poses challenges due to imbalanced and noisy
labels in training data. We propose a unified data augmentation method, named
BalanceMix, to address these challenges. Our approach includes two samplers for
imbalanced labels, generating minority-augmented instances with high diversity.
It also refines multi-labels at the label-wise granularity, categorizing noisy
labels as clean, re-labeled, or ambiguous for robust optimization. Extensive
experiments on three benchmark datasets demonstrate that BalanceMix outperforms
existing state-of-the-art methods. We release the code at
https://github.com/DISL-Lab/BalanceMix.",2312.07087v1,https://arxiv.org/pdf/2312.07087v1
"Predictive variational autoencoder for learning robust representations
  of time-series data","Julia Huiming Wang, Dexter Tsin, Tatiana Engel","Variational autoencoders (VAEs) have been used extensively to discover
low-dimensional latent factors governing neural activity and animal behavior.
However, without careful model selection, the uncovered latent factors may
reflect noise in the data rather than true underlying features, rendering such
representations unsuitable for scientific interpretation. Existing solutions to
this problem involve introducing additional measured variables or data
augmentations specific to a particular data type. We propose a VAE architecture
that predicts the next point in time and show that it mitigates the learning of
spurious features. In addition, we introduce a model selection metric based on
smoothness over time in the latent space. We show that together these two
constraints on VAEs to be smooth over time produce robust latent
representations and faithfully recover latent factors on synthetic datasets.",2312.06932v1,https://arxiv.org/pdf/2312.06932v1
Sparse but Strong: Crafting Adversarially Robust Graph Lottery Tickets,"Subhajit Dutta Chowdhury, Zhiyu Ni, Qingyuan Peng, Souvik Kundu, Pierluigi Nuzzo","Graph Lottery Tickets (GLTs), comprising a sparse adjacency matrix and a
sparse graph neural network (GNN), can significantly reduce the inference
latency and compute footprint compared to their dense counterparts. Despite
these benefits, their performance against adversarial structure perturbations
remains to be fully explored. In this work, we first investigate the resilience
of GLTs against different structure perturbation attacks and observe that they
are highly vulnerable and show a large drop in classification accuracy. Based
on this observation, we then present an adversarially robust graph
sparsification (ARGS) framework that prunes the adjacency matrix and the GNN
weights by optimizing a novel loss function capturing the graph homophily
property and information associated with both the true labels of the train
nodes and the pseudo labels of the test nodes. By iteratively applying ARGS to
prune both the perturbed graph adjacency matrix and the GNN model weights, we
can find adversarially robust graph lottery tickets that are highly sparse yet
achieve competitive performance under different untargeted training-time
structure attacks. Evaluations conducted on various benchmarks, considering
different poisoning structure attacks, namely, PGD, MetaAttack, Meta-PGD, and
PR-BCD demonstrate that the GLTs generated by ARGS can significantly improve
the robustness, even when subjected to high levels of sparsity.",2312.06568v1,https://arxiv.org/pdf/2312.06568v1
Promoting Counterfactual Robustness through Diversity,"Francesco Leofante, Nico Potyka","Counterfactual explanations shed light on the decisions of black-box models
by explaining how an input can be altered to obtain a favourable decision from
the model (e.g., when a loan application has been rejected). However, as noted
recently, counterfactual explainers may lack robustness in the sense that a
minor change in the input can cause a major change in the explanation. This can
cause confusion on the user side and open the door for adversarial attacks. In
this paper, we study some sources of non-robustness. While there are
fundamental reasons for why an explainer that returns a single counterfactual
cannot be robust in all instances, we show that some interesting robustness
guarantees can be given by reporting multiple rather than a single
counterfactual. Unfortunately, the number of counterfactuals that need to be
reported for the theoretical guarantees to hold can be prohibitively large. We
therefore propose an approximation algorithm that uses a diversity criterion to
select a feasible number of most relevant explanations and study its robustness
empirically. Our experiments indicate that our method improves the
state-of-the-art in generating robust explanations, while maintaining other
desirable properties and providing competitive computational performance.",2312.06564v2,https://arxiv.org/pdf/2312.06564v2
Robust Graph Neural Network based on Graph Denoising,"Victor M. Tenorio, Samuel Rey, Antonio G. Marques","Graph Neural Networks (GNNs) have emerged as a notorious alternative to
address learning problems dealing with non-Euclidean datasets. However,
although most works assume that the graph is perfectly known, the observed
topology is prone to errors stemming from observational noise, graph-learning
limitations, or adversarial attacks. If ignored, these perturbations may
drastically hinder the performance of GNNs. To address this limitation, this
work proposes a robust implementation of GNNs that explicitly accounts for the
presence of perturbations in the observed topology. For any task involving
GNNs, our core idea is to i) solve an optimization problem not only over the
learnable parameters of the GNN but also over the true graph, and ii) augment
the fitting cost with a term accounting for discrepancies on the graph.
Specifically, we consider a convolutional GNN based on graph filters and follow
an alternating optimization approach to handle the (non-differentiable and
constrained) optimization problem by combining gradient descent and projected
proximal updates. The resulting algorithm is not limited to a particular type
of graph and is amenable to incorporating prior information about the
perturbations. Finally, we assess the performance of the proposed method
through several numerical experiments.",2312.06557v1,https://arxiv.org/pdf/2312.06557v1
"VisionTraj: A Noise-Robust Trajectory Recovery Framework based on
  Large-scale Camera Network","Zhishuai Li, Ziyue Li, Xiaoru Hu, Guoqing Du, Yunhao Nie, Feng Zhu, Lei Bai, Rui Zhao","Trajectory recovery based on the snapshots from the city-wide multi-camera
network facilitates urban mobility sensing and driveway optimization. The
state-of-the-art solutions devoted to such a vision-based scheme typically
incorporate predefined rules or unsupervised iterative feedback, struggling
with multi-fold challenges such as lack of open-source datasets for training
the whole pipeline, and the vulnerability to the noises from visual inputs. In
response to the dilemma, this paper proposes VisionTraj, the first
learning-based model that reconstructs vehicle trajectories from snapshots
recorded by road network cameras. Coupled with it, we elaborate on two rational
vision-trajectory datasets, which produce extensive trajectory data along with
corresponding visual snapshots, enabling supervised vision-trajectory interplay
extraction. Following the data creation, based on the results from the
off-the-shelf multi-modal vehicle clustering, we first re-formulate the
trajectory recovery problem as a generative task and introduce the canonical
Transformer as the autoregressive backbone. Then, to identify clustering noises
(e.g., false positives) with the bound on the snapshots' spatiotemporal
dependencies, a GCN-based soft-denoising module is conducted based on the fine-
and coarse-grained Re-ID clusters. Additionally, we harness strong semantic
information extracted from the tracklet to provide detailed insights into the
vehicle's entry and exit actions during trajectory recovery. The denoising and
tracklet components can also act as plug-and-play modules to boost baselines.
Experimental results on the two hand-crafted datasets show that the proposed
VisionTraj achieves a maximum +11.5% improvement against the sub-best model.",2312.06428v1,https://arxiv.org/pdf/2312.06428v1
"Partial End-to-end Reinforcement Learning for Robustness Against
  Modelling Error in Autonomous Racing","Andrew Murdoch, Johannes Cornelius Schoeman, Hendrik Willem Jordaan","In this paper, we address the issue of increasing the performance of
reinforcement learning (RL) solutions for autonomous racing cars when
navigating under conditions where practical vehicle modelling errors (commonly
known as \emph{model mismatches}) are present. To address this challenge, we
propose a partial end-to-end algorithm that decouples the planning and control
tasks. Within this framework, an RL agent generates a trajectory comprising a
path and velocity, which is subsequently tracked using a pure pursuit steering
controller and a proportional velocity controller, respectively. In contrast,
many current learning-based (i.e., reinforcement and imitation learning)
algorithms utilise an end-to-end approach whereby a deep neural network
directly maps from sensor data to control commands. By leveraging the
robustness of a classical controller, our partial end-to-end driving algorithm
exhibits better robustness towards model mismatches than standard end-to-end
algorithms.",2312.06406v2,https://arxiv.org/pdf/2312.06406v2
"A Robust Mixed-Effects Bandit Algorithm for Assessing Mobile Health
  Interventions","Easton K. Huch, Jieru Shi, Madeline R. Abbott, Jessica R. Golbus, Alexander Moreno, Walter H. Dempsey","Mobile health leverages personalized, contextually-tailored interventions
optimized through bandit and reinforcement learning algorithms. Despite its
promise, challenges like participant heterogeneity, nonstationarity, and
nonlinearity in rewards hinder algorithm performance. We propose a robust
contextual bandit algorithm, termed ""DML-TS-NNR"", that simultaneously addresses
these challenges via (1) modeling the differential reward with user- and
time-specific incidental parameters, (2) network cohesion penalties, and (3)
debiased machine learning for flexible estimation of baseline rewards. We
establish a high-probability regret bound that depends solely on the dimension
of the differential reward model. This feature enables us to achieve robust
regret bounds even when the baseline reward is highly complex. We demonstrate
the superior performance of the DML-TS-NNR algorithm in a simulation and two
off-policy evaluation studies.",2312.06403v3,https://arxiv.org/pdf/2312.06403v3
"BoschAI @ Causal News Corpus 2023: Robust Cause-Effect Span Extraction
  using Multi-Layer Sequence Tagging and Data Augmentation","Timo Pierre Schrader, Simon Razniewski, Lukas Lange, Annemarie Friedrich","Understanding causality is a core aspect of intelligence. The Event Causality
Identification with Causal News Corpus Shared Task addresses two aspects of
this challenge: Subtask 1 aims at detecting causal relationships in texts, and
Subtask 2 requires identifying signal words and the spans that refer to the
cause or effect, respectively. Our system, which is based on pre-trained
transformers, stacked sequence tagging, and synthetic data augmentation, ranks
third in Subtask 1 and wins Subtask 2 with an F1 score of 72.8, corresponding
to a margin of 13 pp. to the second-best system.",2312.06338v1,https://arxiv.org/pdf/2312.06338v1
Data-Free Hard-Label Robustness Stealing Attack,"Xiaojian Yuan, Kejiang Chen, Wen Huang, Jie Zhang, Weiming Zhang, Nenghai Yu","The popularity of Machine Learning as a Service (MLaaS) has led to increased
concerns about Model Stealing Attacks (MSA), which aim to craft a clone model
by querying MLaaS. Currently, most research on MSA assumes that MLaaS can
provide soft labels and that the attacker has a proxy dataset with a similar
distribution. However, this fails to encapsulate the more practical scenario
where only hard labels are returned by MLaaS and the data distribution remains
elusive. Furthermore, most existing work focuses solely on stealing the model
accuracy, neglecting the model robustness, while robustness is essential in
security-sensitive scenarios, e.g., face-scan payment. Notably, improving model
robustness often necessitates the use of expensive techniques such as
adversarial training, thereby further making stealing robustness a more
lucrative prospect. In response to these identified gaps, we introduce a novel
Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which
enables the stealing of both model accuracy and robustness by simply querying
hard labels of the target model without the help of any natural data.
Comprehensive experiments demonstrate the effectiveness of our method. The
clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51%
against AutoAttack, which are only 4.71% and 8.40% lower than the target model
on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is
available at: https://github.com/LetheSec/DFHL-RS-Attack.",2312.05924v2,https://arxiv.org/pdf/2312.05924v2
"Improving Adversarial Robust Fairness via Anti-Bias Soft Label
  Distillation","Shiji Zhao, Xizhe Wang, Xingxing Wei","Adversarial Training (AT) has been widely proved to be an effective method to
improve the adversarial robustness against adversarial examples for Deep Neural
Networks (DNNs). As a variant of AT, Adversarial Robustness Distillation (ARD)
has demonstrated its superior performance in improving the robustness of small
student models with the guidance of large teacher models. However, both AT and
ARD encounter the robust fairness problem: these models exhibit strong
robustness when facing part of classes (easy class), but weak robustness when
facing others (hard class). In this paper, we give an in-depth analysis of the
potential factors and argue that the smoothness degree of samples' soft labels
for different classes (i.e., hard class or easy class) will affect the robust
fairness of DNN models from both empirical observation and theoretical
analysis. Based on the above finding, we propose an Anti-Bias Soft Label
Distillation (ABSLD) method to mitigate the adversarial robust fairness problem
within the framework of Knowledge Distillation (KD). Specifically, ABSLD
adaptively reduces the student's error risk gap between different classes to
achieve fairness by adjusting the class-wise smoothness degree of samples' soft
labels during the training process, and the smoothness degree of soft labels is
controlled by assigning different temperatures in KD to different classes.
Extensive experiments demonstrate that ABSLD outperforms state-of-the-art AT,
ARD, and robust fairness methods in terms of overall performance of robustness
and fairness.",2312.05508v1,https://arxiv.org/pdf/2312.05508v1
"Poisoning $\times$ Evasion: Symbiotic Adversarial Robustness for Graph
  Neural Networks","Ege Erdogan, Simon Geisler, Stephan Günnemann","It is well-known that deep learning models are vulnerable to small input
perturbations. Such perturbed instances are called adversarial examples.
Adversarial examples are commonly crafted to fool a model either at training
time (poisoning) or test time (evasion). In this work, we study the symbiosis
of poisoning and evasion. We show that combining both threat models can
substantially improve the devastating efficacy of adversarial attacks.
Specifically, we study the robustness of Graph Neural Networks (GNNs) under
structure perturbations and devise a memory-efficient adaptive end-to-end
attack for the novel threat model using first-order optimization.",2312.05502v1,https://arxiv.org/pdf/2312.05502v1
"MIMIR: Masked Image Modeling for Mutual Information-based Adversarial
  Robustness","Xiaoyun Xu, Shujian Yu, Jingzheng Wu, Stjepan Picek","Vision Transformers (ViTs) achieve superior performance on various tasks
compared to convolutional neural networks (CNNs), but ViTs are also vulnerable
to adversarial attacks. Adversarial training is one of the most successful
methods to build robust CNN models. Thus, recent works explored new
methodologies for adversarial training of ViTs based on the differences between
ViTs and CNNs, such as better training strategies, preventing attention from
focusing on a single block, or discarding low-attention embeddings. However,
these methods still follow the design of traditional supervised adversarial
training, limiting the potential of adversarial training on ViTs. This paper
proposes a novel defense method, MIMIR, which aims to build a different
adversarial training methodology by utilizing Masked Image Modeling at
pre-training. We create an autoencoder that accepts adversarial examples as
input but takes the clean examples as the modeling target. Then, we create a
mutual information (MI) penalty following the idea of the Information
Bottleneck. Among the two information source inputs and corresponding
adversarial perturbation, the perturbation information is eliminated due to the
constraint of the modeling target. Next, we provide a theoretical analysis of
MIMIR using the bounds of the MI penalty. We also design two adaptive attacks
when the adversary is aware of the MIMIR defense and show that MIMIR still
performs well. The experimental results show that MIMIR improves (natural and
adversarial) accuracy on average by 4.19% on CIFAR-10 and 5.52% on ImageNet-1K,
compared to baselines. On Tiny-ImageNet, we obtained improved natural accuracy
of 2.99\% on average and comparable adversarial accuracy. Our code and trained
models are publicly available https://github.com/xiaoyunxxy/MIMIR.",2312.04960v2,https://arxiv.org/pdf/2312.04960v2
"HC-Ref: Hierarchical Constrained Refinement for Robust Adversarial
  Training of GNNs","Xiaobing Pei, Haoran Yang, Gang Shen","Recent studies have shown that attackers can catastrophically reduce the
performance of GNNs by maliciously modifying the graph structure or node
features on the graph. Adversarial training, which has been shown to be one of
the most effective defense mechanisms against adversarial attacks in computer
vision, holds great promise for enhancing the robustness of GNNs. There is
limited research on defending against attacks by performing adversarial
training on graphs, and it is crucial to delve deeper into this approach to
optimize its effectiveness. Therefore, based on robust adversarial training on
graphs, we propose a hierarchical constraint refinement framework (HC-Ref) that
enhances the anti-perturbation capabilities of GNNs and downstream classifiers
separately, ultimately leading to improved robustness. We propose corresponding
adversarial regularization terms that are conducive to adaptively narrowing the
domain gap between the normal part and the perturbation part according to the
characteristics of different layers, promoting the smoothness of the predicted
distribution of both parts. Moreover, existing research on graph robust
adversarial training primarily concentrates on training from the standpoint of
node feature perturbations and seldom takes into account alterations in the
graph structure. This limitation makes it challenging to prevent attacks based
on topological changes in the graph. This paper generates adversarial examples
by utilizing graph structure perturbations, offering an effective approach to
defend against attack methods that are based on topological changes. Extensive
experiments on two real-world graph benchmarks show that HC-Ref successfully
resists various attacks and has better node classification performance compared
to several baseline methods.",2312.04879v1,https://arxiv.org/pdf/2312.04879v1
"A Robust and Efficient Boundary Point Detection Method by Measuring
  Local Direction Dispersion","Dehua Peng, Zhipeng Gui, Huayi Wu","Boundary points pose a significant challenge for machine learning tasks,
including classification, clustering, and dimensionality reduction. Due to the
similarity of features, boundary areas can result in mixed-up classes or
clusters, leading to a crowding problem in dimensionality reduction. To address
this challenge, numerous boundary point detection methods have been developed,
but they are insufficiently to accurately and efficiently identify the boundary
points in non-convex structures and high-dimensional manifolds. In this work,
we propose a robust and efficient method for detecting boundary points using
Local Direction Dispersion (LoDD). LoDD considers that internal points are
surrounded by neighboring points in all directions, while neighboring points of
a boundary point tend to be distributed only in a certain directional range.
LoDD adopts a density-independent K-Nearest Neighbors (KNN) method to determine
neighboring points, and defines a statistic-based metric using the eigenvalues
of the covariance matrix of KNN coordinates to measure the centrality of a
query point. We demonstrated the validity of LoDD on five synthetic datasets
(2-D and 3-D) and ten real-world benchmarks, and tested its clustering
performance by equipping with two typical clustering methods, K-means and Ncut.
Our results show that LoDD achieves promising and robust detection accuracy in
a time-efficient manner.",2312.04065v1,https://arxiv.org/pdf/2312.04065v1
"RoAST: Robustifying Language Models via Adversarial Perturbation with
  Selective Training","Jaehyung Kim, Yuning Mao, Rui Hou, Hanchao Yu, Davis Liang, Pascale Fung, Qifan Wang, Fuli Feng, Lifu Huang, Madian Khabsa","Fine-tuning pre-trained language models (LMs) has become the de facto
standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to
robustness issues, such as adversarial robustness and model calibration.
Several perspectives of robustness for LMs have been studied independently, but
lacking a unified consideration in multiple perspectives. In this paper, we
propose Robustifying LMs via Adversarial perturbation with Selective Training
(RoAST), a simple yet effective fine-tuning technique to enhance the
multi-perspective robustness of LMs in a unified way. RoAST effectively
incorporates two important sources for the model robustness, robustness on the
perturbed inputs and generalizable knowledge in pre-trained LMs. To be
specific, RoAST introduces adversarial perturbation during fine-tuning while
the model parameters are selectively updated upon their relative importance to
minimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by
incorporating four representative perspectives of model robustness, we
demonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning
methods on six different types of LMs, which indicates its usefulness in
practice.",2312.04032v1,https://arxiv.org/pdf/2312.04032v1
"Node-aware Bi-smoothing: Certified Robustness against Graph Injection
  Attacks","Yuni Lai, Yulin Zhu, Bailin Pan, Kai Zhou","Deep Graph Learning (DGL) has emerged as a crucial technique across various
domains. However, recent studies have exposed vulnerabilities in DGL models,
such as susceptibility to evasion and poisoning attacks. While empirical and
provable robustness techniques have been developed to defend against graph
modification attacks (GMAs), the problem of certified robustness against graph
injection attacks (GIAs) remains largely unexplored. To bridge this gap, we
introduce the node-aware bi-smoothing framework, which is the first certifiably
robust approach for general node classification tasks against GIAs. Notably,
the proposed node-aware bi-smoothing scheme is model-agnostic and is applicable
for both evasion and poisoning attacks. Through rigorous theoretical analysis,
we establish the certifiable conditions of our smoothing scheme. We also
explore the practical implications of our node-aware bi-smoothing schemes in
two contexts: as an empirical defense approach against real-world GIAs and in
the context of recommendation systems. Furthermore, we extend two
state-of-the-art certified robustness frameworks to address node injection
attacks and compare our approach against them. Extensive evaluations
demonstrate the effectiveness of our proposed certificates.",2312.03979v1,https://arxiv.org/pdf/2312.03979v1
Lite-Mind: Towards Efficient and Robust Brain Representation Network,"Zixuan Gong, Qi Zhang, Guangyin Bao, Lei Zhu, Ke Liu, Liang Hu, Duoqian Miao, Yu Zhang","The limited data availability and the low signal-to-noise ratio of fMRI
signals lead to the challenging task of fMRI-to-image retrieval.
State-of-the-art MindEye remarkably improves fMRI-to-image retrieval
performance by leveraging a large model, i.e., a 996M MLP Backbone per subject,
to align fMRI embeddings to the final hidden layer of CLIP's Vision Transformer
(ViT). However, significant individual variations exist among subjects, even
under identical experimental setups, mandating the training of large
subject-specific models. The substantial parameters pose significant challenges
in deploying fMRI decoding on practical devices. To this end, we propose
Lite-Mind, a lightweight, efficient, and robust brain representation learning
paradigm based on Discrete Fourier Transform (DFT), which efficiently aligns
fMRI voxels to fine-grained information of CLIP. We elaborately design a DFT
backbone with Spectrum Compression and Frequency Projector modules to learn
informative and robust voxel embeddings. Our experiments demonstrate that
Lite-Mind achieves an impressive 94.6% fMRI-to-image retrieval accuracy on the
NSD dataset for Subject 1, with 98.7% fewer parameters than MindEye. Lite-Mind
is also proven to be able to be migrated to smaller fMRI datasets and
establishes a new state-of-the-art for zero-shot classification on the GOD
dataset.",2312.03781v4,https://arxiv.org/pdf/2312.03781v4
f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization,"Sina Baharlouei, Shivam Patel, Meisam Razaviyayn","Training and deploying machine learning models that meet fairness criteria
for protected groups are fundamental in modern artificial intelligence. While
numerous constraints and regularization terms have been proposed in the
literature to promote fairness in machine learning tasks, most of these methods
are not amenable to stochastic optimization due to the complex and nonlinear
structure of constraints and regularizers. Here, the term ""stochastic"" refers
to the ability of the algorithm to work with small mini-batches of data.
Motivated by the limitation of existing literature, this paper presents a
unified stochastic optimization framework for fair empirical risk minimization
based on f-divergence measures (f-FERM). The proposed stochastic algorithm
enjoys theoretical convergence guarantees. In addition, our experiments
demonstrate the superiority of fairness-accuracy tradeoffs offered by f-FERM
for almost all batch sizes (ranging from full-batch to batch size of one).
Moreover, we show that our framework can be extended to the case where there is
a distribution shift from training to the test data. Our extension is based on
a distributionally robust optimization reformulation of f-FERM objective under
$L_p$ norms as uncertainty sets. Again, in this distributionally robust
setting, f-FERM not only enjoys theoretical convergence guarantees but also
outperforms other baselines in the literature in the tasks involving
distribution shifts. An efficient stochastic implementation of $f$-FERM is
publicly available.",2312.03259v2,https://arxiv.org/pdf/2312.03259v2
"A Simple Framework to Enhance the Adversarial Robustness of Deep
  Learning-based Intrusion Detection System","Xinwei Yuan, Shu Han, Wei Huang, Hongliang Ye, Xianglong Kong, Fan Zhang","Deep learning based intrusion detection systems (DL-based IDS) have emerged
as one of the best choices for providing security solutions against various
network intrusion attacks. However, due to the emergence and development of
adversarial deep learning technologies, it becomes challenging for the adoption
of DL models into IDS. In this paper, we propose a novel IDS architecture that
can enhance the robustness of IDS against adversarial attacks by combining
conventional machine learning (ML) models and Deep Learning models. The
proposed DLL-IDS consists of three components: DL-based IDS, adversarial
example (AE) detector, and ML-based IDS. We first develop a novel AE detector
based on the local intrinsic dimensionality (LID). Then, we exploit the low
attack transferability between DL models and ML models to find a robust ML
model that can assist us in determining the maliciousness of AEs. If the input
traffic is detected as an AE, the ML-based IDS will predict the maliciousness
of input traffic, otherwise the DL-based IDS will work for the prediction. The
fusion mechanism can leverage the high prediction accuracy of DL models and low
attack transferability between DL models and ML models to improve the
robustness of the whole system. In our experiments, we observe a significant
improvement in the prediction performance of the IDS when subjected to
adversarial attack, achieving high accuracy with low resource consumption.",2312.03245v1,https://arxiv.org/pdf/2312.03245v1
"REST: Enhancing Group Robustness in DNNs through Reweighted Sparse
  Training","Jiaxu Zhao, Lu Yin, Shiwei Liu, Meng Fang, Mykola Pechenizkiy","The deep neural network (DNN) has been proven effective in various domains.
However, they often struggle to perform well on certain minority groups during
inference, despite showing strong performance on the majority of data groups.
This is because over-parameterized models learned \textit{bias attributes} from
a large number of \textit{bias-aligned} training samples. These bias attributes
are strongly spuriously correlated with the target variable, causing the models
to be biased towards spurious correlations (i.e., \textit{bias-conflicting}).
To tackle this issue, we propose a novel \textbf{re}weighted \textbf{s}parse
\textbf{t}raining framework, dubbed as \textit{\textbf{REST}}, which aims to
enhance the performance of biased data while improving computation and memory
efficiency. Our proposed REST framework has been experimentally validated on
three datasets, demonstrating its effectiveness in exploring unbiased
subnetworks. We found that REST reduces the reliance on spuriously correlated
features, leading to better performance across a wider range of data groups
with fewer training and inference resources. We highlight that the
\textit{REST} framework represents a promising approach for improving the
performance of DNNs on biased data, while simultaneously improving computation
and memory efficiency. By reducing the reliance on spurious correlations, REST
has the potential to enhance the robustness of DNNs and improve their
generalization capabilities. Code is released at
\url{https://github.com/zhao1402072392/REST}",2312.03044v2,https://arxiv.org/pdf/2312.03044v2
"Provable Adversarial Robustness for Group Equivariant Tasks: Graphs,
  Point Clouds, Molecules, and More","Jan Schuchardt, Yan Scholten, Stephan Günnemann","A machine learning model is traditionally considered robust if its prediction
remains (almost) constant under input perturbations with small norm. However,
real-world tasks like molecular property prediction or point cloud segmentation
have inherent equivariances, such as rotation or permutation equivariance. In
such tasks, even perturbations with large norm do not necessarily change an
input's semantic content. Furthermore, there are perturbations for which a
model's prediction explicitly needs to change. For the first time, we propose a
sound notion of adversarial robustness that accounts for task equivariance. We
then demonstrate that provable robustness can be achieved by (1) choosing a
model that matches the task's equivariances (2) certifying traditional
adversarial robustness. Certification methods are, however, unavailable for
many models, such as those with continuous equivariances. We close this gap by
developing the framework of equivariance-preserving randomized smoothing, which
enables architecture-agnostic certification. We additionally derive the first
architecture-specific graph edit distance certificates, i.e. sound robustness
guarantees for isomorphism equivariant tasks like node classification. Overall,
a sound notion of robustness is an important prerequisite for future work at
the intersection of robust and geometric machine learning.",2312.02708v2,https://arxiv.org/pdf/2312.02708v2
Lights out: training RL agents robust to temporary blindness,"N. Ordonez, M. Tromp, P. M. Julbe, W. Böhmer","Agents trained with DQN rely on an observation at each timestep to decide
what action to take next. However, in real world applications observations can
change or be missing entirely. Examples of this could be a light bulb breaking
down, or the wallpaper in a certain room changing. While these situations
change the actual observation, the underlying optimal policy does not change.
Because of this we want our agent to continue taking actions until it receives
a (recognized) observation again. To achieve this we introduce a combination of
a neural network architecture that uses hidden representations of the
observations and a novel n-step loss function. Our implementation is able to
withstand location based blindness stretches longer than the ones it was
trained on, and therefore shows robustness to temporary blindness. For access
to our implementation, please email Nathan, Marije, or Pau.",2312.02665v1,https://arxiv.org/pdf/2312.02665v1
"On Optimal Consistency-Robustness Trade-Off for Learning-Augmented
  Multi-Option Ski Rental","Yongho Shin, Changyeol Lee, Hyung-Chan An","The learning-augmented multi-option ski rental problem generalizes the
classical ski rental problem in two ways: the algorithm is provided with a
prediction on the number of days we can ski, and the ski rental options now
come with a variety of rental periods and prices to choose from, unlike the
classical two-option setting. Subsequent to the initial study of the
multi-option ski rental problem (without learning augmentation) due to Zhang,
Poon, and Xu, significant progress has been made for this problem recently in
particular. The problem is very well understood when we relinquish one of the
two generalizations -- for the learning-augmented classical ski rental problem,
algorithms giving best-possible trade-off between consistency and robustness
exist; for the multi-option ski rental problem without learning augmentation,
deterministic/randomized algorithms giving the best-possible competitiveness
have been found. However, in presence of both generalizations, there remained a
huge gap between the algorithmic and impossibility results. In fact, for
randomized algorithms, we did not have any nontrivial lower bounds on the
consistency-robustness trade-off before.
  This paper bridges this gap for both deterministic and randomized algorithms.
For deterministic algorithms, we present a best-possible algorithm that
completely matches the known lower bound. For randomized algorithms, we show
the first nontrivial lower bound on the consistency-robustness trade-off, and
also present an improved randomized algorithm. Our algorithm matches our lower
bound on robustness within a factor of e/2 when the consistency is at most
1.086.",2312.02547v1,https://arxiv.org/pdf/2312.02547v1
Robust Clustering using Hyperdimensional Computing,"Lulu Ge, Keshab K. Parhi","This paper addresses the clustering of data in the hyperdimensional computing
(HDC) domain. In prior work, an HDC-based clustering framework, referred to as
HDCluster, has been proposed. However, the performance of the existing
HDCluster is not robust. The performance of HDCluster is degraded as the
hypervectors for the clusters are chosen at random during the initialization
step. To overcome this bottleneck, we assign the initial cluster hypervectors
by exploring the similarity of the encoded data, referred to as \textit{query}
hypervectors. Intra-cluster hypervectors have a higher similarity than
inter-cluster hypervectors. Harnessing the similarity results among query
hypervectors, this paper proposes four HDC-based clustering algorithms:
similarity-based k-means, equal bin-width histogram, equal bin-height
histogram, and similarity-based affinity propagation. Experimental results
illustrate that: (i) Compared to the existing HDCluster, our proposed HDC-based
clustering algorithms can achieve better accuracy, more robust performance,
fewer iterations, and less execution time. Similarity-based affinity
propagation outperforms the other three HDC-based clustering algorithms on
eight datasets by 2~38% in clustering accuracy. (ii) Even for one-pass
clustering, i.e., without any iterative update of the cluster hypervectors, our
proposed algorithms can provide more robust clustering accuracy than HDCluster.
(iii) Over eight datasets, five out of eight can achieve higher or comparable
accuracy when projected onto the hyperdimensional space. Traditional clustering
is more desirable than HDC when the number of clusters, $k$, is large.",2312.02407v1,https://arxiv.org/pdf/2312.02407v1
"SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust
  Attention","Isabel Leal, Krzysztof Choromanski, Deepali Jain, Avinava Dubey, Jake Varley, Michael Ryoo, Yao Lu, Frederick Liu, Vikas Sindhwani, Quan Vuong, Tamas Sarlos, Ken Oslund, Karol Hausman, Kanishka Rao","We present Self-Adaptive Robust Attention for Robotics Transformers
(SARA-RT): a new paradigm for addressing the emerging challenge of scaling up
Robotics Transformers (RT) for on-robot deployment. SARA-RT relies on the new
method of fine-tuning proposed by us, called up-training. It converts
pre-trained or already fine-tuned Transformer-based robotic policies of
quadratic time complexity (including massive billion-parameter
vision-language-action models or VLAs), into their efficient linear-attention
counterparts maintaining high quality. We demonstrate the effectiveness of
SARA-RT by speeding up: (a) the class of recently introduced RT-2 models, the
first VLA robotic policies pre-trained on internet-scale data, as well as (b)
Point Cloud Transformer (PCT) robotic policies operating on large point clouds.
We complement our results with the rigorous mathematical analysis providing
deeper insight into the phenomenon of SARA.",2312.01990v1,https://arxiv.org/pdf/2312.01990v1
"Few-Shot Anomaly Detection with Adversarial Loss for Robust Feature
  Representations","Jae Young Lee, Wonjun Lee, Jaehyun Choi, Yongkwi Lee, Young Seog Yoon","Anomaly detection is a critical and challenging task that aims to identify
data points deviating from normal patterns and distributions within a dataset.
Various methods have been proposed using a one-class-one-model approach, but
these techniques often face practical problems such as memory inefficiency and
the requirement of sufficient data for training. In particular, few-shot
anomaly detection presents significant challenges in industrial applications,
where limited samples are available before mass production. In this paper, we
propose a few-shot anomaly detection method that integrates adversarial
training loss to obtain more robust and generalized feature representations. We
utilize the adversarial loss previously employed in domain adaptation to align
feature distributions between source and target domains, to enhance feature
robustness and generalization in few-shot anomaly detection tasks. We
hypothesize that adversarial loss is effective when applied to features that
should have similar characteristics, such as those from the same layer in a
Siamese network's parallel branches or input-output pairs of
reconstruction-based methods. Experimental results demonstrate that the
proposed method generally achieves better performance when utilizing the
adversarial loss.",2312.03005v1,https://arxiv.org/pdf/2312.03005v1
"Robust Streaming, Sampling, and a Perspective on Online Learning","Evan Dogariu, Jiatong Yu","In this work we present an overview of statistical learning, followed by a
survey of robust streaming techniques and challenges, culminating in several
rigorous results proving the relationship that we motivate and hint at
throughout the journey. Furthermore, we unify often disjoint theorems in a
shared framework and notation to clarify the deep connections that are
discovered. We hope that by approaching these results from a shared
perspective, already aware of the technical connections that exist, we can
enlighten the study of both fields and perhaps motivate new and previously
unconsidered directions of research.",2312.01634v1,https://arxiv.org/pdf/2312.01634v1
"Exploring Adversarial Robustness of LiDAR-Camera Fusion Model in
  Autonomous Driving","Bo Yang, Xiaoyu Ji, Zizhi Jin, Yushi Cheng, Wenyuan Xu","Our study assesses the adversarial robustness of LiDAR-camera fusion models
in 3D object detection. We introduce an attack technique that, by simply adding
a limited number of physically constrained adversarial points above a car, can
make the car undetectable by the fusion model. Experimental results reveal that
even without changes to the image data channel, the fusion model can be
deceived solely by manipulating the LiDAR data channel. This finding raises
safety concerns in the field of autonomous driving. Further, we explore how the
quantity of adversarial points, the distance between the front-near car and the
LiDAR-equipped car, and various angular factors affect the attack success rate.
We believe our research can contribute to the understanding of multi-sensor
robustness, offering insights and guidance to enhance the safety of autonomous
driving.",2312.01468v2,https://arxiv.org/pdf/2312.01468v2
"Analyze the robustness of three NMF algorithms (Robust NMF with L1 norm,
  L2-1 norm NMF, L2 NMF)","Cheng Zeng, Jiaqi Tian, Yixuan Xu","Non-negative matrix factorization (NMF) and its variants have been widely
employed in clustering and classification tasks (Long, & Jian , 2021). However,
noises can seriously affect the results of our experiments. Our research is
dedicated to investigating the noise robustness of non-negative matrix
factorization (NMF) in the face of different types of noise. Specifically, we
adopt three different NMF algorithms, namely L1 NMF, L2 NMF, and L21 NMF, and
use the ORL and YaleB data sets to simulate a series of experiments with
salt-and-pepper noise and Block-occlusion noise separately. In the experiment,
we use a variety of evaluation indicators, including root mean square error
(RMSE), accuracy (ACC), and normalized mutual information (NMI), to evaluate
the performance of different NMF algorithms in noisy environments. Through
these indicators, we quantify the resistance of NMF algorithms to noise and
gain insights into their feasibility in practical applications.",2312.01357v1,https://arxiv.org/pdf/2312.01357v1
"Robust Non-parametric Knowledge-based Diffusion Least Mean Squares over
  Adaptive Networks","Soheil Ashkezari-Toussi, Hadi sadoghi-Yazdi","The present study proposes incorporating non-parametric knowledge into the
diffusion least-mean-squares algorithm in the framework of a maximum a
posteriori (MAP) estimation. The proposed algorithm leads to a robust
estimation of an unknown parameter vector in a group of cooperative estimators.
Utilizing kernel density estimation and buffering some intermediate
estimations, the prior distribution and conditional likelihood of the
parameters vector in each node are calculated. Pseudo Huber loss function is
used for designing the likelihood function. Also, an error thresholding
function is defined to reduce the computational overhead as well as more
relaxation against noise, which stops the update every time an error is less
than a predefined threshold. The performance of the proposed algorithm is
examined in the stationary and non-stationary scenarios in the presence of
Gaussian and non-Gaussian noise. Results show the robustness of the proposed
algorithm in the presence of different noise types.",2312.01299v1,https://arxiv.org/pdf/2312.01299v1
"Anomaly Detection Under Uncertainty Using Distributionally Robust
  Optimization Approach","Amir Hossein Noormohammadia, Seyed Ali MirHassania, Farnaz Hooshmand Khaligh","Anomaly detection is defined as the problem of finding data points that do
not follow the patterns of the majority. Among the various proposed methods for
solving this problem, classification-based methods, including one-class Support
Vector Machines (SVM) are considered effective and state-of-the-art. The
one-class SVM method aims to find a decision boundary to distinguish between
normal data points and anomalies using only the normal data. On the other hand,
most real-world problems involve some degree of uncertainty, where the true
probability distribution of each data point is unknown, and estimating it is
often difficult and costly. Assuming partial distribution information such as
the first and second-order moments is known, a distributionally robust
chance-constrained model is proposed in which the probability of
misclassification is low. By utilizing a mapping function to a higher
dimensional space, the proposed model will be capable of classifying
origin-inseparable datasets. Also, by adopting the kernel idea, the need for
explicitly knowing the mapping is eliminated, computations can be performed in
the input space, and computational complexity is reduced. Computational results
validate the robustness of the proposed model under different probability
distributions and also the superiority of the proposed model compared to the
standard one-class SVM in terms of various evaluation metrics.",2312.01296v1,https://arxiv.org/pdf/2312.01296v1
Fast and Robust Sparsity-Aware Block Diagonal Representation,"Aylin Tastan, Michael Muma, Abdelhak M. Zoubir","The block diagonal structure of an affinity matrix is a commonly desired
property in cluster analysis because it represents clusters of feature vectors
by non-zero coefficients that are concentrated in blocks. However, recovering a
block diagonal affinity matrix is challenging in real-world applications, in
which the data may be subject to outliers and heavy-tailed noise that obscure
the hidden cluster structure. To address this issue, we first analyze the
effect of different fundamental outlier types in graph-based cluster analysis.
A key idea that simplifies the analysis is to introduce a vector that
represents a block diagonal matrix as a piece-wise linear function of the
similarity coefficients that form the affinity matrix. We reformulate the
problem as a robust piece-wise linear fitting problem and propose a Fast and
Robust Sparsity-Aware Block Diagonal Representation (FRS-BDR) method, which
jointly estimates cluster memberships and the number of blocks. Comprehensive
experiments on a variety of real-world applications demonstrate the
effectiveness of FRS-BDR in terms of clustering accuracy, robustness against
corrupted features, computation time and cluster enumeration performance.",2312.01137v1,https://arxiv.org/pdf/2312.01137v1
Adaptive Robust Learning using Latent Bernoulli Variables,"Aleksandr Karakulev, Dave Zachariah, Prashant Singh","We present an adaptive approach for robust learning from corrupted training
sets. We identify corrupted and non-corrupted samples with latent Bernoulli
variables and thus formulate the learning problem as maximization of the
likelihood where latent variables are marginalized. The resulting problem is
solved via variational inference, using an efficient Expectation-Maximization
based method. The proposed approach improves over the state-of-the-art by
automatically inferring the corruption level, while adding minimal
computational overhead. We demonstrate our robust learning method and its
parameter-free nature on a wide variety of machine learning tasks including
online learning and deep learning where it adapts to different levels of noise
and maintains high prediction accuracy.",2312.00585v2,https://arxiv.org/pdf/2312.00585v2
REDUCR: Robust Data Downsampling Using Class Priority Reweighting,"William Bankes, George Hughes, Ilija Bogunovic, Zi Wang","Modern machine learning models are becoming increasingly expensive to train
for real-world image and text classification tasks, where massive web-scale
data is collected in a streaming fashion. To reduce the training cost, online
batch selection techniques have been developed to choose the most informative
datapoints. However, these techniques can suffer from poor worst-class
generalization performance due to class imbalance and distributional shifts.
This work introduces REDUCR, a robust and efficient data downsampling method
that uses class priority reweighting. REDUCR reduces the training data while
preserving worst-class generalization performance. REDUCR assigns priority
weights to datapoints in a class-aware manner using an online learning
algorithm. We demonstrate the data efficiency and robust performance of REDUCR
on vision and text classification tasks. On web-scraped datasets with
imbalanced class distributions, REDUCR significantly improves worst-class test
accuracy (and average accuracy), surpassing state-of-the-art methods by around
15%.",2312.00486v1,https://arxiv.org/pdf/2312.00486v1
"Exploring the Robustness of Decentralized Training for Large Language
  Models","Lin Lu, Chenxi Dai, Wangcheng Tao, Binhang Yuan, Yanan Sun, Pan Zhou","Decentralized training of large language models has emerged as an effective
way to democratize this technology. However, the potential threats associated
with this approach have not been carefully discussed, which would hinder the
development of decentralized training infrastructures. This paper aims to
initiate discussion towards this end by exploring the robustness of
decentralized training from three main perspectives. First, we demonstrate the
vulnerabilities inherent in decentralized training frameworks in terms of
hardware, data, and models. Second, we highlight the fundamental difference
between decentralized foundation model training and vanilla federated learning,
where the security techniques employed in federated learning cannot be applied
directly. Third, we discuss the essential components required for a robust and
efficient decentralized training framework and present a case study by modeling
a concrete threat model. Our objective in this vision paper is to emphasize the
importance of addressing security concerns in the context of decentralized
training for large language models.",2312.00843v1,https://arxiv.org/pdf/2312.00843v1
"DFU: scale-robust diffusion model for zero-shot super-resolution image
  generation","Alex Havrilla, Kevin Rojas, Wenjing Liao, Molei Tao","Diffusion generative models have achieved remarkable success in generating
images with a fixed resolution. However, existing models have limited ability
to generalize to different resolutions when training data at those resolutions
are not available. Leveraging techniques from operator learning, we present a
novel deep-learning architecture, Dual-FNO UNet (DFU), which approximates the
score operator by combining both spatial and spectral information at multiple
resolutions. Comparisons of DFU to baselines demonstrate its scalability: 1)
simultaneously training on multiple resolutions improves FID over training at
any single fixed resolution; 2) DFU generalizes beyond its training
resolutions, allowing for coherent, high-fidelity generation at
higher-resolutions with the same model, i.e. zero-shot super-resolution
image-generation; 3) we propose a fine-tuning strategy to further enhance the
zero-shot super-resolution image-generation capability of our model, leading to
a FID of 11.3 at 1.66 times the maximum training resolution on FFHQ, which no
other method can come close to achieving.",2401.06144v2,https://arxiv.org/pdf/2401.06144v2
Robust Concept Erasure via Kernelized Rate-Distortion Maximization,"Somnath Basu Roy Chowdhury, Nicholas Monath, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi","Distributed representations provide a vector space that captures meaningful
relationships between data instances. The distributed nature of these
representations, however, entangles together multiple attributes or concepts of
data instances (e.g., the topic or sentiment of a text, characteristics of the
author (age, gender, etc), etc). Recent work has proposed the task of concept
erasure, in which rather than making a concept predictable, the goal is to
remove an attribute from distributed representations while retaining other
information from the original representation space as much as possible. In this
paper, we propose a new distance metric learning-based objective, the
Kernelized Rate-Distortion Maximizer (KRaM), for performing concept erasure.
KRaM fits a transformation of representations to match a specified distance
measure (defined by a labeled concept to erase) using a modified
rate-distortion function. Specifically, KRaM's objective function aims to make
instances with similar concept labels dissimilar in the learned representation
space while retaining other information. We find that optimizing KRaM
effectively erases various types of concepts: categorical, continuous, and
vector-valued variables from data representations across diverse domains. We
also provide a theoretical analysis of several properties of KRaM's objective.
To assess the quality of the learned representations, we propose an alignment
score to evaluate their similarity with the original representation space.
Additionally, we conduct experiments to showcase KRaM's efficacy in various
settings, from erasing binary gender variables in word embeddings to
vector-valued variables in GPT-3 representations.",2312.00194v1,https://arxiv.org/pdf/2312.00194v1
"Improving the Robustness of Quantized Deep Neural Networks to White-Box
  Attacks using Stochastic Quantization and Information-Theoretic Ensemble
  Training","Saurabh Farkya, Aswin Raghavan, Avi Ziskind","Most real-world applications that employ deep neural networks (DNNs) quantize
them to low precision to reduce the compute needs. We present a method to
improve the robustness of quantized DNNs to white-box adversarial attacks. We
first tackle the limitation of deterministic quantization to fixed ``bins'' by
introducing a differentiable Stochastic Quantizer (SQ). We explore the
hypothesis that different quantizations may collectively be more robust than
each quantized DNN. We formulate a training objective to encourage different
quantized DNNs to learn different representations of the input image. The
training objective captures diversity and accuracy via mutual information
between ensemble members. Through experimentation, we demonstrate substantial
improvement in robustness against $L_\infty$ attacks even if the attacker is
allowed to backpropagate through SQ (e.g., > 50\% accuracy to PGD(5/255) on
CIFAR10 without adversarial training), compared to vanilla DNNs as well as
existing ensembles of quantized DNNs. We extend the method to detect attacks
and generate robustness profiles in the adversarial information plane (AIP),
towards a unified analysis of different threat models by correlating the MI and
accuracy.",2312.00105v1,https://arxiv.org/pdf/2312.00105v1
"Class Distribution Shifts in Zero-Shot Learning: Learning Robust
  Representations","Yuli Slavutsky, Yuval Benjamini","Zero-shot learning methods typically assume that the new, unseen classes that
are encountered at deployment, come from the same distribution as training
classes. However, real-world scenarios often involve class distribution shifts
(e.g., in age or gender for person identification), posing challenges for
zero-shot classifiers that rely on learned representations from training
classes. In this work, we propose a model that assumes that the attribute
responsible for the shift is unknown in advance, and show that standard
training may lead to non-robust representations. To mitigate this, we propose
an algorithm for learning robust representations by (a) constructing synthetic
data environments via hierarchical sampling and (b) applying environment
balancing penalization, inspired by out-of-distribution problems. We show that
our approach improves generalization on diverse class distributions in both
simulations and real-world datasets.",2311.18575v3,https://arxiv.org/pdf/2311.18575v3
"Multi-scale Iterative Refinement towards Robust and Versatile Molecular
  Docking","Jiaxian Yan, Zaixi Zhang, Kai Zhang, Qi Liu","Molecular docking is a key computational tool utilized to predict the binding
conformations of small molecules to protein targets, which is fundamental in
the design of novel drugs. Despite recent advancements in geometric deep
learning-based approaches leading to improvements in blind docking efficiency,
these methods have encountered notable challenges, such as limited
generalization performance on unseen proteins, the inability to concurrently
address the settings of blind docking and site-specific docking, and the
frequent occurrence of physical implausibilities such as inter-molecular steric
clash. In this study, we introduce DeltaDock, a robust and versatile framework
designed for efficient molecular docking to overcome these challenges.
DeltaDock operates in a two-step process: rapid initial complex structures
sampling followed by multi-scale iterative refinement of the initial
structures. In the initial stage, to sample accurate structures with high
efficiency, we develop a ligand-dependent binding site prediction model founded
on large protein models and graph neural networks. This model is then paired
with GPU-accelerated sampling algorithms. The sampled structures are updated
using a multi-scale iterative refinement module that captures both
protein-ligand atom-atom interactions and residue-atom interactions in the
following stage. Distinct from previous geometric deep learning methods that
are conditioned on the blind docking setting, DeltaDock demonstrates superior
performance in both blind docking and site-specific docking settings.
Comprehensive experimental results reveal that DeltaDock consistently surpasses
baseline methods in terms of docking accuracy. Furthermore, it displays
remarkable generalization capabilities and proficiency for predicting
physically valid structures, thereby attesting to its robustness and
reliability in various scenarios.",2311.18574v1,https://arxiv.org/pdf/2311.18574v1
Learning Robust Precipitation Forecaster by Temporal Frame Interpolation,"Lu Han, Xu-Yang Chen, Han-Jia Ye, De-Chuan Zhan","Recent advances in deep learning have significantly elevated weather
prediction models. However, these models often falter in real-world scenarios
due to their sensitivity to spatial-temporal shifts. This issue is particularly
acute in weather forecasting, where models are prone to overfit to local and
temporal variations, especially when tasked with fine-grained predictions. In
this paper, we address these challenges by developing a robust precipitation
forecasting model that demonstrates resilience against such spatial-temporal
discrepancies. We introduce Temporal Frame Interpolation (TFI), a novel
technique that enhances the training dataset by generating synthetic samples
through interpolating adjacent frames from satellite imagery and ground radar
data, thus improving the model's robustness against frame noise. Moreover, we
incorporate a unique Multi-Level Dice (ML-Dice) loss function, leveraging the
ordinal nature of rainfall intensities to improve the model's performance. Our
approach has led to significant improvements in forecasting precision,
culminating in our model securing \textit{1st place} in the transfer learning
leaderboard of the \textit{Weather4cast'23} competition. This achievement not
only underscores the effectiveness of our methodologies but also establishes a
new standard for deep learning applications in weather forecasting. Our code
and weights have been public on \url{https://github.com/Secilia-Cxy/UNetTFI}.",2311.18341v2,https://arxiv.org/pdf/2311.18341v2
"Towards out-of-distribution generalization in large-scale astronomical
  surveys: robust networks learn similar representations","Yash Gondhalekar, Sultan Hassan, Naomi Saphra, Sambatra Andrianomena","The generalization of machine learning (ML) models to out-of-distribution
(OOD) examples remains a key challenge in extracting information from upcoming
astronomical surveys. Interpretability approaches are a natural way to gain
insights into the OOD generalization problem. We use Centered Kernel Alignment
(CKA), a similarity measure metric of neural network representations, to
examine the relationship between representation similarity and performance of
pre-trained Convolutional Neural Networks (CNNs) on the CAMELS Multifield
Dataset. We find that when models are robust to a distribution shift, they
produce substantially different representations across their layers on OOD
data. However, when they fail to generalize, these representations change less
from layer to layer on OOD data. We discuss the potential application of
similarity representation in guiding model design, training strategy, and
mitigating the OOD problem by incorporating CKA as an inductive bias during
training.",2311.18007v1,https://arxiv.org/pdf/2311.18007v1
On the Adversarial Robustness of Graph Contrastive Learning Methods,"Filippo Guerranti, Zinuo Yi, Anna Starovoit, Rafiq Kamel, Simon Geisler, Stephan Günnemann","Contrastive learning (CL) has emerged as a powerful framework for learning
representations of images and text in a self-supervised manner while enhancing
model robustness against adversarial attacks. More recently, researchers have
extended the principles of contrastive learning to graph-structured data,
giving birth to the field of graph contrastive learning (GCL). However, whether
GCL methods can deliver the same advantages in adversarial robustness as their
counterparts in the image and text domains remains an open question. In this
paper, we introduce a comprehensive robustness evaluation protocol tailored to
assess the robustness of GCL models. We subject these models to adaptive
adversarial attacks targeting the graph structure, specifically in the evasion
scenario. We evaluate node and graph classification tasks using diverse
real-world datasets and attack strategies. With our work, we aim to offer
insights into the robustness of GCL methods and hope to open avenues for
potential future research directions.",2311.17853v2,https://arxiv.org/pdf/2311.17853v2
"Robustness Approaches for the Examination Timetabling Problem under Data
  Uncertainty","Bernd Bassimir, Rolf Wanka","In the literature the examination timetabling problem (ETTP) is often
considered a post-enrollment problem (PE-ETTP). In the real world, universities
often schedule their exams before students register using information from
previous terms. A direct consequence of this approach is the uncertainty
present in the resulting models. In this work we discuss several approaches
available in the robust optimization literature. We consider the implications
of each approach in respect to the examination timetabling problem and present
how the most favorable approaches can be applied to the ETTP. Afterwards we
analyze the impact of some possible implementations of the given robustness
approaches on two real world instances and several random instances generated
by our instance generation framework which we introduce in this work.",2311.17766v1,https://arxiv.org/pdf/2311.17766v1
SenTest: Evaluating Robustness of Sentence Encoders,"Tanmay Chavan, Shantanu Patankar, Aditya Kane, Omkar Gokhale, Geetanjali Kale, Raviraj Joshi","Contrastive learning has proven to be an effective method for pre-training
models using weakly labeled data in the vision domain. Sentence transformers
are the NLP counterparts to this architecture, and have been growing in
popularity due to their rich and effective sentence representations. Having
effective sentence representations is paramount in multiple tasks, such as
information retrieval, retrieval augmented generation (RAG), and sentence
comparison. Keeping in mind the deployability factor of transformers,
evaluating the robustness of sentence transformers is of utmost importance.
This work focuses on evaluating the robustness of the sentence encoders. We
employ several adversarial attacks to evaluate its robustness. This system uses
character-level attacks in the form of random character substitution,
word-level attacks in the form of synonym replacement, and sentence-level
attacks in the form of intra-sentence word order shuffling. The results of the
experiments strongly undermine the robustness of sentence encoders. The models
produce significantly different predictions as well as embeddings on perturbed
datasets. The accuracy of the models can fall up to 15 percent on perturbed
datasets as compared to unperturbed datasets. Furthermore, the experiments
demonstrate that these embeddings does capture the semantic and syntactic
structure (sentence order) of sentences. However, existing supervised
classification strategies fail to leverage this information, and merely
function as n-gram detectors.",2311.17722v1,https://arxiv.org/pdf/2311.17722v1
Adversarial Robust Memory-Based Continual Learner,"Xiaoyue Mi, Fan Tang, Zonghan Yang, Danding Wang, Juan Cao, Peng Li, Yang Liu","Despite the remarkable advances that have been made in continual learning,
the adversarial vulnerability of such methods has not been fully discussed. We
delve into the adversarial robustness of memory-based continual learning
algorithms and observe limited robustness improvement by directly applying
adversarial training techniques. Preliminary studies reveal the twin challenges
for building adversarial robust continual learners: accelerated forgetting in
continual learning and gradient obfuscation in adversarial robustness. In this
study, we put forward a novel adversarial robust memory-based continual learner
that adjusts data logits to mitigate the forgetting of pasts caused by
adversarial samples. Furthermore, we devise a gradient-based data selection
mechanism to overcome the gradient obfuscation caused by limited stored data.
The proposed approach can widely integrate with existing memory-based continual
learning as well as adversarial training algorithms in a plug-and-play way.
Extensive experiments on Split-CIFAR10/100 and Split-Tiny-ImageNet demonstrate
the effectiveness of our approach, achieving up to 8.13% higher accuracy for
adversarial data.",2311.17608v1,https://arxiv.org/pdf/2311.17608v1
"Improving the Robustness of Transformer-based Large Language Models with
  Dynamic Attention","Lujia Shen, Yuwen Pu, Shouling Ji, Changjiang Li, Xuhong Zhang, Chunpeng Ge, Ting Wang","Transformer-based models, such as BERT and GPT, have been widely adopted in
natural language processing (NLP) due to their exceptional performance.
However, recent studies show their vulnerability to textual adversarial attacks
where the model's output can be misled by intentionally manipulating the text
inputs. Despite various methods that have been proposed to enhance the model's
robustness and mitigate this vulnerability, many require heavy consumption
resources (e.g., adversarial training) or only provide limited protection
(e.g., defensive dropout). In this paper, we propose a novel method called
dynamic attention, tailored for the transformer architecture, to enhance the
inherent robustness of the model itself against various adversarial attacks.
Our method requires no downstream task knowledge and does not incur additional
costs. The proposed dynamic attention consists of two modules: (I) attention
rectification, which masks or weakens the attention value of the chosen tokens,
and (ii) dynamic modeling, which dynamically builds the set of candidate
tokens. Extensive experiments demonstrate that dynamic attention significantly
mitigates the impact of adversarial attacks, improving up to 33\% better
performance than previous methods against widely-used adversarial attacks. The
model-level design of dynamic attention enables it to be easily combined with
other defense methods (e.g., adversarial training) to further enhance the
model's robustness. Furthermore, we demonstrate that dynamic attention
preserves the state-of-the-art robustness space of the original model compared
to other dynamic modeling methods.",2311.17400v2,https://arxiv.org/pdf/2311.17400v2
"Elo Uncovered: Robustness and Best Practices in Language Model
  Evaluation","Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, Marzieh Fadaee","In Natural Language Processing (NLP), the Elo rating system, originally
designed for ranking players in dynamic games such as chess, is increasingly
being used to evaluate Large Language Models (LLMs) through ""A vs B"" paired
comparisons. However, while popular, the system's suitability for assessing
entities with constant skill levels, such as LLMs, remains relatively
unexplored. We study two fundamental axioms that evaluation methods should
adhere to: reliability and transitivity. We conduct extensive evaluation of Elo
behaviour, illustrating that individual Elo computations exhibit volatility and
delving into the impact of varying the Elo rating system's hyperparameters. We
show that these axioms are not always satisfied raising questions about the
reliability of current comparative evaluations of LLMs. If the current use of
Elo scores is intended to substitute the costly head-to-head comparison of
LLMs, it is crucial to ensure the ranking is as robust as possible. Guided by
the axioms, our findings offer concrete guidelines for enhancing the
reliability of LLM evaluation methods, suggesting a need for reassessment of
existing comparative approaches.",2311.17295v1,https://arxiv.org/pdf/2311.17295v1
"STR-Cert: Robustness Certification for Deep Text Recognition on Deep
  Learning Pipelines and Vision Transformers","Daqian Shao, Lukas Fesser, Marta Kwiatkowska","Robustness certification, which aims to formally certify the predictions of
neural networks against adversarial inputs, has become an integral part of
important tool for safety-critical applications. Despite considerable progress,
existing certification methods are limited to elementary architectures, such as
convolutional networks, recurrent networks and recently Transformers, on
benchmark datasets such as MNIST. In this paper, we focus on the robustness
certification of scene text recognition (STR), which is a complex and
extensively deployed image-based sequence prediction problem. We tackle three
types of STR model architectures, including the standard STR pipelines and the
Vision Transformer. We propose STR-Cert, the first certification method for STR
models, by significantly extending the DeepPoly polyhedral verification
framework via deriving novel polyhedral bounds and algorithms for key STR model
components. Finally, we certify and compare STR models on six datasets,
demonstrating the efficiency and scalability of robustness certification,
particularly for the Vision Transformer.",2401.05338v1,https://arxiv.org/pdf/2401.05338v1
"Deployment of a Robust and Explainable Mortality Prediction Model: The
  COVID-19 Pandemic and Beyond","Jacob R. Epifano, Stephen Glass, Ravi P. Ramachandran, Sharad Patel, Aaron J. Masino, Ghulam Rasool","This study investigated the performance, explainability, and robustness of
deployed artificial intelligence (AI) models in predicting mortality during the
COVID-19 pandemic and beyond. The first study of its kind, we found that
Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our
models to maintain performance amidst significant data shifts. Our results
emphasize the importance of developing robust AI models capable of matching or
surpassing clinician predictions, even under challenging conditions. Our
exploration of model explainability revealed that stochastic models generate
more diverse and personalized explanations thereby highlighting the need for AI
models that provide detailed and individualized insights in real-world clinical
settings. Furthermore, we underscored the importance of quantifying uncertainty
in AI models which enables clinicians to make better-informed decisions based
on reliable predictions. Our study advocates for prioritizing implementation
science in AI research for healthcare and ensuring that AI solutions are
practical, beneficial, and sustainable in real-world clinical environments. By
addressing unique challenges and complexities in healthcare settings,
researchers can develop AI models that effectively improve clinical practice
and patient outcomes.",2311.17133v1,https://arxiv.org/pdf/2311.17133v1
TransNeXt: Robust Foveal Visual Perception for Vision Transformers,Dai Shi,"Due to the depth degradation effect in residual connections, many efficient
Vision Transformers models that rely on stacking layers for information
exchange often fail to form sufficient information mixing, leading to unnatural
visual perception. To address this issue, in this paper, we propose Aggregated
Attention, a biomimetic design-based token mixer that simulates biological
foveal vision and continuous eye movement while enabling each token on the
feature map to have a global perception. Furthermore, we incorporate learnable
tokens that interact with conventional queries and keys, which further
diversifies the generation of affinity matrices beyond merely relying on the
similarity between queries and keys. Our approach does not rely on stacking for
information exchange, thus effectively avoiding depth degradation and achieving
natural visual perception. Additionally, we propose Convolutional GLU, a
channel mixer that bridges the gap between GLU and SE mechanism, which empowers
each token to have channel attention based on its nearest neighbor image
features, enhancing local modeling capability and model robustness. We combine
aggregated attention and convolutional GLU to create a new visual backbone
called TransNeXt. Extensive experiments demonstrate that our TransNeXt achieves
state-of-the-art performance across multiple model sizes. At a resolution of
$224^2$, TransNeXt-Tiny attains an ImageNet accuracy of 84.0%, surpassing
ConvNeXt-B with 69% fewer parameters. Our TransNeXt-Base achieves an ImageNet
accuracy of 86.2% and an ImageNet-A accuracy of 61.6% at a resolution of
$384^2$, a COCO object detection mAP of 57.1, and an ADE20K semantic
segmentation mIoU of 54.7.",2311.17132v3,https://arxiv.org/pdf/2311.17132v3
"Attentional Graph Neural Networks for Robust Massive Network
  Localization","Wenzhong Yan, Juntao Wang, Feng Yin, Yang Tian, Abdelhak M. Zoubir","In recent years, Graph neural networks (GNNs) have emerged as a prominent
tool for classification tasks in machine learning. However, their application
in regression tasks remains underexplored. To tap the potential of GNNs in
regression, this paper integrates GNNs with attention mechanism, a technique
that revolutionized sequential learning tasks with its adaptability and
robustness, to tackle a challenging nonlinear regression problem: network
localization. We first introduce a novel network localization method based on
graph convolutional network (GCN), which exhibits exceptional precision even
under severe non-line-of-sight (NLOS) conditions, thereby diminishing the need
for laborious offline calibration or NLOS identification. We further propose an
attentional graph neural network (AGNN) model, aimed at improving the limited
flexibility and mitigating the high sensitivity to the hyperparameter of the
GCN-based method. The AGNN comprises two crucial modules, each designed with
distinct attention architectures to address specific issues associated with the
GCN-based method, rendering it more practical in real-world scenarios.
Experimental results substantiate the efficacy of our proposed GCN-based method
and AGNN model, as well as the enhancements of AGNN model. Additionally, we
delve into the performance improvements of AGNN model by analyzing it from the
perspectives of dynamic attention and computational complexity.",2311.16856v2,https://arxiv.org/pdf/2311.16856v2
"1-Lipschitz Layers Compared: Memory, Speed, and Certifiable Robustness","Bernd Prach, Fabio Brau, Giorgio Buttazzo, Christoph H. Lampert","The robustness of neural networks against input perturbations with bounded
magnitude represents a serious concern in the deployment of deep learning
models in safety-critical systems. Recently, the scientific community has
focused on enhancing certifiable robustness guarantees by crafting 1-Lipschitz
neural networks that leverage Lipschitz bounded dense and convolutional layers.
Although different methods have been proposed in the literature to achieve this
goal, understanding the performance of such methods is not straightforward,
since different metrics can be relevant (e.g., training time, memory usage,
accuracy, certifiable robustness) for different applications. For this reason,
this work provides a thorough theoretical and empirical comparison between
methods by evaluating them in terms of memory usage, speed, and certifiable
robust accuracy. The paper also provides some guidelines and recommendations to
support the user in selecting the methods that work best depending on the
available resources. We provide code at
https://github.com/berndprach/1LipschitzLayersCompared.",2311.16833v1,https://arxiv.org/pdf/2311.16833v1
On robust overfitting: adversarial training induced distribution matters,"Runzhi Tian, Yongyi Mao","Adversarial training may be regarded as standard training with a modified
loss function. But its generalization error appears much larger than standard
training under standard loss. This phenomenon, known as robust overfitting, has
attracted significant research attention and remains largely as a mystery. In
this paper, we first show empirically that robust overfitting correlates with
the increasing generalization difficulty of the perturbation-induced
distributions along the trajectory of adversarial training (specifically
PGD-based adversarial training). We then provide a novel upper bound for
generalization error with respect to the perturbation-induced distributions, in
which a notion of the perturbation operator, referred to ""local dispersion"",
plays an important role. Experimental results are presented to validate the
usefulness of the bound and various additional insights are provided.",2311.16526v2,https://arxiv.org/pdf/2311.16526v2
On the Robustness of Decision-Focused Learning,Yehya Farhat,"Decision-Focused Learning (DFL) is an emerging learning paradigm that tackles
the task of training a machine learning (ML) model to predict missing
parameters of an incomplete optimization problem, where the missing parameters
are predicted. DFL trains an ML model in an end-to-end system, by integrating
the prediction and optimization tasks, providing better alignment of the
training and testing objectives. DFL has shown a lot of promise and holds the
capacity to revolutionize decision-making in many real-world applications.
However, very little is known about the performance of these models under
adversarial attacks. We adopt ten unique DFL methods and benchmark their
performance under two distinctly focused attacks adapted towards the
Predict-then-Optimize problem setting. Our study proposes the hypothesis that
the robustness of a model is highly correlated with its ability to find
predictions that lead to optimal decisions without deviating from the
ground-truth label. Furthermore, we provide insight into how to target the
models that violate this condition and show how these models respond
differently depending on the achieved optimality at the end of their training
cycles.",2311.16487v3,https://arxiv.org/pdf/2311.16487v3
A Combinatorial Approach to Robust PCA,"Weihao Kong, Mingda Qiao, Rajat Sen","We study the problem of recovering Gaussian data under adversarial
corruptions when the noises are low-rank and the corruptions are on the
coordinate level. Concretely, we assume that the Gaussian noises lie in an
unknown $k$-dimensional subspace $U \subseteq \mathbb{R}^d$, and $s$ randomly
chosen coordinates of each data point fall into the control of an adversary.
This setting models the scenario of learning from high-dimensional yet
structured data that are transmitted through a highly-noisy channel, so that
the data points are unlikely to be entirely clean.
  Our main result is an efficient algorithm that, when $ks^2 = O(d)$, recovers
every single data point up to a nearly-optimal $\ell_1$ error of $\tilde
O(ks/d)$ in expectation. At the core of our proof is a new analysis of the
well-known Basis Pursuit (BP) method for recovering a sparse signal, which is
known to succeed under additional assumptions (e.g., incoherence or the
restricted isometry property) on the underlying subspace $U$. In contrast, we
present a novel approach via studying a natural combinatorial problem and show
that, over the randomness in the support of the sparse signal, a
high-probability error bound is possible even if the subspace $U$ is arbitrary.",2311.16416v1,https://arxiv.org/pdf/2311.16416v1
"Making Self-supervised Learning Robust to Spurious Correlation via
  Learning-speed Aware Sampling","Weicheng Zhu, Sheng Liu, Carlos Fernandez-Granda, Narges Razavian","Self-supervised learning (SSL) has emerged as a powerful technique for
learning rich representations from unlabeled data. The data representations are
able to capture many underlying attributes of data, and be useful in downstream
prediction tasks. In real-world settings, spurious correlations between some
attributes (e.g. race, gender and age) and labels for downstream tasks often
exist, e.g. cancer is usually more prevalent among elderly patients. In this
paper, we investigate SSL in the presence of spurious correlations and show
that the SSL training loss can be minimized by capturing only a subset of the
conspicuous features relevant to those sensitive attributes, despite the
presence of other important predictive features for the downstream tasks. To
address this issue, we investigate the learning dynamics of SSL and observe
that the learning is slower for samples that conflict with such correlations
(e.g. elder patients without cancer). Motivated by these findings, we propose a
learning-speed aware SSL (LA-SSL) approach, in which we sample each training
data with a probability that is inversely related to its learning speed. We
evaluate LA-SSL on three datasets that exhibit spurious correlations between
different attributes, demonstrating that it improves the robustness of
pretrained representations on downstream classification tasks.",2311.16361v2,https://arxiv.org/pdf/2311.16361v2
"RobustState: Boosting Fidelity of Quantum State Preparation via
  Noise-Aware Variational Training","Hanrui Wang, Yilian Liu, Pengyu Liu, Jiaqi Gu, Zirui Li, Zhiding Liang, Jinglei Cheng, Yongshan Ding, Xuehai Qian, Yiyu Shi, David Z. Pan, Frederic T. Chong, Song Han","Quantum state preparation, a crucial subroutine in quantum computing,
involves generating a target quantum state from initialized qubits. Arbitrary
state preparation algorithms can be broadly categorized into arithmetic
decomposition (AD) and variational quantum state preparation (VQSP). AD employs
a predefined procedure to decompose the target state into a series of gates,
whereas VQSP iteratively tunes ansatz parameters to approximate target state.
VQSP is particularly apt for Noisy-Intermediate Scale Quantum (NISQ) machines
due to its shorter circuits. However, achieving noise-robust parameter
optimization still remains challenging.
  We present RobustState, a novel VQSP training methodology that combines high
robustness with high training efficiency. The core idea involves utilizing
measurement outcomes from real machines to perform back-propagation through
classical simulators, thus incorporating real quantum noise into gradient
calculations. RobustState serves as a versatile, plug-and-play technique
applicable for training parameters from scratch or fine-tuning existing
parameters to enhance fidelity on target machines. It is adaptable to various
ansatzes at both gate and pulse levels and can even benefit other variational
algorithms, such as variational unitary synthesis.
  Comprehensive evaluation of RobustState on state preparation tasks for 4
distinct quantum algorithms using 10 real quantum machines demonstrates a
coherent error reduction of up to 7.1 $\times$ and state fidelity improvement
of up to 96\% and 81\% for 4-Q and 5-Q states, respectively. On average,
RobustState improves fidelity by 50\% and 72\% for 4-Q and 5-Q states compared
to baseline approaches.",2311.16035v1,https://arxiv.org/pdf/2311.16035v1
"Relationship between Model Compression and Adversarial Robustness: A
  Review of Current Evidence","Svetlana Pavlitska, Hannes Grolig, J. Marius Zöllner","Increasing the model capacity is a known approach to enhance the adversarial
robustness of deep learning networks. On the other hand, various model
compression techniques, including pruning and quantization, can reduce the size
of the network while preserving its accuracy. Several recent studies have
addressed the relationship between model compression and adversarial
robustness, while some experiments have reported contradictory results. This
work summarizes available evidence and discusses possible explanations for the
observed effects.",2311.15782v1,https://arxiv.org/pdf/2311.15782v1
"A Comparative and Experimental Study on Automatic Question Answering
  Systems and its Robustness against Word Jumbling","Shashidhar Reddy Javaji, Haoran Hu, Sai Sameer Vennam, Vijaya Gajanan Buddhavarapu","Question answer generation using Natural Language Processing models is
ubiquitous in the world around us. It is used in many use cases such as the
building of chat bots, suggestive prompts in google search and also as a way of
navigating information in banking mobile applications etc. It is highly
relevant because a frequently asked questions (FAQ) list can only have a finite
amount of questions but a model which can perform question answer generation
could be able to answer completely new questions that are within the scope of
the data. This helps us to be able to answer new questions accurately as long
as it is a relevant question. In commercial applications, it can be used to
increase customer satisfaction and ease of usage. However a lot of data is
generated by humans so it is susceptible to human error and this can adversely
affect the model's performance and we are investigating this through our work",2311.15513v1,https://arxiv.org/pdf/2311.15513v1
"Robust and Automatic Data Clustering: Dirichlet Process meets
  Median-of-Means","Supratik Basu, Jyotishka Ray Choudhury, Debolina Paul, Swagatam Das","Clustering stands as one of the most prominent challenges within the realm of
unsupervised machine learning. Among the array of centroid-based clustering
algorithms, the classic $k$-means algorithm, rooted in Lloyd's heuristic, takes
center stage as one of the extensively employed techniques in the literature.
Nonetheless, both $k$-means and its variants grapple with noteworthy
limitations. These encompass a heavy reliance on initial cluster centroids,
susceptibility to converging into local minima of the objective function, and
sensitivity to outliers and noise in the data. When confronted with data
containing noisy or outlier-laden observations, the Median-of-Means (MoM)
estimator emerges as a stabilizing force for any centroid-based clustering
framework. On a different note, a prevalent constraint among existing
clustering methodologies resides in the prerequisite knowledge of the number of
clusters prior to analysis. Utilizing model-based methodologies, such as
Bayesian nonparametric models, offers the advantage of infinite mixture models,
thereby circumventing the need for such requirements. Motivated by these facts,
in this article, we present an efficient and automatic clustering technique by
integrating the principles of model-based and centroid-based methodologies that
mitigates the effect of noise on the quality of clustering while ensuring that
the number of clusters need not be specified in advance. Statistical guarantees
on the upper bound of clustering error, and rigorous assessment through
simulated and real datasets suggest the advantages of our proposed method over
existing state-of-the-art clustering algorithms.",2311.15384v1,https://arxiv.org/pdf/2311.15384v1
"Exploring the Robustness of Model-Graded Evaluations and Automated
  Interpretability","Simon Lermen, Ondřej Kvapil","There has been increasing interest in evaluations of language models for a
variety of risks and characteristics. Evaluations relying on natural language
understanding for grading can often be performed at scale by using other
language models. We test the robustness of these model-graded evaluations to
injections on different datasets including a new Deception Eval. These
injections resemble direct communication between the testee and the evaluator
to change their grading. We extrapolate that future, more intelligent models
might manipulate or cooperate with their evaluation model. We find significant
susceptibility to these injections in state-of-the-art commercial models on all
examined evaluations. Furthermore, similar injections can be used on automated
interpretability frameworks to produce misleading model-written explanations.
The results inspire future work and should caution against unqualified trust in
evaluations and automated interpretability.",2312.03721v2,https://arxiv.org/pdf/2312.03721v2
Mixing Classifiers to Alleviate the Accuracy-Robustness Trade-Off,"Yatong Bai, Brendon G. Anderson, Somayeh Sojoudi","Deep neural classifiers have recently found tremendous success in data-driven
control systems. However, existing models suffer from a trade-off between
accuracy and adversarial robustness. This limitation must be overcome in the
control of safety-critical systems that require both high performance and
rigorous robustness guarantees. In this work, we develop classifiers that
simultaneously inherit high robustness from robust models and high accuracy
from standard models. Specifically, we propose a theoretically motivated
formulation that mixes the output probabilities of a standard neural network
and a robust neural network. Both base classifiers are pre-trained, and thus
our method does not require additional training. Our numerical experiments
verify that the mixed classifier noticeably improves the accuracy-robustness
trade-off and identify the confidence property of the robust base classifier as
the key leverage of this more benign trade-off. Our theoretical results prove
that under mild assumptions, when the robustness of the robust base model is
certifiable, no alteration or attack within a closed-form $\ell_p$ radius on an
input can result in the misclassification of the mixed classifier.",2311.15165v2,https://arxiv.org/pdf/2311.15165v2
Hessian Aware Low-Rank Perturbation for Order-Robust Continual Learning,"Jiaqi Li, Yuanhao Lai, Rui Wang, Changjian Shui, Sabyasachi Sahoo, Charles X. Ling, Shichun Yang, Boyu Wang, Christian Gagné, Fan Zhou","Continual learning aims to learn a series of tasks sequentially without
forgetting the knowledge acquired from the previous ones. In this work, we
propose the Hessian Aware Low-Rank Perturbation algorithm for continual
learning. By modeling the parameter transitions along the sequential tasks with
the weight matrix transformation, we propose to apply the low-rank
approximation on the task-adaptive parameters in each layer of the neural
networks. Specifically, we theoretically demonstrate the quantitative
relationship between the Hessian and the proposed low-rank approximation. The
approximation ranks are then globally determined according to the marginal
increment of the empirical loss estimated by the layer-specific gradient and
low-rank approximation error. Furthermore, we control the model capacity by
pruning less important parameters to diminish the parameter growth. We conduct
extensive experiments on various benchmarks, including a dataset with
large-scale tasks, and compare our method against some recent state-of-the-art
methods to demonstrate the effectiveness and scalability of our proposed
method. Empirical results show that our method performs better on different
benchmarks, especially in achieving task order robustness and handling the
forgetting issue. The source code is at https://github.com/lijiaqi/HALRP.",2311.15161v4,https://arxiv.org/pdf/2311.15161v4
"Where2Start: Leveraging initial States for Robust and Sample-Efficient
  Reinforcement Learning","Pouya Parsa, Raoof Zare Moayedi, Mohammad Bornosi, Mohammad Mahdi Bejani","The reinforcement learning algorithms that focus on how to compute the
gradient and choose next actions, are effectively improved the performance of
the agents. However, these algorithms are environment-agnostic. This means that
the algorithms did not use the knowledge that has been captured by trajectory.
This poses that the algorithms should sample many trajectories to train the
model. By considering the essence of environment and how much the agent learn
from each scenario in that environment, the strategy of the learning procedure
can be changed. The strategy retrieves more informative trajectories, so the
agent can learn with fewer trajectory sample. We propose Where2Start algorithm
that selects the initial state so that the agent has more instability in
vicinity of that state. We show that this kind of selection decreases number of
trajectories that should be sampled that the agent reach to acceptable reward.
Our experiments shows that Where2Start can improve sample efficiency up to 8
times. Also Where2Start can combined with most of state-of-the-art algorithms
and improve that robustness and sample efficiency significantly.",2311.15089v1,https://arxiv.org/pdf/2311.15089v1
Robust Graph Neural Networks via Unbiased Aggregation,"Ruiqi Feng, Zhichao Hou, Tyler Derr, Xiaorui Liu","The adversarial robustness of Graph Neural Networks (GNNs) has been
questioned due to the false sense of security uncovered by strong adaptive
attacks despite the existence of numerous defenses. In this work, we delve into
the robustness analysis of representative robust GNNs and provide a unified
robust estimation point of view to understand their robustness and limitations.
Our novel analysis of estimation bias motivates the design of a robust and
unbiased graph signal estimator. We then develop an efficient Quasi-Newton
iterative reweighted least squares algorithm to solve the estimation problem,
which unfolds as robust unbiased aggregation layers in GNNs with a theoretical
convergence guarantee. Our comprehensive experiments confirm the strong
robustness of our proposed model, and the ablation study provides a deep
understanding of its advantages.",2311.14934v1,https://arxiv.org/pdf/2311.14934v1
"Inferring Latent Class Statistics from Text for Robust Visual Few-Shot
  Learning","Yassir Bendou, Vincent Gripon, Bastien Pasdeloup, Giulia Lioi, Lukas Mauch, Fabien Cardinaux, Ghouthi Boukli Hacene","In the realm of few-shot learning, foundation models like CLIP have proven
effective but exhibit limitations in cross-domain robustness especially in
few-shot settings. Recent works add text as an extra modality to enhance the
performance of these models. Most of these approaches treat text as an
auxiliary modality without fully exploring its potential to elucidate the
underlying class visual features distribution. In this paper, we present a
novel approach that leverages text-derived statistics to predict the mean and
covariance of the visual feature distribution for each class. This predictive
framework enriches the latent space, yielding more robust and generalizable
few-shot learning models. We demonstrate the efficacy of incorporating both
mean and covariance statistics in improving few-shot classification performance
across various datasets. Our method shows that we can use text to predict the
mean and covariance of the distribution offering promising improvements in
few-shot learning scenarios.",2311.14544v1,https://arxiv.org/pdf/2311.14544v1
Robust Domain Misinformation Detection via Multi-modal Feature Alignment,"Hui Liu, Wenya Wang, Hao Sun, Anderson Rocha, Haoliang Li","Social media misinformation harms individuals and societies and is
potentialized by fast-growing multi-modal content (i.e., texts and images),
which accounts for higher ""credibility"" than text-only news pieces. Although
existing supervised misinformation detection methods have obtained acceptable
performances in key setups, they may require large amounts of labeled data from
various events, which can be time-consuming and tedious. In turn, directly
training a model by leveraging a publicly available dataset may fail to
generalize due to domain shifts between the training data (a.k.a. source
domains) and the data from target domains. Most prior work on domain shift
focuses on a single modality (e.g., text modality) and ignores the scenario
where sufficient unlabeled target domain data may not be readily available in
an early stage. The lack of data often happens due to the dynamic propagation
trend (i.e., the number of posts related to fake news increases slowly before
catching the public attention). We propose a novel robust domain and
cross-modal approach (\textbf{RDCM}) for multi-modal misinformation detection.
It reduces the domain shift by aligning the joint distribution of textual and
visual modalities through an inter-domain alignment module and bridges the
semantic gap between both modalities through a cross-modality alignment module.
We also propose a framework that simultaneously considers application scenarios
of domain generalization (in which the target domain data is unavailable) and
domain adaptation (in which unlabeled target domain data is available).
Evaluation results on two public multi-modal misinformation detection datasets
(Pheme and Twitter Datasets) evince the superiority of the proposed model. The
formal implementation of this paper can be found in this link:
https://github.com/less-and-less-bugs/RDCM",2311.14315v1,https://arxiv.org/pdf/2311.14315v1
"Robust and Interpretable COVID-19 Diagnosis on Chest X-ray Images using
  Adversarial Training","Karina Yang, Alexis Bennett, Dominique Duncan","The novel 2019 Coronavirus disease (COVID-19) global pandemic is a defining
health crisis. Recent efforts have been increasingly directed towards achieving
quick and accurate detection of COVID-19 across symptomatic patients to
mitigate the intensity and spread of the disease. Artificial intelligence (AI)
algorithms applied to chest X-ray (CXR) images have emerged as promising
diagnostic tools, and previous work has demonstrated impressive classification
performances. However, such methods have faced criticisms from physicians due
to their black-box reasoning process and unpredictable nature. In contrast to
professional radiologist diagnosis, AI systems often lack generalizability,
explainability, and robustness in the clinical decision making process. In our
work, we address these issues by first proposing an extensive baseline study,
training and evaluating 21 convolutional neural network (CNN) models on a
diverse set of 33,000+ CXR images to classify between healthy, COVID-19, and
non-COVID-19 pneumonia CXRs. Our resulting models achieved a 3-way
classification accuracy, recall, and precision of up to 97.03\%, 97.97\%, and
99.95\%, respectively. Next, we investigate the effectiveness of adversarial
training on model robustness and explainability via Gradient-weighted Class
Activation Mapping (Grad-CAM) heatmaps. We find that adversarially trained
models not only significantly outperform their standard counterparts on
classifying perturbed images, but also yield saliency maps that 1) better
specify clinically relevant features, 2) are robust against extraneous
artifacts, and 3) agree considerably more with expert radiologist findings.",2311.14227v1,https://arxiv.org/pdf/2311.14227v1
Efficient and Robust Jet Tagging at the LHC with Knowledge Distillation,"Ryan Liu, Abhijith Gandrakota, Jennifer Ngadiuba, Maria Spiropulu, Jean-Roch Vlimant","The challenging environment of real-time data processing systems at the Large
Hadron Collider (LHC) strictly limits the computational complexity of
algorithms that can be deployed. For deep learning models, this implies that
only models with low computational complexity that have weak inductive bias are
feasible. To address this issue, we utilize knowledge distillation to leverage
both the performance of large models and the reduced computational complexity
of small ones. In this paper, we present an implementation of knowledge
distillation, demonstrating an overall boost in the student models' performance
for the task of classifying jets at the LHC. Furthermore, by using a teacher
model with a strong inductive bias of Lorentz symmetry, we show that we can
induce the same inductive bias in the student model which leads to better
robustness against arbitrary Lorentz boost.",2311.14160v1,https://arxiv.org/pdf/2311.14160v1
"Byzantine Robustness and Partial Participation Can Be Achieved at Once:
  Just Clip Gradient Differences","Grigory Malinovsky, Peter Richtárik, Samuel Horváth, Eduard Gorbunov","Distributed learning has emerged as a leading paradigm for training large
machine learning models. However, in real-world scenarios, participants may be
unreliable or malicious, posing a significant challenge to the integrity and
accuracy of the trained models. Byzantine fault tolerance mechanisms have been
proposed to address these issues, but they often assume full participation from
all clients, which is not always practical due to the unavailability of some
clients or communication constraints. In our work, we propose the first
distributed method with client sampling and provable tolerance to Byzantine
workers. The key idea behind the developed method is the use of gradient
clipping to control stochastic gradient differences in recursive variance
reduction. This allows us to bound the potential harm caused by Byzantine
workers, even during iterations when all sampled clients are Byzantine.
Furthermore, we incorporate communication compression into the method to
enhance communication efficiency. Under general assumptions, we prove
convergence rates for the proposed method that match the existing
state-of-the-art (SOTA) theoretical results. We also propose a heuristic on
adjusting any Byzantine-robust method to a partial participation scenario via
clipping.",2311.14127v2,https://arxiv.org/pdf/2311.14127v2
Robust Decision Aggregation with Second-order Information,"Yuqi Pan, Zhaohua Chen, Yuqing Kong","We consider a decision aggregation problem with two experts who each make a
binary recommendation after observing a private signal about an unknown binary
world state. An agent, who does not know the joint information structure
between signals and states, sees the experts' recommendations and aims to match
the action with the true state. Under the scenario, we study whether
supplemented additionally with second-order information (each expert's forecast
on the other's recommendation) could enable a better aggregation.
  We adopt a minimax regret framework to evaluate the aggregator's performance,
by comparing it to an omniscient benchmark that knows the joint information
structure. With general information structures, we show that second-order
information provides no benefit. No aggregator can improve over a trivial
aggregator, which always follows the first expert's recommendation. However,
positive results emerge when we assume experts' signals are conditionally
independent given the world state. When the aggregator is deterministic, we
present a robust aggregator that leverages second-order information, which can
significantly outperform counterparts without it. Second, when two experts are
homogeneous, by adding a non-degenerate assumption on the signals, we
demonstrate that random aggregators using second-order information can surpass
optimal ones without it. In the remaining settings, the second-order
information is not beneficial. We also extend the above results to the setting
when the aggregator's utility function is more general.",2311.14094v1,https://arxiv.org/pdf/2311.14094v1
Task-Distributionally Robust Data-Free Meta-Learning,"Zixuan Hu, Li Shen, Zhenyi Wang, Yongxian Wei, Baoyuan Wu, Chun Yuan, Dacheng Tao","Data-Free Meta-Learning (DFML) aims to efficiently learn new tasks by
leveraging multiple pre-trained models without requiring their original
training data. Existing inversion-based DFML methods construct pseudo tasks
from a learnable dataset, which is inversely generated from the pre-trained
model pool. For the first time, we reveal two major challenges hindering their
practical deployments: Task-Distribution Shift (TDS) and Task-Distribution
Corruption (TDC). TDS leads to a biased meta-learner because of the skewed task
distribution towards newly generated tasks. TDC occurs when untrusted models
characterized by misleading labels or poor quality pollute the task
distribution. To tackle these issues, we introduce a robust DFML framework that
ensures task distributional robustness. We propose to meta-learn from a pseudo
task distribution, diversified through task interpolation within a compact
task-memory buffer. This approach reduces the meta-learner's overreliance on
newly generated tasks by maintaining consistent performance across a broader
range of interpolated memory tasks, thus ensuring its generalization for unseen
tasks. Additionally, our framework seamlessly incorporates an automated model
selection mechanism into the meta-training phase, parameterizing each model's
reliability as a learnable weight. This is optimized with a policy gradient
algorithm inspired by reinforcement learning, effectively addressing the
non-differentiable challenge posed by model selection. Comprehensive
experiments across various datasets demonstrate the framework's effectiveness
in mitigating TDS and TDC, underscoring its potential to improve DFML in
real-world scenarios.",2311.14756v1,https://arxiv.org/pdf/2311.14756v1
Parameter Exchange for Robust Dynamic Domain Generalization,"Luojun Lin, Zhifeng Shen, Zhishu Sun, Yuanlong Yu, Lei Zhang, Weijie Chen","Agnostic domain shift is the main reason of model degradation on the unknown
target domains, which brings an urgent need to develop Domain Generalization
(DG). Recent advances at DG use dynamic networks to achieve training-free
adaptation on the unknown target domains, termed Dynamic Domain Generalization
(DDG), which compensates for the lack of self-adaptability in static models
with fixed weights. The parameters of dynamic networks can be decoupled into a
static and a dynamic component, which are designed to learn domain-invariant
and domain-specific features, respectively. Based on the existing arts, in this
work, we try to push the limits of DDG by disentangling the static and dynamic
components more thoroughly from an optimization perspective. Our main
consideration is that we can enable the static component to learn
domain-invariant features more comprehensively by augmenting the
domain-specific information. As a result, the more comprehensive
domain-invariant features learned by the static component can then enforce the
dynamic component to focus more on learning adaptive domain-specific features.
To this end, we propose a simple yet effective Parameter Exchange (PE) method
to perturb the combination between the static and dynamic components. We
optimize the model using the gradients from both the perturbed and
non-perturbed feed-forward jointly to implicitly achieve the aforementioned
disentanglement. In this way, the two components can be optimized in a
mutually-beneficial manner, which can resist the agnostic domain shifts and
improve the self-adaptability on the unknown target domain. Extensive
experiments show that PE can be easily plugged into existing dynamic networks
to improve their generalization ability without bells and whistles.",2311.13928v1,https://arxiv.org/pdf/2311.13928v1
A Somewhat Robust Image Watermark against Diffusion-based Editing Models,"Mingtian Tan, Tianhao Wang, Somesh Jha","Recently, diffusion models (DMs) have become the state-of-the-art method for
image synthesis. Editing models based on DMs, known for their high fidelity and
precision, have inadvertently introduced new challenges related to image
copyright infringement and malicious editing. Our work is the first to
formalize and address this issue. After assessing and attempting to enhance
traditional image watermarking techniques, we recognize their limitations in
this emerging context. In response, we develop a novel technique, RIW (Robust
Invisible Watermarking), to embed invisible watermarks leveraging adversarial
example techniques. Our technique ensures a high extraction accuracy of $96\%$
for the invisible watermark after editing, compared to the $0\%$ offered by
conventional methods. We provide access to our code at
https://github.com/BennyTMT/RIW.",2311.13713v2,https://arxiv.org/pdf/2311.13713v2
"Robust Errant Beam Prognostics with Conditional Modeling for Particle
  Accelerators","Kishansingh Rajput, Malachi Schram, Willem Blokland, Yasir Alanazi, Pradeep Ramuhalli, Alexander Zhukov, Charles Peters, Ricardo Vilalta","Particle accelerators are complex and comprise thousands of components, with
many pieces of equipment running at their peak power. Consequently, particle
accelerators can fault and abort operations for numerous reasons. These faults
impact the availability of particle accelerators during scheduled run-time and
hamper the efficiency and the overall science output. To avoid these faults, we
apply anomaly detection techniques to predict any unusual behavior and perform
preemptive actions to improve the total availability of particle accelerators.
Semi-supervised Machine Learning (ML) based anomaly detection approaches such
as autoencoders and variational autoencoders are often used for such tasks.
However, supervised ML techniques such as Siamese Neural Network (SNN) models
can outperform unsupervised or semi-supervised approaches for anomaly detection
by leveraging the label information. One of the challenges specific to anomaly
detection for particle accelerators is the data's variability due to system
configuration changes. To address this challenge, we employ Conditional Siamese
Neural Network (CSNN) models and Conditional Variational Auto Encoder (CVAE)
models to predict errant beam pulses at the Spallation Neutron Source (SNS)
under different system configuration conditions and compare their performance.
We demonstrate that CSNN outperforms CVAE in our application.",2312.10040v2,https://arxiv.org/pdf/2312.10040v2
"Stable Unlearnable Example: Enhancing the Robustness of Unlearnable
  Examples via Stable Error-Minimizing Noise","Yixin Liu, Kaidi Xu, Xun Chen, Lichao Sun","The open source of large amounts of image data promotes the development of
deep learning techniques. Along with this comes the privacy risk of these
open-source image datasets being exploited by unauthorized third parties to
train deep learning models for commercial or illegal purposes. To avoid the
abuse of public data, a poisoning-based technique, the unlearnable example, is
proposed to significantly degrade the generalization performance of models by
adding a kind of imperceptible noise to the data. To further enhance its
robustness against adversarial training, existing works leverage iterative
adversarial training on both the defensive noise and the surrogate model.
However, it still remains unknown whether the robustness of unlearnable
examples primarily comes from the effect of enhancement in the surrogate model
or the defensive noise. Observing that simply removing the adversarial noise on
the training process of the defensive noise can improve the performance of
robust unlearnable examples, we identify that solely the surrogate model's
robustness contributes to the performance. Furthermore, we found a negative
correlation exists between the robustness of defensive noise and the protection
performance, indicating defensive noise's instability issue. Motivated by this,
to further boost the robust unlearnable example, we introduce stable
error-minimizing noise (SEM), which trains the defensive noise against random
perturbation instead of the time-consuming adversarial perturbation to improve
the stability of defensive noise. Through extensive experiments, we demonstrate
that SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100,
and ImageNet Subset in terms of both effectiveness and efficiency. The code is
available at https://github.com/liuyixin-louis/Stable-Unlearnable-Example.",2311.13091v2,https://arxiv.org/pdf/2311.13091v2
"Robustifying Generalizable Implicit Shape Networks with a Tunable
  Non-Parametric Model","Amine Ouasfi, Adnane Boukhayma","Feedforward generalizable models for implicit shape reconstruction from
unoriented point cloud present multiple advantages, including high performance
and inference speed. However, they still suffer from generalization issues,
ranging from underfitting the input point cloud, to misrepresenting samples
outside of the training data distribution, or with toplogies unseen at
training. We propose here an efficient mechanism to remedy some of these
limitations at test time. We combine the inter-shape data prior of the network
with an intra-shape regularization prior of a Nystr\""om Kernel Ridge
Regression, that we further adapt by fitting its hyperprameters to the current
shape. The resulting shape function defined in a shape specific Reproducing
Kernel Hilbert Space benefits from desirable stability and efficiency
properties and grants a shape adaptive expressiveness-robustness trade-off. We
demonstrate the improvement obtained through our method with respect to
baselines and the state-of-the-art using synthetic and real data.",2311.12967v1,https://arxiv.org/pdf/2311.12967v1
Data Diversity Matters for Robust Instruction Tuning,"Alexander Bukharin, Tuo Zhao","Recent works have shown that by curating high quality and diverse instruction
tuning datasets, we can significantly improve instruction-following
capabilities. However, creating such datasets is difficult and most works rely
on manual curation or proprietary language models. Automatic data curation is
difficult as it is still not clear how we can define diversity for instruction
tuning, how diversity and quality depend on one other, and how we can optimize
dataset quality and diversity. To resolve these issue, we propose a new
algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple
method to simultaneously control dataset diversity and quality, allowing us to
conduct an in-depth study on the effect of diversity and quality on instruction
tuning performance. From this study we draw two key insights (1) there is a
natural tradeoff between data diversity and quality and (2) increasing data
diversity significantly improves the worst case instruction following
performance, therefore improving robustness. We validate the performance of
QDIT on several large scale instruction tuning datasets, where we find it can
substantially improve worst and average case performance compared to
quality-driven data selection.",2311.14736v2,https://arxiv.org/pdf/2311.14736v2
"BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse
  Multiview Videos","Georgios Albanis, Nikolaos Zioulis, Kostas Kolomvatsos","Capturing smooth motions from videos using markerless techniques typically
involves complex processes such as temporal constraints, multiple stages with
data-driven regression and optimization, and bundle solving over temporal
windows. These processes can be inefficient and require tuning multiple
objectives across stages. In contrast, BundleMoCap introduces a novel and
efficient approach to this problem. It solves the motion capture task in a
single stage, eliminating the need for temporal smoothness objectives while
still delivering smooth motions. BundleMoCap outperforms the state-of-the-art
without increasing complexity. The key concept behind BundleMoCap is manifold
interpolation between latent keyframes. By relying on a local manifold
smoothness assumption, we can efficiently solve a bundle of frames using a
single code. Additionally, the method can be implemented as a sliding window
optimization and requires only the first frame to be properly initialized,
reducing the overall computational burden. BundleMoCap's strength lies in its
ability to achieve high-quality motion capture results with simplicity and
efficiency. More details can be found at https://moverseai.github.io/bundle/.",2311.12679v1,https://arxiv.org/pdf/2311.12679v1
"FedDRO: Federated Compositional Optimization for Distributionally Robust
  Learning","Prashant Khanduri, Chengyin Li, Rafi Ibn Sultan, Yao Qiang, Joerg Kliewer, Dongxiao Zhu","Recently, compositional optimization (CO) has gained popularity because of
its applications in distributionally robust optimization (DRO) and many other
machine learning problems. Large-scale and distributed availability of data
demands the development of efficient federated learning (FL) algorithms for
solving CO problems. Developing FL algorithms for CO is particularly
challenging because of the compositional nature of the objective. Moreover,
current state-of-the-art methods to solve such problems rely on large batch
gradients (depending on the solution accuracy) not feasible for most practical
settings. To address these challenges, in this work, we propose efficient
FedAvg-type algorithms for solving non-convex CO in the FL setting. We first
establish that vanilla FedAvg is not suitable to solve distributed CO problems
because of the data heterogeneity in the compositional objective at each client
which leads to the amplification of bias in the local compositional gradient
estimates. To this end, we propose a novel FL framework FedDRO that utilizes
the DRO problem structure to design a communication strategy that allows FedAvg
to control the bias in the estimation of the compositional gradient. A key
novelty of our work is to develop solution accuracy-independent algorithms that
do not require large batch gradients (and function evaluations) for solving
federated CO problems. We establish $\mathcal{O}(\epsilon^{-2})$ sample and
$\mathcal{O}(\epsilon^{-3/2})$ communication complexity in the FL setting while
achieving linear speedup with the number of clients. We corroborate our
theoretical findings with empirical studies on large-scale DRO problems.",2311.12652v1,https://arxiv.org/pdf/2311.12652v1
Training robust and generalizable quantum models,"Julian Berberich, Daniel Fink, Daniel Pranjić, Christian Tutschku, Christian Holm","Adversarial robustness and generalization are both crucial properties of
reliable machine learning models. In this paper, we study these properties in
the context of quantum machine learning based on Lipschitz bounds. We derive
parameter-dependent Lipschitz bounds for quantum models with trainable
encoding, showing that the norm of the data encoding has a crucial impact on
the robustness against data perturbations. Further, we derive a bound on the
generalization error which explicitly involves the parameters of the data
encoding. Our theoretical findings give rise to a practical strategy for
training robust and generalizable quantum models by regularizing the Lipschitz
bound in the cost. Further, we show that, for fixed and non-trainable
encodings, as those frequently employed in quantum machine learning, the
Lipschitz bound cannot be influenced by tuning the parameters. Thus, trainable
encodings are crucial for systematically adapting robustness and generalization
during training. The practical implications of our theoretical findings are
illustrated with numerical results.",2311.11871v3,https://arxiv.org/pdf/2311.11871v3
"Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural
  Networks","Mayar Lotfy, Anna Alperovich, Tommaso Giannantonio, Bjorn Barz, Xiaohan Zhang, Felix Holm, Nassir Navab, Felix Boehm, Carolin Schwamborn, Thomas K. Hoffmann, Patrick J. Schuler","Segmenting the boundary between tumor and healthy tissue during surgical
cancer resection poses a significant challenge. In recent years, Hyperspectral
Imaging (HSI) combined with Machine Learning (ML) has emerged as a promising
solution. However, due to the extensive information contained within the
spectral domain, most ML approaches primarily classify individual HSI
(super-)pixels, or tiles, without taking into account their spatial context. In
this paper, we propose an improved methodology that leverages the spatial
context of tiles for more robust and smoother segmentation. To address the
irregular shapes of tiles, we utilize Graph Neural Networks (GNNs) to propagate
context information across neighboring regions. The features for each tile
within the graph are extracted using a Convolutional Neural Network (CNN),
which is trained simultaneously with the subsequent GNN. Moreover, we
incorporate local image quality metrics into the loss function to enhance the
training procedure's robustness against low-quality regions in the training
images. We demonstrate the superiority of our proposed method using a clinical
ex vivo dataset consisting of 51 HSI images from 30 patients. Despite the
limited dataset, the GNN-based model significantly outperforms context-agnostic
approaches, accurately distinguishing between healthy and tumor tissues, even
in images from previously unseen patients. Furthermore, we show that our
carefully designed loss function, accounting for local image quality, results
in additional improvements. Our findings demonstrate that context-aware GNN
algorithms can robustly find tumor demarcations on HSI images, ultimately
contributing to better surgery success and patient outcome.",2311.11782v1,https://arxiv.org/pdf/2311.11782v1
Masked Autoencoders Are Robust Neural Architecture Search Learners,"Yiming Hu, Xiangxiang Chu, Bo Zhang","Neural Architecture Search (NAS) currently relies heavily on labeled data,
which is both expensive and time-consuming to acquire. In this paper, we
propose a novel NAS framework based on Masked Autoencoders (MAE) that
eliminates the need for labeled data during the search process. By replacing
the supervised learning objective with an image reconstruction task, our
approach enables the robust discovery of network architectures without
compromising performance and generalization ability. Additionally, we address
the problem of performance collapse encountered in the widely-used
Differentiable Architecture Search (DARTS) method in the unsupervised paradigm
by introducing a multi-scale decoder. Through extensive experiments conducted
on various search spaces and datasets, we demonstrate the effectiveness and
robustness of the proposed method, providing empirical evidence of its
superiority over baseline approaches.",2311.12086v2,https://arxiv.org/pdf/2311.12086v2
Towards Robust Text Retrieval with Progressive Learning,"Tong Wu, Yulei Qin, Enwei Zhang, Zihan Xu, Yuting Gao, Ke Li, Xing Sun","Retrieval augmentation has become an effective solution to empower large
language models (LLMs) with external and verified knowledge sources from the
database, which overcomes the limitations and hallucinations of LLMs in
handling up-to-date and domain-specific information. However, existing
embedding models for text retrieval usually have three non-negligible
limitations. First, the number and diversity of samples in a batch are too
restricted to supervise the modeling of textual nuances at scale. Second, the
high proportional noise are detrimental to the semantic correctness and
consistency of embeddings. Third, the equal treatment to easy and difficult
samples would cause sub-optimum convergence of embeddings with poorer
generalization. In this paper, we propose the PEG, a progressively learned
embeddings for robust text retrieval. Specifically, we increase the training
in-batch negative samples to 80,000, and for each query, we extracted five hard
negatives. Concurrently, we incorporated a progressive learning mechanism,
enabling the model to dynamically modulate its attention to the samples
throughout the entire training process. Additionally, PEG is trained on more
than 100 million data, encompassing a wide range of domains (e.g., finance,
medicine, and tourism) and covering various tasks (e.g., question-answering,
machine reading comprehension, and similarity matching). Extensive experiments
conducted on C-MTEB and DuReader demonstrate that PEG surpasses
state-of-the-art embeddings in retrieving true positives, highlighting its
significant potential for applications in LLMs. Our model is publicly available
at https://huggingface.co/TownsWu/PEG.",2311.11691v1,https://arxiv.org/pdf/2311.11691v1
"Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and
  Defensive Strategies","Feng Wang, M. Cenk Gursoy, Senem Velipasalar","In this paper, we present a multi-agent deep reinforcement learning (deep RL)
framework for network slicing in a dynamic environment with multiple base
stations and multiple users. In particular, we propose a novel deep RL
framework with multiple actors and centralized critic (MACC) in which actors
are implemented as pointer networks to fit the varying dimension of input. We
evaluate the performance of the proposed deep RL algorithm via simulations to
demonstrate its effectiveness. Subsequently, we develop a deep RL based jammer
with limited prior information and limited power budget. The goal of the jammer
is to minimize the transmission rates achieved with network slicing and thus
degrade the network slicing agents' performance. We design a jammer with both
listening and jamming phases and address jamming location optimization as well
as jamming channel optimization via deep RL. We evaluate the jammer at the
optimized location, generating interference attacks in the optimized set of
channels by switching between the jamming phase and listening phase. We show
that the proposed jammer can significantly reduce the victims' performance
without direct feedback or prior knowledge on the network slicing policies.
Finally, we devise a Nash-equilibrium-supervised policy ensemble mixed strategy
profile for network slicing (as a defensive measure) and jamming. We evaluate
the performance of the proposed policy ensemble algorithm by applying on the
network slicing agents and the jammer agent in simulations to show its
effectiveness.",2311.11206v1,https://arxiv.org/pdf/2311.11206v1
"Robustness Enhancement in Neural Networks with Alpha-Stable Training
  Noise","Xueqiong Yuan, Jipeng Li, Ercan Engin Kuruoğlu","With the increasing use of deep learning on data collected by non-perfect
sensors and in non-perfect environments, the robustness of deep learning
systems has become an important issue. A common approach for obtaining
robustness to noise has been to train deep learning systems with data augmented
with Gaussian noise. In this work, we challenge the common choice of Gaussian
noise and explore the possibility of stronger robustness for non-Gaussian
impulsive noise, specifically alpha-stable noise. Justified by the Generalized
Central Limit Theorem and evidenced by observations in various application
areas, alpha-stable noise is widely present in nature. By comparing the testing
accuracy of models trained with Gaussian noise and alpha-stable noise on data
corrupted by different noise, we find that training with alpha-stable noise is
more effective than Gaussian noise, especially when the dataset is corrupted by
impulsive noise, thus improving the robustness of the model. The generality of
this conclusion is validated through experiments conducted on various deep
learning models with image and time series datasets, and other benchmark
corrupted datasets. Consequently, we propose a novel data augmentation method
that replaces Gaussian noise, which is typically added to the training data,
with alpha-stable noise.",2311.10803v1,https://arxiv.org/pdf/2311.10803v1
"FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning
  Framework","Sheldon C. Ebron Jr., Kan Yang","Federated Learning (FL) enables collaborative machine learning model training
across multiple parties without sharing raw data. However, FL's distributed
nature allows malicious clients to impact model training through Byzantine or
backdoor attacks, using erroneous model updates. Existing defenses measure the
deviation of each update from a 'ground-truth model update.' They often rely on
a benign root dataset on the server or use trimmed mean or median for clipping,
both methods having limitations.
  We introduce FedTruth, a robust defense against model poisoning in FL.
FedTruth doesn't assume specific data distributions nor requires a benign root
dataset. It estimates a global model update with dynamic aggregation weights,
considering contributions from all benign clients. Empirical studies
demonstrate FedTruth's efficacy in mitigating the impacts of poisoned updates
from both Byzantine and backdoor attacks.",2311.10248v1,https://arxiv.org/pdf/2311.10248v1
"Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric
  Learning","Amartya Banerjee, Christopher J. Hazard, Jacob Beel, Cade Mack, Jack Xia, Michael Resnick, Will Goddin","Nonparametric learning is a fundamental concept in machine learning that aims
to capture complex patterns and relationships in data without making strong
assumptions about the underlying data distribution. Owing to simplicity and
familiarity, one of the most well-known algorithms under this paradigm is the
$k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine
learning in safety-critical applications, in this work, we shed new light on
the traditional nearest neighbors algorithm from the perspective of information
theory and propose a robust and interpretable framework for tasks such as
classification, regression, density estimation, and anomaly detection using a
single model. We can determine data point weights as well as feature
contributions by calculating the conditional entropy for adding a feature
without the need for explicit model training. This allows us to compute feature
contributions by providing detailed data point influence weights with perfect
attribution and can be used to query counterfactuals. Instead of using a
traditional distance measure which needs to be scaled and contextualized, we
use a novel formulation of $\textit{surprisal}$ (amount of information required
to explain the difference between the observed and expected result). Finally,
our work showcases the architecture's versatility by achieving state-of-the-art
results in classification and anomaly detection, while also attaining
competitive results for regression across a statistically significant number of
datasets.",2311.10246v2,https://arxiv.org/pdf/2311.10246v2
"Towards Improving Robustness Against Common Corruptions using Mixture of
  Class Specific Experts","Shashank Kotyan, Danilo Vasconcellos Vargas","Neural networks have demonstrated significant accuracy across various
domains, yet their vulnerability to subtle input alterations remains a
persistent challenge. Conventional methods like data augmentation, while
effective to some extent, fall short in addressing unforeseen corruptions,
limiting the adaptability of neural networks in real-world scenarios. In
response, this paper introduces a novel paradigm known as the Mixture of
Class-Specific Expert Architecture. The approach involves disentangling feature
learning for individual classes, offering a nuanced enhancement in scalability
and overall performance. By training dedicated network segments for each class
and subsequently aggregating their outputs, the proposed architecture aims to
mitigate vulnerabilities associated with common neural network structures. The
study underscores the importance of comprehensive evaluation methodologies,
advocating for the incorporation of benchmarks like the common corruptions
benchmark. This inclusion provides nuanced insights into the vulnerabilities of
neural networks, especially concerning their generalization capabilities and
robustness to unforeseen distortions. The research aligns with the broader
objective of advancing the development of highly robust learning systems
capable of nuanced reasoning across diverse and challenging real-world
scenarios. Through this contribution, the paper aims to foster a deeper
understanding of neural network limitations and proposes a practical approach
to enhance their resilience in the face of evolving and unpredictable
conditions.",2311.10177v1,https://arxiv.org/pdf/2311.10177v1
"Breaking Boundaries: Balancing Performance and Robustness in Deep
  Wireless Traffic Forecasting","Romain Ilbert, Thai V. Hoang, Zonghua Zhang, Themis Palpanas","Balancing the trade-off between accuracy and robustness is a long-standing
challenge in time series forecasting. While most of existing robust algorithms
have achieved certain suboptimal performance on clean data, sustaining the same
performance level in the presence of data perturbations remains extremely hard.
In this paper, we study a wide array of perturbation scenarios and propose
novel defense mechanisms against adversarial attacks using real-world telecom
data. We compare our strategy against two existing adversarial training
algorithms under a range of maximal allowed perturbations, defined using
$\ell_{\infty}$-norm, $\in [0.1,0.4]$. Our findings reveal that our hybrid
strategy, which is composed of a classifier to detect adversarial examples, a
denoiser to eliminate noise from the perturbed data samples, and a standard
forecaster, achieves the best performance on both clean and perturbed data. Our
optimal model can retain up to $92.02\%$ the performance of the original
forecasting model in terms of Mean Squared Error (MSE) on clean data, while
being more robust than the standard adversarially trained models on perturbed
data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing
methods on normal and perturbed data, respectively. In addition, the components
of our models can be trained in parallel, resulting in better computational
efficiency. Our results indicate that we can optimally balance the trade-off
between the performance and robustness of forecasting models by improving the
classifier and denoiser, even in the presence of sophisticated and destructive
poisoning attacks.",2311.09790v3,https://arxiv.org/pdf/2311.09790v3
Robust Contrastive Learning With Theory Guarantee,"Ngoc N. Tran, Lam Tran, Hoang Phan, Anh Bui, Tung Pham, Toan Tran, Dinh Phung, Trung Le","Contrastive learning (CL) is a self-supervised training paradigm that allows
us to extract meaningful features without any label information. A typical CL
framework is divided into two phases, where it first tries to learn the
features from unlabelled data, and then uses those features to train a linear
classifier with the labeled data. While a fair amount of existing theoretical
works have analyzed how the unsupervised loss in the first phase can support
the supervised loss in the second phase, none has examined the connection
between the unsupervised loss and the robust supervised loss, which can shed
light on how to construct an effective unsupervised loss for the first phase of
CL. To fill this gap, our work develops rigorous theories to dissect and
identify which components in the unsupervised loss can help improve the robust
supervised loss and conduct proper experiments to verify our findings.",2311.09671v1,https://arxiv.org/pdf/2311.09671v1
VideoCon: Robust Video-Language Alignment via Contrast Captions,"Hritik Bansal, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang, Aditya Grover","Despite being (pre)trained on a massive amount of data, state-of-the-art
video-language alignment models are not robust to semantically-plausible
contrastive changes in the video captions. Our work addresses this by
identifying a broad spectrum of contrast misalignments, such as replacing
entities, actions, and flipping event order, which alignment models should be
robust against. To this end, we introduce the VideoCon, a video-language
alignment dataset constructed by a large language model that generates
plausible contrast video captions and explanations for differences between
original and contrast video captions. Then, a generative video-language model
is finetuned with VideoCon to assess video-language entailment and generate
explanations. Our VideoCon-based alignment model significantly outperforms
current models. It exhibits a 12-point increase in AUC for the video-language
alignment task on human-generated contrast captions. Finally, our model sets
new state of the art zero-shot performance in temporally-extensive
video-language tasks such as text-to-video retrieval (SSv2-Temporal) and video
question answering (ATP-Hard). Moreover, our model shows superior performance
on novel videos and human-crafted captions and explanations. Our code and data
are available at https://github.com/Hritikbansal/videocon.",2311.10111v1,https://arxiv.org/pdf/2311.10111v1
"Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language
  Models","Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu","Retrieval-augmented language models (RALMs) represent a substantial
advancement in the capabilities of large language models, notably in reducing
factual hallucination by leveraging external knowledge sources. However, the
reliability of the retrieved information is not always guaranteed. The
retrieval of irrelevant data can lead to misguided responses, and potentially
causing the model to overlook its inherent knowledge, even when it possesses
adequate information to address the query. Moreover, standard RALMs often
struggle to assess whether they possess adequate knowledge, both intrinsic and
retrieved, to provide an accurate answer. In situations where knowledge is
lacking, these systems should ideally respond with ""unknown"" when the answer is
unattainable. In response to these challenges, we introduces Chain-of-Noting
(CoN), a novel approach aimed at improving the robustness of RALMs in facing
noisy, irrelevant documents and in handling unknown scenarios. The core idea of
CoN is to generate sequential reading notes for retrieved documents, enabling a
thorough evaluation of their relevance to the given question and integrating
this information to formulate the final answer. We employed ChatGPT to create
training data for CoN, which was subsequently trained on an LLaMa-2 7B model.
Our experiments across four open-domain QA benchmarks show that RALMs equipped
with CoN significantly outperform standard RALMs. Notably, CoN achieves an
average improvement of +7.9 in EM score given entirely noisy retrieved
documents and +10.5 in rejection rates for real-time questions that fall
outside the pre-training knowledge scope.",2311.09210v1,https://arxiv.org/pdf/2311.09210v1
Assessing the Robustness of Intelligence-Driven Reinforcement Learning,"Lorenzo Nodari, Federico Cerutti","Robustness to noise is of utmost importance in reinforcement learning
systems, particularly in military contexts where high stakes and uncertain
environments prevail. Noise and uncertainty are inherent features of military
operations, arising from factors such as incomplete information, adversarial
actions, or unpredictable battlefield conditions. In RL, noise can critically
impact decision-making, mission success, and the safety of personnel. Reward
machines offer a powerful tool to express complex reward structures in RL
tasks, enabling the design of tailored reinforcement signals that align with
mission objectives. This paper considers the problem of the robustness of
intelligence-driven reinforcement learning based on reward machines. The
preliminary results presented suggest the need for further research in
evidential reasoning and learning to harden current state-of-the-art
reinforcement learning approaches before being mission-critical-ready.",2311.09027v1,https://arxiv.org/pdf/2311.09027v1
On the Foundation of Distributionally Robust Reinforcement Learning,"Shengbo Wang, Nian Si, Jose Blanchet, Zhengyuan Zhou","Motivated by the need for a robust policy in the face of environment shifts
between training and the deployment, we contribute to the theoretical
foundation of distributionally robust reinforcement learning (DRRL). This is
accomplished through a comprehensive modeling framework centered around
distributionally robust Markov decision processes (DRMDPs). This framework
obliges the decision maker to choose an optimal policy under the worst-case
distributional shift orchestrated by an adversary. By unifying and extending
existing formulations, we rigorously construct DRMDPs that embraces various
modeling attributes for both the decision maker and the adversary. These
attributes include adaptability granularity, exploring history-dependent,
Markov, and Markov time-homogeneous decision maker and adversary dynamics.
Additionally, we delve into the flexibility of shifts induced by the adversary,
examining SA and S-rectangularity. Within this DRMDP framework, we investigate
conditions for the existence or absence of the dynamic programming principle
(DPP). From an algorithmic standpoint, the existence of DPP holds significant
implications, as the vast majority of existing data and computationally
efficiency RL algorithms are reliant on the DPP. To study its existence, we
comprehensively examine combinations of controller and adversary attributes,
providing streamlined proofs grounded in a unified methodology. We also offer
counterexamples for settings in which a DPP with full generality is absent.",2311.09018v3,https://arxiv.org/pdf/2311.09018v3
Semidefinite programs simulate approximate message passing robustly,"Misha Ivkov, Tselil Schramm","Approximate message passing (AMP) is a family of iterative algorithms that
generalize matrix power iteration. AMP algorithms are known to optimally solve
many average-case optimization problems. In this paper, we show that a large
class of AMP algorithms can be simulated in polynomial time by \emph{local
statistics hierarchy} semidefinite programs (SDPs), even when an unknown
principal minor of measure $1/\mathrm{polylog}(\mathrm{dimension})$ is
adversarially corrupted. Ours are the first robust guarantees for many of these
problems. Further, our results offer an interesting counterpoint to strong
lower bounds against less constrained SDP relaxations for average-case
max-cut-gain (a.k.a. ""optimizing the Sherrington-Kirkpatrick Hamiltonian"") and
other problems.",2311.09017v1,https://arxiv.org/pdf/2311.09017v1
Adversarially Robust Spiking Neural Networks Through Conversion,"Ozan Özdenizci, Robert Legenstein","Spiking neural networks (SNNs) provide an energy-efficient alternative to a
variety of artificial neural network (ANN) based AI applications. As the
progress in neuromorphic computing with SNNs expands their use in applications,
the problem of adversarial robustness of SNNs becomes more pronounced. To the
contrary of the widely explored end-to-end adversarial training based
solutions, we address the limited progress in scalable robust SNN training
methods by proposing an adversarially robust ANN-to-SNN conversion algorithm.
Our method provides an efficient approach to embrace various computationally
demanding robust learning objectives that have been proposed for ANNs. During a
post-conversion robust finetuning phase, our method adversarially optimizes
both layer-wise firing thresholds and synaptic connectivity weights of the SNN
to maintain transferred robustness gains from the pre-trained ANN. We perform
experimental evaluations in a novel setting proposed to rigorously assess the
robustness of SNNs, where numerous adaptive adversarial attacks that account
for the spike-based operation dynamics are considered. Results show that our
approach yields a scalable state-of-the-art solution for adversarially robust
deep SNNs with low-latency.",2311.09266v2,https://arxiv.org/pdf/2311.09266v2
"Evaluating Concurrent Robustness of Language Models Across Diverse
  Challenge Sets","Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, Dan Roth","Language models, characterized by their black-box nature, often hallucinate
and display sensitivity to input perturbations, causing concerns about trust.
To enhance trust, it is imperative to gain a comprehensive understanding of the
model's failure modes and develop effective strategies to improve their
performance. In this study, we introduce a methodology designed to examine how
input perturbations affect language models across various scales, including
pre-trained models and large language models (LLMs). Utilizing fine-tuning, we
enhance the model's robustness to input perturbations. Additionally, we
investigate whether exposure to one perturbation enhances or diminishes the
model's performance with respect to other perturbations. To address robustness
against multiple perturbations, we present three distinct fine-tuning
strategies. Furthermore, we broaden the scope of our methodology to encompass
large language models (LLMs) by leveraging a chain of thought (CoT) prompting
approach augmented with exemplars. We employ the Tabular-NLI task to showcase
how our proposed strategies adeptly train a robust model, enabling it to
address diverse perturbations while maintaining accuracy on the original
dataset.",2311.08662v2,https://arxiv.org/pdf/2311.08662v2
"MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable
  Trajectory Generation","Ehsan Asali, Prashant Doshi, Jin Sun","The learn-from-observation (LfO) paradigm is a human-inspired mode for a
robot to learn to perform a task simply by watching it being performed. LfO can
facilitate robot integration on factory floors by minimizing disruption and
reducing tedious programming. A key component of the LfO pipeline is a
transformation of the depth camera frames to the corresponding task state and
action pairs, which are then relayed to learning techniques such as imitation
or inverse reinforcement learning for understanding the task parameters. While
several existing computer vision models analyze videos for activity
recognition, SA-Net specifically targets robotic LfO from RGB-D data. However,
SA-Net and many other models analyze frame data captured from a single
viewpoint. Their analysis is therefore highly sensitive to occlusions of the
observed task, which are frequent in deployments. An obvious way of reducing
occlusions is to simultaneously observe the task from multiple viewpoints and
synchronously fuse the multiple streams in the model. Toward this, we present
multi-view SA-Net, which generalizes the SA-Net model to allow the perception
of multiple viewpoints of the task activity, integrate them, and better
recognize the state and action in each frame. Performance evaluations on two
distinct domains establish that MVSA-Net recognizes the state-action pairs
under occlusion more accurately compared to single-view MVSA-Net and other
baselines. Our ablation studies further evaluate its performance under
different ambient conditions and establish the contribution of the architecture
components. As such, MVSA-Net offers a significantly more robust and deployable
state-action trajectory generation compared to previous methods.",2311.08393v3,https://arxiv.org/pdf/2311.08393v3
The Perception-Robustness Tradeoff in Deterministic Image Restoration,"Guy Ohayon, Tomer Michaeli, Michael Elad","We study the behavior of deterministic methods for solving inverse problems
in imaging. These methods are commonly designed to achieve two goals: (1)
attaining high perceptual quality, and (2) generating reconstructions that are
consistent with the measurements. We provide a rigorous proof that the better a
predictor satisfies these two requirements, the larger its Lipschitz constant
must be, regardless of the nature of the degradation involved. In particular,
to approach perfect perceptual quality and perfect consistency, the Lipschitz
constant of the model must grow to infinity. This implies that such methods are
necessarily more susceptible to adversarial attacks. We demonstrate our theory
on single image super-resolution algorithms, addressing both noisy and
noiseless settings. We also show how this undesired behavior can be leveraged
to explore the posterior distribution, thereby allowing the deterministic model
to imitate stochastic methods.",2311.09253v4,https://arxiv.org/pdf/2311.09253v4
"RECALL: A Benchmark for LLMs Robustness against External Counterfactual
  Knowledge","Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun","LLMs and AI chatbots have improved people's efficiency in various fields.
However, the necessary knowledge for answering the question may be beyond the
models' knowledge boundaries. To mitigate this issue, many researchers try to
introduce external knowledge, such as knowledge graphs and Internet contents,
into LLMs for up-to-date information. However, the external information from
the Internet may include counterfactual information that will confuse the model
and lead to an incorrect response. Thus there is a pressing need for LLMs to
possess the ability to distinguish reliable information from external
knowledge. Therefore, to evaluate the ability of LLMs to discern the
reliability of external knowledge, we create a benchmark from existing
knowledge bases. Our benchmark consists of two tasks, Question Answering and
Text Generation, and for each task, we provide models with a context containing
counterfactual information. Evaluation results show that existing LLMs are
susceptible to interference from unreliable external knowledge with
counterfactual information, and simple intervention methods make limited
contributions to the alleviation of this issue.",2311.08147v1,https://arxiv.org/pdf/2311.08147v1
"Improving the Robustness of Distantly-Supervised Named Entity
  Recognition via Uncertainty-Aware Teacher Learning and Student-Student
  Collaborative Learning","Helan Hu, Shuzheng Si, Haozhe Zhao, Shuang Zeng, Kaikai An, Zefan Cai, Baobao Chang","Distantly-Supervised Named Entity Recognition (DS-NER) is widely used in
real-world scenarios. It can effectively alleviate the burden of annotation by
matching entities in existing knowledge bases with snippets in the text but
suffer from the label noise. Recent works attempt to adopt the teacher-student
framework to gradually refine the training labels and improve the overall
robustness. However, these teacher-student methods achieve limited performance
because the poor calibration of the teacher network produces incorrectly
pseudo-labeled samples, leading to error propagation. Therefore, we propose:
(1) Uncertainty-Aware Teacher Learning that leverages the prediction
uncertainty to reduce the number of incorrect pseudo labels in the
self-training stage; (2) Student-Student Collaborative Learning that allows the
transfer of reliable labels between two student networks instead of
indiscriminately relying on all pseudo labels from its teacher, and further
enables a full exploration of mislabeled samples rather than simply filtering
unreliable pseudo-labeled samples. We evaluate our proposed method on five
DS-NER datasets, demonstrating that our method is superior to the
state-of-the-art DS-NER methods.",2311.08010v2,https://arxiv.org/pdf/2311.08010v2
"Towards Improving Robustness Against Common Corruptions in Object
  Detectors Using Adversarial Contrastive Learning","Shashank Kotyan, Danilo Vasconcellos Vargas","Neural networks have revolutionized various domains, exhibiting remarkable
accuracy in tasks like natural language processing and computer vision.
However, their vulnerability to slight alterations in input samples poses
challenges, particularly in safety-critical applications like autonomous
driving. Current approaches, such as introducing distortions during training,
fall short in addressing unforeseen corruptions. This paper proposes an
innovative adversarial contrastive learning framework to enhance neural network
robustness simultaneously against adversarial attacks and common corruptions.
By generating instance-wise adversarial examples and optimizing contrastive
loss, our method fosters representations that resist adversarial perturbations
and remain robust in real-world scenarios. Subsequent contrastive learning then
strengthens the similarity between clean samples and their adversarial
counterparts, fostering representations resistant to both adversarial attacks
and common distortions. By focusing on improving performance under adversarial
and real-world conditions, our approach aims to bolster the robustness of
neural networks in safety-critical applications, such as autonomous vehicles
navigating unpredictable weather conditions. We anticipate that this framework
will contribute to advancing the reliability of neural networks in challenging
environments, facilitating their widespread adoption in mission-critical
scenarios.",2311.07928v1,https://arxiv.org/pdf/2311.07928v1
"Mixture of Coupled HMMs for Robust Modeling of Multivariate Healthcare
  Time Series","Onur Poyraz, Pekka Marttinen","Analysis of multivariate healthcare time series data is inherently
challenging: irregular sampling, noisy and missing values, and heterogeneous
patient groups with different dynamics violating exchangeability. In addition,
interpretability and quantification of uncertainty are critically important.
Here, we propose a novel class of models, a mixture of coupled hidden Markov
models (M-CHMM), and demonstrate how it elegantly overcomes these challenges.
To make the model learning feasible, we derive two algorithms to sample the
sequences of the latent variables in the CHMM: samplers based on (i) particle
filtering and (ii) factorized approximation. Compared to existing inference
methods, our algorithms are computationally tractable, improve mixing, and
allow for likelihood estimation, which is necessary to learn the mixture model.
Experiments on challenging real-world epidemiological and semi-synthetic data
demonstrate the advantages of the M-CHMM: improved data fit, capacity to
efficiently handle missing and noisy measurements, improved prediction
accuracy, and ability to identify interpretable subsets in the data.",2311.07867v1,https://arxiv.org/pdf/2311.07867v1
"Robust and Scalable Hyperdimensional Computing With Brain-Like Neural
  Adaptations","Junyao Wang, Mohammad Abdullah Al Faruque","The Internet of Things (IoT) has facilitated many applications utilizing
edge-based machine learning (ML) methods to analyze locally collected data.
Unfortunately, popular ML algorithms often require intensive computations
beyond the capabilities of today's IoT devices. Brain-inspired hyperdimensional
computing (HDC) has been introduced to address this issue. However, existing
HDCs use static encoders, requiring extremely high dimensionality and hundreds
of training iterations to achieve reasonable accuracy. This results in a huge
efficiency loss, severely impeding the application of HDCs in IoT systems. We
observed that a main cause is that the encoding module of existing HDCs lacks
the capability to utilize and adapt to information learned during training. In
contrast, neurons in human brains dynamically regenerate all the time and
provide more useful functionalities when learning new information. While the
goal of HDC is to exploit the high-dimensionality of randomly generated base
hypervectors to represent the information as a pattern of neural activity, it
remains challenging for existing HDCs to support a similar behavior as brain
neural regeneration. In this work, we present dynamic HDC learning frameworks
that identify and regenerate undesired dimensions to provide adequate accuracy
with significantly lowered dimensionalities, thereby accelerating both the
training and inference.",2311.07705v1,https://arxiv.org/pdf/2311.07705v1
"Automatic Identification of Driving Maneuver Patterns using a Robust
  Hidden Semi-Markov Models","Matthew Aguirre, Wenbo Sun, Jionghua, Jin, Yang Chen","There is an increase in interest to model driving maneuver patterns via the
automatic unsupervised clustering of naturalistic sequential kinematic driving
data. The patterns learned are often used in transportation research areas such
as eco-driving, road safety, and intelligent vehicles. One such model capable
of modeling these patterns is the Hierarchical Dirichlet Process Hidden
Semi-Markov Model (HDP-HSMM), as it is often used to estimate data
segmentation, state duration, and transition probabilities. While this model is
a powerful tool for automatically clustering observed sequential data, the
existing HDP-HSMM estimation suffers from an inherent tendency to overestimate
the number of states. This can result in poor estimation, which can potentially
impact impact transportation research through incorrect inference of driving
patterns. In this paper, a new robust HDP-HSMM (rHDP-HSMM) method is proposed
to reduce the number of redundant states and improve the consistency of the
model's estimation. Both a simulation study and a case study using naturalistic
driving data are presented to demonstrate the effectiveness of the proposed
rHDP-HSMM in identifying and inference of driving maneuver patterns.",2311.07527v1,https://arxiv.org/pdf/2311.07527v1
"On the Robustness of Neural Collapse and the Neural Collapse of
  Robustness","Jingtong Su, Ya Shi Zhang, Nikolaos Tsilivis, Julia Kempe","Neural Collapse refers to the curious phenomenon in the end of training of a
neural network, where feature vectors and classification weights converge to a
very simple geometrical arrangement (a simplex). While it has been observed
empirically in various cases and has been theoretically motivated, its
connection with crucial properties of neural networks, like their
generalization and robustness, remains unclear. In this work, we study the
stability properties of these simplices. We find that the simplex structure
disappears under small adversarial attacks, and that perturbed examples ""leap""
between simplex vertices. We further analyze the geometry of networks that are
optimized to be robust against adversarial perturbations of the input, and find
that Neural Collapse is a pervasive phenomenon in these cases as well, with
clean and perturbed representations forming aligned simplices, and giving rise
to a robust simple nearest-neighbor classifier. By studying the propagation of
the amount of collapse inside the network, we identify novel properties of both
robust and non-robust machine learning models, and show that earlier, unlike
later layers maintain reliable simplices on perturbed data.",2311.07444v1,https://arxiv.org/pdf/2311.07444v1
Robust Regression over Averaged Uncertainty,"Dimitris Bertsimas, Yu Ma","We propose a new formulation of robust regression by integrating all
realizations of the uncertainty set and taking an averaged approach to obtain
the optimal solution for the ordinary least-squared regression problem. We show
that this formulation surprisingly recovers ridge regression and establishes
the missing link between robust optimization and the mean squared error
approaches for existing regression problems. We first prove the equivalence for
four uncertainty sets: ellipsoidal, box, diamond, and budget, and provide
closed-form formulations of the penalty term as a function of the sample size,
feature size, as well as perturbation protection strength. We then show in
synthetic datasets with different levels of perturbations, a consistent
improvement of the averaged formulation over the existing worst-case
formulation in out-of-sample performance. Importantly, as the perturbation
level increases, the improvement increases, confirming our method's advantage
in high-noise environments. We report similar improvements in the out-of-sample
datasets in real-world regression problems obtained from UCI datasets.",2311.06960v1,https://arxiv.org/pdf/2311.06960v1
"Learning Predictive Safety Filter via Decomposition of Robust Invariant
  Set","Zeyang Li, Chuxiong Hu, Weiye Zhao, Changliu Liu","Ensuring safety of nonlinear systems under model uncertainty and external
disturbances is crucial, especially for real-world control tasks. Predictive
methods such as robust model predictive control (RMPC) require solving
nonconvex optimization problems online, which leads to high computational
burden and poor scalability. Reinforcement learning (RL) works well with
complex systems, but pays the price of losing rigorous safety guarantee. This
paper presents a theoretical framework that bridges the advantages of both RMPC
and RL to synthesize safety filters for nonlinear systems with state- and
action-dependent uncertainty. We decompose the robust invariant set (RIS) into
two parts: a target set that aligns with terminal region design of RMPC, and a
reach-avoid set that accounts for the rest of RIS. We propose a policy
iteration approach for robust reach-avoid problems and establish its monotone
convergence. This method sets the stage for an adversarial actor-critic deep RL
algorithm, which simultaneously synthesizes a reach-avoid policy network, a
disturbance policy network, and a reach-avoid value network. The learned
reach-avoid policy network is utilized to generate nominal trajectories for
online verification, which filters potentially unsafe actions that may drive
the system into unsafe regions when worst-case disturbances are applied. We
formulate a second-order cone programming (SOCP) approach for online
verification using system level synthesis, which optimizes for the worst-case
reach-avoid value of any possible trajectories. The proposed safety filter
requires much lower computational complexity than RMPC and still enjoys
persistent robust safety guarantee. The effectiveness of our method is
illustrated through a numerical example.",2311.06769v1,https://arxiv.org/pdf/2311.06769v1
"Federated Learning for Generalization, Robustness, Fairness: A Survey
  and Benchmark","Wenke Huang, Mang Ye, Zekun Shi, Guancheng Wan, He Li, Bo Du, Qiang Yang","Federated learning has emerged as a promising paradigm for privacy-preserving
collaboration among different parties. Recently, with the popularity of
federated learning, an influx of approaches have delivered towards different
realistic challenges. In this survey, we provide a systematic overview of the
important and recent developments of research on federated learning. Firstly,
we introduce the study history and terminology definition of this area. Then,
we comprehensively review three basic lines of research: generalization,
robustness, and fairness, by introducing their respective background concepts,
task settings, and main challenges. We also offer a detailed overview of
representative literature on both methods and datasets. We further benchmark
the reviewed methods on several well-known datasets. Finally, we point out
several open issues in this field and suggest opportunities for further
research. We also provide a public website to continuously track developments
in this fast advancing field: https://github.com/WenkeHuang/MarsFL.",2311.06750v1,https://arxiv.org/pdf/2311.06750v1
Understanding Grokking Through A Robustness Viewpoint,"Zhiquan Tan, Weiran Huang","Recently, an interesting phenomenon called grokking has gained much
attention, where generalization occurs long after the models have initially
overfitted the training data. We try to understand this seemingly strange
phenomenon through the robustness of the neural network. From a robustness
perspective, we show that the popular $l_2$ weight norm (metric) of the neural
network is actually a sufficient condition for grokking. Based on the previous
observations, we propose perturbation-based methods to speed up the
generalization process. In addition, we examine the standard training process
on the modulo addition dataset and find that it hardly learns other basic group
operations before grokking, for example, the commutative law. Interestingly,
the speed-up of generalization when using our proposed method can be explained
by learning the commutative law, a necessary condition when the model groks on
the test dataset. We also empirically find that $l_2$ norm correlates with
grokking on the test data not in a timely way, we propose new metrics based on
robustness and information theory and find that our new metrics correlate well
with the grokking phenomenon and may be used to predict grokking.",2311.06597v2,https://arxiv.org/pdf/2311.06597v2
"CALLOC: Curriculum Adversarial Learning for Secure and Robust Indoor
  Localization","Danish Gufran, Sudeep Pasricha","Indoor localization has become increasingly vital for many applications from
tracking assets to delivering personalized services. Yet, achieving pinpoint
accuracy remains a challenge due to variations across indoor environments and
devices used to assist with localization. Another emerging challenge is
adversarial attacks on indoor localization systems that not only threaten
service integrity but also reduce localization accuracy. To combat these
challenges, we introduce CALLOC, a novel framework designed to resist
adversarial attacks and variations across indoor environments and devices that
reduce system accuracy and reliability. CALLOC employs a novel adaptive
curriculum learning approach with a domain specific lightweight scaled-dot
product attention neural network, tailored for adversarial and variation
resilience in practical use cases with resource constrained mobile devices.
Experimental evaluations demonstrate that CALLOC can achieve improvements of up
to 6.03x in mean error and 4.6x in worst-case error against state-of-the-art
indoor localization frameworks, across diverse building floorplans, mobile
devices, and adversarial attacks scenarios.",2311.06361v1,https://arxiv.org/pdf/2311.06361v1
Differentiable VQ-VAE's for Robust White Matter Streamline Encodings,"Andrew Lizarraga, Brandon Taraku, Edouardo Honig, Ying Nian Wu, Shantanu H. Joshi","Given the complex geometry of white matter streamlines, Autoencoders have
been proposed as a dimension-reduction tool to simplify the analysis
streamlines in a low-dimensional latent spaces. However, despite these recent
successes, the majority of encoder architectures only perform dimension
reduction on single streamlines as opposed to a full bundle of streamlines.
This is a severe limitation of the encoder architecture that completely
disregards the global geometric structure of streamlines at the expense of
individual fibers. Moreover, the latent space may not be well structured which
leads to doubt into their interpretability. In this paper we propose a novel
Differentiable Vector Quantized Variational Autoencoder, which are engineered
to ingest entire bundles of streamlines as single data-point and provides
reliable trustworthy encodings that can then be later used to analyze
streamlines in the latent space. Comparisons with several state of the art
Autoencoders demonstrate superior performance in both encoding and synthesis.",2311.06212v2,https://arxiv.org/pdf/2311.06212v2
Distributionally Robust Skeleton Learning of Discrete Bayesian Networks,"Yeshu Li, Brian D. Ziebart","We consider the problem of learning the exact skeleton of general discrete
Bayesian networks from potentially corrupted data. Building on distributionally
robust optimization and a regression approach, we propose to optimize the most
adverse risk over a family of distributions within bounded Wasserstein distance
or KL divergence to the empirical distribution. The worst-case risk accounts
for the effect of outliers. The proposed approach applies for general
categorical random variables without assuming faithfulness, an ordinal
relationship or a specific form of conditional distribution. We present
efficient algorithms and show the proposed methods are closely related to the
standard regularized regression approach. Under mild assumptions, we derive
non-asymptotic guarantees for successful structure learning with logarithmic
sample complexities for bounded-degree graphs. Numerical study on synthetic and
real datasets validates the effectiveness of our method. Code is available at
https://github.com/DanielLeee/drslbn.",2311.06117v1,https://arxiv.org/pdf/2311.06117v1
Doubly Robust Structure Identification from Temporal Data,"Emmanouil Angelis, Francesco Quinzan, Ashkan Soleymani, Patrick Jaillet, Stefan Bauer","Learning the causes of time-series data is a fundamental task in many
applications, spanning from finance to earth sciences or bio-medical
applications. Common approaches for this task are based on vector
auto-regression, and they do not take into account unknown confounding between
potential causes. However, in settings with many potential causes and noisy
data, these approaches may be substantially biased. Furthermore, potential
causes may be correlated in practical applications. Moreover, existing
algorithms often do not work with cyclic data. To address these challenges, we
propose a new doubly robust method for Structure Identification from Temporal
Data ( SITD ). We provide theoretical guarantees, showing that our method
asymptotically recovers the true underlying causal structure. Our analysis
extends to cases where the potential causes have cycles and they may be
confounded. We further perform extensive experiments to showcase the superior
performance of our method.",2311.06012v1,https://arxiv.org/pdf/2311.06012v1
"Robust Adversarial Attacks Detection for Deep Learning based Relative
  Pose Estimation for Space Rendezvous","Ziwei Wang, Nabil Aouf, Jose Pizarro, Christophe Honvault","Research on developing deep learning techniques for autonomous spacecraft
relative navigation challenges is continuously growing in recent years.
Adopting those techniques offers enhanced performance. However, such approaches
also introduce heightened apprehensions regarding the trustability and security
of such deep learning methods through their susceptibility to adversarial
attacks. In this work, we propose a novel approach for adversarial attack
detection for deep neural network-based relative pose estimation schemes based
on the explainability concept. We develop for an orbital rendezvous scenario an
innovative relative pose estimation technique adopting our proposed
Convolutional Neural Network (CNN), which takes an image from the chaser's
onboard camera and outputs accurately the target's relative position and
rotation. We perturb seamlessly the input images using adversarial attacks that
are generated by the Fast Gradient Sign Method (FGSM). The adversarial attack
detector is then built based on a Long Short Term Memory (LSTM) network which
takes the explainability measure namely SHapley Value from the CNN-based pose
estimator and flags the detection of adversarial attacks when acting.
Simulation results show that the proposed adversarial attack detector achieves
a detection accuracy of 99.21%. Both the deep relative pose estimator and
adversarial attack detector are then tested on real data captured from our
laboratory-designed setup. The experimental results from our
laboratory-designed setup demonstrate that the proposed adversarial attack
detector achieves an average detection accuracy of 96.29%.",2311.05992v1,https://arxiv.org/pdf/2311.05992v1
Low-Multi-Rank High-Order Bayesian Robust Tensor Factorization,"Jianan Liu, Chunguang Li","The recently proposed tensor robust principal component analysis (TRPCA)
methods based on tensor singular value decomposition (t-SVD) have achieved
numerous successes in many fields. However, most of these methods are only
applicable to third-order tensors, whereas the data obtained in practice are
often of higher order, such as fourth-order color videos, fourth-order
hyperspectral videos, and fifth-order light-field images. Additionally, in the
t-SVD framework, the multi-rank of a tensor can describe more fine-grained
low-rank structure in the tensor compared with the tubal rank. However,
determining the multi-rank of a tensor is a much more difficult problem than
determining the tubal rank. Moreover, most of the existing TRPCA methods do not
explicitly model the noises except the sparse noise, which may compromise the
accuracy of estimating the low-rank tensor. In this work, we propose a novel
high-order TRPCA method, named as Low-Multi-rank High-order Bayesian Robust
Tensor Factorization (LMH-BRTF), within the Bayesian framework. Specifically,
we decompose the observed corrupted tensor into three parts, i.e., the low-rank
component, the sparse component, and the noise component. By constructing a
low-rank model for the low-rank component based on the order-$d$ t-SVD and
introducing a proper prior for the model, LMH-BRTF can automatically determine
the tensor multi-rank. Meanwhile, benefiting from the explicit modeling of both
the sparse and noise components, the proposed method can leverage information
from the noises more effectivly, leading to an improved performance of TRPCA.
Then, an efficient variational inference algorithm is established for
parameters estimation. Empirical studies on synthetic and real-world datasets
demonstrate the effectiveness of the proposed method in terms of both
qualitative and quantitative results.",2311.05888v1,https://arxiv.org/pdf/2311.05888v1
Outlier-Robust Wasserstein DRO,"Sloan Nietert, Ziv Goldfeld, Soroosh Shafiee","Distributionally robust optimization (DRO) is an effective approach for
data-driven decision-making in the presence of uncertainty. Geometric
uncertainty due to sampling or localized perturbations of data points is
captured by Wasserstein DRO (WDRO), which seeks to learn a model that performs
uniformly well over a Wasserstein ball centered around the observed data
distribution. However, WDRO fails to account for non-geometric perturbations
such as adversarial outliers, which can greatly distort the Wasserstein
distance measurement and impede the learned model. We address this gap by
proposing a novel outlier-robust WDRO framework for decision-making under both
geometric (Wasserstein) perturbations and non-geometric (total variation (TV))
contamination that allows an $\varepsilon$-fraction of data to be arbitrarily
corrupted. We design an uncertainty set using a certain robust Wasserstein ball
that accounts for both perturbation types and derive minimax optimal excess
risk bounds for this procedure that explicitly capture the Wasserstein and TV
risks. We prove a strong duality result that enables tractable convex
reformulations and efficient computation of our outlier-robust WDRO problem.
When the loss function depends only on low-dimensional features of the data, we
eliminate certain dimension dependencies from the risk bounds that are
unavoidable in the general setting. Finally, we present experiments validating
our theory on standard regression and classification tasks.",2311.05573v1,https://arxiv.org/pdf/2311.05573v1
"Training Robust Deep Physiological Measurement Models with Synthetic
  Video-based Data","Yuxuan Ou, Yuzhe Zhang, Yuntang Wang, Shwetak Patel, Daniel McDuf, Yuzhe Yang, Xin Liu","Recent advances in supervised deep learning techniques have demonstrated the
possibility to remotely measure human physiological vital signs (e.g.,
photoplethysmograph, heart rate) just from facial videos. However, the
performance of these methods heavily relies on the availability and diversity
of real labeled data. Yet, collecting large-scale real-world data with
high-quality labels is typically challenging and resource intensive, which also
raises privacy concerns when storing personal bio-metric data. Synthetic
video-based datasets (e.g., SCAMPS \cite{mcduff2022scamps}) with
photo-realistic synthesized avatars are introduced to alleviate the issues
while providing high-quality synthetic data. However, there exists a
significant gap between synthetic and real-world data, which hinders the
generalization of neural models trained on these synthetic datasets. In this
paper, we proposed several measures to add real-world noise to synthetic
physiological signals and corresponding facial videos. We experimented with
individual and combined augmentation methods and evaluated our framework on
three public real-world datasets. Our results show that we were able to reduce
the average MAE from 6.9 to 2.0.",2311.05371v2,https://arxiv.org/pdf/2311.05371v2
"Frontier Language Models are not Robust to Adversarial Arithmetic, or
  ""What do I need to say so you agree 2+2=5?","C. Daniel Freeman, Laura Culp, Aaron Parisi, Maxwell L Bileschi, Gamaleldin F Elsayed, Alex Rizkowsky, Isabelle Simpson, Alex Alemi, Azade Nova, Ben Adlam, Bernd Bohnet, Gaurav Mishra, Hanie Sedghi, Igor Mordatch, Izzeddin Gur, Jaehoon Lee, JD Co-Reyes, Jeffrey Pennington, Kelvin Xu, Kevin Swersky, Kshiteej Mahajan, Lechao Xiao, Rosanne Liu, Simon Kornblith, Noah Constant, Peter J. Liu, Roman Novak, Yundi Qian, Noah Fiedel, Jascha Sohl-Dickstein","We introduce and study the problem of adversarial arithmetic, which provides
a simple yet challenging testbed for language model alignment. This problem is
comprised of arithmetic questions posed in natural language, with an arbitrary
adversarial string inserted before the question is complete. Even in the simple
setting of 1-digit addition problems, it is easy to find adversarial prompts
that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and
even to steer models to a particular wrong answer. We additionally provide a
simple algorithm for finding successful attacks by querying those same models,
which we name ""prompt inversion rejection sampling"" (PIRS). We finally show
that models can be partially hardened against these attacks via reinforcement
learning and via agentic constitutional loops. However, we were not able to
make a language model fully robust against adversarial arithmetic attacks.",2311.07587v2,https://arxiv.org/pdf/2311.07587v2
Identifying Semantic Component for Robust Molecular Property Prediction,"Zijian Li, Zunhong Xu, Ruichu Cai, Zhenhui Yang, Yuguang Yan, Zhifeng Hao, Guangyi Chen, Kun Zhang","Although graph neural networks have achieved great success in the task of
molecular property prediction in recent years, their generalization ability
under out-of-distribution (OOD) settings is still under-explored. Different
from existing methods that learn discriminative representations for prediction,
we propose a generative model with semantic-components identifiability, named
SCI. We demonstrate that the latent variables in this generative model can be
explicitly identified into semantic-relevant (SR) and semantic-irrelevant (SI)
components, which contributes to better OOD generalization by involving minimal
change properties of causal mechanisms. Specifically, we first formulate the
data generation process from the atom level to the molecular level, where the
latent space is split into SI substructures, SR substructures, and SR atom
variables. Sequentially, to reduce misidentification, we restrict the minimal
changes of the SR atom variables and add a semantic latent substructure
regularization to mitigate the variance of the SR substructure under augmented
domain changes. Under mild assumptions, we prove the block-wise identifiability
of the SR substructure and the comment-wise identifiability of SR atom
variables. Experimental studies achieve state-of-the-art performance and show
general improvement on 21 datasets in 3 mainstream benchmarks. Moreover, the
visualization results of the proposed SCI method provide insightful case
studies and explanations for the prediction results. The code is available at:
https://github.com/DMIRLAB-Group/SCI.",2311.04837v1,https://arxiv.org/pdf/2311.04837v1
"FetMRQC: a robust quality control system for multi-centric fetal brain
  MRI","Thomas Sanchez, Oscar Esteban, Yvan Gomez, Alexandre Pron, Mériam Koob, Vincent Dunet, Nadine Girard, Andras Jakab, Elisenda Eixarch, Guillaume Auzias, Meritxell Bach Cuadra","Fetal brain MRI is becoming an increasingly relevant complement to
neurosonography for perinatal diagnosis, allowing fundamental insights into
fetal brain development throughout gestation. However, uncontrolled fetal
motion and heterogeneity in acquisition protocols lead to data of variable
quality, potentially biasing the outcome of subsequent studies. We present
FetMRQC, an open-source machine-learning framework for automated image quality
assessment and quality control that is robust to domain shifts induced by the
heterogeneity of clinical data. FetMRQC extracts an ensemble of quality metrics
from unprocessed anatomical MRI and combines them to predict experts' ratings
using random forests. We validate our framework on a pioneeringly large and
diverse dataset of more than 1600 manually rated fetal brain T2-weighted images
from four clinical centers and 13 different scanners. Our study shows that
FetMRQC's predictions generalize well to unseen data while being interpretable.
FetMRQC is a step towards more robust fetal brain neuroimaging, which has the
potential to shed new insights on the developing human brain.",2311.04780v2,https://arxiv.org/pdf/2311.04780v2
Robust Best-arm Identification in Linear Bandits,"Wei Wang, Sattar Vakili, Ilija Bogunovic","We study the robust best-arm identification problem (RBAI) in the case of
linear rewards. The primary objective is to identify a near-optimal robust arm,
which involves selecting arms at every round and assessing their robustness by
exploring potential adversarial actions. This approach is particularly relevant
when utilizing a simulator and seeking to identify a robust solution for
real-world transfer. To this end, we present an instance-dependent lower bound
for the robust best-arm identification problem with linear rewards.
Furthermore, we propose both static and adaptive bandit algorithms that achieve
sample complexity that matches the lower bound. In synthetic experiments, our
algorithms effectively identify the best robust arm and perform similarly to
the oracle strategy. As an application, we examine diabetes care and the
process of learning insulin dose recommendations that are robust with respect
to inaccuracies in standard calculators. Our algorithms prove to be effective
in identifying robust dosage values across various age ranges of patients.",2311.04731v1,https://arxiv.org/pdf/2311.04731v1
"Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust
  Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation","Ha-Yeong Choi, Sang-Hoon Lee, Seong-Whan Lee","Although voice conversion (VC) systems have shown a remarkable ability to
transfer voice style, existing methods still have an inaccurate pitch and low
speaker adaptation quality. To address these challenges, we introduce
Diff-HierVC, a hierarchical VC system based on two diffusion models. We first
introduce DiffPitch, which can effectively generate F0 with the target voice
style. Subsequently, the generated F0 is fed to DiffVoice to convert the speech
with a target voice style. Furthermore, using the source-filter encoder, we
disentangle the speech and use the converted Mel-spectrogram as a data-driven
prior in DiffVoice to improve the voice style transfer capacity. Finally, by
using the masked prior in diffusion models, our model can improve the speaker
adaptation quality. Experimental results verify the superiority of our model in
pitch generation and voice style transfer performance, and our model also
achieves a CER of 0.83% and EER of 3.29% in zero-shot VC scenarios.",2311.04693v1,https://arxiv.org/pdf/2311.04693v1
"Robust and Communication-Efficient Federated Domain Adaptation via
  Random Features","Zhanbo Feng, Yuanjie Wang, Jie Li, Fan Yang, Jiong Lou, Tiebin Mi, Robert. C. Qiu, Zhenyu Liao","Modern machine learning (ML) models have grown to a scale where training them
on a single machine becomes impractical. As a result, there is a growing trend
to leverage federated learning (FL) techniques to train large ML models in a
distributed and collaborative manner. These models, however, when deployed on
new devices, might struggle to generalize well due to domain shifts. In this
context, federated domain adaptation (FDA) emerges as a powerful approach to
address this challenge.
  Most existing FDA approaches typically focus on aligning the distributions
between source and target domains by minimizing their (e.g., MMD) distance.
Such strategies, however, inevitably introduce high communication overheads and
can be highly sensitive to network reliability.
  In this paper, we introduce RF-TCA, an enhancement to the standard Transfer
Component Analysis approach that significantly accelerates computation without
compromising theoretical and empirical performance. Leveraging the
computational advantage of RF-TCA, we further extend it to FDA setting with
FedRF-TCA. The proposed FedRF-TCA protocol boasts communication complexity that
is \emph{independent} of the sample size, while maintaining performance that is
either comparable to or even surpasses state-of-the-art FDA methods. We present
extensive experiments to showcase the superior performance and robustness (to
network condition) of FedRF-TCA.",2311.04686v1,https://arxiv.org/pdf/2311.04686v1
"Constrained Adaptive Attacks: Realistic Evaluation of Adversarial
  Examples and Robust Training of Deep Neural Networks for Tabular Data","Thibault Simonetto, Salah Ghamizi, Antoine Desjardins, Maxime Cordy, Yves Le Traon","State-of-the-art deep learning models for tabular data have recently achieved
acceptable performance to be deployed in industrial settings. However, the
robustness of these models remains scarcely explored. Contrary to computer
vision, there is to date no realistic protocol to properly evaluate the
adversarial robustness of deep tabular models due to intrinsic properties of
tabular data such as categorical features, immutability, and feature
relationship constraints. To fill this gap, we propose CAA, the first efficient
evasion attack for constrained tabular deep learning models. CAA is an
iterative parameter-free attack that combines gradient and search attacks to
generate adversarial examples under constraints. We leverage CAA to build a
benchmark of deep tabular models across three popular use cases: credit
scoring, phishing and botnet attacks detection. Our benchmark supports ten
threat models with increasing capabilities of the attacker, and reflects
real-world attack scenarios for each use case. Overall, our results demonstrate
how domain knowledge, adversarial training, and attack budgets impact the
robustness assessment of deep tabular models and provide security practitioners
with a set of recommendations to improve the robustness of deep tabular models
against various evasion attack scenarios.",2311.04503v1,https://arxiv.org/pdf/2311.04503v1
"Estimator-Coupled Reinforcement Learning for Robust Purely Tactile
  In-Hand Manipulation","Lennart Röstel, Johannes Pitz, Leon Sievers, Berthold Bäuml","This paper identifies and addresses the problems with naively combining
(reinforcement) learning-based controllers and state estimators for robotic
in-hand manipulation. Specifically, we tackle the challenging task of purely
tactile, goal-conditioned, dextrous in-hand reorientation with the hand
pointing downwards. Due to the limited sensing available, many control
strategies that are feasible in simulation when having full knowledge of the
object's state do not allow for accurate state estimation. Hence, separately
training the controller and the estimator and combining the two at test time
leads to poor performance. We solve this problem by coupling the control policy
to the state estimator already during training in simulation. This approach
leads to more robust state estimation and overall higher performance on the
task while maintaining an interpretability advantage over end-to-end policy
learning. With our GPU-accelerated implementation, learning from scratch takes
a median training time of only 6.5 hours on a single, low-cost GPU. In
simulation experiments with the DLR-Hand II and for four significantly
different object shapes, we provide an in-depth analysis of the performance of
our approach. We demonstrate the successful sim2real transfer by rotating the
four objects to all 24 orientations in the $\pi/2$ discretization of SO(3),
which has never been achieved for such a diverse set of shapes. Finally, our
method can reorient a cube consecutively to nine goals (median), which was
beyond the reach of previous methods in this challenging setting.",2311.04060v1,https://arxiv.org/pdf/2311.04060v1
OmniVec: Learning robust representations with cross modal sharing,"Siddharth Srivastava, Gaurav Sharma","Majority of research in learning based methods has been towards designing and
training networks for specific tasks. However, many of the learning based
tasks, across modalities, share commonalities and could be potentially tackled
in a joint framework. We present an approach in such direction, to learn
multiple tasks, in multiple modalities, with a unified architecture. The
proposed network is composed of task specific encoders, a common trunk in the
middle, followed by task specific prediction heads. We first pre-train it by
self-supervised masked training, followed by sequential training for the
different tasks. We train the network on all major modalities, e.g.\ visual,
audio, text and 3D, and report results on $22$ diverse and challenging public
benchmarks. We demonstrate empirically that, using a joint network to train
across modalities leads to meaningful information sharing and this allows us to
achieve state-of-the-art results on most of the benchmarks. We also show
generalization of the trained network on cross-modal tasks as well as unseen
datasets and tasks.",2311.05709v1,https://arxiv.org/pdf/2311.05709v1
"DRAUC: An Instance-wise Distributionally Robust AUC Optimization
  Framework","Siran Dai, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, Qingming Huang","The Area Under the ROC Curve (AUC) is a widely employed metric in long-tailed
classification scenarios. Nevertheless, most existing methods primarily assume
that training and testing examples are drawn i.i.d. from the same distribution,
which is often unachievable in practice. Distributionally Robust Optimization
(DRO) enhances model performance by optimizing it for the local worst-case
scenario, but directly integrating AUC optimization with DRO results in an
intractable optimization problem. To tackle this challenge, methodically we
propose an instance-wise surrogate loss of Distributionally Robust AUC (DRAUC)
and build our optimization framework on top of it. Moreover, we highlight that
conventional DRAUC may induce label bias, hence introducing distribution-aware
DRAUC as a more suitable metric for robust AUC learning. Theoretically, we
affirm that the generalization gap between the training loss and testing error
diminishes if the training set is sufficiently large. Empirically, experiments
on corrupted benchmark datasets demonstrate the effectiveness of our proposed
method. Code is available at: https://github.com/EldercatSAM/DRAUC.",2311.03055v1,https://arxiv.org/pdf/2311.03055v1
"Robust Generalization Strategies for Morpheme Glossing in an Endangered
  Language Documentation Context","Michael Ginn, Alexis Palmer","Generalization is of particular importance in resource-constrained settings,
where the available training data may represent only a small fraction of the
distribution of possible texts. We investigate the ability of morpheme labeling
models to generalize by evaluating their performance on unseen genres of text,
and we experiment with strategies for closing the gap between performance on
in-distribution and out-of-distribution data. Specifically, we use weight decay
optimization, output denoising, and iterative pseudo-labeling, and achieve a 2%
improvement on a test set containing texts from unseen genres. All experiments
are performed using texts written in the Mayan language Uspanteko.",2311.02777v1,https://arxiv.org/pdf/2311.02777v1
Robust Fine-Tuning of Vision-Language Models for Domain Generalization,"Kevin Vogt-Lowell, Noah Lee, Theodoros Tsiligkaridis, Marc Vaillant","Transfer learning enables the sharing of common knowledge among models for a
variety of downstream tasks, but traditional methods suffer in limited training
data settings and produce narrow models incapable of effectively generalizing
under distribution shifts. Foundation models have recently demonstrated
impressive zero-shot inference capabilities and robustness under distribution
shifts. However, zero-shot evaluation for these models has been predominantly
confined to benchmarks with simple distribution shifts, limiting our
understanding of their effectiveness under the more realistic shifts found in
practice. Moreover, common fine-tuning methods for these models have yet to be
evaluated against vision models in few-shot scenarios where training data is
limited. To address these gaps, we present a new recipe for few-shot
fine-tuning of the popular vision-language foundation model CLIP and evaluate
its performance on challenging benchmark datasets with realistic distribution
shifts from the WILDS collection. Our experimentation demonstrates that, while
zero-shot CLIP fails to match performance of trained vision models on more
complex benchmarks, few-shot CLIP fine-tuning outperforms its vision-only
counterparts in terms of in-distribution and out-of-distribution accuracy at
all levels of training data availability. This provides a strong incentive for
adoption of foundation models within few-shot learning applications operating
with real-world data. Code is available at
https://github.com/mit-ll/robust-vision-language-finetuning",2311.02236v1,https://arxiv.org/pdf/2311.02236v1
"What Makes Pre-Trained Visual Representations Successful for Robust
  Manipulation?","Kaylee Burns, Zach Witzel, Jubayer Ibn Hamid, Tianhe Yu, Chelsea Finn, Karol Hausman","Inspired by the success of transfer learning in computer vision, roboticists
have investigated visual pre-training as a means to improve the learning
efficiency and generalization ability of policies learned from pixels. To that
end, past work has favored large object interaction datasets, such as
first-person videos of humans completing diverse tasks, in pursuit of
manipulation-relevant features. Although this approach improves the efficiency
of policy learning, it remains unclear how reliable these representations are
in the presence of distribution shifts that arise commonly in robotic
applications. Surprisingly, we find that visual representations designed for
manipulation and control tasks do not necessarily generalize under subtle
changes in lighting and scene texture or the introduction of distractor
objects. To understand what properties do lead to robust representations, we
compare the performance of 15 pre-trained vision models under different visual
appearances. We find that emergent segmentation ability is a strong predictor
of out-of-distribution generalization among ViT models. The rank order induced
by this metric is more predictive than metrics that have previously guided
generalization research within computer vision and machine learning, such as
downstream ImageNet accuracy, in-domain accuracy, or shape-bias as evaluated by
cue-conflict performance. We test this finding extensively on a suite of
distribution shifts in ten tasks across two simulated manipulation
environments. On the ALOHA setup, segmentation score predicts real-world
performance after offline training with 50 demonstrations.",2312.12444v1,https://arxiv.org/pdf/2312.12444v1
"Obtaining Explainable Classification Models using Distributionally
  Robust Optimization","Sanjeeb Dash, Soumyadip Ghosh, Joao Goncalves, Mark S. Squillante","Model explainability is crucial for human users to be able to interpret how a
proposed classifier assigns labels to data based on its feature values. We
study generalized linear models constructed using sets of feature value rules,
which can capture nonlinear dependencies and interactions. An inherent
trade-off exists between rule set sparsity and its prediction accuracy. It is
computationally expensive to find the right choice of sparsity -- e.g., via
cross-validation -- with existing methods. We propose a new formulation to
learn an ensemble of rule sets that simultaneously addresses these competing
factors. Good generalization is ensured while keeping computational costs low
by utilizing distributionally robust optimization. The formulation utilizes
column generation to efficiently search the space of rule sets and constructs a
sparse ensemble of rule sets, in contrast with techniques like random forests
or boosting and their variants. We present theoretical results that motivate
and justify the use of our distributionally robust formulation. Extensive
numerical experiments establish that our method improves over competing methods
-- on a large set of publicly available binary classification problem instances
-- with respect to one or more of the following metrics: generalization
quality, computational cost, and explainability.",2311.01994v1,https://arxiv.org/pdf/2311.01994v1
Towards Calibrated Robust Fine-Tuning of Vision-Language Models,"Changdae Oh, Hyesu Lim, Mijoo Kim, Dongyoon Han, Sangdoo Yun, Jaegul Choo, Alexander Hauptmann, Zhi-Qi Cheng, Kyungwoo Song","Improving out-of-distribution (OOD) generalization through in-distribution
(ID) adaptation is a primary goal of robust fine-tuning methods beyond the
naive fine-tuning approach. However, despite decent OOD generalization
performance from recent robust fine-tuning methods, OOD confidence calibration
for reliable machine learning has not been fully addressed. This work proposes
a robust fine-tuning method that improves both OOD accuracy and calibration
error in Vision Language Models (VLMs). Firstly, we show that both types of
errors have a shared upper bound consisting of two terms of ID data: 1)
calibration error and 2) the smallest singular value of the input covariance
matrix. Based on this insight, we design a novel framework that conducts
fine-tuning with a constrained multimodal contrastive loss enforcing a larger
smallest singular value, which is further aided by the self-distillation of a
moving averaged model to achieve well-calibrated prediction. Starting from an
empirical validation of our theoretical statements, we provide extensive
experimental results on ImageNet distribution shift benchmarks that demonstrate
the effectiveness of our method.",2311.01723v5,https://arxiv.org/pdf/2311.01723v5
"Detecting Spurious Correlations via Robust Visual Concepts in Real and
  AI-Generated Image Classification","Preetam Prabhu Srikar Dammu, Chirag Shah","Often machine learning models tend to automatically learn associations
present in the training data without questioning their validity or
appropriateness. This undesirable property is the root cause of the
manifestation of spurious correlations, which render models unreliable and
prone to failure in the presence of distribution shifts. Research shows that
most methods attempting to remedy spurious correlations are only effective for
a model's known spurious associations. Current spurious correlation detection
algorithms either rely on extensive human annotations or are too restrictive in
their formulation. Moreover, they rely on strict definitions of visual
artifacts that may not apply to data produced by generative models, as they are
known to hallucinate contents that do not conform to standard specifications.
In this work, we introduce a general-purpose method that efficiently detects
potential spurious correlations, and requires significantly less human
interference in comparison to the prior art. Additionally, the proposed method
provides intuitive explanations while eliminating the need for pixel-level
annotations. We demonstrate the proposed method's tolerance to the peculiarity
of AI-generated images, which is a considerably challenging task, one where
most of the existing methods fall short. Consequently, our method is also
suitable for detecting spurious correlations that may propagate to downstream
applications originating from generative models.",2311.01655v2,https://arxiv.org/pdf/2311.01655v2
"Robust Adversarial Reinforcement Learning via Bounded Rationality
  Curricula","Aryaman Reddi, Maximilian Tölle, Jan Peters, Georgia Chalvatzaki, Carlo D'Eramo","Robustness against adversarial attacks and distribution shifts is a
long-standing goal of Reinforcement Learning (RL). To this end, Robust
Adversarial Reinforcement Learning (RARL) trains a protagonist against
destabilizing forces exercised by an adversary in a competitive zero-sum Markov
game, whose optimal solution, i.e., rational strategy, corresponds to a Nash
equilibrium. However, finding Nash equilibria requires facing complex saddle
point optimization problems, which can be prohibitive to solve, especially for
high-dimensional control. In this paper, we propose a novel approach for
adversarial RL based on entropy regularization to ease the complexity of the
saddle point optimization problem. We show that the solution of this
entropy-regularized problem corresponds to a Quantal Response Equilibrium
(QRE), a generalization of Nash equilibria that accounts for bounded
rationality, i.e., agents sometimes play random actions instead of optimal
ones. Crucially, the connection between the entropy-regularized objective and
QRE enables free modulation of the rationality of the agents by simply tuning
the temperature coefficient. We leverage this insight to propose our novel
algorithm, Quantal Adversarial RL (QARL), which gradually increases the
rationality of the adversary in a curriculum fashion until it is fully
rational, easing the complexity of the optimization problem while retaining
robustness. We provide extensive evidence of QARL outperforming RARL and recent
baselines across several MuJoCo locomotion and navigation problems in overall
performance and robustness.",2311.01642v1,https://arxiv.org/pdf/2311.01642v1
"E(2) Equivariant Neural Networks for Robust Galaxy Morphology
  Classification","Sneh Pandya, Purvik Patel, Franc O, Jonathan Blazek","We propose the use of group convolutional neural network architectures
(GCNNs) equivariant to the 2D Euclidean group, $E(2)$, for the task of galaxy
morphology classification by utilizing symmetries of the data present in galaxy
images as an inductive bias in the architecture. We conduct robustness studies
by introducing artificial perturbations via Poisson noise insertion and
one-pixel adversarial attacks to simulate the effects of limited observational
capabilities. We train, validate, and test GCNNs equivariant to discrete
subgroups of $E(2)$ - the cyclic and dihedral groups of order $N$ - on the
Galaxy10 DECals dataset and find that GCNNs achieve higher classification
accuracy and are consistently more robust than their non-equivariant
counterparts, with an architecture equivariant to the group $D_{16}$ achieving
a $95.52 \pm 0.18\%$ test-set accuracy. We also find that the model loses
$<6\%$ accuracy on a $50\%$-noise dataset and all GCNNs are less susceptible to
one-pixel perturbations than an identically constructed CNN. Our code is
publicly available at https://github.com/snehjp2/GCNNMorphology.",2311.01500v1,https://arxiv.org/pdf/2311.01500v1
"CADSim: Robust and Scalable in-the-wild 3D Reconstruction for
  Controllable Sensor Simulation","Jingkang Wang, Sivabalan Manivasagam, Yun Chen, Ze Yang, Ioan Andrei Bârsan, Anqi Joyce Yang, Wei-Chiu Ma, Raquel Urtasun","Realistic simulation is key to enabling safe and scalable development of %
self-driving vehicles. A core component is simulating the sensors so that the
entire autonomy system can be tested in simulation. Sensor simulation involves
modeling traffic participants, such as vehicles, with high quality appearance
and articulated geometry, and rendering them in real time. The self-driving
industry has typically employed artists to build these assets. However, this is
expensive, slow, and may not reflect reality. Instead, reconstructing assets
automatically from sensor data collected in the wild would provide a better
path to generating a diverse and large set with good real-world coverage.
Nevertheless, current reconstruction approaches struggle on in-the-wild sensor
data, due to its sparsity and noise. To tackle these issues, we present CADSim,
which combines part-aware object-class priors via a small set of CAD models
with differentiable rendering to automatically reconstruct vehicle geometry,
including articulated wheels, with high-quality appearance. Our experiments
show our method recovers more accurate shapes from sparse data compared to
existing approaches. Importantly, it also trains and renders efficiently. We
demonstrate our reconstructed vehicles in several applications, including
accurate testing of autonomy perception systems.",2311.01447v1,https://arxiv.org/pdf/2311.01447v1
"Distilling Out-of-Distribution Robustness from Vision-Language
  Foundation Models","Andy Zhou, Jindong Wang, Yu-Xiong Wang, Haohan Wang","We propose a conceptually simple and lightweight framework for improving the
robustness of vision models through the combination of knowledge distillation
and data augmentation. We address the conjecture that larger models do not make
for better teachers by showing strong gains in out-of-distribution robustness
when distilling from pretrained foundation models. Following this finding, we
propose Discrete Adversarial Distillation (DAD), which leverages a robust
teacher to generate adversarial examples and a VQGAN to discretize them,
creating more informative samples than standard data augmentation techniques.
We provide a theoretical framework for the use of a robust teacher in the
knowledge distillation with data augmentation setting and demonstrate strong
gains in out-of-distribution robustness and clean accuracy across different
student architectures. Notably, our method adds minor computational overhead
compared to similar techniques and can be easily combined with other data
augmentations for further improvements.",2311.01441v2,https://arxiv.org/pdf/2311.01441v2
Combating Bilateral Edge Noise for Robust Link Prediction,"Zhanke Zhou, Jiangchao Yao, Jiaxu Liu, Xiawei Guo, Quanming Yao, Li He, Liang Wang, Bo Zheng, Bo Han","Although link prediction on graphs has achieved great success with the
development of graph neural networks (GNNs), the potential robustness under the
edge noise is still less investigated. To close this gap, we first conduct an
empirical study to disclose that the edge noise bilaterally perturbs both input
topology and target label, yielding severe performance degradation and
representation collapse. To address this dilemma, we propose an
information-theory-guided principle, Robust Graph Information Bottleneck
(RGIB), to extract reliable supervision signals and avoid representation
collapse. Different from the basic information bottleneck, RGIB further
decouples and balances the mutual dependence among graph topology, target
labels, and representation, building new learning objectives for robust
representation against the bilateral noise. Two instantiations, RGIB-SSL and
RGIB-REP, are explored to leverage the merits of different methodologies, i.e.,
self-supervised learning and data reparameterization, for implicit and explicit
data denoising, respectively. Extensive experiments on six datasets and three
GNNs with diverse noisy scenarios verify the effectiveness of our RGIB
instantiations. The code is publicly available at:
https://github.com/tmlr-group/RGIB.",2311.01196v1,https://arxiv.org/pdf/2311.01196v1
"Improving Robustness via Tilted Exponential Layer: A
  Communication-Theoretic Perspective","Bhagyashree Puranik, Ahmad Beirami, Yao Qin, Upamanyu Madhow","State-of-the-art techniques for enhancing robustness of deep networks mostly
rely on empirical risk minimization with suitable data augmentation. In this
paper, we propose a complementary approach motivated by communication theory,
aimed at enhancing the signal-to-noise ratio at the output of a neural network
layer via neural competition during learning and inference. In addition to
standard empirical risk minimization, neurons compete to sparsely represent
layer inputs by maximization of a tilted exponential (TEXP) objective function
for the layer. TEXP learning can be interpreted as maximum likelihood
estimation of matched filters under a Gaussian model for data noise. Inference
in a TEXP layer is accomplished by replacing batch norm by a tilted softmax,
which can be interpreted as computation of posterior probabilities for the
competing signaling hypotheses represented by each neuron. After providing
insights via simplified models, we show, by experimentation on standard image
datasets, that TEXP learning and inference enhances robustness against noise
and other common corruptions, without requiring data augmentation. Further
cumulative gains in robustness against this array of distortions can be
obtained by appropriately combining TEXP with data augmentation techniques. The
code for all our experiments is available at
https://github.com/bhagyapuranik/texp_for_robustness.",2311.01047v3,https://arxiv.org/pdf/2311.01047v3
"Robust Data Pruning under Label Noise via Maximizing Re-labeling
  Accuracy","Dongmin Park, Seola Choi, Doyoung Kim, Hwanjun Song, Jae-Gil Lee","Data pruning, which aims to downsize a large training set into a small
informative subset, is crucial for reducing the enormous computational costs of
modern deep learning. Though large-scale data collections invariably contain
annotation noise and numerous robust learning methods have been developed, data
pruning for the noise-robust learning scenario has received little attention.
With state-of-the-art Re-labeling methods that self-correct erroneous labels
while training, it is challenging to identify which subset induces the most
accurate re-labeling of erroneous labels in the entire training set. In this
paper, we formalize the problem of data pruning with re-labeling. We first show
that the likelihood of a training example being correctly re-labeled is
proportional to the prediction confidence of its neighborhood in the subset.
Therefore, we propose a novel data pruning algorithm, Prune4Rel, that finds a
subset maximizing the total neighborhood confidence of all training examples,
thereby maximizing the re-labeling accuracy and generalization performance.
Extensive experiments on four real and one synthetic noisy datasets show that
\algname{} outperforms the baselines with Re-labeling models by up to 9.1% as
well as those with a standard model by up to 21.6%.",2311.01002v1,https://arxiv.org/pdf/2311.01002v1
"A Robust Deep Learning Method with Uncertainty Estimation for the
  Pathological Classification of Renal Cell Carcinoma based on CT Images","Ni Yao, Hang Hu, Kaicong Chen, Chen Zhao, Yuan Guo, Boya Li, Jiaofen Nan, Yanting Li, Chuang Han, Fubao Zhu, Weihua Zhou, Li Tian","Objectives To develop and validate a deep learning-based diagnostic model
incorporating uncertainty estimation so as to facilitate radiologists in the
preoperative differentiation of the pathological subtypes of renal cell
carcinoma (RCC) based on CT images. Methods Data from 668 consecutive patients,
pathologically proven RCC, were retrospectively collected from Center 1. By
using five-fold cross-validation, a deep learning model incorporating
uncertainty estimation was developed to classify RCC subtypes into clear cell
RCC (ccRCC), papillary RCC (pRCC), and chromophobe RCC (chRCC). An external
validation set of 78 patients from Center 2 further evaluated the model's
performance. Results In the five-fold cross-validation, the model's area under
the receiver operating characteristic curve (AUC) for the classification of
ccRCC, pRCC, and chRCC was 0.868 (95% CI: 0.826-0.923), 0.846 (95% CI:
0.812-0.886), and 0.839 (95% CI: 0.802-0.88), respectively. In the external
validation set, the AUCs were 0.856 (95% CI: 0.838-0.882), 0.787 (95% CI:
0.757-0.818), and 0.793 (95% CI: 0.758-0.831) for ccRCC, pRCC, and chRCC,
respectively. Conclusions The developed deep learning model demonstrated robust
performance in predicting the pathological subtypes of RCC, while the
incorporated uncertainty emphasized the importance of understanding model
confidence, which is crucial for assisting clinical decision-making for
patients with renal tumors. Clinical relevance statement Our deep learning
approach, integrated with uncertainty estimation, offers clinicians a dual
advantage: accurate RCC subtype predictions complemented by diagnostic
confidence references, promoting informed decision-making for patients with
RCC.",2311.00567v2,https://arxiv.org/pdf/2311.00567v2
"Bayes-enhanced Multi-view Attention Networks for Robust POI
  Recommendation","Jiangnan Xia, Yu Yang, Senzhang Wang, Hongzhi Yin, Jiannong Cao, Philip S. Yu","POI recommendation is practically important to facilitate various
Location-Based Social Network services, and has attracted rising research
attention recently. Existing works generally assume the available POI check-ins
reported by users are the ground-truth depiction of user behaviors. However, in
real application scenarios, the check-in data can be rather unreliable due to
both subjective and objective causes including positioning error and user
privacy concerns, leading to significant negative impacts on the performance of
the POI recommendation. To this end, we investigate a novel problem of robust
POI recommendation by considering the uncertainty factors of the user
check-ins, and proposes a Bayes-enhanced Multi-view Attention Network.
Specifically, we construct personal POI transition graph, the semantic-based
POI graph and distance-based POI graph to comprehensively model the
dependencies among the POIs. As the personal POI transition graph is usually
sparse and sensitive to noise, we design a Bayes-enhanced spatial dependency
learning module for data augmentation from the local view. A Bayesian posterior
guided graph augmentation approach is adopted to generate a new graph with
collaborative signals to increase the data diversity. Then both the original
and the augmented graphs are used for POI representation learning to counteract
the data uncertainty issue. Next, the POI representations of the three view
graphs are input into the proposed multi-view attention-based user preference
learning module. By incorporating the semantic and distance correlations of
POIs, the user preference can be effectively refined and finally robust
recommendation results are achieved. The results of extensive experiments show
that BayMAN significantly outperforms the state-of-the-art methods in POI
recommendation when the available check-ins are incomplete and noisy.",2311.00491v1,https://arxiv.org/pdf/2311.00491v1
Group Distributionally Robust Knowledge Distillation,"Konstantinos Vilouras, Xiao Liu, Pedro Sanchez, Alison Q. O'Neil, Sotirios A. Tsaftaris","Knowledge distillation enables fast and effective transfer of features
learned from a bigger model to a smaller one. However, distillation objectives
are susceptible to sub-population shifts, a common scenario in medical imaging
analysis which refers to groups/domains of data that are underrepresented in
the training set. For instance, training models on health data acquired from
multiple scanners or hospitals can yield subpar performance for minority
groups. In this paper, inspired by distributionally robust optimization (DRO)
techniques, we address this shortcoming by proposing a group-aware distillation
loss. During optimization, a set of weights is updated based on the per-group
losses at a given iteration. This way, our method can dynamically focus on
groups that have low performance during training. We empirically validate our
method, GroupDistil on two benchmark datasets (natural images and cardiac MRIs)
and show consistent improvement in terms of worst-group accuracy.",2311.00476v1,https://arxiv.org/pdf/2311.00476v1
Robust and Conjugate Gaussian Process Regression,"Matias Altamirano, François-Xavier Briol, Jeremias Knoblauch","To enable closed form conditioning, a common assumption in Gaussian process
(GP) regression is independent and identically distributed Gaussian observation
noise. This strong and simplistic assumption is often violated in practice,
which leads to unreliable inferences and uncertainty quantification.
Unfortunately, existing methods for robustifying GPs break closed-form
conditioning, which makes them less attractive to practitioners and
significantly more computationally expensive. In this paper, we demonstrate how
to perform provably robust and conjugate Gaussian process (RCGP) regression at
virtually no additional cost using generalised Bayesian inference. RCGP is
particularly versatile as it enables exact conjugate closed form updates in all
settings where standard GPs admit them. To demonstrate its strong empirical
performance, we deploy RCGP for problems ranging from Bayesian optimisation to
sparse variational Gaussian processes.",2311.00463v2,https://arxiv.org/pdf/2311.00463v2
"Improving Robustness for Vision Transformer with a Simple Dynamic
  Scanning Augmentation","Shashank Kotyan, Danilo Vasconcellos Vargas","Vision Transformer (ViT) has demonstrated promising performance in computer
vision tasks, comparable to state-of-the-art neural networks. Yet, this new
type of deep neural network architecture is vulnerable to adversarial attacks
limiting its capabilities in terms of robustness. This article presents a novel
contribution aimed at further improving the accuracy and robustness of ViT,
particularly in the face of adversarial attacks. We propose an augmentation
technique called `Dynamic Scanning Augmentation' that leverages dynamic input
sequences to adaptively focus on different patches, thereby maintaining
performance and robustness. Our detailed investigations reveal that this
adaptability to the input sequence induces significant changes in the attention
mechanism of ViT, even for the same image. We introduce four variations of
Dynamic Scanning Augmentation, outperforming ViT in terms of both robustness to
adversarial attacks and accuracy against natural images, with one variant
showing comparable results. By integrating our augmentation technique, we
observe a substantial increase in ViT's robustness, improving it from $17\%$ to
$92\%$ measured across different types of adversarial attacks. These findings,
together with other comprehensive tests, indicate that Dynamic Scanning
Augmentation enhances accuracy and robustness by promoting a more adaptive type
of attention. In conclusion, this work contributes to the ongoing research on
Vision Transformers by introducing Dynamic Scanning Augmentation as a technique
for improving the accuracy and robustness of ViT. The observed results
highlight the potential of this approach in advancing computer vision tasks and
merit further exploration in future studies.",2311.00441v1,https://arxiv.org/pdf/2311.00441v1
"NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust
  Multi-Exit Neural Networks","Seokil Ham, Jungwuk Park, Dong-Jun Han, Jaekyun Moon","While multi-exit neural networks are regarded as a promising solution for
making efficient inference via early exits, combating adversarial attacks
remains a challenging problem. In multi-exit networks, due to the high
dependency among different submodels, an adversarial example targeting a
specific exit not only degrades the performance of the target exit but also
reduces the performance of all other exits concurrently. This makes multi-exit
networks highly vulnerable to simple adversarial attacks. In this paper, we
propose NEO-KD, a knowledge-distillation-based adversarial training strategy
that tackles this fundamental challenge based on two key contributions. NEO-KD
first resorts to neighbor knowledge distillation to guide the output of the
adversarial examples to tend to the ensemble outputs of neighbor exits of clean
data. NEO-KD also employs exit-wise orthogonal knowledge distillation for
reducing adversarial transferability across different submodels. The result is
a significantly improved robustness against adversarial attacks. Experimental
results on various datasets/models show that our method achieves the best
adversarial accuracy with reduced computation budgets, compared to the
baselines relying on existing adversarial training or knowledge distillation
techniques for multi-exit networks.",2311.00428v1,https://arxiv.org/pdf/2311.00428v1
"Adversarially Robust Distributed Count Tracking via Partial Differential
  Privacy","Zhongzheng Xiong, Xiaoyi Zhu, Zengfeng Huang","We study the distributed tracking model, also known as distributed functional
monitoring. This model involves $k$ sites each receiving a stream of items and
communicating with the central server. The server's task is to track a function
of all items received thus far continuously, with minimum communication cost.
For count tracking, it is known that there is a $\sqrt{k}$ gap in communication
between deterministic and randomized algorithms. However, existing randomized
algorithms assume an ""oblivious adversary"" who constructs the entire input
streams before the algorithm starts. Here we consider adaptive adversaries who
can choose new items based on previous answers from the algorithm.
Deterministic algorithms are trivially robust to adaptive adversaries, while
randomized ones may not. Therefore, we investigate whether the $\sqrt{k}$
advantage of randomized algorithms is from randomness itself or the oblivious
adversary assumption. We provide an affirmative answer to this question by
giving a robust algorithm with optimal communication. Existing robustification
techniques do not yield optimal bounds due to the inherent challenges of the
distributed nature of the problem. To address this, we extend the differential
privacy framework by introducing ""partial differential privacy"" and proving a
new generalization theorem. This theorem may have broader applications beyond
robust count tracking, making it of independent interest.",2311.00346v1,https://arxiv.org/pdf/2311.00346v1
Robust Graph Clustering via Meta Weighting for Noisy Graphs,"Hyeonsoo Jo, Fanchen Bu, Kijung Shin","How can we find meaningful clusters in a graph robustly against noise edges?
Graph clustering (i.e., dividing nodes into groups of similar ones) is a
fundamental problem in graph analysis with applications in various fields.
Recent studies have demonstrated that graph neural network (GNN) based
approaches yield promising results for graph clustering. However, we observe
that their performance degenerates significantly on graphs with noise edges,
which are prevalent in practice. In this work, we propose MetaGC for robust
GNN-based graph clustering. MetaGC employs a decomposable clustering loss
function, which can be rephrased as a sum of losses over node pairs. We add a
learnable weight to each node pair, and MetaGC adaptively adjusts the weights
of node pairs using meta-weighting so that the weights of meaningful node pairs
increase and the weights of less-meaningful ones (e.g., noise edges) decrease.
We show empirically that MetaGC learns weights as intended and consequently
outperforms the state-of-the-art GNN-based competitors, even when they are
equipped with separate denoising schemes, on five real-world graphs under
varying levels of noise. Our code and datasets are available at
https://github.com/HyeonsooJo/MetaGC.",2311.00322v2,https://arxiv.org/pdf/2311.00322v2
"Noisy Exemplars Make Large Language Models More Robust: A
  Domain-Agnostic Behavioral Analysis","Hongyi Zheng, Abulhair Saparov","Recent advances in prompt engineering enable large language models (LLMs) to
solve multi-hop logical reasoning problems with impressive accuracy. However,
there is little existing work investigating the robustness of LLMs with
few-shot prompting techniques. Therefore, we introduce a systematic approach to
test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic
perturbations. We include perturbations at multiple levels of abstractions
(e.g. lexical perturbations such as typos, and semantic perturbations such as
the inclusion of intermediate reasoning steps in the questions) to conduct
behavioral analysis on the LLMs. Throughout our experiments, we find that
models are more sensitive to certain perturbations such as replacing words with
their synonyms. We also demonstrate that increasing the proportion of perturbed
exemplars in the prompts improves the robustness of few-shot prompting methods.",2311.00258v1,https://arxiv.org/pdf/2311.00258v1
"Robust Safety Classifier for Large Language Models: Adversarial Prompt
  Shield","Jinhwa Kim, Ali Derakhshan, Ian G. Harris","Large Language Models' safety remains a critical concern due to their
vulnerability to adversarial attacks, which can prompt these systems to produce
harmful responses. In the heart of these systems lies a safety classifier, a
computational model trained to discern and mitigate potentially harmful,
offensive, or unethical outputs. However, contemporary safety classifiers,
despite their potential, often fail when exposed to inputs infused with
adversarial noise. In response, our study introduces the Adversarial Prompt
Shield (APS), a lightweight model that excels in detection accuracy and
demonstrates resilience against adversarial prompts. Additionally, we propose
novel strategies for autonomously generating adversarial training datasets,
named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are
designed to fortify the safety classifier's robustness, and we investigate the
consequences of incorporating adversarial examples into the training process.
Through evaluations involving Large Language Models, we demonstrate that our
classifier has the potential to decrease the attack success rate resulting from
adversarial attacks by up to 60%. This advancement paves the way for the next
generation of more reliable and resilient conversational agents.",2311.00172v1,https://arxiv.org/pdf/2311.00172v1
Bandit-Driven Batch Selection for Robust Learning under Label Noise,"Michal Lisicki, Mihai Nica, Graham W. Taylor","We introduce a novel approach for batch selection in Stochastic Gradient
Descent (SGD) training, leveraging combinatorial bandit algorithms. Our
methodology focuses on optimizing the learning process in the presence of label
noise, a prevalent issue in real-world datasets. Experimental evaluations on
the CIFAR-10 dataset reveal that our approach consistently outperforms existing
methods across various levels of label corruption. Importantly, we achieve this
superior performance without incurring the computational overhead commonly
associated with auxiliary neural network models. This work presents a balanced
trade-off between computational efficiency and model efficacy, offering a
scalable solution for complex machine learning applications.",2311.00096v1,https://arxiv.org/pdf/2311.00096v1
Dynamic Batch Norm Statistics Update for Natural Robustness,"Shahbaz Rezaei, Mohammad Sadegh Norouzzadeh","DNNs trained on natural clean samples have been shown to perform poorly on
corrupted samples, such as noisy or blurry images. Various data augmentation
methods have been recently proposed to improve DNN's robustness against common
corruptions. Despite their success, they require computationally expensive
training and cannot be applied to off-the-shelf trained models. Recently, it
has been shown that updating BatchNorm (BN) statistics of an off-the-shelf
model on a single corruption improves its accuracy on that corruption
significantly. However, adopting the idea at inference time when the type of
corruption is unknown and changing decreases the effectiveness of this method.
In this paper, we harness the Fourier domain to detect the corruption type, a
challenging task in the image domain. We propose a unified framework consisting
of a corruption-detection model and BN statistics update that improves the
corruption accuracy of any off-the-shelf trained model. We benchmark our
framework on different models and datasets. Our results demonstrate about 8%
and 4% accuracy improvement on CIFAR10-C and ImageNet-C, respectively.
Furthermore, our framework can further improve the accuracy of state-of-the-art
robust models, such as AugMix and DeepAug.",2310.20649v1,https://arxiv.org/pdf/2310.20649v1
"Online Conversion with Switching Costs: Robust and Learning-Augmented
  Algorithms","Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy","We introduce and study online conversion with switching costs, a family of
online problems that capture emerging problems at the intersection of energy
and sustainability. In this problem, an online player attempts to purchase
(alternatively, sell) fractional shares of an asset during a fixed time horizon
with length $T$. At each time step, a cost function (alternatively, price
function) is revealed, and the player must irrevocably decide an amount of
asset to convert. The player also incurs a switching cost whenever their
decision changes in consecutive time steps, i.e., when they increase or
decrease their purchasing amount. We introduce competitive (robust)
threshold-based algorithms for both the minimization and maximization variants
of this problem, and show they are optimal among deterministic online
algorithms. We then propose learning-augmented algorithms that take advantage
of untrusted black-box advice (such as predictions from a machine learning
model) to achieve significantly better average-case performance without
sacrificing worst-case competitive guarantees. Finally, we empirically evaluate
our proposed algorithms using a carbon-aware EV charging case study, showing
that our algorithms substantially improve on baseline methods for this problem.",2310.20598v2,https://arxiv.org/pdf/2310.20598v2
"Assessing and Enhancing Robustness of Deep Learning Models with
  Corruption Emulation in Digital Pathology","Peixiang Huang, Songtao Zhang, Yulu Gan, Rui Xu, Rongqi Zhu, Wenkang Qin, Limei Guo, Shan Jiang, Lin Luo","Deep learning in digital pathology brings intelligence and automation as
substantial enhancements to pathological analysis, the gold standard of
clinical diagnosis. However, multiple steps from tissue preparation to slide
imaging introduce various image corruptions, making it difficult for deep
neural network (DNN) models to achieve stable diagnostic results for clinical
use. In order to assess and further enhance the robustness of the models, we
analyze the physical causes of the full-stack corruptions throughout the
pathological life-cycle and propose an Omni-Corruption Emulation (OmniCE)
method to reproduce 21 types of corruptions quantified with 5-level severity.
We then construct three OmniCE-corrupted benchmark datasets at both patch level
and slide level and assess the robustness of popular DNNs in classification and
segmentation tasks. Further, we explore to use the OmniCE-corrupted datasets as
augmentation data for training and experiments to verify that the
generalization ability of the models has been significantly enhanced.",2310.20427v1,https://arxiv.org/pdf/2310.20427v1
"Is Robustness Transferable across Languages in Multilingual Neural
  Machine Translation?","Leiyu Pan, Supryadi, Deyi Xiong","Robustness, the ability of models to maintain performance in the face of
perturbations, is critical for developing reliable NLP systems. Recent studies
have shown promising results in improving the robustness of models through
adversarial training and data augmentation. However, in machine translation,
most of these studies have focused on bilingual machine translation with a
single translation direction. In this paper, we investigate the transferability
of robustness across different languages in multilingual neural machine
translation. We propose a robustness transfer analysis protocol and conduct a
series of experiments. In particular, we use character-, word-, and multi-level
noises to attack the specific translation direction of the multilingual neural
machine translation model and evaluate the robustness of other translation
directions. Our findings demonstrate that the robustness gained in one
translation direction can indeed transfer to other translation directions.
Additionally, we empirically find scenarios where robustness to character-level
noise and word-level noise is more likely to transfer.",2310.20162v1,https://arxiv.org/pdf/2310.20162v1
Efficient Robust Bayesian Optimization for Arbitrary Uncertain Inputs,"Lin Yang, Junlong Lyu, Wenlong Lyu, Zhitang Chen","Bayesian Optimization (BO) is a sample-efficient optimization algorithm
widely employed across various applications. In some challenging BO tasks,
input uncertainty arises due to the inevitable randomness in the optimization
process, such as machining errors, execution noise, or contextual variability.
This uncertainty deviates the input from the intended value before evaluation,
resulting in significant performance fluctuations in the final result. In this
paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO,
which can effectively identify a robust optimum that performs consistently well
under arbitrary input uncertainty. Our method directly models the uncertain
inputs of arbitrary distributions by empowering the Gaussian Process with the
Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference
via Nystrom approximation. Rigorous theoretical regret bound is established
under MMD estimation error and extensive experiments on synthetic functions and
real problems demonstrate that our approach can handle various input
uncertainties and achieve state-of-the-art performance.",2310.20145v2,https://arxiv.org/pdf/2310.20145v2
"Robust Learning for Smoothed Online Convex Optimization with Feedback
  Delay","Pengfei Li, Jianyi Yang, Adam Wierman, Shaolei Ren","We study a challenging form of Smoothed Online Convex Optimization, a.k.a.
SOCO, including multi-step nonlinear switching costs and feedback delay. We
propose a novel machine learning (ML) augmented online algorithm,
Robustness-Constrained Learning (RCL), which combines untrusted ML predictions
with a trusted expert online algorithm via constrained projection to robustify
the ML prediction. Specifically,we prove that RCL is able to
guarantee$(1+\lambda)$-competitiveness against any given expert for
any$\lambda>0$, while also explicitly training the ML model in a
robustification-aware manner to improve the average-case performance.
Importantly,RCL is the first ML-augmented algorithm with a provable robustness
guarantee in the case of multi-step switching cost and feedback delay.We
demonstrate the improvement of RCL in both robustness and average performance
using battery management for electrifying transportationas a case study.",2310.20098v1,https://arxiv.org/pdf/2310.20098v1
Faithful and Robust Local Interpretability for Textual Predictions,"Gianluigi Lopardo, Frederic Precioso, Damien Garreau","Interpretability is essential for machine learning models to be trusted and
deployed in critical domains. However, existing methods for interpreting text
models are often complex, lack mathematical foundations, and their performance
is not guaranteed. In this paper, we propose FRED (Faithful and Robust
Explainer for textual Documents), a novel method for interpreting predictions
over text. FRED offers three key insights to explain a model prediction: (1) it
identifies the minimal set of words in a document whose removal has the
strongest influence on the prediction, (2) it assigns an importance score to
each token, reflecting its influence on the model's output, and (3) it provides
counterfactual explanations by generating examples similar to the original
document, but leading to a different prediction. We establish the reliability
of FRED through formal definitions and theoretical analyses on interpretable
classifiers. Additionally, our empirical evaluation against state-of-the-art
methods demonstrates the effectiveness of FRED in providing insights into text
models.",2311.01605v3,https://arxiv.org/pdf/2311.01605v3
Robust Causal Bandits for Linear Models,"Zirui Yan, Arpan Mukherjee, Burak Varıcı, Ali Tajer","Sequential design of experiments for optimizing a reward function in causal
systems can be effectively modeled by the sequential design of interventions in
causal bandits (CBs). In the existing literature on CBs, a critical assumption
is that the causal models remain constant over time. However, this assumption
does not necessarily hold in complex systems, which constantly undergo temporal
model fluctuations. This paper addresses the robustness of CBs to such model
fluctuations. The focus is on causal systems with linear structural equation
models (SEMs). The SEMs and the time-varying pre- and post-interventional
statistical models are all unknown. Cumulative regret is adopted as the design
criteria, based on which the objective is to design a sequence of interventions
that incur the smallest cumulative regret with respect to an oracle aware of
the entire causal model and its fluctuations. First, it is established that the
existing approaches fail to maintain regret sub-linearity with even a few
instances of model deviation. Specifically, when the number of instances with
model deviation is as few as $T^\frac{1}{2L}$, where $T$ is the time horizon
and $L$ is the longest causal path in the graph, the existing algorithms will
have linear regret in $T$. Next, a robust CB algorithm is designed, and its
regret is analyzed, where upper and information-theoretic lower bounds on the
regret are established. Specifically, in a graph with $N$ nodes and maximum
degree $d$, under a general measure of model deviation $C$, the cumulative
regret is upper bounded by $\tilde{\mathcal{O}}(d^{L-\frac{1}{2}}(\sqrt{NT} +
NC))$ and lower bounded by $\Omega(d^{\frac{L}{2}-2}\max\{\sqrt{T},d^2C\})$.
Comparing these bounds establishes that the proposed algorithm achieves nearly
optimal $\tilde{\mathcal{O}}(\sqrt{T})$ regret when $C$ is $o(\sqrt{T})$ and
maintains sub-linear regret for a broader range of $C$.",2310.19794v2,https://arxiv.org/pdf/2310.19794v2
"Causal Context Connects Counterfactual Fairness to Robust Prediction and
  Group Fairness","Jacy Reese Anthis, Victor Veitch","Counterfactual fairness requires that a person would have been classified in
the same way by an AI or other algorithmic system if they had a different
protected class, such as a different race or gender. This is an intuitive
standard, as reflected in the U.S. legal system, but its use is limited because
counterfactuals cannot be directly observed in real-world data. On the other
hand, group fairness metrics (e.g., demographic parity or equalized odds) are
less intuitive but more readily observed. In this paper, we use $\textit{causal
context}$ to bridge the gaps between counterfactual fairness, robust
prediction, and group fairness. First, we motivate counterfactual fairness by
showing that there is not necessarily a fundamental trade-off between fairness
and accuracy because, under plausible conditions, the counterfactually fair
predictor is in fact accuracy-optimal in an unbiased target distribution.
Second, we develop a correspondence between the causal graph of the
data-generating process and which, if any, group fairness metrics are
equivalent to counterfactual fairness. Third, we show that in three common
fairness contexts$\unicode{x2013}$measurement error, selection on label, and
selection on predictors$\unicode{x2013}$counterfactual fairness is equivalent
to demographic parity, equalized odds, and calibration, respectively.
Counterfactual fairness can sometimes be tested by measuring relatively simple
group fairness metrics.",2310.19691v1,https://arxiv.org/pdf/2310.19691v1
"Causal Fair Metric: Bridging Causality, Individual Fairness, and
  Adversarial Robustness","Ahmad-Reza Ehyaei, Golnoosh Farnadi, Samira Samadi","Despite the essential need for comprehensive considerations in responsible
AI, factors like robustness, fairness, and causality are often studied in
isolation. Adversarial perturbation, used to identify vulnerabilities in
models, and individual fairness, aiming for equitable treatment of similar
individuals, despite initial differences, both depend on metrics to generate
comparable input data instances. Previous attempts to define such joint metrics
often lack general assumptions about data or structural causal models and were
unable to reflect counterfactual proximity. To address this, our paper
introduces a causal fair metric formulated based on causal structures
encompassing sensitive attributes and protected causal perturbation. To enhance
the practicality of our metric, we propose metric learning as a method for
metric estimation and deployment in real-world problems in the absence of
structural causal models. We also demonstrate the application of our novel
metric in classifiers. Empirical evaluation of real-world and synthetic
datasets illustrates the effectiveness of our proposed metric in achieving an
accurate classifier with fairness, resilience to adversarial perturbations, and
a nuanced understanding of causal relationships.",2310.19391v2,https://arxiv.org/pdf/2310.19391v2
"Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from
  a Minimax Game Perspective","Yifei Wang, Liangchen Li, Jiansheng Yang, Zhouchen Lin, Yisen Wang","Adversarial Training (AT) has become arguably the state-of-the-art algorithm
for extracting robust features. However, researchers recently notice that AT
suffers from severe robust overfitting problems, particularly after learning
rate (LR) decay. In this paper, we explain this phenomenon by viewing
adversarial training as a dynamic minimax game between the model trainer and
the attacker. Specifically, we analyze how LR decay breaks the balance between
the minimax game by empowering the trainer with a stronger memorization
ability, and show such imbalance induces robust overfitting as a result of
memorizing non-robust features. We validate this understanding with extensive
experiments, and provide a holistic view of robust overfitting from the
dynamics of both the two game players. This understanding further inspires us
to alleviate robust overfitting by rebalancing the two players by either
regularizing the trainer's capacity or improving the attack strength.
Experiments show that the proposed ReBalanced Adversarial Training (ReBAT) can
attain good robustness and does not suffer from robust overfitting even after
very long training. Code is available at https://github.com/PKU-ML/ReBAT.",2310.19360v1,https://arxiv.org/pdf/2310.19360v1
Flow-based Distributionally Robust Optimization,"Chen Xu, Jonghyeok Lee, Xiuyuan Cheng, Yao Xie","We present a computationally efficient framework, called $\texttt{FlowDRO}$,
for solving flow-based distributionally robust optimization (DRO) problems with
Wasserstein uncertainty sets while aiming to find continuous worst-case
distribution (also called the Least Favorable Distribution, LFD) and sample
from it. The requirement for LFD to be continuous is so that the algorithm can
be scalable to problems with larger sample sizes and achieve better
generalization capability for the induced robust algorithms. To tackle the
computationally challenging infinitely dimensional optimization problem, we
leverage flow-based models and continuous-time invertible transport maps
between the data distribution and the target distribution and develop a
Wasserstein proximal gradient flow type algorithm. In theory, we establish the
equivalence of the solution by optimal transport map to the original
formulation, as well as the dual form of the problem through Wasserstein
calculus and Brenier theorem. In practice, we parameterize the transport maps
by a sequence of neural networks progressively trained in blocks by gradient
descent. We demonstrate its usage in adversarial learning, distributionally
robust hypothesis testing, and a new mechanism for data-driven distribution
perturbation differential privacy, where the proposed method gives strong
empirical performance on high-dimensional real data.",2310.19253v4,https://arxiv.org/pdf/2310.19253v4
Robustifying Language Models with Test-Time Adaptation,"Noah Thomas McDermott, Junfeng Yang, Chengzhi Mao","Large-scale language models achieved state-of-the-art performance over a
number of language tasks. However, they fail on adversarial language examples,
which are sentences optimized to fool the language models but with similar
semantic meanings for humans. While prior work focuses on making the language
model robust at training time, retraining for robustness is often unrealistic
for large-scale foundation models. Instead, we propose to make the language
models robust at test time. By dynamically adapting the input sentence with
predictions from masked words, we show that we can reverse many language
adversarial attacks. Since our approach does not require any training, it works
for novel tasks at test time and can adapt to novel adversarial corruptions.
Visualizations and empirical results on two popular sentence classification
datasets demonstrate that our method can repair adversarial language attacks
over 65% o",2310.19177v1,https://arxiv.org/pdf/2310.19177v1
BERT Lost Patience Won't Be Robust to Adversarial Slowdown,"Zachary Coalson, Gabriel Ritter, Rakesh Bobba, Sanghyun Hong","In this paper, we systematically evaluate the robustness of multi-exit
language models against adversarial slowdown. To audit their robustness, we
design a slowdown attack that generates natural adversarial text bypassing
early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a
comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark
against adversarial slowdown. We then show our attack significantly reduces the
computational savings provided by the three methods in both white-box and
black-box settings. The more complex a mechanism is, the more vulnerable it is
to adversarial slowdown. We also perform a linguistic analysis of the perturbed
text inputs, identifying common perturbation patterns that our attack
generates, and comparing them with standard adversarial text attacks. Moreover,
we show that adversarial training is ineffective in defeating our slowdown
attack, but input sanitization with a conversational model, e.g., ChatGPT, can
remove perturbations effectively. This result suggests that future work is
needed for developing efficient yet robust multi-exit models. Our code is
available at: https://github.com/ztcoalson/WAFFLE",2310.19152v2,https://arxiv.org/pdf/2310.19152v2
Robust Offline Reinforcement learning with Heavy-Tailed Rewards,"Jin Zhu, Runzhe Wan, Zhengling Qi, Shikai Luo, Chengchun Shi","This paper endeavors to augment the robustness of offline reinforcement
learning (RL) in scenarios laden with heavy-tailed rewards, a prevalent
circumstance in real-world applications. We propose two algorithmic frameworks,
ROAM and ROOM, for robust off-policy evaluation and offline policy optimization
(OPO), respectively. Central to our frameworks is the strategic incorporation
of the median-of-means method with offline RL, enabling straightforward
uncertainty estimation for the value function estimator. This not only adheres
to the principle of pessimism in OPO but also adeptly manages heavy-tailed
rewards. Theoretical results and extensive experiments demonstrate that our two
frameworks outperform existing methods on the logged dataset exhibits
heavy-tailed reward distributions. The implementation of the proposal is
available at https://github.com/Mamba413/ROOM.",2310.18715v2,https://arxiv.org/pdf/2310.18715v2
"Benchmark Generation Framework with Customizable Distortions for Image
  Classifier Robustness","Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Zachariah Carmichael, Vineet Gundecha, Sahand Ghorbanpour, Ricardo Luna, Gutierrez Antonio Guillen, Avisek Naug","We present a novel framework for generating adversarial benchmarks to
evaluate the robustness of image classification models. Our framework allows
users to customize the types of distortions to be optimally applied to images,
which helps address the specific distortions relevant to their deployment. The
benchmark can generate datasets at various distortion levels to assess the
robustness of different image classifiers. Our results show that the
adversarial samples generated by our framework with any of the image
classification models, like ResNet-50, Inception-V3, and VGG-16, are effective
and transferable to other models causing them to fail. These failures happen
even when these models are adversarially retrained using state-of-the-art
techniques, demonstrating the generalizability of our adversarial samples. We
achieve competitive performance in terms of net $L_2$ distortion compared to
state-of-the-art benchmark techniques on CIFAR-10 and ImageNet; however, we
demonstrate our framework achieves such results with simple distortions like
Gaussian noise without introducing unnatural artifacts or color bleeds. This is
made possible by a model-based reinforcement learning (RL) agent and a
technique that reduces a deep tree search of the image for model sensitivity to
perturbations, to a one-level analysis and action. The flexibility of choosing
distortions and setting classification probability thresholds for multiple
classes makes our framework suitable for algorithmic audits.",2310.18626v2,https://arxiv.org/pdf/2310.18626v2
A General Framework for Robust G-Invariance in G-Equivariant Networks,"Sophia Sanborn, Nina Miolane","We introduce a general method for achieving robust group-invariance in
group-equivariant convolutional neural networks ($G$-CNNs), which we call the
$G$-triple-correlation ($G$-TC) layer. The approach leverages the theory of the
triple-correlation on groups, which is the unique, lowest-degree polynomial
invariant map that is also complete. Many commonly used invariant maps--such as
the max--are incomplete: they remove both group and signal structure. A
complete invariant, by contrast, removes only the variation due to the actions
of the group, while preserving all information about the structure of the
signal. The completeness of the triple correlation endows the $G$-TC layer with
strong robustness, which can be observed in its resistance to invariance-based
adversarial attacks. In addition, we observe that it yields measurable
improvements in classification accuracy over standard Max $G$-Pooling in
$G$-CNN architectures. We provide a general and efficient implementation of the
method for any discretized group, which requires only a table defining the
group's product structure. We demonstrate the benefits of this method for
$G$-CNNs defined on both commutative and non-commutative groups--$SO(2)$,
$O(2)$, $SO(3)$, and $O(3)$ (discretized as the cyclic $C8$, dihedral $D16$,
chiral octahedral $O$ and full octahedral $O_h$ groups)--acting on
$\mathbb{R}^2$ and $\mathbb{R}^3$ on both $G$-MNIST and $G$-ModelNet10
datasets.",2310.18564v2,https://arxiv.org/pdf/2310.18564v2
Group Robust Classification Without Any Group Information,"Christos Tsirigotis, Joao Monteiro, Pau Rodriguez, David Vazquez, Aaron Courville","Empirical risk minimization (ERM) is sensitive to spurious correlations in
the training data, which poses a significant risk when deploying systems
trained under this paradigm in high-stake applications. While the existing
literature focuses on maximizing group-balanced or worst-group accuracy,
estimating these accuracies is hindered by costly bias annotations. This study
contends that current bias-unsupervised approaches to group robustness continue
to rely on group information to achieve optimal performance. Firstly, these
methods implicitly assume that all group combinations are represented during
training. To illustrate this, we introduce a systematic generalization task on
the MPI3D dataset and discover that current algorithms fail to improve the ERM
baseline when combinations of observed attribute values are missing. Secondly,
bias labels are still crucial for effective model selection, restricting the
practicality of these methods in real-world scenarios. To address these
limitations, we propose a revised methodology for training and validating
debiased models in an entirely bias-unsupervised manner. We achieve this by
employing pretrained self-supervised models to reliably extract bias
information, which enables the integration of a logit adjustment training loss
with our validation criterion. Our empirical analysis on synthetic and
real-world tasks provides evidence that our approach overcomes the identified
challenges and consistently enhances robust accuracy, attaining performance
which is competitive with or outperforms that of state-of-the-art methods,
which, conversely, rely on bias labels for validation.",2310.18555v1,https://arxiv.org/pdf/2310.18555v1
"Weighted Sampled Split Learning (WSSL): Balancing Privacy, Robustness,
  and Fairness in Distributed Learning Environments","Manish Osti, Aashray Thakuri, Basheer Qolomany, Aos Mulahuwaish","This study presents Weighted Sampled Split Learning (WSSL), an innovative
framework tailored to bolster privacy, robustness, and fairness in distributed
machine learning systems. Unlike traditional approaches, WSSL disperses the
learning process among multiple clients, thereby safeguarding data
confidentiality. Central to WSSL's efficacy is its utilization of weighted
sampling. This approach ensures equitable learning by tactically selecting
influential clients based on their contributions. Our evaluation of WSSL
spanned various client configurations and employed two distinct datasets: Human
Gait Sensor and CIFAR-10. We observed three primary benefits: heightened model
accuracy, enhanced robustness, and maintained fairness across diverse client
compositions. Notably, our distributed frameworks consistently surpassed
centralized counterparts, registering accuracy peaks of 82.63% and 75.51% for
the Human Gait Sensor and CIFAR-10 datasets, respectively. These figures
contrast with the top accuracies of 81.12% and 58.60% achieved by centralized
systems. Collectively, our findings champion WSSL as a potent and scalable
successor to conventional centralized learning, marking it as a pivotal stride
forward in privacy-focused, resilient, and impartial distributed machine
learning.",2310.18479v1,https://arxiv.org/pdf/2310.18479v1
"Bridging Distributionally Robust Learning and Offline RL: An Approach to
  Mitigate Distribution Shift and Partial Data Coverage","Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, Mohammad Ghavamzadeh","The goal of an offline reinforcement learning (RL) algorithm is to learn
optimal polices using historical (offline) data, without access to the
environment for online exploration. One of the main challenges in offline RL is
the distribution shift which refers to the difference between the state-action
visitation distribution of the data generating policy and the learning policy.
Many recent works have used the idea of pessimism for developing offline RL
algorithms and characterizing their sample complexity under a relatively weak
assumption of single policy concentrability. Different from the offline RL
literature, the area of distributionally robust learning (DRL) offers a
principled framework that uses a minimax formulation to tackle model mismatch
between training and testing environments. In this work, we aim to bridge these
two areas by showing that the DRL approach can be used to tackle the
distributional shift problem in offline RL. In particular, we propose two
offline RL algorithms using the DRL framework, for the tabular and linear
function approximation settings, and characterize their sample complexity under
the single policy concentrability assumption. We also demonstrate the superior
performance our proposed algorithm through simulation experiments.",2310.18434v1,https://arxiv.org/pdf/2310.18434v1
On the Fairness ROAD: Robust Optimization for Adversarial Debiasing,"Vincent Grari, Thibault Laugel, Tatsunori Hashimoto, Sylvain Lamprier, Marcin Detyniecki","In the field of algorithmic fairness, significant attention has been put on
group fairness criteria, such as Demographic Parity and Equalized Odds.
Nevertheless, these objectives, measured as global averages, have raised
concerns about persistent local disparities between sensitive groups. In this
work, we address the problem of local fairness, which ensures that the
predictor is unbiased not only in terms of expectations over the whole
population, but also within any subregion of the feature space, unknown at
training time. To enforce this objective, we introduce ROAD, a novel approach
that leverages the Distributionally Robust Optimization (DRO) framework within
a fair adversarial learning objective, where an adversary tries to infer the
sensitive attribute from the predictions. Using an instance-level re-weighting
strategy, ROAD is designed to prioritize inputs that are likely to be locally
unfair, i.e. where the adversary faces the least difficulty in reconstructing
the sensitive attribute. Numerical experiments demonstrate the effectiveness of
our method: it achieves Pareto dominance with respect to local fairness and
accuracy for a given global fairness level across three standard datasets, and
also enhances fairness generalization under distribution shift.",2310.18413v1,https://arxiv.org/pdf/2310.18413v1
LipSim: A Provably Robust Perceptual Similarity Metric,"Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg","Recent years have seen growing interest in developing and applying perceptual
similarity metrics. Research has shown the superiority of perceptual metrics
over pixel-wise metrics in aligning with human perception and serving as a
proxy for the human visual system. On the other hand, as perceptual metrics
rely on neural networks, there is a growing concern regarding their resilience,
given the established vulnerability of neural networks to adversarial attacks.
It is indeed logical to infer that perceptual metrics may inherit both the
strengths and shortcomings of neural networks. In this work, we demonstrate the
vulnerability of state-of-the-art perceptual similarity metrics based on an
ensemble of ViT-based feature extractors to adversarial attacks. We then
propose a framework to train a robust perceptual similarity metric called
LipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging
1-Lipschitz neural networks as the backbone, LipSim provides guarded areas
around each data point and certificates for all perturbations within an
$\ell_2$ ball. Finally, a comprehensive set of experiments shows the
performance of LipSim in terms of natural and certified scores and on the image
retrieval application. The code is available at
https://github.com/SaraGhazanfari/LipSim.",2310.18274v2,https://arxiv.org/pdf/2310.18274v2
"Robustness of Algorithms for Causal Structure Learning to Hyperparameter
  Choice","Damian Machlanski, Spyridon Samothrakis, Paul Clarke","Hyperparameters play a critical role in machine learning. Hyperparameter
tuning can make the difference between state-of-the-art and poor prediction
performance for any algorithm, but it is particularly challenging for structure
learning due to its unsupervised nature. As a result, hyperparameter tuning is
often neglected in favour of using the default values provided by a particular
implementation of an algorithm. While there have been numerous studies on
performance evaluation of causal discovery algorithms, how hyperparameters
affect individual algorithms, as well as the choice of the best algorithm for a
specific problem, has not been studied in depth before. This work addresses
this gap by investigating the influence of hyperparameters on causal structure
learning tasks. Specifically, we perform an empirical evaluation of
hyperparameter selection for some seminal learning algorithms on datasets of
varying levels of complexity. We find that, while the choice of algorithm
remains crucial to obtaining state-of-the-art performance, hyperparameter
selection in ensemble settings strongly influences the choice of algorithm, in
that a poor choice of hyperparameters can lead to analysts using algorithms
which do not give state-of-the-art performance for their data.",2310.18212v2,https://arxiv.org/pdf/2310.18212v2
"Reward Scale Robustness for Proximal Policy Optimization via DreamerV3
  Tricks","Ryan Sullivan, Akarsh Kumar, Shengyi Huang, John P. Dickerson, Joseph Suarez","Most reinforcement learning methods rely heavily on dense, well-normalized
environment rewards. DreamerV3 recently introduced a model-based method with a
number of tricks that mitigate these limitations, achieving state-of-the-art on
a wide range of benchmarks with a single set of hyperparameters. This result
sparked discussion about the generality of the tricks, since they appear to be
applicable to other reinforcement learning algorithms. Our work applies
DreamerV3's tricks to PPO and is the first such empirical study outside of the
original work. Surprisingly, we find that the tricks presented do not transfer
as general improvements to PPO. We use a high quality PPO reference
implementation and present extensive ablation studies totaling over 10,000 A100
hours on the Arcade Learning Environment and the DeepMind Control Suite. Though
our experiments demonstrate that these tricks do not generally outperform PPO,
we identify cases where they succeed and offer insight into the relationship
between the implementation tricks. In particular, PPO with these tricks
performs comparably to PPO on Atari games with reward clipping and
significantly outperforms PPO without reward clipping.",2310.17805v1,https://arxiv.org/pdf/2310.17805v1
Learning Optimal Classification Trees Robust to Distribution Shifts,"Nathan Justin, Sina Aghaei, Andrés Gómez, Phebe Vayanos","We consider the problem of learning classification trees that are robust to
distribution shifts between training and testing/deployment data. This problem
arises frequently in high stakes settings such as public health and social work
where data is often collected using self-reported surveys which are highly
sensitive to e.g., the framing of the questions, the time when and place where
the survey is conducted, and the level of comfort the interviewee has in
sharing information with the interviewer. We propose a method for learning
optimal robust classification trees based on mixed-integer robust optimization
technology. In particular, we demonstrate that the problem of learning an
optimal robust tree can be cast as a single-stage mixed-integer robust
optimization problem with a highly nonlinear and discontinuous objective. We
reformulate this problem equivalently as a two-stage linear robust optimization
problem for which we devise a tailored solution procedure based on constraint
generation. We evaluate the performance of our approach on numerous publicly
available datasets, and compare the performance to a regularized, non-robust
optimal tree. We show an increase of up to 12.48% in worst-case accuracy and of
up to 4.85% in average-case accuracy across several datasets and distribution
shifts from using our robust solution in comparison to the non-robust one.",2310.17772v1,https://arxiv.org/pdf/2310.17772v1
"ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training
  Quantization Framework for W8A8 Transformers","Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He","Quantization techniques are pivotal in reducing the memory and computational
demands of deep neural network inference. Existing solutions, such as
ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook
crucial memory-bounded operators and the complexities of per-token
quantization. Addressing these gaps, we present a novel, fully
hardware-enhanced robust optimized post-training W8A8 quantization framework,
ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and
compute-intensive operators, aiming for optimal hardware performance.
Additionally, it offers flexibility by allowing specific INT8 modules to switch
to FP16/BF16 mode, enhancing accuracy.",2310.17723v1,https://arxiv.org/pdf/2310.17723v1
A minimax optimal control approach for robust neural ODEs,"Cristina Cipriani, Alessandro Scagliotti, Tobias Wöhrer","In this paper, we address the adversarial training of neural ODEs from a
robust control perspective. This is an alternative to the classical training
via empirical risk minimization, and it is widely used to enforce reliable
outcomes for input perturbations. Neural ODEs allow the interpretation of deep
neural networks as discretizations of control systems, unlocking powerful tools
from control theory for the development and the understanding of machine
learning. In this specific case, we formulate the adversarial training with
perturbed data as a minimax optimal control problem, for which we derive first
order optimality conditions in the form of Pontryagin's Maximum Principle. We
provide a novel interpretation of robust training leading to an alternative
weighted technique, which we test on a low-dimensional classification task.",2310.17584v3,https://arxiv.org/pdf/2310.17584v3
"DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor
  Critic","Dexter Neo, Tsuhan Chen","We present a novel extension to the family of Soft Actor-Critic (SAC)
algorithms. We argue that based on the Maximum Entropy Principle, discrete SAC
can be further improved via additional statistical constraints derived from a
surrogate critic policy. Furthermore, our findings suggests that these
constraints provide an added robustness against potential domain shifts, which
are essential for safe deployment of reinforcement learning agents in the
real-world. We provide theoretical analysis and show empirical results on low
data regimes for both in-distribution and out-of-distribution variants of Atari
2600 games.",2310.17173v1,https://arxiv.org/pdf/2310.17173v1
"OptScaler: A Hybrid Proactive-Reactive Framework for Robust Autoscaling
  in the Cloud","Ding Zou, Wei Lu, Zhibo Zhu, Xingyu Lu, Jun Zhou, Xiaojin Wang, Kangyu Liu, Haiqing Wang, Kefan Wang, Renen Sun","Autoscaling is a vital mechanism in cloud computing that supports the
autonomous adjustment of computing resources under dynamic workloads. A primary
goal of autoscaling is to stabilize resource utilization at a desirable level,
thus reconciling the need for resource-saving with the satisfaction of Service
Level Objectives (SLOs). Existing proactive autoscaling methods anticipate the
future workload and scale the resources in advance, whereas the reliability may
suffer from prediction deviations arising from the frequent fluctuations and
noise of cloud workloads; reactive methods rely on real-time system feedback,
while the hysteretic nature of reactive methods could cause violations of the
rigorous SLOs. To this end, this paper presents OptScaler, a hybrid autoscaling
framework that integrates the power of both proactive and reactive methods for
regulating CPU utilization. Specifically, the proactive module of OptScaler
consists of a sophisticated workload prediction model and an optimization
model, where the former provides reliable inputs to the latter for making
optimal scaling decisions. The reactive module provides a self-tuning estimator
of CPU utilization to the optimization model. We embed Model Predictive Control
(MPC) mechanism and robust optimization techniques into the optimization model
to further enhance its reliability. Numerical results have demonstrated the
superiority of both the workload prediction model and the hybrid framework of
OptScaler in the scenario of online services compared to prevalent reactive,
proactive, or hybrid autoscalers. OptScaler has been successfully deployed at
Alipay, supporting the autoscaling of applets in the world-leading payment
platform.",2311.12864v1,https://arxiv.org/pdf/2311.12864v1
"Trust, but Verify: Robust Image Segmentation using Deep Learning","Fahim Ahmed Zaman, Xiaodong Wu, Weiyu Xu, Milan Sonka, Raghuraman Mudumbai","We describe a method for verifying the output of a deep neural network for
medical image segmentation that is robust to several classes of random as well
as worst-case perturbations i.e. adversarial attacks. This method is based on a
general approach recently developed by the authors called ""Trust, but Verify""
wherein an auxiliary verification network produces predictions about certain
masked features in the input image using the segmentation as an input. A
well-designed auxiliary network will produce high-quality predictions when the
input segmentations are accurate, but will produce low-quality predictions when
the segmentations are incorrect. Checking the predictions of such a network
with the original image allows us to detect bad segmentations. However, to
ensure the verification method is truly robust, we need a method for checking
the quality of the predictions that does not itself rely on a black-box neural
network. Indeed, we show that previous methods for segmentation evaluation that
do use deep neural regression networks are vulnerable to false negatives i.e.
can inaccurately label bad segmentations as good. We describe the design of a
verification network that avoids such vulnerability and present results to
demonstrate its robustness compared to previous methods.",2310.16999v3,https://arxiv.org/pdf/2310.16999v3
"Break it, Imitate it, Fix it: Robustness by Generating Human-Like
  Attacks","Aradhana Sinha, Ananth Balashankar, Ahmad Beirami, Thi Avrahami, Jilin Chen, Alex Beutel","Real-world natural language processing systems need to be robust to human
adversaries. Collecting examples of human adversaries for training is an
effective but expensive solution. On the other hand, training on synthetic
attacks with small perturbations - such as word-substitution - does not
actually improve robustness to human adversaries. In this paper, we propose an
adversarial training framework that uses limited human adversarial examples to
generate more useful adversarial examples at scale. We demonstrate the
advantages of this system on the ANLI and hate speech detection benchmark
datasets - both collected via an iterative, adversarial
human-and-model-in-the-loop procedure. Compared to training only on observed
human attacks, also training on our synthetic adversarial examples improves
model robustness to future rounds. In ANLI, we see accuracy gains on the
current set of attacks (44.1%$\,\to\,$50.1%) and on two future unseen rounds of
human generated attacks (32.5%$\,\to\,$43.4%, and 29.4%$\,\to\,$40.2%). In hate
speech detection, we see AUC gains on current attacks (0.76 $\to$ 0.84) and a
future round (0.77 $\to$ 0.79). Attacks from methods that do not learn the
distribution of existing human adversaries, meanwhile, degrade robustness.",2310.16955v2,https://arxiv.org/pdf/2310.16955v2
Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs,"Jianwei Fei, Zhihua Xia, Benedetta Tondi, Mauro Barni","We propose a novel multi-bit box-free watermarking method for the protection
of Intellectual Property Rights (IPR) of GANs with improved robustness against
white-box attacks like fine-tuning, pruning, quantization, and surrogate model
attacks. The watermark is embedded by adding an extra watermarking loss term
during GAN training, ensuring that the images generated by the GAN contain an
invisible watermark that can be retrieved by a pre-trained watermark decoder.
In order to improve the robustness against white-box model-level attacks, we
make sure that the model converges to a wide flat minimum of the watermarking
loss term, in such a way that any modification of the model parameters does not
erase the watermark. To do so, we add random noise vectors to the parameters of
the generator and require that the watermarking loss term is as invariant as
possible with respect to the presence of noise. This procedure forces the
generator to converge to a wide flat minimum of the watermarking loss. The
proposed method is architectureand dataset-agnostic, thus being applicable to
many different generation tasks and models, as well as to CNN-based image
processing architectures. We present the results of extensive experiments
showing that the presence of the watermark has a negligible impact on the
quality of the generated images, and proving the superior robustness of the
watermark against model modification and surrogate model attacks.",2310.16919v1,https://arxiv.org/pdf/2310.16919v1
"TD-MPC2: Scalable, Robust World Models for Continuous Control","Nicklas Hansen, Hao Su, Xiaolong Wang","TD-MPC is a model-based reinforcement learning (RL) algorithm that performs
local trajectory optimization in the latent space of a learned implicit
(decoder-free) world model. In this work, we present TD-MPC2: a series of
improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves
significantly over baselines across 104 online RL tasks spanning 4 diverse task
domains, achieving consistently strong results with a single set of
hyperparameters. We further show that agent capabilities increase with model
and data size, and successfully train a single 317M parameter agent to perform
80 tasks across multiple task domains, embodiments, and action spaces. We
conclude with an account of lessons, opportunities, and risks associated with
large TD-MPC2 agents. Explore videos, models, data, code, and more at
https://tdmpc2.com",2310.16828v2,https://arxiv.org/pdf/2310.16828v2
Robust and Actively Secure Serverless Collaborative Learning,"Olive Franzese, Adam Dziedzic, Christopher A. Choquette-Choo, Mark R. Thomas, Muhammad Ahmad Kaleem, Stephan Rabanser, Congyu Fang, Somesh Jha, Nicolas Papernot, Xiao Wang","Collaborative machine learning (ML) is widely used to enable institutions to
learn better models from distributed data. While collaborative approaches to
learning intuitively protect user data, they remain vulnerable to either the
server, the clients, or both, deviating from the protocol. Indeed, because the
protocol is asymmetric, a malicious server can abuse its power to reconstruct
client data points. Conversely, malicious clients can corrupt learning with
malicious updates. Thus, both clients and servers require a guarantee when the
other cannot be trusted to fully cooperate. In this work, we propose a
peer-to-peer (P2P) learning scheme that is secure against malicious servers and
robust to malicious clients. Our core contribution is a generic framework that
transforms any (compatible) algorithm for robust aggregation of model updates
to the setting where servers and clients can act maliciously. Finally, we
demonstrate the computational efficiency of our approach even with 1-million
parameter models trained by 100s of peers on standard datasets.",2310.16678v1,https://arxiv.org/pdf/2310.16678v1
"How Robust is Federated Learning to Communication Error? A Comparison
  Study Between Uplink and Downlink Channels","Linping Qu, Shenghui Song, Chi-Ying Tsui, Yuyi Mao","Because of its privacy-preserving capability, federated learning (FL) has
attracted significant attention from both academia and industry. However, when
being implemented over wireless networks, it is not clear how much
communication error can be tolerated by FL. This paper investigates the
robustness of FL to the uplink and downlink communication error. Our
theoretical analysis reveals that the robustness depends on two critical
parameters, namely the number of clients and the numerical range of model
parameters. It is also shown that the uplink communication in FL can tolerate a
higher bit error rate (BER) than downlink communication, and this difference is
quantified by a proposed formula. The findings and theoretical analyses are
further validated by extensive experiments.",2310.16652v2,https://arxiv.org/pdf/2310.16652v2
Robust Covariate Shift Adaptation for Density-Ratio Estimation,Masahiro Kato,"Consider a scenario where we have access to train data with both covariates
and outcomes while test data only contains covariates. In this scenario, our
primary aim is to predict the missing outcomes of the test data. With this
objective in mind, we train parametric regression models under a covariate
shift, where covariate distributions are different between the train and test
data. For this problem, existing studies have proposed covariate shift
adaptation via importance weighting using the density ratio. This approach
averages the train data losses, each weighted by an estimated ratio of the
covariate densities between the train and test data, to approximate the
test-data risk. Although it allows us to obtain a test-data risk minimizer, its
performance heavily relies on the accuracy of the density ratio estimation.
Moreover, even if the density ratio can be consistently estimated, the
estimation errors of the density ratio also yield bias in the estimators of the
regression model's parameters of interest. To mitigate these challenges, we
introduce a doubly robust estimator for covariate shift adaptation via
importance weighting, which incorporates an additional estimator for the
regression function. Leveraging double machine learning techniques, our
estimator reduces the bias arising from the density ratio estimation errors. We
demonstrate the asymptotic distribution of the regression parameter estimator.
Notably, our estimator remains consistent if either the density ratio estimator
or the regression function is consistent, showcasing its robustness against
potential errors in density ratio estimation. Finally, we confirm the soundness
of our proposed method via simulation studies.",2310.16638v2,https://arxiv.org/pdf/2310.16638v2
"Back Transcription as a Method for Evaluating Robustness of Natural
  Language Understanding Models to Speech Recognition Errors","Marek Kubis, Paweł Skórzewski, Marcin Sowański, Tomasz Ziętkiewicz","In a spoken dialogue system, an NLU model is preceded by a speech recognition
system that can deteriorate the performance of natural language understanding.
This paper proposes a method for investigating the impact of speech recognition
errors on the performance of natural language understanding models. The
proposed method combines the back transcription procedure with a fine-grained
technique for categorizing the errors that affect the performance of NLU
models. The method relies on the usage of synthesized speech for NLU
evaluation. We show that the use of synthesized speech in place of audio
recording does not change the outcomes of the presented technique in a
significant way.",2310.16609v1,https://arxiv.org/pdf/2310.16609v1
"Enhancing Document Information Analysis with Multi-Task Pre-training: A
  Robust Approach for Information Extraction in Visually-Rich Documents","Tofik Ali, Partha Pratim Roy","This paper introduces a deep learning model tailored for document information
analysis, emphasizing document classification, entity relation extraction, and
document visual question answering. The proposed model leverages
transformer-based models to encode all the information present in a document
image, including textual, visual, and layout information. The model is
pre-trained and subsequently fine-tuned for various document image analysis
tasks. The proposed model incorporates three additional tasks during the
pre-training phase, including reading order identification of different layout
segments in a document image, layout segments categorization as per PubLayNet,
and generation of the text sequence within a given layout segment (text block).
The model also incorporates a collective pre-training scheme where losses of
all the tasks under consideration, including pre-training and fine-tuning tasks
with all datasets, are considered. Additional encoder and decoder blocks are
added to the RoBERTa network to generate results for all tasks. The proposed
model achieved impressive results across all tasks, with an accuracy of 95.87%
on the RVL-CDIP dataset for document classification, F1 scores of 0.9306,
0.9804, 0.9794, and 0.8742 on the FUNSD, CORD, SROIE, and Kleister-NDA datasets
respectively for entity relation extraction, and an ANLS score of 0.8468 on the
DocVQA dataset for visual question answering. The results highlight the
effectiveness of the proposed model in understanding and interpreting complex
document layouts and content, making it a promising tool for document analysis
tasks.",2310.16527v1,https://arxiv.org/pdf/2310.16527v1
"ClearMark: Intuitive and Robust Model Watermarking via Transposed Model
  Training","Torsten Krauß, Jasper Stang, Alexandra Dmitrienko","Due to costly efforts during data acquisition and model training, Deep Neural
Networks (DNNs) belong to the intellectual property of the model creator.
Hence, unauthorized use, theft, or modification may lead to legal
repercussions. Existing DNN watermarking methods for ownership proof are often
non-intuitive, embed human-invisible marks, require trust in algorithmic
assessment that lacks human-understandable attributes, and rely on rigid
thresholds, making it susceptible to failure in cases of partial watermark
erasure.
  This paper introduces ClearMark, the first DNN watermarking method designed
for intuitive human assessment. ClearMark embeds visible watermarks, enabling
human decision-making without rigid value thresholds while allowing
technology-assisted evaluations. ClearMark defines a transposed model
architecture allowing to use of the model in a backward fashion to interwove
the watermark with the main task within all model parameters. Compared to
existing watermarking methods, ClearMark produces visual watermarks that are
easy for humans to understand without requiring complex verification algorithms
or strict thresholds. The watermark is embedded within all model parameters and
entangled with the main task, exhibiting superior robustness. It shows an
8,544-bit watermark capacity comparable to the strongest existing work.
Crucially, ClearMark's effectiveness is model and dataset-agnostic, and
resilient against adversarial model manipulations, as demonstrated in a
comprehensive study performed with four datasets and seven architectures.",2310.16453v1,https://arxiv.org/pdf/2310.16453v1
Fine tuning Pre trained Models for Robustness Under Noisy Labels,"Sumyeong Ahn, Sihyeon Kim, Jongwoo Ko, Se-Young Yun","The presence of noisy labels in a training dataset can significantly impact
the performance of machine learning models. To tackle this issue, researchers
have explored methods for Learning with Noisy Labels to identify clean samples
and reduce the influence of noisy labels. However, constraining the influence
of a certain portion of the training dataset can result in a reduction in
overall generalization performance. To alleviate this, recent studies have
considered the careful utilization of noisy labels by leveraging huge
computational resources. Therefore, the increasing training cost necessitates a
reevaluation of efficiency. In other areas of research, there has been a focus
on developing fine-tuning techniques for large pre-trained models that aim to
achieve both high generalization performance and efficiency. However, these
methods have mainly concentrated on clean datasets, and there has been limited
exploration of the noisy label scenario. In this research, our aim is to find
an appropriate way to fine-tune pre-trained models for noisy labeled datasets.
To achieve this goal, we investigate the characteristics of pre-trained models
when they encounter noisy datasets. Through empirical analysis, we introduce a
novel algorithm called TURN, which robustly and efficiently transfers the prior
knowledge of pre-trained models. The algorithm consists of two main steps: (1)
independently tuning the linear classifier to protect the feature extractor
from being distorted by noisy labels, and (2) reducing the noisy label ratio
and fine-tuning the entire model based on the noise-reduced dataset to adapt it
to the target dataset. The proposed algorithm has been extensively tested and
demonstrates efficient yet improved denoising performance on various benchmarks
compared to previous methods.",2310.17668v1,https://arxiv.org/pdf/2310.17668v1
"Transitivity Recovering Decompositions: Interpretable and Robust
  Fine-Grained Relationships","Abhra Chaudhuri, Massimiliano Mancini, Zeynep Akata, Anjan Dutta","Recent advances in fine-grained representation learning leverage
local-to-global (emergent) relationships for achieving state-of-the-art
results. The relational representations relied upon by such methods, however,
are abstract. We aim to deconstruct this abstraction by expressing them as
interpretable graphs over image views. We begin by theoretically showing that
abstract relational representations are nothing but a way of recovering
transitive relationships among local views. Based on this, we design
Transitivity Recovering Decompositions (TRD), a graph-space search algorithm
that identifies interpretable equivalents of abstract emergent relationships at
both instance and class levels, and with no post-hoc computations. We
additionally show that TRD is provably robust to noisy views, with empirical
evidence also supporting this finding. The latter allows TRD to perform at par
or even better than the state-of-the-art, while being fully interpretable.
Implementation is available at https://github.com/abhrac/trd.",2310.15999v1,https://arxiv.org/pdf/2310.15999v1
"Improving Robustness and Reliability in Medical Image Classification
  with Latent-Guided Diffusion and Nested-Ensembles","Xing Shen, Hengguan Huang, Brennan Nichyporuk, Tal Arbel","While deep learning models have achieved remarkable success across a range of
medical image analysis tasks, deployment of these models in real clinical
contexts requires that they be robust to variability in the acquired images.
While many methods apply predefined transformations to augment the training
data to enhance test-time robustness, these transformations may not ensure the
model's robustness to the diverse variability seen in patient images. In this
paper, we introduce a novel three-stage approach based on transformers coupled
with conditional diffusion models, with the goal of improving model robustness
to the kinds of imaging variability commonly encountered in practice without
the need for pre-determined data augmentation strategies. To this end, multiple
image encoders first learn hierarchical feature representations to build
discriminative latent spaces. Next, a reverse diffusion process, guided by the
latent code, acts on an informative prior and proposes prediction candidates in
a generative manner. Finally, several prediction candidates are aggregated in a
bi-level aggregation protocol to produce the final output. Through extensive
experiments on medical imaging benchmark datasets, we show that our method
improves upon state-of-the-art methods in terms of robustness and confidence
calibration. Additionally, we introduce a strategy to quantify the prediction
uncertainty at the instance level, increasing their trustworthiness to
clinicians using them in clinical practice.",2310.15952v3,https://arxiv.org/pdf/2310.15952v3
Online Robust Mean Estimation,"Daniel M. Kane, Ilias Diakonikolas, Hanshen Xiao, Sihan Liu","We study the problem of high-dimensional robust mean estimation in an online
setting. Specifically, we consider a scenario where $n$ sensors are measuring
some common, ongoing phenomenon. At each time step $t=1,2,\ldots,T$, the
$i^{th}$ sensor reports its readings $x^{(i)}_t$ for that time step. The
algorithm must then commit to its estimate $\mu_t$ for the true mean value of
the process at time $t$. We assume that most of the sensors observe independent
samples from some common distribution $X$, but an $\epsilon$-fraction of them
may instead behave maliciously. The algorithm wishes to compute a good
approximation $\mu$ to the true mean $\mu^\ast := \mathbf{E}[X]$. We note that
if the algorithm is allowed to wait until time $T$ to report its estimate, this
reduces to the well-studied problem of robust mean estimation. However, the
requirement that our algorithm produces partial estimates as the data is coming
in substantially complicates the situation.
  We prove two main results about online robust mean estimation in this model.
First, if the uncorrupted samples satisfy the standard condition of
$(\epsilon,\delta)$-stability, we give an efficient online algorithm that
outputs estimates $\mu_t$, $t \in [T],$ such that with high probability it
holds that $\|\mu-\mu^\ast\|_2 = O(\delta \log(T))$, where $\mu = (\mu_t)_{t
\in [T]}$. We note that this error bound is nearly competitive with the best
offline algorithms, which would achieve $\ell_2$-error of $O(\delta)$. Our
second main result shows that with additional assumptions on the input (most
notably that $X$ is a product distribution) there are inefficient algorithms
whose error does not depend on $T$ at all.",2310.15932v1,https://arxiv.org/pdf/2310.15932v1
Robust Learning via Conditional Prevalence Adjustment,"Minh Nguyen, Alan Q. Wang, Heejong Kim, Mert R. Sabuncu","Healthcare data often come from multiple sites in which the correlations
between confounding variables can vary widely. If deep learning models exploit
these unstable correlations, they might fail catastrophically in unseen sites.
Although many methods have been proposed to tackle unstable correlations, each
has its limitations. For example, adversarial training forces models to
completely ignore unstable correlations, but doing so may lead to poor
predictive performance. Other methods (e.g. Invariant risk minimization [4])
try to learn domain-invariant representations that rely only on stable
associations by assuming a causal data-generating process (input X causes class
label Y ). Thus, they may be ineffective for anti-causal tasks (Y causes X),
which are common in computer vision. We propose a method called CoPA
(Conditional Prevalence-Adjustment) for anti-causal tasks. CoPA assumes that
(1) generation mechanism is stable, i.e. label Y and confounding variable(s) Z
generate X, and (2) the unstable conditional prevalence in each site E fully
accounts for the unstable correlations between X and Y . Our crucial
observation is that confounding variables are routinely recorded in healthcare
settings and the prevalence can be readily estimated, for example, from a set
of (Y, Z) samples (no need for corresponding samples of X). CoPA can work even
if there is a single training site, a scenario which is often overlooked by
existing methods. Our experiments on synthetic and real data show CoPA beating
competitive baselines.",2310.15766v1,https://arxiv.org/pdf/2310.15766v1
Robust Representation Learning for Unified Online Top-K Recommendation,"Minfang Lu, Yuchen Jiang, Huihui Dong, Qi Li, Ziru Xu, Yuanlin Liu, Lixia Wu, Haoyuan Hu, Han Zhu, Yuning Jiang, Jian Xu, Bo Zheng","In large-scale industrial e-commerce, the efficiency of an online
recommendation system is crucial in delivering highly relevant item/content
advertising that caters to diverse business scenarios. However, most existing
studies focus solely on item advertising, neglecting the significance of
content advertising. This oversight results in inconsistencies within the
multi-entity structure and unfair retrieval. Furthermore, the challenge of
retrieving top-k advertisements from multi-entity advertisements across
different domains adds to the complexity. Recent research proves that
user-entity behaviors within different domains exhibit characteristics of
differentiation and homogeneity. Therefore, the multi-domain matching models
typically rely on the hybrid-experts framework with domain-invariant and
domain-specific representations. Unfortunately, most approaches primarily focus
on optimizing the combination mode of different experts, failing to address the
inherent difficulty in optimizing the expert modules themselves. The existence
of redundant information across different domains introduces interference and
competition among experts, while the distinct learning objectives of each
domain lead to varying optimization challenges among experts. To tackle these
issues, we propose robust representation learning for the unified online top-k
recommendation. Our approach constructs unified modeling in entity space to
ensure data fairness. The robust representation learning employs domain
adversarial learning and multi-view wasserstein distribution learning to learn
robust representations. Moreover, the proposed method balances conflicting
objectives through the homoscedastic uncertainty weights and orthogonality
constraints. Various experiments validate the effectiveness and rationality of
our proposed method, which has been successfully deployed online to serve real
business scenarios.",2310.15492v1,https://arxiv.org/pdf/2310.15492v1
A Doubly Robust Approach to Sparse Reinforcement Learning,"Wonyoung Kim, Garud Iyengar, Assaf Zeevi","We propose a new regret minimization algorithm for episodic sparse linear
Markov decision process (SMDP) where the state-transition distribution is a
linear function of observed features. The only previously known algorithm for
SMDP requires the knowledge of the sparsity parameter and oracle access to an
unknown policy. We overcome these limitations by combining the doubly robust
method that allows one to use feature vectors of \emph{all} actions with a
novel analysis technique that enables the algorithm to use data from all
periods in all episodes. The regret of the proposed algorithm is
$\tilde{O}(\sigma^{-1}_{\min} s_{\star} H \sqrt{N})$, where $\sigma_{\min}$
denotes the restrictive the minimum eigenvalue of the average Gram matrix of
feature vectors, $s_\star$ is the sparsity parameter, $H$ is the length of an
episode, and $N$ is the number of rounds. We provide a lower regret bound that
matches the upper bound up to logarithmic factors on a newly identified
subclass of SMDPs. Our numerical experiments support our theoretical results
and demonstrate the superior performance of our algorithm.",2310.15286v1,https://arxiv.org/pdf/2310.15286v1
Machine Learning and Knowledge: Why Robustness Matters,Jonathan Vandenburgh,"Trusting machine learning algorithms requires having confidence in their
outputs. Confidence is typically interpreted in terms of model reliability,
where a model is reliable if it produces a high proportion of correct outputs.
However, model reliability does not address concerns about the robustness of
machine learning models, such as models relying on the wrong features or
variations in performance based on context. I argue that the epistemic
dimension of trust can instead be understood through the concept of knowledge,
where the trustworthiness of an algorithm depends on whether its users are in
the position to know that its outputs are correct. Knowledge requires beliefs
to be formed for the right reasons and to be robust to error, so machine
learning algorithms can only provide knowledge if they work well across
counterfactual scenarios and if they make decisions based on the right
features. This, I argue, can explain why we should care about model properties
like interpretability, causal shortcut independence, and distribution shift
robustness even if such properties are not required for model reliability.",2310.19819v1,https://arxiv.org/pdf/2310.19819v1
"Robust Depth Linear Error Decomposition with Double Total Variation and
  Nuclear Norm for Dynamic MRI Reconstruction","Junpeng Tan, Chunmei Qing, Xiangmin Xu","Compressed Sensing (CS) significantly speeds up Magnetic Resonance Image
(MRI) processing and achieves accurate MRI reconstruction from under-sampled
k-space data. According to the current research, there are still several
problems with dynamic MRI k-space reconstruction based on CS. 1) There are
differences between the Fourier domain and the Image domain, and the
differences between MRI processing of different domains need to be considered.
2) As three-dimensional data, dynamic MRI has its spatial-temporal
characteristics, which need to calculate the difference and consistency of
surface textures while preserving structural integrity and uniqueness. 3)
Dynamic MRI reconstruction is time-consuming and computationally
resource-dependent. In this paper, we propose a novel robust low-rank dynamic
MRI reconstruction optimization model via highly under-sampled and Discrete
Fourier Transform (DFT) called the Robust Depth Linear Error Decomposition
Model (RDLEDM). Our method mainly includes linear decomposition, double Total
Variation (TV), and double Nuclear Norm (NN) regularizations. By adding linear
image domain error analysis, the noise is reduced after under-sampled and DFT
processing, and the anti-interference ability of the algorithm is enhanced.
Double TV and NN regularizations can utilize both spatial-temporal
characteristics and explore the complementary relationship between different
dimensions in dynamic MRI sequences. In addition, Due to the non-smoothness and
non-convexity of TV and NN terms, it is difficult to optimize the unified
objective model. To address this issue, we utilize a fast algorithm by solving
a primal-dual form of the original problem. Compared with five state-of-the-art
methods, extensive experiments on dynamic MRI data demonstrate the superior
performance of the proposed method in terms of both reconstruction accuracy and
time complexity.",2310.14934v1,https://arxiv.org/pdf/2310.14934v1
"Leveraging Ensemble Diversity for Robust Self-Training in the Presence
  of Sample Selection Bias","Ambroise Odonnat, Vasilii Feofanov, Ievgen Redko","Self-training is a well-known approach for semi-supervised learning. It
consists of iteratively assigning pseudo-labels to unlabeled data for which the
model is confident and treating them as labeled examples. For neural networks,
softmax prediction probabilities are often used as a confidence measure,
although they are known to be overconfident, even for wrong predictions. This
phenomenon is particularly intensified in the presence of sample selection
bias, i.e., when data labeling is subject to some constraint. To address this
issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity,
built upon the prediction diversity of an ensemble of linear classifiers. We
provide the theoretical analysis of our approach by studying stationary points
and describing the relationship between the diversity of the individual members
and their performance. We empirically demonstrate the benefit of our confidence
measure for three different pseudo-labeling policies on classification datasets
of various data modalities. The code is available at
https://github.com/ambroiseodt/tsim.",2310.14814v4,https://arxiv.org/pdf/2310.14814v4
"Corruption-Robust Offline Reinforcement Learning with General Function
  Approximation","Chenlu Ye, Rui Yang, Quanquan Gu, Tong Zhang","We investigate the problem of corruption robustness in offline reinforcement
learning (RL) with general function approximation, where an adversary can
corrupt each sample in the offline dataset, and the corruption level
$\zeta\geq0$ quantifies the cumulative corruption amount over $n$ episodes and
$H$ steps. Our goal is to find a policy that is robust to such corruption and
minimizes the suboptimality gap with respect to the optimal policy for the
uncorrupted Markov decision processes (MDPs). Drawing inspiration from the
uncertainty-weighting technique from the robust online RL setting
\citep{he2022nearly,ye2022corruptionrobust}, we design a new uncertainty weight
iteration procedure to efficiently compute on batched samples and propose a
corruption-robust algorithm for offline RL. Notably, under the assumption of
single policy coverage and the knowledge of $\zeta$, our proposed algorithm
achieves a suboptimality bound that is worsened by an additive factor of
$\mathcal{O}(\zeta (C(\widehat{\mathcal{F}},\mu)n)^{-1})$ due to the
corruption. Here $\widehat{\mathcal{F}}$ is the confidence set, and the dataset
$\mathcal{Z}_n^H$, and $C(\widehat{\mathcal{F}},\mu)$ is a coefficient that
depends on $\widehat{\mathcal{F}}$ and the underlying data distribution $\mu$.
When specialized to linear MDPs, the corruption-dependent error term reduces to
$\mathcal{O}(\zeta d n^{-1})$ with $d$ being the dimension of the feature map,
which matches the existing lower bound for corrupted linear MDPs. This suggests
that our analysis is tight in terms of the corruption-dependent term.",2310.14550v3,https://arxiv.org/pdf/2310.14550v3
Robust Visual Imitation Learning with Inverse Dynamics Representations,"Siyuan Li, Xun Wang, Rongchang Zuo, Kewu Sun, Lingfei Cui, Jishiyu Ding, Peng Liu, Zhe Ma","Imitation learning (IL) has achieved considerable success in solving complex
sequential decision-making problems. However, current IL methods mainly assume
that the environment for learning policies is the same as the environment for
collecting expert datasets. Therefore, these methods may fail to work when
there are slight differences between the learning and expert environments,
especially for challenging problems with high-dimensional image observations.
However, in real-world scenarios, it is rare to have the chance to collect
expert trajectories precisely in the target learning environment. To address
this challenge, we propose a novel robust imitation learning approach, where we
develop an inverse dynamics state representation learning objective to align
the expert environment and the learning environment. With the abstract state
representation, we design an effective reward function, which thoroughly
measures the similarity between behavior data and expert data not only
element-wise, but also from the trajectory level. We conduct extensive
experiments to evaluate the proposed approach under various visual
perturbations and in diverse visual control tasks. Our approach can achieve a
near-expert performance in most environments, and significantly outperforms the
state-of-the-art visual IL methods and robust IL methods.",2310.14274v1,https://arxiv.org/pdf/2310.14274v1
Training Image Derivatives: Increased Accuracy and Universal Robustness,Vsevolod I. Avrutskiy,"Derivative training is a known method that significantly improves the
accuracy of neural networks in some low-dimensional applications. In this
paper, a similar improvement is obtained for an image analysis problem:
reconstructing the vertices of a cube from its image. By training the
derivatives with respect to the 6 degrees of freedom of the cube, we obtain 25
times more accurate results for noiseless inputs. The derivatives also offer
insight into the robustness problem, which is currently understood in terms of
two types of network vulnerabilities. The first type involves small
perturbations that dramatically change the output, and the second type relates
to substantial image changes that the network erroneously ignores. Defense
against each is possible, but safeguarding against both while maintaining the
accuracy defies conventional training methods. The first type is analyzed using
the network's gradient, while the second relies on human input evaluation,
serving as an oracle substitute. For the task at hand, the nearest neighbor
oracle can be defined and expanded into Taylor series using image derivatives.
This allows for a robustness analysis that unifies both types of
vulnerabilities and enables training where accuracy and universal robustness
are limited only by network capacity.",2310.14045v2,https://arxiv.org/pdf/2310.14045v2
"A Robust Adversary Detection-Deactivation Method for Metaverse-oriented
  Collaborative Deep Learning","Pengfei Li, Zhibo Zhang, Ameena S. Al-Sumaiti, Naoufel Werghi, Chan Yeob Yeun","Metaverse is trending to create a digital circumstance that can transfer the
real world to an online platform supported by large quantities of real-time
interactions. Pre-trained Artificial Intelligence (AI) models are demonstrating
their increasing capability in aiding the metaverse to achieve an excellent
response with negligible delay, and nowadays, many large models are
collaboratively trained by various participants in a manner named collaborative
deep learning (CDL). However, several security weaknesses can threaten the
safety of the CDL training process, which might result in fatal attacks to
either the pre-trained large model or the local sensitive data sets possessed
by an individual entity. In CDL, malicious participants can hide within the
major innocent and silently uploads deceptive parameters to degenerate the
model performance, or they can abuse the downloaded parameters to construct a
Generative Adversarial Network (GAN) to acquire the private information of
others illegally. To compensate for these vulnerabilities, this paper proposes
an adversary detection-deactivation method, which can limit and isolate the
access of potential malicious participants, quarantine and disable the
GAN-attack or harmful backpropagation of received threatening gradients. A
detailed protection analysis has been conducted on a Multiview CDL case, and
results show that the protocol can effectively prevent harmful access by
heuristic manner analysis and can protect the existing model by swiftly
checking received gradients using only one low-cost branch with an embedded
firewall.",2401.01895v1,https://arxiv.org/pdf/2401.01895v1
Specify Robust Causal Representation from Mixed Observations,"Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, Jun Wang","Learning representations purely from observations concerns the problem of
learning a low-dimensional, compact representation which is beneficial to
prediction models. Under the hypothesis that the intrinsic latent factors
follow some casual generative models, we argue that by learning a causal
representation, which is the minimal sufficient causes of the whole system, we
can improve the robustness and generalization performance of machine learning
models. In this paper, we develop a learning method to learn such
representation from observational data by regularizing the learning procedure
with mutual information measures, according to the hypothetical factored causal
graph. We theoretically and empirically show that the models trained with the
learned causal representations are more robust under adversarial attacks and
distribution shifts compared with baselines. The supplementary materials are
available at https://github.com/ymy $4323460 / \mathrm{CaRI} /$.",2310.13892v1,https://arxiv.org/pdf/2310.13892v1
Distributionally Robust Optimization with Bias and Variance Reduction,"Ronak Mehta, Vincent Roulet, Krishna Pillutla, Zaid Harchaoui","We consider the distributionally robust optimization (DRO) problem with
spectral risk-based uncertainty set and $f$-divergence penalty. This
formulation includes common risk-sensitive learning objectives such as
regularized condition value-at-risk (CVaR) and average top-$k$ loss. We present
Prospect, a stochastic gradient-based algorithm that only requires tuning a
single learning rate hyperparameter, and prove that it enjoys linear
convergence for smooth regularized losses. This contrasts with previous
algorithms that either require tuning multiple hyperparameters or potentially
fail to converge due to biased gradient estimates or inadequate regularization.
Empirically, we show that Prospect can converge 2-3$\times$ faster than
baselines such as stochastic gradient and stochastic saddle-point methods on
distribution shift and fairness benchmarks spanning tabular, vision, and
language domains.",2310.13863v1,https://arxiv.org/pdf/2310.13863v1
"Robust Training for Conversational Question Answering Models with
  Reinforced Reformulation Generation","Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum","Models for conversational question answering (ConvQA) over knowledge graphs
(KGs) are usually trained and tested on benchmarks of gold QA pairs. This
implies that training is limited to surface forms seen in the respective
datasets, and evaluation is on a small set of held-out questions. Through our
proposed framework REIGN, we take several steps to remedy this restricted
learning setup. First, we systematically generate reformulations of training
questions to increase robustness of models to surface form variations. This is
a particularly challenging problem, given the incomplete nature of such
questions. Second, we guide ConvQA models towards higher performance by feeding
it only those reformulations that help improve their answering quality, using
deep reinforcement learning. Third, we demonstrate the viability of training
major model components on one benchmark and applying them zero-shot to another.
Finally, for a rigorous evaluation of robustness for trained models, we use and
release large numbers of diverse reformulations generated by prompting GPT for
benchmark test sets (resulting in 20x increase in sizes). Our findings show
that ConvQA models with robust training via reformulations, significantly
outperform those with standard training from gold QA pairs only.",2310.13505v3,https://arxiv.org/pdf/2310.13505v3
BRFL: A Blockchain-based Byzantine-Robust Federated Learning Model,"Yang Li, Chunhe Xia, Chang Li, Tianbo Wang","With the increasing importance of machine learning, the privacy and security
of training data have become critical. Federated learning, which stores data in
distributed nodes and shares only model parameters, has gained significant
attention for addressing this concern. However, a challenge arises in federated
learning due to the Byzantine Attack Problem, where malicious local models can
compromise the global model's performance during aggregation. This article
proposes the Blockchain-based Byzantine-Robust Federated Learning (BRLF) model
that combines federated learning with blockchain technology. This integration
enables traceability of malicious models and provides incentives for locally
trained clients. Our approach involves selecting the aggregation node based on
Pearson's correlation coefficient, and we perform spectral clustering and
calculate the average gradient within each cluster, validating its accuracy
using local dataset of the aggregation nodes. Experimental results on public
datasets demonstrate the superior byzantine robustness of our secure
aggregation algorithm compared to other baseline byzantine robust aggregation
methods, and proved our proposed model effectiveness in addressing the resource
consumption problem.",2310.13403v1,https://arxiv.org/pdf/2310.13403v1
"Assumption violations in causal discovery and the robustness of score
  matching","Francesco Montagna, Atalanti A. Mastakouri, Elias Eulig, Nicoletta Noceti, Lorenzo Rosasco, Dominik Janzing, Bryon Aragam, Francesco Locatello","When domain knowledge is limited and experimentation is restricted by
ethical, financial, or time constraints, practitioners turn to observational
causal discovery methods to recover the causal structure, exploiting the
statistical properties of their data. Because causal discovery without further
assumptions is an ill-posed problem, each algorithm comes with its own set of
usually untestable assumptions, some of which are hard to meet in real
datasets. Motivated by these considerations, this paper extensively benchmarks
the empirical performance of recent causal discovery methods on observational
i.i.d. data generated under different background conditions, allowing for
violations of the critical assumptions required by each selected approach. Our
experimental findings show that score matching-based methods demonstrate
surprising performance in the false positive and false negative rate of the
inferred graph in these challenging scenarios, and we provide theoretical
insights into their performance. This work is also the first effort to
benchmark the stability of causal discovery algorithms with respect to the
values of their hyperparameters. Finally, we hope this paper will set a new
standard for the evaluation of causal discovery methods and can serve as an
accessible entry point for practitioners interested in the field, highlighting
the empirical implications of different algorithm choices.",2310.13387v1,https://arxiv.org/pdf/2310.13387v1
InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution,"Xiangru Jian, Yimu Wang","Over recent decades, significant advancements in cross-modal retrieval are
mainly driven by breakthroughs in visual and linguistic modeling. However, a
recent study shows that multi-modal data representations tend to cluster within
a limited convex cone (as representation degeneration problem), which hinders
retrieval performance due to the inseparability of these representations. In
our study, we first empirically validate the presence of the representation
degeneration problem across multiple cross-modal benchmarks and methods. Next,
to address it, we introduce a novel method, called InvGC, a post-processing
technique inspired by graph convolution and average pooling. Specifically,
InvGC defines the graph topology within the datasets and then applies graph
convolution in a subtractive manner. This method effectively separates
representations by increasing the distances between data points. To improve the
efficiency and effectiveness of InvGC, we propose an advanced graph topology,
LocalAdj, which only aims to increase the distances between each data point and
its nearest neighbors. To understand why InvGC works, we present a detailed
theoretical analysis, proving that the lower bound of recall will be improved
after deploying InvGC. Extensive empirical results show that InvGC and InvGC
w/LocalAdj significantly mitigate the representation degeneration problem,
thereby enhancing retrieval performance.
  Our code is available at
https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval",2310.13276v2,https://arxiv.org/pdf/2310.13276v2
"Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy
  for Language Models","Jianwei Li, Qi Lei, Wei Cheng, Dongkuan Xu","The pruning objective has recently extended beyond accuracy and sparsity to
robustness in language models. Despite this, existing methods struggle to
enhance robustness against adversarial attacks when continually increasing
model sparsity and require a retraining process. As humans step into the era of
large language models, these issues become increasingly prominent. This paper
proposes that the robustness of language models is proportional to the extent
of pre-trained knowledge they encompass. Accordingly, we introduce a
post-training pruning strategy designed to faithfully replicate the embedding
space and feature space of dense language models, aiming to conserve more
pre-trained knowledge during the pruning process. In this setup, each layer's
reconstruction error not only originates from itself but also includes
cumulative error from preceding layers, followed by an adaptive rectification.
Compared to other state-of-art baselines, our approach demonstrates a superior
balance between accuracy, sparsity, robustness, and pruning cost with BERT on
datasets SST2, IMDB, and AGNews, marking a significant stride towards robust
pruning in language models.",2310.13191v3,https://arxiv.org/pdf/2310.13191v3
Robust multimodal models have outlier features and encode more concepts,"Jonathan Crabbé, Pau Rodríguez, Vaishaal Shankar, Luca Zappella, Arno Blaas","What distinguishes robust models from non-robust ones? This question has
gained traction with the appearance of large-scale multimodal models, such as
CLIP. These models have demonstrated unprecedented robustness with respect to
natural distribution shifts. While it has been shown that such differences in
robustness can be traced back to differences in training data, so far it is not
known what that translates to in terms of what the model has learned. In this
work, we bridge this gap by probing the representation spaces of 12 robust
multimodal models with various backbones (ResNets and ViTs) and pretraining
sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two
signatures of robustness in the representation spaces of these models: (1)
Robust models exhibit outlier features characterized by their activations, with
some being several orders of magnitude above average. These outlier features
induce privileged directions in the model's representation space. We
demonstrate that these privileged directions explain most of the predictive
power of the model by pruning up to $80 \%$ of the least important
representation space directions without negative impacts on model accuracy and
robustness; (2) Robust models encode substantially more concepts in their
representation space. While this superposition of concepts allows robust models
to store much information, it also results in highly polysemantic features,
which makes their interpretation challenging. We discuss how these insights
pave the way for future research in various fields, such as model pruning and
mechanistic interpretability.",2310.13040v1,https://arxiv.org/pdf/2310.13040v1
"Towards Robust Offline Reinforcement Learning under Diverse Data
  Corruption","Rui Yang, Han Zhong, Jiawei Xu, Amy Zhang, Chongjie Zhang, Lei Han, Tong Zhang","Offline reinforcement learning (RL) presents a promising approach for
learning reinforced policies from offline datasets without the need for costly
or unsafe interactions with the environment. However, datasets collected by
humans in real-world environments are often noisy and may even be maliciously
corrupted, which can significantly degrade the performance of offline RL. In
this work, we first investigate the performance of current offline RL
algorithms under comprehensive data corruption, including states, actions,
rewards, and dynamics. Our extensive experiments reveal that implicit
Q-learning (IQL) demonstrates remarkable resilience to data corruption among
various offline RL algorithms. Furthermore, we conduct both empirical and
theoretical analyses to understand IQL's robust performance, identifying its
supervised policy learning scheme as the key factor. Despite its relative
robustness, IQL still suffers from heavy-tail targets of Q functions under
dynamics corruption. To tackle this challenge, we draw inspiration from robust
statistics to employ the Huber loss to handle the heavy-tailedness and utilize
quantile estimators to balance penalization for corrupted data and learning
stability. By incorporating these simple yet effective modifications into IQL,
we propose a more robust offline RL approach named Robust IQL (RIQL). Extensive
experiments demonstrate that RIQL exhibits highly robust performance when
subjected to diverse data corruption scenarios.",2310.12955v3,https://arxiv.org/pdf/2310.12955v3
"OODRobustBench: a Benchmark and Large-Scale Analysis of Adversarial
  Robustness under Distribution Shift","Lin Li, Yifei Wang, Chawin Sitawarin, Michael Spratling","Existing works have made great progress in improving adversarial robustness,
but typically test their method only on data from the same distribution as the
training data, i.e. in-distribution (ID) testing. As a result, it is unclear
how such robustness generalizes under input distribution shifts, i.e.
out-of-distribution (OOD) testing. This omission is concerning as such
distribution shifts are unavoidable when methods are deployed in the wild. To
address this issue we propose a benchmark named OODRobustBench to
comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts
(i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts
(i.e., unforeseen adversarial threat models). OODRobustBench is used to assess
706 robust models using 60.7K adversarial evaluations. This large-scale
analysis shows that: 1) adversarial robustness suffers from a severe OOD
generalization issue; 2) ID robustness correlates strongly with OOD robustness
in a positive linear way. The latter enables the prediction of OOD robustness
from ID robustness. We then predict and verify that existing methods are
unlikely to achieve high OOD robustness. Novel methods are therefore required
to achieve OOD robustness beyond our prediction. To facilitate the development
of these methods, we investigate a wide range of techniques and identify
several promising directions. Code and models are available at:
https://github.com/OODRobustBench/OODRobustBench.",2310.12793v2,https://arxiv.org/pdf/2310.12793v2
"On existence, uniqueness and scalability of adversarial robustness
  measures for AI classifiers",Illia Horenko,"Simply-verifiable mathematical conditions for existence, uniqueness and
explicit analytical computation of minimal adversarial paths (MAP) and minimal
adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for
generalized linear models (GLM), and for entropic AI (EAI) are formulated and
proven. Practical computation of MAP and MAD, their comparison and
interpretations for various classes of AI tools (for neuronal networks, boosted
random forests, GLM and EAI) are demonstrated on the common synthetic
benchmarks: on a double Swiss roll spiral and its extensions, as well as on the
two biomedical data problems (for the health insurance claim predictions, and
for the heart attack lethality classification). On biomedical applications it
is demonstrated how MAP provides unique minimal patient-specific
risk-mitigating interventions in the predefined subsets of accessible control
variables.",2310.14421v4,https://arxiv.org/pdf/2310.14421v4
"Classification-Aided Robust Multiple Target Tracking Using Neural
  Enhanced Message Passing","Xianglong Bai, Zengfu Wang, Quan Pan, Tao Yun, Hua Lan","We address the challenge of tracking an unknown number of targets in strong
clutter environments using measurements from a radar sensor. Leveraging the
range-Doppler spectra information, we identify the measurement classes, which
serve as additional information to enhance clutter rejection and data
association, thus bolstering the robustness of target tracking. We first
introduce a novel neural enhanced message passing approach, where the beliefs
obtained by the unified message passing are fed into the neural network as
additional information. The output beliefs are then utilized to refine the
original beliefs. Then, we propose a classification-aided robust multiple
target tracking algorithm, employing the neural enhanced message passing
technique. This algorithm is comprised of three modules: a message-passing
module, a neural network module, and a Dempster-Shafer module. The
message-passing module is used to represent the statistical model by the factor
graph and infers target kinematic states, visibility states, and data
associations based on the spatial measurement information. The neural network
module is employed to extract features from range-Doppler spectra and derive
beliefs on whether a measurement is target-generated or clutter-generated. The
Dempster-Shafer module is used to fuse the beliefs obtained from both the
factor graph and the neural network. As a result, our proposed algorithm adopts
a model-and-data-driven framework, effectively enhancing clutter suppression
and data association, leading to significant improvements in multiple target
tracking performance. We validate the effectiveness of our approach using both
simulated and real data scenarios, demonstrating its capability to handle
challenging tracking scenarios in practical radar applications.",2310.12407v1,https://arxiv.org/pdf/2310.12407v1
"Robust Graph Matching Using An Unbalanced Hierarchical Optimal Transport
  Framework","Haoran Cheng, Dixin Luo, Hongteng Xu","Graph matching is one of the most significant graph analytic tasks, which
aims to find the node correspondence across different graphs. Most existing
graph matching approaches mainly rely on topological information, whose
performances are often sub-optimal and sensitive to data noise because of not
fully leveraging the multi-modal information hidden in graphs, such as node
attributes, subgraph structures, etc. In this study, we propose a novel and
robust graph matching method based on an unbalanced hierarchical optimal
transport (UHOT) framework, which, to our knowledge, makes the first attempt to
exploit cross-modal alignment in graph matching. In principle, applying
multi-layer message passing, we represent each graph as layer-wise node
embeddings corresponding to different modalities. Given two graphs, we align
their node embeddings within the same modality and across different modalities,
respectively. Then, we infer the node correspondence by the weighted average of
all the alignment results. This method is implemented as computing the UHOT
distance between the two graphs -- each alignment is achieved by a node-level
optimal transport plan between two sets of node embeddings, and the weights of
all alignment results correspond to an unbalanced modality-level optimal
transport plan. Experiments on various graph matching tasks demonstrate the
superiority and robustness of our method compared to state-of-the-art
approaches. Our implementation is available at
https://github.com/Dixin-Lab/UHOT-GM.",2310.12081v4,https://arxiv.org/pdf/2310.12081v4
NeuroCUT: A Neural Approach for Robust Graph Partitioning,"Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, Sayan Ranu","Graph partitioning aims to divide a graph into disjoint subsets while
optimizing a specific partitioning objective. The majority of formulations
related to graph partitioning exhibit NP-hardness due to their combinatorial
nature. Conventional methods, like approximation algorithms or heuristics, are
designed for distinct partitioning objectives and fail to achieve
generalization across other important partitioning objectives. Recently machine
learning-based methods have been developed that learn directly from data.
Further, these methods have a distinct advantage of utilizing node features
that carry additional information. However, these methods assume
differentiability of target partitioning objective functions and cannot
generalize for an unknown number of partitions, i.e., they assume the number of
partitions is provided in advance. In this study, we develop NeuroCUT with two
key innovations over previous methodologies. First, by leveraging a
reinforcement learning-based framework over node representations derived from a
graph neural network and positional features, NeuroCUT can accommodate any
optimization objective, even those with non-differentiable functions. Second,
we decouple the parameter space and the partition count making NeuroCUT
inductive to any unseen number of partition, which is provided at query time.
Through empirical evaluation, we demonstrate that NeuroCUT excels in
identifying high-quality partitions, showcases strong generalization across a
wide spectrum of partitioning objectives, and exhibits strong generalization to
unseen partition count.",2310.11787v3,https://arxiv.org/pdf/2310.11787v3
"An Optimistic-Robust Approach for Dynamic Positioning of Omnichannel
  Inventories","Pavithra Harsha, Shivaram Subramanian, Ali Koc, Mahesh Ramakrishna, Brian Quanz, Dhruv Shah, Chandra Narayanaswami","We introduce a new class of data-driven and distribution-free
optimistic-robust bimodal inventory optimization (BIO) strategy to effectively
allocate inventory across a retail chain to meet time-varying, uncertain
omnichannel demand. While prior Robust optimization (RO) methods emphasize the
downside, i.e., worst-case adversarial demand, BIO also considers the upside to
remain resilient like RO while also reaping the rewards of improved
average-case performance by overcoming the presence of endogenous outliers.
This bimodal strategy is particularly valuable for balancing the tradeoff
between lost sales at the store and the costs of cross-channel e-commerce
fulfillment, which is at the core of our inventory optimization model. These
factors are asymmetric due to the heterogenous behavior of the channels, with a
bias towards the former in terms of lost-sales cost and a dependence on network
effects for the latter. We provide structural insights about the BIO solution
and how it can be tuned to achieve a preferred tradeoff between robustness and
the average-case. Our experiments show that significant benefits can be
achieved by rethinking traditional approaches to inventory management, which
are siloed by channel and location. Using a real-world dataset from a large
American omnichannel retail chain, a business value assessment during a peak
period indicates over a 15% profitability gain for BIO over RO and other
baselines while also preserving the (practical) worst case performance.",2310.12183v1,https://arxiv.org/pdf/2310.12183v1
"Adversarial Robustness Unhardening via Backdoor Attacks in Federated
  Learning","Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong","In today's data-driven landscape, the delicate equilibrium between
safeguarding user privacy and unleashing data potential stands as a paramount
concern. Federated learning, which enables collaborative model training without
necessitating data sharing, has emerged as a privacy-centric solution. This
decentralized approach brings forth security challenges, notably poisoning and
backdoor attacks where malicious entities inject corrupted data. Our research,
initially spurred by test-time evasion attacks, investigates the intersection
of adversarial training and backdoor attacks within federated learning,
introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a
subset of adversaries to intentionally undermine model robustness during
decentralized training, rendering models susceptible to a broader range of
evasion attacks. We present extensive empirical experiments evaluating ARU's
impact on adversarial training and existing robust aggregation defenses against
poisoning and backdoor attacks. Our findings inform strategies for enhancing
ARU to counter current defensive measures and highlight the limitations of
existing defenses, offering insights into bolstering defenses against ARU.",2310.11594v2,https://arxiv.org/pdf/2310.11594v2
Algorithmic Robustness,"David Jensen, Brian LaMacchia, Ufuk Topcu, Pamela Wisniewski","Algorithmic robustness refers to the sustained performance of a computational
system in the face of change in the nature of the environment in which that
system operates or in the task that the system is meant to perform. Below, we
motivate the importance of algorithmic robustness, present a conceptual
framework, and highlight the relevant areas of research for which algorithmic
robustness is relevant. Why robustness? Robustness is an important enabler of
other goals that are frequently cited in the context of public policy decisions
about computational systems, including trustworthiness, accountability,
fairness, and safety. Despite this dependence, it tends to be under-recognized
compared to these other concepts. This is unfortunate, because robustness is
often more immediately achievable than these other ultimate goals, which can be
more subjective and exacting. Thus, we highlight robustness as an important
goal for researchers, engineers, regulators, and policymakers when considering
the design, implementation, and deployment of computational systems. We urge
researchers and practitioners to elevate the attention paid to robustness when
designing and evaluating computational systems. For many key systems, the
immediate question after any demonstration of high performance should be: ""How
robust is that performance to realistic changes in the task or environment?""
Greater robustness will set the stage for systems that are more trustworthy,
accountable, fair, and safe. Toward that end, this document provides a brief
roadmap to some of the concepts and existing research around the idea of
algorithmic robustness.",2311.06275v1,https://arxiv.org/pdf/2311.06275v1
"Non-ergodicity in reinforcement learning: robustness via ergodicity
  transformations","Dominik Baumann, Erfaun Noorani, James Price, Ole Peters, Colm Connaughton, Thomas B. Schön","Envisioned application areas for reinforcement learning (RL) include
autonomous driving, precision agriculture, and finance, which all require RL
agents to make decisions in the real world. A significant challenge hindering
the adoption of RL methods in these domains is the non-robustness of
conventional algorithms. In this paper, we argue that a fundamental issue
contributing to this lack of robustness lies in the focus on the expected value
of the return as the sole ``correct'' optimization objective. The expected
value is the average over the statistical ensemble of infinitely many
trajectories. For non-ergodic returns, this average differs from the average
over a single but infinitely long trajectory. Consequently, optimizing the
expected value can lead to policies that yield exceptionally high returns with
probability zero but almost surely result in catastrophic outcomes. This
problem can be circumvented by transforming the time series of collected
returns into one with ergodic increments. This transformation enables learning
robust policies by optimizing the long-term return for individual agents rather
than the average across infinitely many trajectories. We propose an algorithm
for learning ergodicity transformations from data and demonstrate its
effectiveness in an instructive, non-ergodic environment and on standard RL
benchmarks.",2310.11335v2,https://arxiv.org/pdf/2310.11335v2
SODA: Robust Training of Test-Time Data Adaptors,"Zige Wang, Yonggang Zhang, Zhen Fang, Long Lan, Wenjing Yang, Bo Han","Adapting models deployed to test distributions can mitigate the performance
degradation caused by distribution shifts. However, privacy concerns may render
model parameters inaccessible. One promising approach involves utilizing
zeroth-order optimization (ZOO) to train a data adaptor to adapt the test data
to fit the deployed models. Nevertheless, the data adaptor trained with ZOO
typically brings restricted improvements due to the potential corruption of
data features caused by the data adaptor. To address this issue, we revisit ZOO
in the context of test-time data adaptation. We find that the issue directly
stems from the unreliable estimation of the gradients used to optimize the data
adaptor, which is inherently due to the unreliable nature of the pseudo-labels
assigned to the test data. Based on this observation, we propose
pseudo-label-robust data adaptation (SODA) to improve the performance of data
adaptation. Specifically, SODA leverages high-confidence predicted labels as
reliable labels to optimize the data adaptor with ZOO for label prediction. For
data with low-confidence predictions, SODA encourages the adaptor to preserve
data information to mitigate data corruption. Empirical results indicate that
SODA can significantly enhance the performance of deployed models in the
presence of distribution shifts without requiring access to model parameters.",2310.11093v1,https://arxiv.org/pdf/2310.11093v1
"Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults
  Detection Using Multiple Deep Learning Training Strategies and A Novel Double
  Loss Function","Khoa Tran, Lam Pham, Hai-Canh Vu","This paper presents a comprehensive analysis of motor bearing fault detection
(MBFD), which involves the task of identifying faults in a motor bearing based
on its vibration. To this end, we first propose and evaluate various machine
learning based systems for the MBFD task. Furthermore, we propose three deep
learning based systems for the MBFD task, each of which explores one of the
following training strategies: supervised learning, semi-supervised learning,
and unsupervised learning. The proposed machine learning based systems and deep
learning based systems are evaluated, compared, and then they are used to
identify the best model for the MBFD task. We conducted extensive experiments
on various benchmark datasets of motor bearing faults, including those from the
American Society for Mechanical Failure Prevention Technology (MFPT), Case
Western Reserve University Bearing Center (CWRU), and the Condition Monitoring
of Bearing Damage in Electromechanical Drive Systems from Paderborn University
(PU). The experimental results on different datasets highlight two main
contributions of this study. First, we prove that deep learning based systems
are more effective than machine learning based systems for the MBFD task.
Second, we achieve a robust and general deep learning based system with a novel
loss function for the MBFD task on several benchmark datasets, demonstrating
its potential for real-life MBFD applications.",2310.11477v1,https://arxiv.org/pdf/2310.11477v1
"Understanding Contrastive Learning via Distributionally Robust
  Optimization","Junkang Wu, Jiawei Chen, Jiancan Wu, Wentao Shi, Xiang Wang, Xiangnan He","This study reveals the inherent tolerance of contrastive learning (CL)
towards sampling bias, wherein negative samples may encompass similar semantics
(\eg labels). However, existing theories fall short in providing explanations
for this phenomenon. We bridge this research gap by analyzing CL through the
lens of distributionally robust optimization (DRO), yielding several key
insights: (1) CL essentially conducts DRO over the negative sampling
distribution, thus enabling robust performance across a variety of potential
distributions and demonstrating robustness to sampling bias; (2) The design of
the temperature $\tau$ is not merely heuristic but acts as a Lagrange
Coefficient, regulating the size of the potential distribution set; (3) A
theoretical connection is established between DRO and mutual information, thus
presenting fresh evidence for ``InfoNCE as an estimate of MI'' and a new
estimation approach for $\phi$-divergence-based generalized mutual information.
We also identify CL's potential shortcomings, including over-conservatism and
sensitivity to outliers, and introduce a novel Adjusted InfoNCE loss (ADNCE) to
mitigate these issues. It refines potential distribution, improving performance
and accelerating convergence. Extensive experiments on various domains (image,
sentence, and graphs) validate the effectiveness of the proposal. The code is
available at \url{https://github.com/junkangwu/ADNCE}.",2310.11048v1,https://arxiv.org/pdf/2310.11048v1
"Spoofing Attack Detection in the Physical Layer with Robustness to User
  Movement","Daniel Romero, Tien Ngoc Ha, Peter Gerstoft","In a spoofing attack, an attacker impersonates a legitimate user to access or
modify data belonging to the latter. Typical approaches for spoofing detection
in the physical layer declare an attack when a change is observed in certain
channel features, such as the received signal strength (RSS) measured by
spatially distributed receivers. However, since channels change over time, for
example due to user movement, such approaches are impractical. To sidestep this
limitation, this paper proposes a scheme that combines the decisions of a
position-change detector based on a deep neural network to distinguish spoofing
from movement. Building upon community detection on graphs, the sequence of
received frames is partitioned into subsequences to detect concurrent
transmissions from distinct locations. The scheme can be easily deployed in
practice since it just involves collecting a small dataset of measurements at a
few tens of locations that need not even be computed or recorded. The scheme is
evaluated on real data collected for this purpose.",2310.11043v1,https://arxiv.org/pdf/2310.11043v1
"Robust Multi-Agent Reinforcement Learning via Adversarial
  Regularization: Theoretical Foundation and Stable Algorithms","Alexander Bukharin, Yan Li, Yue Yu, Qingru Zhang, Zhehui Chen, Simiao Zuo, Chao Zhang, Songan Zhang, Tuo Zhao","Multi-Agent Reinforcement Learning (MARL) has shown promising results across
several domains. Despite this promise, MARL policies often lack robustness and
are therefore sensitive to small changes in their environment. This presents a
serious concern for the real world deployment of MARL algorithms, where the
testing environment may slightly differ from the training environment. In this
work we show that we can gain robustness by controlling a policy's Lipschitz
constant, and under mild conditions, establish the existence of a Lipschitz and
close-to-optimal policy. Based on these insights, we propose a new robust MARL
framework, ERNIE, that promotes the Lipschitz continuity of the policies with
respect to the state observations and actions by adversarial regularization.
The ERNIE framework provides robustness against noisy observations, changing
transition dynamics, and malicious actions of agents. However, ERNIE's
adversarial regularization may introduce some training instability. To reduce
this instability, we reformulate adversarial regularization as a Stackelberg
game. We demonstrate the effectiveness of the proposed framework with extensive
experiments in traffic light control and particle environments. In addition, we
extend ERNIE to mean-field MARL with a formulation based on distributionally
robust optimization that outperforms its non-robust counterpart and is of
independent interest. Our code is available at
https://github.com/abukharin3/ERNIE.",2310.10810v1,https://arxiv.org/pdf/2310.10810v1
Quantifying Assistive Robustness Via the Natural-Adversarial Frontier,"Jerry Zhi-Yang He, Zackory Erickson, Daniel S. Brown, Anca D. Dragan","Our ultimate goal is to build robust policies for robots that assist people.
What makes this hard is that people can behave unexpectedly at test time,
potentially interacting with the robot outside its training distribution and
leading to failures. Even just measuring robustness is a challenge. Adversarial
perturbations are the default, but they can paint the wrong picture: they can
correspond to human motions that are unlikely to occur during natural
interactions with people. A robot policy might fail under small adversarial
perturbations but work under large natural perturbations. We propose that
capturing robustness in these interactive settings requires constructing and
analyzing the entire natural-adversarial frontier: the Pareto-frontier of human
policies that are the best trade-offs between naturalness and low robot
performance. We introduce RIGID, a method for constructing this frontier by
training adversarial human policies that trade off between minimizing robot
reward and acting human-like (as measured by a discriminator). On an Assistive
Gym task, we use RIGID to analyze the performance of standard collaborative
Reinforcement Learning, as well as the performance of existing methods meant to
increase robustness. We also compare the frontier RIGID identifies with the
failures identified in expert adversarial interaction, and with
naturally-occurring failures during user interaction. Overall, we find evidence
that RIGID can provide a meaningful measure of robustness predictive of
deployment performance, and uncover failure cases in human-robot interaction
that are difficult to find manually. https://ood-human.github.io.",2310.10610v1,https://arxiv.org/pdf/2310.10610v1
"Orthogonal Uncertainty Representation of Data Manifold for Robust
  Long-Tailed Learning","Yanbiao Ma, Licheng Jiao, Fang Liu, Shuyuan Yang, Xu Liu, Lingling Li","In scenarios with long-tailed distributions, the model's ability to identify
tail classes is limited due to the under-representation of tail samples. Class
rebalancing, information augmentation, and other techniques have been proposed
to facilitate models to learn the potential distribution of tail classes. The
disadvantage is that these methods generally pursue models with balanced class
accuracy on the data manifold, while ignoring the ability of the model to
resist interference. By constructing noisy data manifold, we found that the
robustness of models trained on unbalanced data has a long-tail phenomenon.
That is, even if the class accuracy is balanced on the data domain, it still
has bias on the noisy data manifold. However, existing methods cannot
effectively mitigate the above phenomenon, which makes the model vulnerable in
long-tailed scenarios. In this work, we propose an Orthogonal Uncertainty
Representation (OUR) of feature embedding and an end-to-end training strategy
to improve the long-tail phenomenon of model robustness. As a general
enhancement tool, OUR has excellent compatibility with other methods and does
not require additional data generation, ensuring fast and efficient training.
Comprehensive evaluations on long-tailed datasets show that our method
significantly improves the long-tail phenomenon of robustness, bringing
consistent performance gains to other long-tailed learning methods.",2310.10090v1,https://arxiv.org/pdf/2310.10090v1
SoTTA: Robust Test-Time Adaptation on Noisy Data Streams,"Taesik Gong, Yewon Kim, Taeckyung Lee, Sorn Chottananurak, Sung-Ju Lee","Test-time adaptation (TTA) aims to address distributional shifts between
training and testing data using only unlabeled test data streams for continual
model adaptation. However, most TTA methods assume benign test streams, while
test samples could be unexpectedly diverse in the wild. For instance, an unseen
object or noise could appear in autonomous driving. This leads to a new threat
to existing TTA algorithms; we found that prior TTA algorithms suffer from
those noisy test samples as they blindly adapt to incoming samples. To address
this problem, we present Screening-out Test-Time Adaptation (SoTTA), a novel
TTA algorithm that is robust to noisy samples. The key enabler of SoTTA is
two-fold: (i) input-wise robustness via high-confidence uniform-class sampling
that effectively filters out the impact of noisy samples and (ii)
parameter-wise robustness via entropy-sharpness minimization that improves the
robustness of model parameters against large gradients from noisy samples. Our
evaluation with standard TTA benchmarks with various noisy scenarios shows that
our method outperforms state-of-the-art TTA methods under the presence of noisy
samples and achieves comparable accuracy to those methods without noisy
samples. The source code is available at https://github.com/taeckyung/SoTTA .",2310.10074v1,https://arxiv.org/pdf/2310.10074v1
Robust Collaborative Filtering to Popularity Distribution Shift,"An Zhang, Wenchang Ma, Jingnan Zheng, Xiang Wang, Tat-seng Chua","In leading collaborative filtering (CF) models, representations of users and
items are prone to learn popularity bias in the training data as shortcuts. The
popularity shortcut tricks are good for in-distribution (ID) performance but
poorly generalized to out-of-distribution (OOD) data, i.e., when popularity
distribution of test data shifts w.r.t. the training one. To close the gap,
debiasing strategies try to assess the shortcut degrees and mitigate them from
the representations. However, there exist two deficiencies: (1) when measuring
the shortcut degrees, most strategies only use statistical metrics on a single
aspect (i.e., item frequency on item and user frequency on user aspect),
failing to accommodate the compositional degree of a user-item pair; (2) when
mitigating shortcuts, many strategies assume that the test distribution is
known in advance. This results in low-quality debiased representations. Worse
still, these strategies achieve OOD generalizability with a sacrifice on ID
performance. In this work, we present a simple yet effective debiasing
strategy, PopGo, which quantifies and reduces the interaction-wise popularity
shortcut without any assumptions on the test data. It first learns a shortcut
model, which yields a shortcut degree of a user-item pair based on their
popularity representations. Then, it trains the CF model by adjusting the
predictions with the interaction-wise shortcut degrees. By taking both causal-
and information-theoretical looks at PopGo, we can justify why it encourages
the CF model to capture the critical popularity-agnostic features while leaving
the spurious popularity-relevant patterns out. We use PopGo to debias two
high-performing CF models (MF, LightGCN) on four benchmark datasets. On both ID
and OOD test sets, PopGo achieves significant gains over the state-of-the-art
debiasing strategies (e.g., DICE, MACR).",2310.10696v1,https://arxiv.org/pdf/2310.10696v1
Conformal Contextual Robust Optimization,"Yash Patel, Sahana Rayan, Ambuj Tewari","Data-driven approaches to predict-then-optimize decision-making problems seek
to mitigate the risk of uncertainty region misspecification in safety-critical
settings. Current approaches, however, suffer from considering overly
conservative uncertainty regions, often resulting in suboptimal decisionmaking.
To this end, we propose Conformal-Predict-Then-Optimize (CPO), a framework for
leveraging highly informative, nonconvex conformal prediction regions over
high-dimensional spaces based on conditional generative models, which have the
desired distribution-free coverage guarantees. Despite guaranteeing robustness,
such black-box optimization procedures alone inspire little confidence owing to
the lack of explanation of why a particular decision was found to be optimal.
We, therefore, augment CPO to additionally provide semantically meaningful
visual summaries of the uncertainty regions to give qualitative intuition for
the optimal decision. We highlight the CPO framework by demonstrating results
on a suite of simulation-based inference benchmark tasks and a vehicle routing
task based on probabilistic weather prediction.",2310.10003v1,https://arxiv.org/pdf/2310.10003v1
"Towards Deep Learning Models Resistant to Transfer-based Adversarial
  Attacks via Data-centric Robust Learning","Yulong Yang, Chenhao Lin, Xiang Ji, Qiwei Tian, Qian Li, Hongshan Yang, Zhibo Wang, Chao Shen","Transfer-based adversarial attacks raise a severe threat to real-world deep
learning systems since they do not require access to target models. Adversarial
training (AT), which is recognized as the strongest defense against white-box
attacks, has also guaranteed high robustness to (black-box) transfer-based
attacks. However, AT suffers from heavy computational overhead since it
optimizes the adversarial examples during the whole training process. In this
paper, we demonstrate that such heavy optimization is not necessary for AT
against transfer-based attacks. Instead, a one-shot adversarial augmentation
prior to training is sufficient, and we name this new defense paradigm
Data-centric Robust Learning (DRL). Our experimental results show that DRL
outperforms widely-used AT techniques (e.g., PGD-AT, TRADES, EAT, and FAT) in
terms of black-box robustness and even surpasses the top-1 defense on
RobustBench when combined with diverse data augmentations and loss
regularizations. We also identify other benefits of DRL, for instance, the
model generalization capability and robust fairness.",2310.09891v1,https://arxiv.org/pdf/2310.09891v1
Secure and Robust Communications for Cislunar Space Networks,"Selen Gecgel Cetin, Gunes Karabulut Kurt, Angeles Vazquez-Castro","There is no doubt that the Moon has become the center of interest for
commercial and international actors. Over the past decade, the number of
planned long-term missions has increased dramatically. This makes the
establishment of cislunar space networks (CSNs) crucial to orchestrate
uninterrupted communications between the Moon and Earth. However, there are
numerous challenges, unknowns, and uncertainties associated with cislunar
communications that may pose various risks to lunar missions. In this study, we
aim to address these challenges for cislunar communications by proposing a
machine learning-based cislunar space domain awareness (SDA) capability that
enables robust and secure communications. To this end, we first propose a
detailed channel model for selected cislunar scenarios. Secondly, we propose
two types of interference that could model anomalies that occur in cislunar
space and are so far known only to a limited extent. Finally, we discuss our
cislunar SDA to work in conjunction with the spacecraft communication system.
Our proposed cislunar SDA, involving heuristic learning capabilities with
machine learning algorithms, detects interference models with over 96%
accuracy. The results demonstrate the promising performance of our cislunar SDA
approach for secure and robust cislunar communication.",2310.09835v1,https://arxiv.org/pdf/2310.09835v1
"Robust Multi-Agent Reinforcement Learning by Mutual Information
  Regularization","Simin Li, Ruixiao Xu, Jingqiao Xiu, Yuwei Zheng, Pu Feng, Yaodong Yang, Xianglong Liu","In multi-agent reinforcement learning (MARL), ensuring robustness against
unpredictable or worst-case actions by allies is crucial for real-world
deployment. Existing robust MARL methods either approximate or enumerate all
possible threat scenarios against worst-case adversaries, leading to
computational intensity and reduced robustness. In contrast, human learning
efficiently acquires robust behaviors in daily life without preparing for every
possible threat. Inspired by this, we frame robust MARL as an inference
problem, with worst-case robustness implicitly optimized under all threat
scenarios via off-policy evaluation. Within this framework, we demonstrate that
Mutual Information Regularization as Robust Regularization (MIR3) during
routine training is guaranteed to maximize a lower bound on robustness, without
the need for adversaries. Further insights show that MIR3 acts as an
information bottleneck, preventing agents from over-reacting to others and
aligning policies with robust action priors. In the presence of worst-case
adversaries, our MIR3 significantly surpasses baseline methods in robustness
and training efficiency while maintaining cooperative performance in StarCraft
II and robot swarm control. When deploying the robot swarm control algorithm in
the real world, our method also outperforms the best baseline by 14.29%.",2310.09833v3,https://arxiv.org/pdf/2310.09833v3
"Communication Compression for Byzantine Robust Learning: New Efficient
  Algorithms and Improved Rates","Ahmad Rammal, Kaja Gruntkowska, Nikita Fedin, Eduard Gorbunov, Peter Richtárik","Byzantine robustness is an essential feature of algorithms for certain
distributed optimization problems, typically encountered in
collaborative/federated learning. These problems are usually huge-scale,
implying that communication compression is also imperative for their
resolution. These factors have spurred recent algorithmic and theoretical
developments in the literature of Byzantine-robust learning with compression.
In this paper, we contribute to this research area in two main directions.
First, we propose a new Byzantine-robust method with compression -
Byz-DASHA-PAGE - and prove that the new method has better convergence rate (for
non-convex and Polyak-Lojasiewicz smooth optimization problems), smaller
neighborhood size in the heterogeneous case, and tolerates more Byzantine
workers under over-parametrization than the previous method with SOTA
theoretical convergence guarantees (Byz-VR-MARINA). Secondly, we develop the
first Byzantine-robust method with communication compression and error feedback
- Byz-EF21 - along with its bidirectional compression version - Byz-EF21-BC -
and derive the convergence rates for these methods for non-convex and
Polyak-Lojasiewicz smooth case. We test the proposed methods and illustrate our
theoretical findings in the numerical experiments.",2310.09804v2,https://arxiv.org/pdf/2310.09804v2
"ASSERT: Automated Safety Scenario Red Teaming for Evaluating the
  Robustness of Large Language Models","Alex Mei, Sharon Levy, William Yang Wang","As large language models are integrated into society, robustness toward a
suite of prompts is increasingly important to maintain reliability in a
high-variance environment.Robustness evaluations must comprehensively
encapsulate the various settings in which a user may invoke an intelligent
system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming,
consisting of three methods -- semantically aligned augmentation, target
bootstrapping, and adversarial knowledge injection. For robust safety
evaluation, we apply these methods in the critical domain of AI safety to
algorithmically generate a test suite of prompts covering diverse robustness
settings -- semantic equivalence, related scenarios, and adversarial. We
partition our prompts into four safety domains for a fine-grained analysis of
how the domain affects model performance. Despite dedicated safeguards in
existing state-of-the-art models, we find statistically significant performance
differences of up to 11% in absolute classification accuracy among semantically
related scenarios and error rates of up to 19% absolute error in zero-shot
adversarial settings, raising concerns for users' physical safety.",2310.09624v2,https://arxiv.org/pdf/2310.09624v2
"Protein 3D Graph Structure Learning for Robust Structure-based Protein
  Property Prediction","Yufei Huang, Siyuan Li, Jin Su, Lirong Wu, Odin Zhang, Haitao Lin, Jingqi Qi, Zihan Liu, Zhangyang Gao, Yuyang Liu, Jiangbin Zheng, Stan. ZQ. Li","Protein structure-based property prediction has emerged as a promising
approach for various biological tasks, such as protein function prediction and
sub-cellular location estimation. The existing methods highly rely on
experimental protein structure data and fail in scenarios where these data are
unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were
utilized as alternatives. However, we observed that current practices, which
simply employ accurately predicted structures during inference, suffer from
notable degradation in prediction accuracy. While similar phenomena have been
extensively studied in general fields (e.g., Computer Vision) as model
robustness, their impact on protein property prediction remains unexplored. In
this paper, we first investigate the reason behind the performance decrease
when utilizing predicted structures, attributing it to the structure embedding
bias from the perspective of structure representation learning. To study this
problem, we identify a Protein 3D Graph Structure Learning Problem for Robust
Protein Property Prediction (PGSL-RP3), collect benchmark datasets, and present
a protein Structure embedding Alignment Optimization framework (SAO) to
mitigate the problem of structure embedding bias between the predicted and
experimental protein structures. Extensive experiments have shown that our
framework is model-agnostic and effective in improving the property prediction
of both predicted structures and experimental structures. The benchmark
datasets and codes will be released to benefit the community.",2310.11466v2,https://arxiv.org/pdf/2310.11466v2
"Assessing and Enhancing the Robustness of Large Language Models with
  Task Structure Variations for Logical Reasoning","Qiming Bao, Gael Gendron, Alex Yuxuan Peng, Wanjun Zhong, Neset Tan, Yang Chen, Michael Witbrock, Jiamou Liu","Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and
GPT-4, have advanced the performance of AI systems on various natural language
processing tasks to human-like levels. However, their generalisation and
robustness when performing logical reasoning has not been sufficiently
assessed. To comprehensively evaluate this ability, we develop three new
logical reasoning datasets named ""ReClor-plus"", ""LogiQA-plus"" and
""LogiQAv2-plus"" that extend standard logical reasoning datasets to evaluate the
robustness of the LLM's reasoning. For each, we create three subsets: the first
with randomly shuffled options, the second with the correct choices replaced by
""none of the other options is correct"", and the third with a combination of
shuffling and substitution. Experiments on these datasets show that these
simple augmentations greatly hinder the models' performance. Despite their high
performance on the original publicly available datasets, we find that all
models perform poorly on these newly constructed datasets. We also demonstrate
that introducing task variations into the training set can markedly improve the
model's performance on both the original and our developed datasets. Finally,
we show that applying logic-driven data augmentation for fine-tuning and
prompting can enhance generalisation in both discriminative and generative
models, offering a path to improving their robustness for tasks involving
logical reasoning. Source code and data are made publicly available at
https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning.",2310.09430v4,https://arxiv.org/pdf/2310.09430v4
Is Certifying $\ell_p$ Robustness Still Worthwhile?,"Ravi Mangal, Klas Leino, Zifan Wang, Kai Hu, Weicheng Yu, Corina Pasareanu, Anupam Datta, Matt Fredrikson","Over the years, researchers have developed myriad attacks that exploit the
ubiquity of adversarial examples, as well as defenses that aim to guard against
the security vulnerabilities posed by such attacks. Of particular interest to
this paper are defenses that provide provable guarantees against the class of
$\ell_p$-bounded attacks. Certified defenses have made significant progress,
taking robustness certification from toy models and datasets to large-scale
problems like ImageNet classification. While this is undoubtedly an interesting
academic problem, as the field has matured, its impact in practice remains
unclear, thus we find it useful to revisit the motivation for continuing this
line of research. There are three layers to this inquiry, which we address in
this paper: (1) why do we care about robustness research? (2) why do we care
about the $\ell_p$-bounded threat model? And (3) why do we care about
certification as opposed to empirical defenses? In brief, we take the position
that local robustness certification indeed confers practical value to the field
of machine learning. We focus especially on the latter two questions from
above. With respect to the first of the two, we argue that the $\ell_p$-bounded
threat model acts as a minimal requirement for safe application of models in
security-critical domains, while at the same time, evidence has mounted
suggesting that local robustness may lead to downstream external benefits not
immediately related to robustness. As for the second, we argue that (i)
certification provides a resolution to the cat-and-mouse game of adversarial
attacks; and furthermore, that (ii) perhaps contrary to popular belief, there
may not exist a fundamental trade-off between accuracy, robustness, and
certifiability, while moreover, certified training techniques constitute a
particularly promising way for learning robust models.",2310.09361v1,https://arxiv.org/pdf/2310.09361v1
"SiamAF: Learning Shared Information from ECG and PPG Signals for Robust
  Atrial Fibrillation Detection","Zhicheng Guo, Cheng Ding, Duc H. Do, Amit Shah, Randall J. Lee, Xiao Hu, Cynthia Rudin","Atrial fibrillation (AF) is the most common type of cardiac arrhythmia. It is
associated with an increased risk of stroke, heart failure, and other
cardiovascular complications, but can be clinically silent. Passive AF
monitoring with wearables may help reduce adverse clinical outcomes related to
AF. Detecting AF in noisy wearable data poses a significant challenge, leading
to the emergence of various deep learning techniques. Previous deep learning
models learn from a single modality, either electrocardiogram (ECG) or
photoplethysmography (PPG) signals. However, deep learning models often
struggle to learn generalizable features and rely on features that are more
susceptible to corruption from noise, leading to sub-optimal performances in
certain scenarios, especially with low-quality signals. Given the increasing
availability of ECG and PPG signal pairs from wearables and bedside monitors,
we propose a new approach, SiamAF, leveraging a novel Siamese network
architecture and joint learning loss function to learn shared information from
both ECG and PPG signals. At inference time, the proposed model is able to
predict AF from either PPG or ECG and outperforms baseline methods on three
external test sets. It learns medically relevant features as a result of our
novel architecture design. The proposed model also achieves comparable
performance to traditional learning regimes while requiring much fewer training
labels, providing a potential approach to reduce future reliance on manual
labeling.",2310.09203v2,https://arxiv.org/pdf/2310.09203v2
"On the Over-Memorization During Natural, Robust and Catastrophic
  Overfitting","Runqi Lin, Chaojian Yu, Bo Han, Tongliang Liu","Overfitting negatively impacts the generalization ability of deep neural
networks (DNNs) in both natural and adversarial training. Existing methods
struggle to consistently address different types of overfitting, typically
designing strategies that focus separately on either natural or adversarial
patterns. In this work, we adopt a unified perspective by solely focusing on
natural patterns to explore different types of overfitting. Specifically, we
examine the memorization effect in DNNs and reveal a shared behaviour termed
over-memorization, which impairs their generalization capacity. This behaviour
manifests as DNNs suddenly becoming high-confidence in predicting certain
training patterns and retaining a persistent memory for them. Furthermore, when
DNNs over-memorize an adversarial pattern, they tend to simultaneously exhibit
high-confidence prediction for the corresponding natural pattern. These
findings motivate us to holistically mitigate different types of overfitting by
hindering the DNNs from over-memorization training patterns. To this end, we
propose a general framework, Distraction Over-Memorization (DOM), which
explicitly prevents over-memorization by either removing or augmenting the
high-confidence natural patterns. Extensive experiments demonstrate the
effectiveness of our proposed method in mitigating overfitting across various
training paradigms.",2310.08847v3,https://arxiv.org/pdf/2310.08847v3
"Robustness to Multi-Modal Environment Uncertainty in MARL using
  Curriculum Learning","Aakriti Agrawal, Rohith Aralikatti, Yanchao Sun, Furong Huang","Multi-agent reinforcement learning (MARL) plays a pivotal role in tackling
real-world challenges. However, the seamless transition of trained policies
from simulations to real-world requires it to be robust to various
environmental uncertainties. Existing works focus on finding Nash Equilibrium
or the optimal policy under uncertainty in one environment variable (i.e.
action, state or reward). This is because a multi-agent system itself is highly
complex and unstationary. However, in real-world situation uncertainty can
occur in multiple environment variables simultaneously. This work is the first
to formulate the generalised problem of robustness to multi-modal environment
uncertainty in MARL. To this end, we propose a general robust training approach
for multi-modal uncertainty based on curriculum learning techniques. We handle
two distinct environmental uncertainty simultaneously and present extensive
results across both cooperative and competitive MARL environments,
demonstrating that our approach achieves state-of-the-art levels of robustness.",2310.08746v1,https://arxiv.org/pdf/2310.08746v1
Provably Robust Cost-Sensitive Learning via Randomized Smoothing,"Yuan Xin, Michael Backes, Xiao Zhang","We study the problem of robust learning against adversarial perturbations
under cost-sensitive scenarios, where the potential harm of different types of
misclassifications is encoded in a cost matrix. Existing approaches are either
empirical and cannot certify robustness or suffer from inherent scalability
issues. In this work, we investigate whether randomized smoothing, a scalable
framework for robustness certification, can be leveraged to certify and train
for cost-sensitive robustness. Built upon the notion of cost-sensitive
certified radius, we first illustrate how to adapt the standard certification
algorithm of randomized smoothing to produce tight robustness certificates for
any binary cost matrix, and then develop a robust training method to promote
certified cost-sensitive robustness while maintaining the model's overall
accuracy. Through extensive experiments on image benchmarks, we demonstrate the
superiority of our proposed certification algorithm and training method under
various cost-sensitive scenarios. Our implementation is available as open
source code at: https://github.com/TrustMLRG/CS-RS.",2310.08732v2,https://arxiv.org/pdf/2310.08732v2
Towards Robust Multi-Modal Reasoning via Model Selection,"Xiangyan Liu, Rongxue Li, Wei Ji, Tao Lin","The reasoning capabilities of LLM (Large Language Model) are widely
acknowledged in recent research, inspiring studies on tool learning and
autonomous agents. LLM serves as the ""brain"" of the agent, orchestrating
multiple tools for collaborative multi-step task solving. Unlike methods
invoking tools like calculators or weather APIs for straightforward tasks,
multi-modal agents excel by integrating diverse AI models for complex
challenges. However, current multi-modal agents neglect the significance of
model selection: they primarily focus on the planning and execution phases, and
will only invoke predefined task-specific models for each subtask, making the
execution fragile. Meanwhile, other traditional model selection methods are
either incompatible with or suboptimal for the multi-modal agent scenarios, due
to ignorance of dependencies among subtasks arising by multi-step reasoning. To
this end, we identify the key challenges therein and propose the $\textit{M}^3$
framework as a plug-in with negligible runtime overhead at test-time. This
framework improves model selection and bolsters the robustness of multi-modal
agents in multi-step reasoning. In the absence of suitable benchmarks, we
create MS-GQA, a new dataset specifically designed to investigate the model
selection challenge in multi-modal agents. Our experiments reveal that our
framework enables dynamic model selection, considering both user inputs and
subtask dependencies, thereby robustifying the overall reasoning process. Our
code and benchmark: https://github.com/LINs-lab/M3.",2310.08446v2,https://arxiv.org/pdf/2310.08446v2
2SFGL: A Simple And Robust Protocol For Graph-Based Fraud Detection,"Zhirui Pan, Guangzhong Wang, Zhaoning Li, Lifeng Chen, Yang Bian, Zhongyuan Lai","Financial crime detection using graph learning improves financial safety and
efficiency. However, criminals may commit financial crimes across different
institutions to avoid detection, which increases the difficulty of detection
for financial institutions which use local data for graph learning. As most
financial institutions are subject to strict regulations in regards to data
privacy protection, the training data is often isolated and conventional
learning technology cannot handle the problem. Federated learning (FL) allows
multiple institutions to train a model without revealing their datasets to each
other, hence ensuring data privacy protection. In this paper, we proposes a
novel two-stage approach to federated graph learning (2SFGL): The first stage
of 2SFGL involves the virtual fusion of multiparty graphs, and the second
involves model training and inference on the virtual graph. We evaluate our
framework on a conventional fraud detection task based on the
FraudAmazonDataset and FraudYelpDataset. Experimental results show that
integrating and applying a GCN (Graph Convolutional Network) with our 2SFGL
framework to the same task results in a 17.6\%-30.2\% increase in performance
on several typical metrics compared to the case only using FedAvg, while
integrating GraphSAGE with 2SFGL results in a 6\%-16.2\% increase in
performance compared to the case only using FedAvg. We conclude that our
proposed framework is a robust and simple protocol which can be simply
integrated to pre-existing graph-based fraud detection methods.",2310.08335v1,https://arxiv.org/pdf/2310.08335v1
"XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness
  Evaluation","Qiang Li, Dan Zhang, Shengzhao Lei, Xun Zhao, Porawit Kamnoedboon, WeiWei Li, Junhao Dong, Shuyan Li","Despite the promising performance of existing visual models on public
benchmarks, the critical assessment of their robustness for real-world
applications remains an ongoing challenge. To bridge this gap, we propose an
explainable visual dataset, XIMAGENET-12, to evaluate the robustness of visual
models. XIMAGENET-12 consists of over 200K images with 15,410 manual semantic
annotations. Specifically, we deliberately selected 12 categories from
ImageNet, representing objects commonly encountered in practical life. To
simulate real-world situations, we incorporated six diverse scenarios, such as
overexposure, blurring, and color changes, etc. We further develop a
quantitative criterion for robustness assessment, allowing for a nuanced
understanding of how visual models perform under varying conditions, notably in
relation to the background. We make the XIMAGENET-12 dataset and its
corresponding code openly accessible at
\url{https://sites.google.com/view/ximagenet-12/home}. We expect the
introduction of the XIMAGENET-12 dataset will empower researchers to thoroughly
evaluate the robustness of their visual models under challenging conditions.",2310.08182v2,https://arxiv.org/pdf/2310.08182v2
Robust 1-bit Compressed Sensing with Iterative Hard Thresholding,"Namiko Matsumoto, Arya Mazumdar","In 1-bit compressed sensing, the aim is to estimate a $k$-sparse unit vector
$x\in S^{n-1}$ within an $\epsilon$ error (in $\ell_2$) from minimal number of
linear measurements that are quantized to just their signs, i.e., from
measurements of the form $y = \mathrm{Sign}(\langle a, x\rangle).$ In this
paper, we study a noisy version where a fraction of the measurements can be
flipped, potentially by an adversary. In particular, we analyze the Binary
Iterative Hard Thresholding (BIHT) algorithm, a proximal gradient descent on a
properly defined loss function used for 1-bit compressed sensing, in this noisy
setting. It is known from recent results that, with
$\tilde{O}(\frac{k}{\epsilon})$ noiseless measurements, BIHT provides an
estimate within $\epsilon$ error. This result is optimal and universal, meaning
one set of measurements work for all sparse vectors. In this paper, we show
that BIHT also provides better results than all known methods for the noisy
setting. We show that when up to $\tau$-fraction of the sign measurements are
incorrect (adversarial error), with the same number of measurements as before,
BIHT agnostically provides an estimate of $x$ within an
$\tilde{O}(\epsilon+\tau)$ error, maintaining the universality of measurements.
This establishes stability of iterative hard thresholding in the presence of
measurement error. To obtain the result, we use the restricted approximate
invertibility of Gaussian matrices, as well as a tight analysis of the
high-dimensional geometry of the adversarially corrupted measurements.",2310.08019v1,https://arxiv.org/pdf/2310.08019v1
"PG-NeuS: Robust and Efficient Point Guidance for Multi-View Neural
  Surface Reconstruction","Chen Zhang, Wanjuan Su, Qingshan Xu, Wenbing Tao","Recently, learning multi-view neural surface reconstruction with the
supervision of point clouds or depth maps has been a promising way. However,
due to the underutilization of prior information, current methods still
struggle with the challenges of limited accuracy and excessive time complexity.
In addition, prior data perturbation is also an important but rarely considered
issue. To address these challenges, we propose a novel point-guided method
named PG-NeuS, which achieves accurate and efficient reconstruction while
robustly coping with point noise. Specifically, aleatoric uncertainty of the
point cloud is modeled to capture the distribution of noise, leading to noise
robustness. Furthermore, a Neural Projection module connecting points and
images is proposed to add geometric constraints to implicit surface, achieving
precise point guidance. To better compensate for geometric bias between volume
rendering and point modeling, high-fidelity points are filtered into a Bias
Network to further improve details representation. Benefiting from the
effective point guidance, even with a lightweight network, the proposed PG-NeuS
achieves fast convergence with an impressive 11x speedup compared to NeuS.
Extensive experiments show that our method yields high-quality surfaces with
high efficiency, especially for fine-grained details and smooth regions,
outperforming the state-of-the-art methods. Moreover, it exhibits strong
robustness to noisy data and sparse data.",2310.07997v2,https://arxiv.org/pdf/2310.07997v2
"Promoting Robustness of Randomized Smoothing: Two Cost-Effective
  Approaches","Linbo Liu, Trong Nghia Hoang, Lam M. Nguyen, Tsui-Wei Weng","Randomized smoothing has recently attracted attentions in the field of
adversarial robustness to provide provable robustness guarantees on smoothed
neural network classifiers. However, existing works show that vanilla
randomized smoothing usually does not provide good robustness performance and
often requires (re)training techniques on the base classifier in order to boost
the robustness of the resulting smoothed classifier. In this work, we propose
two cost-effective approaches to boost the robustness of randomized smoothing
while preserving its clean performance. The first approach introduces a new
robust training method AdvMacerwhich combines adversarial training and
robustness certification maximization for randomized smoothing. We show that
AdvMacer can improve the robustness performance of randomized smoothing
classifiers compared to SOTA baselines, while being 3x faster to train than
MACER baseline. The second approach introduces a post-processing method EsbRS
which greatly improves the robustness certificate based on building model
ensembles. We explore different aspects of model ensembles that has not been
studied by prior works and propose a novel design methodology to further
improve robustness of the ensemble based on our theoretical analysis.",2310.07780v1,https://arxiv.org/pdf/2310.07780v1
"RobustGEC: Robust Grammatical Error Correction Against Subtle Context
  Perturbation","Yue Zhang, Leyang Cui, Enbo Zhao, Wei Bi, Shuming Shi","Grammatical Error Correction (GEC) systems play a vital role in assisting
people with their daily writing tasks. However, users may sometimes come across
a GEC system that initially performs well but fails to correct errors when the
inputs are slightly modified. To ensure an ideal user experience, a reliable
GEC system should have the ability to provide consistent and accurate
suggestions when encountering irrelevant context perturbations, which we refer
to as context robustness. In this paper, we introduce RobustGEC, a benchmark
designed to evaluate the context robustness of GEC systems. RobustGEC comprises
5,000 GEC cases, each with one original error-correct sentence pair and five
variants carefully devised by human annotators. Utilizing RobustGEC, we reveal
that state-of-the-art GEC systems still lack sufficient robustness against
context perturbations. In addition, we propose a simple yet effective method
for remitting this issue.",2310.07299v1,https://arxiv.org/pdf/2310.07299v1
Robust Safe Reinforcement Learning under Adversarial Disturbances,"Zeyang Li, Chuxiong Hu, Shengbo Eben Li, Jia Cheng, Yunan Wang","Safety is a primary concern when applying reinforcement learning to
real-world control tasks, especially in the presence of external disturbances.
However, existing safe reinforcement learning algorithms rarely account for
external disturbances, limiting their applicability and robustness in practice.
To address this challenge, this paper proposes a robust safe reinforcement
learning framework that tackles worst-case disturbances. First, this paper
presents a policy iteration scheme to solve for the robust invariant set, i.e.,
a subset of the safe set, where persistent safety is only possible for states
within. The key idea is to establish a two-player zero-sum game by leveraging
the safety value function in Hamilton-Jacobi reachability analysis, in which
the protagonist (i.e., control inputs) aims to maintain safety and the
adversary (i.e., external disturbances) tries to break down safety. This paper
proves that the proposed policy iteration algorithm converges monotonically to
the maximal robust invariant set. Second, this paper integrates the proposed
policy iteration scheme into a constrained reinforcement learning algorithm
that simultaneously synthesizes the robust invariant set and uses it for
constrained policy optimization. This algorithm tackles both optimality and
safety, i.e., learning a policy that attains high rewards while maintaining
safety under worst-case disturbances. Experiments on classic control tasks show
that the proposed method achieves zero constraint violation with learned
worst-case adversarial disturbances, while other baseline algorithms violate
the safety constraints substantially. Our proposed method also attains
comparable performance as the baselines even in the absence of the adversary.",2310.07207v1,https://arxiv.org/pdf/2310.07207v1
"Investigating the Adversarial Robustness of Density Estimation Using the
  Probability Flow ODE","Marius Arvinte, Cory Cornelius, Jason Martin, Nageen Himayat","Beyond their impressive sampling capabilities, score-based diffusion models
offer a powerful analysis tool in the form of unbiased density estimation of a
query sample under the training data distribution. In this work, we investigate
the robustness of density estimation using the probability flow (PF) neural
ordinary differential equation (ODE) model against gradient-based likelihood
maximization attacks and the relation to sample complexity, where the
compressed size of a sample is used as a measure of its complexity. We
introduce and evaluate six gradient-based log-likelihood maximization attacks,
including a novel reverse integration attack. Our experimental evaluations on
CIFAR-10 show that density estimation using the PF ODE is robust against
high-complexity, high-likelihood attacks, and that in some cases adversarial
samples are semantically meaningful, as expected from a robust estimator.",2310.07084v1,https://arxiv.org/pdf/2310.07084v1
"Comparing the Robustness of Modern No-Reference Image- and Video-Quality
  Metrics to Adversarial Attacks","Anastasia Antsiferova, Khaled Abud, Aleksandr Gushchin, Ekaterina Shumitskaya, Sergey Lavrushkin, Dmitriy Vatolin","Nowadays, neural-network-based image- and video-quality metrics perform
better than traditional methods. However, they also became more vulnerable to
adversarial attacks that increase metrics' scores without improving visual
quality. The existing benchmarks of quality metrics compare their performance
in terms of correlation with subjective quality and calculation time.
Nonetheless, the adversarial robustness of image-quality metrics is also an
area worth researching. This paper analyses modern metrics' robustness to
different adversarial attacks. We adapted adversarial attacks from computer
vision tasks and compared attacks' efficiency against 15 no-reference image-
and video-quality metrics. Some metrics showed high resistance to adversarial
attacks, which makes their usage in benchmarks safer than vulnerable metrics.
The benchmark accepts submissions of new metrics for researchers who want to
make their metrics more robust to attacks or to find such metrics for their
needs. The latest results can be found online:
https://videoprocessing.ai/benchmarks/metrics-robustness.html.",2310.06958v4,https://arxiv.org/pdf/2310.06958v4
"A Variational Autoencoder Framework for Robust, Physics-Informed
  Cyberattack Recognition in Industrial Cyber-Physical Systems","Navid Aftabi, Dan Li, Paritosh Ramanan","Cybersecurity of Industrial Cyber-Physical Systems is drawing significant
concerns as data communication increasingly leverages wireless networks. A lot
of data-driven methods were develope for detecting cyberattacks, but few are
focused on distinguishing them from equipment faults. In this paper, we develop
a data-driven framework that can be used to detect, diagnose, and localize a
type of cyberattack called covert attacks on networked industrial control
systems. The framework has a hybrid design that combines a variational
autoencoder (VAE), a recurrent neural network (RNN), and a Deep Neural Network
(DNN). This data-driven framework considers the temporal behavior of a generic
physical system that extracts features from the time series of the sensor
measurements that can be used for detecting covert attacks, distinguishing them
from equipment faults, as well as localize the attack/fault. We evaluate the
performance of the proposed method through a realistic simulation study on a
networked power transmission system as a typical example of ICS. We compare the
performance of the proposed method with the traditional model-based method to
show its applicability and efficacy.",2310.06948v1,https://arxiv.org/pdf/2310.06948v1
"Robustness May be More Brittle than We Think under Different Degrees of
  Distribution Shifts","Kaican Li, Yifan Zhang, Lanqing Hong, Zhenguo Li, Nevin L. Zhang","Out-of-distribution (OOD) generalization is a complicated problem due to the
idiosyncrasies of possible distribution shifts between training and test
domains. Most benchmarks employ diverse datasets to address this issue;
however, the degree of the distribution shift between the training domains and
the test domains of each dataset remains largely fixed. This may lead to biased
conclusions that either underestimate or overestimate the actual OOD
performance of a model. Our study delves into a more nuanced evaluation setting
that covers a broad range of shift degrees. We show that the robustness of
models can be quite brittle and inconsistent under different degrees of
distribution shifts, and therefore one should be more cautious when drawing
conclusions from evaluations under a limited range of degrees. In addition, we
observe that large-scale pre-trained models, such as CLIP, are sensitive to
even minute distribution shifts of novel downstream tasks. This indicates that
while pre-trained representations may help improve downstream in-distribution
performance, they could have minimal or even adverse effects on generalization
in certain OOD scenarios of the downstream task if not used properly. In light
of these findings, we encourage future research to conduct evaluations across a
broader range of shift degrees whenever possible.",2310.06622v2,https://arxiv.org/pdf/2310.06622v2
FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics,"Yupei Du, Albert Gatt, Dong Nguyen","Despite the massive success of fine-tuning Pre-trained Language Models
(PLMs), they remain susceptible to out-of-distribution input. Dataset
cartography is a simple yet effective dual-model approach that improves the
robustness of fine-tuned PLMs. It involves fine-tuning a model on the original
training set (i.e. reference model), selecting a subset of important training
instances based on the training dynamics, and fine-tuning again only on these
selected examples (i.e. main model). However, this approach requires
fine-tuning the same model twice, which is computationally expensive for large
PLMs. In this paper, we show that (1) training dynamics are highly transferable
across model sizes and pre-training methods, and that (2) fine-tuning main
models using these selected training instances achieves higher training
efficiency than empirical risk minimization (ERM). Building on these
observations, we propose a novel fine-tuning approach: Fine-Tuning by
transFerring Training dynamics (FTFT). Compared with dataset cartography, FTFT
uses more efficient reference models and aggressive early stopping. FTFT
achieves robustness improvements over ERM while lowering the training cost by
up to $\sim 50\%$.",2310.06588v2,https://arxiv.org/pdf/2310.06588v2
"Revisit Input Perturbation Problems for LLMs: A Unified Robustness
  Evaluation Framework for Noisy Slot Filling Task","Guanting Dong, Jinxu Zhao, Tingfeng Hui, Daichi Guo, Wenlong Wan, Boqi Feng, Yueyan Qiu, Zhuoma Gongque, Keqing He, Zechen Wang, Weiran Xu","With the increasing capabilities of large language models (LLMs), these
high-performance models have achieved state-of-the-art results on a wide range
of natural language processing (NLP) tasks. However, the models' performance on
commonly-used benchmark datasets often fails to accurately reflect their
reliability and robustness when applied to real-world noisy data. To address
these challenges, we propose a unified robustness evaluation framework based on
the slot-filling task to systematically evaluate the dialogue understanding
capability of LLMs in diverse input perturbation scenarios. Specifically, we
construct a input perturbation evaluation dataset, Noise-LLM, which contains
five types of single perturbation and four types of mixed perturbation data.
Furthermore, we utilize a multi-level data augmentation method (character,
word, and sentence levels) to construct a candidate data pool, and carefully
design two ways of automatic task demonstration construction strategies
(instance-level and entity-level) with various prompt templates. Our aim is to
assess how well various robustness methods of LLMs perform in real-world noisy
scenarios. The experiments have demonstrated that the current open-source LLMs
generally achieve limited perturbation robustness performance. Based on these
experimental observations, we make some forward-looking suggestions to fuel the
research in this direction.",2310.06504v1,https://arxiv.org/pdf/2310.06504v1
Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach,"Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, Wee Peng Tay","Graph neural networks (GNNs) are vulnerable to adversarial perturbations,
including those that affect both node features and graph topology. This paper
investigates GNNs derived from diverse neural flows, concentrating on their
connection to various stability notions such as BIBO stability, Lyapunov
stability, structural stability, and conservative stability. We argue that
Lyapunov stability, despite its common use, does not necessarily ensure
adversarial robustness. Inspired by physics principles, we advocate for the use
of conservative Hamiltonian neural flows to construct GNNs that are robust to
adversarial attacks. The adversarial robustness of different neural flow GNNs
is empirically compared on several benchmark datasets under a variety of
adversarial attacks. Extensive numerical experiments demonstrate that GNNs
leveraging conservative Hamiltonian flows with Lyapunov stability substantially
improve robustness against adversarial perturbations. The implementation code
of experiments is available at
https://github.com/zknus/NeurIPS-2023-HANG-Robustness.",2310.06396v1,https://arxiv.org/pdf/2310.06396v1
"What Makes for Robust Multi-Modal Models in the Face of Missing
  Modalities?","Siting Li, Chenzhuang Du, Yue Zhao, Yu Huang, Hang Zhao","With the growing success of multi-modal learning, research on the robustness
of multi-modal models, especially when facing situations with missing
modalities, is receiving increased attention. Nevertheless, previous studies in
this domain exhibit certain limitations, as they often lack theoretical
insights or their methodologies are tied to specific network architectures or
modalities. We model the scenarios of multi-modal models encountering missing
modalities from an information-theoretic perspective and illustrate that the
performance ceiling in such scenarios can be approached by efficiently
utilizing the information inherent in non-missing modalities. In practice,
there are two key aspects: (1) The encoder should be able to extract
sufficiently good features from the non-missing modality; (2) The extracted
features should be robust enough not to be influenced by noise during the
fusion process across modalities. To this end, we introduce Uni-Modal Ensemble
with Missing Modality Adaptation (UME-MMA). UME-MMA employs uni-modal
pre-trained weights for the multi-modal model to enhance feature extraction and
utilizes missing modality data augmentation techniques to better adapt to
situations with missing modalities. Apart from that, UME-MMA, built on a
late-fusion learning framework, allows for the plug-and-play use of various
encoders, making it suitable for a wide range of modalities and enabling
seamless integration of large-scale pre-trained encoders to further enhance
performance. And we demonstrate UME-MMA's effectiveness in audio-visual
datasets~(e.g., AV-MNIST, Kinetics-Sound, AVE) and vision-language
datasets~(e.g., MM-IMDB, UPMC Food101).",2310.06383v1,https://arxiv.org/pdf/2310.06383v1
"Leveraging Diffusion-Based Image Variations for Robust Training on
  Poisoned Data","Lukas Struppek, Martin B. Hentschel, Clifton Poth, Dominik Hintersdorf, Kristian Kersting","Backdoor attacks pose a serious security threat for training neural networks
as they surreptitiously introduce hidden functionalities into a model. Such
backdoors remain silent during inference on clean inputs, evading detection due
to inconspicuous behavior. However, once a specific trigger pattern appears in
the input data, the backdoor activates, causing the model to execute its
concealed function. Detecting such poisoned samples within vast datasets is
virtually impossible through manual inspection. To address this challenge, we
propose a novel approach that enables model training on potentially poisoned
datasets by utilizing the power of recent diffusion models. Specifically, we
create synthetic variations of all training samples, leveraging the inherent
resilience of diffusion models to potential trigger patterns in the data. By
combining this generative approach with knowledge distillation, we produce
student models that maintain their general performance on the task while
exhibiting robust resistance to backdoor triggers.",2310.06372v2,https://arxiv.org/pdf/2310.06372v2
"Adversarial Masked Image Inpainting for Robust Detection of Mpox and
  Non-Mpox","Yubiao Yue, Zhenzhang Li","Due to the lack of efficient mpox diagnostic technology, mpox cases continue
to increase. Recently, the great potential of deep learning models in detecting
mpox and non-mpox has been proven. However, existing models learn image
representations via image classification, which results in they may be easily
susceptible to interference from real-world noise, require diverse non-mpox
images, and fail to detect abnormal input. These drawbacks make classification
models inapplicable in real-world settings. To address these challenges, we
propose ""Mask, Inpainting, and Measure"" (MIM). In MIM's pipeline, a generative
adversarial network only learns mpox image representations by inpainting the
masked mpox images. Then, MIM determines whether the input belongs to mpox by
measuring the similarity between the inpainted image and the original image.
The underlying intuition is that since MIM solely models mpox images, it
struggles to accurately inpaint non-mpox images in real-world settings. Without
utilizing any non-mpox images, MIM cleverly detects mpox and non-mpox and can
handle abnormal inputs. We used the recognized mpox dataset (MSLD) and images
of eighteen non-mpox skin diseases to verify the effectiveness and robustness
of MIM. Experimental results show that the average AUROC of MIM achieves
0.8237. In addition, we demonstrated the drawbacks of classification models and
buttressed the potential of MIM through clinical validation. Finally, we
developed an online smartphone app to provide free testing to the public in
affected areas. This work first employs generative models to improve mpox
detection and provides new insights into binary decision-making tasks in
medical images.",2310.06318v1,https://arxiv.org/pdf/2310.06318v1
"PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust
  Generalization","Jiancong Xiao, Ruoyu Sun, Zhi- Quan Luo","Deep neural networks (DNNs) are vulnerable to adversarial attacks. It is
found empirically that adversarially robust generalization is crucial in
establishing defense algorithms against adversarial attacks. Therefore, it is
interesting to study the theoretical guarantee of robust generalization. This
paper focuses on norm-based complexity, based on a PAC-Bayes approach
(Neyshabur et al., 2017). The main challenge lies in extending the key
ingredient, which is a weight perturbation bound in standard settings, to the
robust settings. Existing attempts heavily rely on additional strong
assumptions, leading to loose bounds. In this paper, we address this issue and
provide a spectrally-normalized robust generalization bound for DNNs. Compared
to existing bounds, our bound offers two significant advantages: Firstly, it
does not depend on additional assumptions. Secondly, it is considerably
tighter, aligning with the bounds of standard generalization. Therefore, our
result provides a different perspective on understanding robust generalization:
The mismatch terms between standard and robust generalization bounds shown in
previous studies do not contribute to the poor robust generalization. Instead,
these disparities solely due to mathematical issues. Finally, we extend the
main result to adversarial robustness against general non-$\ell_p$ attacks and
other neural network architectures.",2310.06182v2,https://arxiv.org/pdf/2310.06182v2
"Mitigating Simplicity Bias in Deep Learning for Improved OOD
  Generalization and Robustness","Bhavya Vasudeva, Kameron Shahabi, Vatsal Sharan","Neural networks (NNs) are known to exhibit simplicity bias where they tend to
prefer learning 'simple' features over more 'complex' ones, even when the
latter may be more informative. Simplicity bias can lead to the model making
biased predictions which have poor out-of-distribution (OOD) generalization. To
address this, we propose a framework that encourages the model to use a more
diverse set of features to make predictions. We first train a simple model, and
then regularize the conditional mutual information with respect to it to obtain
the final model. We demonstrate the effectiveness of this framework in various
problem settings and real-world applications, showing that it effectively
addresses simplicity bias and leads to more features being used, enhances OOD
generalization, and improves subgroup robustness and fairness. We complement
these results with theoretical analyses of the effect of the regularization and
its OOD generalization properties.",2310.06161v1,https://arxiv.org/pdf/2310.06161v1
"Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK
  Approach","Shaopeng Fu, Di Wang","Adversarial training (AT) is a canonical method for enhancing the robustness
of deep neural networks (DNNs). However, recent studies empirically
demonstrated that it suffers from robust overfitting, i.e., a long time AT can
be detrimental to the robustness of DNNs. This paper presents a theoretical
explanation of robust overfitting for DNNs. Specifically, we non-trivially
extend the neural tangent kernel (NTK) theory to AT and prove that an
adversarially trained wide DNN can be well approximated by a linearized DNN.
Moreover, for squared loss, closed-form AT dynamics for the linearized DNN can
be derived, which reveals a new AT degeneration phenomenon: a long-term AT will
result in a wide DNN degenerates to that obtained without AT and thus cause
robust overfitting. Based on our theoretical results, we further design a
method namely Adv-NTK, the first AT algorithm for infinite-width DNNs.
Experiments on real-world datasets show that Adv-NTK can help infinite-width
DNNs enhance comparable robustness to that of their finite-width counterparts,
which in turn justifies our theoretical findings. The code is available at
https://github.com/fshp971/adv-ntk.",2310.06112v2,https://arxiv.org/pdf/2310.06112v2
Robust Angular Synchronization via Directed Graph Neural Networks,"Yixuan He, Gesine Reinert, David Wipf, Mihai Cucuringu","The angular synchronization problem aims to accurately estimate (up to a
constant additive phase) a set of unknown angles $\theta_1, \dots,
\theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets
$\theta_i-\theta_j \;\mbox{mod} \; 2\pi.$ Applications include, for example,
sensor network localization, phase retrieval, and distributed clock
synchronization. An extension of the problem to the heterogeneous setting
(dubbed $k$-synchronization) is to estimate $k$ groups of angles
simultaneously, given noisy observations (with unknown group assignment) from
each group. Existing methods for angular synchronization usually perform poorly
in high-noise regimes, which are common in applications. In this paper, we
leverage neural networks for the angular synchronization problem, and its
heterogeneous extension, by proposing GNNSync, a theoretically-grounded
end-to-end trainable framework using directed graph neural networks. In
addition, new loss functions are devised to encode synchronization objectives.
Experimental results on extensive data sets demonstrate that GNNSync attains
competitive, and often superior, performance against a comprehensive set of
baselines for the angular synchronization problem and its extension, validating
the robustness of GNNSync even at high noise levels.",2310.05842v2,https://arxiv.org/pdf/2310.05842v2
"A Simple and Robust Framework for Cross-Modality Medical Image
  Segmentation applied to Vision Transformers","Matteo Bastico, David Ryckelynck, Laurent Corté, Yannick Tillier, Etienne Decencière","When it comes to clinical images, automatic segmentation has a wide variety
of applications and a considerable diversity of input domains, such as
different types of Magnetic Resonance Images (MRIs) and Computerized Tomography
(CT) scans. This heterogeneity is a challenge for cross-modality algorithms
that should equally perform independently of the input image type fed to them.
Often, segmentation models are trained using a single modality, preventing
generalization to other types of input data without resorting to transfer
learning techniques. Furthermore, the multi-modal or cross-modality
architectures proposed in the literature frequently require registered images,
which are not easy to collect in clinical environments, or need additional
processing steps, such as synthetic image generation. In this work, we propose
a simple framework to achieve fair image segmentation of multiple modalities
using a single conditional model that adapts its normalization layers based on
the input type, trained with non-registered interleaved mixed data. We show
that our framework outperforms other cross-modality segmentation methods, when
applied to the same 3D UNet baseline model, on the Multi-Modality Whole Heart
Segmentation Challenge. Furthermore, we define the Conditional Vision
Transformer (C-ViT) encoder, based on the proposed cross-modality framework,
and we show that it brings significant improvements to the resulting
segmentation, up to 6.87\% of Dice accuracy, with respect to its baseline
reference. The code to reproduce our experiments and the trained model weights
are available at https://github.com/matteo-bastico/MI-Seg.",2310.05572v1,https://arxiv.org/pdf/2310.05572v1
"Robust Image Watermarking based on Cross-Attention and Invariant Domain
  Learning","Agnibh Dasgupta, Xin Zhong","Image watermarking involves embedding and extracting watermarks within a
cover image, with deep learning approaches emerging to bolster generalization
and robustness. Predominantly, current methods employ convolution and
concatenation for watermark embedding, while also integrating conceivable
augmentation in the training process. This paper explores a robust image
watermarking methodology by harnessing cross-attention and invariant domain
learning, marking two novel, significant advancements. First, we design a
watermark embedding technique utilizing a multi-head cross attention mechanism,
enabling information exchange between the cover image and watermark to identify
semantically suitable embedding locations. Second, we advocate for learning an
invariant domain representation that encapsulates both semantic and
noise-invariant information concerning the watermark, shedding light on
promising avenues for enhancing image watermarking techniques.",2310.05395v1,https://arxiv.org/pdf/2310.05395v1
"A Knowledge Graph-Based Search Engine for Robustly Finding Doctors and
  Locations in the Healthcare Domain","Mayank Kejriwal, Hamid Haidarian, Min-Hsueh Chiu, Andy Xiang, Deep Shrestha, Faizan Javed","Efficiently finding doctors and locations is an important search problem for
patients in the healthcare domain, for which traditional information retrieval
methods tend not to work optimally. In the last ten years, knowledge graphs
(KGs) have emerged as a powerful way to combine the benefits of gleaning
insights from semi-structured data using semantic modeling, natural language
processing techniques like information extraction, and robust querying using
structured query languages like SPARQL and Cypher. In this short paper, we
present a KG-based search engine architecture for robustly finding doctors and
locations in the healthcare domain. Early results demonstrate that our approach
can lead to significantly higher coverage for complex queries without degrading
quality.",2310.05258v1,https://arxiv.org/pdf/2310.05258v1
Multi-Ship Tracking by Robust Similarity metric,"Hongyu Zhao, Gongming Wei, Yang Xiao, Xianglei Xing","Multi-ship tracking (MST) as a core technology has been proven to be applied
to situational awareness at sea and the development of a navigational system
for autonomous ships. Despite impressive tracking outcomes achieved by
multi-object tracking (MOT) algorithms for pedestrian and vehicle datasets,
these models and techniques exhibit poor performance when applied to ship
datasets. Intersection of Union (IoU) is the most popular metric for computing
similarity used in object tracking. The low frame rates and severe image shake
caused by wave turbulence in ship datasets often result in minimal, or even
zero, Intersection of Union (IoU) between the predicted and detected bounding
boxes. This issue contributes to frequent identity switches of tracked objects,
undermining the tracking performance. In this paper, we address the weaknesses
of IoU by incorporating the smallest convex shapes that enclose both the
predicted and detected bounding boxes. The calculation of the tracking version
of IoU (TIoU) metric considers not only the size of the overlapping area
between the detection bounding box and the prediction box, but also the
similarity of their shapes. Through the integration of the TIoU into
state-of-the-art object tracking frameworks, such as DeepSort and ByteTrack, we
consistently achieve improvements in the tracking performance of these
frameworks.",2310.05171v1,https://arxiv.org/pdf/2310.05171v1
"Robust-GBDT: GBDT with Nonconvex Loss for Tabular Classification in the
  Presence of Label Noise and Class Imbalance","Jiaqi Luo, Yuedong Quan, Shixin Xu","Dealing with label noise in tabular classification tasks poses a persistent
challenge in machine learning. While robust boosting methods have shown promise
in binary classification, their effectiveness in complex, multi-class scenarios
is often limited. Additionally, issues like imbalanced datasets, missing
values, and computational inefficiencies further complicate their practical
utility. This study introduces Robust-GBDT, a groundbreaking approach that
combines the power of Gradient Boosted Decision Trees (GBDT) with the
resilience of nonconvex loss functions against label noise. By leveraging local
convexity within specific regions, Robust-GBDT demonstrates unprecedented
robustness, challenging conventional wisdom. Through seamless integration of
advanced GBDT with a novel Robust Focal Loss tailored for class imbalance,
Robust-GBDT significantly enhances generalization capabilities, particularly in
noisy and imbalanced datasets. Notably, its user-friendly design facilitates
integration with existing open-source code, enhancing computational efficiency
and scalability. Extensive experiments validate Robust-GBDT's superiority over
other noise-robust methods, establishing a new standard for accurate
classification amidst label noise. This research heralds a paradigm shift in
machine learning, paving the way for a new era of robust and precise
classification across diverse real-world applications.",2310.05067v2,https://arxiv.org/pdf/2310.05067v2
Distantly-Supervised Joint Extraction with Noise-Robust Learning,"Yufei Li, Xiao Yu, Yanghong Guo, Yanchi Liu, Haifeng Chen, Cong Liu","Joint entity and relation extraction is a process that identifies entity
pairs and their relations using a single model. We focus on the problem of
joint extraction in distantly-labeled data, whose labels are generated by
aligning entity mentions with the corresponding entity and relation tags using
a knowledge base (KB). One key challenge is the presence of noisy labels
arising from both incorrect entity and relation annotations, which
significantly impairs the quality of supervised learning. Existing approaches,
either considering only one source of noise or making decisions using external
knowledge, cannot well-utilize significant information in the training data. We
propose DENRL, a generalizable framework that 1) incorporates a lightweight
transformer backbone into a sequence labeling scheme for joint tagging, and 2)
employs a noise-robust framework that regularizes the tagging model with
significant relation patterns and entity-relation dependencies, then
iteratively self-adapts to instances with less noise from both sources.
Surprisingly, experiments on two benchmark datasets show that DENRL, using
merely its own parametric distribution and simple data-driven heuristics,
outperforms large language model-based baselines by a large margin with better
interpretability.",2310.04994v2,https://arxiv.org/pdf/2310.04994v2
"Understanding the Robustness of Multi-modal Contrastive Learning to
  Distribution Shift","Yihao Xue, Siddharth Joshi, Dang Nguyen, Baharan Mirzasoleiman","Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP,
have achieved a remarkable success in learning representations that are robust
against distribution shift and generalize to new domains. Despite the empirical
success, the mechanism behind learning such generalizable representations is
not understood. In this work, we rigorously analyze this problem and uncover
two mechanisms behind MMCL's robustness: \emph{intra-class contrasting}, which
allows the model to learn features with a high variance, and \emph{inter-class
feature sharing}, where annotated details in one class help learning other
classes better. Both mechanisms prevent spurious features that are
over-represented in the training data to overshadow the generalizable core
features. This yields superior zero-shot classification accuracy under
distribution shift. Furthermore, we theoretically demonstrate the benefits of
using rich captions on robustness and explore the effect of annotating
different types of details in the captions. We validate our theoretical
findings through experiments, including a well-designed synthetic experiment
and an experiment involving training CLIP models on MSCOCO/Conceptual Captions
and evaluating them on shifted ImageNets.",2310.04971v2,https://arxiv.org/pdf/2310.04971v2
SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning,"Lei You, Hei Victor Cheng","This study addresses the challenge of inaccurate gradients in computing the
empirical Fisher Information Matrix during neural network pruning. We introduce
SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning,
capitalizing on the geometric properties of the optimal transport problem. The
``swap'' of the commonly used linear regression with the EWR in optimization is
analytically demonstrated to offer noise mitigation effects by incorporating
neighborhood interpolation across data points with only marginal additional
computational cost. The unique strength of SWAP is its intrinsic ability to
balance noise reduction and covariance information preservation effectively.
Extensive experiments performed on various networks and datasets show
comparable performance of SWAP with state-of-the-art (SoTA) network pruning
algorithms. Our proposed method outperforms the SoTA when the network size or
the target sparsity is large, the gain is even larger with the existence of
noisy gradients, possibly from noisy data, analog memory, or adversarial
attacks. Notably, our proposed method achieves a gain of 6% improvement in
accuracy and 8% improvement in testing loss for MobileNetV1 with less than
one-fourth of the network parameters remaining.",2310.04918v4,https://arxiv.org/pdf/2310.04918v4
"Tight Certified Robustness via Min-Max Representations of ReLU Neural
  Networks","Brendon G. Anderson, Samuel Pfrommer, Somayeh Sojoudi","The reliable deployment of neural networks in control systems requires
rigorous robustness guarantees. In this paper, we obtain tight robustness
certificates over convex attack sets for min-max representations of ReLU neural
networks by developing a convex reformulation of the nonconvex certification
problem. This is done by ""lifting"" the problem to an infinite-dimensional
optimization over probability measures, leveraging recent results in
distributionally robust optimization to solve for an optimal discrete
distribution, and proving that solutions of the original nonconvex problem are
generated by the discrete distribution under mild boundedness, nonredundancy,
and Slater conditions. As a consequence, optimal (worst-case) attacks against
the model may be solved for exactly. This contrasts prior state-of-the-art that
either requires expensive branch-and-bound schemes or loose relaxation
techniques. Experiments on robust control and MNIST image classification
examples highlight the benefits of our approach.",2310.04916v1,https://arxiv.org/pdf/2310.04916v1
"Robust Low-Rank Matrix Completion via a New Sparsity-Inducing
  Regularizer","Zhi-Yong Wang, Hing Cheung So, Abdelhak M. Zoubir","This paper presents a novel loss function referred to as hybrid
ordinary-Welsch (HOW) and a new sparsity-inducing regularizer associated with
HOW. We theoretically show that the regularizer is quasiconvex and that the
corresponding Moreau envelope is convex. Moreover, the closed-form solution to
its Moreau envelope, namely, the proximity operator, is derived. Compared with
nonconvex regularizers like the lp-norm with 0<p<1 that requires iterations to
find the corresponding proximity operator, the developed regularizer has a
closed-form proximity operator. We apply our regularizer to the robust matrix
completion problem, and develop an efficient algorithm based on the alternating
direction method of multipliers. The convergence of the suggested method is
analyzed and we prove that any generated accumulation point is a stationary
point. Finally, experimental results based on synthetic and real-world datasets
demonstrate that our algorithm is superior to the state-of-the-art methods in
terms of restoration performance.",2310.04762v1,https://arxiv.org/pdf/2310.04762v1
"Robustness-enhanced Uplift Modeling with Adversarial Feature
  Desensitization","Zexu Sun, Bowei He, Ming Ma, Jiakai Tang, Yuchen Wang, Chen Ma, Dugang Liu","Uplift modeling has shown very promising results in online marketing.
However, most existing works are prone to the robustness challenge in some
practical applications. In this paper, we first present a possible explanation
for the above phenomenon. We verify that there is a feature sensitivity problem
in online marketing using different real-world datasets, where the perturbation
of some key features will seriously affect the performance of the uplift model
and even cause the opposite trend. To solve the above problem, we propose a
novel robustness-enhanced uplift modeling framework with adversarial feature
desensitization (RUAD). Specifically, our RUAD can more effectively alleviate
the feature sensitivity of the uplift model through two customized modules,
including a feature selection module with joint multi-label modeling to
identify a key subset from the input features and an adversarial feature
desensitization module using adversarial training and soft interpolation
operations to enhance the robustness of the model against this selected subset
of features. Finally, we conduct extensive experiments on a public dataset and
a real product dataset to verify the effectiveness of our RUAD in online
marketing. In addition, we also demonstrate the robustness of our RUAD to the
feature sensitivity, as well as the compatibility with different uplift models.",2310.04693v3,https://arxiv.org/pdf/2310.04693v3
"Profit: Benchmarking Personalization and Robustness Trade-off in
  Federated Prompt Tuning","Liam Collins, Shanshan Wu, Sewoong Oh, Khe Chai Sim","In many applications of federated learning (FL), clients desire models that
are personalized using their local data, yet are also robust in the sense that
they retain general global knowledge. However, the presence of data
heterogeneity across clients induces a fundamental trade-off between
personalization (i.e., adaptation to a local distribution) and robustness
(i.e., not forgetting previously learned general knowledge). It is critical to
understand how to navigate this personalization vs robustness trade-off when
designing federated systems, which are increasingly moving towards a paradigm
of fine-tuning large foundation models. Due to limited computational and
communication capabilities in most federated settings, this foundation model
fine-tuning must be done using parameter-efficient fine-tuning (PEFT)
approaches. While some recent work has studied federated approaches to PEFT,
the personalization vs robustness trade-off of federated PEFT has been largely
unexplored. In this work, we take a step towards bridging this gap by
benchmarking fundamental FL algorithms -- FedAvg and FedSGD plus
personalization (via client local fine-tuning) -- applied to one of the most
ubiquitous PEFT approaches to large language models (LLMs) -- prompt tuning --
in a multitude of hyperparameter settings under varying levels of data
heterogeneity. Our results show that federated-trained prompts can be
surprisingly robust when using a small learning rate with many local epochs for
personalization, especially when using an adaptive optimizer as the client
optimizer during federated training. We also demonstrate that simple approaches
such as adding regularization and interpolating two prompts are effective in
improving the personalization vs robustness trade-off in computation-limited
settings with few local updates allowed for personalization.",2310.04627v1,https://arxiv.org/pdf/2310.04627v1
Robust Transfer Learning with Unreliable Source Data,"Jianqing Fan, Cheng Gao, Jason M. Klusowski","This paper addresses challenges in robust transfer learning stemming from
ambiguity in Bayes classifiers and weak transferable signals between the target
and source distribution. We introduce a novel quantity called the ''ambiguity
level'' that measures the discrepancy between the target and source regression
functions, propose a simple transfer learning procedure, and establish a
general theorem that shows how this new quantity is related to the
transferability of learning in terms of risk improvements. Our proposed
''Transfer Around Boundary'' (TAB) model, with a threshold balancing the
performance of target and source data, is shown to be both efficient and
robust, improving classification while avoiding negative transfer. Moreover, we
demonstrate the effectiveness of the TAB model on non-parametric classification
and logistic regression tasks, achieving upper bounds which are optimal up to
logarithmic factors. Simulation studies lend further support to the
effectiveness of TAB. We also provide simple approaches to bound the excess
misclassification error without the need for specialized knowledge in transfer
learning.",2310.04606v1,https://arxiv.org/pdf/2310.04606v1
"Generating Less Certain Adversarial Examples Improves Robust
  Generalization","Minxing Zhang, Michael Backes, Xiao Zhang","This paper revisits the robust overfitting phenomenon of adversarial
training. Observing that models with better robust generalization performance
are less certain in predicting adversarially generated training inputs, we
argue that overconfidence in predicting adversarial examples is a potential
cause. Therefore, we hypothesize that generating less certain adversarial
examples improves robust generalization, and propose a formal definition of
adversarial certainty that captures the variance of the model's predicted
logits on adversarial examples. Our theoretical analysis of synthetic
distributions characterizes the connection between adversarial certainty and
robust generalization. Accordingly, built upon the notion of adversarial
certainty, we develop a general method to search for models that can generate
training-time adversarial inputs with reduced certainty, while maintaining the
model's capability in distinguishing adversarial examples. Extensive
experiments on image benchmarks demonstrate that our method effectively learns
models with consistently improved robustness and mitigates robust overfitting,
confirming the importance of generating less certain adversarial examples for
robust generalization.",2310.04539v2,https://arxiv.org/pdf/2310.04539v2
Neur2RO: Neural Two-Stage Robust Optimization,"Justin Dumouchelle, Esther Julien, Jannis Kurtz, Elias B. Khalil","Robust optimization provides a mathematical framework for modeling and
solving decision-making problems under worst-case uncertainty. This work
addresses two-stage robust optimization (2RO) problems (also called adjustable
robust optimization), wherein first-stage and second-stage decisions are made
before and after uncertainty is realized, respectively. This results in a
nested min-max-min optimization problem which is extremely challenging
computationally, especially when the decisions are discrete. We propose
Neur2RO, an efficient machine learning-driven instantiation of
column-and-constraint generation (CCG), a classical iterative algorithm for
2RO. Specifically, we learn to estimate the value function of the second-stage
problem via a novel neural network architecture that is easy to optimize over
by design. Embedding our neural network into CCG yields high-quality solutions
quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital
budgeting. For knapsack, Neur2RO finds solutions that are within roughly $2\%$
of the best-known values in a few seconds compared to the three hours of the
state-of-the-art exact branch-and-price algorithm; for larger and more complex
instances, Neur2RO finds even better solutions. For capital budgeting, Neur2RO
outperforms three variants of the $k$-adaptability algorithm, particularly on
the largest instances, with a 10 to 100-fold reduction in solution time. Our
code and data are available at https://github.com/khalil-research/Neur2RO.",2310.04345v2,https://arxiv.org/pdf/2310.04345v2
Robust Losses for Decision-Focused Learning,"Noah Schutte, Krzysztof Postek, Neil Yorke-Smith","Optimization models used to make discrete decisions often contain uncertain
parameters that are context-dependent and estimated through prediction. To
account for the quality of the decision made based on the prediction,
decision-focused learning (end-to-end predict-then-optimize) aims at training
the predictive model to minimize regret, i.e., the loss incurred by making a
suboptimal decision. Despite the challenge of the gradient of this loss w.r.t.
the predictive model parameters being zero almost everywhere for optimization
problems with a linear objective, effective gradient-based learning approaches
have been proposed to minimize the expected loss, using the empirical loss as a
surrogate. However, empirical regret can be an ineffective surrogate because
empirical optimal decisions can vary substantially from expected optimal
decisions. To understand the impact of this deficiency, we evaluate the effect
of aleatoric and epistemic uncertainty on the accuracy of empirical regret as a
surrogate. Next, we propose three novel loss functions that approximate
expected regret more robustly. Experimental results show that training two
state-of-the-art decision-focused learning approaches using robust regret
losses improves test-sample empirical regret in general while keeping
computational time equivalent relative to the number of training epochs.",2310.04328v2,https://arxiv.org/pdf/2310.04328v2
Adjustable Robust Reinforcement Learning for Online 3D Bin Packing,"Yuxin Pan, Yize Chen, Fangzhen Lin","Designing effective policies for the online 3D bin packing problem (3D-BPP)
has been a long-standing challenge, primarily due to the unpredictable nature
of incoming box sequences and stringent physical constraints. While current
deep reinforcement learning (DRL) methods for online 3D-BPP have shown
promising results in optimizing average performance over an underlying box
sequence distribution, they often fail in real-world settings where some
worst-case scenarios can materialize. Standard robust DRL algorithms tend to
overly prioritize optimizing the worst-case performance at the expense of
performance under normal problem instance distribution. To address these
issues, we first introduce a permutation-based attacker to investigate the
practical robustness of both DRL-based and heuristic methods proposed for
solving online 3D-BPP. Then, we propose an adjustable robust reinforcement
learning (AR2L) framework that allows efficient adjustment of robustness
weights to achieve the desired balance of the policy's performance in average
and worst-case environments. Specifically, we formulate the objective function
as a weighted sum of expected and worst-case returns, and derive the lower
performance bound by relating to the return under a mixture dynamics. To
realize this lower bound, we adopt an iterative procedure that searches for the
associated mixture dynamics and improves the corresponding policy. We integrate
this procedure into two popular robust adversarial algorithms to develop the
exact and approximate AR2L algorithms. Experiments demonstrate that AR2L is
versatile in the sense that it improves policy robustness while maintaining an
acceptable level of performance for the nominal case.",2310.04323v1,https://arxiv.org/pdf/2310.04323v1
"Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware
  Learning","Qing Zhu, Qirong Mao, Jialin Zhang, Xiaohua Huang, Wenming Zheng","Group-level emotion recognition (GER) is an inseparable part of human
behavior analysis, aiming to recognize an overall emotion in a multi-person
scene. However, the existing methods are devoted to combing diverse emotion
cues while ignoring the inherent uncertainties under unconstrained
environments, such as congestion and occlusion occurring within a group.
Additionally, since only group-level labels are available, inconsistent emotion
predictions among individuals in one group can confuse the network. In this
paper, we propose an uncertainty-aware learning (UAL) method to extract more
robust representations for GER. By explicitly modeling the uncertainty of each
individual, we utilize stochastic embedding drawn from a Gaussian distribution
instead of deterministic point embedding. This representation captures the
probabilities of different emotions and generates diverse predictions through
this stochasticity during the inference stage. Furthermore,
uncertainty-sensitive scores are adaptively assigned as the fusion weights of
individuals' face within each group. Moreover, we develop an image enhancement
module to enhance the model's robustness against severe noise. The overall
three-branch model, encompassing face, object, and scene component, is guided
by a proportional-weighted fusion strategy and integrates the proposed
uncertainty-aware method to produce the final group-level output. Experimental
results demonstrate the effectiveness and generalization ability of our method
across three widely used databases.",2310.04306v1,https://arxiv.org/pdf/2310.04306v1
Assessing Robustness via Score-Based Adversarial Image Generation,"Marcel Kollovieh, Lukas Gosch, Yan Scholten, Marten Lienen, Stephan Günnemann","Most adversarial attacks and defenses focus on perturbations within small
$\ell_p$-norm constraints. However, $\ell_p$ threat models cannot capture all
relevant semantic-preserving perturbations, and hence, the scope of robustness
evaluations is limited. In this work, we introduce Score-Based Adversarial
Generation (ScoreAG), a novel framework that leverages the advancements in
score-based generative models to generate adversarial examples beyond
$\ell_p$-norm constraints, so-called unrestricted adversarial examples,
overcoming their limitations. Unlike traditional methods, ScoreAG maintains the
core semantics of images while generating realistic adversarial examples,
either by transforming existing images or synthesizing new ones entirely from
scratch. We further exploit the generative capability of ScoreAG to purify
images, empirically enhancing the robustness of classifiers. Our extensive
empirical evaluation demonstrates that ScoreAG matches the performance of
state-of-the-art attacks and defenses across multiple benchmarks. This work
highlights the importance of investigating adversarial examples bounded by
semantics rather than $\ell_p$-norm constraints. ScoreAG represents an
important step towards more encompassing robustness assessments.",2310.04285v1,https://arxiv.org/pdf/2310.04285v1
"Robust Multimodal Learning with Missing Modalities via
  Parameter-Efficient Adaptation","Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif","Multimodal learning seeks to utilize data from multiple sources to improve
the overall performance of downstream tasks. It is desirable for redundancies
in the data to make multimodal systems robust to missing or corrupted
observations in some correlated modalities. However, we observe that the
performance of several existing multimodal networks significantly deteriorates
if one or multiple modalities are absent at test time. To enable robustness to
missing modalities, we propose a simple and parameter-efficient adaptation
procedure for pretrained multimodal networks. In particular, we exploit
modulation of intermediate features to compensate for the missing modalities.
We demonstrate that such adaptation can partially bridge performance drop due
to missing modalities and outperform independent, dedicated networks trained
for the available modality combinations in some cases. The proposed adaptation
requires extremely small number of parameters (e.g., fewer than 1% of the total
parameters) and applicable to a wide range of modality combinations and tasks.
We conduct a series of experiments to highlight the missing modality robustness
of our proposed method on five different multimodal tasks across seven
datasets. Our proposed method demonstrates versatility across various tasks and
datasets, and outperforms existing methods for robust multimodal learning with
missing modalities.",2310.03986v4,https://arxiv.org/pdf/2310.03986v4
"Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust
  Closed-Loop Control","Neehal Tumma, Mathias Lechner, Noel Loo, Ramin Hasani, Daniela Rus","Developing autonomous agents that can interact with changing environments is
an open challenge in machine learning. Robustness is particularly important in
these settings as agents are often fit offline on expert demonstrations but
deployed online where they must generalize to the closed feedback loop within
the environment. In this work, we explore the application of recurrent neural
networks to tasks of this nature and understand how a parameterization of their
recurrent connectivity influences robustness in closed-loop settings.
Specifically, we represent the recurrent connectivity as a function of rank and
sparsity and show both theoretically and empirically that modulating these two
variables has desirable effects on network dynamics. The proposed low-rank,
sparse connectivity induces an interpretable prior on the network that proves
to be most amenable for a class of models known as closed-form continuous-time
neural networks (CfCs). We find that CfCs with fewer parameters can outperform
their full-rank, fully-connected counterparts in the online setting under
distribution shift. This yields memory-efficient and robust agents while
opening a new perspective on how we can modulate network dynamics through
connectivity.",2310.03915v3,https://arxiv.org/pdf/2310.03915v3
Stylist: Style-Driven Feature Ranking for Robust Novelty Detection,"Stefan Smeu, Elena Burceanu, Emanuela Haller, Andrei Liviu Nicolicioiu","Novelty detection aims at finding samples that differ in some form from the
distribution of seen samples. But not all changes are created equal. Data can
suffer a multitude of distribution shifts, and we might want to detect only
some types of relevant changes. Similar to works in out-of-distribution
generalization, we propose to use the formalization of separating into semantic
or content changes, that are relevant to our task, and style changes, that are
irrelevant. Within this formalization, we define the robust novelty detection
as the task of finding semantic changes while being robust to style
distributional shifts. Leveraging pretrained, large-scale model
representations, we introduce Stylist, a novel method that focuses on dropping
environment-biased features. First, we compute a per-feature score based on the
feature distribution distances between environments. Next, we show that our
selection manages to remove features responsible for spurious correlations and
improve novelty detection performance. For evaluation, we adapt domain
generalization datasets to our task and analyze the methods behaviors. We
additionally built a large synthetic dataset where we have control over the
spurious correlations degree. We prove that our selection mechanism improves
novelty detection algorithms across multiple datasets, containing both
stylistic and content shifts.",2310.03738v1,https://arxiv.org/pdf/2310.03738v1
"Towards Robust and Generalizable Training: An Empirical Study of Noisy
  Slot Filling for Input Perturbations","Jiachi Liu, Liwen Wang, Guanting Dong, Xiaoshuai Song, Zechen Wang, Zhengyang Wang, Shanglin Lei, Jinzheng Zhao, Keqing He, Bo Xiao, Weiran Xu","In real dialogue scenarios, as there are unknown input noises in the
utterances, existing supervised slot filling models often perform poorly in
practical applications. Even though there are some studies on noise-robust
models, these works are only evaluated on rule-based synthetic datasets, which
is limiting, making it difficult to promote the research of noise-robust
methods. In this paper, we introduce a noise robustness evaluation dataset
named Noise-SF for slot filling task. The proposed dataset contains five types
of human-annotated noise, and all those noises are exactly existed in real
extensive robust-training methods of slot filling into the proposed framework.
By conducting exhaustive empirical evaluation experiments on Noise-SF, we find
that baseline models have poor performance in robustness evaluation, and the
proposed framework can effectively improve the robustness of models. Based on
the empirical experimental results, we make some forward-looking suggestions to
fuel the research in this direction. Our dataset Noise-SF will be released at
https://github.com/dongguanting/Noise-SF.",2310.03518v1,https://arxiv.org/pdf/2310.03518v1
"Enhancing Robust Representation in Adversarial Training: Alignment and
  Exclusion Criteria","Nuoyan Zhou, Nannan Wang, Decheng Liu, Dawei Zhou, Xinbo Gao","Deep neural networks are vulnerable to adversarial noise. Adversarial
Training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two criteria of robust
representation: (1) Exclusion: \emph{the feature of examples keeps away from
that of other classes}; (2) Alignment: \emph{the feature of natural and
corresponding adversarial examples is close to each other}. These motivate us
to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance.",2310.03358v2,https://arxiv.org/pdf/2310.03358v2
"An Integrated Algorithm for Robust and Imperceptible Audio Adversarial
  Examples","Armin Ettenhofer, Jan-Philipp Schulze, Karla Pizzi","Audio adversarial examples are audio files that have been manipulated to fool
an automatic speech recognition (ASR) system, while still sounding benign to a
human listener. Most methods to generate such samples are based on a two-step
algorithm: first, a viable adversarial audio file is produced, then, this is
fine-tuned with respect to perceptibility and robustness. In this work, we
present an integrated algorithm that uses psychoacoustic models and room
impulse responses (RIR) in the generation step. The RIRs are dynamically
created by a neural network during the generation process to simulate a
physical environment to harden our examples against transformations experienced
in over-the-air attacks. We compare the different approaches in three
experiments: in a simulated environment and in a realistic over-the-air
scenario to evaluate the robustness, and in a human study to evaluate the
perceptibility. Our algorithms considering psychoacoustics only or in addition
to the robustness show an improvement in the signal-to-noise ratio (SNR) as
well as in the human perception study, at the cost of an increased word error
rate (WER).",2310.03349v1,https://arxiv.org/pdf/2310.03349v1
Certifiably Robust Graph Contrastive Learning,"Minhua Lin, Teng Xiao, Enyan Dai, Xiang Zhang, Suhang Wang","Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph
representation learning method. However, it has been shown that GCL is
vulnerable to adversarial attacks on both the graph structure and node
attributes. Although empirical approaches have been proposed to enhance the
robustness of GCL, the certifiable robustness of GCL is still remain
unexplored. In this paper, we develop the first certifiably robust framework in
GCL. Specifically, we first propose a unified criteria to evaluate and certify
the robustness of GCL. We then introduce a novel technique, RES (Randomized
Edgedrop Smoothing), to ensure certifiable robustness for any GCL model, and
this certified robustness can be provably preserved in downstream tasks.
Furthermore, an effective training method is proposed for robust GCL. Extensive
experiments on real-world datasets demonstrate the effectiveness of our
proposed method in providing effective certifiable robustness and enhancing the
robustness of any GCL model. The source code of RES is available at
https://github.com/ventr1c/RES-GCL.",2310.03312v1,https://arxiv.org/pdf/2310.03312v1
"Burning the Adversarial Bridges: Robust Windows Malware Detection
  Against Binary-level Mutations","Ahmed Abusnaina, Yizhen Wang, Sunpreet Arora, Ke Wang, Mihai Christodorescu, David Mohaisen","Toward robust malware detection, we explore the attack surface of existing
malware detection systems. We conduct root-cause analyses of the practical
binary-level black-box adversarial malware examples. Additionally, we uncover
the sensitivity of volatile features within the detection engines and exhibit
their exploitability. Highlighting volatile information channels within the
software, we introduce three software pre-processing steps to eliminate the
attack surface, namely, padding removal, software stripping, and inter-section
information resetting. Further, to counter the emerging section injection
attacks, we propose a graph-based section-dependent information extraction
scheme for software representation. The proposed scheme leverages aggregated
information within various sections in the software to enable robust malware
detection and mitigate adversarial settings. Our experimental results show that
traditional malware detection models are ineffective against adversarial
threats. However, the attack surface can be largely reduced by eliminating the
volatile information. Therefore, we propose simple-yet-effective methods to
mitigate the impacts of binary manipulation attacks. Overall, our graph-based
malware detection scheme can accurately detect malware with an area under the
curve score of 88.32\% and a score of 88.19% under a combination of binary
manipulation attacks, exhibiting the efficiency of our proposed scheme.",2310.03285v1,https://arxiv.org/pdf/2310.03285v1
"Robust and Interpretable Medical Image Classifiers via Concept
  Bottleneck Models","An Yan, Yu Wang, Yiwu Zhong, Zexue He, Petros Karypis, Zihan Wang, Chengyu Dong, Amilcare Gentili, Chun-Nan Hsu, Jingbo Shang, Julian McAuley","Medical image classification is a critical problem for healthcare, with the
potential to alleviate the workload of doctors and facilitate diagnoses of
patients. However, two challenges arise when deploying deep learning models to
real-world healthcare applications. First, neural models tend to learn spurious
correlations instead of desired features, which could fall short when
generalizing to new domains (e.g., patients with different ages). Second, these
black-box models lack interpretability. When making diagnostic predictions, it
is important to understand why a model makes a decision for trustworthy and
safety considerations. In this paper, to address these two limitations, we
propose a new paradigm to build robust and interpretable medical image
classifiers with natural language concepts. Specifically, we first query
clinical concepts from GPT-4, then transform latent image features into
explicit concepts with a vision-language model. We systematically evaluate our
method on eight medical image classification datasets to verify its
effectiveness. On challenging datasets with strong confounding factors, our
method can mitigate spurious correlations thus substantially outperform
standard visual encoders and other baselines. Finally, we show how
classification with a small number of concepts brings a level of
interpretability for understanding model decisions through case studies in real
medical data.",2310.03182v1,https://arxiv.org/pdf/2310.03182v1
"FedHyper: A Universal and Robust Learning Rate Scheduler for Federated
  Learning with Hypergradient Descent","Ziyao Wang, Jianyu Wang, Ang Li","The theoretical landscape of federated learning (FL) undergoes rapid
evolution, but its practical application encounters a series of intricate
challenges, and hyperparameter optimization is one of these critical
challenges. Amongst the diverse adjustments in hyperparameters, the adaptation
of the learning rate emerges as a crucial component, holding the promise of
significantly enhancing the efficacy of FL systems. In response to this
critical need, this paper presents FedHyper, a novel hypergradient-based
learning rate adaptation algorithm specifically designed for FL. FedHyper
serves as a universal learning rate scheduler that can adapt both global and
local rates as the training progresses. In addition, FedHyper not only
showcases unparalleled robustness to a spectrum of initial learning rate
configurations but also significantly alleviates the necessity for laborious
empirical learning rate adjustments. We provide a comprehensive theoretical
analysis of FedHyper's convergence rate and conduct extensive experiments on
vision and language benchmark datasets. The results demonstrate that FEDHYPER
consistently converges 1.1-3x faster than FedAvg and the competing baselines
while achieving superior final accuracy. Moreover, FedHyper catalyzes a
remarkable surge in accuracy, augmenting it by up to 15% compared to FedAvg
under suboptimal initial learning rate settings.",2310.03156v2,https://arxiv.org/pdf/2310.03156v2
"Robust Ocean Subgrid-Scale Parameterizations Using Fourier Neural
  Operators","Victor Mangeleer, Gilles Louppe","In climate simulations, small-scale processes shape ocean dynamics but remain
computationally expensive to resolve directly. For this reason, their
contributions are commonly approximated using empirical parameterizations,
which lead to significant errors in long-term projections. In this work, we
develop parameterizations based on Fourier Neural Operators, showcasing their
accuracy and generalizability in comparison to other approaches. Finally, we
discuss the potential and limitations of neural networks operating in the
frequency domain, paving the way for future investigation.",2310.02691v2,https://arxiv.org/pdf/2310.02691v2
"Online Estimation and Inference for Robust Policy Evaluation in
  Reinforcement Learning","Weidong Liu, Jiyuan Tu, Yichen Zhang, Xi Chen","Recently, reinforcement learning has gained prominence in modern statistics,
with policy evaluation being a key component. Unlike traditional machine
learning literature on this topic, our work places emphasis on statistical
inference for the parameter estimates computed using reinforcement learning
algorithms. While most existing analyses assume random rewards to follow
standard distributions, limiting their applicability, we embrace the concept of
robust statistics in reinforcement learning by simultaneously addressing issues
of outlier contamination and heavy-tailed rewards within a unified framework.
In this paper, we develop an online robust policy evaluation procedure, and
establish the limiting distribution of our estimator, based on its Bahadur
representation. Furthermore, we develop a fully-online procedure to efficiently
conduct statistical inference based on the asymptotic distribution. This paper
bridges the gap between robust statistics and statistical inference in
reinforcement learning, offering a more versatile and reliable approach to
policy evaluation. Finally, we validate the efficacy of our algorithm through
numerical experiments conducted in real-world reinforcement learning
experiments.",2310.02581v1,https://arxiv.org/pdf/2310.02581v1
A Recipe for Improved Certifiable Robustness,"Kai Hu, Klas Leino, Zifan Wang, Matt Fredrikson","Recent studies have highlighted the potential of Lipschitz-based methods for
training certifiably robust neural networks against adversarial attacks. A key
challenge, supported both theoretically and empirically, is that robustness
demands greater network capacity and more data than standard training. However,
effectively adding capacity under stringent Lipschitz constraints has proven
more difficult than it may seem, evident by the fact that state-of-the-art
approach tend more towards \emph{underfitting} than overfitting. Moreover, we
posit that a lack of careful exploration of the design space for Lipshitz-based
approaches has left potential performance gains on the table. In this work, we
provide a more comprehensive evaluation to better uncover the potential of
Lipschitz-based certification methods. Using a combination of novel techniques,
design optimizations, and synthesis of prior work, we are able to significantly
improve the state-of-the-art VRA for deterministic certification on a variety
of benchmark datasets, and over a range of perturbation sizes. Of particular
note, we discover that the addition of large ``Cholesky-orthogonalized residual
dense'' layers to the end of existing state-of-the-art Lipschitz-controlled
ResNet architectures is especially effective for increasing network capacity
and performance. Combined with filtered generative data augmentation, our final
results further the state of the art deterministic VRA by up to 8.5 percentage
points\footnote{Code is available at \url{https://github.com/hukkai/liresnet}}.",2310.02513v2,https://arxiv.org/pdf/2310.02513v2
"Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit
  Quantization and Robustness","Young Jin Kim, Raffy Fahim, Hany Hassan Awadalla","Large Mixture of Experts (MoE) models could achieve state-of-the-art quality
on various language tasks, including machine translation task, thanks to the
efficient model scaling capability with expert parallelism. However, it has
brought a fundamental issue of larger memory consumption and increased memory
bandwidth bottleneck at deployment time. In this paper, we propose Mixture of
Quantized Experts (MoQE) which is a simple weight-only quantization method
applying ultra low-bit down to 2-bit quantizations only to expert weights for
mitigating the increased memory and latency issues of MoE models. We show that
low-bit quantization together with the MoE architecture delivers a reliable
model performance while reducing the memory size significantly even without any
additional training in most cases. In particular, expert layers in MoE models
are much more robust to the quantization than conventional feedforward networks
(FFN) layers. In our comprehensive analysis, we show that MoE models with 2-bit
expert weights can deliver better model performance than the dense model
trained on the same dataset. As a result of low-bit quantization, we show the
model size can be reduced by 79.6% of the original half precision floating
point (fp16) MoE model. Combined with an optimized GPU runtime implementation,
it also achieves 1.24X speed-up on A100 GPUs.",2310.02410v1,https://arxiv.org/pdf/2310.02410v1
Exploring Model Learning Heterogeneity for Boosting Ensemble Robustness,"Yanzhao Wu, Ka-Ho Chow, Wenqi Wei, Ling Liu","Deep neural network ensembles hold the potential of improving generalization
performance for complex learning tasks. This paper presents formal analysis and
empirical evaluation to show that heterogeneous deep ensembles with high
ensemble diversity can effectively leverage model learning heterogeneity to
boost ensemble robustness. We first show that heterogeneous DNN models trained
for solving the same learning problem, e.g., object detection, can
significantly strengthen the mean average precision (mAP) through our weighted
bounding box ensemble consensus method. Second, we further compose ensembles of
heterogeneous models for solving different learning problems, e.g., object
detection and semantic segmentation, by introducing the connected component
labeling (CCL) based alignment. We show that this two-tier heterogeneity driven
ensemble construction method can compose an ensemble team that promotes high
ensemble diversity and low negative correlation among member models of the
ensemble, strengthening ensemble robustness against both negative examples and
adversarial attacks. Third, we provide a formal analysis of the ensemble
robustness in terms of negative correlation. Extensive experiments validate the
enhanced robustness of heterogeneous ensembles in both benign and adversarial
settings. The source codes are available on GitHub at
https://github.com/git-disl/HeteRobust.",2310.02237v1,https://arxiv.org/pdf/2310.02237v1
"Towards Robust Fidelity for Evaluating Explainability of Graph Neural
  Networks","Xu Zheng, Farhad Shirani, Tianchun Wang, Wei Cheng, Zhuomin Chen, Haifeng Chen, Hua Wei, Dongsheng Luo","Graph Neural Networks (GNNs) are neural models that leverage the dependency
structure in graphical data via message passing among the graph nodes. GNNs
have emerged as pivotal architectures in analyzing graph-structured data, and
their expansive application in sensitive domains requires a comprehensive
understanding of their decision-making processes -- necessitating a framework
for GNN explainability. An explanation function for GNNs takes a pre-trained
GNN along with a graph as input, to produce a `sufficient statistic' subgraph
with respect to the graph label. A main challenge in studying GNN
explainability is to provide fidelity measures that evaluate the performance of
these explanation functions. This paper studies this foundational challenge,
spotlighting the inherent limitations of prevailing fidelity metrics, including
$Fid_+$, $Fid_-$, and $Fid_\Delta$. Specifically, a formal,
information-theoretic definition of explainability is introduced and it is
shown that existing metrics often fail to align with this definition across
various statistical scenarios. The reason is due to potential distribution
shifts when subgraphs are removed in computing these fidelity measures.
Subsequently, a robust class of fidelity measures are introduced, and it is
shown analytically that they are resilient to distribution shift issues and are
applicable in a wide range of scenarios. Extensive empirical analysis on both
synthetic and real datasets are provided to illustrate that the proposed
metrics are more coherent with gold standard metrics. The source code is
available at https://trustai4s-lab.github.io/fidelity.",2310.01820v2,https://arxiv.org/pdf/2310.01820v2
AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework,"Xilie Xu, Jingfeng Zhang, Mohan Kankanhalli","Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial
robustness in downstream applications, without requiring a lot of computational
resources and collecting significant amounts of data. This paper uncovers an
issue with the existing RFT, where optimizing both adversarial and natural
objectives through the feature extractor (FE) yields significantly divergent
gradient directions. This divergence introduces instability in the optimization
process, thereby hindering the attainment of adversarial robustness and
rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we
propose a low-rank (LoRa) branch that disentangles RFT into two distinct
components: optimizing natural objectives via the LoRa branch and adversarial
objectives via the FE. Besides, we introduce heuristic strategies for
automating the scheduling of the learning rate and the scalars of loss terms.
Extensive empirical evaluations demonstrate that our proposed automated RFT
disentangled via the LoRa branch (AutoLoRa) achieves new state-of-the-art
results across a range of downstream tasks. AutoLoRa holds significant
practical utility, as it automatically converts a pre-trained FE into an
adversarially robust model for downstream tasks without the need for searching
hyperparameters.",2310.01818v1,https://arxiv.org/pdf/2310.01818v1
"Blending Imitation and Reinforcement Learning for Robust Policy
  Improvement","Xuefeng Liu, Takuma Yoneda, Rick L. Stevens, Matthew R. Walter, Yuxin Chen","While reinforcement learning (RL) has shown promising performance, its sample
complexity continues to be a substantial hurdle, restricting its broader
application across a variety of domains. Imitation learning (IL) utilizes
oracles to improve sample efficiency, yet it is often constrained by the
quality of the oracles deployed. which actively interleaves between IL and RL
based on an online estimate of their performance. RPI draws on the strengths of
IL, using oracle queries to facilitate exploration, an aspect that is notably
challenging in sparse-reward RL, particularly during the early stages of
learning. As learning unfolds, RPI gradually transitions to RL, effectively
treating the learned policy as an improved oracle. This algorithm is capable of
learning from and improving upon a diverse set of black-box oracles. Integral
to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient
(RPG), both of which reason over whether to perform state-wise imitation from
the oracles or learn from its own value function when the learner's performance
surpasses that of the oracles in a specific state. Empirical evaluations and
theoretical analysis validate that RPI excels in comparison to existing
state-of-the-art methodologies, demonstrating superior performance across
various benchmark domains.",2310.01737v2,https://arxiv.org/pdf/2310.01737v2
"Robustifying State-space Models for Long Sequences via Approximate
  Diagonalization","Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson","State-space models (SSMs) have recently emerged as a framework for learning
long-range sequence tasks. An example is the structured state-space sequence
(S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO
initialization framework. However, the complicated structure of the S4 layer
poses challenges; and, in an effort to address these challenges, models such as
S4D and S5 have considered a purely diagonal structure. This choice simplifies
the implementation, improves computational efficiency, and allows channel
communication. However, diagonalizing the HiPPO framework is itself an
ill-posed problem. In this paper, we propose a general solution for this and
related ill-posed diagonalization problems in machine learning. We introduce a
generic, backward-stable ""perturb-then-diagonalize"" (PTD) methodology, which is
based on the pseudospectral theory of non-normal operators, and which may be
interpreted as the approximate diagonalization of the non-normal matrices
defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models.
Through theoretical analysis of the transfer functions of different
initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization
strongly converges to the HiPPO framework, while the S4D/S5 initialization only
achieves weak convergences. As a result, our new models show resilience to
Fourier-mode noise-perturbed inputs, a crucial property not achieved by the
S4D/S5 models. In addition to improved robustness, our S5-PTD model averages
87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD
methodology helps to improve the accuracy of deep learning models.",2310.01698v1,https://arxiv.org/pdf/2310.01698v1
Making Retrieval-Augmented Language Models Robust to Irrelevant Context,"Ori Yoran, Tomer Wolfson, Ori Ram, Jonathan Berant","Retrieval-augmented language models (RALMs) hold promise to produce language
understanding systems that are are factual, efficient, and up-to-date. An
important desideratum of RALMs, is that retrieved information helps model
performance when it is relevant, and does not harm performance when it is not.
This is particularly important in multi-hop reasoning scenarios, where misuse
of irrelevant evidence can lead to cascading errors. However, recent work has
shown that retrieval augmentation can sometimes have a negative effect on
performance. In this work, we present a thorough analysis on five open-domain
question answering benchmarks, characterizing cases when retrieval reduces
accuracy. We then propose two methods to mitigate this issue. First, a simple
baseline that filters out retrieved passages that do not entail question-answer
pairs according to a natural language inference (NLI) model. This is effective
in preventing performance reduction, but at a cost of also discarding relevant
passages. Thus, we propose a method for automatically generating data to
fine-tune the language model to properly leverage retrieved passages, using a
mix of relevant and irrelevant contexts at training time. We empirically show
that even 1,000 examples suffice to train the model to be robust to irrelevant
contexts while maintaining high performance on examples with relevant ones.",2310.01558v2,https://arxiv.org/pdf/2310.01558v2
Towards Robust Cardiac Segmentation using Graph Convolutional Networks,"Gilles Van De Vyver, Sarina Thomas, Guy Ben-Yosef, Sindre Hellum Olaisen, Håvard Dalen, Lasse Løvstakken, Erik Smistad","Fully automatic cardiac segmentation can be a fast and reproducible method to
extract clinical measurements from an echocardiography examination. The U-Net
architecture is the current state-of-the-art deep learning architecture for
medical segmentation and can segment cardiac structures in real-time with
average errors comparable to inter-observer variability. However, this
architecture still generates large outliers that are often anatomically
incorrect. This work uses the concept of graph convolutional neural networks
that predict the contour points of the structures of interest instead of
labeling each pixel. We propose a graph architecture that uses two
convolutional rings based on cardiac anatomy and show that this eliminates
anatomical incorrect multi-structure segmentations on the publicly available
CAMUS dataset. Additionally, this work contributes with an ablation study on
the graph convolutional architecture and an evaluation of clinical measurements
on the clinical HUNT4 dataset. Finally, we propose to use the inter-model
agreement of the U-Net and the graph network as a predictor of both the input
and segmentation quality. We show this predictor can detect out-of-distribution
and unsuitable input images in real-time. Source code is available online:
https://github.com/gillesvntnu/GCN_multistructure",2310.01210v5,https://arxiv.org/pdf/2310.01210v5
"A Robust Machine Learning Approach for Path Loss Prediction in 5G
  Networks with Nested Cross Validation","Ibrahim Yazıcı, Emre Gures","The design and deployment of fifth-generation (5G) wireless networks pose
significant challenges due to the increasing number of wireless devices. Path
loss has a landmark importance in network performance optimization, and
accurate prediction of the path loss, which characterizes the attenuation of
signal power during transmission, is critical for effective network planning,
coverage estimation, and optimization. In this sense, we utilize machine
learning (ML) methods, which overcome conventional path loss prediction models
drawbacks, for path loss prediction in a 5G network system to facilitate more
accurate network planning, resource optimization, and performance improvement
in wireless communication systems. To this end, we utilize a novel approach,
nested cross validation scheme, with ML to prevent overfitting, thereby getting
better generalization error and stable results for ML deployment. First, we
acquire a publicly available dataset obtained through a comprehensive
measurement campaign conducted in an urban macro-cell scenario located in
Beijing, China. The dataset includes crucial information such as longitude,
latitude, elevation, altitude, clutter height, and distance, which are utilized
as essential features to predict the path loss in the 5G network system. We
deploy Support Vector Regression (SVR), CatBoost Regression (CBR), eXtreme
Gradient Boosting Regression (XGBR), Artificial Neural Network (ANN), and
Random Forest (RF) methods to predict the path loss, and compare the prediction
results in terms of Mean Absolute Error (MAE) and Mean Square Error (MSE). As
per obtained results, XGBR outperforms the rest of the methods. It outperforms
CBR with a slight performance differences by 0.4 % and 1 % in terms of MAE and
MSE metrics, respectively. On the other hand, it outperforms the rest of the
methods with clear performance differences.",2310.01030v1,https://arxiv.org/pdf/2310.01030v1
Towards Robust 3D Object Detection In Rainy Conditions,"Aldi Piroli, Vinzenz Dallabetta, Johannes Kopp, Marc Walessa, Daniel Meissner, Klaus Dietmayer","LiDAR sensors are used in autonomous driving applications to accurately
perceive the environment. However, they are affected by adverse weather
conditions such as snow, fog, and rain. These everyday phenomena introduce
unwanted noise into the measurements, severely degrading the performance of
LiDAR-based perception systems. In this work, we propose a framework for
improving the robustness of LiDAR-based 3D object detectors against road spray.
Our approach uses a state-of-the-art adverse weather detection network to
filter out spray from the LiDAR point cloud, which is then used as input for
the object detector. In this way, the detected objects are less affected by the
adverse weather in the scene, resulting in a more accurate perception of the
environment. In addition to adverse weather filtering, we explore the use of
radar targets to further filter false positive detections. Tests on real-world
data show that our approach improves the robustness to road spray of several
popular 3D object detectors.",2310.00944v2,https://arxiv.org/pdf/2310.00944v2
COMPOSER: Scalable and Robust Modular Policies for Snake Robots,"Yuyou Zhang, Yaru Niu, Xingyu Liu, Ding Zhao","Snake robots have showcased remarkable compliance and adaptability in their
interaction with environments, mirroring the traits of their natural
counterparts. While their hyper-redundant and high-dimensional characteristics
add to this adaptability, they also pose great challenges to robot control.
Instead of perceiving the hyper-redundancy and flexibility of snake robots as
mere challenges, there lies an unexplored potential in leveraging these traits
to enhance robustness and generalizability at the control policy level. We seek
to develop a control policy that effectively breaks down the high
dimensionality of snake robots while harnessing their redundancy. In this work,
we consider the snake robot as a modular robot and formulate the control of the
snake robot as a cooperative Multi-Agent Reinforcement Learning (MARL) problem.
Each segment of the snake robot functions as an individual agent. Specifically,
we incorporate a self-attention mechanism to enhance the cooperative behavior
between agents. A high-level imagination policy is proposed to provide
additional rewards to guide the low-level control policy. We validate the
proposed method COMPOSER with five snake robot tasks, including goal reaching,
wall climbing, shape formation, tube crossing, and block pushing. COMPOSER
achieves the highest success rate across all tasks when compared to a
centralized baseline and four modular policy baselines. Additionally, we show
enhanced robustness against module corruption and significantly superior
zero-shot generalizability in our proposed method. The videos of this work are
available on our project page: https://sites.google.com/view/composer-snake/.",2310.00871v1,https://arxiv.org/pdf/2310.00871v1
"Counterfactual Image Generation for adversarially robust and
  interpretable Classifiers","Rafael Bischof, Florian Scheidegger, Michael A. Kraus, A. Cristiano I. Malossi","Neural Image Classifiers are effective but inherently hard to interpret and
susceptible to adversarial attacks. Solutions to both problems exist, among
others, in the form of counterfactual examples generation to enhance
explainability or adversarially augment training datasets for improved
robustness. However, existing methods exclusively address only one of the
issues. We propose a unified framework leveraging image-to-image translation
Generative Adversarial Networks (GANs) to produce counterfactual samples that
highlight salient regions for interpretability and act as adversarial samples
to augment the dataset for more robustness. This is achieved by combining the
classifier and discriminator into a single model that attributes real images to
their respective classes and flags generated images as ""fake"". We assess the
method's effectiveness by evaluating (i) the produced explainability masks on a
semantic segmentation task for concrete cracks and (ii) the model's resilience
against the Projected Gradient Descent (PGD) attack on a fruit defects
detection problem. Our produced saliency maps are highly descriptive, achieving
competitive IoU values compared to classical segmentation models despite being
trained exclusively on classification labels. Furthermore, the model exhibits
improved robustness to adversarial attacks, and we show how the discriminator's
""fakeness"" value serves as an uncertainty measure of the predictions.",2310.00761v1,https://arxiv.org/pdf/2310.00761v1
"Robust Sentiment Analysis for Low Resource languages Using Data
  Augmentation Approaches: A Case Study in Marathi","Aabha Pingle, Aditya Vyawahare, Isha Joshi, Rahul Tangsali, Geetanjali Kale, Raviraj Joshi","Sentiment analysis plays a crucial role in understanding the sentiment
expressed in text data. While sentiment analysis research has been extensively
conducted in English and other Western languages, there exists a significant
gap in research efforts for sentiment analysis in low-resource languages.
Limited resources, including datasets and NLP research, hinder the progress in
this area. In this work, we present an exhaustive study of data augmentation
approaches for the low-resource Indic language Marathi. Although
domain-specific datasets for sentiment analysis in Marathi exist, they often
fall short when applied to generalized and variable-length inputs. To address
this challenge, this research paper proposes four data augmentation techniques
for sentiment analysis in Marathi. The paper focuses on augmenting existing
datasets to compensate for the lack of sufficient resources. The primary
objective is to enhance sentiment analysis model performance in both in-domain
and cross-domain scenarios by leveraging data augmentation strategies. The data
augmentation approaches proposed showed a significant performance improvement
for cross-domain accuracies. The augmentation methods include paraphrasing,
back-translation; BERT-based random token replacement, named entity
replacement, and pseudo-label generation; GPT-based text and label generation.
Furthermore, these techniques can be extended to other low-resource languages
and for general text classification tasks.",2310.00734v1,https://arxiv.org/pdf/2310.00734v1
A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm,"Qi Wang, Yiqin Lv, Yanghe Feng, Zheng Xie, Jincai Huang","Meta learning is a promising paradigm to enable skill transfer across tasks.
Most previous methods employ the empirical risk minimization principle in
optimization. However, the resulting worst fast adaptation to a subset of tasks
can be catastrophic in risk-sensitive scenarios. To robustify fast adaptation,
this paper optimizes meta learning pipelines from a distributionally robust
perspective and meta trains models with the measure of expected tail risk. We
take the two-stage strategy as heuristics to solve the robust meta learning
problem, controlling the worst fast adaptation cases at a certain probabilistic
level. Experimental results show that our simple method can improve the
robustness of meta learning to task distributions and reduce the conditional
expectation of the worst fast adaptation risk.",2310.00708v1,https://arxiv.org/pdf/2310.00708v1
"Balancing Efficiency vs. Effectiveness and Providing Missing Label
  Robustness in Multi-Label Stream Classification","Sepehr Bakhshi, Fazli Can","Available works addressing multi-label classification in a data stream
environment focus on proposing accurate models; however, these models often
exhibit inefficiency and cannot balance effectiveness and efficiency. In this
work, we propose a neural network-based approach that tackles this issue and is
suitable for high-dimensional multi-label classification. Our model uses a
selective concept drift adaptation mechanism that makes it suitable for a
non-stationary environment. Additionally, we adapt our model to an environment
with missing labels using a simple yet effective imputation strategy and
demonstrate that it outperforms a vast majority of the state-of-the-art
supervised models. To achieve our purposes, we introduce a weighted binary
relevance-based approach named ML-BELS using the Broad Ensemble Learning System
(BELS) as its base classifier. Instead of a chain of stacked classifiers, our
model employs independent weighted ensembles, with the weights generated by the
predictions of a BELS classifier. We show that using the weighting strategy on
datasets with low label cardinality negatively impacts the accuracy of the
model; with this in mind, we use the label cardinality as a trigger for
applying the weights. We present an extensive assessment of our model using 11
state-of-the-art baselines, five synthetics, and 13 real-world datasets, all
with different characteristics. Our results demonstrate that the proposed
approach ML-BELS is successful in balancing effectiveness and efficiency, and
is robust to missing labels and concept drift.",2310.00665v1,https://arxiv.org/pdf/2310.00665v1
"A Survey of Robustness and Safety of 2D and 3D Deep Learning Models
  Against Adversarial Attacks","Yanjie Li, Bin Xie, Songtao Guo, Yuanyuan Yang, Bin Xiao","Benefiting from the rapid development of deep learning, 2D and 3D computer
vision applications are deployed in many safe-critical systems, such as
autopilot and identity authentication. However, deep learning models are not
trustworthy enough because of their limited robustness against adversarial
attacks. The physically realizable adversarial attacks further pose fatal
threats to the application and human safety. Lots of papers have emerged to
investigate the robustness and safety of deep learning models against
adversarial attacks. To lead to trustworthy AI, we first construct a general
threat model from different perspectives and then comprehensively review the
latest progress of both 2D and 3D adversarial attacks. We extend the concept of
adversarial examples beyond imperceptive perturbations and collate over 170
papers to give an overview of deep learning model robustness against various
adversarial attacks. To the best of our knowledge, we are the first to
systematically investigate adversarial attacks for 3D models, a flourishing
field applied to many real-world applications. In addition, we examine physical
adversarial attacks that lead to safety violations. Last but not least, we
summarize present popular topics, give insights on challenges, and shed light
on future research on trustworthy AI.",2310.00633v1,https://arxiv.org/pdf/2310.00633v1
"Understanding Robust Overfitting from the Feature Generalization
  Perspective","Chaojian Yu, Xiaolong Shi, Jun Yu, Bo Han, Tongliang Liu","Adversarial training (AT) constructs robust neural networks by incorporating
adversarial perturbations into natural data. However, it is plagued by the
issue of robust overfitting (RO), which severely damages the model's
robustness. In this paper, we investigate RO from a novel feature
generalization perspective. Specifically, we design factor ablation experiments
to assess the respective impacts of natural data and adversarial perturbations
on RO, identifying that the inducing factor of RO stems from natural data.
Given that the only difference between adversarial and natural training lies in
the inclusion of adversarial perturbations, we further hypothesize that
adversarial perturbations degrade the generalization of features in natural
data and verify this hypothesis through extensive experiments. Based on these
findings, we provide a holistic view of RO from the feature generalization
perspective and explain various empirical behaviors associated with RO. To
examine our feature generalization perspective, we devise two representative
methods, attack strength and data augmentation, to prevent the feature
generalization degradation during AT. Extensive experiments conducted on
benchmark datasets demonstrate that the proposed methods can effectively
mitigate RO and enhance adversarial robustness.",2310.00607v2,https://arxiv.org/pdf/2310.00607v2
"Understanding the Robustness of Randomized Feature Defense Against
  Query-Based Adversarial Attacks","Quang H. Nguyen, Yingjie Lao, Tung Pham, Kok-Seng Wong, Khoa D. Doan","Recent works have shown that deep neural networks are vulnerable to
adversarial examples that find samples close to the original image but can make
the model misclassify. Even with access only to the model's output, an attacker
can employ black-box attacks to generate such adversarial examples. In this
work, we propose a simple and lightweight defense against black-box attacks by
adding random noise to hidden features at intermediate layers of the model at
inference time. Our theoretical analysis confirms that this method effectively
enhances the model's resilience against both score-based and decision-based
black-box attacks. Importantly, our defense does not necessitate adversarial
training and has minimal impact on accuracy, rendering it applicable to any
pre-trained model. Our analysis also reveals the significance of selectively
adding noise to different parts of the model based on the gradient of the
adversarial objective function, which can be varied during the attack. We
demonstrate the robustness of our defense against multiple black-box attacks
through extensive empirical experiments involving diverse models with various
architectures.",2310.00567v1,https://arxiv.org/pdf/2310.00567v1
"Robust Nonparametric Hypothesis Testing to Understand Variability in
  Training Neural Networks","Sinjini Banerjee, Reilly Cannon, Tim Marrinan, Tony Chiang, Anand D. Sarwate","Training a deep neural network (DNN) often involves stochastic optimization,
which means each run will produce a different model. Several works suggest this
variability is negligible when models have the same performance, which in the
case of classification is test accuracy. However, models with similar test
accuracy may not be computing the same function. We propose a new measure of
closeness between classification models based on the output of the network
before thresholding. Our measure is based on a robust hypothesis-testing
framework and can be adapted to other quantities derived from trained models.",2310.00541v1,https://arxiv.org/pdf/2310.00541v1
"Certified Robustness via Dynamic Margin Maximization and Improved
  Lipschitz Regularization","Mahyar Fazlyab, Taha Entesari, Aniket Roy, Rama Chellappa","To improve the robustness of deep classifiers against adversarial
perturbations, many approaches have been proposed, such as designing new
architectures with better robustness properties (e.g., Lipschitz-capped
networks), or modifying the training process itself (e.g., min-max
optimization, constrained learning, or regularization). These approaches,
however, might not be effective at increasing the margin in the input (feature)
space. As a result, there has been an increasing interest in developing
training procedures that can directly manipulate the decision boundary in the
input space. In this paper, we build upon recent developments in this category
by developing a robust training algorithm whose objective is to increase the
margin in the output (logit) space while regularizing the Lipschitz constant of
the model along vulnerable directions. We show that these two objectives can
directly promote larger margins in the input space. To this end, we develop a
scalable method for calculating guaranteed differentiable upper bounds on the
Lipschitz constant of neural networks accurately and efficiently. The relative
accuracy of the bounds prevents excessive regularization and allows for more
direct manipulation of the decision boundary. Furthermore, our Lipschitz
bounding algorithm exploits the monotonicity and Lipschitz continuity of the
activation layers, and the resulting bounds can be used to design new layers
with controllable bounds on their Lipschitz constant. Experiments on the MNIST,
CIFAR-10, and Tiny-ImageNet data sets verify that our proposed algorithm
obtains competitively improved results compared to the state-of-the-art.",2310.00116v3,https://arxiv.org/pdf/2310.00116v3
Intrinsic Biologically Plausible Adversarial Robustness,"Matilde Tristany Farinha, Thomas Ortner, Giorgia Dellaferrera, Benjamin Grewe, Angeliki Pantazi","Artificial Neural Networks (ANNs) trained with Backpropagation (BP) excel in
different daily tasks but have a dangerous vulnerability: inputs with small
targeted perturbations, also known as adversarial samples, can drastically
disrupt their performance. Adversarial training, a technique in which the
training dataset is augmented with exemplary adversarial samples, is proven to
mitigate this problem but comes at a high computational cost. In contrast to
ANNs, humans are not susceptible to misclassifying these same adversarial
samples. Thus, one can postulate that biologically-plausible trained ANNs might
be more robust against adversarial attacks. In this work, we chose the
biologically-plausible learning algorithm Present the Error to Perturb the
Input To modulate Activity (PEPITA) as a case study and investigated this
question through a comparative analysis with BP-trained ANNs on various
computer vision tasks. We observe that PEPITA has a higher intrinsic
adversarial robustness and, when adversarially trained, also has a more
favorable natural-vs-adversarial performance trade-off. In particular, for the
same natural accuracies on the MNIST task, PEPITA's adversarial accuracies
decrease on average only by 0.26% while BP's decrease by 8.05%.",2309.17348v5,https://arxiv.org/pdf/2309.17348v5
Robust Stochastic Optimization via Gradient Quantile Clipping,"Ibrahim Merad, Stéphane Gaïffas","We introduce a clipping strategy for Stochastic Gradient Descent (SGD) which
uses quantiles of the gradient norm as clipping thresholds. We prove that this
new strategy provides a robust and efficient optimization algorithm for smooth
objectives (convex or non-convex), that tolerates heavy-tailed samples
(including infinite variance) and a fraction of outliers in the data stream
akin to Huber contamination. Our mathematical analysis leverages the connection
between constant step size SGD and Markov chains and handles the bias
introduced by clipping in an original way. For strongly convex objectives, we
prove that the iteration converges to a concentrated distribution and derive
high probability bounds on the final estimation error. In the non-convex case,
we prove that the limit distribution is localized on a neighborhood with low
gradient. We propose an implementation of this algorithm using rolling
quantiles which leads to a highly efficient optimization procedure with strong
robustness properties, as confirmed by our numerical experiments.",2309.17316v1,https://arxiv.org/pdf/2309.17316v1
Toward Robust Recommendation via Real-time Vicinal Defense,"Yichang Xu, Chenwang Wu, Defu Lian","Recommender systems have been shown to be vulnerable to poisoning attacks,
where malicious data is injected into the dataset to cause the recommender
system to provide biased recommendations. To defend against such attacks,
various robust learning methods have been proposed. However, most methods are
model-specific or attack-specific, making them lack generality, while other
methods, such as adversarial training, are oriented towards evasion attacks and
thus have a weak defense strength in poisoning attacks.
  In this paper, we propose a general method, Real-time Vicinal Defense (RVD),
which leverages neighboring training data to fine-tune the model before making
a recommendation for each user. RVD works in the inference phase to ensure the
robustness of the specific sample in real-time, so there is no need to change
the model structure and training process, making it more practical. Extensive
experimental results demonstrate that RVD effectively mitigates targeted
poisoning attacks across various models without sacrificing accuracy. Moreover,
the defensive effect can be further amplified when our method is combined with
other strategies.",2309.17278v1,https://arxiv.org/pdf/2309.17278v1
"RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit
  Neural Representations","Jiajun He, Gergely Flamich, Zongyu Guo, José Miguel Hernández-Lobato","COMpression with Bayesian Implicit NEural Representations (COMBINER) is a
recent data compression method that addresses a key inefficiency of previous
Implicit Neural Representation (INR)-based approaches: it avoids quantization
and enables direct optimization of the rate-distortion performance. However,
COMBINER still has significant limitations: 1) it uses factorized priors and
posterior approximations that lack flexibility; 2) it cannot effectively adapt
to local deviations from global patterns in the data; and 3) its performance
can be susceptible to modeling choices and the variational parameters'
initializations. Our proposed method, Robust and Enhanced COMBINER
(RECOMBINER), addresses these issues by 1) enriching the variational
approximation while retaining a low computational cost via a linear
reparameterization of the INR weights, 2) augmenting our INRs with learnable
positional encodings that enable them to adapt to local details and 3)
splitting high-resolution data into patches to increase robustness and
utilizing expressive hierarchical priors to capture dependency across patches.
We conduct extensive experiments across several data modalities, showcasing
that RECOMBINER achieves competitive results with the best INR-based methods
and even outperforms autoencoder-based codecs on low-resolution images at low
bitrates. Our PyTorch implementation is available at
https://github.com/cambridge-mlg/RECOMBINER/.",2309.17182v2,https://arxiv.org/pdf/2309.17182v2
"Prototype Generation: Robust Feature Visualisation for Data Independent
  Interpretability","Arush Tagade, Jessica Rumbelow","We introduce Prototype Generation, a stricter and more robust form of feature
visualisation for model-agnostic, data-independent interpretability of image
classification models. We demonstrate its ability to generate inputs that
result in natural activation paths, countering previous claims that feature
visualisation algorithms are untrustworthy due to the unnatural internal
activations. We substantiate these claims by quantitatively measuring
similarity between the internal activations of our generated prototypes and
natural images. We also demonstrate how the interpretation of generated
prototypes yields important insights, highlighting spurious correlations and
biases learned by models which quantitative methods over test-sets cannot
identify.",2309.17144v1,https://arxiv.org/pdf/2309.17144v1
On Continuity of Robust and Accurate Classifiers,"Ramin Barati, Reza Safabakhsh, Mohammad Rahmati","The reliability of a learning model is key to the successful deployment of
machine learning in various applications. Creating a robust model, particularly
one unaffected by adversarial attacks, requires a comprehensive understanding
of the adversarial examples phenomenon. However, it is difficult to describe
the phenomenon due to the complicated nature of the problems in machine
learning. It has been shown that adversarial training can improve the
robustness of the hypothesis. However, this improvement comes at the cost of
decreased performance on natural samples. Hence, it has been suggested that
robustness and accuracy of a hypothesis are at odds with each other. In this
paper, we put forth the alternative proposal that it is the continuity of a
hypothesis that is incompatible with its robustness and accuracy. In other
words, a continuous function cannot effectively learn the optimal robust
hypothesis. To this end, we will introduce a framework for a rigorous study of
harmonic and holomorphic hypothesis in learning theory terms and provide
empirical evidence that continuous hypotheses does not perform as well as
discontinuous hypotheses in some common machine learning tasks. From a
practical point of view, our results suggests that a robust and accurate
learning rule would train different continuous hypotheses for different regions
of the domain. From a theoretical perspective, our analysis explains the
adversarial examples phenomenon as a conflict between the continuity of a
sequence of functions and its uniform convergence to a discontinuous function.",2309.17048v1,https://arxiv.org/pdf/2309.17048v1
"Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty
  and Smoothness","Xiaoyu Wen, Xudong Yu, Rui Yang, Chenjia Bai, Zhen Wang","To obtain a near-optimal policy with fewer interactions in Reinforcement
Learning (RL), a promising approach involves the combination of offline RL,
which enhances sample efficiency by leveraging offline datasets, and online RL,
which explores informative transitions by interacting with the environment.
Offline-to-Online (O2O) RL provides a paradigm for improving an offline trained
agent within limited online interactions. However, due to the significant
distribution shift between online experiences and offline data, most offline RL
algorithms suffer from performance drops and fail to achieve stable policy
improvement in O2O adaptation. To address this problem, we propose the Robust
Offline-to-Online (RO2O) algorithm, designed to enhance offline policies
through uncertainty and smoothness, and to mitigate the performance drop in
online adaptation. Specifically, RO2O incorporates Q-ensemble for uncertainty
penalty and adversarial samples for policy and value smoothness, which enable
RO2O to maintain a consistent learning procedure in online adaptation without
requiring special changes to the learning objective. Theoretical analyses in
linear MDPs demonstrate that the uncertainty and smoothness lead to a tighter
optimality bound in O2O against distribution shift. Experimental results
illustrate the superiority of RO2O in facilitating stable offline-to-online
learning and achieving significant improvement with limited online
interactions.",2309.16973v1,https://arxiv.org/pdf/2309.16973v1
Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow,"Sizhe Wei, Yuxi Wei, Yue Hu, Yifan Lu, Yiqi Zhong, Siheng Chen, Ya Zhang","Collaborative perception can substantially boost each agent's perception
ability by facilitating communication among multiple agents. However, temporal
asynchrony among agents is inevitable in the real world due to communication
delays, interruptions, and clock misalignments. This issue causes information
mismatch during multi-agent fusion, seriously shaking the foundation of
collaboration. To address this issue, we propose CoBEVFlow, an
asynchrony-robust collaborative perception system based on bird's eye view
(BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align
asynchronous collaboration messages sent by multiple agents. To model the
motion in a scene, we propose BEV flow, which is a collection of the motion
vector corresponding to each spatial location. Based on BEV flow, asynchronous
perceptual features can be reassigned to appropriate positions, mitigating the
impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle
asynchronous collaboration messages sent at irregular, continuous time stamps
without discretization; and (ii) with BEV flow, CoBEVFlow only transports the
original perceptual features, instead of generating new perceptual features,
avoiding additional noises. To validate CoBEVFlow's efficacy, we create
IRregular V2V(IRV2V), the first synthetic collaborative perception dataset with
various temporal asynchronies that simulate different real-world scenarios.
Extensive experiments conducted on both IRV2V and the real-world dataset
DAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is
robust in extremely asynchronous settings. The code is available at
https://github.com/MediaBrain-SJTU/CoBEVFlow.",2309.16940v2,https://arxiv.org/pdf/2309.16940v2
Robust Offline Reinforcement Learning -- Certify the Confidence Interval,"Jiarui Yao, Simon Shaolei Du","Currently, reinforcement learning (RL), especially deep RL, has received more
and more attention in the research area. However, the security of RL has been
an obvious problem due to the attack manners becoming mature. In order to
defend against such adversarial attacks, several practical approaches are
developed, such as adversarial training, data filtering, etc. However, these
methods are mostly based on empirical algorithms and experiments, without
rigorous theoretical analysis of the robustness of the algorithms. In this
paper, we develop an algorithm to certify the robustness of a given policy
offline with random smoothing, which could be proven and conducted as
efficiently as ones without random smoothing. Experiments on different
environments confirm the correctness of our algorithm.",2309.16631v2,https://arxiv.org/pdf/2309.16631v2
"""AI enhances our performance, I have no doubt this one will do the
  same"": The Placebo effect is robust to negative descriptions of AI","Agnes M. Kloft, Robin Welsch, Thomas Kosch, Steeven Villa","Heightened AI expectations facilitate performance in human-AI interactions
through placebo effects. While lowering expectations to control for placebo
effects is advisable, overly negative expectations could induce nocebo effects.
In a letter discrimination task, we informed participants that an AI would
either increase or decrease their performance by adapting the interface, but in
reality, no AI was present in any condition. A Bayesian analysis showed that
participants had high expectations and performed descriptively better
irrespective of the AI description when a sham-AI was present. Using cognitive
modeling, we could trace this advantage back to participants gathering more
information. A replication study verified that negative AI descriptions do not
alter expectations, suggesting that performance expectations with AI are biased
and robust to negative verbal descriptions. We discuss the impact of user
expectations on AI interactions and evaluation and provide a behavioral placebo
marker for human-AI interaction",2309.16606v2,https://arxiv.org/pdf/2309.16606v2
"High-dimensional robust regression under heavy-tailed data: Asymptotics
  and Universality","Urte Adomaityte, Leonardo Defilippis, Bruno Loureiro, Gabriele Sicuro","We investigate the high-dimensional properties of robust regression
estimators in the presence of heavy-tailed contamination of both the covariates
and response functions. In particular, we provide a sharp asymptotic
characterisation of M-estimators trained on a family of elliptical covariate
and noise data distributions including cases where second and higher moments do
not exist. We show that, despite being consistent, the Huber loss with
optimally tuned location parameter $\delta$ is suboptimal in the
high-dimensional regime in the presence of heavy-tailed noise, highlighting the
necessity of further regularisation to achieve optimal performance. This result
also uncovers the existence of a transition in $\delta$ as a function of the
sample complexity and contamination. Moreover, we derive the decay rates for
the excess risk of ridge regression. We show that, while it is both optimal and
universal for covariate distributions with finite second moment, its decay rate
can be considerably faster when the covariates' second moment does not exist.
Finally, we show that our formulas readily generalise to a richer family of
models and data distributions, such as generalised linear estimation with
arbitrary convex regularisation trained on mixture models.",2309.16476v2,https://arxiv.org/pdf/2309.16476v2
"On the Trade-offs between Adversarial Robustness and Actionable
  Explanations","Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju","As machine learning models are increasingly being employed in various
high-stakes settings, it becomes important to ensure that predictions of these
models are not only adversarially robust, but also readily explainable to
relevant stakeholders. However, it is unclear if these two notions can be
simultaneously achieved or if there exist trade-offs between them. In this
work, we make one of the first attempts at studying the impact of adversarially
robust models on actionable explanations which provide end users with a means
for recourse. We theoretically and empirically analyze the cost (ease of
implementation) and validity (probability of obtaining a positive model
prediction) of recourses output by state-of-the-art algorithms when the
underlying models are adversarially robust vs. non-robust. More specifically,
we derive theoretical bounds on the differences between the cost and the
validity of the recourses generated by state-of-the-art algorithms for
adversarially robust vs. non-robust linear and non-linear models. Our empirical
results with multiple real-world datasets validate our theoretical results and
show the impact of varying degrees of model robustness on the cost and validity
of the resulting recourses. Our analyses demonstrate that adversarially robust
models significantly increase the cost and reduce the validity of the resulting
recourses, thus shedding light on the inherent trade-offs between adversarial
robustness and actionable explanations.",2309.16452v2,https://arxiv.org/pdf/2309.16452v2
"Adversarial Examples Might be Avoidable: The Role of Data Concentration
  in Adversarial Robustness","Ambar Pal, Jeremias Sulam, René Vidal","The susceptibility of modern machine learning classifiers to adversarial
examples has motivated theoretical results suggesting that these might be
unavoidable. However, these results can be too general to be applicable to
natural data distributions. Indeed, humans are quite robust for tasks involving
vision. This apparent conflict motivates a deeper dive into the question: Are
adversarial examples truly unavoidable? In this work, we theoretically
demonstrate that a key property of the data distribution -- concentration on
small-volume subsets of the input space -- determines whether a robust
classifier exists. We further demonstrate that, for a data distribution
concentrated on a union of low-dimensional linear subspaces, utilizing
structure in data naturally leads to classifiers that enjoy data-dependent
polyhedral robustness guarantees, improving upon methods for provable
certification in certain regimes.",2309.16096v2,https://arxiv.org/pdf/2309.16096v2
Robust Internal Representations for Domain Generalization,Mohammad Rostami,"This paper which is part of the New Faculty Highlights Invited Speaker
Program of AAAI'23, serves as a comprehensive survey of my research in transfer
learning by utilizing embedding spaces. The work reviewed in this paper
specifically revolves around the inherent challenges associated with continual
learning and limited availability of labeled data. By providing an overview of
my past and ongoing contributions, this paper aims to present a holistic
understanding of my research, paving the way for future explorations and
advancements in the field. My research delves into the various settings of
transfer learning, including, few-shot learning, zero-shot learning, continual
learning, domain adaptation, and distributed learning. I hope this survey
provides a forward-looking perspective for researchers who would like to focus
on similar research directions.",2309.15522v1,https://arxiv.org/pdf/2309.15522v1
The Robust Semantic Segmentation UNCV2023 Challenge Results,"Xuanlong Yu, Yi Zuo, Zitao Wang, Xiaowen Zhang, Jiaxuan Zhao, Yuting Yang, Licheng Jiao, Rui Peng, Xinyi Wang, Junpei Zhang, Kexin Zhang, Fang Liu, Roberto Alcover-Couso, Juan C. SanMiguel, Marcos Escudero-Viñolo, Hanlin Tian, Kenta Matsui, Tianhao Wang, Fahmy Adan, Zhitong Gao, Xuming He, Quentin Bouniot, Hossein Moghaddam, Shyam Nandan Rai, Fabio Cermelli, Carlo Masone, Andrea Pilzer, Elisa Ricci, Andrei Bursuc, Arno Solin, Martin Trapp, Rui Li, Angela Yao, Wenlong Chen, Ivor Simpson, Neill D. F. Campbell, Gianni Franchi","This paper outlines the winning solutions employed in addressing the MUAD
uncertainty quantification challenge held at ICCV 2023. The challenge was
centered around semantic segmentation in urban environments, with a particular
focus on natural adversarial scenarios. The report presents the results of 19
submitted entries, with numerous techniques drawing inspiration from
cutting-edge uncertainty quantification methodologies presented at prominent
conferences in the fields of computer vision and machine learning and journals
over the past few years. Within this document, the challenge is introduced,
shedding light on its purpose and objectives, which primarily revolved around
enhancing the robustness of semantic segmentation in urban scenes under varying
natural adversarial conditions. The report then delves into the top-performing
solutions. Moreover, the document aims to provide a comprehensive overview of
the diverse solutions deployed by all participants. By doing so, it seeks to
offer readers a deeper insight into the array of strategies that can be
leveraged to effectively handle the inherent uncertainties associated with
autonomous driving and semantic segmentation, especially within urban
environments.",2309.15478v1,https://arxiv.org/pdf/2309.15478v1
"Neural Stochastic Differential Equations for Robust and Explainable
  Analysis of Electromagnetic Unintended Radiated Emissions","Sumit Kumar Jha, Susmit Jha, Rickard Ewetz, Alvaro Velasquez","We present a comprehensive evaluation of the robustness and explainability of
ResNet-like models in the context of Unintended Radiated Emission (URE)
classification and suggest a new approach leveraging Neural Stochastic
Differential Equations (SDEs) to address identified limitations. We provide an
empirical demonstration of the fragility of ResNet-like models to Gaussian
noise perturbations, where the model performance deteriorates sharply and its
F1-score drops to near insignificance at 0.008 with a Gaussian noise of only
0.5 standard deviation. We also highlight a concerning discrepancy where the
explanations provided by ResNet-like models do not reflect the inherent
periodicity in the input data, a crucial attribute in URE detection from stable
devices. In response to these findings, we propose a novel application of
Neural SDEs to build models for URE classification that are not only robust to
noise but also provide more meaningful and intuitive explanations. Neural SDE
models maintain a high F1-score of 0.93 even when exposed to Gaussian noise
with a standard deviation of 0.5, demonstrating superior resilience to ResNet
models. Neural SDE models successfully recover the time-invariant or periodic
horizontal bands from the input data, a feature that was conspicuously missing
in the explanations generated by ResNet-like models. This advancement presents
a small but significant step in the development of robust and interpretable
models for real-world URE applications where data is inherently noisy and
assurance arguments demand interpretable machine learning predictions.",2309.15386v1,https://arxiv.org/pdf/2309.15386v1
Prediction Model For Wordle Game Results With High Robustness,"Jiaqi Weng, Chunlin Feng","In this study, we delve into the dynamics of Wordle using data analysis and
machine learning. Our analysis initially focused on the correlation between the
date and the number of submitted results. Due to initial popularity bias, we
modeled stable data using an ARIMAX model with coefficient values of 9, 0, 2,
and weekdays/weekends as the exogenous variable. We found no significant
relationship between word attributes and hard mode results.
  To predict word difficulty, we employed a Backpropagation Neural Network,
overcoming overfitting via feature engineering. We also used K-means
clustering, optimized at five clusters, to categorize word difficulty
numerically. Our findings indicate that on March 1st, 2023, around 12,884
results will be submitted and the word ""eerie"" averages 4.8 attempts, falling
into the hardest difficulty cluster.
  We further examined the percentage of loyal players and their propensity to
undertake daily challenges. Our models underwent rigorous sensitivity analyses,
including ADF, ACF, PACF tests, and cross-validation, confirming their
robustness. Overall, our study provides a predictive framework for Wordle
gameplay based on date or a given five-letter word. Results have been
summarized and submitted to the Puzzle Editor of the New York Times.",2309.14250v1,https://arxiv.org/pdf/2309.14250v1
"Diffusion Conditional Expectation Model for Efficient and Robust Target
  Speech Extraction","Leying Zhang, Yao Qian, Linfeng Yu, Heming Wang, Xinkai Wang, Hemin Yang, Long Zhou, Shujie Liu, Yanmin Qian, Michael Zeng","Target Speech Extraction (TSE) is a crucial task in speech processing that
focuses on isolating the clean speech of a specific speaker from complex
mixtures. While discriminative methods are commonly used for TSE, they can
introduce distortion in terms of speech perception quality. On the other hand,
generative approaches, particularly diffusion-based methods, can enhance speech
quality perceptually but suffer from slower inference speed. We propose an
efficient generative approach named Diffusion Conditional Expectation Model
(DCEM) for TSE. It can handle multi- and single-speaker scenarios in both noisy
and clean conditions. Additionally, we introduce Regenerate-DCEM (R-DCEM) that
can regenerate and optimize speech quality based on pre-processed speech from a
discriminative model. Our method outperforms conventional methods in terms of
both intrusive and non-intrusive metrics and demonstrates notable strengths in
inference efficiency and robustness to unseen tasks. Audio examples are
available online (https://vivian556123.github.io/dcem).",2309.13874v1,https://arxiv.org/pdf/2309.13874v1
"Benchmarking Local Robustness of High-Accuracy Binary Neural Networks
  for Enhanced Traffic Sign Recognition","Andreea Postovan, Mădălina Eraşcu","Traffic signs play a critical role in road safety and traffic management for
autonomous driving systems. Accurate traffic sign classification is essential
but challenging due to real-world complexities like adversarial examples and
occlusions. To address these issues, binary neural networks offer promise in
constructing classifiers suitable for resource-constrained devices.
  In our previous work, we proposed high-accuracy BNN models for traffic sign
recognition, focusing on compact size for limited computation and energy
resources. To evaluate their local robustness, this paper introduces a set of
benchmark problems featuring layers that challenge state-of-the-art
verification tools. These layers include binarized convolutions, max pooling,
batch normalization, fully connected. The difficulty of the verification
problem is given by the high number of network parameters (905k - 1.7 M), of
the input dimension (2.7k-12k), and of the number of regions (43) as well by
the fact that the neural networks are not sparse.
  The proposed BNN models and local robustness properties can be checked at
https://github.com/ChristopherBrix/vnncomp2023_benchmarks/tree/main/benchmarks/traffic_signs_recognition.
  The results of the 4th International Verification of Neural Networks
Competition (VNN-COMP'23) revealed the fact that 4, out of 7, solvers can
handle many of our benchmarks randomly selected (minimum is 6, maximum is 36,
out of 45). Surprisingly, tools output also wrong results or missing
counterexample (ranging from 1 to 4). Currently, our focus lies in exploring
the possibility of achieving a greater count of solved instances by extending
the allotted time (previously set at 8 minutes). Furthermore, we are intrigued
by the reasons behind the erroneous outcomes provided by the tools for certain
benchmarks.",2310.03033v1,https://arxiv.org/pdf/2310.03033v1
Projected Randomized Smoothing for Certified Adversarial Robustness,"Samuel Pfrommer, Brendon G. Anderson, Somayeh Sojoudi","Randomized smoothing is the current state-of-the-art method for producing
provably robust classifiers. While randomized smoothing typically yields robust
$\ell_2$-ball certificates, recent research has generalized provable robustness
to different norm balls as well as anisotropic regions. This work considers a
classifier architecture that first projects onto a low-dimensional
approximation of the data manifold and then applies a standard classifier. By
performing randomized smoothing in the low-dimensional projected space, we
characterize the certified region of our smoothed composite classifier back in
the high-dimensional input space and prove a tractable lower bound on its
volume. We show experimentally on CIFAR-10 and SVHN that classifiers without
the initial projection are vulnerable to perturbations that are normal to the
data manifold and yet are captured by the certified regions of our method. We
compare the volume of our certified regions against various baselines and show
that our method improves on the state-of-the-art by many orders of magnitude.",2309.13794v1,https://arxiv.org/pdf/2309.13794v1
"Design Principles of Robust Multi-Armed Bandit Framework in Video
  Recommendations","Belhassen Bayar, Phanideep Gampa, Ainur Yessenalina, Zhen Wen","Current multi-armed bandit approaches in recommender systems (RS) have
focused more on devising effective exploration techniques, while not adequately
addressing common exploitation challenges related to distributional changes and
item cannibalization. Little work exists to guide the design of robust bandit
frameworks that can address these frequent challenges in RS. In this paper, we
propose a new design principles to (i) make bandit models robust to
time-variant metadata signals, (ii) less prone to item cannibalization, and
(iii) prevent their weights fluctuating due to data sparsity. Through a series
of experiments, we systematically examine the influence of several important
bandit design choices. We demonstrate the advantage of our proposed design
principles at making bandit models robust to dynamic behavioral changes through
in-depth analyses. Noticeably, we show improved relative gain compared to a
baseline bandit model not incorporating our design choices of up to $11.88\%$
and $44.85\%$, respectively in ROC-AUC and PR-AUC. Case studies about fairness
in recommending specific popular and unpopular titles are presented, to
demonstrate the robustness of our proposed design at addressing popularity
biases.",2310.01419v1,https://arxiv.org/pdf/2310.01419v1
"GHN-QAT: Training Graph Hypernetworks to Predict Quantization-Robust
  Parameters of Unseen Limited Precision Neural Networks","Stone Yun, Alexander Wong","Graph Hypernetworks (GHN) can predict the parameters of varying unseen CNN
architectures with surprisingly good accuracy at a fraction of the cost of
iterative optimization. Following these successes, preliminary research has
explored the use of GHNs to predict quantization-robust parameters for 8-bit
and 4-bit quantized CNNs. However, this early work leveraged full-precision
float32 training and only quantized for testing. We explore the impact of
quantization-aware training and/or other quantization-based training strategies
on quantized robustness and performance of GHN predicted parameters for
low-precision CNNs. We show that quantization-aware training can significantly
improve quantized accuracy for GHN predicted parameters of 4-bit quantized CNNs
and even lead to greater-than-random accuracy for 2-bit quantized CNNs. These
promising results open the door for future explorations such as investigating
the use of GHN predicted parameters as initialization for further quantized
training of individual CNNs, further exploration of ""extreme bitwidth""
quantization, and mixed precision quantization schemes.",2309.13773v1,https://arxiv.org/pdf/2309.13773v1
Devil in the Number: Towards Robust Multi-modality Data Filter,"Yichen Xu, Zihan Xu, Wenhao Chai, Zhonghan Zhao, Enxin Song, Gaoang Wang","In order to appropriately filter multi-modality data sets on a web-scale, it
becomes crucial to employ suitable filtering methods to boost performance and
reduce training costs. For instance, LAION papers employs the CLIP score filter
to select data with CLIP scores surpassing a certain threshold. On the other
hand, T-MARS achieves high-quality data filtering by detecting and masking text
within images and then filtering by CLIP score. Through analyzing the dataset,
we observe a significant proportion of redundant information, such as numbers,
present in the textual content. Our experiments on a subset of the data unveil
the profound impact of these redundant elements on the CLIP scores. A logical
approach would involve reevaluating the CLIP scores after eliminating these
influences. Experimentally, our text-based CLIP filter outperforms the
top-ranked method on the ``small scale"" of DataComp (a data filtering
benchmark) on ImageNet distribution shifts, achieving a 3.6% performance
improvement. The results also demonstrate that our proposed text-masked filter
outperforms the original CLIP score filter when selecting the top 40% of the
data. The impact of numbers on CLIP and their handling provide valuable
insights for improving the effectiveness of CLIP training, including language
rewrite techniques.",2309.13770v1,https://arxiv.org/pdf/2309.13770v1
"Improving Robustness of Deep Convolutional Neural Networks via
  Multiresolution Learning","Hongyan Zhou, Yao Liang","The current learning process of deep learning, regardless of any deep neural
network (DNN) architecture and/or learning algorithm used, is essentially a
single resolution training. We explore multiresolution learning and show that
multiresolution learning can significantly improve robustness of DNN models for
both 1D signal and 2D signal (image) prediction problems. We demonstrate this
improvement in terms of both noise and adversarial robustness as well as with
small training dataset size. Our results also suggest that it may not be
necessary to trade standard accuracy for robustness with multiresolution
learning, which is, interestingly, contrary to the observation obtained from
the traditional single resolution learning setting.",2309.13752v2,https://arxiv.org/pdf/2309.13752v2
PanopticNDT: Efficient and Robust Panoptic Mapping,"Daniel Seichter, Benedict Stephan, Söhnke Benedikt Fischedick, Steffen Müller, Leonard Rabes, Horst-Michael Gross","As the application scenarios of mobile robots are getting more complex and
challenging, scene understanding becomes increasingly crucial. A mobile robot
that is supposed to operate autonomously in indoor environments must have
precise knowledge about what objects are present, where they are, what their
spatial extent is, and how they can be reached; i.e., information about free
space is also crucial. Panoptic mapping is a powerful instrument providing such
information. However, building 3D panoptic maps with high spatial resolution is
challenging on mobile robots, given their limited computing capabilities. In
this paper, we propose PanopticNDT - an efficient and robust panoptic mapping
approach based on occupancy normal distribution transform (NDT) mapping. We
evaluate our approach on the publicly available datasets Hypersim and
ScanNetV2. The results reveal that our approach can represent panoptic
information at a higher level of detail than other state-of-the-art approaches
while enabling real-time panoptic mapping on mobile robots. Finally, we prove
the real-world applicability of PanopticNDT with qualitative results in a
domestic application.",2309.13635v2,https://arxiv.org/pdf/2309.13635v2
PRIS: Practical robust invertible network for image steganography,"Hang Yang, Yitian Xu, Xuhua Liu, Xiaodong Ma","Image steganography is a technique of hiding secret information inside
another image, so that the secret is not visible to human eyes and can be
recovered when needed. Most of the existing image steganography methods have
low hiding robustness when the container images affected by distortion. Such as
Gaussian noise and lossy compression. This paper proposed PRIS to improve the
robustness of image steganography, it based on invertible neural networks, and
put two enhance modules before and after the extraction process with a 3-step
training strategy. Moreover, rounding error is considered which is always
ignored by existing methods, but actually it is unavoidable in practical. A
gradient approximation function (GAF) is also proposed to overcome the
undifferentiable issue of rounding distortion. Experimental results show that
our PRIS outperforms the state-of-the-art robust image steganography method in
both robustness and practicability. Codes are available at
https://github.com/yanghangAI/PRIS, demonstration of our model in practical at
http://yanghang.site/hide/.",2309.13620v2,https://arxiv.org/pdf/2309.13620v2
"Robust Distributed Learning: Tight Error Bounds and Breakdown Point
  under Data Heterogeneity","Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Rafaël Pinot, Geovani Rizk","The theory underlying robust distributed learning algorithms, designed to
resist adversarial machines, matches empirical observations when data is
homogeneous. Under data heterogeneity however, which is the norm in practical
scenarios, established lower bounds on the learning error are essentially
vacuous and greatly mismatch empirical observations. This is because the
heterogeneity model considered is too restrictive and does not cover basic
learning tasks such as least-squares regression. We consider in this paper a
more realistic heterogeneity model, namely (G,B)-gradient dissimilarity, and
show that it covers a larger class of learning problems than existing theory.
Notably, we show that the breakdown point under heterogeneity is lower than the
classical fraction 1/2. We also prove a new lower bound on the learning error
of any distributed learning algorithm. We derive a matching upper bound for a
robust variant of distributed gradient descent, and empirically show that our
analysis reduces the gap between theory and practice.",2309.13591v2,https://arxiv.org/pdf/2309.13591v2
"DFRD: Data-Free Robustness Distillation for Heterogeneous Federated
  Learning","Kangyang Luo, Shuai Wang, Yexuan Fu, Xiang Li, Yunshi Lan, Ming Gao","Federated Learning (FL) is a privacy-constrained decentralized machine
learning paradigm in which clients enable collaborative training without
compromising private data. However, how to learn a robust global model in the
data-heterogeneous and model-heterogeneous FL scenarios is challenging. To
address it, we resort to data-free knowledge distillation to propose a new FL
method (namely DFRD). DFRD equips a conditional generator on the server to
approximate the training space of the local models uploaded by clients, and
systematically investigates its training in terms of fidelity, transferability}
and diversity. To overcome the catastrophic forgetting of the global model
caused by the distribution shifts of the generator across communication rounds,
we maintain an exponential moving average copy of the generator on the server.
Additionally, we propose dynamic weighting and label sampling to accurately
extract knowledge from local models. Finally, our extensive experiments on
various image classification tasks illustrate that DFRD achieves significant
performance gains compared to SOTA baselines.",2309.13546v2,https://arxiv.org/pdf/2309.13546v2
Robust Navigation with Cross-Modal Fusion and Knowledge Transfer,"Wenzhe Cai, Guangran Cheng, Lingyue Kong, Lu Dong, Changyin Sun","Recently, learning-based approaches show promising results in navigation
tasks. However, the poor generalization capability and the simulation-reality
gap prevent a wide range of applications. We consider the problem of improving
the generalization of mobile robots and achieving sim-to-real transfer for
navigation skills. To that end, we propose a cross-modal fusion method and a
knowledge transfer framework for better generalization. This is realized by a
teacher-student distillation architecture. The teacher learns a discriminative
representation and the near-perfect policy in an ideal environment. By
imitating the behavior and representation of the teacher, the student is able
to align the features from noisy multi-modal input and reduce the influence of
variations on navigation policy. We evaluate our method in simulated and
real-world environments. Experiments show that our method outperforms the
baselines by a large margin and achieves robust navigation performance with
varying working conditions.",2309.13266v1,https://arxiv.org/pdf/2309.13266v1
"Spatial-frequency channels, shape bias, and adversarial robustness","Ajay Subramanian, Elena Sizikova, Najib J. Majaj, Denis G. Pelli","What spatial frequency information do humans and neural networks use to
recognize objects? In neuroscience, critical band masking is an established
tool that can reveal the frequency-selective filters used for object
recognition. Critical band masking measures the sensitivity of recognition
performance to noise added at each spatial frequency. Existing critical band
masking studies show that humans recognize periodic patterns (gratings) and
letters by means of a spatial-frequency filter (or ""channel"") that has a
frequency bandwidth of one octave (doubling of frequency). Here, we introduce
critical band masking as a task for network-human comparison and test 14 humans
and 76 neural networks on 16-way ImageNet categorization in the presence of
narrowband noise. We find that humans recognize objects in natural images using
the same one-octave-wide channel that they use for letters and gratings, making
it a canonical feature of human object recognition. Unlike humans, the neural
network channel is very broad, 2-4 times wider than the human channel. Thus,
noise at certain high and low frequencies will impair network performance and
spare human performance. Adversarial and augmented-image training are commonly
used to increase network robustness and shape bias. Does this training align
network and human object recognition channels? Three network channel properties
(bandwidth, center frequency, peak noise sensitivity) correlate strongly with
shape bias (51% variance explained) and robustness of adversarially-trained
networks (66% variance explained). Adversarial training increases robustness
but expands the channel bandwidth even further beyond the human bandwidth.
Thus, critical band masking reveals that the network channel is more than twice
as wide as the human channel, and that adversarial training only makes it
worse. Networks with narrower channels might be more robust.",2309.13190v2,https://arxiv.org/pdf/2309.13190v2
"Pixel-wise Smoothing for Certified Robustness against Camera Motion
  Perturbations","Hanjiang Hu, Zuxin Liu, Linyi Li, Jiacheng Zhu, Ding Zhao","Deep learning-based visual perception models lack robustness when faced with
camera motion perturbations in practice. The current certification process for
assessing robustness is costly and time-consuming due to the extensive number
of image projections required for Monte Carlo sampling in the 3D camera motion
space. To address these challenges, we present a novel, efficient, and
practical framework for certifying the robustness of 3D-2D projective
transformations against camera motion perturbations. Our approach leverages a
smoothing distribution over the 2D pixel space instead of in the 3D physical
space, eliminating the need for costly camera motion sampling and significantly
enhancing the efficiency of robustness certifications. With the pixel-wise
smoothed classifier, we are able to fully upper bound the projection errors
using a technique of uniform partitioning in camera motion space. Additionally,
we extend our certification framework to a more general scenario where only a
single-frame point cloud is required in the projection oracle. Through
extensive experimentation, we validate the trade-off between effectiveness and
efficiency enabled by our proposed method. Remarkably, our approach achieves
approximately 80% certified accuracy while utilizing only 30% of the projected
image frames. The code is available at
https://github.com/HanjiangHu/pixel-wise-smoothing.",2309.13150v2,https://arxiv.org/pdf/2309.13150v2
"FairComp: Workshop on Fairness and Robustness in Machine Learning for
  Ubiquitous Computing","Sofia Yfantidou, Dimitris Spathis, Marios Constantinides, Tong Xia, Niels van Berkel","How can we ensure that Ubiquitous Computing (UbiComp) research outcomes are
both ethical and fair? While fairness in machine learning (ML) has gained
traction in recent years, fairness in UbiComp remains unexplored. This workshop
aims to discuss fairness in UbiComp research and its social, technical, and
legal implications. From a social perspective, we will examine the relationship
between fairness and UbiComp research and identify pathways to ensure that
ubiquitous technologies do not cause harm or infringe on individual rights.
From a technical perspective, we will initiate a discussion on data practices
to develop bias mitigation approaches tailored to UbiComp research. From a
legal perspective, we will examine how new policies shape our community's work
and future research. We aim to foster a vibrant community centered around the
topic of responsible UbiComp, while also charting a clear path for future
research endeavours in this field.",2309.12877v1,https://arxiv.org/pdf/2309.12877v1
"Robotic Handling of Compliant Food Objects by Robust Learning from
  Demonstration","Ekrem Misimi, Alexander Olofsson, Aleksander Eilertsen, Elling Ruud Øye, John Reidar Mathiassen","The robotic handling of compliant and deformable food raw materials,
characterized by high biological variation, complex geometrical 3D shapes, and
mechanical structures and texture, is currently in huge demand in the ocean
space, agricultural, and food industries. Many tasks in these industries are
performed manually by human operators who, due to the laborious and tedious
nature of their tasks, exhibit high variability in execution, with variable
outcomes. The introduction of robotic automation for most complex processing
tasks has been challenging due to current robot learning policies. A more
consistent learning policy involving skilled operators is desired. In this
paper, we address the problem of robot learning when presented with
inconsistent demonstrations. To this end, we propose a robust learning policy
based on Learning from Demonstration (LfD) for robotic grasping of food
compliant objects. The approach uses a merging of RGB-D images and tactile data
in order to estimate the necessary pose of the gripper, gripper finger
configuration and forces exerted on the object in order to achieve effective
robot handling. During LfD training, the gripper pose, finger configurations
and tactile values for the fingers, as well as RGB-D images are saved. We
present an LfD learning policy that automatically removes inconsistent
demonstrations, and estimates the teacher's intended policy. The performance of
our approach is validated and demonstrated for fragile and compliant food
objects with complex 3D shapes. The proposed approach has a vast range of
potential applications in the aforementioned industry sectors.",2309.12856v1,https://arxiv.org/pdf/2309.12856v1
Doubly Robust Proximal Causal Learning for Continuous Treatments,"Yong Wu, Yanwei Fu, Shouyan Wang, Xinwei Sun","Proximal causal learning is a promising framework for identifying the causal
effect under the existence of unmeasured confounders. Within this framework,
the doubly robust (DR) estimator was derived and has shown its effectiveness in
estimation, especially when the model assumption is violated. However, the
current form of the DR estimator is restricted to binary treatments, while the
treatment can be continuous in many real-world applications. The primary
obstacle to continuous treatments resides in the delta function present in the
original DR estimator, making it infeasible in causal effect estimation and
introducing a heavy computational burden in nuisance function estimation. To
address these challenges, we propose a kernel-based DR estimator that can well
handle continuous treatments. Equipped with its smoothness, we show that its
oracle form is a consistent approximation of the influence function. Further,
we propose a new approach to efficiently solve the nuisance functions. We then
provide a comprehensive convergence analysis in terms of the mean square error.
We demonstrate the utility of our estimator on synthetic datasets and
real-world applications.",2309.12819v3,https://arxiv.org/pdf/2309.12819v3
"Multiply Robust Federated Estimation of Targeted Average Treatment
  Effects","Larry Han, Zhu Shen, Jose Zubizarreta","Federated or multi-site studies have distinct advantages over single-site
studies, including increased generalizability, the ability to study
underrepresented populations, and the opportunity to study rare exposures and
outcomes. However, these studies are challenging due to the need to preserve
the privacy of each individual's data and the heterogeneity in their covariate
distributions. We propose a novel federated approach to derive valid causal
inferences for a target population using multi-site data. We adjust for
covariate shift and covariate mismatch between sites by developing
multiply-robust and privacy-preserving nuisance function estimation. Our
methodology incorporates transfer learning to estimate ensemble weights to
combine information from source sites. We show that these learned weights are
efficient and optimal under different scenarios. We showcase the finite sample
advantages of our approach in terms of efficiency and robustness compared to
existing approaches.",2309.12600v1,https://arxiv.org/pdf/2309.12600v1
Improving Machine Learning Robustness via Adversarial Training,"Long Dang, Thushari Hapuarachchi, Kaiqi Xiong, Jing Lin","As Machine Learning (ML) is increasingly used in solving various tasks in
real-world applications, it is crucial to ensure that ML algorithms are robust
to any potential worst-case noises, adversarial attacks, and highly unusual
situations when they are designed. Studying ML robustness will significantly
help in the design of ML algorithms. In this paper, we investigate ML
robustness using adversarial training in centralized and decentralized
environments, where ML training and testing are conducted in one or multiple
computers. In the centralized environment, we achieve a test accuracy of 65.41%
and 83.0% when classifying adversarial examples generated by Fast Gradient Sign
Method and DeepFool, respectively. Comparing to existing studies, these results
demonstrate an improvement of 18.41% for FGSM and 47% for DeepFool. In the
decentralized environment, we study Federated learning (FL) robustness by using
adversarial training with independent and identically distributed (IID) and
non-IID data, respectively, where CIFAR-10 is used in this research. In the IID
data case, our experimental results demonstrate that we can achieve such a
robust accuracy that it is comparable to the one obtained in the centralized
environment. Moreover, in the non-IID data case, the natural accuracy drops
from 66.23% to 57.82%, and the robust accuracy decreases by 25% and 23.4% in
C&W and Projected Gradient Descent (PGD) attacks, compared to the IID data
case, respectively. We further propose an IID data-sharing approach, which
allows for increasing the natural accuracy to 85.04% and the robust accuracy
from 57% to 72% in C&W attacks and from 59% to 67% in PGD attacks.",2309.12593v1,https://arxiv.org/pdf/2309.12593v1
"Provably Robust and Plausible Counterfactual Explanations for Neural
  Networks via Robust Optimisation","Junqi Jiang, Jianglin Lan, Francesco Leofante, Antonio Rago, Francesca Toni","Counterfactual Explanations (CEs) have received increasing interest as a
major methodology for explaining neural network classifiers. Usually, CEs for
an input-output pair are defined as data points with minimum distance to the
input that are classified with a different label than the output. To tackle the
established problem that CEs are easily invalidated when model parameters are
updated (e.g. retrained), studies have proposed ways to certify the robustness
of CEs under model parameter changes bounded by a norm ball. However, existing
methods targeting this form of robustness are not sound or complete, and they
may generate implausible CEs, i.e., outliers wrt the training dataset. In fact,
no existing method simultaneously optimises for closeness and plausibility
while preserving robustness guarantees. In this work, we propose Provably
RObust and PLAusible Counterfactual Explanations (PROPLACE), a method
leveraging on robust optimisation techniques to address the aforementioned
limitations in the literature. We formulate an iterative algorithm to compute
provably robust CEs and prove its convergence, soundness and completeness.
Through a comparative experiment involving six baselines, five of which target
robustness, we show that PROPLACE achieves state-of-the-art performances
against metrics on three evaluation aspects.",2309.12545v2,https://arxiv.org/pdf/2309.12545v2
"Robust Energy Consumption Prediction with a Missing Value-Resilient
  Metaheuristic-based Neural Network in Mobile App Development","Seyed Jalaleddin Mousavirad, Luís A. Alexandre","Energy consumption is a fundamental concern in mobile application
development, bearing substantial significance for both developers and
end-users. Main objective of this research is to propose a novel neural
network-based framework, enhanced by a metaheuristic approach, to achieve
robust energy prediction in the context of mobile app development. The
metaheuristic approach here aims to achieve two goals: 1) identifying suitable
learning algorithms and their corresponding hyperparameters, and 2) determining
the optimal number of layers and neurons within each layer. Moreover, due to
limitations in accessing certain aspects of a mobile phone, there might be
missing data in the data set, and the proposed framework can handle this. In
addition, we conducted an optimal algorithm selection strategy, employing 13
base and advanced metaheuristic algorithms, to identify the best algorithm
based on accuracy and resistance to missing values. The representation in our
proposed metaheuristic algorithm is variable-size, meaning that the length of
the candidate solutions changes over time. We compared the algorithms based on
the architecture found by each algorithm at different levels of missing values,
accuracy, F-measure, and stability analysis. Additionally, we conducted a
Wilcoxon signed-rank test for statistical comparison of the results. The
extensive experiments show that our proposed approach significantly improves
energy consumption prediction. Particularly, the JADE algorithm, a variant of
Differential Evolution (DE), DE, and the Covariance Matrix Adaptation Evolution
Strategy deliver superior results under various conditions and across different
missing value levels.",2309.12484v2,https://arxiv.org/pdf/2309.12484v2
"Impact of architecture on robustness and interpretability of
  multispectral deep neural networks","Charles Godfrey, Elise Bishoff, Myles McKay, Eleanor Byler","Including information from additional spectral bands (e.g., near-infrared)
can improve deep learning model performance for many vision-oriented tasks.
There are many possible ways to incorporate this additional information into a
deep learning model, but the optimal fusion strategy has not yet been
determined and can vary between applications. At one extreme, known as ""early
fusion,"" additional bands are stacked as extra channels to obtain an input
image with more than three channels. At the other extreme, known as ""late
fusion,"" RGB and non-RGB bands are passed through separate branches of a deep
learning model and merged immediately before a final classification or
segmentation layer. In this work, we characterize the performance of a suite of
multispectral deep learning models with different fusion approaches, quantify
their relative reliance on different input bands and evaluate their robustness
to naturalistic image corruptions affecting one or more input channels.",2309.12463v2,https://arxiv.org/pdf/2309.12463v2
A Convex Framework for Confounding Robust Inference,"Kei Ishikawa, Niao He, Takafumi Kanamori","We study policy evaluation of offline contextual bandits subject to
unobserved confounders. Sensitivity analysis methods are commonly used to
estimate the policy value under the worst-case confounding over a given
uncertainty set. However, existing work often resorts to some coarse relaxation
of the uncertainty set for the sake of tractability, leading to overly
conservative estimation of the policy value. In this paper, we propose a
general estimator that provides a sharp lower bound of the policy value using
convex programming. The generality of our estimator enables various extensions
such as sensitivity analysis with f-divergence, model selection with cross
validation and information criterion, and robust policy learning with the sharp
lower bound. Furthermore, our estimation method can be reformulated as an
empirical risk minimization problem thanks to the strong duality, which enables
us to provide strong theoretical guarantees of the proposed estimator using
techniques of the M-estimation.",2309.12450v2,https://arxiv.org/pdf/2309.12450v2
Environment-biased Feature Ranking for Novelty Detection Robustness,"Stefan Smeu, Elena Burceanu, Emanuela Haller, Andrei Liviu Nicolicioiu","We tackle the problem of robust novelty detection, where we aim to detect
novelties in terms of semantic content while being invariant to changes in
other, irrelevant factors. Specifically, we operate in a setup with multiple
environments, where we determine the set of features that are associated more
with the environments, rather than to the content relevant for the task. Thus,
we propose a method that starts with a pretrained embedding and a multi-env
setup and manages to rank the features based on their environment-focus. First,
we compute a per-feature score based on the feature distribution variance
between envs. Next, we show that by dropping the highly scored ones, we manage
to remove spurious correlations and improve the overall performance by up to
6%, both in covariance and sub-population shift cases, both for a real and a
synthetic benchmark, that we introduce for this task.",2309.12301v2,https://arxiv.org/pdf/2309.12301v2
"Soft Merging: A Flexible and Robust Soft Model Merging Approach for
  Enhanced Neural Network Performance","Hao Chen, Yusen Wu, Phuong Nguyen, Chao Liu, Yelena Yesha","Stochastic Gradient Descent (SGD), a widely used optimization algorithm in
deep learning, is often limited to converging to local optima due to the
non-convex nature of the problem. Leveraging these local optima to improve
model performance remains a challenging task. Given the inherent complexity of
neural networks, the simple arithmetic averaging of the obtained local optima
models in undesirable results. This paper proposes a {\em soft merging} method
that facilitates rapid merging of multiple models, simplifies the merging of
specific parts of neural networks, and enhances robustness against malicious
models with extreme values. This is achieved by learning gate parameters
through a surrogate of the $l_0$ norm using hard concrete distribution without
modifying the model weights of the given local optima models. This merging
process not only enhances the model performance by converging to a better local
optimum, but also minimizes computational costs, offering an efficient and
explicit learning process integrated with stochastic gradient descent. Thorough
experiments underscore the effectiveness and superior performance of the merged
neural networks.",2309.12259v1,https://arxiv.org/pdf/2309.12259v1
Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval,"Luis Carvalho, Gerhard Widmer","A range of applications of multi-modal music information retrieval is centred
around the problem of connecting large collections of sheet music (images) to
corresponding audio recordings, that is, identifying pairs of audio and score
excerpts that refer to the same musical content. One of the typical and most
recent approaches to this task employs cross-modal deep learning architectures
to learn joint embedding spaces that link the two distinct modalities - audio
and sheet music images. While there has been steady improvement on this front
over the past years, a number of open problems still prevent large-scale
employment of this methodology. In this article we attempt to provide an
insightful examination of the current developments on audio-sheet music
retrieval via deep learning methods. We first identify a set of main challenges
on the road towards robust and large-scale cross-modal music retrieval in real
scenarios. We then highlight the steps we have taken so far to address some of
these challenges, documenting step-by-step improvement along several
dimensions. We conclude by analysing the remaining challenges and present ideas
for solving these, in order to pave the way to a unified and robust methodology
for cross-modal music retrieval.",2309.12158v1,https://arxiv.org/pdf/2309.12158v1
"Self-Supervised Contrastive Learning for Robust Audio-Sheet Music
  Retrieval Systems","Luis Carvalho, Tobias Washüttl, Gerhard Widmer","Linking sheet music images to audio recordings remains a key problem for the
development of efficient cross-modal music retrieval systems. One of the
fundamental approaches toward this task is to learn a cross-modal embedding
space via deep neural networks that is able to connect short snippets of audio
and sheet music. However, the scarcity of annotated data from real musical
content affects the capability of such methods to generalize to real retrieval
scenarios. In this work, we investigate whether we can mitigate this limitation
with self-supervised contrastive learning, by exposing a network to a large
amount of real music data as a pre-training step, by contrasting randomly
augmented views of snippets of both modalities, namely audio and sheet images.
Through a number of experiments on synthetic and real piano data, we show that
pre-trained models are able to retrieve snippets with better precision in all
scenarios and pre-training configurations. Encouraged by these results, we
employ the snippet embeddings in the higher-level task of cross-modal piece
identification and conduct more experiments on several retrieval
configurations. In this task, we observe that the retrieval quality improves
from 30% up to 100% when real music data is present. We then conclude by
arguing for the potential of self-supervised contrastive learning for
alleviating the annotated data scarcity in multi-modal music retrieval models.",2309.12134v1,https://arxiv.org/pdf/2309.12134v1
"Robust Approximation Algorithms for Non-monotone $k$-Submodular
  Maximization under a Knapsack Constraint","Dung T. K. Ha, Canh V. Pham, Tan D. Tran, Huan X. Hoang","The problem of non-monotone $k$-submodular maximization under a knapsack
constraint ($\kSMK$) over the ground set size $n$ has been raised in many
applications in machine learning, such as data summarization, information
propagation, etc. However, existing algorithms for the problem are facing
questioning of how to overcome the non-monotone case and how to fast return a
good solution in case of the big size of data. This paper introduces two
deterministic approximation algorithms for the problem that competitively
improve the query complexity of existing algorithms.
  Our first algorithm, $\LAA$, returns an approximation ratio of $1/19$ within
$O(nk)$ query complexity. The second one, $\RLA$, improves the approximation
ratio to $1/5-\epsilon$ in $O(nk)$ queries, where $\epsilon$ is an input
parameter.
  Our algorithms are the first ones that provide constant approximation ratios
within only $O(nk)$ query complexity for the non-monotone objective. They,
therefore, need fewer the number of queries than state-of-the-the-art ones by a
factor of $\Omega(\log n)$.
  Besides the theoretical analysis, we have evaluated our proposed ones with
several experiments in some instances: Influence Maximization and Sensor
Placement for the problem. The results confirm that our algorithms ensure
theoretical quality as the cutting-edge techniques and significantly reduce the
number of queries.",2309.12025v1,https://arxiv.org/pdf/2309.12025v1
"Learning Noise-Robust Joint Representation for Multimodal Emotion
  Recognition under Incomplete Data Scenarios","Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian, Guanglai Gao","Multimodal emotion recognition (MER) in practical scenarios is significantly
challenged by the presence of missing or incomplete data across different
modalities. To overcome these challenges, researchers have aimed to simulate
incomplete conditions during the training phase to enhance the system's overall
robustness. Traditional methods have often involved discarding data or
substituting data segments with zero vectors to approximate these
incompletenesses. However, such approaches neither accurately represent
real-world conditions nor adequately address the issue of noisy data
availability. For instance, a blurry image cannot be simply replaced with zero
vectors, and still retain information. To tackle this issue and develop a more
precise MER system, we introduce a novel noise-robust MER model that
effectively learns robust multimodal joint representations from noisy data.
This approach includes two pivotal components: firstly, a noise scheduler that
adjusts the type and level of noise in the data to emulate various realistic
incomplete situations. Secondly, a Variational AutoEncoder (VAE)-based module
is employed to reconstruct these robust multimodal joint representations from
the noisy inputs. Notably, the introduction of the noise scheduler enables the
exploration of an entirely new type of incomplete data condition, which is
impossible with existing methods. Extensive experimental evaluations on the
benchmark datasets IEMOCAP and CMU-MOSEI demonstrate the effectiveness of the
noise scheduler and the excellent performance of our proposed model.",2311.16114v2,https://arxiv.org/pdf/2311.16114v2
How Robust is Google's Bard to Adversarial Image Attacks?,"Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu","Multimodal Large Language Models (MLLMs) that integrate text and other
modalities (especially vision) have achieved unprecedented performance in
various multimodal tasks. However, due to the unsolved adversarial robustness
problem of vision models, MLLMs can have more severe safety and security risks
by introducing the vision inputs. In this work, we study the adversarial
robustness of Google's Bard, a competitive chatbot to ChatGPT that released its
multimodal capability recently, to better understand the vulnerabilities of
commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs,
the generated adversarial examples can mislead Bard to output wrong image
descriptions with a 22% success rate based solely on the transferability. We
show that the adversarial examples can also attack other MLLMs, e.g., a 26%
attack success rate against Bing Chat and a 86% attack success rate against
ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face
detection and toxicity detection of images. We design corresponding attacks to
evade these defenses, demonstrating that the current defenses of Bard are also
vulnerable. We hope this work can deepen our understanding on the robustness of
MLLMs and facilitate future research on defenses. Our code is available at
https://github.com/thu-ml/Attack-Bard.
  Update: GPT-4V is available at October 2023. We further evaluate its
robustness under the same set of adversarial examples, achieving a 45% attack
success rate.",2309.11751v2,https://arxiv.org/pdf/2309.11751v2
"Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk
  Minimization Framework","Sina Baharlouei, Meisam Razaviyayn","While training fair machine learning models has been studied extensively in
recent years, most developed methods rely on the assumption that the training
and test data have similar distributions. In the presence of distribution
shifts, fair models may behave unfairly on test data. There have been some
developments for fair learning robust to distribution shifts to address this
shortcoming. However, most proposed solutions are based on the assumption of
having access to the causal graph describing the interaction of different
features. Moreover, existing algorithms require full access to data and cannot
be used when small batches are used (stochastic/batch implementation). This
paper proposes the first stochastic distributionally robust fairness framework
with convergence guarantees that do not require knowledge of the causal graph.
More specifically, we formulate the fair inference in the presence of the
distribution shift as a distributionally robust optimization problem under
$L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual
Information (ERMI) as the measure of fairness violation. We then discuss how
the proposed method can be implemented in a stochastic fashion. We have
evaluated the presented framework's performance and efficiency through
extensive experiments on real datasets consisting of distribution shifts.",2309.11682v1,https://arxiv.org/pdf/2309.11682v1
RHALE: Robust and Heterogeneity-aware Accumulated Local Effects,"Vasilis Gkolemis, Theodore Dalamagas, Eirini Ntoutsi, Christos Diou","Accumulated Local Effects (ALE) is a widely-used explainability method for
isolating the average effect of a feature on the output, because it handles
cases with correlated features well. However, it has two limitations. First, it
does not quantify the deviation of instance-level (local) effects from the
average (global) effect, known as heterogeneity. Second, for estimating the
average effect, it partitions the feature domain into user-defined, fixed-sized
bins, where different bin sizes may lead to inconsistent ALE estimations. To
address these limitations, we propose Robust and Heterogeneity-aware ALE
(RHALE). RHALE quantifies the heterogeneity by considering the standard
deviation of the local effects and automatically determines an optimal
variable-size bin-splitting. In this paper, we prove that to achieve an
unbiased approximation of the standard deviation of local effects within each
bin, bin splitting must follow a set of sufficient conditions. Based on these
conditions, we propose an algorithm that automatically determines the optimal
partitioning, balancing the estimation bias and variance. Through evaluations
on synthetic and real datasets, we demonstrate the superiority of RHALE
compared to other methods, including the advantages of automatic bin splitting,
especially in cases with correlated features.",2309.11193v1,https://arxiv.org/pdf/2309.11193v1
"Compilation as a Defense: Enhancing DL Model Attack Robustness via
  Tensor Optimization","Stefan Trawicki, William Hackett, Lewis Birch, Neeraj Suri, Peter Garraghan","Adversarial Machine Learning (AML) is a rapidly growing field of security
research, with an often overlooked area being model attacks through
side-channels. Previous works show such attacks to be serious threats, though
little progress has been made on efficient remediation strategies that avoid
costly model re-engineering. This work demonstrates a new defense against AML
side-channel attacks using model compilation techniques, namely tensor
optimization. We show relative model attack effectiveness decreases of up to
43% using tensor optimization, discuss the implications, and direction of
future work.",2309.16577v1,https://arxiv.org/pdf/2309.16577v1
Are Large Language Models Really Robust to Word-Level Perturbations?,"Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao","The swift advancement in the scales and capabilities of Large Language Models
(LLMs) positions them as promising tools for a variety of downstream tasks. In
addition to the pursuit of better performance and the avoidance of violent
feedback on a certain prompt, to ensure the responsibility of the LLM, much
attention is drawn to the robustness of LLMs. However, existing evaluation
methods mostly rely on traditional question answering datasets with predefined
supervised labels, which do not align with the superior generation capabilities
of contemporary LLMs. To address this issue, we propose a novel rational
evaluation approach that leverages pre-trained reward models as diagnostic
tools to evaluate the longer conversation generated from more challenging open
questions by LLMs, which we refer to as the Reward Model for Reasonable
Robustness Evaluation (TREvaL). Longer conversations manifest the comprehensive
grasp of language models in terms of their proficiency in understanding
questions, a capability not entirely encompassed by individual words or
letters, which may exhibit oversimplification and inherent biases. Our
extensive empirical experiments demonstrate that TREvaL provides an innovative
method for evaluating the robustness of an LLM. Furthermore, our results
demonstrate that LLMs frequently exhibit vulnerability to word-level
perturbations that are commonplace in daily language usage. Notably, we are
surprised to discover that robustness tends to decrease as fine-tuning (SFT and
RLHF) is conducted. The code of TREval is available in
https://github.com/Harry-mic/TREvaL.",2309.11166v2,https://arxiv.org/pdf/2309.11166v2
It's Simplex! Disaggregating Measures to Improve Certified Robustness,"Andrew C. Cullen, Paul Montague, Shijie Liu, Sarah M. Erfani, Benjamin I. P. Rubinstein","Certified robustness circumvents the fragility of defences against
adversarial attacks, by endowing model predictions with guarantees of class
invariance for attacks up to a calculated size. While there is value in these
certifications, the techniques through which we assess their performance do not
present a proper accounting of their strengths and weaknesses, as their
analysis has eschewed consideration of performance over individual samples in
favour of aggregated measures. By considering the potential output space of
certified models, this work presents two distinct approaches to improve the
analysis of certification mechanisms, that allow for both dataset-independent
and dataset-dependent measures of certification performance. Embracing such a
perspective uncovers new certification approaches, which have the potential to
more than double the achievable radius of certification, relative to current
state-of-the-art. Empirical evaluation verifies that our new approach can
certify $9\%$ more samples at noise scale $\sigma = 1$, with greater relative
improvements observed as the difficulty of the predictive task increases.",2309.11005v1,https://arxiv.org/pdf/2309.11005v1
"Extreme Image Transformations Facilitate Robust Latent Object
  Representations","Girik Malik, Dakarai Crowder, Ennio Mingolla","Adversarial attacks can affect the object recognition capabilities of
machines in wild. These can often result from spurious correlations between
input and class labels, and are prone to memorization in large networks. While
networks are expected to do automated feature selection, it is not effective at
the scale of the object. Humans, however, are able to select the minimum set of
features required to form a robust representation of an object. In this work,
we show that finetuning any pretrained off-the-shelf network with Extreme Image
Transformations (EIT) not only helps in learning a robust latent
representation, it also improves the performance of these networks against
common adversarial attacks of various intensities. Our EIT trained networks
show strong activations in the object regions even when tested with more
intense noise, showing promising generalizations across different kinds of
adversarial attacks.",2310.07725v1,https://arxiv.org/pdf/2310.07725v1
"Crypto'Graph: Leveraging Privacy-Preserving Distributed Link Prediction
  for Robust Graph Learning","Sofiane Azogagh, Zelma Aubin Birba, Sébastien Gambs, Marc-Olivier Killijian","Graphs are a widely used data structure for collecting and analyzing
relational data. However, when the graph structure is distributed across
several parties, its analysis is particularly challenging. In particular, due
to the sensitivity of the data each party might want to keep their partial
knowledge of the graph private, while still willing to collaborate with the
other parties for tasks of mutual benefit, such as data curation or the removal
of poisoned data. To address this challenge, we propose Crypto'Graph, an
efficient protocol for privacy-preserving link prediction on distributed
graphs. More precisely, it allows parties partially sharing a graph with
distributed links to infer the likelihood of formation of new links in the
future. Through the use of cryptographic primitives, Crypto'Graph is able to
compute the likelihood of these new links on the joint network without
revealing the structure of the private individual graph of each party, even
though they know the number of nodes they have, since they share the same graph
but not the same links. Crypto'Graph improves on previous works by enabling the
computation of a certain number of similarity metrics without any additional
cost. The use of Crypto'Graph is illustrated for defense against graph
poisoning attacks, in which it is possible to identify potential adversarial
links without compromising the privacy of the graphs of individual parties. The
effectiveness of Crypto'Graph in mitigating graph poisoning attacks and
achieving high prediction accuracy on a graph neural network node
classification task is demonstrated through extensive experimentation on a
real-world dataset.",2309.10890v1,https://arxiv.org/pdf/2309.10890v1
An Extendable Python Implementation of Robust Optimisation Monte Carlo,"Vasilis Gkolemis, Michael Gutmann, Henri Pesonen","Performing inference in statistical models with an intractable likelihood is
challenging, therefore, most likelihood-free inference (LFI) methods encounter
accuracy and efficiency limitations. In this paper, we present the
implementation of the LFI method Robust Optimisation Monte Carlo (ROMC) in the
Python package ELFI. ROMC is a novel and efficient (highly-parallelizable) LFI
framework that provides accurate weighted samples from the posterior. Our
implementation can be used in two ways. First, a scientist may use it as an
out-of-the-box LFI algorithm; we provide an easy-to-use API harmonized with the
principles of ELFI, enabling effortless comparisons with the rest of the
methods included in the package. Additionally, we have carefully split ROMC
into isolated components for supporting extensibility. A researcher may
experiment with novel method(s) for solving part(s) of ROMC without
reimplementing everything from scratch. In both scenarios, the ROMC parts can
run in a fully-parallelized manner, exploiting all CPU cores. We also provide
helpful functionalities for (i) inspecting the inference process and (ii)
evaluating the obtained samples. Finally, we test the robustness of our
implementation on some typical LFI examples.",2309.10612v1,https://arxiv.org/pdf/2309.10612v1
Improving CLIP Robustness with Knowledge Distillation and Self-Training,"Clement Laroudie, Andrei Bursuc, Mai Lan Ha, Gianni Franchi","This paper examines the robustness of a multi-modal computer vision model,
CLIP (Contrastive Language-Image Pretraining), in the context of unsupervised
learning. The main objective is twofold: first, to evaluate the robustness of
CLIP, and second, to explore strategies for augmenting its robustness. To
achieve this, we introduce a novel approach named LP-CLIP. This technique
involves the distillation of CLIP features through the incorporation of a
linear probing layer positioned atop its encoding structure. This newly added
layer is trained utilizing pseudo-labels produced by CLIP, coupled with a
self-training strategy. The LP-CLIP technique offers a promising approach to
enhance the robustness of CLIP without the need for annotations. By leveraging
a simple linear probing layer, we aim to improve the model's ability to
withstand various uncertainties and challenges commonly encountered in
real-world scenarios. Importantly, our approach does not rely on annotated
data, which makes it particularly valuable in situations where labeled data
might be scarce or costly to obtain. Our proposed approach increases the
robustness of CLIP with SOTA results compared to supervised technique on
various datasets.",2309.10361v1,https://arxiv.org/pdf/2309.10361v1
"Analysis of the Memorization and Generalization Capabilities of AI
  Agents: Are Continual Learners Robust?","Minsu Kim, Walid Saad","In continual learning (CL), an AI agent (e.g., autonomous vehicles or
robotics) learns from non-stationary data streams under dynamic environments.
For the practical deployment of such applications, it is important to guarantee
robustness to unseen environments while maintaining past experiences. In this
paper, a novel CL framework is proposed to achieve robust generalization to
dynamic environments while retaining past knowledge. The considered CL agent
uses a capacity-limited memory to save previously observed environmental
information to mitigate forgetting issues. Then, data points are sampled from
the memory to estimate the distribution of risks over environmental change so
as to obtain predictors that are robust with unseen changes. The generalization
and memorization performance of the proposed framework are theoretically
analyzed. This analysis showcases the tradeoff between memorization and
generalization with the memory size. Experiments show that the proposed
algorithm outperforms memory-based CL baselines across all environments while
significantly improving the generalization performance on unseen target
environments.",2309.10149v2,https://arxiv.org/pdf/2309.10149v2
Evaluating Adversarial Robustness with Expected Viable Performance,"Ryan McCoppin, Colin Dawson, Sean M. Kennedy, Leslie M. Blaha","We introduce a metric for evaluating the robustness of a classifier, with
particular attention to adversarial perturbations, in terms of expected
functionality with respect to possible adversarial perturbations. A classifier
is assumed to be non-functional (that is, has a functionality of zero) with
respect to a perturbation bound if a conventional measure of performance, such
as classification accuracy, is less than a minimally viable threshold when the
classifier is tested on examples from that perturbation bound. Defining
robustness in terms of an expected value is motivated by a domain general
approach to robustness quantification.",2309.09928v1,https://arxiv.org/pdf/2309.09928v1
"An Iterative Method for Unsupervised Robust Anomaly Detection Under Data
  Contamination","Minkyung Kim, Jongmin Yu, Junsik Kim, Tae-Hyun Oh, Jun Kyun Choi","Most deep anomaly detection models are based on learning normality from
datasets due to the difficulty of defining abnormality by its diverse and
inconsistent nature. Therefore, it has been a common practice to learn
normality under the assumption that anomalous data are absent in a training
dataset, which we call normality assumption. However, in practice, the
normality assumption is often violated due to the nature of real data
distributions that includes anomalous tails, i.e., a contaminated dataset.
Thereby, the gap between the assumption and actual training data affects
detrimentally in learning of an anomaly detection model. In this work, we
propose a learning framework to reduce this gap and achieve better normality
representation. Our key idea is to identify sample-wise normality and utilize
it as an importance weight, which is updated iteratively during the training.
Our framework is designed to be model-agnostic and hyperparameter insensitive
so that it applies to a wide range of existing methods without careful
parameter tuning. We apply our framework to three different representative
approaches of deep anomaly detection that are classified into one-class
classification-, probabilistic model-, and reconstruction-based approaches. In
addition, we address the importance of a termination condition for iterative
methods and propose a termination criterion inspired by the anomaly detection
objective. We validate that our framework improves the robustness of the
anomaly detection models under different levels of contamination ratios on five
anomaly detection benchmark datasets and two image datasets. On various
contaminated datasets, our framework improves the performance of three
representative anomaly detection methods, measured by area under the ROC curve.",2309.09436v1,https://arxiv.org/pdf/2309.09436v1
Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM,"Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen","Recently, Large Language Models (LLMs) have made significant advancements and
are now widely used across various domains. Unfortunately, there has been a
rising concern that LLMs can be misused to generate harmful or malicious
content. Though a line of research has focused on aligning LLMs with human
values and preventing them from producing inappropriate content, such
alignments are usually vulnerable and can be bypassed by alignment-breaking
attacks via adversarially optimized or handcrafted jailbreaking prompts. In
this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against
potential alignment-breaking attacks. RA-LLM can be directly constructed upon
an existing aligned LLM with a robust alignment checking function, without
requiring any expensive retraining or fine-tuning process of the original LLM.
Furthermore, we also provide a theoretical analysis for RA-LLM to verify its
effectiveness in defending against alignment-breaking attacks. Through
real-world experiments on open-source large language models, we demonstrate
that RA-LLM can successfully defend against both state-of-the-art adversarial
prompts and popular handcrafted jailbreaking prompts by reducing their attack
success rates from nearly 100% to around 10% or less.",2309.14348v3,https://arxiv.org/pdf/2309.14348v3
"Robust Online Covariance and Sparse Precision Estimation Under Arbitrary
  Data Corruption","Tong Yao, Shreyas Sundaram","Gaussian graphical models are widely used to represent correlations among
entities but remain vulnerable to data corruption. In this work, we introduce a
modified trimmed-inner-product algorithm to robustly estimate the covariance in
an online scenario even in the presence of arbitrary and adversarial data
attacks. At each time step, data points, drawn nominally independently and
identically from a multivariate Gaussian distribution, arrive. However, a
certain fraction of these points may have been arbitrarily corrupted. We
propose an online algorithm to estimate the sparse inverse covariance (i.e.,
precision) matrix despite this corruption. We provide the error-bound and
convergence properties of the estimates to the true precision matrix under our
algorithms.",2309.08884v1,https://arxiv.org/pdf/2309.08884v1
Distributionally Robust Post-hoc Classifiers under Prior Shifts,"Jiaheng Wei, Harikrishna Narasimhan, Ehsan Amid, Wen-Sheng Chu, Yang Liu, Abhishek Kumar","The generalization ability of machine learning models degrades significantly
when the test distribution shifts away from the training distribution. We
investigate the problem of training models that are robust to shifts caused by
changes in the distribution of class-priors or group-priors. The presence of
skewed training priors can often lead to the models overfitting to spurious
features. Unlike existing methods, which optimize for either the worst or the
average performance over classes or groups, our work is motivated by the need
for finer control over the robustness properties of the model. We present an
extremely lightweight post-hoc approach that performs scaling adjustments to
predictions from a pre-trained model, with the goal of minimizing a
distributionally robust loss around a chosen target distribution. These
adjustments are computed by solving a constrained optimization problem on a
validation set and applied to the model during test time. Our constrained
optimization objective is inspired by a natural notion of robustness to
controlled distribution shifts. Our method comes with provable guarantees and
empirically makes a strong case for distributional robust post-hoc classifiers.
An empirical implementation is available at
https://github.com/weijiaheng/Drops.",2309.08825v1,https://arxiv.org/pdf/2309.08825v1
"Wasserstein Distributionally Robust Policy Evaluation and Learning for
  Contextual Bandits","Yi Shen, Pan Xu, Michael M. Zavlanos","Off-policy evaluation and learning are concerned with assessing a given
policy and learning an optimal policy from offline data without direct
interaction with the environment. Often, the environment in which the data are
collected differs from the environment in which the learned policy is applied.
To account for the effect of different environments during learning and
execution, distributionally robust optimization (DRO) methods have been
developed that compute worst-case bounds on the policy values assuming that the
distribution of the new environment lies within an uncertainty set. Typically,
this uncertainty set is defined based on the KL divergence around the empirical
distribution computed from the logging dataset. However, the KL uncertainty set
fails to encompass distributions with varying support and lacks awareness of
the geometry of the distribution support. As a result, KL approaches fall short
in addressing practical environment mismatches and lead to over-fitting to
worst-case scenarios. To overcome these limitations, we propose a novel DRO
approach that employs the Wasserstein distance instead. While Wasserstein DRO
is generally computationally more expensive compared to KL DRO, we present a
regularized method and a practical (biased) stochastic gradient descent method
to optimize the policy efficiently. We also provide a theoretical analysis of
the finite sample complexity and iteration complexity for our proposed method.
We further validate our approach using a public dataset that was recorded in a
randomized stoke trial.",2309.08748v3,https://arxiv.org/pdf/2309.08748v3
"Wasserstein Distributionally Robust Control Barrier Function using
  Conditional Value-at-Risk with Differentiable Convex Programming","Alaa Eddine Chriat, Chuangchuang Sun","Control Barrier functions (CBFs) have attracted extensive attention for
designing safe controllers for their deployment in real-world safety-critical
systems. However, the perception of the surrounding environment is often
subject to stochasticity and further distributional shift from the nominal one.
In this paper, we present distributional robust CBF (DR-CBF) to achieve
resilience under distributional shift while keeping the advantages of CBF, such
as computational efficacy and forward invariance.
  To achieve this goal, we first propose a single-level convex reformulation to
estimate the conditional value at risk (CVaR) of the safety constraints under
distributional shift measured by a Wasserstein metric, which is by nature
tri-level programming. Moreover, to construct a control barrier condition to
enforce the forward invariance of the CVaR, the technique of differentiable
convex programming is applied to enable differentiation through the
optimization layer of CVaR estimation. We also provide an approximate variant
of DR-CBF for higher-order systems. Simulation results are presented to
validate the chance-constrained safety guarantee under the distributional shift
in both first and second-order systems.",2309.08700v1,https://arxiv.org/pdf/2309.08700v1
A Bayesian Approach to Robust Inverse Reinforcement Learning,"Ran Wei, Siliang Zeng, Chenliang Li, Alfredo Garcia, Anthony McDonald, Mingyi Hong","We consider a Bayesian approach to offline model-based inverse reinforcement
learning (IRL). The proposed framework differs from existing offline
model-based IRL approaches by performing simultaneous estimation of the
expert's reward function and subjective model of environment dynamics. We make
use of a class of prior distributions which parameterizes how accurate the
expert's model of the environment is to develop efficient algorithms to
estimate the expert's reward and subjective dynamics in high-dimensional
settings. Our analysis reveals a novel insight that the estimated policy
exhibits robust performance when the expert is believed (a priori) to have a
highly accurate model of the environment. We verify this observation in the
MuJoCo environments and show that our algorithms outperform state-of-the-art
offline IRL algorithms.",2309.08571v2,https://arxiv.org/pdf/2309.08571v2
"Towards Robust Continual Learning with Bayesian Adaptive Moment
  Regularization","Jack Foster, Alexandra Brintrup","The pursuit of long-term autonomy mandates that machine learning models must
continuously adapt to their changing environments and learn to solve new tasks.
Continual learning seeks to overcome the challenge of catastrophic forgetting,
where learning to solve new tasks causes a model to forget previously learnt
information. Prior-based continual learning methods are appealing as they are
computationally efficient and do not require auxiliary models or data storage.
However, prior-based approaches typically fail on important benchmarks and are
thus limited in their potential applications compared to their memory-based
counterparts. We introduce Bayesian adaptive moment regularization (BAdam), a
novel prior-based method that better constrains parameter growth, reducing
catastrophic forgetting. Our method boasts a range of desirable properties such
as being lightweight and task label-free, converging quickly, and offering
calibrated uncertainty that is important for safe real-world deployment.
Results show that BAdam achieves state-of-the-art performance for prior-based
methods on challenging single-headed class-incremental experiments such as
Split MNIST and Split FashionMNIST, and does so without relying on task labels
or discrete task boundaries.",2309.08546v3,https://arxiv.org/pdf/2309.08546v3
Efficient and robust Sensor Placement in Complex Environments,"Lukas Taus, Yen-Hsi Richard Tsai","We address the problem of efficient and unobstructed surveillance or
communication in complex environments. On one hand, one wishes to use a minimal
number of sensors to cover the environment. On the other hand, it is often
important to consider solutions that are robust against sensor failure or
adversarial attacks. This paper addresses these challenges of designing minimal
sensor sets that achieve multi-coverage constraints -- every point in the
environment is covered by a prescribed number of sensors. We propose a greedy
algorithm to achieve the objective. Further, we explore deep learning
techniques to accelerate the evaluation of the objective function formulated in
the greedy algorithm. The training of the neural network reveals that the
geometric properties of the data significantly impact the network's
performance, particularly at the end stage. By taking into account these
properties, we discuss the differences in using greedy and $\epsilon$-greedy
algorithms to generate data and their impact on the robustness of the network.",2309.08545v1,https://arxiv.org/pdf/2309.08545v1
"Towards Last-layer Retraining for Group Robustness with Fewer
  Annotations","Tyler LaBonte, Vidya Muthukumar, Abhishek Kumar","Empirical risk minimization (ERM) of neural networks is prone to
over-reliance on spurious correlations and poor generalization on minority
groups. The recent deep feature reweighting (DFR) technique achieves
state-of-the-art group robustness via simple last-layer retraining, but it
requires held-out group and class annotations to construct a group-balanced
reweighting dataset. In this work, we examine this impractical requirement and
find that last-layer retraining can be surprisingly effective with no group
annotations (other than for model selection) and only a handful of class
annotations. We first show that last-layer retraining can greatly improve
worst-group accuracy even when the reweighting dataset has only a small
proportion of worst-group data. This implies a ""free lunch"" where holding out a
subset of training data to retrain the last layer can substantially outperform
ERM on the entire dataset with no additional data or annotations. To further
improve group robustness, we introduce a lightweight method called selective
last-layer finetuning (SELF), which constructs the reweighting dataset using
misclassifications or disagreements. Our empirical and theoretical results
present the first evidence that model disagreement upsamples worst-group data,
enabling SELF to nearly match DFR on four well-established benchmarks across
vision and language tasks with no group annotations and less than 3% of the
held-out class annotations. Our code is available at
https://github.com/tmlabonte/last-layer-retraining.",2309.08534v3,https://arxiv.org/pdf/2309.08534v3
"Cross-lingual Knowledge Distillation via Flow-based Voice Conversion for
  Robust Polyglot Text-To-Speech","Dariusz Piotrowski, Renard Korzeniowski, Alessio Falai, Sebastian Cygert, Kamil Pokora, Georgi Tinchev, Ziyao Zhang, Kayoko Yanagisawa","In this work, we introduce a framework for cross-lingual speech synthesis,
which involves an upstream Voice Conversion (VC) model and a downstream
Text-To-Speech (TTS) model. The proposed framework consists of 4 stages. In the
first two stages, we use a VC model to convert utterances in the target locale
to the voice of the target speaker. In the third stage, the converted data is
combined with the linguistic features and durations from recordings in the
target language, which are then used to train a single-speaker acoustic model.
Finally, the last stage entails the training of a locale-independent vocoder.
Our evaluations show that the proposed paradigm outperforms state-of-the-art
approaches which are based on training a large multilingual TTS model. In
addition, our experiments demonstrate the robustness of our approach with
different model architectures, languages, speakers and amounts of data.
Moreover, our solution is especially beneficial in low-resource settings.",2309.08255v1,https://arxiv.org/pdf/2309.08255v1
"Physics-constrained robust learning of open-form partial differential
  equations from limited and noisy data","Mengge Du, Yuntian Chen, Longfeng Nie, Siyu Lou, Dongxiao Zhang","Unveiling the underlying governing equations of nonlinear dynamic systems
remains a significant challenge. Insufficient prior knowledge hinders the
determination of an accurate candidate library, while noisy observations lead
to imprecise evaluations, which in turn result in redundant function terms or
erroneous equations. This study proposes a framework to robustly uncover
open-form partial differential equations (PDEs) from limited and noisy data.
The framework operates through two alternating update processes: discovering
and embedding. The discovering phase employs symbolic representation and a
novel reinforcement learning (RL)-guided hybrid PDE generator to efficiently
produce diverse open-form PDEs with tree structures. A neural network-based
predictive model fits the system response and serves as the reward evaluator
for the generated PDEs. PDEs with higher rewards are utilized to iteratively
optimize the generator via the RL strategy and the best-performing PDE is
selected by a parameter-free stability metric. The embedding phase integrates
the initially identified PDE from the discovering process as a physical
constraint into the predictive model for robust training. The traversal of PDE
trees automates the construction of the computational graph and the embedding
process without human intervention. Numerical experiments demonstrate our
framework's capability to uncover governing equations from nonlinear dynamic
systems with limited and highly noisy data and outperform other
physics-informed neural network-based discovery methods. This work opens new
potential for exploring real-world systems with limited understanding.",2309.07672v2,https://arxiv.org/pdf/2309.07672v2
"Proximal Bellman mappings for reinforcement learning and their
  application to robust adaptive filtering","Yuki Akiyama, Konstantinos Slavakis","This paper aims at the algorithmic/theoretical core of reinforcement learning
(RL) by introducing the novel class of proximal Bellman mappings. These
mappings are defined in reproducing kernel Hilbert spaces (RKHSs), to benefit
from the rich approximation properties and inner product of RKHSs, they are
shown to belong to the powerful Hilbertian family of (firmly) nonexpansive
mappings, regardless of the values of their discount factors, and possess ample
degrees of design freedom to even reproduce attributes of the classical Bellman
mappings and to pave the way for novel RL designs. An approximate
policy-iteration scheme is built on the proposed class of mappings to solve the
problem of selecting online, at every time instance, the ""optimal"" exponent $p$
in a $p$-norm loss to combat outliers in linear adaptive filtering, without
training data and any knowledge on the statistical properties of the outliers.
Numerical tests on synthetic data showcase the superior performance of the
proposed framework over several non-RL and kernel-based RL schemes.",2309.07548v1,https://arxiv.org/pdf/2309.07548v1
"Beta quantile regression for robust estimation of uncertainty in the
  presence of outliers","Haleh Akrami, Omar Zamzam, Anand Joshi, Sergul Aydore, Richard Leahy","Quantile Regression (QR) can be used to estimate aleatoric uncertainty in
deep neural networks and can generate prediction intervals. Quantifying
uncertainty is particularly important in critical applications such as clinical
diagnosis, where a realistic assessment of uncertainty is essential in
determining disease status and planning the appropriate treatment. The most
common application of quantile regression models is in cases where the
parametric likelihood cannot be specified. Although quantile regression is
quite robust to outlier response observations, it can be sensitive to outlier
covariate observations (features). Outlier features can compromise the
performance of deep learning regression problems such as style translation,
image reconstruction, and deep anomaly detection, potentially leading to
misleading conclusions. To address this problem, we propose a robust solution
for quantile regression that incorporates concepts from robust divergence. We
compare the performance of our proposed method with (i) least trimmed quantile
regression and (ii) robust regression based on the regularization of
case-specific parameters in a simple real dataset in the presence of outlier.
These methods have not been applied in a deep learning framework. We also
demonstrate the applicability of the proposed method by applying it to a
medical imaging translation task using diffusion models.",2309.07374v1,https://arxiv.org/pdf/2309.07374v1
"Getting More for Less: Using Weak Labels and AV-Mixup for Robust
  Audio-Visual Speaker Verification","Anith Selvakumar, Homa Fashandi","Distance Metric Learning (DML) has typically dominated the audio-visual
speaker verification problem space, owing to strong performance in new and
unseen classes. In our work, we explored multitask learning techniques to
further enhance DML, and show that an auxiliary task with even weak labels can
increase the quality of the learned speaker representation without increasing
model complexity during inference. We also extend the Generalized End-to-End
Loss (GE2E) to multimodal inputs and demonstrate that it can achieve
competitive performance in an audio-visual space. Finally, we introduce
AV-Mixup, a multimodal augmentation technique during training time that has
shown to reduce speaker overfit. Our network achieves state of the art
performance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal
Error Rate (EER) on the VoxCeleb1-O/E/H test sets, which is to our knowledge,
the best published results on VoxCeleb1-E and VoxCeleb1-H.",2309.07115v2,https://arxiv.org/pdf/2309.07115v2
"The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in
  Deep Learning","Alexander Bastounis, Alexander N. Gorban, Anders C. Hansen, Desmond J. Higham, Danil Prokhorov, Oliver Sutton, Ivan Y. Tyukin, Qinghua Zhou","In this work, we assess the theoretical limitations of determining guaranteed
stability and accuracy of neural networks in classification tasks. We consider
classical distribution-agnostic framework and algorithms minimising empirical
risks and potentially subjected to some weights regularisation. We show that
there is a large family of tasks for which computing and verifying ideal stable
and accurate neural networks in the above settings is extremely challenging, if
at all possible, even when such ideal solutions exist within the given class of
neural architectures.",2309.07072v1,https://arxiv.org/pdf/2309.07072v1
"Sensitivity, Performance, Robustness: Deconstructing the Effect of
  Sociodemographic Prompting","Tilman Beck, Hendrik Schuff, Anne Lauscher, Iryna Gurevych","Annotators' sociodemographic backgrounds (i.e., the individual compositions
of their gender, age, educational background, etc.) have a strong impact on
their decisions when working on subjective NLP tasks, such as toxic language
detection. Often, heterogeneous backgrounds result in high disagreements. To
model this variation, recent work has explored sociodemographic prompting, a
technique, which steers the output of prompt-based models towards answers that
humans with specific sociodemographic profiles would give. However, the
available NLP literature disagrees on the efficacy of this technique - it
remains unclear for which tasks and scenarios it can help, and the role of the
individual factors in sociodemographic prompting is still unexplored. We
address this research gap by presenting the largest and most comprehensive
study of sociodemographic prompting today. We analyze its influence on model
sensitivity, performance and robustness across seven datasets and six
instruction-tuned model families. We show that sociodemographic information
affects model predictions and can be beneficial for improving zero-shot
learning in subjective NLP tasks. However, its outcomes largely vary for
different model types, sizes, and datasets, and are subject to large variance
with regards to prompt formulations. Most importantly, our results show that
sociodemographic prompting should be used with care for sensitive applications,
such as toxicity annotation or when studying LLM alignment. Code and data:
https://github.com/UKPLab/arxiv2023-sociodemographic-prompting",2309.07034v2,https://arxiv.org/pdf/2309.07034v2
"A Robust SINDy Approach by Combining Neural Networks and an Integral
  Form","Ali Forootani, Pawan Goyal, Peter Benner","The discovery of governing equations from data has been an active field of
research for decades. One widely used methodology for this purpose is sparse
regression for nonlinear dynamics, known as SINDy. Despite several attempts,
noisy and scarce data still pose a severe challenge to the success of the SINDy
approach. In this work, we discuss a robust method to discover nonlinear
governing equations from noisy and scarce data. To do this, we make use of
neural networks to learn an implicit representation based on measurement data
so that not only it produces the output in the vicinity of the measurements but
also the time-evolution of output can be described by a dynamical system.
Additionally, we learn such a dynamic system in the spirit of the SINDy
framework. Leveraging the implicit representation using neural networks, we
obtain the derivative information -- required for SINDy -- using an automatic
differentiation tool. To enhance the robustness of our methodology, we further
incorporate an integral condition on the output of the implicit networks.
Furthermore, we extend our methodology to handle data collected from multiple
initial conditions. We demonstrate the efficiency of the proposed methodology
to discover governing equations under noisy and scarce data regimes by means of
several examples and compare its performance with existing methods.",2309.07193v1,https://arxiv.org/pdf/2309.07193v1
Safe Reinforcement Learning with Dual Robustness,"Zeyang Li, Chuxiong Hu, Yunan Wang, Yujie Yang, Shengbo Eben Li","Reinforcement learning (RL) agents are vulnerable to adversarial
disturbances, which can deteriorate task performance or compromise safety
specifications. Existing methods either address safety requirements under the
assumption of no adversary (e.g., safe RL) or only focus on robustness against
performance adversaries (e.g., robust RL). Learning one policy that is both
safe and robust remains a challenging open problem. The difficulty is how to
tackle two intertwined aspects in the worst cases: feasibility and optimality.
Optimality is only valid inside a feasible region, while identification of
maximal feasible region must rely on learning the optimal policy. To address
this issue, we propose a systematic framework to unify safe RL and robust RL,
including problem formulation, iteration scheme, convergence analysis and
practical algorithm design. This unification is built upon constrained
two-player zero-sum Markov games. A dual policy iteration scheme is proposed,
which simultaneously optimizes a task policy and a safety policy. The
convergence of this iteration scheme is proved. Furthermore, we design a deep
RL algorithm for practical implementation, called dually robust actor-critic
(DRAC). The evaluations with safety-critical benchmarks demonstrate that DRAC
achieves high performance and persistent safety under all scenarios (no
adversary, safety adversary, performance adversary), outperforming all
baselines significantly.",2309.06835v1,https://arxiv.org/pdf/2309.06835v1
"Robust experimental data assimilation for the Spalart-Allmaras
  turbulence model","Deepinder Jot Singh Aulakh, Xiang Yang, Romit Maulik","This study presents a methodology focusing on the use of computational model
and experimental data fusion to improve the Spalart-Allmaras (SA) closure model
for Reynolds-averaged Navier-Stokes solutions. In particular, our goal is to
develop a technique that not only assimilates sparse experimental data to
improve turbulence model performance, but also preserves generalization for
unseen cases by recovering classical SA behavior. We achieve our goals using
data assimilation, namely the Ensemble Kalman filtering approach (EnKF), to
calibrate the coefficients of the SA model for separated flows. A holistic
calibration strategy is implemented via the parameterization of the production,
diffusion, and destruction terms. This calibration relies on the assimilation
of experimental data collected in the form of velocity profiles, skin friction,
and pressure coefficients. Despite using observational data from a single flow
condition around a backward-facing step (BFS), the recalibrated SA model
demonstrates generalization to other separated flows, including cases such as
the 2D NASA wall mounted hump (2D-WMH) and modified BFS. Significant
improvement is observed in the quantities of interest, i.e., skin friction
coefficient ($C_f$) and pressure coefficient ($C_p$) for each flow tested.
Finally, it is also demonstrated that the newly proposed model recovers SA
proficiency for flows, such as a NACA-0012 airfoil and axisymmetric jet (ASJ),
and that the individually calibrated terms in the SA model target specific
flow-physics wherein the calibrated production term improves the re-circulation
zone while destruction improves the recovery zone.",2309.06679v3,https://arxiv.org/pdf/2309.06679v3
Distributionally Robust Transfer Learning,"Xin Xiong, Zijian Guo, Tianxi Cai","Many existing transfer learning methods rely on leveraging information from
source data that closely resembles the target data. However, this approach
often overlooks valuable knowledge that may be present in different yet
potentially related auxiliary samples. When dealing with a limited amount of
target data and a diverse range of source models, our paper introduces a novel
approach, Distributionally Robust Optimization for Transfer Learning
(TransDRO), that breaks free from strict similarity constraints. TransDRO is
designed to optimize the most adversarial loss within an uncertainty set,
defined as a collection of target populations generated as a convex combination
of source distributions that guarantee excellent prediction performances for
the target data. TransDRO effectively bridges the realms of transfer learning
and distributional robustness prediction models. We establish the
identifiability of TransDRO and its interpretation as a weighted average of
source models closest to the baseline model. We also show that TransDRO
achieves a faster convergence rate than the model fitted with the target data.
Our comprehensive numerical studies and analysis of multi-institutional
electronic health records data using TransDRO further substantiate the
robustness and accuracy of TransDRO, highlighting its potential as a powerful
tool in transfer learning applications.",2309.06534v1,https://arxiv.org/pdf/2309.06534v1
Certified Robust Models with Slack Control and Large Lipschitz Constants,"Max Losch, David Stutz, Bernt Schiele, Mario Fritz","Despite recent success, state-of-the-art learning-based models remain highly
vulnerable to input changes such as adversarial examples. In order to obtain
certifiable robustness against such perturbations, recent work considers
Lipschitz-based regularizers or constraints while at the same time increasing
prediction margin. Unfortunately, this comes at the cost of significantly
decreased accuracy. In this paper, we propose a Calibrated Lipschitz-Margin
Loss (CLL) that addresses this issue and improves certified robustness by
tackling two problems: Firstly, commonly used margin losses do not adjust the
penalties to the shrinking output distribution; caused by minimizing the
Lipschitz constant $K$. Secondly, and most importantly, we observe that
minimization of $K$ can lead to overly smooth decision functions. This limits
the model's complexity and thus reduces accuracy. Our CLL addresses these
issues by explicitly calibrating the loss w.r.t. margin and Lipschitz constant,
thereby establishing full control over slack and improving robustness
certificates even with larger Lipschitz constants. On CIFAR-10, CIFAR-100 and
Tiny-ImageNet, our models consistently outperform losses that leave the
constant unattended. On CIFAR-100 and Tiny-ImageNet, CLL improves upon
state-of-the-art deterministic $L_2$ robust accuracies. In contrast to current
trends, we unlock potential of much smaller models without $K=1$ constraints.",2309.06166v1,https://arxiv.org/pdf/2309.06166v1
"Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for
  Remaining Useful Life Prediction and Operational Condition Identification of
  Rotating Machines","Khoa Tran, Hai-Canh Vu, Lam Pham, Nassim Boudaoud","In this paper, a Robust Multi-branch Deep learning-based system for remaining
useful life (RUL) prediction and condition operations (CO) identification of
rotating machines is proposed. In particular, the proposed system comprises
main components: (1) an LSTM-Autoencoder to denoise the vibration data; (2) a
feature extraction to generate time-domain, frequency-domain, and
time-frequency based features from the denoised data; (3) a novel and robust
multi-branch deep learning network architecture to exploit the multiple
features. The performance of our proposed system was evaluated and compared to
the state-of-the-art systems on two benchmark datasets of XJTU-SY and
PRONOSTIA. The experimental results prove that our proposed system outperforms
the state-of-the-art systems and presents potential for real-life applications
on bearing machines.",2309.06157v2,https://arxiv.org/pdf/2309.06157v2
"A robust synthetic data generation framework for machine learning in
  High-Resolution Transmission Electron Microscopy (HRTEM)","Luis Rangel DaCosta, Katherine Sytwu, Catherine Groschner, Mary Scott","Machine learning techniques are attractive options for developing
highly-accurate automated analysis tools for nanomaterials characterization,
including high-resolution transmission electron microscopy (HRTEM). However,
successfully implementing such machine learning tools can be difficult due to
the challenges in procuring sufficiently large, high-quality training datasets
from experiments. In this work, we introduce Construction Zone, a Python
package for rapidly generating complex nanoscale atomic structures, and develop
an end-to-end workflow for creating large simulated databases for training
neural networks. Construction Zone enables fast, systematic sampling of
realistic nanomaterial structures, and can be used as a random structure
generator for simulated databases, which is important for generating large,
diverse synthetic datasets. Using HRTEM imaging as an example, we train a
series of neural networks on various subsets of our simulated databases to
segment nanoparticles and holistically study the data curation process to
understand how various aspects of the curated simulated data -- including
simulation fidelity, the distribution of atomic structures, and the
distribution of imaging conditions -- affect model performance across several
experimental benchmarks. Using our results, we are able to achieve
state-of-the-art segmentation performance on experimental HRTEM images of
nanoparticles from several experimental benchmarks and, further, we discuss
robust strategies for consistently achieving high performance with machine
learning in experimental settings using purely synthetic data.",2309.06122v1,https://arxiv.org/pdf/2309.06122v1
Errors are Robustly Tamed in Cumulative Knowledge Processes,"Anna Brandenberger, Cassandra Marcussen, Elchanan Mossel, Madhu Sudan","We study processes of societal knowledge accumulation, where the validity of
a new unit of knowledge depends both on the correctness of its derivation and
on the validity of the units it depends on. A fundamental question in this
setting is: If a constant fraction of the new derivations is wrong, can
investing a constant fraction, bounded away from one, of effort ensure that a
constant fraction of knowledge in society is valid? Ben-Eliezer, Mikulincer,
Mossel, and Sudan (ITCS 2023) introduced a concrete probabilistic model to
analyze such questions and showed an affirmative answer to this question. Their
study, however, focuses on the simple case where each new unit depends on just
one existing unit, and units attach according to a $\textit{preferential
attachment rule}$.
  In this work, we consider much more general families of cumulative knowledge
processes, where new units may attach according to varied attachment mechanisms
and depend on multiple existing units. We also allow a (random) fraction of
insertions of adversarial nodes.
  We give a robust affirmative answer to the above question by showing that for
$\textit{all}$ of these models, as long as many of the units follow simple
heuristics for checking a bounded number of units they depend on, all errors
will be eventually eliminated. Our results indicate that preserving the quality
of large interdependent collections of units of knowledge is feasible, as long
as careful but not too costly checks are performed when new units are
derived/deposited.",2309.05638v3,https://arxiv.org/pdf/2309.05638v3
SABLE: Secure And Byzantine robust LEarning,"Antoine Choffrut, Rachid Guerraoui, Rafael Pinot, Renaud Sirdey, John Stephan, Martin Zuber","Due to the widespread availability of data, machine learning (ML) algorithms
are increasingly being implemented in distributed topologies, wherein various
nodes collaborate to train ML models via the coordination of a central server.
However, distributed learning approaches face significant vulnerabilities,
primarily stemming from two potential threats. Firstly, the presence of
Byzantine nodes poses a risk of corrupting the learning process by transmitting
inaccurate information to the server. Secondly, a curious server may compromise
the privacy of individual nodes, sometimes reconstructing the entirety of the
nodes' data. Homomorphic encryption (HE) has emerged as a leading security
measure to preserve privacy in distributed learning under non-Byzantine
scenarios. However, the extensive computational demands of HE, particularly for
high-dimensional ML models, have deterred attempts to design purely homomorphic
operators for non-linear robust aggregators. This paper introduces SABLE, the
first homomorphic and Byzantine robust distributed learning algorithm. SABLE
leverages HTS, a novel and efficient homomorphic operator implementing the
prominent coordinate-wise trimmed mean robust aggregator. Designing HTS enables
us to implement HMED, a novel homomorphic median aggregator. Extensive
experiments on standard ML tasks demonstrate that SABLE achieves practical
execution times while maintaining an ML accuracy comparable to its non-private
counterpart.",2309.05395v4,https://arxiv.org/pdf/2309.05395v4
Outlier Robust Adversarial Training,"Shu Hu, Zhenhuan Yang, Xin Wang, Yiming Ying, Siwei Lyu","Supervised learning models are challenged by the intrinsic complexities of
training data such as outliers and minority subpopulations and intentional
attacks at inference time with adversarial samples. While traditional robust
learning methods and the recent adversarial training approaches are designed to
handle each of the two challenges, to date, no work has been done to develop
models that are robust with regard to the low-quality training data and the
potential adversarial attack at inference time simultaneously. It is for this
reason that we introduce Outlier Robust Adversarial Training (ORAT) in this
work. ORAT is based on a bi-level optimization formulation of adversarial
training with a robust rank-based loss function. Theoretically, we show that
the learning objective of ORAT satisfies the $\mathcal{H}$-consistency in
binary classification, which establishes it as a proper surrogate to
adversarial 0/1 loss. Furthermore, we analyze its generalization ability and
provide uniform convergence rates in high probability. ORAT can be optimized
with a simple algorithm. Experimental evaluations on three benchmark datasets
demonstrate the effectiveness and robustness of ORAT in handling outliers and
adversarial attacks. Our code is available at
https://github.com/discovershu/ORAT.",2309.05145v1,https://arxiv.org/pdf/2309.05145v1
Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler,"Zhijun Chen, Hailong Sun, Wanhao Zhang, Chunyi Xu, Qianren Mao, Pengpeng Chen","We propose a neuralized undirected graphical model called Neural-Hidden-CRF
to solve the weakly-supervised sequence labeling problem. Under the umbrella of
probabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded
with a hidden CRF layer models the variables of word sequence, latent ground
truth sequence, and weak label sequence with the global perspective that
undirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can
capitalize on the powerful language model BERT or other deep models to provide
rich contextual semantic knowledge to the latent ground truth sequence, and use
the hidden CRF layer to capture the internal label dependencies.
Neural-Hidden-CRF is conceptually simple and empirically powerful. It obtains
new state-of-the-art results on one crowdsourcing benchmark and three
weak-supervision benchmarks, including outperforming the recent advanced model
CHMM by 2.80 F1 points and 2.23 F1 points in average generalization and
inference performance, respectively.",2309.05086v2,https://arxiv.org/pdf/2309.05086v2
Towards Robust Model Watermark via Reducing Parametric Vulnerability,"Guanhao Gan, Yiming Li, Dongxian Wu, Shu-Tao Xia","Deep neural networks are valuable assets considering their commercial
benefits and huge demands for costly annotation and computation resources. To
protect the copyright of DNNs, backdoor-based ownership verification becomes
popular recently, in which the model owner can watermark the model by embedding
a specific backdoor behavior before releasing it. The defenders (usually the
model owners) can identify whether a suspicious third-party model is ``stolen''
from them based on the presence of the behavior. Unfortunately, these
watermarks are proven to be vulnerable to removal attacks even like
fine-tuning. To further explore this vulnerability, we investigate the
parameter space and find there exist many watermark-removed models in the
vicinity of the watermarked one, which may be easily used by removal attacks.
Inspired by this finding, we propose a mini-max formulation to find these
watermark-removed models and recover their watermark behavior. Extensive
experiments demonstrate that our method improves the robustness of the model
watermarking against parametric changes and numerous watermark-removal attacks.
The codes for reproducing our main experiments are available at
\url{https://github.com/GuanhaoGan/robust-model-watermarking}.",2309.04777v1,https://arxiv.org/pdf/2309.04777v1
"Flexible and Robust Counterfactual Explanations with Minimal Satisfiable
  Perturbations","Yongjie Wang, Hangwei Qian, Yongjie Liu, Wei Guo, Chunyan Miao","Counterfactual explanations (CFEs) exemplify how to minimally modify a
feature vector to achieve a different prediction for an instance. CFEs can
enhance informational fairness and trustworthiness, and provide suggestions for
users who receive adverse predictions. However, recent research has shown that
multiple CFEs can be offered for the same instance or instances with slight
differences. Multiple CFEs provide flexible choices and cover diverse
desiderata for user selection. However, individual fairness and model
reliability will be damaged if unstable CFEs with different costs are returned.
Existing methods fail to exploit flexibility and address the concerns of
non-robustness simultaneously. To address these issues, we propose a
conceptually simple yet effective solution named Counterfactual Explanations
with Minimal Satisfiable Perturbations (CEMSP). Specifically, CEMSP constrains
changing values of abnormal features with the help of their semantically
meaningful normal ranges. For efficiency, we model the problem as a Boolean
satisfiability problem to modify as few features as possible. Additionally,
CEMSP is a general framework and can easily accommodate more practical
requirements, e.g., casualty and actionability. Compared to existing methods,
we conduct comprehensive experiments on both synthetic and real-world datasets
to demonstrate that our method provides more robust explanations while
preserving flexibility.",2309.04676v1,https://arxiv.org/pdf/2309.04676v1
"Robust Representation Learning for Privacy-Preserving Machine Learning:
  A Multi-Objective Autoencoder Approach","Sofiane Ouaari, Ali Burak Ünal, Mete Akgün, Nico Pfeifer","Several domains increasingly rely on machine learning in their applications.
The resulting heavy dependence on data has led to the emergence of various laws
and regulations around data ethics and privacy and growing awareness of the
need for privacy-preserving machine learning (ppML). Current ppML techniques
utilize methods that are either purely based on cryptography, such as
homomorphic encryption, or that introduce noise into the input, such as
differential privacy. The main criticism given to those techniques is the fact
that they either are too slow or they trade off a model s performance for
improved confidentiality. To address this performance reduction, we aim to
leverage robust representation learning as a way of encoding our data while
optimizing the privacy-utility trade-off. Our method centers on training
autoencoders in a multi-objective manner and then concatenating the latent and
learned features from the encoding part as the encoded form of our data. Such a
deep learning-powered encoding can then safely be sent to a third party for
intensive training and hyperparameter tuning. With our proposed framework, we
can share our data and use third party tools without being under the threat of
revealing its original form. We empirically validate our results on unimodal
and multimodal settings, the latter following a vertical splitting system and
show improved performance over state-of-the-art.",2309.04427v1,https://arxiv.org/pdf/2309.04427v1
Zero-Shot Robustification of Zero-Shot Models,"Dyah Adila, Changho Shin, Linrong Cai, Frederic Sala","Zero-shot inference is a powerful paradigm that enables the use of large
pretrained models for downstream classification tasks without further training.
However, these models are vulnerable to inherited biases that can impact their
performance. The traditional solution is fine-tuning, but this undermines the
key advantage of pretrained models, which is their ability to be used
out-of-the-box. We propose RoboShot, a method that improves the robustness of
pretrained model embeddings in a fully zero-shot fashion. First, we use
language models (LMs) to obtain useful insights from task descriptions. These
insights are embedded and used to remove harmful and boost useful components in
embeddings -- without any supervision. Theoretically, we provide a simple and
tractable model for biases in zero-shot embeddings and give a result
characterizing under what conditions our approach can boost performance.
Empirically, we evaluate RoboShot on nine image and NLP classification tasks
and show an average improvement of 15.98% on worst group accuracy, with trivial
decrease in overall accuracy over several zero-shot baselines. Additionally, we
demonstrate that RoboShot is compatible with a variety of pretrained and
language models and propose a way to further boost performance with a zero-shot
adaptation variant.",2309.04344v2,https://arxiv.org/pdf/2309.04344v2
"REALM: Robust Entropy Adaptive Loss Minimization for Improved
  Single-Sample Test-Time Adaptation","Skyler Seto, Barry-John Theobald, Federico Danieli, Navdeep Jaitly, Dan Busbridge","Fully-test-time adaptation (F-TTA) can mitigate performance loss due to
distribution shifts between train and test data (1) without access to the
training data, and (2) without knowledge of the model training procedure. In
online F-TTA, a pre-trained model is adapted using a stream of test samples by
minimizing a self-supervised objective, such as entropy minimization. However,
models adapted with online using entropy minimization, are unstable especially
in single sample settings, leading to degenerate solutions, and limiting the
adoption of TTA inference strategies. Prior works identify noisy, or
unreliable, samples as a cause of failure in online F-TTA. One solution is to
ignore these samples, which can lead to bias in the update procedure, slow
adaptation, and poor generalization. In this work, we present a general
framework for improving robustness of F-TTA to these noisy samples, inspired by
self-paced learning and robust loss functions. Our proposed approach, Robust
Entropy Adaptive Loss Minimization (REALM), achieves better adaptation accuracy
than previous approaches throughout the adaptation process on corruptions of
CIFAR-10 and ImageNet-1K, demonstrating its effectiveness.",2309.03964v1,https://arxiv.org/pdf/2309.03964v1
"Adversarially Robust Deep Learning with Optimal-Transport-Regularized
  Divergences","Jeremiah Birrell, Mohammadreza Ebrahimi","We introduce the $ARMOR_D$ methods as novel approaches to enhancing the
adversarial robustness of deep learning models. These methods are based on a
new class of optimal-transport-regularized divergences, constructed via an
infimal convolution between an information divergence and an optimal-transport
(OT) cost. We use these as tools to enhance adversarial robustness by
maximizing the expected loss over a neighborhood of distributions, a technique
known as distributionally robust optimization. Viewed as a tool for
constructing adversarial samples, our method allows samples to be both
transported, according to the OT cost, and re-weighted, according to the
information divergence. We demonstrate the effectiveness of our method on
malware detection and image recognition applications and find that, to our
knowledge, it outperforms existing methods at enhancing the robustness against
adversarial attacks. $ARMOR_D$ yields the robustified accuracy of $98.29\%$
against $FGSM$ and $98.18\%$ against $PGD^{40}$ on the MNIST dataset, reducing
the error rate by more than $19.7\%$ and $37.2\%$ respectively compared to
prior methods. Similarly, in malware detection, a discrete (binary) data
domain, $ARMOR_D$ improves the robustified accuracy under $rFGSM^{50}$ attack
compared to the previous best-performing adversarial training methods by
$37.0\%$ while lowering false negative and false positive rates by $51.1\%$ and
$57.53\%$, respectively.",2309.03791v1,https://arxiv.org/pdf/2309.03791v1
Understanding Data Augmentation from a Robustness Perspective,"Zhendong Liu, Jie Zhang, Qiangqiang He, Chongjun Wang","In the realm of visual recognition, data augmentation stands out as a pivotal
technique to amplify model robustness. Yet, a considerable number of existing
methodologies lean heavily on heuristic foundations, rendering their intrinsic
mechanisms ambiguous. This manuscript takes both a theoretical and empirical
approach to understanding the phenomenon. Theoretically, we frame the discourse
around data augmentation within game theory's constructs. Venturing deeper, our
empirical evaluations dissect the intricate mechanisms of emblematic data
augmentation strategies, illuminating that these techniques primarily stimulate
mid- and high-order game interactions. Beyond the foundational exploration, our
experiments span multiple datasets and diverse augmentation techniques,
underscoring the universal applicability of our findings. Recognizing the vast
array of robustness metrics with intricate correlations, we unveil a
streamlined proxy. This proxy not only simplifies robustness assessment but
also offers invaluable insights, shedding light on the inherent dynamics of
model game interactions and their relation to overarching system robustness.
These insights provide a novel lens through which we can re-evaluate model
safety and robustness in visual recognition tasks.",2311.12800v1,https://arxiv.org/pdf/2311.12800v1
"A Robust Negative Learning Approach to Partial Domain Adaptation Using
  Source Prototypes","Sandipan Choudhuri, Suli Adeniye, Arunabha Sen","This work proposes a robust Partial Domain Adaptation (PDA) framework that
mitigates the negative transfer problem by incorporating a robust
target-supervision strategy. It leverages ensemble learning and includes
diverse, complementary label feedback, alleviating the effect of incorrect
feedback and promoting pseudo-label refinement. Rather than relying exclusively
on first-order moments for distribution alignment, our approach offers explicit
objectives to optimize intra-class compactness and inter-class separation with
the inferred source prototypes and highly-confident target samples in a
domain-invariant fashion. Notably, we ensure source data privacy by eliminating
the need to access the source data during the adaptation phase through a priori
inference of source prototypes. We conducted a series of comprehensive
experiments, including an ablation analysis, covering a range of partial domain
adaptation tasks. Comprehensive evaluations on benchmark datasets corroborate
our framework's enhanced robustness and generalization, demonstrating its
superiority over existing state-of-the-art PDA approaches.",2309.03531v2,https://arxiv.org/pdf/2309.03531v2
"Towards Robust Natural-Looking Mammography Lesion Synthesis on
  Ipsilateral Dual-Views Breast Cancer Analysis","Thanh-Huy Nguyen, Quang Hien Kha, Thai Ngoc Toan Truong, Ba Thinh Lam, Ba Hung Ngo, Quang Vinh Dinh, Nguyen Quoc Khanh Le","In recent years, many mammographic image analysis methods have been
introduced for improving cancer classification tasks. Two major issues of
mammogram classification tasks are leveraging multi-view mammographic
information and class-imbalance handling. In the first problem, many multi-view
methods have been released for concatenating features of two or more views for
the training and inference stage. Having said that, most multi-view existing
methods are not explainable in the meaning of feature fusion, and treat many
views equally for diagnosing. Our work aims to propose a simple but novel
method for enhancing examined view (main view) by leveraging low-level feature
information from the auxiliary view (ipsilateral view) before learning the
high-level feature that contains the cancerous features. For the second issue,
we also propose a simple but novel malignant mammogram synthesis framework for
upsampling minor class samples. Our easy-to-implement and no-training framework
has eliminated the current limitation of the CutMix algorithm which is
unreliable synthesized images with random pasted patches, hard-contour
problems, and domain shift problems. Our results on VinDr-Mammo and CMMD
datasets show the effectiveness of our two new frameworks for both multi-view
training and synthesizing mammographic images, outperforming the previous
conventional methods in our experimental settings.",2309.03506v1,https://arxiv.org/pdf/2309.03506v1
"Byzantine-Robust Federated Learning with Variance Reduction and
  Differential Privacy","Zikai Zhang, Rui Hu","Federated learning (FL) is designed to preserve data privacy during model
training, where the data remains on the client side (i.e., IoT devices), and
only model updates of clients are shared iteratively for collaborative
learning. However, this process is vulnerable to privacy attacks and Byzantine
attacks: the local model updates shared throughout the FL network will leak
private information about the local training data, and they can also be
maliciously crafted by Byzantine attackers to disturb the learning. In this
paper, we propose a new FL scheme that guarantees rigorous privacy and
simultaneously enhances system robustness against Byzantine attacks. Our
approach introduces sparsification- and momentum-driven variance reduction into
the client-level differential privacy (DP) mechanism, to defend against
Byzantine attackers. The security design does not violate the privacy guarantee
of the client-level DP mechanism; hence, our approach achieves the same
client-level DP guarantee as the state-of-the-art. We conduct extensive
experiments on both IID and non-IID datasets and different tasks and evaluate
the performance of our approach against different Byzantine attacks by
comparing it with state-of-the-art defense methods. The results of our
experiments show the efficacy of our framework and demonstrate its ability to
improve system robustness against Byzantine attacks while achieving a strong
privacy guarantee.",2309.03437v1,https://arxiv.org/pdf/2309.03437v1
"ViewMix: Augmentation for Robust Representation in Self-Supervised
  Learning","Arjon Das, Xin Zhong","Joint Embedding Architecture-based self-supervised learning methods have
attributed the composition of data augmentations as a crucial factor for their
strong representation learning capabilities. While regional dropout strategies
have proven to guide models to focus on lesser indicative parts of the objects
in supervised methods, it hasn't been adopted by self-supervised methods for
generating positive pairs. This is because the regional dropout methods are not
suitable for the input sampling process of the self-supervised methodology.
Whereas dropping informative pixels from the positive pairs can result in
inefficient training, replacing patches of a specific object with a different
one can steer the model from maximizing the agreement between different
positive pairs. Moreover, joint embedding representation learning methods have
not made robustness their primary training outcome. To this end, we propose the
ViewMix augmentation policy, specially designed for self-supervised learning,
upon generating different views of the same image, patches are cut and pasted
from one view to another. By leveraging the different views created by this
augmentation strategy, multiple joint embedding-based self-supervised
methodologies obtained better localization capability and consistently
outperformed their corresponding baseline methods. It is also demonstrated that
incorporating ViewMix augmentation policy promotes robustness of the
representations in the state-of-the-art methods. Furthermore, our
experimentation and analysis of compute times suggest that ViewMix augmentation
doesn't introduce any additional overhead compared to other counterparts.",2309.03360v1,https://arxiv.org/pdf/2309.03360v1
"J-Guard: Journalism Guided Adversarially Robust Detection of
  AI-generated News","Tharindu Kumarage, Amrita Bhattacharjee, Djordje Padejski, Kristy Roschke, Dan Gillmor, Scott Ruston, Huan Liu, Joshua Garland","The rapid proliferation of AI-generated text online is profoundly reshaping
the information landscape. Among various types of AI-generated text,
AI-generated news presents a significant threat as it can be a prominent source
of misinformation online. While several recent efforts have focused on
detecting AI-generated text in general, these methods require enhanced
reliability, given concerns about their vulnerability to simple adversarial
attacks. Furthermore, due to the eccentricities of news writing, applying these
detection methods for AI-generated news can produce false positives,
potentially damaging the reputation of news organizations. To address these
challenges, we leverage the expertise of an interdisciplinary team to develop a
framework, J-Guard, capable of steering existing supervised AI text detectors
for detecting AI-generated news while boosting adversarial robustness. By
incorporating stylistic cues inspired by the unique journalistic attributes,
J-Guard effectively distinguishes between real-world journalism and
AI-generated news articles. Our experiments on news articles generated by a
vast array of AI models, including ChatGPT (GPT3.5), demonstrate the
effectiveness of J-Guard in enhancing detection capabilities while maintaining
an average performance decrease of as low as 7% when faced with adversarial
attacks.",2309.03164v1,https://arxiv.org/pdf/2309.03164v1
"A Theoretical Explanation of Activation Sparsity through Flat Minima and
  Adversarial Robustness","Ze Peng, Lei Qi, Yinghuan Shi, Yang Gao","A recent empirical observation (Li et al., 2022b) of activation sparsity in
MLP blocks offers an opportunity to drastically reduce computation costs for
free. Although having attributed it to training dynamics, existing theoretical
explanations of activation sparsity are restricted to shallow networks, small
training steps and special training, despite its emergence in deep models
standardly trained for a large number of steps. To fill these gaps, we propose
the notion of gradient sparsity as one source of activation sparsity and a
theoretical explanation based on it that sees sparsity a necessary step to
adversarial robustness w.r.t. hidden features and parameters, which is
approximately the flatness of minima for well-learned models. The theory
applies to standardly trained LayerNorm-ed MLPs, and further to Transformers or
other architectures trained with weight noises. Eliminating other sources of
flatness except for sparsity, we discover the phenomenon that the ratio between
the largest and smallest non-zero singular values of weight matrices is small.
When discussing the emergence of this spectral concentration, we use random
matrix theory (RMT) as a powerful tool to analyze stochastic gradient noises.
Validational experiments are conducted to verify our gradient-sparsity-based
explanation. We propose two plug-and-play modules for both training and
finetuning for sparsity. Experiments on ImageNet-1k and C4 demonstrate their
50% sparsity improvements, indicating further potential cost reduction in both
training and inference.",2309.03004v4,https://arxiv.org/pdf/2309.03004v4
"Natural and Robust Walking using Reinforcement Learning without
  Demonstrations in High-Dimensional Musculoskeletal Models","Pierre Schumacher, Thomas Geijtenbeek, Vittorio Caggiano, Vikash Kumar, Syn Schmitt, Georg Martius, Daniel F. B. Haeufle","Humans excel at robust bipedal walking in complex natural environments. In
each step, they adequately tune the interaction of biomechanical muscle
dynamics and neuronal signals to be robust against uncertainties in ground
conditions. However, it is still not fully understood how the nervous system
resolves the musculoskeletal redundancy to solve the multi-objective control
problem considering stability, robustness, and energy efficiency. In computer
simulations, energy minimization has been shown to be a successful optimization
target, reproducing natural walking with trajectory optimization or
reflex-based control methods. However, these methods focus on particular
motions at a time and the resulting controllers are limited when compensating
for perturbations. In robotics, reinforcement learning~(RL) methods recently
achieved highly stable (and efficient) locomotion on quadruped systems, but the
generation of human-like walking with bipedal biomechanical models has required
extensive use of expert data sets. This strong reliance on demonstrations often
results in brittle policies and limits the application to new behaviors,
especially considering the potential variety of movements for high-dimensional
musculoskeletal models in 3D. Achieving natural locomotion with RL without
sacrificing its incredible robustness might pave the way for a novel approach
to studying human walking in complex natural environments. Videos:
https://sites.google.com/view/naturalwalkingrl",2309.02976v2,https://arxiv.org/pdf/2309.02976v2
M3D-NCA: Robust 3D Segmentation with Built-in Quality Control,"John Kalkhof, Anirban Mukhopadhyay","Medical image segmentation relies heavily on large-scale deep learning
models, such as UNet-based architectures. However, the real-world utility of
such models is limited by their high computational requirements, which makes
them impractical for resource-constrained environments such as primary care
facilities and conflict zones. Furthermore, shifts in the imaging domain can
render these models ineffective and even compromise patient safety if such
errors go undetected. To address these challenges, we propose M3D-NCA, a novel
methodology that leverages Neural Cellular Automata (NCA) segmentation for 3D
medical images using n-level patchification. Moreover, we exploit the variance
in M3D-NCA to develop a novel quality metric which can automatically detect
errors in the segmentation process of NCAs. M3D-NCA outperforms the two
magnitudes larger UNet models in hippocampus and prostate segmentation by 2%
Dice and can be run on a Raspberry Pi 4 Model B (2GB RAM). This highlights the
potential of M3D-NCA as an effective and efficient alternative for medical
image segmentation in resource-constrained environments.",2309.02954v1,https://arxiv.org/pdf/2309.02954v1
Improved Outlier Robust Seeding for k-means,"Amit Deshpande, Rameshwar Pratap","The $k$-means is a popular clustering objective, although it is inherently
non-robust and sensitive to outliers. Its popular seeding or initialization
called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\log k)$
approximation guarantee \cite{AV2007}. However, in the presence of adversarial
noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant
outliers instead of inlier clusters, and therefore its approximation guarantees
\textit{w.r.t.} $k$-means solution on inliers, does not hold.
  Assuming that the outliers constitute a constant fraction of the given data,
we propose a simple variant in the $D^2$ sampling distribution, which makes it
robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$
clusters, discards marginally more points than the optimal number of outliers,
and comes with a provable $O(1)$ approximation guarantee.
  Our algorithm can also be modified to output exactly $k$ clusters instead of
$O(k)$ clusters, while keeping its running time linear in $n$ and $d$. This is
an improvement over previous results for robust $k$-means based on LP
relaxation and rounding \cite{Charikar}, \cite{KrishnaswamyLS18} and
\textit{robust $k$-means++} \cite{DeshpandeKP20}. Our empirical results show
the advantage of our algorithm over $k$-means++~\cite{AV2007}, uniform random
seeding, greedy sampling for $k$ means~\cite{tkmeanspp}, and robust
$k$-means++~\cite{DeshpandeKP20}, on standard real-world and synthetic data
sets used in previous work. Our proposal is easily amenable to scalable,
faster, parallel implementations of $k$-means++ \cite{Bahmani,BachemL017} and
is of independent interest for coreset constructions in the presence of
outliers \cite{feldman2007ptas,langberg2010universal,feldman2011unified}.",2309.02710v1,https://arxiv.org/pdf/2309.02710v1
A Lightweight and Transferable Design for Robust LEGO Manipulation,"Ruixuan Liu, Yifan Sun, Changliu Liu","Lego is a well-known platform for prototyping pixelized objects. However,
robotic Lego prototyping (i.e., manipulating Lego bricks) is challenging due to
the tight connections and accuracy requirements. This paper investigates safe
and efficient robotic Lego manipulation. In particular, this paper reduces the
complexity of the manipulation by hardware-software co-design. An end-of-arm
tool (EOAT) is designed, which reduces the problem dimension and allows large
industrial robots to manipulate small Lego bricks. In addition, this paper uses
evolution strategy to optimize the robot motion for Lego manipulation.
Experiments demonstrate that the EOAT can reliably manipulate Lego bricks and
the learning framework can effectively and safely improve the manipulation
performance to a 100% success rate. The co-design is deployed to multiple
robots (i.e., FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its
generalizability and transferability. In the end, we show that the proposed
solution enables sustainable robotic Lego prototyping, in which the robot can
repeatedly assemble and disassemble different prototypes.",2309.02354v3,https://arxiv.org/pdf/2309.02354v3
"RoBoSS: A Robust, Bounded, Sparse, and Smooth Loss Function for
  Supervised Learning","Mushir Akhtar, M. Tanveer, Mohd. Arshad","In the domain of machine learning algorithms, the significance of the loss
function is paramount, especially in supervised learning tasks. It serves as a
fundamental pillar that profoundly influences the behavior and efficacy of
supervised learning algorithms. Traditional loss functions, while widely used,
often struggle to handle noisy and high-dimensional data, impede model
interpretability, and lead to slow convergence during training. In this paper,
we address the aforementioned constraints by proposing a novel robust, bounded,
sparse, and smooth (RoBoSS) loss function for supervised learning. Further, we
incorporate the RoBoSS loss function within the framework of support vector
machine (SVM) and introduce a new robust algorithm named
$\mathcal{L}_{rbss}$-SVM. For the theoretical analysis, the
classification-calibrated property and generalization ability are also
presented. These investigations are crucial for gaining deeper insights into
the performance of the RoBoSS loss function in the classification tasks and its
potential to generalize well to unseen data. To empirically demonstrate the
effectiveness of the proposed $\mathcal{L}_{rbss}$-SVM, we evaluate it on $88$
real-world UCI and KEEL datasets from diverse domains. Additionally, to
exemplify the effectiveness of the proposed $\mathcal{L}_{rbss}$-SVM within the
biomedical realm, we evaluated it on two medical datasets: the
electroencephalogram (EEG) signal dataset and the breast cancer (BreaKHis)
dataset. The numerical results substantiate the superiority of the proposed
$\mathcal{L}_{rbss}$-SVM model, both in terms of its remarkable generalization
performance and its efficiency in training time.",2309.02250v1,https://arxiv.org/pdf/2309.02250v1
RobustEdge: Low Power Adversarial Detection for Cloud-Edge Systems,"Abhishek Moitra, Abhiroop Bhattacharjee, Youngeun Kim, Priyadarshini Panda","In practical cloud-edge scenarios, where a resource constrained edge performs
data acquisition and a cloud system (having sufficient resources) performs
inference tasks with a deep neural network (DNN), adversarial robustness is
critical for reliability and ubiquitous deployment. Adversarial detection is a
prime adversarial defence technique used in prior literature. However, in prior
detection works, the detector is attached to the classifier model and both
detector and classifier work in tandem to perform adversarial detection that
requires a high computational overhead which is not available at the low-power
edge. Therefore, prior works can only perform adversarial detection at the
cloud and not at the edge. This means that in case of adversarial attacks, the
unfavourable adversarial samples must be communicated to the cloud which leads
to energy wastage at the edge device. Therefore, a low-power edge-friendly
adversarial detection method is required to improve the energy efficiency of
the edge and robustness of the cloud-based classifier. To this end, RobustEdge
proposes Quantization-enabled Energy Separation (QES) training with ""early
detection and exit"" to perform edge-based low cost adversarial detection. The
QES-trained detector implemented at the edge blocks adversarial data
transmission to the classifier model, thereby improving adversarial robustness
and energy-efficiency of the Cloud-Edge system.",2310.06845v1,https://arxiv.org/pdf/2310.06845v1
"Distributionally Robust Model-based Reinforcement Learning with Large
  State Spaces","Shyam Sundhar Ramesh, Pier Giuseppe Sessa, Yifan Hu, Andreas Krause, Ilija Bogunovic","Three major challenges in reinforcement learning are the complex dynamical
systems with large state spaces, the costly data acquisition processes, and the
deviation of real-world dynamics from the training environment deployment. To
overcome these issues, we study distributionally robust Markov decision
processes with continuous state spaces under the widely used Kullback-Leibler,
chi-square, and total variation uncertainty sets. We propose a model-based
approach that utilizes Gaussian Processes and the maximum variance reduction
algorithm to efficiently learn multi-output nominal transition dynamics,
leveraging access to a generative model (i.e., simulator). We further
demonstrate the statistical sample complexity of the proposed method for
different uncertainty sets. These complexity bounds are independent of the
number of states and extend beyond linear dynamics, ensuring the effectiveness
of our approach in identifying near-optimal distributionally-robust policies.
The proposed method can be further combined with other model-free
distributionally robust reinforcement learning methods to obtain a near-optimal
robust policy. Experimental results demonstrate the robustness of our algorithm
to distributional shifts and its superior performance in terms of the number of
samples needed.",2309.02236v1,https://arxiv.org/pdf/2309.02236v1
Distributionally Robust Machine Learning with Multi-source Data,"Zhenyu Wang, Peter Bühlmann, Zijian Guo","Classical machine learning methods may lead to poor prediction performance
when the target distribution differs from the source populations. This paper
utilizes data from multiple sources and introduces a group distributionally
robust prediction model defined to optimize an adversarial reward about
explained variance with respect to a class of target distributions. Compared to
classical empirical risk minimization, the proposed robust prediction model
improves the prediction accuracy for target populations with distribution
shifts. We show that our group distributionally robust prediction model is a
weighted average of the source populations' conditional outcome models. We
leverage this key identification result to robustify arbitrary machine learning
algorithms, including, for example, random forests and neural networks. We
devise a novel bias-corrected estimator to estimate the optimal aggregation
weight for general machine-learning algorithms and demonstrate its improvement
in the convergence rate. Our proposal can be seen as a distributionally robust
federated learning approach that is computationally efficient and easy to
implement using arbitrary machine learning base algorithms, satisfies some
privacy constraints, and has a nice interpretation of different sources'
importance for predicting a given target covariate distribution. We demonstrate
the performance of our proposed group distributionally robust method on
simulated and real data with random forests and neural networks as
base-learning algorithms.",2309.02211v2,https://arxiv.org/pdf/2309.02211v2
On the Robustness of Post-hoc GNN Explainers to Label Noise,"Zhiqiang Zhong, Yangqianzi Jiang, Davide Mottin","Proposed as a solution to the inherent black-box limitations of graph neural
networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful
explanations of the behaviours exhibited by trained GNNs. Despite their recent
notable advancements in academic and industrial contexts, the robustness of
post-hoc GNN explainers remains unexplored when confronted with label noise. To
bridge this gap, we conduct a systematic empirical investigation to evaluate
the efficacy of diverse post-hoc GNN explainers under varying degrees of label
noise. Our results reveal several key insights: Firstly, post-hoc GNN
explainers are susceptible to label perturbations. Secondly, even minor levels
of label noise, inconsequential to GNN performance, harm the quality of
generated explanations substantially. Lastly, we engage in a discourse
regarding the progressive recovery of explanation effectiveness with escalating
noise levels.",2309.01706v2,https://arxiv.org/pdf/2309.01706v2
Robust Online Classification: From Estimation to Denoising,"Changlong Wu, Ananth Grama, Wojciech Szpankowski","We study online classification in the presence of noisy labels. The noise
mechanism is modeled by a general kernel that specifies, for any feature-label
pair, a (known) set of distributions over noisy labels. At each time step, an
adversary selects an unknown distribution from the distribution set specified
by the kernel based on the actual feature-label pair, and generates the noisy
label from the selected distribution. The learner then makes a prediction based
on the actual features and noisy labels observed thus far, and incurs loss $1$
if the prediction differs from the underlying truth (and $0$ otherwise). The
prediction quality is quantified through minimax risk, which computes the
cumulative loss over a finite horizon $T$. We show that for a wide range of
natural noise kernels, adversarially selected features, and finite class of
labeling functions, minimax risk can be upper bounded independent of the time
horizon and logarithmic in the size of labeling function class. We then extend
these results to inifinite classes and stochastically generated features via
the concept of stochastic sequential covering. Our results extend and encompass
findings of Ben-David et al. (2009) through substantial generality, and provide
intuitive understanding through a novel reduction to online conditional
distribution estimation.",2309.01698v1,https://arxiv.org/pdf/2309.01698v1
"Robust penalized least squares of depth trimmed residuals regression for
  high-dimensional data",Yijun Zuo,"Challenges with data in the big-data era include (i) the dimension $p$ is
often larger than the sample size $n$ (ii) outliers or contaminated points are
frequently hidden and more difficult to detect. Challenge (i) renders most
conventional methods inapplicable. Thus, it attracts tremendous attention from
statistics, computer science, and bio-medical communities. Numerous penalized
regression methods have been introduced as modern methods for analyzing
high-dimensional data. Disproportionate attention has been paid to the
challenge (ii) though. Penalized regression methods can do their job very well
and are expected to handle the challenge (ii) simultaneously. Most of them,
however, can break down by a single outlier (or single adversary contaminated
point) as revealed in this article.
  The latter systematically examines leading penalized regression methods in
the literature in terms of their robustness, provides quantitative assessment,
and reveals that most of them can break down by a single outlier. Consequently,
a novel robust penalized regression method based on the least sum of squares of
depth trimmed residuals is proposed and studied carefully. Experiments with
simulated and real data reveal that the newly proposed method can outperform
some leading competitors in estimation and prediction accuracy in the cases
considered.",2309.01666v1,https://arxiv.org/pdf/2309.01666v1
"DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion","Cédric Rommel, Eduardo Valle, Mickaël Chen, Souhaiel Khalfaoui, Renaud Marlet, Matthieu Cord, Patrick Pérez","We present an innovative approach to 3D Human Pose Estimation (3D-HPE) by
integrating cutting-edge diffusion models, which have revolutionized diverse
fields, but are relatively unexplored in 3D-HPE. We show that diffusion models
enhance the accuracy, robustness, and coherence of human pose estimations. We
introduce DiffHPE, a novel strategy for harnessing diffusion models in 3D-HPE,
and demonstrate its ability to refine standard supervised 3D-HPE. We also show
how diffusion models lead to more robust estimations in the face of occlusions,
and improve the time-coherence and the sagittal symmetry of predictions. Using
the Human\,3.6M dataset, we illustrate the effectiveness of our approach and
its superiority over existing models, even under adverse situations where the
occlusion patterns in training do not match those in inference. Our findings
indicate that while standalone diffusion models provide commendable
performance, their accuracy is even better in combination with supervised
models, opening exciting new avenues for 3D-HPE research.",2309.01575v1,https://arxiv.org/pdf/2309.01575v1
"On the Consistency and Robustness of Saliency Explanations for Time
  Series Classification","Chiara Balestra, Bin Li, Emmanuel Müller","Interpretable machine learning and explainable artificial intelligence have
become essential in many applications. The trade-off between interpretability
and model performance is the traitor to developing intrinsic and model-agnostic
interpretation methods. Although model explanation approaches have achieved
significant success in vision and natural language domains, explaining time
series remains challenging. The complex pattern in the feature domain, coupled
with the additional temporal dimension, hinders efficient interpretation.
Saliency maps have been applied to interpret time series windows as images.
However, they are not naturally designed for sequential data, thus suffering
various issues.
  This paper extensively analyzes the consistency and robustness of saliency
maps for time series features and temporal attribution. Specifically, we
examine saliency explanations from both perturbation-based and gradient-based
explanation models in a time series classification task. Our experimental
results on five real-world datasets show that they all lack consistent and
robust performances to some extent. By drawing attention to the flawed saliency
explanation models, we motivate to develop consistent and robust explanations
for time series classification.",2309.01457v1,https://arxiv.org/pdf/2309.01457v1
"Robust and Efficient Interference Neural Networks for Defending Against
  Adversarial Attacks in ImageNet","Yunuo Xiong, Shujuan Liu, Hongwei Xiong","The existence of adversarial images has seriously affected the task of image
recognition and practical application of deep learning, it is also a key
scientific problem that deep learning urgently needs to solve. By far the most
effective approach is to train the neural network with a large number of
adversarial examples. However, this adversarial training method requires a huge
amount of computing resources when applied to ImageNet, and has not yet
achieved satisfactory results for high-intensity adversarial attacks. In this
paper, we construct an interference neural network by applying additional
background images and corresponding labels, and use pre-trained ResNet-152 to
efficiently complete the training. Compared with the state-of-the-art results
under the PGD attack, it has a better defense effect with much smaller
computing resources. This work provides new ideas for academic research and
practical applications of effective defense against adversarial attacks.",2310.05947v1,https://arxiv.org/pdf/2310.05947v1
"Noise robust speech emotion recognition with signal-to-noise ratio
  adapting speech enhancement","Yu-Wen Chen, Julia Hirschberg, Yu Tsao","Speech emotion recognition (SER) often experiences reduced performance due to
background noise. In addition, making a prediction on signals with only
background noise could undermine user trust in the system. In this study, we
propose a Noise Robust Speech Emotion Recognition system, NRSER. NRSER employs
speech enhancement (SE) to effectively reduce the noise in input signals. Then,
the signal-to-noise-ratio (SNR)-level detection structure and waveform
reconstitution strategy are introduced to reduce the negative impact of SE on
speech signals with no or little background noise. Our experimental results
show that NRSER can effectively improve the noise robustness of the SER system,
including preventing the system from making emotion recognition on signals
consisting solely of background noise. Moreover, the proposed SNR-level
detection structure can be used individually for tasks such as data selection.",2309.01164v1,https://arxiv.org/pdf/2309.01164v1
Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization,"Uri Gadot, Esther Derman, Navdeep Kumar, Maxence Mohamed Elfatihi, Kfir Levy, Shie Mannor","In robust Markov decision processes (RMDPs), it is assumed that the reward
and the transition dynamics lie in a given uncertainty set. By targeting
maximal return under the most adversarial model from that set, RMDPs address
performance sensitivity to misspecified environments. Yet, to preserve
computational tractability, the uncertainty set is traditionally independently
structured for each state. This so-called rectangularity condition is solely
motivated by computational concerns. As a result, it lacks a practical
incentive and may lead to overly conservative behavior. In this work, we study
coupled reward RMDPs where the transition kernel is fixed, but the reward
function lies within an $\alpha$-radius from a nominal one. We draw a direct
connection between this type of non-rectangular reward-RMDPs and applying
policy visitation frequency regularization. We introduce a policy-gradient
method and prove its convergence. Numerical experiments illustrate the learned
policy's robustness and its less conservative behavior when compared to
rectangular uncertainty.",2309.01107v2,https://arxiv.org/pdf/2309.01107v2
Robust Adversarial Defense by Tensor Factorization,"Manish Bhattarai, Mehmet Cagri Kaymak, Ryan Barron, Ben Nebgen, Kim Rasmussen, Boian Alexandrov","As machine learning techniques become increasingly prevalent in data
analysis, the threat of adversarial attacks has surged, necessitating robust
defense mechanisms. Among these defenses, methods exploiting low-rank
approximations for input data preprocessing and neural network (NN) parameter
factorization have shown potential. Our work advances this field further by
integrating the tensorization of input data with low-rank decomposition and
tensorization of NN parameters to enhance adversarial defense. The proposed
approach demonstrates significant defense capabilities, maintaining robust
accuracy even when subjected to the strongest known auto-attacks. Evaluations
against leading-edge robust performance benchmarks reveal that our results not
only hold their ground against the best defensive methods available but also
exceed all current defense strategies that rely on tensor factorizations. This
study underscores the potential of integrating tensorization and low-rank
decomposition as a robust defense against adversarial attacks in machine
learning.",2309.01077v1,https://arxiv.org/pdf/2309.01077v1
"Generative Data Augmentation using LLMs improves Distributional
  Robustness in Question Answering","Arijit Ghosh Chowdhury, Aman Chadha","Robustness in Natural Language Processing continues to be a pertinent issue,
where state of the art models under-perform under naturally shifted
distributions. In the context of Question Answering, work on domain adaptation
methods continues to be a growing body of research. However, very little
attention has been given to the notion of domain generalization under natural
distribution shifts, where the target domain is unknown. With drastic
improvements in the quality and access to generative models, we answer the
question: How do generated datasets influence the performance of QA models
under natural distribution shifts? We perform experiments on 4 different
datasets under varying amounts of distribution shift, and analyze how
""in-the-wild"" generation can help achieve domain generalization. We take a
two-step generation approach, generating both contexts and QA pairs to augment
existing datasets. Through our experiments, we demonstrate how augmenting
reading comprehension datasets with generated data leads to better robustness
towards natural distribution shifts.",2309.06358v2,https://arxiv.org/pdf/2309.06358v2
"Mitigating Motion Blur for Robust 3D Baseball Player Pose Modeling for
  Pitch Analysis","Jerrin Bright, Yuhao Chen, John Zelek","Using videos to analyze pitchers in baseball can play a vital role in
strategizing and injury prevention. Computer vision-based pose analysis offers
a time-efficient and cost-effective approach. However, the use of accessible
broadcast videos, with a 30fps framerate, often results in partial body motion
blur during fast actions, limiting the performance of existing pose keypoint
estimation models. Previous works have primarily relied on fixed backgrounds,
assuming minimal motion differences between frames, or utilized multiview data
to address this problem. To this end, we propose a synthetic data augmentation
pipeline to enhance the model's capability to deal with the pitcher's blurry
actions. In addition, we leverage in-the-wild videos to make our model robust
under different real-world conditions and camera positions. By carefully
optimizing the augmentation parameters, we observed a notable reduction in the
loss by 54.2% and 36.2% on the test dataset for 2D and 3D pose estimation
respectively. By applying our approach to existing state-of-the-art pose
estimators, we demonstrate an average improvement of 29.2%. The findings
highlight the effectiveness of our method in mitigating the challenges posed by
motion blur, thereby enhancing the overall quality of pose estimation.",2309.01010v1,https://arxiv.org/pdf/2309.01010v1
Towards Certified Probabilistic Robustness with High Accuracy,"Ruihan Zhang, Peixin Zhang, Jun Sun","Adversarial examples pose a security threat to many critical systems built on
neural networks (such as face recognition systems, and self-driving cars).
While many methods have been proposed to build robust models, how to build
certifiably robust yet accurate neural network models remains an open problem.
For example, adversarial training improves empirical robustness, but they do
not provide certification of the model's robustness. On the other hand,
certified training provides certified robustness but at the cost of a
significant accuracy drop. In this work, we propose a novel approach that aims
to achieve both high accuracy and certified probabilistic robustness. Our
method has two parts, i.e., a probabilistic robust training method with an
additional goal of minimizing variance in terms of divergence and a runtime
inference method for certified probabilistic robustness of the prediction. The
latter enables efficient certification of the model's probabilistic robustness
at runtime with statistical guarantees. This is supported by our training
objective, which minimizes the variance of the model's predictions in a given
vicinity, derived from a general definition of model robustness. Our approach
works for a variety of perturbations and is reasonably efficient. Our
experiments on multiple models trained on different datasets demonstrate that
our approach significantly outperforms existing approaches in terms of both
certification rate and accuracy.",2309.00879v1,https://arxiv.org/pdf/2309.00879v1
Robust Online Learning over Networks,"Nicola Bastianello, Diego Deplano, Mauro Franceschelli, Karl H. Johansson","The recent deployment of multi-agent networks has enabled the distributed
solution of learning problems, where agents cooperate to train a global model
without sharing their local, private data. This work specifically targets some
prevalent challenges inherent to distributed learning: (i) online training,
i.e., the local data change over time; (ii) asynchronous agent computations;
(iii) unreliable and limited communications; and (iv) inexact local
computations. To tackle these challenges, we apply the Distributed Operator
Theoretical (DOT) version of the Alternating Direction Method of Multipliers
(ADMM), which we call ""DOT-ADMM"". We prove that if the DOT-ADMM operator is
metric subregular, then it converges with a linear rate for a large class of
(not necessarily strongly) convex learning problems toward a bounded
neighborhood of the optimal time-varying solution, and characterize how such
neighborhood depends on (i)-(iv). We first derive an easy-to-verify condition
for ensuring the metric subregularity of an operator, followed by tutorial
examples on linear and logistic regression problems. We corroborate the
theoretical analysis with numerical simulations comparing DOT-ADMM with other
state-of-the-art algorithms, showing that only the proposed algorithm exhibits
robustness to (i)-(iv).",2309.00520v2,https://arxiv.org/pdf/2309.00520v2
Robust Networked Federated Learning for Localization,"Reza Mirzaeifard, Naveen K. D. Venkategowda, Stefan Werner","This paper addresses the problem of localization, which is inherently
non-convex and non-smooth in a federated setting where the data is distributed
across a multitude of devices. Due to the decentralized nature of federated
environments, distributed learning becomes essential for scalability and
adaptability. Moreover, these environments are often plagued by outlier data,
which presents substantial challenges to conventional methods, particularly in
maintaining estimation accuracy and ensuring algorithm convergence. To mitigate
these challenges, we propose a method that adopts an $L_1$-norm robust
formulation within a distributed sub-gradient framework, explicitly designed to
handle these obstacles. Our approach addresses the problem in its original
form, without resorting to iterative simplifications or approximations,
resulting in enhanced computational efficiency and improved estimation
accuracy. We demonstrate that our method converges to a stationary point,
highlighting its effectiveness and reliability. Through numerical simulations,
we confirm the superior performance of our approach, notably in outlier-rich
environments, which surpasses existing state-of-the-art localization methods.",2308.16737v2,https://arxiv.org/pdf/2308.16737v2
Robust Representation Learning for Unreliable Partial Label Learning,"Yu Shi, Dong-Dong Wu, Xin Geng, Min-Ling Zhang","Partial Label Learning (PLL) is a type of weakly supervised learning where
each training instance is assigned a set of candidate labels, but only one
label is the ground-truth. However, this idealistic assumption may not always
hold due to potential annotation inaccuracies, meaning the ground-truth may not
be present in the candidate label set. This is known as Unreliable Partial
Label Learning (UPLL) that introduces an additional complexity due to the
inherent unreliability and ambiguity of partial labels, often resulting in a
sub-optimal performance with existing methods. To address this challenge, we
propose the Unreliability-Robust Representation Learning framework (URRL) that
leverages unreliability-robust contrastive learning to help the model fortify
against unreliable partial labels effectively. Concurrently, we propose a dual
strategy that combines KNN-based candidate label set correction and
consistency-regularization-based label disambiguation to refine label quality
and enhance the ability of representation learning within the URRL framework.
Extensive experiments demonstrate that the proposed method outperforms
state-of-the-art PLL methods on various datasets with diverse degrees of
unreliability and ambiguity. Furthermore, we provide a theoretical analysis of
our approach from the perspective of the expectation maximization (EM)
algorithm. Upon acceptance, we pledge to make the code publicly accessible.",2308.16718v1,https://arxiv.org/pdf/2308.16718v1
"Adversarial Finetuning with Latent Representation Constraint to Mitigate
  Accuracy-Robustness Tradeoff","Satoshi Suzuki, Shin'ya Yamaguchi, Shoichiro Takeda, Sekitoshi Kanai, Naoki Makishima, Atsushi Ando, Ryo Masumura","This paper addresses the tradeoff between standard accuracy on clean examples
and robustness against adversarial examples in deep neural networks (DNNs).
Although adversarial training (AT) improves robustness, it degrades the
standard accuracy, thus yielding the tradeoff. To mitigate this tradeoff, we
propose a novel AT method called ARREST, which comprises three components: (i)
adversarial finetuning (AFT), (ii) representation-guided knowledge distillation
(RGKD), and (iii) noisy replay (NR). AFT trains a DNN on adversarial examples
by initializing its parameters with a DNN that is standardly pretrained on
clean examples. RGKD and NR respectively entail a regularization term and an
algorithm to preserve latent representations of clean examples during AFT. RGKD
penalizes the distance between the representations of the standardly pretrained
and AFT DNNs. NR switches input adversarial examples to nonadversarial ones
when the representation changes significantly during AFT. By combining these
components, ARREST achieves both high standard accuracy and robustness.
Experimental results demonstrate that ARREST mitigates the tradeoff more
effectively than previous AT-based methods do.",2308.16454v1,https://arxiv.org/pdf/2308.16454v1
"Benchmarking Robustness and Generalization in Multi-Agent Systems: A
  Case Study on Neural MMO","Yangkun Chen, Joseph Suarez, Junjie Zhang, Chenghui Yu, Bo Wu, Hanmo Chen, Hengman Zhu, Rui Du, Shanliang Qian, Shuai Liu, Weijun Hong, Jinke He, Yibing Zhang, Liang Zhao, Clare Zhu, Julian Togelius, Sharada Mohanty, Jiaxin Chen, Xiu Li, Xiaolong Zhu, Phillip Isola","We present the results of the second Neural MMO challenge, hosted at IJCAI
2022, which received 1600+ submissions. This competition targets robustness and
generalization in multi-agent systems: participants train teams of agents to
complete a multi-task objective against opponents not seen during training. The
competition combines relatively complex environment design with large numbers
of agents in the environment. The top submissions demonstrate strong success on
this task using mostly standard reinforcement learning (RL) methods combined
with domain-specific engineering. We summarize the competition design and
results and suggest that, as an academic community, competitions may be a
powerful approach to solving hard problems and establishing a solid benchmark
for algorithms. We will open-source our benchmark including the environment
wrapper, baselines, a visualization tool, and selected policies for further
research.",2308.15802v1,https://arxiv.org/pdf/2308.15802v1
Gap-Free Clustering: Sensitivity and Robustness of SDP,"Matthew Zurek, Yudong Chen","We study graph clustering in the Stochastic Block Model (SBM) in the presence
of both large clusters and small, unrecoverable clusters. Previous convex
relaxation approaches achieving exact recovery do not allow any small clusters
of size $o(\sqrt{n})$, or require a size gap between the smallest recovered
cluster and the largest non-recovered cluster. We provide an algorithm based on
semidefinite programming (SDP) which removes these requirements and provably
recovers large clusters regardless of the remaining cluster sizes. Mid-sized
clusters pose unique challenges to the analysis, since their proximity to the
recovery threshold makes them highly sensitive to small noise perturbations and
precludes a closed-form candidate solution. We develop novel techniques,
including a leave-one-out-style argument which controls the correlation between
SDP solutions and noise vectors even when the removal of one row of noise can
drastically change the SDP solution. We also develop improved eigenvalue
perturbation bounds of potential independent interest. Our results are robust
to certain semirandom settings that are challenging for alternative algorithms.
Using our gap-free clustering procedure, we obtain efficient algorithms for the
problem of clustering with a faulty oracle with superior query complexities,
notably achieving $o(n^2)$ sample complexity even in the presence of a large
number of small clusters. Our gap-free clustering procedure also leads to
improved algorithms for recursive clustering.",2308.15642v2,https://arxiv.org/pdf/2308.15642v2
"Prototype Fission: Closing Set for Robust Open-set Semi-supervised
  Learning","Xuwei Tan, Yi-Jie Huang, Yaqian Li","Semi-supervised Learning (SSL) has been proven vulnerable to
out-of-distribution (OOD) samples in realistic large-scale unsupervised
datasets due to over-confident pseudo-labeling OODs as in-distribution (ID). A
key underlying problem is class-wise latent space spreading from closed seen
space to open unseen space, and the bias is further magnified in SSL's
self-training loops. To close the ID distribution set so that OODs are better
rejected for safe SSL, we propose Prototype Fission(PF) to divide class-wise
latent spaces into compact sub-spaces by automatic fine-grained latent space
mining, driven by coarse-grained labels only. Specifically, we form multiple
unique learnable sub-class prototypes for each class, optimized towards both
diversity and consistency. The Diversity Modeling term encourages samples to be
clustered by one of the multiple sub-class prototypes, while the Consistency
Modeling term clusters all samples of the same class to a global prototype.
Instead of ""opening set"", i.e., modeling OOD distribution, Prototype Fission
""closes set"" and makes it hard for OOD samples to fit in sub-class latent
space. Therefore, PF is compatible with existing methods for further
performance gains. Extensive experiments validate the effectiveness of our
method in open-set SSL settings in terms of successfully forming sub-classes,
discriminating OODs from IDs and improving overall accuracy. Codes will be
released.",2308.15575v1,https://arxiv.org/pdf/2308.15575v1
"Adversarial Style Transfer for Robust Policy Optimization in Deep
  Reinforcement Learning","Md Masudur Rahman, Yexiang Xue","This paper proposes an algorithm that aims to improve generalization for
reinforcement learning agents by removing overfitting to confounding features.
Our approach consists of a max-min game theoretic objective. A generator
transfers the style of observation during reinforcement learning. An additional
goal of the generator is to perturb the observation, which maximizes the
agent's probability of taking a different action. In contrast, a policy network
updates its parameters to minimize the effect of such perturbations, thus
staying robust while maximizing the expected future reward. Based on this
setup, we propose a practical deep reinforcement learning algorithm,
Adversarial Robust Policy Optimization (ARPO), to find a robust policy that
generalizes to unseen environments. We evaluate our approach on Procgen and
Distracting Control Suite for generalization and sample efficiency.
Empirically, ARPO shows improved performance compared to a few baseline
algorithms, including data augmentation.",2308.15550v1,https://arxiv.org/pdf/2308.15550v1
3D Adversarial Augmentations for Robust Out-of-Domain Predictions,"Alexander Lehner, Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Nassir Navab, Benjamin Busam, Federico Tombari","Since real-world training datasets cannot properly sample the long tail of
the underlying data distribution, corner cases and rare out-of-domain samples
can severely hinder the performance of state-of-the-art models. This problem
becomes even more severe for dense tasks, such as 3D semantic segmentation,
where points of non-standard objects can be confidently associated to the wrong
class. In this work, we focus on improving the generalization to out-of-domain
data. We achieve this by augmenting the training set with adversarial examples.
First, we learn a set of vectors that deform the objects in an adversarial
fashion. To prevent the adversarial examples from being too far from the
existing data distribution, we preserve their plausibility through a series of
constraints, ensuring sensor-awareness and shapes smoothness. Then, we perform
adversarial augmentation by applying the learned sample-independent vectors to
the available objects when training a model. We conduct extensive experiments
across a variety of scenarios on data from KITTI, Waymo, and CrashD for 3D
object detection, and on data from SemanticKITTI, Waymo, and nuScenes for 3D
semantic segmentation. Despite training on a standard single dataset, our
approach substantially improves the robustness and generalization of both 3D
object detection and 3D semantic segmentation methods to out-of-domain data.",2308.15479v1,https://arxiv.org/pdf/2308.15479v1
Robust Long-Tailed Learning via Label-Aware Bounded CVaR,"Hong Zhu, Runpeng Yu, Xing Tang, Yifei Wang, Yuan Fang, Yisen Wang","Data in the real-world classification problems are always imbalanced or
long-tailed, wherein the majority classes have the most of the samples that
dominate the model training. In such setting, the naive model tends to have
poor performance on the minority classes. Previously, a variety of loss
modifications have been proposed to address the long-tailed leaning problem,
while these methods either treat the samples in the same class
indiscriminatingly or lack a theoretical guarantee. In this paper, we propose
two novel approaches based on CVaR (Conditional Value at Risk) to improve the
performance of long-tailed learning with a solid theoretical ground.
Specifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss
to overcome the pessimistic result of the original CVaR, and further design the
optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we
additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to
stabilize the optimization process, where we also offer the theoretical
support. Extensive experiments on real-world datasets with long-tailed label
distributions verify the superiority of our proposed methods.",2308.15405v1,https://arxiv.org/pdf/2308.15405v1
"Enhancing Mobile Face Anti-Spoofing: A Robust Framework for Diverse
  Attack Types under Screen Flash","Weihua Liu, Chaochao Lin, Yu Yan","Face anti-spoofing (FAS) is crucial for securing face recognition systems.
However, existing FAS methods with handcrafted binary or pixel-wise labels have
limitations due to diverse presentation attacks (PAs). In this paper, we
propose an attack type robust face anti-spoofing framework under light flash,
called ATR-FAS. Due to imaging differences caused by various attack types,
traditional FAS methods based on single binary classification network may
result in excessive intra-class distance of spoof faces, leading to a challenge
of decision boundary learning. Therefore, we employed multiple networks to
reconstruct multi-frame depth maps as auxiliary supervision, and each network
experts in one type of attack. A dual gate module (DGM) consisting of a type
gate and a frame-attention gate is introduced, which perform attack type
recognition and multi-frame attention generation, respectively. The outputs of
DGM are utilized as weight to mix the result of multiple expert networks. The
multi-experts mixture enables ATR-FAS to generate spoof-differentiated depth
maps, and stably detects spoof faces without being affected by different types
of PAs. Moreover, we design a differential normalization procedure to convert
original flash frames into differential frames. This simple but effective
processing enhances the details in flash frames, aiding in the generation of
depth maps. To verify the effectiveness of our framework, we collected a
large-scale dataset containing 12,660 live and spoof videos with diverse PAs
under dynamic flash from the smartphone screen. Extensive experiments
illustrate that the proposed ATR-FAS significantly outperforms existing
state-of-the-art methods. The code and dataset will be available at
https://github.com/Chaochao-Lin/ATR-FAS.",2308.15346v1,https://arxiv.org/pdf/2308.15346v1
Advancing Adversarial Robustness Through Adversarial Logit Update,"Hao Xuan, Peican Zhu, Xingyu Li","Deep Neural Networks are susceptible to adversarial perturbations.
Adversarial training and adversarial purification are among the most widely
recognized defense strategies. Although these methods have different underlying
logic, both rely on absolute logit values to generate label predictions. In
this study, we theoretically analyze the logit difference around successful
adversarial attacks from a theoretical point of view and propose a new
principle, namely Adversarial Logit Update (ALU), to infer adversarial sample's
labels. Based on ALU, we introduce a new classification paradigm that utilizes
pre- and post-purification logit differences for model's adversarial robustness
boost. Without requiring adversarial or additional data for model training, our
clean data synthesis model can be easily applied to various pre-trained models
for both adversarial sample detection and ALU-based data classification.
Extensive experiments on both CIFAR-10, CIFAR-100, and tiny-ImageNet datasets
show that even with simple components, the proposed solution achieves superior
robustness performance compared to state-of-the-art methods against a wide
range of adversarial attacks. Our python implementation is submitted in our
Supplementary document and will be published upon the paper's acceptance.",2308.15072v1,https://arxiv.org/pdf/2308.15072v1
"Robust Open-Set Spoken Language Identification and the CU MultiLang
  Dataset","Mustafa Eyceoz, Justin Lee, Siddharth Pittie, Homayoon Beigi","Most state-of-the-art spoken language identification models are closed-set;
in other words, they can only output a language label from the set of classes
they were trained on. Open-set spoken language identification systems, however,
gain the ability to detect when an input exhibits none of the original
languages. In this paper, we implement a novel approach to open-set spoken
language identification that uses MFCC and pitch features, a TDNN model to
extract meaningful feature embeddings, confidence thresholding on softmax
outputs, and LDA and pLDA for learning to classify new unknown languages. We
present a spoken language identification system that achieves 91.76% accuracy
on trained languages and has the capability to adapt to unknown languages on
the fly. To that end, we also built the CU MultiLang Dataset, a large and
diverse multilingual speech corpus which was used to train and evaluate our
system.",2308.14951v1,https://arxiv.org/pdf/2308.14951v1
"RobustCLEVR: A Benchmark and Framework for Evaluating Robustness in
  Object-centric Learning","Nathan Drenkow, Mathias Unberath","Object-centric representation learning offers the potential to overcome
limitations of image-level representations by explicitly parsing image scenes
into their constituent components. While image-level representations typically
lack robustness to natural image corruptions, the robustness of object-centric
methods remains largely untested. To address this gap, we present the
RobustCLEVR benchmark dataset and evaluation framework. Our framework takes a
novel approach to evaluating robustness by enabling the specification of causal
dependencies in the image generation process grounded in expert knowledge and
capable of producing a wide range of image corruptions unattainable in existing
robustness evaluations. Using our framework, we define several causal models of
the image corruption process which explicitly encode assumptions about the
causal relationships and distributions of each corruption type. We generate
dataset variants for each causal model on which we evaluate state-of-the-art
object-centric methods. Overall, we find that object-centric methods are not
inherently robust to image corruptions. Our causal evaluation approach exposes
model sensitivities not observed using conventional evaluation processes,
yielding greater insight into robustness differences across algorithms. Lastly,
while conventional robustness evaluations view corruptions as
out-of-distribution, we use our causal framework to show that even training on
in-distribution image corruptions does not guarantee increased model
robustness. This work provides a step towards more concrete and substantiated
understanding of model performance and deterioration under complex corruption
processes of the real-world.",2308.14899v1,https://arxiv.org/pdf/2308.14899v1
"Robust Activity Recognition for Adaptive Worker-Robot Interaction using
  Transfer Learning","Farid Shahnavaz, Riley Tavassoli, Reza Akhavian","Human activity recognition (HAR) using machine learning has shown tremendous
promise in detecting construction workers' activities. HAR has many
applications in human-robot interaction research to enable robots'
understanding of human counterparts' activities. However, many existing HAR
approaches lack robustness, generalizability, and adaptability. This paper
proposes a transfer learning methodology for activity recognition of
construction workers that requires orders of magnitude less data and compute
time for comparable or better classification accuracy. The developed algorithm
transfers features from a model pre-trained by the original authors and
fine-tunes them for the downstream task of activity recognition in
construction. The model was pre-trained on Kinetics-400, a large-scale
video-based human activity recognition dataset with 400 distinct classes. The
model was fine-tuned and tested using videos captured from manual material
handling (MMH) activities found on YouTube. Results indicate that the
fine-tuned model can recognize distinct MMH tasks in a robust and adaptive
manner which is crucial for the widespread deployment of collaborative robots
in construction.",2308.14843v1,https://arxiv.org/pdf/2308.14843v1
"Distributionally Robust Statistical Verification with Imprecise Neural
  Networks","Souradeep Dutta, Michele Caprio, Vivian Lin, Matthew Cleaveland, Kuk Jin Jang, Ivan Ruchkin, Oleg Sokolsky, Insup Lee","A particularly challenging problem in AI safety is providing guarantees on
the behavior of high-dimensional autonomous systems. Verification approaches
centered around reachability analysis fail to scale, and purely statistical
approaches are constrained by the distributional assumptions about the sampling
process. Instead, we pose a distributionally robust version of the statistical
verification problem for black-box systems, where our performance guarantees
hold over a large family of distributions. This paper proposes a novel approach
based on a combination of active learning, uncertainty quantification, and
neural network verification. A central piece of our approach is an ensemble
technique called Imprecise Neural Networks, which provides the uncertainty to
guide active learning. The active learning uses an exhaustive neural-network
verification tool Sherlock to collect samples. An evaluation on multiple
physical simulators in the openAI gym Mujoco environments with
reinforcement-learned controllers demonstrates that our approach can provide
useful and scalable guarantees for high-dimensional systems.",2308.14815v3,https://arxiv.org/pdf/2308.14815v3
"Diversified Ensemble of Independent Sub-Networks for Robust
  Self-Supervised Representation Learning","Amirhossein Vahidi, Lisa Wimmer, Hüseyin Anil Gündüz, Bernd Bischl, Eyke Hüllermeier, Mina Rezaei","Ensembling a neural network is a widely recognized approach to enhance model
performance, estimate uncertainty, and improve robustness in deep supervised
learning. However, deep ensembles often come with high computational costs and
memory demands. In addition, the efficiency of a deep ensemble is related to
diversity among the ensemble members which is challenging for large,
over-parameterized deep neural networks. Moreover, ensemble learning has not
yet seen such widespread adoption, and it remains a challenging endeavor for
self-supervised or unsupervised representation learning. Motivated by these
challenges, we present a novel self-supervised training regime that leverages
an ensemble of independent sub-networks, complemented by a new loss function
designed to encourage diversity. Our method efficiently builds a sub-model
ensemble with high diversity, leading to well-calibrated estimates of model
uncertainty, all achieved with minimal computational overhead compared to
traditional deep self-supervised ensembles. To evaluate the effectiveness of
our approach, we conducted extensive experiments across various tasks,
including in-distribution generalization, out-of-distribution detection,
dataset corruption, and semi-supervised settings. The results demonstrate that
our method significantly improves prediction reliability. Our approach not only
achieves excellent accuracy but also enhances calibration, surpassing baseline
performance across a wide range of self-supervised architectures in computer
vision, natural language processing, and genomics data.",2308.14705v2,https://arxiv.org/pdf/2308.14705v2
"On the Tradeoff between Privacy Preservation and Byzantine-Robustness in
  Decentralized Learning","Haoxiang Ye, Heng Zhu, Qing Ling","This paper jointly considers privacy preservation and Byzantine-robustness in
decentralized learning. In a decentralized network, honest-but-curious agents
faithfully follow the prescribed algorithm, but expect to infer their
neighbors' private data from messages received during the learning process,
while dishonest-and-Byzantine agents disobey the prescribed algorithm, and
deliberately disseminate wrong messages to their neighbors so as to bias the
learning process. For this novel setting, we investigate a generic
privacy-preserving and Byzantine-robust decentralized stochastic gradient
descent (SGD) framework, in which Gaussian noise is injected to preserve
privacy and robust aggregation rules are adopted to counteract Byzantine
attacks. We analyze its learning error and privacy guarantee, discovering an
essential tradeoff between privacy preservation and Byzantine-robustness in
decentralized learning -- the learning error caused by defending against
Byzantine attacks is exacerbated by the Gaussian noise added to preserve
privacy. For a class of state-of-the-art robust aggregation rules, we give
unified analysis of the ""mixing abilities"". Building upon this analysis, we
reveal how the ""mixing abilities"" affect the tradeoff between privacy
preservation and Byzantine-robustness. The theoretical results provide
guidelines for achieving a favorable tradeoff with proper design of robust
aggregation rules. Numerical experiments are conducted and corroborate our
theoretical findings.",2308.14606v3,https://arxiv.org/pdf/2308.14606v3
Some issues in robust clustering,Christian Hennig,"Some key issues in robust clustering are discussed with focus on Gaussian
mixture model based clustering, namely the formal definition of outliers,
ambiguity between groups of outliers and clusters, the interaction between
robust clustering and the estimation of the number of clusters, the essential
dependence of (not only) robust clustering on tuning decisions, and
shortcomings of existing measurements of cluster stability when it comes to
outliers.",2308.14478v1,https://arxiv.org/pdf/2308.14478v1
"DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local
  Smoothing","Jiawei Zhang, Zhongzhu Chen, Huan Zhang, Chaowei Xiao, Bo Li","Diffusion models have been leveraged to perform adversarial purification and
thus provide both empirical and certified robustness for a standard model. On
the other hand, different robustly trained smoothed models have been studied to
improve the certified robustness. Thus, it raises a natural question: Can
diffusion model be used to achieve improved certified robustness on those
robustly trained smoothed models? In this work, we first theoretically show
that recovered instances by diffusion models are in the bounded neighborhood of
the original instance with high probability; and the ""one-shot"" denoising
diffusion probabilistic models (DDPM) can approximate the mean of the generated
distribution of a continuous-time diffusion model, which approximates the
original instance under mild conditions. Inspired by our analysis, we propose a
certifiably robust pipeline DiffSmooth, which first performs adversarial
purification via diffusion models and then maps the purified instances to a
common region via a simple yet effective local smoothing strategy. We conduct
extensive experiments on different datasets and show that DiffSmooth achieves
SOTA-certified robustness compared with eight baselines. For instance,
DiffSmooth improves the SOTA-certified accuracy from $36.0\%$ to $53.0\%$ under
$\ell_2$ radius $1.5$ on ImageNet. The code is available at
[https://github.com/javyduck/DiffSmooth].",2308.14333v1,https://arxiv.org/pdf/2308.14333v1
Evaluating the Robustness to Instructions of Large Language Models,"Yuansheng Ni, Sichao Jiang, Xinyu wu, Hui Shen, Yuli Zhou","Recently, Instruction fine-tuning has risen to prominence as a potential
method for enhancing the zero-shot capabilities of Large Language Models (LLMs)
on novel tasks. This technique has shown an exceptional ability to boost the
performance of moderately sized LLMs, sometimes even reaching performance
levels comparable to those of much larger model variants. The focus is on the
robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an
exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional
Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction
datasets as case studies. We carried out a comprehensive evaluation of these
instruction-following LLMs which have been tuned based on open-domain
instructions and task-oriented instructions. The main discussion is their
performance and robustness towards instructions. We have observed that in most
cases, the model's performance in dealing with unfamiliar instructions tends to
worsen significantly, and the robustness of the model for RE instructions
deteriorates compared to QA. Further, we discovered that up until a certain
parameter size threshold (3B), the performance of the FLAN-T5 model improves as
the parameter count increases. The robustness of different scales of FLAN-T5
models to RE instruction is worse than the robustness to QA instruction.",2308.14306v3,https://arxiv.org/pdf/2308.14306v3
Practical Edge Detection via Robust Collaborative Learning,"Yuanbin Fu, Xiaojie Guo","Edge detection, as a core component in a wide range of visionoriented tasks,
is to identify object boundaries and prominent edges in natural images. An edge
detector is desired to be both efficient and accurate for practical use. To
achieve the goal, two key issues should be concerned: 1) How to liberate deep
edge models from inefficient pre-trained backbones that are leveraged by most
existing deep learning methods, for saving the computational cost and cutting
the model size; and 2) How to mitigate the negative influence from noisy or
even wrong labels in training data, which widely exist in edge detection due to
the subjectivity and ambiguity of annotators, for the robustness and accuracy.
In this paper, we attempt to simultaneously address the above problems via
developing a collaborative learning based model, termed PEdger. The principle
behind our PEdger is that, the information learned from different training
moments and heterogeneous (recurrent and non recurrent in this work)
architectures, can be assembled to explore robust knowledge against noisy
annotations, even without the help of pre-training on extra data. Extensive
ablation studies together with quantitative and qualitative experimental
comparisons on the BSDS500 and NYUD datasets are conducted to verify the
effectiveness of our design, and demonstrate its superiority over other
competitors in terms of accuracy, speed, and model size. Codes can be found at
https://github.co/ForawardStar/PEdger.",2308.14084v1,https://arxiv.org/pdf/2308.14084v1
"Deep learning assisted robust detection techniques for a chipless RFID
  sensor tag","Nadeem Rather, Roy B. V. B. Simorangkir, John L. Buckley, Brendan O'Flynn, Salvatore Tedesco","In this paper, we present a new approach for robust reading of identification
and sensor data from chipless RFID sensor tags. For the first time, Machine
Learning (ML) and Deep Learning (DL) regression modelling techniques are
applied to a dataset of measured Radar Cross Section (RCS) data that has been
derived from large-scale robotic measurements of custom-designed, 3-bit
chipless RFID sensor tags. The robotic system is implemented using the
first-of-its-kind automated data acquisition method using an ur16e
industry-standard robot. A large data set of 9,600 Electromagnetic (EM) RCS
signatures collected using the automated system is used to train and validate
four ML models and four 1-dimensional Convolutional Neural Network (1D CNN)
architectures. For the first time, we report an end-to-end design and
implementation methodology for robust detection of identification (ID) and
sensing data using ML/DL models. Also, we report, for the first time, the
effect of varying tag surface shapes, tilt angles, and read ranges that were
incorporated into the training of models for robust detection of ID and sensing
values. The results show that all the models were able to generalise well on
the given data. However, the 1D CNN models outperformed the conventional ML
models in the detection of ID and sensing values. The best 1D CNN model
architectures performed well with a low Root Mean Square Error (RSME) of 0.061
(0.87%) for tag ID and 0.0241 (3.44%) error for the capacitive sensing.",2308.13944v1,https://arxiv.org/pdf/2308.13944v1
"Brain-like representational straightening of natural movies in robust
  feedforward neural networks","Tahereh Toosi, Elias B. Issa","Representational straightening refers to a decrease in curvature of visual
feature representations of a sequence of frames taken from natural movies.
Prior work established straightening in neural representations of the primate
primary visual cortex (V1) and perceptual straightening in human behavior as a
hallmark of biological vision in contrast to artificial feedforward neural
networks which did not demonstrate this phenomenon as they were not explicitly
optimized to produce temporally predictable movie representations. Here, we
show robustness to noise in the input image can produce representational
straightening in feedforward neural networks. Both adversarial training (AT)
and base classifiers for Random Smoothing (RS) induced remarkably straightened
feature codes. Demonstrating their utility within the domain of natural movies,
these codes could be inverted to generate intervening movie frames by linear
interpolation in the feature space even though they were not trained on these
trajectories. Demonstrating their biological utility, we found that AT and RS
training improved predictions of neural data in primate V1 over baseline models
providing a parsimonious, bio-plausible mechanism -- noise in the sensory input
stages -- for generating representations in early visual cortex. Finally, we
compared the geometric properties of frame representations in these networks to
better understand how they produced representations that mimicked the
straightening phenomenon from biology. Overall, this work elucidating emergent
properties of robust neural networks demonstrates that it is not necessary to
utilize predictive objectives or train directly on natural movie statistics to
achieve models supporting straightened movie representations similar to human
perception that also predict V1 neural responses.",2308.13870v1,https://arxiv.org/pdf/2308.13870v1
"Auto-weighted Bayesian Physics-Informed Neural Networks and robust
  estimations for multitask inverse problems in pore-scale imaging of
  dissolution","Sarah Perez, Philippe Poncet","In this article, we present a novel data assimilation strategy in pore-scale
imaging and demonstrate that this makes it possible to robustly address
reactive inverse problems incorporating Uncertainty Quantification (UQ).
Pore-scale modeling of reactive flow offers a valuable opportunity to
investigate the evolution of macro-scale properties subject to dynamic
processes. Yet, they suffer from imaging limitations arising from the
associated X-ray microtomography (X-ray microCT) process, which induces
discrepancies in the properties estimates. Assessment of the kinetic parameters
also raises challenges, as reactive coefficients are critical parameters that
can cover a wide range of values. We account for these two issues and ensure
reliable calibration of pore-scale modeling, based on dynamical microCT images,
by integrating uncertainty quantification in the workflow.
  The present method is based on a multitasking formulation of reactive inverse
problems combining data-driven and physics-informed techniques in calcite
dissolution. This allows quantifying morphological uncertainties on the
porosity field and estimating reactive parameter ranges through prescribed PDE
models with a latent concentration field and dynamical microCT. The data
assimilation strategy relies on sequential reinforcement incorporating
successively additional PDE constraints. We guarantee robust and unbiased
uncertainty quantification by straightforward adaptive weighting of Bayesian
Physics-Informed Neural Networks (BPINNs), ensuring reliable micro-porosity
changes during geochemical transformations. We demonstrate successful Bayesian
Inference in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT
images with meaningful posterior distribution on the reactive parameters and
dimensionless numbers.",2308.12864v1,https://arxiv.org/pdf/2308.12864v1
"A Huber Loss Minimization Approach to Byzantine Robust Federated
  Learning","Puning Zhao, Fei Yu, Zhiguo Wan","Federated learning systems are susceptible to adversarial attacks. To combat
this, we introduce a novel aggregator based on Huber loss minimization, and
provide a comprehensive theoretical analysis. Under independent and identically
distributed (i.i.d) assumption, our approach has several advantages compared to
existing methods. Firstly, it has optimal dependence on $\epsilon$, which
stands for the ratio of attacked clients. Secondly, our approach does not need
precise knowledge of $\epsilon$. Thirdly, it allows different clients to have
unequal data sizes. We then broaden our analysis to include non-i.i.d data,
such that clients have slightly different distributions.",2308.12581v2,https://arxiv.org/pdf/2308.12581v2
"LCANets++: Robust Audio Classification using Multi-layer Neural Networks
  with Lateral Competition","Sayanton V. Dibbo, Juston S. Moore, Garrett T. Kenyon, Michael A. Teti","Audio classification aims at recognizing audio signals, including speech
commands or sound events. However, current audio classifiers are susceptible to
perturbations and adversarial attacks. In addition, real-world audio
classification tasks often suffer from limited labeled data. To help bridge
these gaps, previous work developed neuro-inspired convolutional neural
networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)
in the first layer (i.e., LCANets) for computer vision. LCANets learn in a
combination of supervised and unsupervised learning, reducing dependency on
labeled samples. Motivated by the fact that auditory cortex is also sparse, we
extend LCANets to audio recognition tasks and introduce LCANets++, which are
CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that
LCANets++ are more robust than standard CNNs and LCANets against perturbations,
e.g., background noise, as well as black-box and white-box attacks, e.g.,
evasion and fast gradient sign (FGSM) attacks.",2308.12882v2,https://arxiv.org/pdf/2308.12882v2
"Robustness Analysis of Continuous-Depth Models with Lagrangian
  Techniques","Sophie A. Neubauer, Radu Grosu","This paper presents, in a unified fashion, deterministic as well as
statistical Lagrangian-verification techniques. They formally quantify the
behavioral robustness of any time-continuous process, formulated as a
continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube,
algorithms for constructing a tight reachtube, that is, an over-approximation
of the set of states reachable within a given time-horizon, and provide
guarantees for the reachtube bounds. We compare the usage of the variational
equations, associated to the system equations, the mean value theorem, and the
Lipschitz constants, in achieving deterministic and statistical guarantees. In
LRT-NG, the Lipschitz constant is used as a bloating factor of the initial
perturbation, to compute the radius of an ellipsoid in an optimal metric, which
over-approximates the set of reachable states. In SLR and GoTube, we get
statistical guarantees, by using the Lipschitz constants to compute local balls
around samples. These are needed to calculate the probability of having found
an upper bound, of the true maximum perturbation at every timestep. Our
experiments demonstrate the superior performance of Lagrangian techniques, when
compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness
analysis of various continuous-depth models.",2308.12192v1,https://arxiv.org/pdf/2308.12192v1
Sample Complexity of Robust Learning against Evasion Attacks,Pascale Gourdeau,"It is becoming increasingly important to understand the vulnerability of
machine learning models to adversarial attacks. One of the fundamental problems
in adversarial machine learning is to quantify how much training data is needed
in the presence of evasion attacks, where data is corrupted at test time. In
this thesis, we work with the exact-in-the-ball notion of robustness and study
the feasibility of adversarially robust learning from the perspective of
learning theory, considering sample complexity.
  We first explore the setting where the learner has access to random examples
only, and show that distributional assumptions are essential. We then focus on
learning problems with distributions on the input data that satisfy a Lipschitz
condition and show that robustly learning monotone conjunctions has sample
complexity at least exponential in the adversary's budget (the maximum number
of bits it can perturb on each input). However, if the adversary is restricted
to perturbing $O(\log n)$ bits, then one can robustly learn conjunctions and
decision lists w.r.t. log-Lipschitz distributions.
  We then study learning models where the learner is given more power. We first
consider local membership queries, where the learner can query the label of
points near the training sample. We show that, under the uniform distribution,
the exponential dependence on the adversary's budget to robustly learn
conjunctions remains inevitable. We then introduce a local equivalence query
oracle, which returns whether the hypothesis and target concept agree in a
given region around a point in the training sample, and a counterexample if it
exists. We show that if the query radius is equal to the adversary's budget, we
can develop robust empirical risk minimization algorithms in the
distribution-free setting. We give general query complexity upper and lower
bounds, as well as for concrete concept classes.",2308.12054v1,https://arxiv.org/pdf/2308.12054v1
"Designing an attack-defense game: how to increase robustness of
  financial transaction models via a competition","Alexey Zaytsev, Alex Natekin, Evgeni Vorsin, Valerii Smirnov, Georgii Smirnov, Oleg Sidorshin, Alexander Senin, Alexander Dudin, Dmitry Berestnev","Given the escalating risks of malicious attacks in the finance sector and the
consequential severe damage, a thorough understanding of adversarial strategies
and robust defense mechanisms for machine learning models is critical. The
threat becomes even more severe with the increased adoption in banks more
accurate, but potentially fragile neural networks. We aim to investigate the
current state and dynamics of adversarial attacks and defenses for neural
network models that use sequential financial data as the input.
  To achieve this goal, we have designed a competition that allows realistic
and detailed investigation of problems in modern financial transaction data.
The participants compete directly against each other, so possible attacks and
defenses are examined in close-to-real-life conditions. Our main contributions
are the analysis of the competition dynamics that answers the questions on how
important it is to conceal a model from malicious users, how long does it take
to break it, and what techniques one should use to make it more robust, and
introduction additional way to attack models or increase their robustness.
  Our analysis continues with a meta-study on the used approaches with their
power, numerical experiments, and accompanied ablations studies. We show that
the developed attacks and defenses outperform existing alternatives from the
literature while being practical in terms of execution, proving the validity of
the competition as a tool for uncovering vulnerabilities of machine learning
models and mitigating them in various domains.",2308.11406v2,https://arxiv.org/pdf/2308.11406v2
"Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained
  Markov Decision Processes",David M. Bossens,"The robust constrained Markov decision process (RCMDP) is a recent
task-modelling framework for reinforcement learning that incorporates
behavioural constraints and that provides robustness to errors in the
transition dynamics model through the use of an uncertainty set. Simulating
RCMDPs requires computing the worst-case dynamics based on value estimates for
each state, an approach which has previously been used in the Robust
Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG
such as not robustifying the full constrained objective and the lack of
incremental learning, this paper introduces two algorithms, called RCPG with
Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies
RCPG by taking the worst-case dynamics based on the Lagrangian rather than
either the value or the constraint. Adversarial RCPG also formulates the
worst-case dynamics based on the Lagrangian but learns this directly and
incrementally as an adversarial policy through gradient descent rather than
indirectly and abruptly through constrained optimisation on a sorted value
list. A theoretical analysis first derives the Lagrangian policy gradient for
the policy optimisation of both proposed algorithms and then the adversarial
policy gradient to learn the adversary for Adversarial RCPG. Empirical
experiments injecting perturbations in inventory management and safe navigation
tasks demonstrate the competitive performance of both algorithms compared to
traditional RCPG variants as well as non-robust and non-constrained ablations.
In particular, Adversarial RCPG ranks among the top two performing algorithms
on all tests.",2308.11267v3,https://arxiv.org/pdf/2308.11267v3
"Long-Term Prediction of Natural Video Sequences with Robust Video
  Predictors","Luke Ditria, Tom Drummond","Predicting high dimensional video sequences is a curiously difficult problem.
The number of possible futures for a given video sequence grows exponentially
over time due to uncertainty. This is especially evident when trying to predict
complicated natural video scenes from a limited snapshot of the world. The
inherent uncertainty accumulates the further into the future you predict making
long-term prediction very difficult. In this work we introduce a number of
improvements to existing work that aid in creating Robust Video Predictors
(RoViPs). We show that with a combination of deep Perceptual and
uncertainty-based reconstruction losses we are able to create high quality
short-term predictions. Attention-based skip connections are utilised to allow
for long range spatial movement of input features to further improve
performance. Finally, we show that by simply making the predictor robust to its
own prediction errors, it is possible to produce very long, realistic natural
video sequences using an iterated single-step prediction task.",2308.11079v1,https://arxiv.org/pdf/2308.11079v1
On the Adversarial Robustness of Multi-Modal Foundation Models,"Christian Schlarmann, Matthias Hein","Multi-modal foundation models combining vision and language models such as
Flamingo or GPT-4 have recently gained enormous interest. Alignment of
foundation models is used to prevent models from providing toxic or harmful
output. While malicious users have successfully tried to jailbreak foundation
models, an equally important question is if honest users could be harmed by
malicious third-party content. In this paper we show that imperceivable attacks
on images in order to change the caption output of a multi-modal foundation
model can be used by malicious content providers to harm honest users e.g. by
guiding them to malicious websites or broadcast fake information. This
indicates that countermeasures to adversarial attacks should be used by any
deployed multi-modal foundation model.",2308.10741v1,https://arxiv.org/pdf/2308.10741v1
"Measuring the Effect of Causal Disentanglement on the Adversarial
  Robustness of Neural Network Models","Preben M. Ness, Dusica Marijan, Sunanda Bose","Causal Neural Network models have shown high levels of robustness to
adversarial attacks as well as an increased capacity for generalisation tasks
such as few-shot learning and rare-context classification compared to
traditional Neural Networks. This robustness is argued to stem from the
disentanglement of causal and confounder input signals. However, no
quantitative study has yet measured the level of disentanglement achieved by
these types of causal models or assessed how this relates to their adversarial
robustness.
  Existing causal disentanglement metrics are not applicable to deterministic
models trained on real-world datasets. We, therefore, utilise metrics of
content/style disentanglement from the field of Computer Vision to measure
different aspects of the causal disentanglement for four state-of-the-art
causal Neural Network models. By re-implementing these models with a common
ResNet18 architecture we are able to fairly measure their adversarial
robustness on three standard image classification benchmarking datasets under
seven common white-box attacks. We find a strong association (r=0.820, p=0.001)
between the degree to which models decorrelate causal and confounder signals
and their adversarial robustness. Additionally, we find a moderate negative
association between the pixel-level information content of the confounder
signal and adversarial robustness (r=-0.597, p=0.040).",2308.10708v1,https://arxiv.org/pdf/2308.10708v1
"A Best-of-both-worlds Algorithm for Bandits with Delayed Feedback with
  Robustness to Excessive Delays","Saeed Masoudian, Julian Zimmert, Yevgeny Seldin","We propose a new best-of-both-worlds algorithm for bandits with variably
delayed feedback. In contrast to prior work, which required prior knowledge of
the maximal delay $d_{\mathrm{max}}$ and had a linear dependence of the regret
on it, our algorithm can tolerate arbitrary excessive delays up to order $T$
(where $T$ is the time horizon). The algorithm is based on three technical
innovations, which may all be of independent interest: (1) We introduce the
first implicit exploration scheme that works in best-of-both-worlds setting.
(2) We introduce the first control of distribution drift that does not rely on
boundedness of delays. The control is based on the implicit exploration scheme
and adaptive skipping of observations with excessive delays. (3) We introduce a
procedure relating standard regret with drifted regret that does not rely on
boundedness of delays. At the conceptual level, we demonstrate that complexity
of best-of-both-worlds bandits with delayed feedback is characterized by the
amount of information missing at the time of decision making (measured by the
number of outstanding observations) rather than the time that the information
is missing (measured by the delays).",2308.10675v2,https://arxiv.org/pdf/2308.10675v2
"Foundation Model-oriented Robustness: Robust Image Model Evaluation with
  Pretrained Models","Peiyan Zhang, Haoyang Liu, Chaozhuo Li, Xing Xie, Sunghun Kim, Haohan Wang","Machine learning has demonstrated remarkable performance over finite
datasets, yet whether the scores over the fixed benchmarks can sufficiently
indicate the model's performance in the real world is still in discussion. In
reality, an ideal robust model will probably behave similarly to the oracle
(e.g., the human users), thus a good evaluation protocol is probably to
evaluate the models' behaviors in comparison to the oracle. In this paper, we
introduce a new robustness measurement that directly measures the image
classification model's performance compared with a surrogate oracle (i.e., a
foundation model). Besides, we design a simple method that can accomplish the
evaluation beyond the scope of the benchmarks. Our method extends the image
datasets with new samples that are sufficiently perturbed to be distinct from
the ones in the original sets, but are still bounded within the same
image-label structure the original test image represents, constrained by a
foundation model pretrained with a large amount of samples. As a result, our
new method will offer us a new way to evaluate the models' robustness
performance, free of limitations of fixed benchmarks or constrained
perturbations, although scoped by the power of the oracle. In addition to the
evaluation results, we also leverage our generated data to understand the
behaviors of the model and our new evaluation strategies.",2308.10632v3,https://arxiv.org/pdf/2308.10632v3
"BackTrack: Robust template update via Backward Tracking of candidate
  template","Dongwook Lee, Wonjun Choi, Seohyung Lee, ByungIn Yoo, Eunho Yang, Seongju Hwang","Variations of target appearance such as deformations, illumination variance,
occlusion, etc., are the major challenges of visual object tracking that
negatively impact the performance of a tracker. An effective method to tackle
these challenges is template update, which updates the template to reflect the
change of appearance in the target object during tracking. However, with
template updates, inadequate quality of new templates or inappropriate timing
of updates may induce a model drift problem, which severely degrades the
tracking performance. Here, we propose BackTrack, a robust and reliable method
to quantify the confidence of the candidate template by backward tracking it on
the past frames. Based on the confidence score of candidates from BackTrack, we
can update the template with a reliable candidate at the right time while
rejecting unreliable candidates. BackTrack is a generic template update scheme
and is applicable to any template-based trackers. Extensive experiments on
various tracking benchmarks verify the effectiveness of BackTrack over existing
template update algorithms, as it achieves SOTA performance on various tracking
benchmarks.",2308.10604v1,https://arxiv.org/pdf/2308.10604v1
"Federated Learning Robust to Byzantine Attacks: Achieving Zero
  Optimality Gap","Shiyuan Zuo, Rongfei Fan, Han Hu, Ning Zhang, Shimin Gong","In this paper, we propose a robust aggregation method for federated learning
(FL) that can effectively tackle malicious Byzantine attacks. At each user,
model parameter is firstly updated by multiple steps, which is adjustable over
iterations, and then pushed to the aggregation center directly. This decreases
the number of interactions between the aggregation center and users, allows
each user to set training parameter in a flexible way, and reduces computation
burden compared with existing works that need to combine multiple historical
model parameters. At the aggregation center, geometric median is leveraged to
combine the received model parameters from each user. Rigorous proof shows that
zero optimality gap is achieved by our proposed method with linear convergence,
as long as the fraction of Byzantine attackers is below half. Numerical results
verify the effectiveness of our proposed method.",2308.10427v1,https://arxiv.org/pdf/2308.10427v1
"HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with
  Adaptive Firing Thresholds","Hejia Geng, Peng Li","While spiking neural networks (SNNs) offer a promising neurally-inspired
model of computation, they are vulnerable to adversarial attacks. We present
the first study that draws inspiration from neural homeostasis to design a
threshold-adapting leaky integrate-and-fire (TA-LIF) neuron model and utilize
TA-LIF neurons to construct the adversarially robust homeostatic SNNs (HoSNNs)
for improved robustness. The TA-LIF model incorporates a self-stabilizing
dynamic thresholding mechanism, offering a local feedback control solution to
the minimization of each neuron's membrane potential error caused by
adversarial disturbance. Theoretical analysis demonstrates favorable dynamic
properties of TA-LIF neurons in terms of the bounded-input bounded-output
stability and suppressed time growth of membrane potential error, underscoring
their superior robustness compared with the standard LIF neurons. When trained
with weak FGSM attacks (attack budget = 2/255) and tested with much stronger
PGD attacks (attack budget = 8/255), our HoSNNs significantly improve model
accuracy on several datasets: from 30.54% to 74.91% on FashionMNIST, from 0.44%
to 35.06% on SVHN, from 0.56% to 42.63% on CIFAR10, from 0.04% to 16.66% on
CIFAR100, over the conventional LIF-based SNNs.",2308.10373v3,https://arxiv.org/pdf/2308.10373v3
"Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability
  of Large Language Model Code Generation","Li Zhong, Zilong Wang","Recently, the large language models (LLMs) have shown extraordinary ability
in understanding natural language and generating programming code. It has been
a common practice of software engineers to consult LLMs when encountering
coding questions. Although efforts have been made to avoid syntax errors and
align the code with the intended semantics, the reliability and robustness of
the code generationfrom LLMs have not yet been thoroughly studied. The
executable code is not equivalent to the reliable and robust code, especially
in the context of real-world software development. The misuse of APIs in the
generated code could lead to severe problem, such as resource leaks, program
crashes. To make things worse, the users of LLM code generation services are
actually the developers that are most vulnerable to these code that seems right
-- They are always novice developers that are not familiar with the APIs that
LLMs generate code for them. Therefore, they could hardly tell the misuse in
the code generated by LLMs, which further facilitates the incorrect code
applied in real-world software. Existing code evaluation benchmark and datasets
focus on crafting small tasks such as programming questions in coding
interviews, which however deviates from the problem that developers would ask
LLM for real-world coding help. To fill the missing piece, in this work, we
propose a dataset RobustAPI for evaluating the reliability and robustness of
code generated by LLMs. We collect 1208 coding questions from StackOverflow on
24 representative Java APIs. We summarize thecommon misuse patterns of these
APIs and evaluate them oncurrent popular LLMs. The evaluation results show that
evenfor GPT-4, 62% of the generated code contains API misuses,which would cause
unexpected consequences if the code isintroduced into real-world software.",2308.10335v5,https://arxiv.org/pdf/2308.10335v5
OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision,"Shujie Zhang, Tianyue Zheng, Zhe Chen, Jingzhi Hu, Abdelwahed Khamis, Jiajun Liu, Jun Luo","Hand Pose Estimation (HPE) is crucial to many applications, but conventional
cameras-based CM-HPE methods are completely subject to Line-of-Sight (LoS), as
cameras cannot capture occluded objects. In this paper, we propose to exploit
Radio-Frequency-Vision (RF-vision) capable of bypassing obstacles for achieving
occluded HPE, and we introduce OCHID-Fi as the first RF-HPE method with 3D pose
estimation capability. OCHID-Fi employs wideband RF sensors widely available on
smart devices (e.g., iPhones) to probe 3D human hand pose and extract their
skeletons behind obstacles. To overcome the challenge in labeling RF imaging
given its human incomprehensible nature, OCHID-Fi employs a cross-modality and
cross-domain training process. It uses a pre-trained CM-HPE network and a
synchronized CM/RF dataset, to guide the training of its complex-valued RF-HPE
network under LoS conditions. It further transfers knowledge learned from
labeled LoS domain to unlabeled occluded domain via adversarial learning,
enabling OCHID-Fi to generalize to unseen occluded scenarios. Experimental
results demonstrate the superiority of OCHID-Fi: it achieves comparable
accuracy to CM-HPE under normal conditions while maintaining such accuracy even
in occluded scenarios, with empirical evidence for its generalizability to new
domains.",2308.10146v1,https://arxiv.org/pdf/2308.10146v1
"3D-Aware Neural Body Fitting for Occlusion Robust 3D Human Pose
  Estimation","Yi Zhang, Pengliang Ji, Angtian Wang, Jieru Mei, Adam Kortylewski, Alan Yuille","Regression-based methods for 3D human pose estimation directly predict the 3D
pose parameters from a 2D image using deep networks. While achieving
state-of-the-art performance on standard benchmarks, their performance degrades
under occlusion. In contrast, optimization-based methods fit a parametric body
model to 2D features in an iterative manner. The localized reconstruction loss
can potentially make them robust to occlusion, but they suffer from the 2D-3D
ambiguity.
  Motivated by the recent success of generative models in rigid object pose
estimation, we propose 3D-aware Neural Body Fitting (3DNBF) - an approximate
analysis-by-synthesis approach to 3D human pose estimation with SOTA
performance and occlusion robustness. In particular, we propose a generative
model of deep features based on a volumetric human representation with Gaussian
ellipsoidal kernels emitting 3D pose-dependent feature vectors. The neural
features are trained with contrastive learning to become 3D-aware and hence to
overcome the 2D-3D ambiguity.
  Experiments show that 3DNBF outperforms other approaches on both occluded and
standard benchmarks. Code is available at https://github.com/edz-o/3DNBF",2308.10123v1,https://arxiv.org/pdf/2308.10123v1
Robust Mixture-of-Expert Training for Convolutional Neural Networks,"Yihua Zhang, Ruisi Cai, Tianlong Chen, Guanhua Zhang, Huan Zhang, Pin-Yu Chen, Shiyu Chang, Zhangyang Wang, Sijia Liu","Sparsely-gated Mixture of Expert (MoE), an emerging deep model architecture,
has demonstrated a great promise to enable high-accuracy and ultra-efficient
model inference. Despite the growing popularity of MoE, little work
investigated its potential to advance convolutional neural networks (CNNs),
especially in the plane of adversarial robustness. Since the lack of robustness
has become one of the main hurdles for CNNs, in this paper we ask: How to
adversarially robustify a CNN-based MoE model? Can we robustly train it like an
ordinary CNN model? Our pilot study shows that the conventional adversarial
training (AT) mechanism (developed for vanilla CNNs) no longer remains
effective to robustify an MoE-CNN. To better understand this phenomenon, we
dissect the robustness of an MoE-CNN into two dimensions: Robustness of routers
(i.e., gating functions to select data-specific experts) and robustness of
experts (i.e., the router-guided pathways defined by the subnetworks of the
backbone CNN). Our analyses show that routers and experts are hard to adapt to
each other in the vanilla AT. Thus, we propose a new router-expert alternating
Adversarial training framework for MoE, termed AdvMoE. The effectiveness of our
proposal is justified across 4 commonly-used CNN model architectures over 4
benchmark datasets. We find that AdvMoE achieves 1% ~ 4% adversarial robustness
improvement over the original dense CNN, and enjoys the efficiency merit of
sparsity-gated MoE, leading to more than 50% inference cost reduction. Codes
are available at https://github.com/OPTML-Group/Robust-MoE-CNN.",2308.10110v1,https://arxiv.org/pdf/2308.10110v1
"ASPIRE: Language-Guided Data Augmentation for Improving Robustness
  Against Spurious Correlations","Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Utkarsh Tyagi, Sakshi Singh, Sanjoy Chowdhury, Dinesh Manocha","Neural image classifiers can often learn to make predictions by overly
relying on non-predictive features that are spuriously correlated with the
class labels in the training data. This leads to poor performance in real-world
atypical scenarios where such features are absent. This paper presents ASPIRE
(Language-guided Data Augmentation for SPurIous correlation REmoval), a simple
yet effective solution for supplementing the training dataset with images
without spurious features, for robust learning against spurious correlations
via better generalization. ASPIRE, guided by language at various steps, can
generate non-spurious images without requiring any group labeling or existing
non-spurious images in the training set. Precisely, we employ LLMs to first
extract foreground and background features from textual descriptions of an
image, followed by advanced language-guided image editing to discover the
features that are spuriously correlated with the class label. Finally, we
personalize a text-to-image generation model using the edited images to
generate diverse in-domain images without spurious features. ASPIRE is
complementary to all prior robust training methods in literature, and we
demonstrate its effectiveness across 4 datasets and 9 baselines and show that
ASPIRE improves the worst-group classification accuracy of prior methods by 1%
- 38%. We also contribute a novel test set for the challenging Hard ImageNet
dataset.",2308.10103v3,https://arxiv.org/pdf/2308.10103v3
Robust Fraud Detection via Supervised Contrastive Learning,"Vinay M. S., Shuhan Yuan, Xintao Wu","Deep learning models have recently become popular for detecting malicious
user activity sessions in computing platforms. In many real-world scenarios,
only a few labeled malicious and a large amount of normal sessions are
available. These few labeled malicious sessions usually do not cover the entire
diversity of all possible malicious sessions. In many scenarios, possible
malicious sessions can be highly diverse. As a consequence, learned session
representations of deep learning models can become ineffective in achieving a
good generalization performance for unseen malicious sessions. To tackle this
open-set fraud detection challenge, we propose a robust supervised contrastive
learning based framework called ConRo, which specifically operates in the
scenario where only a few malicious sessions having limited diversity is
available. ConRo applies an effective data augmentation strategy to generate
diverse potential malicious sessions. By employing these generated and
available training set sessions, ConRo derives separable representations w.r.t
open-set fraud detection task by leveraging supervised contrastive learning. We
empirically evaluate our ConRo framework and other state-of-the-art baselines
on benchmark datasets. Our ConRo framework demonstrates noticeable performance
improvement over state-of-the-art baselines.",2308.10055v1,https://arxiv.org/pdf/2308.10055v1
Distributionally Robust Cross Subject EEG Decoding,"Tiehang Duan, Zhenyi Wang, Gianfranco Doretto, Fang Li, Cui Tao, Donald Adjeroh","Recently, deep learning has shown to be effective for Electroencephalography
(EEG) decoding tasks. Yet, its performance can be negatively influenced by two
key factors: 1) the high variance and different types of corruption that are
inherent in the signal, 2) the EEG datasets are usually relatively small given
the acquisition cost, annotation cost and amount of effort needed. Data
augmentation approaches for alleviation of this problem have been empirically
studied, with augmentation operations on spatial domain, time domain or
frequency domain handcrafted based on expertise of domain knowledge. In this
work, we propose a principled approach to perform dynamic evolution on the data
for improvement of decoding robustness. The approach is based on
distributionally robust optimization and achieves robustness by optimizing on a
family of evolved data distributions instead of the single training data
distribution. We derived a general data evolution framework based on
Wasserstein gradient flow (WGF) and provides two different forms of evolution
within the framework. Intuitively, the evolution process helps the EEG decoder
to learn more robust and diverse features. It is worth mentioning that the
proposed approach can be readily integrated with other data augmentation
approaches for further improvements. We performed extensive experiments on the
proposed approach and tested its performance on different types of corrupted
EEG signals. The model significantly outperforms competitive baselines on
challenging decoding scenarios.",2308.11651v1,https://arxiv.org/pdf/2308.11651v1
"On the Robustness of Open-World Test-Time Training: Self-Training with
  Dynamic Prototype Expansion","Yushu Li, Xun Xu, Yongyi Su, Kui Jia","Generalizing deep learning models to unknown target domain distribution with
low latency has motivated research into test-time training/adaptation
(TTT/TTA). Existing approaches often focus on improving test-time training
performance under well-curated target domain data. As figured out in this work,
many state-of-the-art methods fail to maintain the performance when the target
domain is contaminated with strong out-of-distribution (OOD) data, a.k.a.
open-world test-time training (OWTTT). The failure is mainly due to the
inability to distinguish strong OOD samples from regular weak OOD samples. To
improve the robustness of OWTTT we first develop an adaptive strong OOD pruning
which improves the efficacy of the self-training TTT method. We further propose
a way to dynamically expand the prototypes to represent strong OOD samples for
an improved weak/strong OOD data separation. Finally, we regularize
self-training with distribution alignment and the combination yields the
state-of-the-art performance on 5 OWTTT benchmarks. The code is available at
https://github.com/Yushu-Li/OWTTT.",2308.09942v1,https://arxiv.org/pdf/2308.09942v1
"Synergistic Integration of Large Language Models and Cognitive
  Architectures for Robust AI: An Exploratory Analysis","Oscar J. Romero, John Zimmerman, Aaron Steinfeld, Anthony Tomasic","This paper explores the integration of two AI subdisciplines employed in the
development of artificial agents that exhibit intelligent behavior: Large
Language Models (LLMs) and Cognitive Architectures (CAs). We present three
integration approaches, each grounded in theoretical models and supported by
preliminary empirical evidence. The modular approach, which introduces four
models with varying degrees of integration, makes use of chain-of-thought
prompting, and draws inspiration from augmented LLMs, the Common Model of
Cognition, and the simulation theory of cognition. The agency approach,
motivated by the Society of Mind theory and the LIDA cognitive architecture,
proposes the formation of agent collections that interact at micro and macro
cognitive levels, driven by either LLMs or symbolic components. The
neuro-symbolic approach, which takes inspiration from the CLARION cognitive
architecture, proposes a model where bottom-up learning extracts symbolic
representations from an LLM layer and top-down guidance utilizes symbolic
representations to direct prompt engineering in the LLM layer. These approaches
aim to harness the strengths of both LLMs and CAs, while mitigating their
weaknesses, thereby advancing the development of more robust AI systems. We
discuss the tradeoffs and challenges associated with each approach.",2308.09830v3,https://arxiv.org/pdf/2308.09830v3
Robust Monocular Depth Estimation under Challenging Conditions,"Stefano Gasperini, Nils Morbitzer, HyunJun Jung, Nassir Navab, Federico Tombari","While state-of-the-art monocular depth estimation approaches achieve
impressive results in ideal settings, they are highly unreliable under
challenging illumination and weather conditions, such as at nighttime or in the
presence of rain. In this paper, we uncover these safety-critical issues and
tackle them with md4all: a simple and effective solution that works reliably
under both adverse and ideal conditions, as well as for different types of
learning supervision. We achieve this by exploiting the efficacy of existing
methods under perfect settings. Therefore, we provide valid training signals
independently of what is in the input. First, we generate a set of complex
samples corresponding to the normal training ones. Then, we train the model by
guiding its self- or full-supervision by feeding the generated samples and
computing the standard losses on the corresponding original images. Doing so
enables a single model to recover information across diverse conditions without
modifications at inference time. Extensive experiments on two challenging
public datasets, namely nuScenes and Oxford RobotCar, demonstrate the
effectiveness of our techniques, outperforming prior works by a large margin in
both standard and challenging conditions. Source code and data are available
at: https://md4all.github.io.",2308.09711v1,https://arxiv.org/pdf/2308.09711v1
A Lightweight Transformer for Faster and Robust EBSD Data Collection,"Harry Dong, Sean Donegan, Megna Shah, Yuejie Chi","Three dimensional electron back-scattered diffraction (EBSD) microscopy is a
critical tool in many applications in materials science, yet its data quality
can fluctuate greatly during the arduous collection process, particularly via
serial-sectioning. Fortunately, 3D EBSD data is inherently sequential, opening
up the opportunity to use transformers, state-of-the-art deep learning
architectures that have made breakthroughs in a plethora of domains, for data
processing and recovery. To be more robust to errors and accelerate this 3D
EBSD data collection, we introduce a two step method that recovers missing
slices in an 3D EBSD volume, using an efficient transformer model and a
projection algorithm to process the transformer's outputs. Overcoming the
computational and practical hurdles of deep learning with scarce high
dimensional data, we train this model using only synthetic 3D EBSD data with
self-supervision and obtain superior recovery accuracy on real 3D EBSD data,
compared to existing methods.",2308.09693v1,https://arxiv.org/pdf/2308.09693v1
"PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D
  Human Pose Estimation","Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie","Existing 3D human pose estimators face challenges in adapting to new datasets
due to the lack of 2D-3D pose pairs in training sets. To overcome this issue,
we propose \textit{Multi-Hypothesis \textbf{P}ose \textbf{Syn}thesis
\textbf{D}omain \textbf{A}daptation} (\textbf{PoSynDA}) framework to bridge
this data disparity gap in target domain. Typically, PoSynDA uses a
diffusion-inspired structure to simulate 3D pose distribution in the target
domain. By incorporating a multi-hypothesis network, PoSynDA generates diverse
pose hypotheses and aligns them with the target domain. To do this, it first
utilizes target-specific source augmentation to obtain the target domain
distribution data from the source domain by decoupling the scale and position
parameters. The process is then further refined through the teacher-student
paradigm and low-rank adaptation. With extensive comparison of benchmarks such
as Human3.6M and MPI-INF-3DHP, PoSynDA demonstrates competitive performance,
even comparable to the target-trained MixSTE model\cite{zhang2022mixste}. This
work paves the way for the practical application of 3D human pose estimation in
unseen domains. The code is available at https://github.com/hbing-l/PoSynDA.",2308.09678v2,https://arxiv.org/pdf/2308.09678v2
"Robust Uncertainty Quantification Using Conformalised Monte Carlo
  Prediction","Daniel Bethell, Simos Gerasimou, Radu Calinescu","Deploying deep learning models in safety-critical applications remains a very
challenging task, mandating the provision of assurances for the dependable
operation of these models. Uncertainty quantification (UQ) methods estimate the
model's confidence per prediction, informing decision-making by considering the
effect of randomness and model misspecification. Despite the advances of
state-of-the-art UQ methods, they are computationally expensive or produce
conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ
method that combines a new adaptive Monte Carlo (MC) dropout method with
conformal prediction (CP). MC-CP adaptively modulates the traditional MC
dropout at runtime to save memory and computation resources, enabling
predictions to be consumed by CP, yielding robust prediction sets/intervals.
Throughout comprehensive experiments, we show that MC-CP delivers significant
improvements over advanced UQ methods, like MC dropout, RAPS and CQR, both in
classification and regression benchmarks. MC-CP can be easily added to existing
models, making its deployment simple.",2308.09647v2,https://arxiv.org/pdf/2308.09647v2
Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents,"Arrasy Rahman, Jiaxun Cui, Peter Stone","Robustly cooperating with unseen agents and human partners presents
significant challenges due to the diverse cooperative conventions these
partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this
challenge by training an agent with a population of diverse teammate policies
obtained through maximizing specific diversity metrics. However, prior
heuristic-based diversity metrics do not always maximize the agent's robustness
in all cooperative problems. In this work, we first propose that maximizing an
AHT agent's robustness requires it to emulate policies in the minimum coverage
set (MCS), the set of best-response policies to any partner policies in the
environment. We then introduce the L-BRDiv algorithm that generates a set of
teammate policies that, when used for AHT training, encourage agents to emulate
policies from the MCS. L-BRDiv works by solving a constrained optimization
problem to jointly train teammate policies for AHT training and approximating
AHT agent policies that are members of the MCS. We empirically demonstrate that
L-BRDiv produces more robust AHT agents than state-of-the-art methods in a
broader range of two-player cooperative problems without the need for extensive
hyperparameter tuning for its objectives. Our study shows that L-BRDiv
outperforms the baseline methods by prioritizing discovering distinct members
of the MCS instead of repeatedly finding redundant policies.",2308.09595v2,https://arxiv.org/pdf/2308.09595v2
"Distributed Neurodynamics-Based Backstepping Optimal Control for Robust
  Constrained Consensus of Underactuated Underwater Vehicles Fleet","Tao Yan, Zhe Xu, Simon X. Yang, S. Andrew Gadsden","Robust constrained formation tracking control of underactuated underwater
vehicles (UUVs) fleet in three-dimensional space is a challenging but practical
problem. To address this problem, this paper develops a novel consensus based
optimal coordination protocol and a robust controller, which adopts a
hierarchical architecture. On the top layer, the spherical coordinate transform
is introduced to tackle the nonholonomic constraint, and then a distributed
optimal motion coordination strategy is developed. As a result, the optimal
formation tracking of UUVs fleet can be achieved, and the constraints are
fulfilled. To realize the generated optimal commands better and, meanwhile,
deal with the underactuation, at the lower-level control loop a neurodynamics
based robust backstepping controller is designed, and in particular, the issue
of ""explosion of terms"" appearing in conventional backstepping based
controllers is avoided and control activities are improved. The stability of
the overall UUVs formation system is established to ensure that all the states
of the UUVs are uniformly ultimately bounded in the presence of unknown
disturbances. Finally, extensive simulation comparisons are made to illustrate
the superiority and effectiveness of the derived optimal formation tracking
protocol.",2308.09326v1,https://arxiv.org/pdf/2308.09326v1
"Distributed Robust Learning-Based Backstepping Control Aided with
  Neurodynamics for Consensus Formation Tracking of Underwater Vessels","Tao Yan, Zhe Xu, Simon X. Yang","This paper addresses distributed robust learning-based control for consensus
formation tracking of multiple underwater vessels, in which the system
parameters of the marine vessels are assumed to be entirely unknown and subject
to the modeling mismatch, oceanic disturbances, and noises. Towards this end,
graph theory is used to allow us to synthesize the distributed controller with
a stability guarantee. Due to the fact that the parameter uncertainties only
arise in the vessels' dynamic model, the backstepping control technique is then
employed. Subsequently, to overcome the difficulties in handling time-varying
and unknown systems, an online learning procedure is developed in the proposed
distributed formation control protocol. Moreover, modeling errors,
environmental disturbances, and measurement noises are considered and tackled
by introducing a neurodynamics model in the controller design to obtain a
robust solution. Then, the stability analysis of the overall closed-loop system
under the proposed scheme is provided to ensure the robust adaptive performance
at the theoretical level. Finally, extensive simulation experiments are
conducted to further verify the efficacy of the presented distributed control
protocol.",2308.09320v1,https://arxiv.org/pdf/2308.09320v1
"Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on
  Multi-Order Spectrograms","Penghui Wen, Kun Hu, Wenxi Yue, Sen Zhang, Wanlei Zhou, Zhiyong Wang","Robust audio anti-spoofing has been increasingly challenging due to the
recent advancements on deepfake techniques. While spectrograms have
demonstrated their capability for anti-spoofing, complementary information
presented in multi-order spectral patterns have not been well explored, which
limits their effectiveness for varying spoofing attacks. Therefore, we propose
a novel deep learning method with a spectral fusion-reconstruction strategy,
namely S2pecNet, to utilise multi-order spectral patterns for robust audio
anti-spoofing representations. Specifically, spectral patterns up to
second-order are fused in a coarse-to-fine manner and two branches are designed
for the fine-level fusion from the spectral and temporal contexts. A
reconstruction from the fused representation to the input spectrograms further
reduces the potential fused information loss. Our method achieved the
state-of-the-art performance with an EER of 0.77% on a widely used dataset:
ASVspoof2019 LA Challenge.",2308.09302v1,https://arxiv.org/pdf/2308.09302v1
"A Robust Policy Bootstrapping Algorithm for Multi-objective
  Reinforcement Learning in Non-stationary Environments","Sherif Abdelfattah, Kathryn Kasmarik, Jiankun Hu","Multi-objective Markov decision processes are a special kind of
multi-objective optimization problem that involves sequential decision making
while satisfying the Markov property of stochastic processes. Multi-objective
reinforcement learning methods address this problem by fusing the reinforcement
learning paradigm with multi-objective optimization techniques. One major
drawback of these methods is the lack of adaptability to non-stationary
dynamics in the environment. This is because they adopt optimization procedures
that assume stationarity to evolve a coverage set of policies that can solve
the problem. This paper introduces a developmental optimization approach that
can evolve the policy coverage set while exploring the preference space over
the defined objectives in an online manner. We propose a novel multi-objective
reinforcement learning algorithm that can robustly evolve a convex coverage set
of policies in an online manner in non-stationary environments. We compare the
proposed algorithm with two state-of-the-art multi-objective reinforcement
learning algorithms in stationary and non-stationary environments. Results
showed that the proposed algorithm significantly outperforms the existing
algorithms in non-stationary environments while achieving comparable results in
stationary environments.",2308.09734v1,https://arxiv.org/pdf/2308.09734v1
"Discretization-Induced Dirichlet Posterior for Robust Uncertainty
  Quantification on Regression","Xuanlong Yu, Gianni Franchi, Jindong Gu, Emanuel Aldea","Uncertainty quantification is critical for deploying deep neural networks
(DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE)
is one of the most effective means to estimate the uncertainty of the main task
prediction without modifying the main task model. To be considered robust, an
AuxUE must be capable of maintaining its performance and triggering higher
uncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., to
provide robust aleatoric and epistemic uncertainty. However, for vision
regression tasks, current AuxUE designs are mainly adopted for aleatoric
uncertainty estimates, and AuxUE robustness has not been explored. In this
work, we propose a generalized AuxUE scheme for more robust uncertainty
quantification on regression tasks. Concretely, to achieve a more robust
aleatoric uncertainty estimation, different distribution assumptions are
considered for heteroscedastic noise, and Laplace distribution is finally
chosen to approximate the prediction error. For epistemic uncertainty, we
propose a novel solution named Discretization-Induced Dirichlet pOsterior
(DIDO), which models the Dirichlet posterior on the discretized prediction
error. Extensive experiments on age estimation, monocular depth estimation, and
super-resolution tasks show that our proposed method can provide robust
uncertainty estimates in the face of noisy inputs and that it can be scalable
to both image-level and pixel-wise tasks. Code is available at
https://github.com/ENSTA-U2IS/DIDO .",2308.09065v2,https://arxiv.org/pdf/2308.09065v2
"General Lipschitz: Certified Robustness Against Resolvable Semantic
  Transformations via Transformation-Dependent Randomized Smoothing","Dmitrii Korzh, Mikhail Pautov, Olga Tsymboi, Ivan Oseledets","Randomized smoothing is the state-of-the-art approach to construct image
classifiers that are provably robust against additive adversarial perturbations
of bounded magnitude. However, it is more complicated to construct reasonable
certificates against semantic transformation (e.g., image blurring,
translation, gamma correction) and their compositions. In this work, we propose
\emph{General Lipschitz (GL),} a new framework to certify neural networks
against composable resolvable semantic perturbations. Within the framework, we
analyze transformation-dependent Lipschitz-continuity of smoothed classifiers
w.r.t. transformation parameters and derive corresponding robustness
certificates. Our method performs comparably to state-of-the-art approaches on
the ImageNet dataset.",2309.16710v2,https://arxiv.org/pdf/2309.16710v2
"Causal Adversarial Perturbations for Individual Fairness and Robustness
  in Heterogeneous Data Spaces","Ahmad-Reza Ehyaei, Kiarash Mohammadi, Amir-Hossein Karimi, Samira Samadi, Golnoosh Farnadi","As responsible AI gains importance in machine learning algorithms, properties
such as fairness, adversarial robustness, and causality have received
considerable attention in recent years. However, despite their individual
significance, there remains a critical gap in simultaneously exploring and
integrating these properties. In this paper, we propose a novel approach that
examines the relationship between individual fairness, adversarial robustness,
and structural causal models in heterogeneous data spaces, particularly when
dealing with discrete sensitive attributes. We use causal structural models and
sensitive attributes to create a fair metric and apply it to measure semantic
similarity among individuals. By introducing a novel causal adversarial
perturbation and applying adversarial training, we create a new regularizer
that combines individual fairness, causality, and robustness in the classifier.
Our method is evaluated on both real-world and synthetic datasets,
demonstrating its effectiveness in achieving an accurate classifier that
simultaneously exhibits fairness, adversarial robustness, and causal awareness.",2308.08938v1,https://arxiv.org/pdf/2308.08938v1
"Evaluating the Instruction-Following Robustness of Large Language Models
  to Prompt Injection","Zekun Li, Baolin Peng, Pengcheng He, Xifeng Yan","Large Language Models (LLMs) have demonstrated exceptional proficiency in
instruction-following, becoming increasingly crucial across various
applications. However, this capability brings with it the risk of prompt
injection attacks, where attackers inject instructions into LLMs' input to
elicit undesirable actions or content. Understanding the robustness of LLMs
against such attacks is vital for their safe implementation. In this work, we
establish a benchmark to evaluate the robustness of instruction-following LLMs
against prompt injection attacks. Our objective is to determine the extent to
which LLMs can be influenced by injected instructions and their ability to
differentiate between these injected and original target instructions. Through
extensive experiments with leading instruction-following LLMs, we uncover
significant vulnerabilities in their robustness to such attacks. Our results
indicate that some models are overly tuned to follow any embedded instructions
in the prompt, overly focusing on the latter parts of the prompt without fully
grasping the entire context. By contrast, models with a better grasp of the
context and instruction-following capabilities will potentially be more
susceptible to compromise by injected instructions. This underscores the need
to shift the focus from merely enhancing LLMs' instruction-following
capabilities to improving their overall comprehension of prompts and
discernment of instructions that are appropriate to follow. We hope our
in-depth analysis offers insights into the underlying causes of these
vulnerabilities, aiding in the development of future solutions. Code and data
are available at
https://github.com/Leezekun/instruction-following-robustness-eval",2308.10819v3,https://arxiv.org/pdf/2308.10819v3
"Dynamic Neural Network is All You Need: Understanding the Robustness of
  Dynamic Mechanisms in Neural Networks","Mirazul Haque, Wei Yang","Deep Neural Networks (DNNs) have been used to solve different day-to-day
problems. Recently, DNNs have been deployed in real-time systems, and lowering
the energy consumption and response time has become the need of the hour. To
address this scenario, researchers have proposed incorporating dynamic
mechanism to static DNNs (SDNN) to create Dynamic Neural Networks (DyNNs)
performing dynamic amounts of computation based on the input complexity.
Although incorporating dynamic mechanism into SDNNs would be preferable in
real-time systems, it also becomes important to evaluate how the introduction
of dynamic mechanism impacts the robustness of the models. However, there has
not been a significant number of works focusing on the robustness trade-off
between SDNNs and DyNNs. To address this issue, we propose to investigate the
robustness of dynamic mechanism in DyNNs and how dynamic mechanism design
impacts the robustness of DyNNs. For that purpose, we evaluate three research
questions. These evaluations are performed on three models and two datasets.
Through the studies, we find that attack transferability from DyNNs to SDNNs is
higher than attack transferability from SDNNs to DyNNs. Also, we find that
DyNNs can be used to generate adversarial samples more efficiently than SDNNs.
Then, through research studies, we provide insight into the design choices that
can increase robustness of DyNNs against the attack generated using static
model. Finally, we propose a novel attack to understand the additional attack
surface introduced by the dynamic mechanism and provide design choices to
improve robustness against the attack.",2308.08709v1,https://arxiv.org/pdf/2308.08709v1
Robust Bayesian Satisficing,"Artun Saday, Yaşar Cahit Yıldırım, Cem Tekin","Distributional shifts pose a significant challenge to achieving robustness in
contemporary machine learning. To overcome this challenge, robust satisficing
(RS) seeks a robust solution to an unspecified distributional shift while
achieving a utility above a desired threshold. This paper focuses on the
problem of RS in contextual Bayesian optimization when there is a discrepancy
between the true and reference distributions of the context. We propose a novel
robust Bayesian satisficing algorithm called RoBOS for noisy black-box
optimization. Our algorithm guarantees sublinear lenient regret under certain
assumptions on the amount of distribution shift. In addition, we define a
weaker notion of regret called robust satisficing regret, in which our
algorithm achieves a sublinear upper bound independent of the amount of
distribution shift. To demonstrate the effectiveness of our method, we apply it
to various learning problems and compare it to other approaches, such as
distributionally robust optimization.",2308.08291v1,https://arxiv.org/pdf/2308.08291v1
"HyperSNN: A new efficient and robust deep learning model for resource
  constrained control applications","Zhanglu Yan, Shida Wang, Kaiwen Tang, Weng-Fai Wong","In light of the increasing adoption of edge computing in areas such as
intelligent furniture, robotics, and smart homes, this paper introduces
HyperSNN, an innovative method for control tasks that uses spiking neural
networks (SNNs) in combination with hyperdimensional computing. HyperSNN
substitutes expensive 32-bit floating point multiplications with 8-bit integer
additions, resulting in reduced energy consumption while enhancing robustness
and potentially improving accuracy. Our model was tested on AI Gym benchmarks,
including Cartpole, Acrobot, MountainCar, and Lunar Lander. HyperSNN achieves
control accuracies that are on par with conventional machine learning methods
but with only 1.36% to 9.96% of the energy expenditure. Furthermore, our
experiments showed increased robustness when using HyperSNN. We believe that
HyperSNN is especially suitable for interactive, mobile, and wearable devices,
promoting energy-efficient and robust system design. Furthermore, it paves the
way for the practical implementation of complex algorithms like model
predictive control (MPC) in real-world industrial scenarios.",2308.08222v2,https://arxiv.org/pdf/2308.08222v2
"Expressivity of Graph Neural Networks Through the Lens of Adversarial
  Robustness","Francesco Campi, Lukas Gosch, Tom Wollschläger, Yan Scholten, Stephan Günnemann","We perform the first adversarial robustness study into Graph Neural Networks
(GNNs) that are provably more powerful than traditional Message Passing Neural
Networks (MPNNs). In particular, we use adversarial robustness as a tool to
uncover a significant gap between their theoretically possible and empirically
achieved expressive power. To do so, we focus on the ability of GNNs to count
specific subgraph patterns, which is an established measure of expressivity,
and extend the concept of adversarial robustness to this task. Based on this,
we develop efficient adversarial attacks for subgraph counting and show that
more powerful GNNs fail to generalize even to small perturbations to the
graph's structure. Expanding on this, we show that such architectures also fail
to count substructures on out-of-distribution graphs.",2308.08173v2,https://arxiv.org/pdf/2308.08173v2
Benchmarking Adversarial Robustness of Compressed Deep Learning Models,"Brijesh Vora, Kartik Patwari, Syed Mahbub Hafiz, Zubair Shafiq, Chen-Nee Chuah","The increasing size of Deep Neural Networks (DNNs) poses a pressing need for
model compression, particularly when employed on resource constrained devices.
Concurrently, the susceptibility of DNNs to adversarial attacks presents
another significant hurdle. Despite substantial research on both model
compression and adversarial robustness, their joint examination remains
underexplored. Our study bridges this gap, seeking to understand the effect of
adversarial inputs crafted for base models on their pruned versions. To examine
this relationship, we have developed a comprehensive benchmark across diverse
adversarial attacks and popular DNN models. We uniquely focus on models not
previously exposed to adversarial training and apply pruning schemes optimized
for accuracy and performance. Our findings reveal that while the benefits of
pruning enhanced generalizability, compression, and faster inference times are
preserved, adversarial robustness remains comparable to the base model. This
suggests that model compression while offering its unique advantages, does not
undermine adversarial robustness.",2308.08160v1,https://arxiv.org/pdf/2308.08160v1
"Robust Bayesian Tensor Factorization with Zero-Inflated Poisson Model
  and Consensus Aggregation","Daniel Chafamo, Vignesh Shanmugam, Neriman Tokcan","Tensor factorizations (TF) are powerful tools for the efficient
representation and analysis of multidimensional data. However, classic TF
methods based on maximum likelihood estimation underperform when applied to
zero-inflated count data, such as single-cell RNA sequencing (scRNA-seq) data.
Additionally, the stochasticity inherent in TFs results in factors that vary
across repeated runs, making interpretation and reproducibility of the results
challenging. In this paper, we introduce Zero Inflated Poisson Tensor
Factorization (ZIPTF), a novel approach for the factorization of
high-dimensional count data with excess zeros. To address the challenge of
stochasticity, we introduce Consensus Zero Inflated Poisson Tensor
Factorization (C-ZIPTF), which combines ZIPTF with a consensus-based
meta-analysis. We evaluate our proposed ZIPTF and C-ZIPTF on synthetic
zero-inflated count data and synthetic and real scRNA-seq data. ZIPTF
consistently outperforms baseline matrix and tensor factorization methods in
terms of reconstruction accuracy for zero-inflated data. When the probability
of excess zeros is high, ZIPTF achieves up to $2.4\times$ better accuracy.
Additionally, C-ZIPTF significantly improves the consistency and accuracy of
the factorization. When tested on both synthetic and real scRNA-seq data, ZIPTF
and C-ZIPTF consistently recover known and biologically meaningful gene
expression programs.",2308.08060v1,https://arxiv.org/pdf/2308.08060v1
A Robust Adaptive Workload Orchestration in Pure Edge Computing,"Zahra Safavifar, Charafeddine Mechalikh, Fatemeh Golpayegani","Pure Edge computing (PEC) aims to bring cloud applications and services to
the edge of the network to support the growing user demand for time-sensitive
applications and data-driven computing. However, mobility and limited
computational capacity of edge devices pose challenges in supporting some
urgent and computationally intensive tasks with strict response time demands.
If the execution results of these tasks exceed the deadline, they become
worthless and can cause severe safety issues. Therefore, it is essential to
ensure that edge nodes complete as many latency-sensitive tasks as possible.
\\In this paper, we propose a Robust Adaptive Workload Orchestration
(R-AdWOrch) model to minimize deadline misses and data loss by using priority
definition and a reallocation strategy. The results show that R-AdWOrch can
minimize deadline misses of urgent tasks while minimizing the data loss of
lower priority tasks under all conditions.",2309.03913v1,https://arxiv.org/pdf/2309.03913v1
Robustified ANNs Reveal Wormholes Between Human Category Percepts,"Guy Gaziv, Michael J. Lee, James J. DiCarlo","The visual object category reports of artificial neural networks (ANNs) are
notoriously sensitive to tiny, adversarial image perturbations. Because human
category reports (aka human percepts) are thought to be insensitive to those
same small-norm perturbations -- and locally stable in general -- this argues
that ANNs are incomplete scientific models of human visual perception.
Consistent with this, we show that when small-norm image perturbations are
generated by standard ANN models, human object category percepts are indeed
highly stable. However, in this very same ""human-presumed-stable"" regime, we
find that robustified ANNs reliably discover low-norm image perturbations that
strongly disrupt human percepts. These previously undetectable human perceptual
disruptions are massive in amplitude, approaching the same level of sensitivity
seen in robustified ANNs. Further, we show that robustified ANNs support
precise perceptual state interventions: they guide the construction of low-norm
image perturbations that strongly alter human category percepts toward specific
prescribed percepts. These observations suggest that for arbitrary starting
points in image space, there exists a set of nearby ""wormholes"", each leading
the subject from their current category perceptual state into a semantically
very different state. Moreover, contemporary ANN models of biological visual
processing are now accurate enough to consistently guide us to those portals.",2308.06887v2,https://arxiv.org/pdf/2308.06887v2
"Robust Infidelity: When Faithfulness Measures on Masked Language Models
  Are Misleading","Evan Crothers, Herna Viktor, Nathalie Japkowicz","A common approach to quantifying neural text classifier interpretability is
to calculate faithfulness metrics based on iteratively masking salient input
tokens and measuring changes in the model prediction. We propose that this
property is better described as ""sensitivity to iterative masking"", and
highlight pitfalls in using this measure for comparing text classifier
interpretability. We show that iterative masking produces large variation in
faithfulness scores between otherwise comparable Transformer encoder text
classifiers. We then demonstrate that iteratively masked samples produce
embeddings outside the distribution seen during training, resulting in
unpredictable behaviour. We further explore task-specific considerations that
undermine principled comparison of interpretability using iterative masking,
such as an underlying similarity to salience-based adversarial attacks. Our
findings give insight into how these behaviours affect neural text classifiers,
and provide guidance on how sensitivity to iterative masking should be
interpreted.",2308.06795v2,https://arxiv.org/pdf/2308.06795v2
CDR: Conservative Doubly Robust Learning for Debiased Recommendation,"ZiJie Song, JiaWei Chen, Sheng Zhou, QiHao Shi, Yan Feng, Chun Chen, Can Wang","In recommendation systems (RS), user behavior data is observational rather
than experimental, resulting in widespread bias in the data. Consequently,
tackling bias has emerged as a major challenge in the field of recommendation
systems. Recently, Doubly Robust Learning (DR) has gained significant attention
due to its remarkable performance and robust properties. However, our
experimental findings indicate that existing DR methods are severely impacted
by the presence of so-called Poisonous Imputation, where the imputation
significantly deviates from the truth and becomes counterproductive.
  To address this issue, this work proposes Conservative Doubly Robust strategy
(CDR) which filters imputations by scrutinizing their mean and variance.
Theoretical analyses show that CDR offers reduced variance and improved tail
bounds.In addition, our experimental investigations illustrate that CDR
significantly enhances performance and can indeed reduce the frequency of
poisonous imputation.",2308.08461v2,https://arxiv.org/pdf/2308.08461v2
"Understanding the robustness difference between stochastic gradient
  descent and adaptive gradient methods","Avery Ma, Yangchen Pan, Amir-massoud Farahmand","Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam
and RMSProp, have been widely used in training deep neural networks. We
empirically show that while the difference between the standard generalization
performance of models trained using these methods is small, those trained using
SGD exhibit far greater robustness under input perturbations. Notably, our
investigation demonstrates the presence of irrelevant frequencies in natural
datasets, where alterations do not affect models' generalization performance.
However, models trained with adaptive methods show sensitivity to these
changes, suggesting that their use of irrelevant frequencies can lead to
solutions sensitive to perturbations. To better understand this difference, we
study the learning dynamics of gradient descent (GD) and sign gradient descent
(signGD) on a synthetic dataset that mirrors natural signals. With a
three-dimensional input space, the models optimized with GD and signGD have
standard risks close to zero but vary in their adversarial risks. Our result
shows that linear models' robustness to $\ell_2$-norm bounded changes is
inversely proportional to the model parameters' weight norm: a smaller weight
norm implies better robustness. In the context of deep learning, our
experiments show that SGD-trained neural networks have smaller Lipschitz
constants, explaining the better robustness to input perturbations than those
trained with adaptive gradient methods.",2308.06703v2,https://arxiv.org/pdf/2308.06703v2
On the Interplay of Convolutional Padding and Adversarial Robustness,"Paul Gavrikov, Janis Keuper","It is common practice to apply padding prior to convolution operations to
preserve the resolution of feature-maps in Convolutional Neural Networks (CNN).
While many alternatives exist, this is often achieved by adding a border of
zeros around the inputs. In this work, we show that adversarial attacks often
result in perturbation anomalies at the image boundaries, which are the areas
where padding is used. Consequently, we aim to provide an analysis of the
interplay between padding and adversarial attacks and seek an answer to the
question of how different padding modes (or their absence) affect adversarial
robustness in various scenarios.",2308.06612v1,https://arxiv.org/pdf/2308.06612v1
EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes,"Jiaxi Jiang, Paul Streli, Manuel Meier, Christian Holz","Full-body ego-pose estimation from head and hand poses alone has become an
active area of research to power articulate avatar representation on
headset-based platforms. However, existing methods over-rely on the confines of
the motion-capture spaces in which datasets were recorded, while simultaneously
assuming continuous capture of joint motions and uniform body dimensions. In
this paper, we propose EgoPoser, which overcomes these limitations by 1)
rethinking the input representation for headset-based ego-pose estimation and
introducing a novel motion decomposition method that predicts full-body pose
independent of global positions, 2) robustly modeling body pose from
intermittent hand position and orientation tracking only when inside a
headset's field of view, and 3) generalizing across various body sizes for
different users. Our experiments show that EgoPoser outperforms
state-of-the-art methods both qualitatively and quantitatively, while
maintaining a high inference speed of over 600 fps. EgoPoser establishes a
robust baseline for future work, where full-body pose estimation needs no
longer rely on outside-in capture and can scale to large-scene environments.",2308.06493v2,https://arxiv.org/pdf/2308.06493v2
"Not So Robust After All: Evaluating the Robustness of Deep Neural
  Networks to Unseen Adversarial Attacks","Roman Garaev, Bader Rasheed, Adil Khan","Deep neural networks (DNNs) have gained prominence in various applications,
such as classification, recognition, and prediction, prompting increased
scrutiny of their properties. A fundamental attribute of traditional DNNs is
their vulnerability to modifications in input data, which has resulted in the
investigation of adversarial attacks. These attacks manipulate the data in
order to mislead a DNN. This study aims to challenge the efficacy and
generalization of contemporary defense mechanisms against adversarial attacks.
Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits
that DNN image features can be either robust or non-robust, with adversarial
attacks targeting the latter. This hypothesis suggests that training a DNN on a
dataset consisting solely of robust features should produce a model resistant
to adversarial attacks. However, our experiments demonstrate that this is not
universally true. To gain further insights into our findings, we analyze the
impact of adversarial attack norms on DNN representations, focusing on samples
subjected to $L_2$ and $L_{\infty}$ norm attacks. Further, we employ canonical
correlation analysis, visualize the representations, and calculate the mean
distance between these representations and various DNN decision boundaries. Our
results reveal a significant difference between $L_2$ and $L_{\infty}$ norms,
which could provide insights into the potential dangers posed by $L_{\infty}$
norm attacks, previously underestimated by the research community.",2308.06467v1,https://arxiv.org/pdf/2308.06467v1
"Audio-Visual Spatial Integration and Recursive Attention for Robust
  Sound Source Localization","Sung Jin Um, Dongjin Kim, Jung Uk Kim","The objective of the sound source localization task is to enable machines to
detect the location of sound-making objects within a visual scene. While the
audio modality provides spatial cues to locate the sound source, existing
approaches only use audio as an auxiliary role to compare spatial regions of
the visual modality. Humans, on the other hand, utilize both audio and visual
modalities as spatial cues to locate sound sources. In this paper, we propose
an audio-visual spatial integration network that integrates spatial cues from
both modalities to mimic human behavior when detecting sound-making objects.
Additionally, we introduce a recursive attention network to mimic human
behavior of iterative focusing on objects, resulting in more accurate attention
regions. To effectively encode spatial information from both modalities, we
propose audio-visual pair matching loss and spatial region alignment loss. By
utilizing the spatial cues of audio-visual modalities and recursively focusing
objects, our method can perform more robust sound source localization.
Comprehensive experimental results on the Flickr SoundNet and VGG-Sound Source
datasets demonstrate the superiority of our proposed method over existing
approaches. Our code is available at: https://github.com/VisualAIKHU/SIRA-SSL",2308.06087v2,https://arxiv.org/pdf/2308.06087v2
"Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence
  and Variance Reduction","Xiaowen Jiang, Sebastian U. Stich","The recently proposed stochastic Polyak stepsize (SPS) and stochastic
line-search (SLS) for SGD have shown remarkable effectiveness when training
over-parameterized models. However, in non-interpolation settings, both
algorithms only guarantee convergence to a neighborhood of a solution which may
result in a worse output than the initial guess. While artificially decreasing
the adaptive stepsize has been proposed to address this issue (Orvieto et al.
[2022]), this approach results in slower convergence rates for convex and
over-parameterized models. In this work, we make two contributions: Firstly, we
propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which
guarantee convergence in non-interpolation settings and maintain sub-linear and
linear convergence rates for convex and strongly convex functions when training
over-parameterized models. AdaSLS requires no knowledge of problem-dependent
parameters, and AdaSPS requires only a lower bound of the optimal function
value as input. Secondly, we equip AdaSPS and AdaSLS with a novel variance
reduction technique and obtain algorithms that require
$\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achieve
an $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improves
upon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS without
variance reduction in the non-interpolation regimes. Moreover, our result
matches the fast rates of AdaSVRG but removes the inner-outer-loop structure,
which is easier to implement and analyze. Finally, numerical experiments on
synthetic and real datasets validate our theory and demonstrate the
effectiveness and robustness of our algorithms.",2308.06058v2,https://arxiv.org/pdf/2308.06058v2
"TrajPAC: Towards Robustness Verification of Pedestrian Trajectory
  Prediction Models","Liang Zhang, Nathaniel Xu, Pengfei Yang, Gaojie Jin, Cheng-Chao Huang, Lijun Zhang","Robust pedestrian trajectory forecasting is crucial to developing safe
autonomous vehicles. Although previous works have studied adversarial
robustness in the context of trajectory forecasting, some significant issues
remain unaddressed. In this work, we try to tackle these crucial problems.
Firstly, the previous definitions of robustness in trajectory prediction are
ambiguous. We thus provide formal definitions for two kinds of robustness,
namely label robustness and pure robustness. Secondly, as previous works fail
to consider robustness about all points in a disturbance interval, we utilise a
probably approximately correct (PAC) framework for robustness verification.
Additionally, this framework can not only identify potential counterexamples,
but also provides interpretable analyses of the original methods. Our approach
is applied using a prototype tool named TrajPAC. With TrajPAC, we evaluate the
robustness of four state-of-the-art trajectory prediction models --
Trajectron++, MemoNet, AgentFormer, and MID -- on trajectories from five scenes
of the ETH/UCY dataset and scenes of the Stanford Drone Dataset. Using our
framework, we also experimentally study various factors that could influence
robustness performance.",2308.05985v1,https://arxiv.org/pdf/2308.05985v1
Robustifying Point Cloud Networks by Refocusing,"Meir Yossef Levi, Guy Gilboa","The ability to cope with out-of-distribution (OOD) corruptions and
adversarial attacks is crucial in real-world safety-demanding applications. In
this study, we develop a general mechanism to increase neural network
robustness based on focus analysis.
  Recent studies have revealed the phenomenon of \textit{Overfocusing}, which
leads to a performance drop. When the network is primarily influenced by small
input regions, it becomes less robust and prone to misclassify under noise and
corruptions.
  However, quantifying overfocusing is still vague and lacks clear definitions.
Here, we provide a mathematical definition of \textbf{focus},
\textbf{overfocusing} and \textbf{underfocusing}. The notions are general, but
in this study, we specifically investigate the case of 3D point clouds.
  We observe that corrupted sets result in a biased focus distribution compared
to the clean training set.
  We show that as focus distribution deviates from the one learned in the
training phase - classification performance deteriorates.
  We thus propose a parameter-free \textbf{refocusing} algorithm that aims to
unify all corruptions under the same distribution.
  We validate our findings on a 3D zero-shot classification task, achieving
SOTA in robust 3D classification on ModelNet-C dataset, and in adversarial
defense against Shape-Invariant attack. Code is available in:
https://github.com/yossilevii100/refocusing.",2308.05525v3,https://arxiv.org/pdf/2308.05525v3
"Comprehensive Analysis of Network Robustness Evaluation Based on
  Convolutional Neural Networks with Spatial Pyramid Pooling","Wenjun Jiang, Tianlong Fan, Changhao Li, Chuanfu Zhang, Tao Zhang, Zong-fu Luo","Connectivity robustness, a crucial aspect for understanding, optimizing, and
repairing complex networks, has traditionally been evaluated through
time-consuming and often impractical simulations. Fortunately, machine learning
provides a new avenue for addressing this challenge. However, several key
issues remain unresolved, including the performance in more general edge
removal scenarios, capturing robustness through attack curves instead of
directly training for robustness, scalability of predictive tasks, and
transferability of predictive capabilities. In this paper, we address these
challenges by designing a convolutional neural networks (CNN) model with
spatial pyramid pooling networks (SPP-net), adapting existing evaluation
metrics, redesigning the attack modes, introducing appropriate filtering rules,
and incorporating the value of robustness as training data. The results
demonstrate the thoroughness of the proposed CNN framework in addressing the
challenges of high computational time across various network types, failure
component types and failure scenarios. However, the performance of the proposed
CNN model varies: for evaluation tasks that are consistent with the trained
network type, the proposed CNN model consistently achieves accurate evaluations
of both attack curves and robustness values across all removal scenarios. When
the predicted network type differs from the trained network, the CNN model
still demonstrates favorable performance in the scenario of random node
failure, showcasing its scalability and performance transferability.
Nevertheless, the performance falls short of expectations in other removal
scenarios. This observed scenario-sensitivity in the evaluation of network
features has been overlooked in previous studies and necessitates further
attention and optimization. Lastly, we discuss important unresolved questions
and further investigation.",2308.08012v3,https://arxiv.org/pdf/2308.08012v3
"Byzantine-Robust Decentralized Stochastic Optimization with Stochastic
  Gradient Noise-Independent Learning Error","Jie Peng, Weiyu Li, Qing Ling","This paper studies Byzantine-robust stochastic optimization over a
decentralized network, where every agent periodically communicates with its
neighbors to exchange local models, and then updates its own local model by
stochastic gradient descent (SGD). The performance of such a method is affected
by an unknown number of Byzantine agents, which conduct adversarially during
the optimization process. To the best of our knowledge, there is no existing
work that simultaneously achieves a linear convergence speed and a small
learning error. We observe that the learning error is largely dependent on the
intrinsic stochastic gradient noise. Motivated by this observation, we
introduce two variance reduction methods, stochastic average gradient algorithm
(SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to
Byzantine-robust decentralized stochastic optimization for eliminating the
negative effect of the stochastic gradient noise. The two resulting methods,
BRAVO-SAGA and BRAVO-LSVRG, enjoy both linear convergence speeds and stochastic
gradient noise-independent learning errors. Such learning errors are optimal
for a class of methods based on total variation (TV)-norm regularization and
stochastic subgradient update. We conduct extensive numerical experiments to
demonstrate their effectiveness under various Byzantine attacks.",2308.05292v1,https://arxiv.org/pdf/2308.05292v1
"Competitions in AI -- Robustly Ranking Solvers Using Statistical
  Resampling","Chris Fawcett, Mauro Vallati, Holger H. Hoos, Alfonso E. Gerevini","Solver competitions play a prominent role in assessing and advancing the
state of the art for solving many problems in AI and beyond. Notably, in many
areas of AI, competitions have had substantial impact in guiding research and
applications for many years, and for a solver to be ranked highly in a
competition carries considerable weight. But to which extent can we expect
competition results to generalise to sets of problem instances different from
those used in a particular competition? This is the question we investigate
here, using statistical resampling techniques. We show that the rankings
resulting from the standard interpretation of competition results can be very
sensitive to even minor changes in the benchmark instance set used as the basis
for assessment and can therefore not be expected to carry over to other samples
from the same underlying instance distribution. To address this problem, we
introduce a novel approach to statistically meaningful analysis of competition
results based on resampling performance data. Our approach produces confidence
intervals of competition scores as well as statistically robust solver rankings
with bounded error. Applied to recent SAT, AI planning and computer vision
competitions, our analysis reveals frequent statistical ties in solver
performance as well as some inversions of ranks compared to the official
results based on simple scoring.",2308.05062v1,https://arxiv.org/pdf/2308.05062v1
"Adversarial ModSecurity: Countering Adversarial SQL Injections with
  Robust Machine Learning","Biagio Montaruli, Luca Demetrio, Andrea Valenza, Luca Compagna, Davide Ariu, Luca Piras, Davide Balzarotti, Battista Biggio","ModSecurity is widely recognized as the standard open-source Web Application
Firewall (WAF), maintained by the OWASP Foundation. It detects malicious
requests by matching them against the Core Rule Set, identifying well-known
attack patterns. Each rule in the CRS is manually assigned a weight, based on
the severity of the corresponding attack, and a request is detected as
malicious if the sum of the weights of the firing rules exceeds a given
threshold. In this work, we show that this simple strategy is largely
ineffective for detecting SQL injection (SQLi) attacks, as it tends to block
many legitimate requests, while also being vulnerable to adversarial SQLi
attacks, i.e., attacks intentionally manipulated to evade detection. To
overcome these issues, we design a robust machine learning model, named
AdvModSec, which uses the CRS rules as input features, and it is trained to
detect adversarial SQLi attacks. Our experiments show that AdvModSec, being
trained on the traffic directed towards the protected web services, achieves a
better trade-off between detection and false positive rates, improving the
detection rate of the vanilla version of ModSecurity with CRS by 21%. Moreover,
our approach is able to improve its adversarial robustness against adversarial
SQLi attacks by 42%, thereby taking a step forward towards building more robust
and trustworthy WAFs.",2308.04964v2,https://arxiv.org/pdf/2308.04964v2
"Representation Learning for Audio Privacy Preservation using Source
  Separation and Robust Adversarial Learning","Diep Luong, Minh Tran, Shayan Gharib, Konstantinos Drossos, Tuomas Virtanen","Privacy preservation has long been a concern in smart acoustic monitoring
systems, where speech can be passively recorded along with a target signal in
the system's operating environment. In this study, we propose the integration
of two commonly used approaches in privacy preservation: source separation and
adversarial representation learning. The proposed system learns the latent
representation of audio recordings such that it prevents differentiating
between speech and non-speech recordings. Initially, the source separation
network filters out some of the privacy-sensitive data, and during the
adversarial learning process, the system will learn privacy-preserving
representation on the filtered signal. We demonstrate the effectiveness of our
proposed method by comparing our method against systems without source
separation, without adversarial learning, and without both. Overall, our
results suggest that the proposed system can significantly improve speech
privacy preservation compared to that of using source separation or adversarial
learning solely while maintaining good performance in the acoustic monitoring
task.",2308.04960v1,https://arxiv.org/pdf/2308.04960v1
"Comprehensive Assessment of the Performance of Deep Learning Classifiers
  Reveals a Surprising Lack of Robustness",Michael W. Spratling,"Reliable and robust evaluation methods are a necessary first step towards
developing machine learning models that are themselves robust and reliable.
Unfortunately, current evaluation protocols typically used to assess
classifiers fail to comprehensively evaluate performance as they tend to rely
on limited types of test data, and ignore others. For example, using the
standard test data fails to evaluate the predictions made by the classifier to
samples from classes it was not trained on. On the other hand, testing with
data containing samples from unknown classes fails to evaluate how well the
classifier can predict the labels for known classes. This article advocates
bench-marking performance using a wide range of different types of data and
using a single metric that can be applied to all such data types to produce a
consistent evaluation of performance. Using such a benchmark it is found that
current deep neural networks, including those trained with methods that are
believed to produce state-of-the-art robustness, are extremely vulnerable to
making mistakes on certain types of data. This means that such models will be
unreliable in real-world scenarios where they may encounter data from many
different domains, and that they are insecure as they can easily be fooled into
making the wrong decisions. It is hoped that these results will motivate the
wider adoption of more comprehensive testing methods that will, in turn, lead
to the development of more robust machine learning methods in the future.
  Code is available at:
\url{https://codeberg.org/mwspratling/RobustnessEvaluation}",2308.04137v1,https://arxiv.org/pdf/2308.04137v1
"Enhancing Adversarial Robustness in Low-Label Regime via Adaptively
  Weighted Regularization and Knowledge Distillation","Dongyoon Yang, Insung Kong, Yongdai Kim","Adversarial robustness is a research area that has recently received a lot of
attention in the quest for trustworthy artificial intelligence. However, recent
works on adversarial robustness have focused on supervised learning where it is
assumed that labeled data is plentiful. In this paper, we investigate
semi-supervised adversarial training where labeled data is scarce. We derive
two upper bounds for the robust risk and propose a regularization term for
unlabeled data motivated by these two upper bounds. Then, we develop a
semi-supervised adversarial training algorithm that combines the proposed
regularization term with knowledge distillation using a semi-supervised teacher
(i.e., a teacher model trained using a semi-supervised learning algorithm). Our
experiments show that our proposed algorithm achieves state-of-the-art
performance with significant margins compared to existing algorithms. In
particular, compared to supervised learning algorithms, performance of our
proposed algorithm is not much worse even when the amount of labeled data is
very small. For example, our algorithm with only 8\% labeled data is comparable
to supervised adversarial training algorithms that use all labeled data, both
in terms of standard and robust accuracies on CIFAR-10.",2308.04061v1,https://arxiv.org/pdf/2308.04061v1
Fixed Inter-Neuron Covariability Induces Adversarial Robustness,"Muhammad Ahmed Shah, Bhiksha Raj","The vulnerability to adversarial perturbations is a major flaw of Deep Neural
Networks (DNNs) that raises question about their reliability when in real-world
scenarios. On the other hand, human perception, which DNNs are supposed to
emulate, is highly robust to such perturbations, indicating that there may be
certain features of the human perception that make it robust but are not
represented in the current class of DNNs. One such feature is that the activity
of biological neurons is correlated and the structure of this correlation tends
to be rather rigid over long spans of times, even if it hampers performance and
learning. We hypothesize that integrating such constraints on the activations
of a DNN would improve its adversarial robustness, and, to test this
hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which
comprises of neurons whose activations are consistent with each other, as they
conform to a fixed, but learned, covariability pattern. When evaluated on image
and sound recognition tasks, the models with a SCA layer achieved high
accuracy, and exhibited significantly greater robustness than multi-layer
perceptron models to state-of-the-art Auto-PGD adversarial attacks
\textit{without being trained on adversarially perturbed data",2308.03956v1,https://arxiv.org/pdf/2308.03956v1
"Bridging Trustworthiness and Open-World Learning: An Exploratory Neural
  Approach for Enhancing Interpretability, Generalization, and Robustness","Shide Du, Zihan Fang, Shiyang Lan, Yanchao Tan, Manuel Günther, Shiping Wang, Wenzhong Guo","As researchers strive to narrow the gap between machine intelligence and
human through the development of artificial intelligence technologies, it is
imperative that we recognize the critical importance of trustworthiness in
open-world, which has become ubiquitous in all aspects of daily life for
everyone. However, several challenges may create a crisis of trust in current
artificial intelligence systems that need to be bridged: 1) Insufficient
explanation of predictive results; 2) Inadequate generalization for learning
models; 3) Poor adaptability to uncertain environments. Consequently, we
explore a neural program to bridge trustworthiness and open-world learning,
extending from single-modal to multi-modal scenarios for readers. 1) To enhance
design-level interpretability, we first customize trustworthy networks with
specific physical meanings; 2) We then design environmental well-being
task-interfaces via flexible learning regularizers for improving the
generalization of trustworthy learning; 3) We propose to increase the
robustness of trustworthy learning by integrating open-world recognition losses
with agent mechanisms. Eventually, we enhance various trustworthy properties
through the establishment of design-level explainability, environmental
well-being task-interfaces and open-world recognition programs. These designed
open-world protocols are applicable across a wide range of surroundings, under
open-world multimedia recognition scenarios with significant performance
improvements observed.",2308.03666v4,https://arxiv.org/pdf/2308.03666v4
Distributionally Robust Classification on a Data Budget,"Benjamin Feuer, Ameya Joshi, Minh Pham, Chinmay Hegde","Real world uses of deep learning require predictable model behavior under
distribution shifts. Models such as CLIP show emergent natural distributional
robustness comparable to humans, but may require hundreds of millions of
training samples. Can we train robust learners in a domain where data is
limited? To rigorously address this question, we introduce JANuS (Joint
Annotations and Names Set), a collection of four new training datasets with
images, labels, and corresponding captions, and perform a series of carefully
controlled investigations of factors contributing to robustness in image
classification, then compare those results to findings derived from a
large-scale meta-analysis. Using this approach, we show that standard ResNet-50
trained with the cross-entropy loss on 2.4 million image samples can attain
comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To
our knowledge, this is the first result showing (near) state-of-the-art
distributional robustness on limited data budgets. Our dataset is available at
\url{https://huggingface.co/datasets/penfever/JANuS_dataset}, and the code used
to reproduce our experiments can be found at
\url{https://github.com/penfever/vlhub/}.",2308.03821v1,https://arxiv.org/pdf/2308.03821v1
Exploring the Physical World Adversarial Robustness of Vehicle Detection,"Wei Jiang, Tianyuan Zhang, Shuangcheng Liu, Weiyu Ji, Zichao Zhang, Gang Xiao","Adversarial attacks can compromise the robustness of real-world detection
models. However, evaluating these models under real-world conditions poses
challenges due to resource-intensive experiments. Virtual simulations offer an
alternative, but the absence of standardized benchmarks hampers progress.
Addressing this, we propose an innovative instant-level data generation
pipeline using the CARLA simulator. Through this pipeline, we establish the
Discrete and Continuous Instant-level (DCI) dataset, enabling comprehensive
experiments involving three detection models and three physical adversarial
attacks. Our findings highlight diverse model performances under adversarial
conditions. Yolo v6 demonstrates remarkable resilience, experiencing just a
marginal 6.59% average drop in average precision (AP). In contrast, the ASA
attack yields a substantial 14.51% average AP reduction, twice the effect of
other algorithms. We also note that static scenes yield higher recognition AP
values, and outcomes remain relatively consistent across varying weather
conditions. Intriguingly, our study suggests that advancements in adversarial
attack algorithms may be approaching its ``limitation''.In summary, our work
underscores the significance of adversarial attacks in real-world contexts and
introduces the DCI dataset as a versatile benchmark. Our findings provide
valuable insights for enhancing the robustness of detection models and offer
guidance for future research endeavors in the realm of adversarial attacks.",2308.03476v1,https://arxiv.org/pdf/2308.03476v1
"Doubly Robust Estimator for Off-Policy Evaluation with Large Action
  Spaces","Tatsuhiro Shimizu, Laura Forastiere","We study Off-Policy Evaluation (OPE) in contextual bandit settings with large
action spaces. The benchmark estimators suffer from severe bias and variance
tradeoffs. Parametric approaches suffer from bias due to difficulty specifying
the correct model, whereas ones with importance weight suffer from variance. To
overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was
proposed to mitigate the estimator's variance via embeddings of an action.
Nevertheless, MIPS is unbiased under the no direct effect, which assumes that
the action embedding completely mediates the effect of an action on a reward.
To overcome the dependency on these unrealistic assumptions, we propose a
Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the
proposed estimator is unbiased under weaker assumptions than MIPS while
reducing the variance against MIPS. The empirical experiment verifies the
supremacy of MDR against existing estimators with large action spaces.",2308.03443v3,https://arxiv.org/pdf/2308.03443v3
Robust Ordinal Regression for Subsets Comparisons with Interactions,"Hugo Gilbert, Mohamed Ouaguenouni, Meltem Ozturk, Olivier Spanjaard","This paper is dedicated to a robust ordinal method for learning the
preferences of a decision maker between subsets. The decision model, derived
from Fishburn and LaValle (1996) and whose parameters we learn, is general
enough to be compatible with any strict weak order on subsets, thanks to the
consideration of possible interactions between elements. Moreover, we accept
not to predict some preferences if the available preference data are not
compatible with a reliable prediction. A predicted preference is considered
reliable if all the simplest models (Occam's razor) explaining the preference
data agree on it. Following the robust ordinal regression methodology, our
predictions are based on an uncertainty set encompassing the possible values of
the model parameters. We define a robust ordinal dominance relation between
subsets and we design a procedure to determine whether this dominance relation
holds. Numerical tests are provided on synthetic and real-world data to
evaluate the richness and reliability of the preference predictions made.",2308.03376v1,https://arxiv.org/pdf/2308.03376v1
"Auditing and Robustifying COVID-19 Misinformation Datasets via
  Anticontent Sampling","Clay H. Yoo, Ashiqur R. KhudaBukhsh","This paper makes two key contributions. First, it argues that highly
specialized rare content classifiers trained on small data typically have
limited exposure to the richness and topical diversity of the negative class
(dubbed anticontent) as observed in the wild. As a result, these classifiers'
strong performance observed on the test set may not translate into real-world
settings. In the context of COVID-19 misinformation detection, we conduct an
in-the-wild audit of multiple datasets and demonstrate that models trained with
several prominently cited recent datasets are vulnerable to anticontent when
evaluated in the wild. Second, we present a novel active learning pipeline that
requires zero manual annotation and iteratively augments the training data with
challenging anticontent, robustifying these classifiers.",2310.07078v1,https://arxiv.org/pdf/2310.07078v1
"SureFED: Robust Federated Learning via Uncertainty-Aware Inward and
  Outward Inspection","Nasimeh Heydaribeni, Ruisi Zhang, Tara Javidi, Cristina Nita-Rotaru, Farinaz Koushanfar","In this work, we introduce SureFED, a novel framework for byzantine robust
federated learning. Unlike many existing defense methods that rely on
statistically robust quantities, making them vulnerable to stealthy and
colluding attacks, SureFED establishes trust using the local information of
benign clients. SureFED utilizes an uncertainty aware model evaluation and
introspection to safeguard against poisoning attacks. In particular, each
client independently trains a clean local model exclusively using its local
dataset, acting as the reference point for evaluating model updates. SureFED
leverages Bayesian models that provide model uncertainties and play a crucial
role in the model evaluation process. Our framework exhibits robustness even
when the majority of clients are compromised, remains agnostic to the number of
malicious clients, and is well-suited for non-IID settings. We theoretically
prove the robustness of our algorithm against data and model poisoning attacks
in a decentralized linear regression setting. Proof-of Concept evaluations on
benchmark image classification data demonstrate the superiority of SureFED over
the state of the art defense methods under various colluding and non-colluding
data and model poisoning attacks.",2308.02747v2,https://arxiv.org/pdf/2308.02747v2
"Adapting to Change: Robust Counterfactual Explanations in Dynamic Data
  Landscapes","Bardh Prenkaj, Mario Villaizan-Vallelado, Tobias Leemann, Gjergji Kasneci","We introduce a novel semi-supervised Graph Counterfactual Explainer (GCE)
methodology, Dynamic GRAph Counterfactual Explainer (DyGRACE). It leverages
initial knowledge about the data distribution to search for valid
counterfactuals while avoiding using information from potentially outdated
decision functions in subsequent time steps. Employing two graph autoencoders
(GAEs), DyGRACE learns the representation of each class in a binary
classification scenario. The GAEs minimise the reconstruction error between the
original graph and its learned representation during training. The method
involves (i) optimising a parametric density function (implemented as a
logistic regression function) to identify counterfactuals by maximising the
factual autoencoder's reconstruction error, (ii) minimising the counterfactual
autoencoder's error, and (iii) maximising the similarity between the factual
and counterfactual graphs. This semi-supervised approach is independent of an
underlying black-box oracle. A logistic regression model is trained on a set of
graph pairs to learn weights that aid in finding counterfactuals. At inference,
for each unseen graph, the logistic regressor identifies the best
counterfactual candidate using these learned weights, while the GAEs can be
iteratively updated to represent the continual adaptation of the learned graph
representation over iterations. DyGRACE is quite effective and can act as a
drift detector, identifying distributional drift based on differences in
reconstruction errors between iterations. It avoids reliance on the oracle's
predictions in successive iterations, thereby increasing the efficiency of
counterfactual discovery. DyGRACE, with its capacity for contrastive learning
and drift detection, will offer new avenues for semi-supervised learning and
explanation generation.",2308.02353v1,https://arxiv.org/pdf/2308.02353v1
RobustMQ: Benchmarking Robustness of Quantized Models,"Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang Guo, Xianglong Liu","Quantization has emerged as an essential technique for deploying deep neural
networks (DNNs) on devices with limited resources. However, quantized models
exhibit vulnerabilities when exposed to various noises in real-world
applications. Despite the importance of evaluating the impact of quantization
on robustness, existing research on this topic is limited and often disregards
established principles of robustness evaluation, resulting in incomplete and
inconclusive findings. To address this gap, we thoroughly evaluated the
robustness of quantized models against various noises (adversarial attacks,
natural corruptions, and systematic noises) on ImageNet. The comprehensive
evaluation results empirically provide valuable insights into the robustness of
quantized models in various scenarios, for example: (1) quantized models
exhibit higher adversarial robustness than their floating-point counterparts,
but are more vulnerable to natural corruptions and systematic noises; (2) in
general, increasing the quantization bit-width results in a decrease in
adversarial robustness, an increase in natural robustness, and an increase in
systematic robustness; (3) among corruption methods, \textit{impulse noise} and
\textit{glass blur} are the most harmful to quantized models, while
\textit{brightness} has the least impact; (4) among systematic noises, the
\textit{nearest neighbor interpolation} has the highest impact, while bilinear
interpolation, cubic interpolation, and area interpolation are the three least
harmful. Our research contributes to advancing the robust quantization of
models and their deployment in real-world scenarios.",2308.02350v1,https://arxiv.org/pdf/2308.02350v1
"AdvFAS: A robust face anti-spoofing framework against adversarial
  examples","Jiawei Chen, Xiao Yang, Heng Yin, Mingzhi Ma, Bihui Chen, Jianteng Peng, Yandong Guo, Zhaoxia Yin, Hang Su","Ensuring the reliability of face recognition systems against presentation
attacks necessitates the deployment of face anti-spoofing techniques. Despite
considerable advancements in this domain, the ability of even the most
state-of-the-art methods to defend against adversarial examples remains
elusive. While several adversarial defense strategies have been proposed, they
typically suffer from constrained practicability due to inevitable trade-offs
between universality, effectiveness, and efficiency. To overcome these
challenges, we thoroughly delve into the coupled relationship between
adversarial detection and face anti-spoofing. Based on this, we propose a
robust face anti-spoofing framework, namely AdvFAS, that leverages two coupled
scores to accurately distinguish between correctly detected and wrongly
detected face images. Extensive experiments demonstrate the effectiveness of
our framework in a variety of settings, including different attacks, datasets,
and backbones, meanwhile enjoying high accuracy on clean examples. Moreover, we
successfully apply the proposed method to detect real-world adversarial
examples.",2308.02116v1,https://arxiv.org/pdf/2308.02116v1
"Robust Independence Tests with Finite Sample Guarantees for Synchronous
  Stochastic Linear Systems","Ambrus Tamás, Dániel Ágoston Bálint, Balázs Csanád Csáji","The paper introduces robust independence tests with non-asymptotically
guaranteed significance levels for stochastic linear time-invariant systems,
assuming that the observed outputs are synchronous, which means that the
systems are driven by jointly i.i.d. noises. Our method provides bounds for the
type I error probabilities that are distribution-free, i.e., the innovations
can have arbitrary distributions. The algorithm combines confidence region
estimates with permutation tests and general dependence measures, such as the
Hilbert-Schmidt independence criterion and the distance covariance, to detect
any nonlinear dependence between the observed systems. We also prove the
consistency of our hypothesis tests under mild assumptions and demonstrate the
ideas through the example of autoregressive systems.",2308.02054v1,https://arxiv.org/pdf/2308.02054v1
URET: Universal Robustness Evaluation Toolkit (for Evasion),"Kevin Eykholt, Taesung Lee, Douglas Schales, Jiyong Jang, Ian Molloy, Masha Zorin","Machine learning models are known to be vulnerable to adversarial evasion
attacks as illustrated by image classification models. Thoroughly understanding
such attacks is critical in order to ensure the safety and robustness of
critical AI tasks. However, most evasion attacks are difficult to deploy
against a majority of AI systems because they have focused on image domain with
only few constraints. An image is composed of homogeneous, numerical,
continuous, and independent features, unlike many other input types to AI
systems used in practice. Furthermore, some input types include additional
semantic and functional constraints that must be observed to generate realistic
adversarial inputs. In this work, we propose a new framework to enable the
generation of adversarial inputs irrespective of the input type and task
domain. Given an input and a set of pre-defined input transformations, our
framework discovers a sequence of transformations that result in a semantically
correct and functional adversarial input. We demonstrate the generality of our
approach on several diverse machine learning tasks with various input
representations. We also show the importance of generating adversarial examples
as they enable the deployment of mitigation techniques.",2308.01840v1,https://arxiv.org/pdf/2308.01840v1
Hard Adversarial Example Mining for Improving Robust Fairness,"Chenhao Lin, Xiang Ji, Yulong Yang, Qian Li, Chao Shen, Run Wang, Liming Fang","Adversarial training (AT) is widely considered the state-of-the-art technique
for improving the robustness of deep neural networks (DNNs) against adversarial
examples (AE). Nevertheless, recent studies have revealed that adversarially
trained models are prone to unfairness problems, restricting their
applicability. In this paper, we empirically observe that this limitation may
be attributed to serious adversarial confidence overfitting, i.e., certain
adversarial examples with overconfidence. To alleviate this problem, we propose
HAM, a straightforward yet effective framework via adaptive Hard Adversarial
example Mining.HAM concentrates on mining hard adversarial examples while
discarding the easy ones in an adaptive fashion. Specifically, HAM identifies
hard AEs in terms of their step sizes needed to cross the decision boundary
when calculating loss value. Besides, an early-dropping mechanism is
incorporated to discard the easy examples at the initial stages of AE
generation, resulting in efficient AT. Extensive experimental results on
CIFAR-10, SVHN, and Imagenette demonstrate that HAM achieves significant
improvement in robust fairness while reducing computational cost compared to
several state-of-the-art adversarial training methods. The code will be made
publicly available.",2308.01823v1,https://arxiv.org/pdf/2308.01823v1
"Careful Whisper -- leveraging advances in automatic speech recognition
  for robust and interpretable aphasia subtype classification","Laurin Wagner, Mario Zusag, Theresa Bloder","This paper presents a fully automated approach for identifying speech
anomalies from voice recordings to aid in the assessment of speech impairments.
By combining Connectionist Temporal Classification (CTC) and
encoder-decoder-based automatic speech recognition models, we generate rich
acoustic and clean transcripts. We then apply several natural language
processing methods to extract features from these transcripts to produce
prototypes of healthy speech. Basic distance measures from these prototypes
serve as input features for standard machine learning classifiers, yielding
human-level accuracy for the distinction between recordings of people with
aphasia and a healthy control group. Furthermore, the most frequently occurring
aphasia types can be distinguished with 90% accuracy. The pipeline is directly
applicable to other diseases and languages, showing promise for robustly
extracting diagnostic speech biomarkers.",2308.01327v1,https://arxiv.org/pdf/2308.01327v1
"Isolation and Induction: Training Robust Deep Neural Networks against
  Model Stealing Attacks","Jun Guo, Aishan Liu, Xingyu Zheng, Siyuan Liang, Yisong Xiao, Yichao Wu, Xianglong Liu","Despite the broad application of Machine Learning models as a Service
(MLaaS), they are vulnerable to model stealing attacks. These attacks can
replicate the model functionality by using the black-box query process without
any prior knowledge of the target victim model. Existing stealing defenses add
deceptive perturbations to the victim's posterior probabilities to mislead the
attackers. However, these defenses are now suffering problems of high inference
computational overheads and unfavorable trade-offs between benign accuracy and
stealing robustness, which challenges the feasibility of deployed models in
practice. To address the problems, this paper proposes Isolation and Induction
(InI), a novel and effective training framework for model stealing defenses.
Instead of deploying auxiliary defense modules that introduce redundant
inference time, InI directly trains a defensive model by isolating the
adversary's training gradient from the expected gradient, which can effectively
reduce the inference computational cost. In contrast to adding perturbations
over model predictions that harm the benign accuracy, we train models to
produce uninformative outputs against stealing queries, which can induce the
adversary to extract little useful knowledge from victim models with minimal
impact on the benign performance. Extensive experiments on several visual
classification datasets (e.g., MNIST and CIFAR10) demonstrate the superior
robustness (up to 48% reduction on stealing accuracy) and speed (up to 25.4x
faster) of our InI over other state-of-the-art methods. Our codes can be found
in https://github.com/DIG-Beihang/InI-Model-Stealing-Defense.",2308.00958v2,https://arxiv.org/pdf/2308.00958v2
Training on Foveated Images Improves Robustness to Adversarial Attacks,"Muhammad A. Shah, Bhiksha Raj","Deep neural networks (DNNs) have been shown to be vulnerable to adversarial
attacks -- subtle, perceptually indistinguishable perturbations of inputs that
change the response of the model. In the context of vision, we hypothesize that
an important contributor to the robustness of human visual perception is
constant exposure to low-fidelity visual stimuli in our peripheral vision. To
investigate this hypothesis, we develop \RBlur, an image transform that
simulates the loss in fidelity of peripheral vision by blurring the image and
reducing its color saturation based on the distance from a given fixation
point. We show that compared to DNNs trained on the original images, DNNs
trained on images transformed by \RBlur are substantially more robust to
adversarial attacks, as well as other, non-adversarial, corruptions, achieving
up to 25\% higher accuracy on perturbed data.",2308.00854v1,https://arxiv.org/pdf/2308.00854v1
"Robust Linear Regression: Phase-Transitions and Precise Tradeoffs for
  General Norms","Elvis Dohmatob, Meyer Scetbon","In this paper, we investigate the impact of test-time adversarial attacks on
linear regression models and determine the optimal level of robustness that any
model can reach while maintaining a given level of standard predictive
performance (accuracy). Through quantitative estimates, we uncover fundamental
tradeoffs between adversarial robustness and accuracy in different regimes. We
obtain a precise characterization which distinguishes between regimes where
robustness is achievable without hurting standard accuracy and regimes where a
tradeoff might be unavoidable. Our findings are empirically confirmed with
simple experiments that represent a variety of settings. This work applies to
feature covariance matrices and attack norms of any nature, and extends beyond
previous works in this area.",2308.00556v1,https://arxiv.org/pdf/2308.00556v1
"Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious
  Correlations?","Phuong Quynh Le, Jörg Schlötterer, Christin Seifert","Models trained with empirical risk minimization (ERM) are known to learn to
rely on spurious features, i.e., their prediction is based on undesired
auxiliary features which are strongly correlated with class labels but lack
causal reasoning. This behavior particularly degrades accuracy in groups of
samples of the correlated class that are missing the spurious feature or
samples of the opposite class but with the spurious feature present. The
recently proposed Deep Feature Reweighting (DFR) method improves accuracy of
these worst groups. Based on the main argument that ERM mods can learn core
features sufficiently well, DFR only needs to retrain the last layer of the
classification model with a small group-balanced data set. In this work, we
examine the applicability of DFR to realistic data in the medical domain.
Furthermore, we investigate the reasoning behind the effectiveness of
last-layer retraining and show that even though DFR has the potential to
improve the accuracy of the worst group, it remains susceptible to spurious
correlations.",2308.00473v2,https://arxiv.org/pdf/2308.00473v2
"A Majority Invariant Approach to Patch Robustness Certification for Deep
  Learning Models","Qilin Zhou, Zhengyuan Wei, Haipeng Wang, W. K. Chan","Patch robustness certification ensures no patch within a given bound on a
sample can manipulate a deep learning model to predict a different label.
However, existing techniques cannot certify samples that cannot meet their
strict bars at the classifier or patch region levels. This paper proposes
MajorCert. MajorCert firstly finds all possible label sets manipulatable by the
same patch region on the same sample across the underlying classifiers, then
enumerates their combinations element-wise, and finally checks whether the
majority invariant of all these combinations is intact to certify samples.",2308.00452v2,https://arxiv.org/pdf/2308.00452v2
Learning to Generate Training Datasets for Robust Semantic Segmentation,"Marwane Hariat, Olivier Laurent, Rémi Kazmierczak, Shihao Zhang, Andrei Bursuc, Angela Yao, Gianni Franchi","Semantic segmentation methods have advanced significantly. Still, their
robustness to real-world perturbations and object types not seen during
training remains a challenge, particularly in safety-critical applications. We
propose a novel approach to improve the robustness of semantic segmentation
techniques by leveraging the synergy between label-to-image generators and
image-to-label segmentation models. Specifically, we design Robusta, a novel
robust conditional generative adversarial network to generate realistic and
plausible perturbed images that can be used to train reliable segmentation
models. We conduct in-depth studies of the proposed generative model, assess
the performance and robustness of the downstream segmentation network, and
demonstrate that our approach can significantly enhance the robustness in the
face of real-world perturbations, distribution shifts, and out-of-distribution
samples. Our results suggest that this approach could be valuable in
safety-critical applications, where the reliability of perception modules such
as semantic segmentation is of utmost importance and comes with a limited
computational budget in inference. We release our code at
https://github.com/ENSTA-U2IS-AI/robusta.",2308.02535v4,https://arxiv.org/pdf/2308.02535v4
"Improving Generalization of Adversarial Training via Robust Critical
  Fine-Tuning","Kaijie Zhu, Jindong Wang, Xixu Hu, Xing Xie, Ge Yang","Deep neural networks are susceptible to adversarial examples, posing a
significant security risk in critical applications. Adversarial Training (AT)
is a well-established technique to enhance adversarial robustness, but it often
comes at the cost of decreased generalization ability. This paper proposes
Robustness Critical Fine-Tuning (RiFT), a novel approach to enhance
generalization without compromising adversarial robustness. The core idea of
RiFT is to exploit the redundant capacity for robustness by fine-tuning the
adversarially trained model on its non-robust-critical module. To do so, we
introduce module robust criticality (MRC), a measure that evaluates the
significance of a given module to model robustness under worst-case weight
perturbations. Using this measure, we identify the module with the lowest MRC
value as the non-robust-critical module and fine-tune its weights to obtain
fine-tuned weights. Subsequently, we linearly interpolate between the
adversarially trained weights and fine-tuned weights to derive the optimal
fine-tuned model weights. We demonstrate the efficacy of RiFT on ResNet18,
ResNet34, and WideResNet34-10 models trained on CIFAR10, CIFAR100, and
Tiny-ImageNet datasets. Our experiments show that \method can significantly
improve both generalization and out-of-distribution robustness by around 1.5%
while maintaining or even slightly enhancing adversarial robustness. Code is
available at https://github.com/microsoft/robustlearn.",2308.02533v1,https://arxiv.org/pdf/2308.02533v1
"Dynamic ensemble selection based on Deep Neural Network Uncertainty
  Estimation for Adversarial Robustness","Ruoxi Qin, Linyuan Wang, Xuehui Du, Xingyuan Chen, Bin Yan","The deep neural network has attained significant efficiency in image
recognition. However, it has vulnerable recognition robustness under extensive
data uncertainty in practical applications. The uncertainty is attributed to
the inevitable ambient noise and, more importantly, the possible adversarial
attack. Dynamic methods can effectively improve the defense initiative in the
arms race of attack and defense of adversarial examples. Different from the
previous dynamic method depend on input or decision, this work explore the
dynamic attributes in model level through dynamic ensemble selection technology
to further protect the model from white-box attacks and improve the robustness.
Specifically, in training phase the Dirichlet distribution is apply as prior of
sub-models' predictive distribution, and the diversity constraint in parameter
space is introduced under the lightweight sub-models to construct alternative
ensembel model spaces. In test phase, the certain sub-models are dynamically
selected based on their rank of uncertainty value for the final prediction to
ensure the majority accurate principle in ensemble robustness and accuracy.
Compared with the previous dynamic method and staic adversarial traning model,
the presented approach can achieve significant robustness results without
damaging accuracy by combining dynamics and diversity property.",2308.00346v1,https://arxiv.org/pdf/2308.00346v1
Doubly Robust Instance-Reweighted Adversarial Training,"Daouda Sow, Sen Lin, Zhangyang Wang, Yingbin Liang","Assigning importance weights to adversarial data has achieved great success
in training adversarially robust networks under limited model capacity.
However, existing instance-reweighted adversarial training (AT) methods heavily
depend on heuristics and/or geometric interpretations to determine those
importance weights, making these algorithms lack rigorous theoretical
justification/guarantee. Moreover, recent research has shown that adversarial
training suffers from a severe non-uniform robust performance across the
training distribution, e.g., data points belonging to some classes can be much
more vulnerable to adversarial attacks than others. To address both issues, in
this paper, we propose a novel doubly-robust instance reweighted AT framework,
which allows to obtain the importance weights via exploring distributionally
robust optimization (DRO) techniques, and at the same time boosts the
robustness on the most vulnerable examples. In particular, our importance
weights are obtained by optimizing the KL-divergence regularized loss function,
which allows us to devise new algorithms with a theoretical convergence
guarantee. Experiments on standard classification datasets demonstrate that our
proposed approach outperforms related state-of-the-art baseline methods in
terms of average robust performance, and at the same time improves the
robustness against attacks on the weakest data points. Codes will be available
soon.",2308.00311v1,https://arxiv.org/pdf/2308.00311v1
"Robust Positive-Unlabeled Learning via Noise Negative Sample
  Self-correction","Zhangchi Zhu, Lu Wang, Pu Zhao, Chao Du, Wei Zhang, Hang Dong, Bo Qiao, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang","Learning from positive and unlabeled data is known as positive-unlabeled (PU)
learning in literature and has attracted much attention in recent years. One
common approach in PU learning is to sample a set of pseudo-negatives from the
unlabeled data using ad-hoc thresholds so that conventional supervised methods
can be applied with both positive and negative samples. Owing to the label
uncertainty among the unlabeled data, errors of misclassifying unlabeled
positive samples as negative samples inevitably appear and may even accumulate
during the training processes. Those errors often lead to performance
degradation and model instability. To mitigate the impact of label uncertainty
and improve the robustness of learning with positive and unlabeled data, we
propose a new robust PU learning method with a training strategy motivated by
the nature of human learning: easy cases should be learned first. Similar
intuition has been utilized in curriculum learning to only use easier cases in
the early stage of training before introducing more complex cases.
Specifically, we utilize a novel ``hardness'' measure to distinguish unlabeled
samples with a high chance of being negative from unlabeled samples with large
label noise. An iterative training strategy is then implemented to fine-tune
the selection of negative samples during the training process in an iterative
manner to include more ``easy'' samples in the early stage of training.
Extensive experimental validations over a wide range of learning tasks show
that this approach can effectively improve the accuracy and stability of
learning with positive and unlabeled data. Our code is available at
https://github.com/woriazzc/Robust-PU",2308.00279v1,https://arxiv.org/pdf/2308.00279v1
Adversarially Robust Neural Legal Judgement Systems,"Rohit Raj, V Susheela Devi","Legal judgment prediction is the task of predicting the outcome of court
cases on a given text description of facts of cases. These tasks apply Natural
Language Processing (NLP) techniques to predict legal judgment results based on
facts. Recently, large-scale public datasets and NLP models have increased
research in areas related to legal judgment prediction systems. For such
systems to be practically helpful, they should be robust from adversarial
attacks. Previous works mainly focus on making a neural legal judgement system;
however, significantly less or no attention has been given to creating a robust
Legal Judgement Prediction(LJP) system. We implemented adversarial attacks on
early existing LJP systems and found that none of them could handle attacks. In
this work, we proposed an approach for making robust LJP systems. Extensive
experiments on three legal datasets show significant improvements in our
approach over the state-of-the-art LJP system in handling adversarial attacks.
To the best of our knowledge, we are the first to increase the robustness of
early-existing LJP systems.",2308.00165v1,https://arxiv.org/pdf/2308.00165v1
"Text-CRS: A Generalized Certified Robustness Framework against Textual
  Adversarial Attacks","Xinyu Zhang, Hanbin Hong, Yuan Hong, Peng Huang, Binghui Wang, Zhongjie Ba, Kui Ren","The language models, especially the basic text classification models, have
been shown to be susceptible to textual adversarial attacks such as synonym
substitution and word insertion attacks. To defend against such attacks, a
growing body of research has been devoted to improving the model robustness.
However, providing provable robustness guarantees instead of empirical
robustness is still widely unexplored. In this paper, we propose Text-CRS, a
generalized certified robustness framework for natural language processing
(NLP) based on randomized smoothing. To our best knowledge, existing certified
schemes for NLP can only certify the robustness against $\ell_0$ perturbations
in synonym substitution attacks. Representing each word-level adversarial
operation (i.e., synonym substitution, word reordering, insertion, and
deletion) as a combination of permutation and embedding transformation, we
propose novel smoothing theorems to derive robustness bounds in both
permutation and embedding space against such adversarial operations. To further
improve certified accuracy and radius, we consider the numerical relationships
between discrete words and select proper noise distributions for the randomized
smoothing. Finally, we conduct substantial experiments on multiple language
models and datasets. Text-CRS can address all four different word-level
adversarial operations and achieve a significant accuracy improvement. We also
provide the first benchmark on certified accuracy and radius of four word-level
operations, besides outperforming the state-of-the-art certification against
synonym substitution attacks.",2307.16630v2,https://arxiv.org/pdf/2307.16630v2
"Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks
  for Defending Adversarial Examples","Qiufan Ji, Lin Wang, Cong Shi, Shengshan Hu, Yingying Chen, Lichao Sun","Deep Neural Networks (DNNs) for 3D point cloud recognition are vulnerable to
adversarial examples, threatening their practical deployment. Despite the many
research endeavors have been made to tackle this issue in recent years, the
diversity of adversarial examples on 3D point clouds makes them more
challenging to defend against than those on 2D images. For examples, attackers
can generate adversarial examples by adding, shifting, or removing points.
Consequently, existing defense strategies are hard to counter unseen point
cloud adversarial examples. In this paper, we first establish a comprehensive,
and rigorous point cloud adversarial robustness benchmark to evaluate
adversarial robustness, which can provide a detailed understanding of the
effects of the defense and attack methods. We then collect existing defense
tricks in point cloud adversarial defenses and then perform extensive and
systematic experiments to identify an effective combination of these tricks.
Furthermore, we propose a hybrid training augmentation methods that consider
various types of point cloud adversarial examples to adversarial training,
significantly improving the adversarial robustness. By combining these tricks,
we construct a more robust defense framework achieving an average accuracy of
83.45\% against various attacks, demonstrating its capability to enabling
robust learners. Our codebase are open-sourced on:
\url{https://github.com/qiufan319/benchmark_pc_attack.git}.",2307.16361v2,https://arxiv.org/pdf/2307.16361v2
Probabilistically robust conformal prediction,"Subhankar Ghosh, Yuanjie Shi, Taha Belkhouja, Yan Yan, Jana Doppa, Brian Jones","Conformal prediction (CP) is a framework to quantify uncertainty of machine
learning classifiers including deep neural networks. Given a testing example
and a trained classifier, CP produces a prediction set of candidate labels with
a user-specified coverage (i.e., true class label is contained with high
probability). Almost all the existing work on CP assumes clean testing data and
there is not much known about the robustness of CP algorithms w.r.t
natural/adversarial perturbations to testing examples. This paper studies the
problem of probabilistically robust conformal prediction (PRCP) which ensures
robustness to most perturbations around clean input examples. PRCP generalizes
the standard CP (cannot handle perturbations) and adversarially robust CP
(ensures robustness w.r.t worst-case perturbations) to achieve better
trade-offs between nominal performance and robustness. We propose a novel
adaptive PRCP (aPRCP) algorithm to achieve probabilistically robust coverage.
The key idea behind aPRCP is to determine two parallel thresholds, one for data
samples and another one for the perturbations on data (aka
""quantile-of-quantile"" design). We provide theoretical analysis to show that
aPRCP algorithm achieves robust coverage. Our experiments on CIFAR-10,
CIFAR-100, and ImageNet datasets using deep neural networks demonstrate that
aPRCP achieves better trade-offs than state-of-the-art CP and adversarially
robust CP algorithms.",2307.16360v1,https://arxiv.org/pdf/2307.16360v1
"Distributionally Robust Safety Filter for Learning-Based Control in
  Active Distribution Systems","Hoang Tien Nguyen, Dae-Hyun Choi","Operational constraint violations may occur when deep reinforcement learning
(DRL) agents interact with real-world active distribution systems to learn
their optimal policies during training. This letter presents a universal
distributionally robust safety filter (DRSF) using which any DRL agent can
reduce the constraint violations of distribution systems significantly during
training while maintaining near-optimal solutions. The DRSF is formulated as a
distributionally robust optimization problem with chance constraints of
operational limits. This problem aims to compute near-optimal actions that are
minimally modified from the optimal actions of DRL-based Volt/VAr control by
leveraging the distribution system model, thereby providing constraint
satisfaction guarantee with a probability level under the model uncertainty.
The performance of the proposed DRSF is verified using the IEEE 33-bus and
123-bus systems.",2307.16351v1,https://arxiv.org/pdf/2307.16351v1
Towards Practical Robustness Auditing for Linear Regression,"Daniel Freund, Samuel B. Hopkins","We investigate practical algorithms to find or disprove the existence of
small subsets of a dataset which, when removed, reverse the sign of a
coefficient in an ordinary least squares regression involving that dataset. We
empirically study the performance of well-established algorithmic techniques
for this task -- mixed integer quadratically constrained optimization for
general linear regression problems and exact greedy methods for special cases.
We show that these methods largely outperform the state of the art and provide
a useful robustness check for regression problems in a few dimensions. However,
significant computational bottlenecks remain, especially for the important task
of disproving the existence of such small sets of influential samples for
regression problems of dimension $3$ or greater. We make some headway on this
challenge via a spectral algorithm using ideas drawn from recent innovations in
algorithmic robust statistics. We summarize the limitations of known techniques
in several challenge datasets to encourage further algorithmic innovation.",2307.16315v1,https://arxiv.org/pdf/2307.16315v1
"Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand
  System: A Multi-Agent Reinforcement Learning Approach","Sihong He, Shuo Han, Fei Miao","Electric autonomous vehicles (EAVs) are getting attention in future
autonomous mobility-on-demand (AMoD) systems due to their economic and societal
benefits. However, EAVs' unique charging patterns (long charging time, high
charging frequency, unpredictable charging behaviors, etc.) make it challenging
to accurately predict the EAVs supply in E-AMoD systems. Furthermore, the
mobility demand's prediction uncertainty makes it an urgent and challenging
task to design an integrated vehicle balancing solution under supply and demand
uncertainties. Despite the success of reinforcement learning-based E-AMoD
balancing algorithms, state uncertainties under the EV supply or mobility
demand remain unexplored. In this work, we design a multi-agent reinforcement
learning (MARL)-based framework for EAVs balancing in E-AMoD systems, with
adversarial agents to model both the EAVs supply and mobility demand
uncertainties that may undermine the vehicle balancing solutions. We then
propose a robust E-AMoD Balancing MARL (REBAMA) algorithm to train a robust
EAVs balancing policy to balance both the supply-demand ratio and charging
utilization rate across the whole city. Experiments show that our proposed
robust method performs better compared with a non-robust MARL method that does
not consider state uncertainties; it improves the reward, charging utilization
fairness, and supply-demand fairness by 19.28%, 28.18%, and 3.97%,
respectively. Compared with a robust optimization-based method, the proposed
MARL algorithm can improve the reward, charging utilization fairness, and
supply-demand fairness by 8.21%, 8.29%, and 9.42%, respectively.",2307.16228v1,https://arxiv.org/pdf/2307.16228v1
Robust Multi-Agent Reinforcement Learning with State Uncertainty,"Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, Fei Miao","In real-world multi-agent reinforcement learning (MARL) applications, agents
may not have perfect state information (e.g., due to inaccurate measurement or
malicious attacks), which challenges the robustness of agents' policies. Though
robustness is getting important in MARL deployment, little prior work has
studied state uncertainties in MARL, neither in problem formulation nor
algorithm design. Motivated by this robustness issue and the lack of
corresponding studies, we study the problem of MARL with state uncertainty in
this work. We provide the first attempt to the theoretical and empirical
analysis of this challenging problem. We first model the problem as a Markov
Game with state perturbation adversaries (MG-SPA) by introducing a set of state
perturbation adversaries into a Markov Game. We then introduce robust
equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental
analysis regarding MG-SPA such as giving conditions under which such a robust
equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ)
algorithm to find such an equilibrium, with convergence guarantees. To handle
high-dimensional state-action space, we design a robust multi-agent
actor-critic (RMAAC) algorithm based on an analytical expression of the policy
gradient derived in the paper. Our experiments show that the proposed RMAQ
algorithm converges to the optimal value function; our RMAAC algorithm
outperforms several MARL and robust MARL methods in multiple multi-agent
environments when state uncertainty is present. The source code is public on
\url{https://github.com/sihongho/robust_marl_with_state_uncertainty}.",2307.16212v1,https://arxiv.org/pdf/2307.16212v1
"Uncertainty-Encoded Multi-Modal Fusion for Robust Object Detection in
  Autonomous Driving","Yang Lou, Qun Song, Qian Xu, Rui Tan, Jianping Wang","Multi-modal fusion has shown initial promising results for object detection
of autonomous driving perception. However, many existing fusion schemes do not
consider the quality of each fusion input and may suffer from adverse
conditions on one or more sensors. While predictive uncertainty has been
applied to characterize single-modal object detection performance at run time,
incorporating uncertainties into the multi-modal fusion still lacks effective
solutions due primarily to the uncertainty's cross-modal incomparability and
distinct sensitivities to various adverse conditions. To fill this gap, this
paper proposes Uncertainty-Encoded Mixture-of-Experts (UMoE) that explicitly
incorporates single-modal uncertainties into LiDAR-camera fusion. UMoE uses
individual expert network to process each sensor's detection result together
with encoded uncertainty. Then, the expert networks' outputs are analyzed by a
gating network to determine the fusion weights. The proposed UMoE module can be
integrated into any proposal fusion pipeline. Evaluation shows that UMoE
achieves a maximum of 10.67%, 3.17%, and 5.40% performance gain compared with
the state-of-the-art proposal-level multi-modal object detectors under extreme
weather, adversarial, and blinding attack scenarios.",2307.16121v1,https://arxiv.org/pdf/2307.16121v1
"Evaluating the Robustness of Test Selection Methods for Deep Neural
  Networks","Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Wei Ma, Mike Papadakis, Yves Le Traon","Testing deep learning-based systems is crucial but challenging due to the
required time and labor for labeling collected raw data. To alleviate the
labeling effort, multiple test selection methods have been proposed where only
a subset of test data needs to be labeled while satisfying testing
requirements. However, we observe that such methods with reported promising
results are only evaluated under simple scenarios, e.g., testing on original
test data. This brings a question to us: are they always reliable? In this
paper, we explore when and to what extent test selection methods fail for
testing. Specifically, first, we identify potential pitfalls of 11 selection
methods from top-tier venues based on their construction. Second, we conduct a
study on five datasets with two model architectures per dataset to empirically
confirm the existence of these pitfalls. Furthermore, we demonstrate how
pitfalls can break the reliability of these methods. Concretely, methods for
fault detection suffer from test data that are: 1) correctly classified but
uncertain, or 2) misclassified but confident. Remarkably, the test relative
coverage achieved by such methods drops by up to 86.85%. On the other hand,
methods for performance estimation are sensitive to the choice of
intermediate-layer output. The effectiveness of such methods can be even worse
than random selection when using an inappropriate layer.",2308.01314v1,https://arxiv.org/pdf/2308.01314v1
"UniBriVL: Robust Universal Representation and Generation of Audio Driven
  Diffusion Models","Sen Fang, Bowen Gao, Yangjian Wu, Teik Toe Teoh","Multimodal large models have been recognized for their advantages in various
performance and downstream tasks. The development of these models is crucial
towards achieving general artificial intelligence in the future. In this paper,
we propose a novel universal language representation learning method called
UniBriVL, which is based on Bridging-Vision-and-Language (BriVL). Universal
BriVL embeds audio, image, and text into a shared space, enabling the
realization of various multimodal applications. Our approach addresses major
challenges in robust language (both text and audio) representation learning and
effectively captures the correlation between audio and image. Additionally, we
demonstrate the qualitative evaluation of the generated images from UniBriVL,
which serves to highlight the potential of our approach in creating images from
audio. Overall, our experimental results demonstrate the efficacy of UniBriVL
in downstream tasks and its ability to choose appropriate images from audio.
The proposed approach has the potential for various applications such as speech
recognition, music signal processing, and captioning systems.",2307.15898v2,https://arxiv.org/pdf/2307.15898v2
First-order Policy Optimization for Robust Policy Evaluation,"Yan Li, Guanghui Lan","We adopt a policy optimization viewpoint towards policy evaluation for robust
Markov decision process with $\mathrm{s}$-rectangular ambiguity sets. The
developed method, named first-order policy evaluation (FRPE), provides the
first unified framework for robust policy evaluation in both deterministic
(offline) and stochastic (online) settings, with either tabular representation
or generic function approximation. In particular, we establish linear
convergence in the deterministic setting, and
$\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity in the stochastic
setting. FRPE also extends naturally to evaluating the robust state-action
value function with $(\mathrm{s}, \mathrm{a})$-rectangular ambiguity sets. We
discuss the application of the developed results for stochastic policy
optimization of large-scale robust MDPs.",2307.15890v1,https://arxiv.org/pdf/2307.15890v1
Robust Distortion-free Watermarks for Language Models,"Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, Percy Liang","We propose a methodology for planting watermarks in text from an
autoregressive language model that are robust to perturbations without changing
the distribution over text up to a certain maximum generation budget. We
generate watermarked text by mapping a sequence of random numbers -- which we
compute using a randomized watermark key -- to a sample from the language
model. To detect watermarked text, any party who knows the key can align the
text to the random number sequence. We instantiate our watermark methodology
with two sampling schemes: inverse transform sampling and exponential minimum
sampling. We apply these watermarks to three language models -- OPT-1.3B,
LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power
and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B
and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq
0.01$) from $35$ tokens even after corrupting between $40$-$50\%$ of the tokens
via random edits (i.e., substitutions, insertions or deletions). For the
Alpaca-7B model, we conduct a case study on the feasibility of watermarking
responses to typical user instructions. Due to the lower entropy of the
responses, detection is more difficult: around $25\%$ of the responses -- whose
median length is around $100$ tokens -- are detectable with $p \leq 0.01$, and
the watermark is also less robust to certain automated paraphrasing attacks we
implement.",2307.15593v3,https://arxiv.org/pdf/2307.15593v3
"From continuous-time formulations to discretization schemes: tensor
  trains and robust regression for BSDEs and parabolic PDEs","Lorenz Richter, Leon Sallandt, Nikolas Nüsken","The numerical approximation of partial differential equations (PDEs) poses
formidable challenges in high dimensions since classical grid-based methods
suffer from the so-called curse of dimensionality. Recent attempts rely on a
combination of Monte Carlo methods and variational formulations, using neural
networks for function approximation. Extending previous work (Richter et al.,
2021), we argue that tensor trains provide an appealing framework for parabolic
PDEs: The combination of reformulations in terms of backward stochastic
differential equations and regression-type methods holds the promise of
leveraging latent low-rank structures, enabling both compression and efficient
computation. Emphasizing a continuous-time viewpoint, we develop iterative
schemes, which differ in terms of computational efficiency and robustness. We
demonstrate both theoretically and numerically that our methods can achieve a
favorable trade-off between accuracy and computational efficiency. While
previous methods have been either accurate or fast, we have identified a novel
numerical strategy that can often combine both of these aspects.",2307.15496v1,https://arxiv.org/pdf/2307.15496v1
Robust Visual Sim-to-Real Transfer for Robotic Manipulation,"Ricardo Garcia, Robin Strudel, Shizhe Chen, Etienne Arlaud, Ivan Laptev, Cordelia Schmid","Learning visuomotor policies in simulation is much safer and cheaper than in
the real world. However, due to discrepancies between the simulated and real
data, simulator-trained policies often fail when transferred to real robots.
One common approach to bridge the visual sim-to-real domain gap is domain
randomization (DR). While previous work mainly evaluates DR for disembodied
tasks, such as pose estimation and object detection, here we systematically
explore visual domain randomization methods and benchmark them on a rich set of
challenging robotic manipulation tasks. In particular, we propose an off-line
proxy task of cube localization to select DR parameters for texture
randomization, lighting randomization, variations of object colors and camera
parameters. Notably, we demonstrate that DR parameters have similar impact on
our off-line proxy task and on-line policies. We, hence, use off-line optimized
DR parameters to train visuomotor policies in simulation and directly apply
such policies to a real robot. Our approach achieves 93% success rate on
average when tested on a diverse set of challenging manipulation tasks.
Moreover, we evaluate the robustness of policies to visual variations in real
scenes and show that our simulator-trained policies outperform policies learned
using real but limited data. Code, simulation environment, real robot datasets
and trained models are available at
https://www.di.ens.fr/willow/research/robust_s2r/.",2307.15320v1,https://arxiv.org/pdf/2307.15320v1
R-LPIPS: An Adversarially Robust Perceptual Similarity Metric,"Sara Ghazanfari, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, Alexandre Araujo","Similarity metrics have played a significant role in computer vision to
capture the underlying semantics of images. In recent years, advanced
similarity metrics, such as the Learned Perceptual Image Patch Similarity
(LPIPS), have emerged. These metrics leverage deep features extracted from
trained neural networks and have demonstrated a remarkable ability to closely
align with human perception when evaluating relative image similarity. However,
it is now well-known that neural networks are susceptible to adversarial
examples, i.e., small perturbations invisible to humans crafted to deliberately
mislead the model. Consequently, the LPIPS metric is also sensitive to such
adversarial examples. This susceptibility introduces significant security
concerns, especially considering the widespread adoption of LPIPS in
large-scale applications. In this paper, we propose the Robust Learned
Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages
adversarially trained deep features. Through a comprehensive set of
experiments, we demonstrate the superiority of R-LPIPS compared to the
classical LPIPS metric. The code is available at
https://github.com/SaraGhazanfari/R-LPIPS.",2307.15157v2,https://arxiv.org/pdf/2307.15157v2
"A/B Testing and Best-arm Identification for Linear Bandits with
  Robustness to Non-stationarity","Zhihan Xiong, Romain Camilleri, Maryam Fazel, Lalit Jain, Kevin Jamieson","We investigate the fixed-budget best-arm identification (BAI) problem for
linear bandits in a potentially non-stationary environment. Given a finite arm
set $\mathcal{X}\subset\mathbb{R}^d$, a fixed budget $T$, and an unpredictable
sequence of parameters $\left\lbrace\theta_t\right\rbrace_{t=1}^{T}$, an
algorithm will aim to correctly identify the best arm $x^* :=
\arg\max_{x\in\mathcal{X}}x^\top\sum_{t=1}^{T}\theta_t$ with probability as
high as possible. Prior work has addressed the stationary setting where
$\theta_t = \theta_1$ for all $t$ and demonstrated that the error probability
decreases as $\exp(-T /\rho^*)$ for a problem-dependent constant $\rho^*$. But
in many real-world $A/B/n$ multivariate testing scenarios that motivate our
work, the environment is non-stationary and an algorithm expecting a stationary
setting can easily fail. For robust identification, it is well-known that if
arms are chosen randomly and non-adaptively from a G-optimal design over
$\mathcal{X}$ at each time then the error probability decreases as
$\exp(-T\Delta^2_{(1)}/d)$, where $\Delta_{(1)} = \min_{x \neq x^*} (x^* -
x)^\top \frac{1}{T}\sum_{t=1}^T \theta_t$. As there exist environments where
$\Delta_{(1)}^2/ d \ll 1/ \rho^*$, we are motivated to propose a novel
algorithm $\mathsf{P1}$-$\mathsf{RAGE}$ that aims to obtain the best of both
worlds: robustness to non-stationarity and fast rates of identification in
benign settings. We characterize the error probability of
$\mathsf{P1}$-$\mathsf{RAGE}$ and demonstrate empirically that the algorithm
indeed never performs worse than G-optimal design but compares favorably to the
best algorithms in the stationary setting.",2307.15154v2,https://arxiv.org/pdf/2307.15154v2
Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners,"Jihyeon Lee, Dain Kim, Doohae Jung, Boseop Kim, Kyoung-Woon On","In-context learning, which offers substantial advantages over fine-tuning, is
predominantly observed in decoder-only models, while encoder-decoder (i.e.,
seq2seq) models excel in methods that rely on weight updates. Recently, a few
studies have demonstrated the feasibility of few-shot learning with seq2seq
models; however, this has been limited to tasks that align well with the
seq2seq architecture, such as summarization and translation. Inspired by these
initial studies, we provide a first-ever extensive experiment comparing the
in-context few-shot learning capabilities of decoder-only and encoder-decoder
models on a broad range of tasks. Furthermore, we propose two methods to more
effectively elicit in-context learning ability in seq2seq models:
objective-aligned prompting and a fusion-based approach. Remarkably, our
approach outperforms a decoder-only model that is six times larger and exhibits
significant performance improvements compared to conventional seq2seq models
across a variety of settings. We posit that, with the right configuration and
prompt design, seq2seq models can be highly effective few-shot learners for a
wide spectrum of applications.",2307.14856v1,https://arxiv.org/pdf/2307.14856v1
"Robust vertebra identification using simultaneous node and edge
  predicting Graph Neural Networks","Vincent Bürgin, Raphael Prevost, Marijn F. Stollenga","Automatic vertebra localization and identification in CT scans is important
for numerous clinical applications. Much progress has been made on this topic,
but it mostly targets positional localization of vertebrae, ignoring their
orientation. Additionally, most methods employ heuristics in their pipeline
that can be sensitive in real clinical images which tend to contain
abnormalities. We introduce a simple pipeline that employs a standard
prediction with a U-Net, followed by a single graph neural network to associate
and classify vertebrae with full orientation. To test our method, we introduce
a new vertebra dataset that also contains pedicle detections that are
associated with vertebra bodies, creating a more challenging landmark
prediction, association and classification task. Our method is able to
accurately associate the correct body and pedicle landmarks, ignore false
positives and classify vertebrae in a simple, fully trainable pipeline avoiding
application-specific heuristics. We show our method outperforms traditional
approaches such as Hungarian Matching and Hidden Markov Models. We also show
competitive performance on the standard VerSe challenge body identification
task.",2308.02509v1,https://arxiv.org/pdf/2308.02509v1
"Attention for Robot Touch: Tactile Saliency Prediction for Robust
  Sim-to-Real Tactile Control","Yijiong Lin, Mauro Comi, Alex Church, Dandan Zhang, Nathan F. Lepora","High-resolution tactile sensing can provide accurate information about local
contact in contact-rich robotic tasks. However, the deployment of such tasks in
unstructured environments remains under-investigated. To improve the robustness
of tactile robot control in unstructured environments, we propose and study a
new concept: \textit{tactile saliency} for robot touch, inspired by the human
touch attention mechanism from neuroscience and the visual saliency prediction
problem from computer vision. In analogy to visual saliency, this concept
involves identifying key information in tactile images captured by a tactile
sensor. While visual saliency datasets are commonly annotated by humans,
manually labelling tactile images is challenging due to their counterintuitive
patterns. To address this challenge, we propose a novel approach comprised of
three interrelated networks: 1) a Contact Depth Network (ConDepNet), which
generates a contact depth map to localize deformation in a real tactile image
that contains target and noise features; 2) a Tactile Saliency Network
(TacSalNet), which predicts a tactile saliency map to describe the target areas
for an input contact depth map; 3) and a Tactile Noise Generator (TacNGen),
which generates noise features to train the TacSalNet. Experimental results in
contact pose estimation and edge-following in the presence of distractors
showcase the accurate prediction of target features from real tactile images.
Overall, our tactile saliency prediction approach gives robust sim-to-real
tactile control in environments with unknown distractors. Project page:
https://sites.google.com/view/tactile-saliency/.",2307.14510v2,https://arxiv.org/pdf/2307.14510v2
"Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo
  Matching","Junpeng Jing, Jiankun Li, Pengfei Xiong, Jiangyu Liu, Shuaicheng Liu, Yichen Guo, Xin Deng, Mai Xu, Lai Jiang, Leonid Sigal","Correlation based stereo matching has achieved outstanding performance, which
pursues cost volume between two feature maps. Unfortunately, current methods
with a fixed model do not work uniformly well across various datasets, greatly
limiting their real-world applicability. To tackle this issue, this paper
proposes a new perspective to dynamically calculate correlation for robust
stereo matching. A novel Uncertainty Guided Adaptive Correlation (UGAC) module
is introduced to robustly adapt the same model for different scenarios.
Specifically, a variance-based uncertainty estimation is employed to adaptively
adjust the sampling area during warping operation. Additionally, we improve the
traditional non-parametric warping with learnable parameters, such that the
position-specific weights can be learned. We show that by empowering the
recurrent network with the UGAC module, stereo matching can be exploited more
robustly and effectively. Extensive experiments demonstrate that our method
achieves state-of-the-art performance over the ETH3D, KITTI, and Middlebury
datasets when employing the same fixed model over these datasets without any
retraining procedure. To target real-time applications, we further design a
lightweight model based on UGAC, which also outperforms other methods over
KITTI benchmarks with only 0.6 M parameters.",2307.14071v1,https://arxiv.org/pdf/2307.14071v1
"Topology-aware Robust Optimization for Out-of-distribution
  Generalization","Fengchun Qiao, Xi Peng","Out-of-distribution (OOD) generalization is a challenging machine learning
problem yet highly desirable in many high-stake applications. Existing methods
suffer from overly pessimistic modeling with low generalization confidence. As
generalizing to arbitrary test distributions is impossible, we hypothesize that
further structure on the topology of distributions is crucial in developing
strong OOD resilience. To this end, we propose topology-aware robust
optimization (TRO) that seamlessly integrates distributional topology in a
principled optimization framework. More specifically, TRO solves two
optimization objectives: (1) Topology Learning which explores data manifold to
uncover the distributional topology; (2) Learning on Topology which exploits
the topology to constrain robust optimization for tightly-bounded
generalization risks. We theoretically demonstrate the effectiveness of our
approach and empirically show that it significantly outperforms the state of
the arts in a wide range of tasks including classification, regression, and
semantic segmentation. Moreover, we empirically find the data-driven
distributional topology is consistent with domain knowledge, enhancing the
explainability of our approach.",2307.13943v1,https://arxiv.org/pdf/2307.13943v1
"Robustness Verification of Deep Neural Networks using Star-Based
  Reachability Analysis with Variable-Length Time Series Input","Neelanjana Pal, Diego Manzanas Lopez, Taylor T Johnson","Data-driven, neural network (NN) based anomaly detection and predictive
maintenance are emerging research areas. NN-based analytics of time-series data
offer valuable insights into past behaviors and estimates of critical
parameters like remaining useful life (RUL) of equipment and state-of-charge
(SOC) of batteries. However, input time series data can be exposed to
intentional or unintentional noise when passing through sensors, necessitating
robust validation and verification of these NNs. This paper presents a case
study of the robustness verification approach for time series regression NNs
(TSRegNN) using set-based formal methods. It focuses on utilizing
variable-length input data to streamline input manipulation and enhance network
architecture generalizability. The method is applied to two data sets in the
Prognostics and Health Management (PHM) application areas: (1) SOC estimation
of a Lithium-ion battery and (2) RUL estimation of a turbine engine. The NNs'
robustness is checked using star-based reachability analysis, and several
performance measures evaluate the effect of bounded perturbations in the input
on network outputs, i.e., future outcomes. Overall, the paper offers a
comprehensive case study for validating and verifying NN-based analytics of
time-series data in real-world applications, emphasizing the importance of
robustness testing for accurate and reliable predictions, especially
considering the impact of noise on future outcomes.",2307.13907v1,https://arxiv.org/pdf/2307.13907v1
Corruption-Robust Lipschitz Contextual Search,Shiliang Zuo,"I study the problem of learning a Lipschitz function with corrupted binary
signals. The learner tries to learn a $L$-Lipschitz function $f: [0,1]^d
\rightarrow [0, L]$ that the adversary chooses. There is a total of $T$ rounds.
In each round $t$, the adversary selects a context vector $x_t$ in the input
space, and the learner makes a guess to the true function value $f(x_t)$ and
receives a binary signal indicating whether the guess is high or low. In a
total of $C$ rounds, the signal may be corrupted, though the value of $C$ is
\emph{unknown} to the learner. The learner's goal is to incur a small
cumulative loss. This work introduces the new algorithmic technique
\emph{agnostic checking} as well as new analysis techniques. I design
algorithms which: for the symmetric loss, the learner achieves regret $L\cdot
O(C\log T)$ with $d = 1$ and $L\cdot O_d(C\log T + T^{(d-1)/d})$ with $d > 1$;
for the pricing loss, the learner achieves regret $L\cdot \widetilde{O}
(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.",2307.13903v4,https://arxiv.org/pdf/2307.13903v4
Characterizing Data Point Vulnerability via Average-Case Robustness,"Tessa Han, Suraj Srinivas, Himabindu Lakkaraju","Studying the robustness of machine learning models is important to ensure
consistent model behaviour across real-world settings. To this end, adversarial
robustness is a standard framework, which views robustness of predictions
through a binary lens: either a worst-case adversarial misclassification exists
in the local region around an input, or it does not. However, this binary
perspective does not account for the degrees of vulnerability, as data points
with a larger number of misclassified examples in their neighborhoods are more
vulnerable. In this work, we consider a complementary framework for robustness,
called average-case robustness, which measures the fraction of points in a
local region that provides consistent predictions. However, computing this
quantity is hard, as standard Monte Carlo approaches are inefficient especially
for high-dimensional inputs. In this work, we propose the first analytical
estimators for average-case robustness for multi-class classifiers. We show
empirically that our estimators are accurate and efficient for standard deep
learning models and demonstrate their usefulness for identifying vulnerable
data points, as well as quantifying robustness bias of models. Overall, our
tools provide a complementary view to robustness, improving our ability to
characterize model behaviour.",2307.13885v6,https://arxiv.org/pdf/2307.13885v6
"Source Condition Double Robust Inference on Functionals of Inverse
  Problems","Andrew Bennett, Nathan Kallus, Xiaojie Mao, Whitney Newey, Vasilis Syrgkanis, Masatoshi Uehara","We consider estimation of parameters defined as linear functionals of
solutions to linear inverse problems. Any such parameter admits a doubly robust
representation that depends on the solution to a dual linear inverse problem,
where the dual solution can be thought as a generalization of the inverse
propensity function. We provide the first source condition double robust
inference method that ensures asymptotic normality around the parameter of
interest as long as either the primal or the dual inverse problem is
sufficiently well-posed, without knowledge of which inverse problem is the more
well-posed one. Our result is enabled by novel guarantees for iterated Tikhonov
regularized adversarial estimators for linear inverse problems, over general
hypothesis spaces, which are developments of independent interest.",2307.13793v1,https://arxiv.org/pdf/2307.13793v1
"Robust Assignment of Labels for Active Learning with Sparse and Noisy
  Annotations","Daniel Kałuża, Andrzej Janusz, Dominik Ślęzak","Supervised classification algorithms are used to solve a growing number of
real-life problems around the globe. Their performance is strictly connected
with the quality of labels used in training. Unfortunately, acquiring
good-quality annotations for many tasks is infeasible or too expensive to be
done in practice. To tackle this challenge, active learning algorithms are
commonly employed to select only the most relevant data for labeling. However,
this is possible only when the quality and quantity of labels acquired from
experts are sufficient. Unfortunately, in many applications, a trade-off
between annotating individual samples by multiple annotators to increase label
quality vs. annotating new samples to increase the total number of labeled
instances is necessary. In this paper, we address the issue of faulty data
annotations in the context of active learning. In particular, we propose two
novel annotation unification algorithms that utilize unlabeled parts of the
sample space. The proposed methods require little to no intersection between
samples annotated by different experts. Our experiments on four public datasets
indicate the robustness and superiority of the proposed methods in both, the
estimation of the annotator's reliability, and the assignment of actual labels,
against the state-of-the-art algorithms and the simple majority voting.",2307.14380v1,https://arxiv.org/pdf/2307.14380v1
Deep Reinforcement Learning for Robust Goal-Based Wealth Management,"Tessa Bauman, Bruno Gašperov, Stjepan Begušić, Zvonko Kostanjčar","Goal-based investing is an approach to wealth management that prioritizes
achieving specific financial goals. It is naturally formulated as a sequential
decision-making problem as it requires choosing the appropriate investment
until a goal is achieved. Consequently, reinforcement learning, a machine
learning technique appropriate for sequential decision-making, offers a
promising path for optimizing these investment strategies. In this paper, a
novel approach for robust goal-based wealth management based on deep
reinforcement learning is proposed. The experimental results indicate its
superiority over several goal-based wealth management benchmarks on both
simulated and historical market data.",2307.13501v1,https://arxiv.org/pdf/2307.13501v1
Scaff-PD: Communication Efficient Fair and Robust Federated Learning,"Yaodong Yu, Sai Praneeth Karimireddy, Yi Ma, Michael I. Jordan","We present Scaff-PD, a fast and communication-efficient algorithm for
distributionally robust federated learning. Our approach improves fairness by
optimizing a family of distributionally robust objectives tailored to
heterogeneous clients. We leverage the special structure of these objectives,
and design an accelerated primal dual (APD) algorithm which uses bias corrected
local steps (as in Scaffold) to achieve significant gains in communication
efficiency and convergence speed. We evaluate Scaff-PD on several benchmark
datasets and demonstrate its effectiveness in improving fairness and robustness
while maintaining competitive accuracy. Our results suggest that Scaff-PD is a
promising approach for federated learning in resource-constrained and
heterogeneous settings.",2307.13381v1,https://arxiv.org/pdf/2307.13381v1
"Federated Distributionally Robust Optimization with Non-Convex
  Objectives: Algorithm and Analysis","Yang Jiao, Kai Yang, Dongjin Song","Distributionally Robust Optimization (DRO), which aims to find an optimal
decision that minimizes the worst case cost over the ambiguity set of
probability distribution, has been widely applied in diverse applications,
e.g., network behavior analysis, risk management, etc. However, existing DRO
techniques face three key challenges: 1) how to deal with the asynchronous
updating in a distributed environment; 2) how to leverage the prior
distribution effectively; 3) how to properly adjust the degree of robustness
according to different scenarios. To this end, we propose an asynchronous
distributed algorithm, named Asynchronous Single-looP alternatIve gRadient
projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to
tackle the federated distributionally robust optimization (FDRO) problem.
Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set,
is developed to effectively leverage the prior distribution and flexibly
control the degree of robustness. Finally, our theoretical analysis elucidates
that the proposed algorithm is guaranteed to converge and the iteration
complexity is also analyzed. Extensive empirical studies on real-world datasets
demonstrate that the proposed method can not only achieve fast convergence, and
remain robust against data heterogeneity as well as malicious attacks, but also
tradeoff robustness with performance.",2307.14364v1,https://arxiv.org/pdf/2307.14364v1
"Investigating the Robustness of Sequential Recommender Systems Against
  Training Data Perturbations","Filippo Betello, Federico Siciliano, Pushkar Mishra, Fabrizio Silvestri","Sequential Recommender Systems (SRSs) are widely employed to model user
behavior over time. However, their robustness in the face of perturbations in
training data remains a largely understudied yet critical issue. A fundamental
challenge emerges in previous studies aimed at assessing the robustness of
SRSs: the Rank-Biased Overlap (RBO) similarity is not particularly suited for
this task as it is designed for infinite rankings of items and thus shows
limitations in real-world scenarios. For instance, it fails to achieve a
perfect score of 1 for two identical finite-length rankings. To address this
challenge, we introduce a novel contribution: Finite Rank-Biased Overlap
(FRBO), an enhanced similarity tailored explicitly for finite rankings. This
innovation facilitates a more intuitive evaluation in practical settings. In
pursuit of our goal, we empirically investigate the impact of removing items at
different positions within a temporally ordered sequence. We evaluate two
distinct SRS models across multiple datasets, measuring their performance using
metrics such as Normalized Discounted Cumulative Gain (NDCG) and Rank List
Sensitivity. Our results demonstrate that removing items at the end of the
sequence has a statistically significant impact on performance, with NDCG
decreasing up to 60%. Conversely, removing items from the beginning or middle
has no significant effect. These findings underscore the criticality of the
position of perturbed items in the training data. As we spotlight the
vulnerabilities inherent in current SRSs, we fervently advocate for intensified
research efforts to fortify their robustness against adversarial perturbations.",2307.13165v2,https://arxiv.org/pdf/2307.13165v2
"Adaptive Certified Training: Towards Better Accuracy-Robustness
  Tradeoffs","Zhakshylyk Nurlanov, Frank R. Schmidt, Florian Bernard","As deep learning models continue to advance and are increasingly utilized in
real-world systems, the issue of robustness remains a major challenge. Existing
certified training methods produce models that achieve high provable robustness
guarantees at certain perturbation levels. However, the main problem of such
models is a dramatically low standard accuracy, i.e. accuracy on clean
unperturbed data, that makes them impractical. In this work, we consider a more
realistic perspective of maximizing the robustness of a model at certain levels
of (high) standard accuracy. To this end, we propose a novel certified training
method based on a key insight that training with adaptive certified radii helps
to improve both the accuracy and robustness of the model, advancing
state-of-the-art accuracy-robustness tradeoffs. We demonstrate the
effectiveness of the proposed method on MNIST, CIFAR-10, and TinyImageNet
datasets. Particularly, on CIFAR-10 and TinyImageNet, our method yields models
with up to two times higher robustness, measured as an average certified radius
of a test set, at the same levels of standard accuracy compared to baseline
approaches.",2307.13078v1,https://arxiv.org/pdf/2307.13078v1
Learning Provably Robust Estimators for Inverse Problems via Jittering,"Anselm Krainovic, Mahdi Soltanolkotabi, Reinhard Heckel","Deep neural networks provide excellent performance for inverse problems such
as denoising. However, neural networks can be sensitive to adversarial or
worst-case perturbations. This raises the question of whether such networks can
be trained efficiently to be worst-case robust. In this paper, we investigate
whether jittering, a simple regularization technique that adds isotropic
Gaussian noise during training, is effective for learning worst-case robust
estimators for inverse problems. While well studied for prediction in
classification tasks, the effectiveness of jittering for inverse problems has
not been systematically investigated. In this paper, we present a novel
analytical characterization of the optimal $\ell_2$-worst-case robust estimator
for linear denoising and show that jittering yields optimal robust denoisers.
Furthermore, we examine jittering empirically via training deep neural networks
(U-nets) for natural image denoising, deconvolution, and accelerated magnetic
resonance imaging (MRI). The results show that jittering significantly enhances
the worst-case robustness, but can be suboptimal for inverse problems beyond
denoising. Moreover, our results imply that training on real data which often
contains slight noise is somewhat robustness enhancing.",2307.12822v1,https://arxiv.org/pdf/2307.12822v1
Homophily-Driven Sanitation View for Robust Graph Contrastive Learning,"Yulin Zhu, Xing Ai, Yevgeniy Vorobeychik, Kai Zhou","We investigate adversarial robustness of unsupervised Graph Contrastive
Learning (GCL) against structural attacks. First, we provide a comprehensive
empirical and theoretical analysis of existing attacks, revealing how and why
they downgrade the performance of GCL. Inspired by our analytic results, we
present a robust GCL framework that integrates a homophily-driven sanitation
view, which can be learned jointly with contrastive learning. A key challenge
this poses, however, is the non-differentiable nature of the sanitation
objective. To address this challenge, we propose a series of techniques to
enable gradient-based end-to-end robust GCL. Moreover, we develop a fully
unsupervised hyperparameter tuning method which, unlike prior approaches, does
not require knowledge of node labels. We conduct extensive experiments to
evaluate the performance of our proposed model, GCHS (Graph Contrastive
Learning with Homophily-driven Sanitation View), against two state of the art
structural attacks on GCL. Our results demonstrate that GCHS consistently
outperforms all state of the art baselines in terms of the quality of generated
node embeddings as well as performance on two important downstream tasks.",2307.12555v1,https://arxiv.org/pdf/2307.12555v1
"On the Connection between Pre-training Data Diversity and Fine-tuning
  Robustness","Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ludwig Schmidt, Ali Farhadi","Pre-training has been widely adopted in deep learning to improve model
performance, especially when the training data for a target task is limited. In
our work, we seek to understand the implications of this training strategy on
the generalization properties of downstream models. More specifically, we ask
the following question: how do properties of the pre-training distribution
affect the robustness of a fine-tuned model? The properties we explore include
the label space, label semantics, image diversity, data domains, and data
quantity of the pre-training distribution. We find that the primary factor
influencing downstream effective robustness (Taori et al., 2020) is data
quantity, while other factors have limited significance. For example, reducing
the number of ImageNet pre-training classes by 4x while increasing the number
of images per class by 4x (that is, keeping total data quantity fixed) does not
impact the robustness of fine-tuned models. We demonstrate our findings on
pre-training distributions drawn from various natural and synthetic data
sources, primarily using the iWildCam-WILDS distribution shift as a test for
downstream robustness.",2307.12532v1,https://arxiv.org/pdf/2307.12532v1
"Lost In Translation: Generating Adversarial Examples Robust to
  Round-Trip Translation","Neel Bhandari, Pin-Yu Chen","Language Models today provide a high accuracy across a large number of
downstream tasks. However, they remain susceptible to adversarial attacks,
particularly against those where the adversarial examples maintain considerable
similarity to the original text. Given the multilingual nature of text, the
effectiveness of adversarial examples across translations and how machine
translations can improve the robustness of adversarial examples remain largely
unexplored. In this paper, we present a comprehensive study on the robustness
of current text adversarial attacks to round-trip translation. We demonstrate
that 6 state-of-the-art text-based adversarial attacks do not maintain their
efficacy after round-trip translation. Furthermore, we introduce an
intervention-based solution to this problem, by integrating Machine Translation
into the process of adversarial example generation and demonstrating increased
robustness to round-trip translation. Our results indicate that finding
adversarial examples robust to translation can help identify the insufficiency
of language models that is common across languages, and motivate further
research into multilingual adversarial attacks.",2307.12520v1,https://arxiv.org/pdf/2307.12520v1
"Learning Universal and Robust 3D Molecular Representations with Graph
  Convolutional Networks","Shuo Zhang, Yang Liu, Li Xie, Lei Xie","To learn accurate representations of molecules, it is essential to consider
both chemical and geometric features. To encode geometric information, many
descriptors have been proposed in constrained circumstances for specific types
of molecules and do not have the properties to be ``robust"": 1. Invariant to
rotations and translations; 2. Injective when embedding molecular structures.
In this work, we propose a universal and robust Directional Node Pair (DNP)
descriptor based on the graph representations of 3D molecules. Our DNP
descriptor is robust compared to previous ones and can be applied to multiple
molecular types. To combine the DNP descriptor and chemical features in
molecules, we construct the Robust Molecular Graph Convolutional Network
(RoM-GCN) which is capable to take both node and edge features into
consideration when generating molecule representations. We evaluate our model
on protein and small molecule datasets. Our results validate the superiority of
the DNP descriptor in incorporating 3D geometric information of molecules.
RoM-GCN outperforms all compared baselines.",2307.12491v1,https://arxiv.org/pdf/2307.12491v1
A Machine Learning Approach to Two-Stage Adaptive Robust Optimization,"Dimitris Bertsimas, Cheol Woo Kim","We propose an approach based on machine learning to solve two-stage linear
adaptive robust optimization (ARO) problems with binary here-and-now variables
and polyhedral uncertainty sets. We encode the optimal here-and-now decisions,
the worst-case scenarios associated with the optimal here-and-now decisions,
and the optimal wait-and-see decisions into what we denote as the strategy. We
solve multiple similar ARO instances in advance using the column and constraint
generation algorithm and extract the optimal strategies to generate a training
set. We train a machine learning model that predicts high-quality strategies
for the here-and-now decisions, the worst-case scenarios associated with the
optimal here-and-now decisions, and the wait-and-see decisions. We also
introduce an algorithm to reduce the number of different target classes the
machine learning algorithm needs to be trained on. We apply the proposed
approach to the facility location, the multi-item inventory control and the
unit commitment problems. Our approach solves ARO problems drastically faster
than the state-of-the-art algorithms with high accuracy.",2307.12409v2,https://arxiv.org/pdf/2307.12409v2
"FATRER: Full-Attention Topic Regularizer for Accurate and Robust
  Conversational Emotion Recognition","Yuzhao Mao, Di Lu, Xiaojie Wang, Yang Zhang","This paper concentrates on the understanding of interlocutors' emotions
evoked in conversational utterances. Previous studies in this literature mainly
focus on more accurate emotional predictions, while ignoring model robustness
when the local context is corrupted by adversarial attacks. To maintain
robustness while ensuring accuracy, we propose an emotion recognizer augmented
by a full-attention topic regularizer, which enables an emotion-related global
view when modeling the local context in a conversation. A joint topic modeling
strategy is introduced to implement regularization from both representation and
loss perspectives. To avoid over-regularization, we drop the constraints on
prior distributions that exist in traditional topic modeling and perform
probabilistic approximations based entirely on attention alignment. Experiments
show that our models obtain more favorable results than state-of-the-art
models, and gain convincing robustness under three types of adversarial
attacks.",2307.12221v1,https://arxiv.org/pdf/2307.12221v1
"Improving Out-of-Distribution Robustness of Classifiers via Generative
  Interpolation","Haoyue Bai, Ceyuan Yang, Yinghao Xu, S. -H. Gary Chan, Bolei Zhou","Deep neural networks achieve superior performance for learning from
independent and identically distributed (i.i.d.) data. However, their
performance deteriorates significantly when handling out-of-distribution (OoD)
data, where the training and test are drawn from different distributions. In
this paper, we explore utilizing the generative models as a data augmentation
source for improving out-of-distribution robustness of neural classifiers.
Specifically, we develop a simple yet effective method called Generative
Interpolation to fuse generative models trained from multiple domains for
synthesizing diverse OoD samples. Training a generative model directly on the
source domains tends to suffer from mode collapse and sometimes amplifies the
data bias. Instead, we first train a StyleGAN model on one source domain and
then fine-tune it on the other domains, resulting in many correlated generators
where their model parameters have the same initialization thus are aligned. We
then linearly interpolate the model parameters of the generators to spawn new
sets of generators. Such interpolated generators are used as an extra data
augmentation source to train the classifiers. The interpolation coefficients
can flexibly control the augmentation direction and strength. In addition, a
style-mixing mechanism is applied to further improve the diversity of the
generated OoD samples. Our experiments show that the proposed method explicitly
increases the diversity of training domains and achieves consistent
improvements over baselines across datasets and multiple different distribution
shifts.",2307.12219v1,https://arxiv.org/pdf/2307.12219v1
"Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled
  Perturbations","Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Xiangyu Liu, Benjamin Eysenbach, Tuomas Sandholm, Furong Huang, Stephen McAleer","Deploying reinforcement learning (RL) systems requires robustness to
uncertainty and model misspecification, yet prior robust RL methods typically
only study noise introduced independently across time. However, practical
sources of uncertainty are usually coupled across time. We formally introduce
temporally-coupled perturbations, presenting a novel challenge for existing
robust RL methods. To tackle this challenge, we propose GRAD, a novel
game-theoretic approach that treats the temporally-coupled robust RL problem as
a partially observable two-player zero-sum game. By finding an approximate
equilibrium within this game, GRAD optimizes for general robustness against
temporally-coupled perturbations. Experiments on continuous control tasks
demonstrate that, compared with prior methods, our approach achieves a higher
degree of robustness to various types of attacks on different attack domains,
both in settings with temporally-coupled perturbations and decoupled
perturbations.",2307.12062v3,https://arxiv.org/pdf/2307.12062v3
Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?,"Cheng-En Wu, Yu Tian, Haichao Yu, Heng Wang, Pedro Morgado, Yu Hen Hu, Linjie Yang","Vision-language models such as CLIP learn a generic text-image embedding from
large-scale training data. A vision-language model can be adapted to a new
classification task through few-shot prompt tuning. We find that such a prompt
tuning process is highly robust to label noises. This intrigues us to study the
key reasons contributing to the robustness of the prompt tuning paradigm. We
conducted extensive experiments to explore this property and find the key
factors are: 1) the fixed classname tokens provide a strong regularization to
the optimization of the model, reducing gradients induced by the noisy samples;
2) the powerful pre-trained image-text embedding that is learned from diverse
and generic web data provides strong prior knowledge for image classification.
Further, we demonstrate that noisy zero-shot predictions from CLIP can be used
to tune its own prompt, significantly enhancing prediction accuracy in the
unsupervised setting. The code is available at https://github.com/CEWu/PTNL.",2307.11978v1,https://arxiv.org/pdf/2307.11978v1
"HybridAugment++: Unified Frequency Spectra Perturbations for Model
  Robustness","Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, Pinar Duygulu","Convolutional Neural Networks (CNN) are known to exhibit poor generalization
performance under distribution shifts. Their generalization have been studied
extensively, and one line of work approaches the problem from a
frequency-centric perspective. These studies highlight the fact that humans and
CNNs might focus on different frequency components of an image. First, inspired
by these observations, we propose a simple yet effective data augmentation
method HybridAugment that reduces the reliance of CNNs on high-frequency
components, and thus improves their robustness while keeping their clean
accuracy high. Second, we propose HybridAugment++, which is a hierarchical
augmentation method that attempts to unify various frequency-spectrum
augmentations. HybridAugment++ builds on HybridAugment, and also reduces the
reliance of CNNs on the amplitude component of images, and promotes phase
information instead. This unification results in competitive to or better than
state-of-the-art results on clean accuracy (CIFAR-10/100 and ImageNet),
corruption benchmarks (ImageNet-C, CIFAR-10-C and CIFAR-100-C), adversarial
robustness on CIFAR-10 and out-of-distribution detection on various datasets.
HybridAugment and HybridAugment++ are implemented in a few lines of code, does
not require extra data, ensemble models or additional networks.",2307.11823v1,https://arxiv.org/pdf/2307.11823v1
Fast Adaptive Test-Time Defense with Robust Features,"Anurag Singh, Mahalakshmi Sabanayagam, Krikamol Muandet, Debarghya Ghoshdastidar","Adaptive test-time defenses are used to improve the robustness of deep neural
networks to adversarial examples. However, existing methods significantly
increase the inference time due to additional optimization on the model
parameters or the input at test time. In this work, we propose a novel adaptive
test-time defense strategy that is easy to integrate with any existing (robust)
training procedure without additional test-time computation. Based on the
notion of robustness of features that we present, the key idea is to project
the trained models to the most robust feature space, thereby reducing the
vulnerability to adversarial attacks in non-robust directions. We theoretically
show that the top eigenspace of the feature matrix are more robust for a
generalized additive model and support our argument for a large width neural
network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive
experiments on CIFAR-10 and CIFAR-100 datasets for several robustness
benchmarks, including the state-of-the-art methods in RobustBench, and observe
that the proposed method outperforms existing adaptive test-time defenses at
much lower computation costs.",2307.11672v1,https://arxiv.org/pdf/2307.11672v1
"Robust Fully-Asynchronous Methods for Distributed Training over General
  Architecture","Zehan Zhu, Ye Tian, Yan Huang, Jinming Xu, Shibo He","Perfect synchronization in distributed machine learning problems is
inefficient and even impossible due to the existence of latency, package losses
and stragglers. We propose a Robust Fully-Asynchronous Stochastic Gradient
Tracking method (R-FAST), where each device performs local computation and
communication at its own pace without any form of synchronization. Different
from existing asynchronous distributed algorithms, R-FAST can eliminate the
impact of data heterogeneity across devices and allow for packet losses by
employing a robust gradient tracking strategy that relies on properly designed
auxiliary variables for tracking and buffering the overall gradient vector.
More importantly, the proposed method utilizes two spanning-tree graphs for
communication so long as both share at least one common root, enabling flexible
designs in communication architectures. We show that R-FAST converges in
expectation to a neighborhood of the optimum with a geometric rate for smooth
and strongly convex objectives; and to a stationary point with a sublinear rate
for general non-convex settings. Extensive experiments demonstrate that R-FAST
runs 1.5-2 times faster than synchronous benchmark algorithms, such as
Ring-AllReduce and D-PSGD, while still achieving comparable accuracy, and
outperforms existing asynchronous SOTA algorithms, such as AD-PSGD and OSGP,
especially in the presence of stragglers.",2307.11617v2,https://arxiv.org/pdf/2307.11617v2
"Robust Visual Question Answering: Datasets, Methods, and Future
  Challenges","Jie Ma, Pinghui Wang, Dechen Kong, Zewei Wang, Jun Liu, Hongbin Pei, Junzhou Zhao","Visual question answering requires a system to provide an accurate natural
language answer given an image and a natural language question. However, it is
widely recognized that previous generic VQA methods often exhibit a tendency to
memorize biases present in the training data rather than learning proper
behaviors, such as grounding images before predicting answers. Therefore, these
methods usually achieve high in-distribution but poor out-of-distribution
performance. In recent years, various datasets and debiasing methods have been
proposed to evaluate and enhance the VQA robustness, respectively. This paper
provides the first comprehensive survey focused on this emerging fashion.
Specifically, we first provide an overview of the development process of
datasets from in-distribution and out-of-distribution perspectives. Then, we
examine the evaluation metrics employed by these datasets. Thirdly, we propose
a typology that presents the development process, similarities and differences,
robustness comparison, and technical features of existing debiasing methods.
Furthermore, we analyze and discuss the robustness of representative
vision-and-language pre-training models on VQA. Finally, through a thorough
review of the available literature and experimental analysis, we discuss the
key areas for future research from various viewpoints.",2307.11471v2,https://arxiv.org/pdf/2307.11471v2
HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework,"Kai Lei, Zhan Chen, Shuman Jia, Xiaoteng Zhang","In the field of autonomous driving, 3D object detection is a very important
perception module. Although the current SOTA algorithm combines Camera and
Lidar sensors, limited by the high price of Lidar, the current mainstream
landing schemes are pure Camera sensors or Camera+Radar sensors. In this study,
we propose a new detection algorithm called HVDetFusion, which is a multi-modal
detection algorithm that not only supports pure camera data as input for
detection, but also can perform fusion input of radar data and camera data. The
camera stream does not depend on the input of Radar data, thus addressing the
downside of previous methods. In the pure camera stream, we modify the
framework of Bevdet4D for better perception and more efficient inference, and
this stream has the whole 3D detection output. Further, to incorporate the
benefits of Radar signals, we use the prior information of different object
positions to filter the false positive information of the original radar data,
according to the positioning information and radial velocity information
recorded by the radar sensors to supplement and fuse the BEV features generated
by the original camera data, and the effect is further improved in the process
of fusion training. Finally, HVDetFusion achieves the new state-of-the-art
67.4\% NDS on the challenging nuScenes test set among all camera-radar 3D
object detectors. The code is available at
https://github.com/HVXLab/HVDetFusion",2307.11323v1,https://arxiv.org/pdf/2307.11323v1
Risk-optimized Outlier Removal for Robust 3D Point Cloud Classification,"Xinke Li, Junchi Lu, Henghui Ding, Changsheng Sun, Joey Tianyi Zhou, Chee Yeow Meng","With the growth of 3D sensing technology, deep learning system for 3D point
clouds has become increasingly important, especially in applications like
autonomous vehicles where safety is a primary concern. However, there are also
growing concerns about the reliability of these systems when they encounter
noisy point clouds, whether occurring naturally or introduced with malicious
intent. This paper highlights the challenges of point cloud classification
posed by various forms of noise, from simple background noise to malicious
backdoor attacks that can intentionally skew model predictions. While there's
an urgent need for optimized point cloud denoising, current point outlier
removal approaches, an essential step for denoising, rely heavily on
handcrafted strategies and are not adapted for higher-level tasks, such as
classification. To address this issue, we introduce an innovative point outlier
cleansing method that harnesses the power of downstream classification models.
By employing gradient-based attribution analysis, we define a novel concept:
point risk. Drawing inspiration from tail risk minimization in finance, we
recast the outlier removal process as an optimization problem, named PointCVaR.
Extensive experiments show that our proposed technique not only robustly
filters diverse point cloud outliers but also consistently and significantly
enhances existing robust methods for point cloud classification.",2307.10875v3,https://arxiv.org/pdf/2307.10875v3
Robust Driving Policy Learning with Guided Meta Reinforcement Learning,"Kanghoon Lee, Jiachen Li, David Isele, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochenderfer","Although deep reinforcement learning (DRL) has shown promising results for
autonomous navigation in interactive traffic scenarios, existing work typically
adopts a fixed behavior policy to control social vehicles in the training
environment. This may cause the learned driving policy to overfit the
environment, making it difficult to interact well with vehicles with different,
unseen behaviors. In this work, we introduce an efficient method to train
diverse driving policies for social vehicles as a single meta-policy. By
randomizing the interaction-based reward functions of social vehicles, we can
generate diverse objectives and efficiently train the meta-policy through
guiding policies that achieve specific objectives. We further propose a
training strategy to enhance the robustness of the ego vehicle's driving policy
using the environment where social vehicles are controlled by the learned
meta-policy. Our method successfully learns an ego driving policy that
generalizes well to unseen situations with out-of-distribution (OOD) social
agents' behaviors in a challenging uncontrolled T-intersection scenario.",2307.10160v1,https://arxiv.org/pdf/2307.10160v1
Online Continual Learning for Robust Indoor Object Recognition,"Umberto Michieli, Mete Ozay","Vision systems mounted on home robots need to interact with unseen classes in
changing environments. Robots have limited computational resources, labelled
data and storage capability. These requirements pose some unique challenges:
models should adapt without forgetting past knowledge in a data- and
parameter-efficient way. We characterize the problem as few-shot (FS) online
continual learning (OCL), where robotic agents learn from a non-repeated stream
of few-shot data updating only a few model parameters. Additionally, such
models experience variable conditions at test time, where objects may appear in
different poses (e.g., horizontal or vertical) and environments (e.g., day or
night). To improve robustness of CL agents, we propose RobOCLe, which; 1)
constructs an enriched feature space computing high order statistical moments
from the embedded features of samples; and 2) computes similarity between high
order statistics of the samples on the enriched feature space, and predicts
their class labels. We evaluate robustness of CL models to train/test
augmentations in various cases. We show that different moments allow RobOCLe to
capture different properties of deformations, providing higher robustness with
no decrease of inference speed.",2307.09827v1,https://arxiv.org/pdf/2307.09827v1
Towards Building More Robust Models with Frequency Bias,"Qingwen Bu, Dong Huang, Heming Cui","The vulnerability of deep neural networks to adversarial samples has been a
major impediment to their broad applications, despite their success in various
fields. Recently, some works suggested that adversarially-trained models
emphasize the importance of low-frequency information to achieve higher
robustness. While several attempts have been made to leverage this frequency
characteristic, they have all faced the issue that applying low-pass filters
directly to input images leads to irreversible loss of discriminative
information and poor generalizability to datasets with distinct frequency
features. This paper presents a plug-and-play module called the Frequency
Preference Control Module that adaptively reconfigures the low- and
high-frequency components of intermediate feature representations, providing
better utilization of frequency in robust learning. Empirical studies show that
our proposed module can be easily incorporated into any adversarial training
framework, further improving model robustness across different architectures
and datasets. Additionally, experiments were conducted to examine how the
frequency bias of robust models impacts the adversarial training process and
its final robustness, revealing interesting insights.",2307.09763v2,https://arxiv.org/pdf/2307.09763v2
Causality-oriented robustness: exploiting general additive interventions,"Xinwei Shen, Peter Bühlmann, Armeen Taeb","Since distribution shifts are common in real-world applications, there is a
pressing need for developing prediction models that are robust against such
shifts. Existing frameworks, such as empirical risk minimization or
distributionally robust optimization, either lack generalizability for unseen
distributions or rely on postulated distance measures. Alternatively, causality
offers a data-driven and structural perspective to robust predictions. However,
the assumptions necessary for causal inference can be overly stringent, and the
robustness offered by such causal models often lacks flexibility. In this
paper, we focus on causality-oriented robustness and propose Distributional
Robustness via Invariant Gradients (DRIG), a method that exploits general
additive interventions in training data for robust predictions against unseen
interventions, and naturally interpolates between in-distribution prediction
and causality. In a linear setting, we prove that DRIG yields predictions that
are robust among a data-dependent class of distribution shifts. Furthermore, we
show that our framework includes anchor regression (Rothenh\""ausler et al.\
2021) as a special case, and that it yields prediction models that protect
against more diverse perturbations. We extend our approach to the
semi-supervised domain adaptation setting to further improve prediction
performance. Finally, we empirically validate our methods on synthetic
simulations and on single-cell data.",2307.10299v1,https://arxiv.org/pdf/2307.10299v1
"An Evaluation of Zero-Cost Proxies -- from Neural Architecture
  Performance to Model Robustness","Jovita Lukasik, Michael Moeller, Margret Keuper","Zero-cost proxies are nowadays frequently studied and used to search for
neural architectures. They show an impressive ability to predict the
performance of architectures by making use of their untrained weights. These
techniques allow for immense search speed-ups. So far the joint search for
well-performing and robust architectures has received much less attention in
the field of NAS. Therefore, the main focus of zero-cost proxies is the clean
accuracy of architectures, whereas the model robustness should play an evenly
important part. In this paper, we analyze the ability of common zero-cost
proxies to serve as performance predictors for robustness in the popular
NAS-Bench-201 search space. We are interested in the single prediction task for
robustness and the joint multi-objective of clean and robust accuracy. We
further analyze the feature importance of the proxies and show that predicting
the robustness makes the prediction task from existing zero-cost proxies more
challenging. As a result, the joint consideration of several proxies becomes
necessary to predict a model's robustness while the clean accuracy can be
regressed from a single such feature.",2307.09365v1,https://arxiv.org/pdf/2307.09365v1
"Robust Data Clustering with Outliers via Transformed Tensor Low-Rank
  Representation",Tong Wu,"Recently, tensor low-rank representation (TLRR) has become a popular tool for
tensor data recovery and clustering, due to its empirical success and
theoretical guarantees. However, existing TLRR methods consider Gaussian or
gross sparse noise, inevitably leading to performance degradation when the
tensor data are contaminated by outliers or sample-specific corruptions. This
paper develops an outlier-robust tensor low-rank representation (OR-TLRR)
method that provides outlier detection and tensor data clustering
simultaneously based on the t-SVD framework. For tensor observations with
arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for
exactly recovering the row space of clean data and detecting outliers under
mild conditions. Moreover, an extension of OR-TLRR is proposed to handle the
case when parts of the data are missing. Finally, extensive experimental
results on synthetic and real data demonstrate the effectiveness of the
proposed algorithms. We release our code at
https://github.com/twugithub/2024-AISTATS-ORTLRR.",2307.09055v3,https://arxiv.org/pdf/2307.09055v3
Discretization-based ensemble model for robust learning in IoT,"Anahita Namvar, Chandra Thapa, Salil S. Kanhere","IoT device identification is the process of recognizing and verifying
connected IoT devices to the network. This is an essential process for ensuring
that only authorized devices can access the network, and it is necessary for
network management and maintenance. In recent years, machine learning models
have been used widely for automating the process of identifying devices in the
network. However, these models are vulnerable to adversarial attacks that can
compromise their accuracy and effectiveness. To better secure device
identification models, discretization techniques enable reduction in the
sensitivity of machine learning models to adversarial attacks contributing to
the stability and reliability of the model. On the other hand, Ensemble methods
combine multiple heterogeneous models to reduce the impact of remaining noise
or errors in the model. Therefore, in this paper, we integrate discretization
techniques and ensemble methods and examine it on model robustness against
adversarial attacks. In other words, we propose a discretization-based ensemble
stacking technique to improve the security of our ML models. We evaluate the
performance of different ML-based IoT device identification models against
white box and black box attacks using a real-world dataset comprised of network
traffic from 28 IoT devices. We demonstrate that the proposed method enables
robustness to the models for IoT device identification.",2307.08955v1,https://arxiv.org/pdf/2307.08955v1
"Natural Actor-Critic for Robust Reinforcement Learning with Function
  Approximation","Ruida Zhou, Tao Liu, Min Cheng, Dileep Kalathil, P. R. Kumar, Chao Tian","We study robust reinforcement learning (RL) with the goal of determining a
well-performing policy that is robust against model mismatch between the
training simulator and the testing environment. Previous policy-based robust RL
algorithms mainly focus on the tabular setting under uncertainty sets that
facilitate robust policy evaluation, but are no longer tractable when the
number of states scales up. To this end, we propose two novel uncertainty set
formulations, one based on double sampling and the other on an integral
probability metric. Both make large-scale robust RL tractable even when one
only has access to a simulator. We propose a robust natural actor-critic (RNAC)
approach that incorporates the new uncertainty sets and employs function
approximation. We provide finite-time convergence guarantees for the proposed
RNAC algorithm to the optimal robust policy within the function approximation
error. Finally, we demonstrate the robust performance of the policy learned by
our proposed RNAC approach in multiple MuJoCo environments and a real-world
TurtleBot navigation task.",2307.08875v2,https://arxiv.org/pdf/2307.08875v2
"CohortFinder: an open-source tool for data-driven partitioning of
  biomedical image cohorts to yield robust machine learning models","Fan Fan, Georgia Martinez, Thomas Desilvio, John Shin, Yijiang Chen, Bangchen Wang, Takaya Ozeki, Maxime W. Lafarge, Viktor H. Koelzer, Laura Barisoni, Anant Madabhushi, Satish E. Viswanath, Andrew Janowczyk","Batch effects (BEs) refer to systematic technical differences in data
collection unrelated to biological variations whose noise is shown to
negatively impact machine learning (ML) model generalizability. Here we release
CohortFinder, an open-source tool aimed at mitigating BEs via data-driven
cohort partitioning. We demonstrate CohortFinder improves ML model performance
in downstream medical image processing tasks. CohortFinder is freely available
for download at cohortfinder.com.",2307.08673v1,https://arxiv.org/pdf/2307.08673v1
"Neural Image Compression: Generalization, Robustness, and Spectral
  Biases","Kelsey Lieberman, James Diffenderfer, Charles Godfrey, Bhavya Kailkhura","Recent advances in neural image compression (NIC) have produced models that
are starting to outperform classic codecs. While this has led to growing
excitement about using NIC in real-world applications, the successful adoption
of any machine learning system in the wild requires it to generalize (and be
robust) to unseen distribution shifts at deployment. Unfortunately, current
research lacks comprehensive datasets and informative tools to evaluate and
understand NIC performance in real-world settings. To bridge this crucial gap,
first, this paper presents a comprehensive benchmark suite to evaluate the
out-of-distribution (OOD) performance of image compression methods.
Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions to
the popular CLIC and Kodak benchmarks. Next, we propose spectrally-inspired
inspection tools to gain deeper insight into errors introduced by image
compression methods as well as their OOD performance. We then carry out a
detailed performance comparison of several classic codecs and NIC variants,
revealing intriguing findings that challenge our current understanding of the
strengths and limitations of NIC. Finally, we corroborate our empirical
findings with theoretical analysis, providing an in-depth view of the OOD
performance of NIC and its dependence on the spectral properties of the data.
Our benchmarks, spectral inspection tools, and findings provide a crucial
bridge to the real-world adoption of NIC. We hope that our work will propel
future efforts in designing robust and generalizable NIC methods. Code and data
will be made available at https://github.com/klieberman/ood_nic.",2307.08657v2,https://arxiv.org/pdf/2307.08657v2
"Revisiting the Robustness of the Minimum Error Entropy Criterion: A
  Transfer Learning Case Study","Luis Pedro Silvestrin, Shujian Yu, Mark Hoogendoorn","Coping with distributional shifts is an important part of transfer learning
methods in order to perform well in real-life tasks. However, most of the
existing approaches in this area either focus on an ideal scenario in which the
data does not contain noises or employ a complicated training paradigm or model
design to deal with distributional shifts. In this paper, we revisit the
robustness of the minimum error entropy (MEE) criterion, a widely used
objective in statistical signal processing to deal with non-Gaussian noises,
and investigate its feasibility and usefulness in real-life transfer learning
regression tasks, where distributional shifts are common. Specifically, we put
forward a new theoretical result showing the robustness of MEE against
covariate shift. We also show that by simply replacing the mean squared error
(MSE) loss with the MEE on basic transfer learning algorithms such as
fine-tuning and linear probing, we can achieve competitive performance with
respect to state-of-the-art transfer learning algorithms. We justify our
arguments on both synthetic data and 5 real-world time-series data.",2307.08572v4,https://arxiv.org/pdf/2307.08572v4
Systematic Testing of the Data-Poisoning Robustness of KNN,"Yannan Li, Jingbo Wang, Chao Wang","Data poisoning aims to compromise a machine learning based software component
by contaminating its training set to change its prediction results for test
inputs. Existing methods for deciding data-poisoning robustness have either
poor accuracy or long running time and, more importantly, they can only certify
some of the truly-robust cases, but remain inconclusive when certification
fails. In other words, they cannot falsify the truly-non-robust cases. To
overcome this limitation, we propose a systematic testing based method, which
can falsify as well as certify data-poisoning robustness for a widely used
supervised-learning technique named k-nearest neighbors (KNN). Our method is
faster and more accurate than the baseline enumeration method, due to a novel
over-approximate analysis in the abstract domain, to quickly narrow down the
search space, and systematic testing in the concrete domain, to find the actual
violations. We have evaluated our method on a set of supervised-learning
datasets. Our results show that the method significantly outperforms
state-of-the-art techniques, and can decide data-poisoning robustness of KNN
prediction results for most of the test inputs.",2307.08288v1,https://arxiv.org/pdf/2307.08288v1
"Evaluating and Enhancing Robustness of Deep Recommendation Systems
  Against Hardware Errors","Dongning Ma, Xun Jiao, Fred Lin, Mengshi Zhang, Alban Desmaison, Thomas Sellinger, Daniel Moore, Sriram Sankar","Deep recommendation systems (DRS) heavily depend on specialized HPC hardware
and accelerators to optimize energy, efficiency, and recommendation quality.
Despite the growing number of hardware errors observed in large-scale fleet
systems where DRS are deployed, the robustness of DRS has been largely
overlooked. This paper presents the first systematic study of DRS robustness
against hardware errors. We develop Terrorch, a user-friendly, efficient and
flexible error injection framework on top of the widely-used PyTorch. We
evaluate a wide range of models and datasets and observe that the DRS
robustness against hardware errors is influenced by various factors from model
parameters to input characteristics. We also explore 3 error mitigation methods
including algorithm based fault tolerance (ABFT), activation clipping and
selective bit protection (SBP). We find that applying activation clipping can
recover up to 30% of the degraded AUC-ROC score, making it a promising
mitigation method.",2307.10244v1,https://arxiv.org/pdf/2307.10244v1
"POMDP inference and robust solution via deep reinforcement learning: An
  application to railway optimal maintenance","Giacomo Arcieri, Cyprien Hoelzl, Oliver Schwery, Daniel Straub, Konstantinos G. Papakonstantinou, Eleni Chatzi","Partially Observable Markov Decision Processes (POMDPs) can model complex
sequential decision-making problems under stochastic and uncertain
environments. A main reason hindering their broad adoption in real-world
applications is the lack of availability of a suitable POMDP model or a
simulator thereof. Available solution algorithms, such as Reinforcement
Learning (RL), require the knowledge of the transition dynamics and the
observation generating process, which are often unknown and non-trivial to
infer. In this work, we propose a combined framework for inference and robust
solution of POMDPs via deep RL. First, all transition and observation model
parameters are jointly inferred via Markov Chain Monte Carlo sampling of a
hidden Markov model, which is conditioned on actions, in order to recover full
posterior distributions from the available data. The POMDP with uncertain
parameters is then solved via deep RL techniques with the parameter
distributions incorporated into the solution via domain randomization, in order
to develop solutions that are robust to model uncertainty. As a further
contribution, we compare the use of transformers and long short-term memory
networks, which constitute model-free RL solutions, with a
model-based/model-free hybrid approach. We apply these methods to the
real-world problem of optimal maintenance planning for railway assets.",2307.08082v1,https://arxiv.org/pdf/2307.08082v1
"Byzantine-Robust Distributed Online Learning: Taming Adversarial
  Participants in An Adversarial Environment","Xingrong Dong, Zhaoxian Wu, Qing Ling, Zhi Tian","This paper studies distributed online learning under Byzantine attacks. The
performance of an online learning algorithm is often characterized by
(adversarial) regret, which evaluates the quality of one-step-ahead
decision-making when an environment provides adversarial losses, and a
sublinear bound is preferred. But we prove that, even with a class of
state-of-the-art robust aggregation rules, in an adversarial environment and in
the presence of Byzantine participants, distributed online gradient descent can
only achieve a linear adversarial regret bound, which is tight. This is the
inevitable consequence of Byzantine attacks, even though we can control the
constant of the linear adversarial regret to a reasonable level. Interestingly,
when the environment is not fully adversarial so that the losses of the honest
participants are i.i.d. (independent and identically distributed), we show that
sublinear stochastic regret, in contrast to the aforementioned adversarial
regret, is possible. We develop a Byzantine-robust distributed online momentum
algorithm to attain such a sublinear stochastic regret bound. Extensive
numerical experiments corroborate our theoretical analysis.",2307.07980v3,https://arxiv.org/pdf/2307.07980v3
On the Robustness of Split Learning against Adversarial Attacks,"Mingyuan Fan, Cen Chen, Chengyu Wang, Wenmeng Zhou, Jun Huang","Split learning enables collaborative deep learning model training while
preserving data privacy and model security by avoiding direct sharing of raw
data and model details (i.e., sever and clients only hold partial sub-networks
and exchange intermediate computations). However, existing research has mainly
focused on examining its reliability for privacy protection, with little
investigation into model security. Specifically, by exploring full models,
attackers can launch adversarial attacks, and split learning can mitigate this
severe threat by only disclosing part of models to untrusted servers.This paper
aims to evaluate the robustness of split learning against adversarial attacks,
particularly in the most challenging setting where untrusted servers only have
access to the intermediate layers of the model.Existing adversarial attacks
mostly focus on the centralized setting instead of the collaborative setting,
thus, to better evaluate the robustness of split learning, we develop a
tailored attack called SPADV, which comprises two stages: 1) shadow model
training that addresses the issue of lacking part of the model and 2) local
adversarial attack that produces adversarial examples to evaluate.The first
stage only requires a few unlabeled non-IID data, and, in the second stage,
SPADV perturbs the intermediate output of natural samples to craft the
adversarial ones. The overall cost of the proposed attack process is relatively
low, yet the empirical attack effectiveness is significantly high,
demonstrating the surprising vulnerability of split learning to adversarial
attacks.",2307.07916v2,https://arxiv.org/pdf/2307.07916v2
"Seeing is not Believing: Robust Reinforcement Learning against Spurious
  Correlation","Wenhao Ding, Laixi Shi, Yuejie Chi, Ding Zhao","Robustness has been extensively studied in reinforcement learning (RL) to
handle various forms of uncertainty such as random perturbations, rare events,
and malicious attacks. In this work, we consider one critical type of
robustness against spurious correlation, where different portions of the state
do not have correlations induced by unobserved confounders. These spurious
correlations are ubiquitous in real-world tasks, for instance, a self-driving
car usually observes heavy traffic in the daytime and light traffic at night
due to unobservable human activity. A model that learns such useless or even
harmful correlation could catastrophically fail when the confounder in the test
case deviates from the training one. Although motivated, enabling robustness
against spurious correlation poses significant challenges since the uncertainty
set, shaped by the unobserved confounder and causal structure, is difficult to
characterize and identify. Existing robust algorithms that assume simple and
unstructured uncertainty sets are therefore inadequate to address this
challenge. To solve this issue, we propose Robust State-Confounded Markov
Decision Processes (RSC-MDPs) and theoretically demonstrate its superiority in
avoiding learning spurious correlations compared with other robust RL
counterparts. We also design an empirical algorithm to learn the robust optimal
policy for RSC-MDPs, which outperforms all baselines in eight realistic
self-driving and manipulation tasks.",2307.07907v2,https://arxiv.org/pdf/2307.07907v2
"Intuitionistic Fuzzy Broad Learning System: Enhancing Robustness Against
  Noise and Outliers","M. Sajid, A. K. Malik, M. Tanveer","In the realm of data classification, broad learning system (BLS) has proven
to be a potent tool that utilizes a layer-by-layer feed-forward neural network.
However, the traditional BLS treats all samples as equally significant, which
makes it less robust and less effective for real-world datasets with noises and
outliers. To address this issue, we propose fuzzy broad learning system (F-BLS)
and the intuitionistic fuzzy broad learning system (IF-BLS) models that
confront challenges posed by the noise and outliers present in the dataset and
enhance overall robustness. Employing a fuzzy membership technique, the
proposed F-BLS model embeds sample neighborhood information based on the
proximity of each class center within the inherent feature space of the BLS
framework. Furthermore, the proposed IF-BLS model introduces intuitionistic
fuzzy concepts encompassing membership, non-membership, and score value
functions. IF-BLS strategically considers homogeneity and heterogeneity in
sample neighborhoods in the kernel space. We evaluate the performance of
proposed F-BLS and IF-BLS models on UCI benchmark datasets with and without
Gaussian noise. As an application, we implement the proposed F-BLS and IF-BLS
models to diagnose Alzheimer's disease (AD). Experimental findings and
statistical analyses consistently highlight the superior generalization
capabilities of the proposed F-BLS and IF-BLS models over baseline models
across all scenarios. The proposed models offer a promising solution to enhance
the BLS framework's ability to handle noise and outliers. The source code link
of the proposed model is available at https://github.com/mtanveer1/IF-BLS.",2307.08713v2,https://arxiv.org/pdf/2307.08713v2
"Why Does Little Robustness Help? Understanding and Improving Adversarial
  Transferability from Surrogate Training","Yechao Zhang, Shengshan Hu, Leo Yu Zhang, Junyu Shi, Minghui Li, Xiaogeng Liu, Wei Wan, Hai Jin","Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs
that successfully fool white-box surrogate models can also deceive other
black-box models with different architectures. Although a bunch of empirical
studies have provided guidance on generating highly transferable AEs, many of
these findings lack explanations and even lead to inconsistent advice. In this
paper, we take a further step towards understanding adversarial
transferability, with a particular focus on surrogate aspects. Starting from
the intriguing little robustness phenomenon, where models adversarially trained
with mildly perturbed adversarial samples can serve as better surrogates, we
attribute it to a trade-off between two predominant factors: model smoothness
and gradient similarity. Our investigations focus on their joint effects,
rather than their separate correlations with transferability. Through a series
of theoretical and empirical analyses, we conjecture that the data distribution
shift in adversarial training explains the degradation of gradient similarity.
Building on these insights, we explore the impacts of data augmentation and
gradient regularization on transferability and identify that the trade-off
generally exists in the various training mechanisms, thus building a
comprehensive blueprint for the regulation mechanism behind transferability.
Finally, we provide a general route for constructing better surrogates to boost
transferability which optimizes both model smoothness and gradient similarity
simultaneously, e.g., the combination of input gradient regularization and
sharpness-aware minimization (SAM), validated by extensive experiments. In
summary, we call for attention to the united impacts of these two factors for
launching effective transfer attacks, rather than optimizing one while ignoring
the other, and emphasize the crucial role of manipulating surrogate models.",2307.07873v6,https://arxiv.org/pdf/2307.07873v6
"Coupling Large Language Models with Logic Programming for Robust and
  General Reasoning from Text","Zhun Yang, Adam Ishay, Joohyung Lee","While large language models (LLMs), such as GPT-3, appear to be robust and
general, their reasoning ability is not at a level to compete with the best
models trained for specific natural language reasoning problems. In this study,
we observe that a large language model can serve as a highly effective few-shot
semantic parser. It can convert natural language sentences into a logical form
that serves as input for answer set programs, a logic-based declarative
knowledge representation formalism. The combination results in a robust and
general system that can handle multiple question-answering tasks without
requiring retraining for each new task. It only needs a few examples to guide
the LLM's adaptation to a specific task, along with reusable ASP knowledge
modules that can be applied to multiple tasks. We demonstrate that this method
achieves state-of-the-art performance on several NLP benchmarks, including
bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot
planning tasks that an LLM alone fails to solve.",2307.07696v1,https://arxiv.org/pdf/2307.07696v1
"On the Robustness of Epoch-Greedy in Multi-Agent Contextual Bandit
  Mechanisms","Yinglun Xu, Bhuvesh Kumar, Jacob Abernethy","Efficient learning in multi-armed bandit mechanisms such as pay-per-click
(PPC) auctions typically involves three challenges: 1) inducing truthful
bidding behavior (incentives), 2) using personalization in the users (context),
and 3) circumventing manipulations in click patterns (corruptions). Each of
these challenges has been studied orthogonally in the literature; incentives
have been addressed by a line of work on truthful multi-armed bandit
mechanisms, context has been extensively tackled by contextual bandit
algorithms, while corruptions have been discussed via a recent line of work on
bandits with adversarial corruptions. Since these challenges co-exist, it is
important to understand the robustness of each of these approaches in
addressing the other challenges, provide algorithms that can handle all
simultaneously, and highlight inherent limitations in this combination. In this
work, we show that the most prominent contextual bandit algorithm,
$\epsilon$-greedy can be extended to handle the challenges introduced by
strategic arms in the contextual multi-arm bandit mechanism setting. We further
show that $\epsilon$-greedy is inherently robust to adversarial data corruption
attacks and achieves performance that degrades linearly with the amount of
corruption.",2307.07675v1,https://arxiv.org/pdf/2307.07675v1
"Efficient Action Robust Reinforcement Learning with Probabilistic Policy
  Execution Uncertainty","Guanlin Liu, Zhihan Zhou, Han Liu, Lifeng Lai","Robust reinforcement learning (RL) aims to find a policy that optimizes the
worst-case performance in the face of uncertainties. In this paper, we focus on
action robust RL with the probabilistic policy execution uncertainty, in which,
instead of always carrying out the action specified by the policy, the agent
will take the action specified by the policy with probability $1-\rho$ and an
alternative adversarial action with probability $\rho$. We establish the
existence of an optimal policy on the action robust MDPs with probabilistic
policy execution uncertainty and provide the action robust Bellman optimality
equation for its solution. Furthermore, we develop Action Robust Reinforcement
Learning with Certificates (ARRLC) algorithm that achieves minimax optimal
regret and sample complexity. Furthermore, we conduct numerical experiments to
validate our approach's robustness, demonstrating that ARRLC outperforms
non-robust RL algorithms and converges faster than the robust TD algorithm in
the presence of action perturbations.",2307.07666v2,https://arxiv.org/pdf/2307.07666v2
"Frequency Domain Adversarial Training for Robust Volumetric Medical
  Segmentation","Asif Hanif, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan","It is imperative to ensure the robustness of deep learning models in critical
applications such as, healthcare. While recent advances in deep learning have
improved the performance of volumetric medical image segmentation models, these
models cannot be deployed for real-world applications immediately due to their
vulnerability to adversarial attacks. We present a 3D frequency domain
adversarial attack for volumetric medical image segmentation models and
demonstrate its advantages over conventional input or voxel domain attacks.
Using our proposed attack, we introduce a novel frequency domain adversarial
training approach for optimizing a robust model against voxel and frequency
domain attacks. Moreover, we propose frequency consistency loss to regulate our
frequency domain adversarial training that achieves a better tradeoff between
model's performance on clean and adversarial samples. Code is publicly
available at https://github.com/asif-hanif/vafa.",2307.07269v2,https://arxiv.org/pdf/2307.07269v2
"Multiplicative update rules for accelerating deep learning training and
  increasing robustness","Manos Kirtas, Nikolaos Passalis, Anastasios Tefas","Even nowadays, where Deep Learning (DL) has achieved state-of-the-art
performance in a wide range of research domains, accelerating training and
building robust DL models remains a challenging task. To this end, generations
of researchers have pursued to develop robust methods for training DL
architectures that can be less sensitive to weight distributions, model
architectures and loss landscapes. However, such methods are limited to
adaptive learning rate optimizers, initialization schemes, and clipping
gradients without investigating the fundamental rule of parameters update.
Although multiplicative updates have contributed significantly to the early
development of machine learning and hold strong theoretical claims, to best of
our knowledge, this is the first work that investigate them in context of DL
training acceleration and robustness. In this work, we propose an optimization
framework that fits to a wide range of optimization algorithms and enables one
to apply alternative update rules. To this end, we propose a novel
multiplicative update rule and we extend their capabilities by combining it
with a traditional additive update term, under a novel hybrid update method. We
claim that the proposed framework accelerates training, while leading to more
robust models in contrast to traditionally used additive update rule and we
experimentally demonstrate their effectiveness in a wide range of task and
optimization methods. Such tasks ranging from convex and non-convex
optimization to difficult image classification benchmarks applying a wide range
of traditionally used optimization methods and Deep Neural Network (DNN)
architectures.",2307.07189v1,https://arxiv.org/pdf/2307.07189v1
Certified Robustness for Large Language Models with Self-Denoising,"Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, Shiyu Chang","Although large language models (LLMs) have achieved great success in vast
real-world applications, their vulnerabilities towards noisy inputs have
significantly limited their uses, especially in high-stake environments. In
these contexts, it is crucial to ensure that every prediction made by large
language models is stable, i.e., LLM predictions should be consistent given
minor differences in the input. This largely falls into the study of certified
robust LLMs, i.e., all predictions of LLM are certified to be correct in a
local region around the input. Randomized smoothing has demonstrated great
potential in certifying the robustness and prediction stability of LLMs.
However, randomized smoothing requires adding noise to the input before model
prediction, and its certification performance depends largely on the model's
performance on corrupted data. As a result, its direct application to LLMs
remains challenging and often results in a small certification radius. To
address this issue, we take advantage of the multitasking nature of LLMs and
propose to denoise the corrupted inputs with LLMs in a self-denoising manner.
Different from previous works like denoised smoothing, which requires training
a separate model to robustify LLM, our method enjoys far better efficiency and
flexibility. Our experiment results show that our method outperforms the
existing certification methods under both certified robustness and empirical
robustness. The codes are available at
https://github.com/UCSB-NLP-Chang/SelfDenoise.",2307.07171v1,https://arxiv.org/pdf/2307.07171v1
"CaRT: Certified Safety and Robust Tracking in Learning-based Motion
  Planning for Multi-Agent Systems","Hiroyasu Tsukamoto, Benjamin Rivière, Changrak Choi, Amir Rahmani, Soon-Jo Chung","The key innovation of our analytical method, CaRT, lies in establishing a new
hierarchical, distributed architecture to guarantee the safety and robustness
of a given learning-based motion planning policy. First, in a nominal setting,
the analytical form of our CaRT safety filter formally ensures safe maneuvers
of nonlinear multi-agent systems, optimally with minimal deviation from the
learning-based policy. Second, in off-nominal settings, the analytical form of
our CaRT robust filter optimally tracks the certified safe trajectory,
generated by the previous layer in the hierarchy, the CaRT safety filter. We
show using contraction theory that CaRT guarantees safety and the exponential
boundedness of the trajectory tracking error, even under the presence of
deterministic and stochastic disturbance. Also, the hierarchical nature of CaRT
enables enhancing its robustness for safety just by its superior tracking to
the certified safe trajectory, thereby making it suitable for off-nominal
scenarios with large disturbances. This is a major distinction from
conventional safety function-driven approaches, where the robustness originates
from the stability of a safe set, which could pull the system
over-conservatively to the interior of the safe set. Our log-barrier
formulation in CaRT allows for its distributed implementation in multi-agent
settings. We demonstrate the effectiveness of CaRT in several examples of
nonlinear motion planning and control problems, including optimal,
multi-spacecraft reconfiguration.",2307.08602v2,https://arxiv.org/pdf/2307.08602v2
"DeepIPCv2: LiDAR-powered Robust Environmental Perception and
  Navigational Control for Autonomous Vehicle","Oskar Natan, Jun Miura","We present DeepIPCv2, an autonomous driving model that perceives the
environment using a LiDAR sensor for more robust drivability, especially when
driving under poor illumination conditions where everything is not clearly
visible. DeepIPCv2 takes a set of LiDAR point clouds as the main perception
input. Since point clouds are not affected by illumination changes, they can
provide a clear observation of the surroundings no matter what the condition
is. This results in a better scene understanding and stable features provided
by the perception module to support the controller module in estimating
navigational control properly. To evaluate its performance, we conduct several
tests by deploying the model to predict a set of driving records and perform
real automated driving under three different conditions. We also conduct
ablation and comparative studies with some recent models to justify its
performance. Based on the experimental results, DeepIPCv2 shows a robust
performance by achieving the best drivability in all driving scenarios.
Furthermore, to support future research, we will upload the codes and data to
https://github.com/oskarnatan/DeepIPCv2.",2307.06647v3,https://arxiv.org/pdf/2307.06647v3
Deep Unrolling for Nonconvex Robust Principal Component Analysis,"Elizabeth Z. C. Tan, Caroline Chaux, Emmanuel Soubies, Vincent Y. F. Tan","We design algorithms for Robust Principal Component Analysis (RPCA) which
consists in decomposing a matrix into the sum of a low rank matrix and a sparse
matrix. We propose a deep unrolled algorithm based on an accelerated
alternating projection algorithm which aims to solve RPCA in its nonconvex
form. The proposed procedure combines benefits of deep neural networks and the
interpretability of the original algorithm and it automatically learns
hyperparameters. We demonstrate the unrolled algorithm's effectiveness on
synthetic datasets and also on a face modeling problem, where it leads to both
better numerical and visual performances.",2307.05893v1,https://arxiv.org/pdf/2307.05893v1
"Stochastic Nested Compositional Bi-level Optimization for Robust Feature
  Learning","Xuxing Chen, Krishnakumar Balasubramanian, Saeed Ghadimi","We develop and analyze stochastic approximation algorithms for solving nested
compositional bi-level optimization problems. These problems involve a nested
composition of $T$ potentially non-convex smooth functions in the upper-level,
and a smooth and strongly convex function in the lower-level. Our proposed
algorithm does not rely on matrix inversions or mini-batches and can achieve an
$\epsilon$-stationary solution with an oracle complexity of approximately
$\tilde{O}_T(1/\epsilon^{2})$, assuming the availability of stochastic
first-order oracles for the individual functions in the composition and the
lower-level, which are unbiased and have bounded moments. Here, $\tilde{O}_T$
hides polylog factors and constants that depend on $T$. The key challenge we
address in establishing this result relates to handling three distinct sources
of bias in the stochastic gradients. The first source arises from the
compositional nature of the upper-level, the second stems from the bi-level
structure, and the third emerges due to the utilization of Neumann series
approximations to avoid matrix inversion. To demonstrate the effectiveness of
our approach, we apply it to the problem of robust feature learning for deep
neural networks under covariate shift, showcasing the benefits and advantages
of our methodology in that context.",2307.05384v1,https://arxiv.org/pdf/2307.05384v1
"RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named
  Entity Recognition","Sihan Song, Furao Shen, Jian Zhao","Data augmentation has been widely used in low-resource NER tasks to tackle
the problem of data sparsity. However, previous data augmentation methods have
the disadvantages of disrupted syntactic structures, token-label mismatch, and
requirement for external knowledge or manual effort. To address these issues,
we propose Robust Prompt-based Data Augmentation (RoPDA) for low-resource NER.
Based on pre-trained language models (PLMs) with continuous prompt, RoPDA
performs entity augmentation and context augmentation through five fundamental
augmentation operations to generate label-flipping and label-preserving
examples. To optimize the utilization of the augmented samples, we present two
techniques: Self-Consistency Filtering and mixup. The former effectively
eliminates low-quality samples, while the latter prevents performance
degradation arising from the direct utilization of label-flipping samples.
Extensive experiments on three benchmarks from different domains demonstrate
that RoPDA significantly improves upon strong baselines, and also outperforms
state-of-the-art semi-supervised learning methods when unlabeled data is
included.",2307.07417v2,https://arxiv.org/pdf/2307.07417v2
Enhancing Adversarial Robustness via Score-Based Optimization,"Boya Zhang, Weijian Luo, Zhihua Zhang","Adversarial attacks have the potential to mislead deep neural network
classifiers by introducing slight perturbations. Developing algorithms that can
mitigate the effects of these attacks is crucial for ensuring the safe use of
artificial intelligence. Recent studies have suggested that score-based
diffusion models are effective in adversarial defenses. However, existing
diffusion-based defenses rely on the sequential simulation of the reversed
stochastic differential equations of diffusion models, which are
computationally inefficient and yield suboptimal results. In this paper, we
introduce a novel adversarial defense scheme named ScoreOpt, which optimizes
adversarial samples at test-time, towards original clean data in the direction
guided by score-based priors. We conduct comprehensive experiments on multiple
datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results
demonstrate that our approach outperforms existing adversarial defenses in
terms of both robustness performance and inference speed.",2307.04333v3,https://arxiv.org/pdf/2307.04333v3
Robust Ranking Explanations,"Chao Chen, Chenghua Guo, Guixiang Ma, Ming Zeng, Xi Zhang, Sihong Xie","Robust explanations of machine learning models are critical to establish
human trust in the models. Due to limited cognition capability, most humans can
only interpret the top few salient features. It is critical to make top salient
features robust to adversarial attacks, especially those against the more
vulnerable gradient-based explanations. Existing defense measures robustness
using $\ell_p$-norms, which have weaker protection power. We define explanation
thickness for measuring salient features ranking stability, and derive
tractable surrogate bounds of the thickness to design the \textit{R2ET}
algorithm to efficiently maximize the thickness and anchor top salient
features. Theoretically, we prove a connection between R2ET and adversarial
training. Experiments with a wide spectrum of network architectures and data
modalities, including brain networks, demonstrate that R2ET attains higher
explanation robustness under stealthy attacks while retaining accuracy.",2307.04024v1,https://arxiv.org/pdf/2307.04024v1
"Robust Learning-Based Incipient Slip Detection using the PapillArray
  Optical Tactile Sensor for Improved Robotic Gripping","Qiang Wang, Pablo Martinez Ulloa, Robert Burke, David Cordova Bulens, Stephen J. Redmond","The ability to detect slip, particularly incipient slip, enables robotic
systems to take corrective measures to prevent a grasped object from being
dropped. Therefore, slip detection can enhance the overall security of robotic
gripping. However, accurately detecting incipient slip remains a significant
challenge. In this paper, we propose a novel learning-based approach to detect
incipient slip using the PapillArray (Contactile, Australia) tactile sensor.
The resulting model is highly effective in identifying patterns associated with
incipient slip, achieving a detection success rate of 95.6% when tested with an
offline dataset. Furthermore, we introduce several data augmentation methods to
enhance the robustness of our model. When transferring the trained model to a
robotic gripping environment distinct from where the training data was
collected, our model maintained robust performance, with a success rate of
96.8%, providing timely feedback for stabilizing several practical gripping
tasks. Our project website:
https://sites.google.com/view/incipient-slip-detection.",2307.04011v1,https://arxiv.org/pdf/2307.04011v1
RADAR: Robust AI-Text Detection via Adversarial Learning,"Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho","Recent advances in large language models (LLMs) and the intensifying
popularity of ChatGPT-like applications have blurred the boundary of
high-quality text generation between humans and machines. However, in addition
to the anticipated revolutionary changes to our technology and society, the
difficulty of distinguishing LLM-generated texts (AI-text) from human-generated
texts poses new challenges of misuse and fairness, such as fake content
generation, plagiarism, and false accusations of innocent writers. While
existing works show that current AI-text detectors are not robust to LLM-based
paraphrasing, this paper aims to bridge this gap by proposing a new framework
called RADAR, which jointly trains a robust AI-text detector via adversarial
learning. RADAR is based on adversarial training of a paraphraser and a
detector. The paraphraser's goal is to generate realistic content to evade
AI-text detection. RADAR uses the feedback from the detector to update the
paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly
2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,
experimental results show that RADAR significantly outperforms existing AI-text
detection methods, especially when paraphrasing is in place. We also identify
the strong transferability of RADAR from instruction-tuned LLMs to other LLMs,
and evaluate the improved capability of RADAR via GPT-3.5-Turbo.",2307.03838v2,https://arxiv.org/pdf/2307.03838v2
"A Theoretical Perspective on Subnetwork Contributions to Adversarial
  Robustness","Jovon Craig, Josh Andle, Theodore S. Nowak, Salimeh Yasaei Sekeh","The robustness of deep neural networks (DNNs) against adversarial attacks has
been studied extensively in hopes of both better understanding how deep
learning models converge and in order to ensure the security of these models in
safety-critical applications. Adversarial training is one approach to
strengthening DNNs against adversarial attacks, and has been shown to offer a
means for doing so at the cost of applying computationally expensive training
methods to the entire model. To better understand these attacks and facilitate
more efficient adversarial training, in this paper we develop a novel
theoretical framework that investigates how the adversarial robustness of a
subnetwork contributes to the robustness of the entire network. To do so we
first introduce the concept of semirobustness, which is a measure of the
adversarial robustness of a subnetwork. Building on this concept, we then
provide a theoretical analysis to show that if a subnetwork is semirobust and
there is a sufficient dependency between it and each subsequent layer in the
network, then the remaining layers are also guaranteed to be robust. We
validate these findings empirically across multiple DNN architectures,
datasets, and adversarial attacks. Experiments show the ability of a robust
subnetwork to promote full-network robustness, and investigate the layer-wise
dependencies required for this full-network robustness to be achieved.",2307.03803v1,https://arxiv.org/pdf/2307.03803v1
"Physical Color Calibration of Digital Pathology Scanners for Robust
  Artificial Intelligence Assisted Cancer Diagnosis","Xiaoyi Ji, Richard Salmon, Nita Mulliqi, Umair Khan, Yinxi Wang, Anders Blilie, Henrik Olsson, Bodil Ginnerup Pedersen, Karina Dalsgaard Sørensen, Benedicte Parm Ulhøi, Svein R Kjosavik, Emilius AM Janssen, Mattias Rantalainen, Lars Egevad, Pekka Ruusuvuori, Martin Eklund, Kimmo Kartasalo","The potential of artificial intelligence (AI) in digital pathology is limited
by technical inconsistencies in the production of whole slide images (WSIs),
leading to degraded AI performance and posing a challenge for widespread
clinical application as fine-tuning algorithms for each new site is
impractical. Changes in the imaging workflow can also lead to compromised
diagnoses and patient safety risks. We evaluated whether physical color
calibration of scanners can standardize WSI appearance and enable robust AI
performance. We employed a color calibration slide in four different
laboratories and evaluated its impact on the performance of an AI system for
prostate cancer diagnosis on 1,161 WSIs. Color standardization resulted in
consistently improved AI model calibration and significant improvements in
Gleason grading performance. The study demonstrates that physical color
calibration provides a potential solution to the variation introduced by
different scanners, making AI-based cancer diagnostics more reliable and
applicable in clinical settings.",2307.05519v1,https://arxiv.org/pdf/2307.05519v1
RCDN -- Robust X-Corner Detection Algorithm based on Advanced CNN Model,"Ben Chen, Caihua Xiong, Quanlin Li, Zhonghua Wan","Accurate detection and localization of X-corner on both planar and non-planar
patterns is a core step in robotics and machine vision. However, previous works
could not make a good balance between accuracy and robustness, which are both
crucial criteria to evaluate the detectors performance. To address this
problem, in this paper we present a novel detection algorithm which can
maintain high sub-pixel precision on inputs under multiple interference, such
as lens distortion, extreme poses and noise. The whole algorithm, adopting a
coarse-to-fine strategy, contains a X-corner detection network and three
post-processing techniques to distinguish the correct corner candidates, as
well as a mixed sub-pixel refinement technique and an improved region growth
strategy to recover the checkerboard pattern partially visible or occluded
automatically. Evaluations on real and synthetic images indicate that the
presented algorithm has the higher detection rate, sub-pixel accuracy and
robustness than other commonly used methods. Finally, experiments of camera
calibration and pose estimation verify it can also get smaller re-projection
error in quantitative comparisons to the state-of-the-art.",2307.03505v1,https://arxiv.org/pdf/2307.03505v1
"LEA: Improving Sentence Similarity Robustness to Typos Using Lexical
  Attention Bias","Mario Almagro, Emilio Almazán, Diego Ortego, David Jiménez","Textual noise, such as typos or abbreviations, is a well-known issue that
penalizes vanilla Transformers for most downstream tasks. We show that this is
also the case for sentence similarity, a fundamental task in multiple domains,
e.g. matching, retrieval or paraphrasing. Sentence similarity can be approached
using cross-encoders, where the two sentences are concatenated in the input
allowing the model to exploit the inter-relations between them. Previous works
addressing the noise issue mainly rely on data augmentation strategies, showing
improved robustness when dealing with corrupted samples that are similar to the
ones used for training. However, all these methods still suffer from the token
distribution shift induced by typos. In this work, we propose to tackle textual
noise by equipping cross-encoders with a novel LExical-aware Attention module
(LEA) that incorporates lexical similarities between words in both sentences.
By using raw text similarities, our approach avoids the tokenization shift
problem obtaining improved robustness. We demonstrate that the attention bias
introduced by LEA helps cross-encoders to tackle complex scenarios with textual
noise, specially in domains with short-text descriptions and limited context.
Experiments using three popular Transformer encoders in five e-commerce
datasets for product matching show that LEA consistently boosts performance
under the presence of noise, while remaining competitive on the original
(clean) splits. We also evaluate our approach in two datasets for textual
entailment and paraphrasing showing that LEA is robust to typos in domains with
longer sentences and more natural context. Additionally, we thoroughly analyze
several design choices in our approach, providing insights about the impact of
the decisions made and fostering future research in cross-encoders dealing with
typos.",2307.02912v1,https://arxiv.org/pdf/2307.02912v1
"Transgressing the boundaries: towards a rigorous understanding of deep
  learning and its (non-)robustness","Carsten Hartmann, Lorenz Richter","The recent advances in machine learning in various fields of applications can
be largely attributed to the rise of deep learning (DL) methods and
architectures. Despite being a key technology behind autonomous cars, image
processing, speech recognition, etc., a notorious problem remains the lack of
theoretical understanding of DL and related interpretability and (adversarial)
robustness issues. Understanding the specifics of DL, as compared to, say,
other forms of nonlinear regression methods or statistical learning, is
interesting from a mathematical perspective, but at the same time it is of
crucial importance in practice: treating neural networks as mere black boxes
might be sufficient in certain cases, but many applications require waterproof
performance guarantees and a deeper understanding of what could go wrong and
why it could go wrong. It is probably fair to say that, despite being
mathematically well founded as a method to approximate complicated functions,
DL is mostly still more like modern alchemy that is firmly in the hands of
engineers and computer scientists. Nevertheless, it is evident that certain
specifics of DL that could explain its success in applications demands
systematic mathematical approaches. In this work, we review robustness issues
of DL and particularly bridge concerns and attempts from approximation theory
to statistical learning theory. Further, we review Bayesian Deep Learning as a
means for uncertainty quantification and rigorous explainability.",2307.02454v1,https://arxiv.org/pdf/2307.02454v1
"On the Adversarial Robustness of Generative Autoencoders in the Latent
  Space","Mingfei Lu, Badong Chen","The generative autoencoders, such as the variational autoencoders or the
adversarial autoencoders, have achieved great success in lots of real-world
applications, including image generation, and signal communication.
  However, little concern has been devoted to their robustness during practical
deployment.
  Due to the probabilistic latent structure, variational autoencoders (VAEs)
may confront problems such as a mismatch between the posterior distribution of
the latent and real data manifold, or discontinuity in the posterior
distribution of the latent.
  This leaves a back door for malicious attackers to collapse VAEs from the
latent space, especially in scenarios where the encoder and decoder are used
separately, such as communication and compressed sensing.
  In this work, we provide the first study on the adversarial robustness of
generative autoencoders in the latent space.
  Specifically, we empirically demonstrate the latent vulnerability of popular
generative autoencoders through attacks in the latent space.
  We also evaluate the difference between variational autoencoders and their
deterministic variants and observe that the latter performs better in latent
robustness.
  Meanwhile, we identify a potential trade-off between the adversarial
robustness and the degree of the disentanglement of the latent codes.
  Additionally, we also verify the feasibility of improvement for the latent
robustness of VAEs through adversarial training.
  In summary, we suggest concerning the adversarial latent robustness of the
generative autoencoders, analyze several robustness-relative issues, and give
some insights into a series of key challenges.",2307.02202v1,https://arxiv.org/pdf/2307.02202v1
"Robust Graph Structure Learning with the Alignment of Features and
  Adjacency Matrix","Shaogao Lv, Gang Wen, Shiyu Liu, Linsen Wei, Ming Li","To improve the robustness of graph neural networks (GNN), graph structure
learning (GSL) has attracted great interest due to the pervasiveness of noise
in graph data. Many approaches have been proposed for GSL to jointly learn a
clean graph structure and corresponding representations. To extend the previous
work, this paper proposes a novel regularized GSL approach, particularly with
an alignment of feature information and graph information, which is motivated
mainly by our derived lower bound of node-level Rademacher complexity for GNNs.
Additionally, our proposed approach incorporates sparse dimensional reduction
to leverage low-dimensional node features that are relevant to the graph
structure. To evaluate the effectiveness of our approach, we conduct
experiments on real-world graphs. The results demonstrate that our proposed GSL
method outperforms several competitive baselines, especially in scenarios where
the graph structures are heavily affected by noise. Overall, our research
highlights the importance of integrating feature and graph information
alignment in GSL, as inspired by our derived theoretical result, and showcases
the superiority of our approach in handling noisy graph structures through
comprehensive experiments on real-world datasets.",2307.02126v1,https://arxiv.org/pdf/2307.02126v1
"DARE: Towards Robust Text Explanations in Biomedical and Healthcare
  Applications","Adam Ivankay, Mattia Rigotti, Pascal Frossard","Along with the successful deployment of deep neural networks in several
application domains, the need to unravel the black-box nature of these networks
has seen a significant increase recently. Several methods have been introduced
to provide insight into the inference process of deep neural networks. However,
most of these explainability methods have been shown to be brittle in the face
of adversarial perturbations of their inputs in the image and generic textual
domain. In this work we show that this phenomenon extends to specific and
important high stakes domains like biomedical datasets. In particular, we
observe that the robustness of explanations should be characterized in terms of
the accuracy of the explanation in linking a model's inputs and its decisions -
faithfulness - and its relevance from the perspective of domain experts -
plausibility. This is crucial to prevent explanations that are inaccurate but
still look convincing in the context of the domain at hand. To this end, we
show how to adapt current attribution robustness estimation methods to a given
domain, so as to take into account domain-specific plausibility. This results
in our DomainAdaptiveAREstimator (DARE) attribution robustness estimator,
allowing us to properly characterize the domain-specific robustness of faithful
explanations. Next, we provide two methods, adversarial training and FAR
training, to mitigate the brittleness characterized by DARE, allowing us to
train networks that display robust attributions. Finally, we empirically
validate our methods with extensive experiments on three established biomedical
benchmarks.",2307.02094v1,https://arxiv.org/pdf/2307.02094v1
FEMDA: Une méthode de classification robuste et flexible,"Pierre Houdouin, Matthieu Jonckheere, Frederic Pascal","Linear and Quadratic Discriminant Analysis (LDA and QDA) are well-known
classical methods but can heavily suffer from non-Gaussian distributions and/or
contaminated datasets, mainly because of the underlying Gaussian assumption
that is not robust. This paper studies the robustness to scale changes in the
data of a new discriminant analysis technique where each data point is drawn by
its own arbitrary Elliptically Symmetrical (ES) distribution and its own
arbitrary scale parameter. Such a model allows for possibly very heterogeneous,
independent but non-identically distributed samples. The new decision rule
derived is simple, fast, and robust to scale changes in the data compared to
other state-of-the-art method",2307.01954v1,https://arxiv.org/pdf/2307.01954v1
"FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor
  Localization with Mobile Devices","Danish Gufran, Sudeep Pasricha","Indoor localization plays a vital role in applications such as emergency
response, warehouse management, and augmented reality experiences. By deploying
machine learning (ML) based indoor localization frameworks on their mobile
devices, users can localize themselves in a variety of indoor and subterranean
environments. However, achieving accurate indoor localization can be
challenging due to heterogeneity in the hardware and software stacks of mobile
devices, which can result in inconsistent and inaccurate location estimates.
Traditional ML models also heavily rely on initial training data, making them
vulnerable to degradation in performance with dynamic changes across indoor
environments. To address the challenges due to device heterogeneity and lack of
adaptivity, we propose a novel embedded ML framework called FedHIL. Our
framework combines indoor localization and federated learning (FL) to improve
indoor localization accuracy in device-heterogeneous environments while also
preserving user data privacy. FedHIL integrates a domain-specific selective
weight adjustment approach to preserve the ML model's performance for indoor
localization during FL, even in the presence of extremely noisy data.
Experimental evaluations in diverse real-world indoor environments and with
heterogeneous mobile devices show that FedHIL outperforms state-of-the-art FL
and non-FL indoor localization frameworks. FedHIL is able to achieve 1.62x
better localization accuracy on average than the best performing FL-based
indoor localization framework from prior work.",2307.01780v1,https://arxiv.org/pdf/2307.01780v1
"LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via
  Latent Ensemble Attack","Joonkyo Shim, Hyunsoo Yoon","Deepfakes, malicious visual contents created by generative models, pose an
increasingly harmful threat to society. To proactively mitigate deepfake
damages, recent studies have employed adversarial perturbation to disrupt
deepfake model outputs. However, previous approaches primarily focus on
generating distorted outputs based on only predetermined target attributes,
leading to a lack of robustness in real-world scenarios where target attributes
are unknown. Additionally, the transferability of perturbations between two
prominent generative models, Generative Adversarial Networks (GANs) and
Diffusion Models, remains unexplored. In this paper, we emphasize the
importance of target attribute-transferability and model-transferability for
achieving robust deepfake disruption. To address this challenge, we propose a
simple yet effective disruption method called Latent Ensemble ATtack (LEAT),
which attacks the independent latent encoding process. By disrupting the latent
encoding process, it generates distorted output images in subsequent generation
processes, regardless of the given target attributes. This target
attribute-agnostic attack ensures robust disruption even when the target
attributes are unknown. Additionally, we introduce a Normalized Gradient
Ensemble strategy that effectively aggregates gradients for iterative gradient
attacks, enabling simultaneous attacks on various types of deepfake models,
involving both GAN-based and Diffusion-based models. Moreover, we demonstrate
the insufficiency of evaluating disruption quality solely based on pixel-level
differences. As a result, we propose an alternative protocol for
comprehensively evaluating the success of defense. Extensive experiments
confirm the efficacy of our method in disrupting deepfakes in real-world
scenarios, reporting a higher defense success rate compared to previous
methods.",2307.01520v1,https://arxiv.org/pdf/2307.01520v1
"Analyzing the vulnerabilities in SplitFed Learning: Assessing the
  robustness against Data Poisoning Attacks","Aysha Thahsin Zahir Ismail, Raj Mani Shukla","Distributed Collaborative Machine Learning (DCML) is a potential alternative
to address the privacy concerns associated with centralized machine learning.
The Split learning (SL) and Federated Learning (FL) are the two effective
learning approaches in DCML. Recently there have been an increased interest on
the hybrid of FL and SL known as the SplitFed Learning (SFL). This research is
the earliest attempt to study, analyze and present the impact of data poisoning
attacks in SFL. We propose three kinds of novel attack strategies namely
untargeted, targeted and distance-based attacks for SFL. All the attacks
strategies aim to degrade the performance of the DCML-based classifier. We test
the proposed attack strategies for two different case studies on
Electrocardiogram signal classification and automatic handwritten digit
recognition. A series of attack experiments were conducted by varying the
percentage of malicious clients and the choice of the model split layer between
the clients and the server. The results after the comprehensive analysis of
attack strategies clearly convey that untargeted and distance-based poisoning
attacks have greater impacts in evading the classifier outcomes compared to
targeted attacks in SFL",2307.03197v1,https://arxiv.org/pdf/2307.03197v1
Robust Uncertainty Estimation for Classification of Maritime Objects,"Jonathan Becktor, Frederik Scholler, Evangelos Boukas, Lazaros Nalpantidis","We explore the use of uncertainty estimation in the maritime domain, showing
the efficacy on toy datasets (CIFAR10) and proving it on an in-house dataset,
SHIPS. We present a method joining the intra-class uncertainty achieved using
Monte Carlo Dropout, with recent discoveries in the field of outlier detection,
to gain more holistic uncertainty measures. We explore the relationship between
the introduced uncertainty measures and examine how well they work on CIFAR10
and in a real-life setting. Our work improves the FPR95 by 8% compared to the
current highest-performing work when the models are trained without
out-of-distribution data. We increase the performance by 77% compared to a
vanilla implementation of the Wide ResNet. We release the SHIPS dataset and
show the effectiveness of our method by improving the FPR95 by 44.2% with
respect to the baseline. Our approach is model agnostic, easy to implement, and
often does not require model retraining.",2307.01325v1,https://arxiv.org/pdf/2307.01325v1
"What Distributions are Robust to Indiscriminate Poisoning Attacks for
  Linear Learners?","Fnu Suya, Xiao Zhang, Yuan Tian, David Evans","We study indiscriminate poisoning for linear learners where an adversary
injects a few crafted examples into the training data with the goal of forcing
the induced model to incur higher test error. Inspired by the observation that
linear learners on some datasets are able to resist the best known attacks even
without any defenses, we further investigate whether datasets can be inherently
robust to indiscriminate poisoning attacks for linear learners. For theoretical
Gaussian distributions, we rigorously characterize the behavior of an optimal
poisoning attack, defined as the poisoning strategy that attains the maximum
risk of the induced model at a given poisoning budget. Our results prove that
linear learners can indeed be robust to indiscriminate poisoning if the
class-wise data distributions are well-separated with low variance and the size
of the constraint set containing all permissible poisoning points is also
small. These findings largely explain the drastic variation in empirical attack
performance of the state-of-the-art poisoning attacks on linear learners across
benchmark datasets, making an important initial step towards understanding the
underlying reasons some learning tasks are vulnerable to data poisoning
attacks.",2307.01073v2,https://arxiv.org/pdf/2307.01073v2
Enhancing the Robustness of QMIX against State-adversarial Attacks,"Weiran Guo, Guanjun Liu, Ziyuan Zhou, Ling Wang, Jiacun Wang","Deep reinforcement learning (DRL) performance is generally impacted by
state-adversarial attacks, a perturbation applied to an agent's observation.
Most recent research has concentrated on robust single-agent reinforcement
learning (SARL) algorithms against state-adversarial attacks. Still, there has
yet to be much work on robust multi-agent reinforcement learning. Using QMIX,
one of the popular cooperative multi-agent reinforcement algorithms, as an
example, we discuss four techniques to improve the robustness of SARL
algorithms and extend them to multi-agent scenarios. To increase the robustness
of multi-agent reinforcement learning (MARL) algorithms, we train models using
a variety of attacks in this research. We then test the models taught using the
other attacks by subjecting them to the corresponding attacks throughout the
training phase. In this way, we organize and summarize techniques for enhancing
robustness when used with MARL.",2307.00907v1,https://arxiv.org/pdf/2307.00907v1
"RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting
  Self-Supervised Representations","Neha Sahipjohn, Neil Shah, Vishal Tambrahalli, Vineet Gandhi","Significant progress has been made in speaker dependent Lip-to-Speech
synthesis, which aims to generate speech from silent videos of talking faces.
Current state-of-the-art approaches primarily employ non-autoregressive
sequence-to-sequence architectures to directly predict mel-spectrograms or
audio waveforms from lip representations. We hypothesize that the direct
mel-prediction hampers training/model efficiency due to the entanglement of
speech content with ambient information and speaker characteristics. To this
end, we propose RobustL2S, a modularized framework for Lip-to-Speech synthesis.
First, a non-autoregressive sequence-to-sequence model maps self-supervised
visual features to a representation of disentangled speech content. A vocoder
then converts the speech features into raw waveforms. Extensive evaluations
confirm the effectiveness of our setup, achieving state-of-the-art performance
on the unconstrained Lip2Wav dataset and the constrained GRID and TCD-TIMIT
datasets. Speech samples from RobustL2S can be found at
https://neha-sherin.github.io/RobustL2S/",2307.01233v1,https://arxiv.org/pdf/2307.01233v1
Robust Surgical Tools Detection in Endoscopic Videos with Noisy Data,"Adnan Qayyum, Hassan Ali, Massimo Caputo, Hunaid Vohra, Taofeek Akinosho, Sofiat Abioye, Ilhem Berrou, Paweł Capik, Junaid Qadir, Muhammad Bilal","Over the past few years, surgical data science has attracted substantial
interest from the machine learning (ML) community. Various studies have
demonstrated the efficacy of emerging ML techniques in analysing surgical data,
particularly recordings of procedures, for digitizing clinical and non-clinical
functions like preoperative planning, context-aware decision-making, and
operating skill assessment. However, this field is still in its infancy and
lacks representative, well-annotated datasets for training robust models in
intermediate ML tasks. Also, existing datasets suffer from inaccurate labels,
hindering the development of reliable models. In this paper, we propose a
systematic methodology for developing robust models for surgical tool detection
using noisy data. Our methodology introduces two key innovations: (1) an
intelligent active learning strategy for minimal dataset identification and
label correction by human experts; and (2) an assembling strategy for a
student-teacher model-based self-training framework to achieve the robust
classification of 14 surgical tools in a semi-supervised fashion. Furthermore,
we employ weighted data loaders to handle difficult class labels and address
class imbalance issues. The proposed methodology achieves an average F1-score
of 85.88\% for the ensemble model-based self-training with class weights, and
80.88\% without class weights for noisy labels. Also, our proposed method
significantly outperforms existing approaches, which effectively demonstrates
its effectiveness.",2307.01232v1,https://arxiv.org/pdf/2307.01232v1
"Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for
  Robust 3D Medical Image Segmentation","Jingjie Guo, Weitong Zhang, Matthew Sinclair, Daniel Rueckert, Chen Chen","Convolutional neural networks (CNNs) often suffer from poor performance when
tested on target data that differs from the training (source) data
distribution, particularly in medical imaging applications where variations in
imaging protocols across different clinical sites and scanners lead to
different imaging appearances. However, re-accessing source training data for
unsupervised domain adaptation or labeling additional test data for model
fine-tuning can be difficult due to privacy issues and high labeling costs,
respectively. To solve this problem, we propose a novel atlas-guided test-time
adaptation (TTA) method for robust 3D medical image segmentation, called
AdaAtlas. AdaAtlas only takes one single unlabeled test sample as input and
adapts the segmentation network by minimizing an atlas-based loss.
Specifically, the network is adapted so that its prediction after registration
is aligned with the learned atlas in the atlas space, which helps to reduce
anatomical segmentation errors at test time. In addition, different from most
existing TTA methods which restrict the adaptation to batch normalization
blocks in the segmentation network only, we further exploit the use of channel
and spatial attention blocks for improved adaptability at test time. Extensive
experiments on multiple datasets from different sites show that AdaAtlas with
attention blocks adapted (AdaAtlas-Attention) achieves superior performance
improvements, greatly outperforming other competitive TTA methods.",2307.00676v2,https://arxiv.org/pdf/2307.00676v2
"More for Less: Compact Convolutional Transformers Enable Robust Medical
  Image Classification with Limited Data",Andrew Kean Gao,"Transformers are very powerful tools for a variety of tasks across domains,
from text generation to image captioning. However, transformers require
substantial amounts of training data, which is often a challenge in biomedical
settings, where high quality labeled data can be challenging or expensive to
obtain. This study investigates the efficacy of Compact Convolutional
Transformers (CCT) for robust medical image classification with limited data,
addressing a key issue faced by conventional Vision Transformers - their
requirement for large datasets. A hybrid of transformers and convolutional
layers, CCTs demonstrate high accuracy on modestly sized datasets. We employed
a benchmark dataset of peripheral blood cell images of eight distinct cell
types, each represented by approximately 2,000 low-resolution (28x28x3 pixel)
samples. Despite the dataset size being smaller than those typically used with
Vision Transformers, we achieved a commendable classification accuracy of
92.49% and a micro-average ROC AUC of 0.9935. The CCT also learned quickly,
exceeding 80% validation accuracy after five epochs. Analysis of per-class
precision, recall, F1, and ROC showed that performance was strong across cell
types. Our findings underscore the robustness of CCTs, indicating their
potential as a solution to data scarcity issues prevalent in biomedical
imaging. We substantiate the applicability of CCTs in data-constrained areas
and encourage further work on CCTs.",2307.00213v1,https://arxiv.org/pdf/2307.00213v1
"Automatic Counterfactual Augmentation for Robust Text Classification
  Based on Word-Group Search","Rui Song, Fausto Giunchiglia, Yingji Li, Hao Xu","Despite large-scale pre-trained language models have achieved striking
results for text classificaion, recent work has raised concerns about the
challenge of shortcut learning. In general, a keyword is regarded as a shortcut
if it creates a superficial association with the label, resulting in a false
prediction. Conversely, shortcut learning can be mitigated if the model relies
on robust causal features that help produce sound predictions. To this end,
many studies have explored post-hoc interpretable methods to mine shortcuts and
causal features for robustness and generalization. However, most existing
methods focus only on single word in a sentence and lack consideration of
word-group, leading to wrong causal features. To solve this problem, we propose
a new Word-Group mining approach, which captures the causal effect of any
keyword combination and orders the combinations that most affect the
prediction. Our approach bases on effective post-hoc analysis and beam search,
which ensures the mining effect and reduces the complexity. Then, we build a
counterfactual augmentation method based on the multiple word-groups, and use
an adaptive voting mechanism to learn the influence of different augmentated
samples on the prediction results, so as to force the model to pay attention to
effective causal features. We demonstrate the effectiveness of the proposed
method by several tasks on 8 affective review datasets and 4 toxic language
datasets, including cross-domain text classificaion, text attack and gender
fairness test.",2307.01214v1,https://arxiv.org/pdf/2307.01214v1
Provable Robust Watermarking for AI-Generated Text,"Xuandong Zhao, Prabhanjan Ananth, Lei Li, Yu-Xiang Wang","We study the problem of watermarking large language models (LLMs) generated
text -- one of the most promising approaches for addressing the safety
challenges of LLM usage. In this paper, we propose a rigorous theoretical
framework to quantify the effectiveness and robustness of LLM watermarks. We
propose a robust and high-quality watermark method, Unigram-Watermark, by
extending an existing approach with a simplified fixed grouping strategy. We
prove that our watermark method enjoys guaranteed generation quality,
correctness in watermark detection, and is robust against text editing and
paraphrasing. Experiments on three varying LLMs and two datasets verify that
our Unigram-Watermark achieves superior detection accuracy and comparable
generation quality in perplexity, thus promoting the responsible use of LLMs.
Code is available at https://github.com/XuandongZhao/Unigram-Watermark.",2306.17439v2,https://arxiv.org/pdf/2306.17439v2
Fast and Robust State Estimation and Tracking via Hierarchical Learning,"Connor Mclaughlin, Matthew Ding, Deniz Edogmus, Lili Su","Fully distributed estimation and tracking solutions to large-scale
multi-agent networks suffer slow convergence and are vulnerable to network
failures. In this paper, we aim to speed up the convergence and enhance the
resilience of state estimation and tracking using a simple hierarchical system
architecture wherein agents are clusters into smaller networks, and a parameter
server exists to aid the information exchanges among networks. The information
exchange among networks is expensive and occurs only once in a while.
  We propose two consensus + innovation algorithms for the state estimation and
tracking problems, respectively. In both algorithms, we use a novel
hierarchical push-sum consensus component. For the state estimation, we use
dual averaging as the local innovation component. State tracking is much harder
to tackle in the presence of dropping-link failures and the standard
integration of the consensus and innovation approaches are no longer
applicable. Moreover, dual averaging is no longer feasible. Our algorithm
introduces a pair of additional variables per link and ensure the relevant
local variables evolve according to the state dynamics, and use projected local
gradient descent as the local innovation component. We also characterize the
convergence rates of both of the algorithms under linear local observation
model and minimal technical assumptions. We numerically validate our algorithm
through simulation of both state estimation and tracking problems.",2306.17267v1,https://arxiv.org/pdf/2306.17267v1
The Importance of Robust Features in Mitigating Catastrophic Forgetting,"Hikmat Khan, Nidhal C. Bouaynaya, Ghulam Rasool","Continual learning (CL) is an approach to address catastrophic forgetting,
which refers to forgetting previously learned knowledge by neural networks when
trained on new tasks or data distributions. The adversarial robustness has
decomposed features into robust and non-robust types and demonstrated that
models trained on robust features significantly enhance adversarial robustness.
However, no study has been conducted on the efficacy of robust features from
the lens of the CL model in mitigating catastrophic forgetting in CL. In this
paper, we introduce the CL robust dataset and train four baseline models on
both the standard and CL robust datasets. Our results demonstrate that the CL
models trained on the CL robust dataset experienced less catastrophic
forgetting of the previously learned tasks than when trained on the standard
dataset. Our observations highlight the significance of the features provided
to the underlying CL models, showing that CL robust features can alleviate
catastrophic forgetting.",2306.17091v1,https://arxiv.org/pdf/2306.17091v1
Evaluating Paraphrastic Robustness in Textual Entailment Models,"Dhruv Verma, Yash Kumar Lal, Shreyashee Sinha, Benjamin Van Durme, Adam Poliak","We present PaRTE, a collection of 1,126 pairs of Recognizing Textual
Entailment (RTE) examples to evaluate whether models are robust to
paraphrasing. We posit that if RTE models understand language, their
predictions should be consistent across inputs that share the same meaning. We
use the evaluation set to determine if RTE models' predictions change when
examples are paraphrased. In our experiments, contemporary models change their
predictions on 8-16\% of paraphrased examples, indicating that there is still
room for improvement.",2306.16722v1,https://arxiv.org/pdf/2306.16722v1
"Group-based Robustness: A General Framework for Customized Robustness in
  the Real World","Weiran Lin, Keane Lucas, Neo Eyal, Lujo Bauer, Michael K. Reiter, Mahmood Sharif","Machine-learning models are known to be vulnerable to evasion attacks that
perturb model inputs to induce misclassifications. In this work, we identify
real-world scenarios where the true threat cannot be assessed accurately by
existing attacks. Specifically, we find that conventional metrics measuring
targeted and untargeted robustness do not appropriately reflect a model's
ability to withstand attacks from one set of source classes to another set of
target classes. To address the shortcomings of existing methods, we formally
define a new metric, termed group-based robustness, that complements existing
metrics and is better-suited for evaluating model performance in certain attack
scenarios. We show empirically that group-based robustness allows us to
distinguish between models' vulnerability against specific threat models in
situations where traditional robustness metrics do not apply. Moreover, to
measure group-based robustness efficiently and accurately, we 1) propose two
loss functions and 2) identify three new attack strategies. We show empirically
that with comparable success rates, finding evasive samples using our new loss
functions saves computation by a factor as large as the number of targeted
classes, and finding evasive samples using our new attack strategies saves time
by up to 99\% compared to brute-force search methods. Finally, we propose a
defense method that increases group-based robustness by up to 3.52$\times$.",2306.16614v3,https://arxiv.org/pdf/2306.16614v3
"Does Saliency-Based Training bring Robustness for Deep Neural Networks
  in Image Classification?",Ali Karkehabadi,"Deep Neural Networks are powerful tools to understand complex patterns and
making decisions. However, their black-box nature impedes a complete
understanding of their inner workings. While online saliency-guided training
methods try to highlight the prominent features in the model's output to
alleviate this problem, it is still ambiguous if the visually explainable
features align with robustness of the model against adversarial examples. In
this paper, we investigate the saliency trained model's vulnerability to
adversarial examples methods. Models are trained using an online
saliency-guided training method and evaluated against popular algorithms of
adversarial examples. We quantify the robustness and conclude that despite the
well-explained visualizations in the model's output, the salient models suffer
from the lower performance against adversarial examples attacks.",2306.16581v1,https://arxiv.org/pdf/2306.16581v1
"Non-Convex Optimizations for Machine Learning with Theoretical
  Guarantee: Robust Matrix Completion and Neural Network Learning",Shuai Zhang,"Despite the recent development in machine learning, most learning systems are
still under the concept of ""black box"", where the performance cannot be
understood and derived. With the rise of safety and privacy concerns in public,
designing an explainable learning system has become a new trend in machine
learning. In general, many machine learning problems are formulated as
minimizing (or maximizing) some loss function. Since real data are most likely
generated from non-linear models, the loss function is non-convex in general.
Unlike the convex optimization problem, gradient descent algorithms will be
trapped in spurious local minima in solving non-convex optimization. Therefore,
it is challenging to provide explainable algorithms when studying non-convex
optimization problems. In this thesis, two popular non-convex problems are
studied: (1) low-rank matrix completion and (2) neural network learning.",2306.16557v1,https://arxiv.org/pdf/2306.16557v1
"Mitigating Accuracy-Robustness Trade-off via Balanced Multi-Teacher
  Adversarial Distillation","Shiji Zhao, Xizhe Wang, Xingxing Wei","Adversarial Training is a practical approach for improving the robustness of
deep neural networks against adversarial attacks. Although bringing reliable
robustness, the performance towards clean examples is negatively affected after
Adversarial Training, which means a trade-off exists between accuracy and
robustness. Recently, some studies have tried to use knowledge distillation
methods in Adversarial Training, achieving competitive performance in improving
the robustness but the accuracy for clean samples is still limited. In this
paper, to mitigate the accuracy-robustness trade-off, we introduce the Balanced
Multi-Teacher Adversarial Robustness Distillation (B-MTARD) to guide the
model's Adversarial Training process by applying a strong clean teacher and a
strong robust teacher to handle the clean examples and adversarial examples,
respectively. During the optimization process, to ensure that different
teachers show similar knowledge scales, we design the Entropy-Based Balance
algorithm to adjust the teacher's temperature and keep the teachers'
information entropy consistent. Besides, to ensure that the student has a
relatively consistent learning speed from multiple teachers, we propose the
Normalization Loss Balance algorithm to adjust the learning weights of
different types of knowledge. A series of experiments conducted on three public
datasets demonstrate that B-MTARD outperforms the state-of-the-art methods
against various adversarial attacks.",2306.16170v3,https://arxiv.org/pdf/2306.16170v3
"Evaluating Similitude and Robustness of Deep Image Denoising Models via
  Adversarial Attack","Jie Ning, Jiebao Sun, Yao Li, Zhichang Guo, Wangmeng Zuo","Deep neural networks (DNNs) have shown superior performance comparing to
traditional image denoising algorithms. However, DNNs are inevitably vulnerable
while facing adversarial attacks. In this paper, we propose an adversarial
attack method named denoising-PGD which can successfully attack all the current
deep denoising models while keep the noise distribution almost unchanged. We
surprisingly find that the current mainstream non-blind denoising models
(DnCNN, FFDNet, ECNDNet, BRDNet), blind denoising models (DnCNN-B, Noise2Noise,
RDDCNN-B, FAN), plug-and-play (DPIR, CurvPnP) and unfolding denoising models
(DeamNet) almost share the same adversarial sample set on both grayscale and
color images, respectively. Shared adversarial sample set indicates that all
these models are similar in term of local behaviors at the neighborhood of all
the test samples. Thus, we further propose an indicator to measure the local
similarity of models, called robustness similitude. Non-blind denoising models
are found to have high robustness similitude across each other, while
hybrid-driven models are also found to have high robustness similitude with
pure data-driven non-blind denoising models. According to our robustness
assessment, data-driven non-blind denoising models are the most robust. We use
adversarial training to complement the vulnerability to adversarial attacks.
Moreover, the model-driven image denoising BM3D shows resistance on adversarial
attacks.",2306.16050v2,https://arxiv.org/pdf/2306.16050v2
A Population-Level Analysis of Neural Dynamics in Robust Legged Robots,"Eugene R. Rush, Christoffer Heckman, Kaushik Jayaram, J. Sean Humbert","Recurrent neural network-based reinforcement learning systems are capable of
complex motor control tasks such as locomotion and manipulation, however, much
of their underlying mechanisms still remain difficult to interpret. Our aim is
to leverage computational neuroscience methodologies to understanding the
population-level activity of robust robot locomotion controllers. Our
investigation begins by analyzing topological structure, discovering that
fragile controllers have a higher number of fixed points with unstable
directions, resulting in poorer balance when instructed to stand in place.
Next, we analyze the forced response of the system by applying targeted neural
perturbations along directions of dominant population-level activity. We find
evidence that recurrent state dynamics are structured and low-dimensional
during walking, which aligns with primate studies. Additionally, when recurrent
states are perturbed to zero, fragile agents continue to walk, which is
indicative of a stronger reliance on sensory input and weaker recurrence.",2306.15793v1,https://arxiv.org/pdf/2306.15793v1
PyBADS: Fast and robust black-box optimization in Python,"Gurjeet Sangra Singh, Luigi Acerbi","PyBADS is a Python implementation of the Bayesian Adaptive Direct Search
(BADS) algorithm for fast and robust black-box optimization (Acerbi and Ma
2017). BADS is an optimization algorithm designed to efficiently solve
difficult optimization problems where the objective function is rough
(non-convex, non-smooth), mildly expensive (e.g., the function evaluation
requires more than 0.1 seconds), possibly noisy, and gradient information is
unavailable. With BADS, these issues are well addressed, making it an excellent
choice for fitting computational models using methods such as
maximum-likelihood estimation. The algorithm scales efficiently to black-box
functions with up to $D \approx 20$ continuous input parameters and supports
bounds or no constraints. PyBADS comes along with an easy-to-use Pythonic
interface for running the algorithm and inspecting its results. PyBADS only
requires the user to provide a Python function for evaluating the target
function, and optionally other constraints.
  Extensive benchmarks on both artificial test problems and large real
model-fitting problems models drawn from cognitive, behavioral and
computational neuroscience, show that BADS performs on par with or better than
many other common and state-of-the-art optimizers (Acerbi and Ma 2017), making
it a general model-fitting tool which provides fast and robust solutions.",2306.15576v1,https://arxiv.org/pdf/2306.15576v1
"Cooperation or Competition: Avoiding Player Domination for Multi-Target
  Robustness via Adaptive Budgets","Yimu Wang, Dinghuai Zhang, Yihan Wu, Heng Huang, Hongyang Zhang","Despite incredible advances, deep learning has been shown to be susceptible
to adversarial attacks. Numerous approaches have been proposed to train robust
networks both empirically and certifiably. However, most of them defend against
only a single type of attack, while recent work takes steps forward in
defending against multiple attacks. In this paper, to understand multi-target
robustness, we view this problem as a bargaining game in which different
players (adversaries) negotiate to reach an agreement on a joint direction of
parameter updating. We identify a phenomenon named player domination in the
bargaining game, namely that the existing max-based approaches, such as MAX and
MSD, do not converge. Based on our theoretical analysis, we design a novel
framework that adjusts the budgets of different adversaries to avoid any player
dominance. Experiments on standard benchmarks show that employing the proposed
framework to the existing approaches significantly advances multi-target
robustness.",2306.15482v1,https://arxiv.org/pdf/2306.15482v1
Robust Proxy: Improving Adversarial Robustness by Robust Proxy Learning,"Hong Joo Lee, Yong Man Ro","Recently, it has been widely known that deep neural networks are highly
vulnerable and easily broken by adversarial attacks. To mitigate the
adversarial vulnerability, many defense algorithms have been proposed.
Recently, to improve adversarial robustness, many works try to enhance feature
representation by imposing more direct supervision on the discriminative
feature. However, existing approaches lack an understanding of learning
adversarially robust feature representation. In this paper, we propose a novel
training framework called Robust Proxy Learning. In the proposed method, the
model explicitly learns robust feature representations with robust proxies. To
this end, firstly, we demonstrate that we can generate class-representative
robust features by adding class-wise robust perturbations. Then, we use the
class representative features as robust proxies. With the class-wise robust
features, the model explicitly learns adversarially robust features through the
proposed robust proxy learning framework. Through extensive experiments, we
verify that we can manually generate robust features, and our proposed learning
framework could increase the robustness of the DNNs.",2306.15457v1,https://arxiv.org/pdf/2306.15457v1
Robust Wind Turbine Blade Segmentation from RGB Images in the Wild,"Raül Pérez-Gonzalo, Andreas Espersen, Antonio Agudo","With the relentless growth of the wind industry, there is an imperious need
to design automatic data-driven solutions for wind turbine maintenance. As
structural health monitoring mainly relies on visual inspections, the first
stage in any automatic solution is to identify the blade region on the image.
Thus, we propose a novel segmentation algorithm that strengthens the U-Net
results by a tailored loss, which pools the focal loss with a contiguity
regularization term. To attain top performing results, a set of additional
steps are proposed to ensure a reliable, generic, robust and efficient
algorithm. First, we leverage our prior knowledge on the images by filling the
holes enclosed by temporarily-classified blade pixels and by the image
boundaries. Subsequently, the mislead classified pixels are successfully
amended by training an on-the-fly random forest. Our algorithm demonstrates its
effectiveness reaching a non-trivial 97.39% of accuracy.",2306.14810v1,https://arxiv.org/pdf/2306.14810v1
"Leveraging Locality and Robustness to Achieve Massively Scalable
  Gaussian Process Regression","Robert Allison, Anthony Stephenson, Samuel F, Edward Pyzer-Knapp","The accurate predictions and principled uncertainty measures provided by GP
regression incur O(n^3) cost which is prohibitive for modern-day large-scale
applications. This has motivated extensive work on computationally efficient
approximations. We introduce a new perspective by exploring robustness
properties and limiting behaviour of GP nearest-neighbour (GPnn) prediction. We
demonstrate through theory and simulation that as the data-size n increases,
accuracy of estimated parameters and GP model assumptions become increasingly
irrelevant to GPnn predictive accuracy. Consequently, it is sufficient to spend
small amounts of work on parameter estimation in order to achieve high MSE
accuracy, even in the presence of gross misspecification. In contrast, as n
tends to infinity, uncertainty calibration and NLL are shown to remain
sensitive to just one parameter, the additive noise-variance; but we show that
this source of inaccuracy can be corrected for, thereby achieving both
well-calibrated uncertainty measures and accurate predictions at remarkably low
computational cost. We exhibit a very simple GPnn regression algorithm with
stand-out performance compared to other state-of-the-art GP approximations as
measured on large UCI datasets. It operates at a small fraction of those other
methods' training costs, for example on a basic laptop taking about 30 seconds
to train on a dataset of size n = 1.6 x 10^6.",2306.14731v2,https://arxiv.org/pdf/2306.14731v2
"The race to robustness: exploiting fragile models for urban camouflage
  and the imperative for machine learning security","Harriet Farlow, Matthew Garratt, Gavin Mount, Tim Lynar","Adversarial Machine Learning (AML) represents the ability to disrupt Machine
Learning (ML) algorithms through a range of methods that broadly exploit the
architecture of deep learning optimisation. This paper presents Distributed
Adversarial Regions (DAR), a novel method that implements distributed
instantiations of computer vision-based AML attack methods that may be used to
disguise objects from image recognition in both white and black box settings.
We consider the context of object detection models used in urban environments,
and benchmark the MobileNetV2, NasNetMobile and DenseNet169 models against a
subset of relevant images from the ImageNet dataset. We evaluate optimal
parameters (size, number and perturbation method), and compare to
state-of-the-art AML techniques that perturb the entire image. We find that
DARs can cause a reduction in confidence of 40.4% on average, but with the
benefit of not requiring the entire image, or the focal object, to be
perturbed. The DAR method is a deliberately simple approach where the intention
is to highlight how an adversary with very little skill could attack models
that may already be productionised, and to emphasise the fragility of
foundational object detection models. We present this as a contribution to the
field of ML security as well as AML. This paper contributes a novel adversarial
method, an original comparison between DARs and other AML methods, and frames
it in a new context - that of urban camouflage and the necessity for ML
security and model robustness.",2306.14609v1,https://arxiv.org/pdf/2306.14609v1
"Exploring the Robustness of Large Language Models for Solving
  Programming Problems","Atsushi Shirafuji, Yutaka Watanobe, Takumi Ito, Makoto Morishita, Yuki Nakamura, Yusuke Oda, Jun Suzuki","Using large language models (LLMs) for source code has recently gained
attention. LLMs, such as Transformer-based models like Codex and ChatGPT, have
been shown to be highly capable of solving a wide range of programming
problems. However, the extent to which LLMs understand problem descriptions and
generate programs accordingly or just retrieve source code from the most
relevant problem in training data based on superficial cues has not been
discovered yet. To explore this research question, we conduct experiments to
understand the robustness of several popular LLMs, CodeGen and GPT-3.5 series
models, capable of tackling code generation tasks in introductory programming
problems. Our experimental results show that CodeGen and Codex are sensitive to
the superficial modifications of problem descriptions and significantly impact
code generation performance. Furthermore, we observe that Codex relies on
variable names, as randomized variables decrease the solved rate significantly.
However, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT,
show higher robustness to superficial modifications and have an outstanding
capability for solving programming problems. This highlights the fact that
slight modifications to the prompts given to the LLMs can greatly affect code
generation performance, and careful formatting of prompts is essential for
high-quality code generation, while the SOTA models are becoming more robust to
perturbations.",2306.14583v1,https://arxiv.org/pdf/2306.14583v1
"Mitigating Hallucination in Large Multi-Modal Models via Robust
  Instruction Tuning","Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang","Despite the promising progress in multi-modal tasks, current large
multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions
with respect to the associated image and human instructions. This paper
addresses this issue by introducing the first large and diverse visual
instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction.
Our dataset comprises 400k visual instructions generated by GPT4, covering 16
vision-and-language tasks with open-ended instructions and answers. Unlike
existing studies that primarily focus on positive instruction samples, we
design LRV-Instruction to include both positive and negative instructions for
more robust visual instruction tuning. Our negative instructions are designed
at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent
Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure
the hallucination generated by LMMs, we propose GPT4-Assisted Visual
Instruction Evaluation (GAVIE), a stable approach to evaluate visual
instruction tuning like human experts. GAVIE does not require human-annotated
groundtruth answers and can adapt to diverse instruction formats. We conduct
comprehensive experiments to investigate the hallucination of LMMs. Our results
demonstrate existing LMMs exhibit significant hallucinations when presented
with our negative instructions, particularly Existent Object and Knowledge
Manipulation instructions. Moreover, we successfully mitigate hallucination by
finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving
performance on several public datasets compared to state-of-the-art methods.
Additionally, we observed that a balanced ratio of positive and negative
instances in the training data leads to a more robust model. Code and data are
available at https://github.com/FuxiaoLiu/LRV-Instruction.",2306.14565v4,https://arxiv.org/pdf/2306.14565v4
"Evolution of $K$-means solution landscapes with the addition of dataset
  outliers and a robust clustering comparison measure for their analysis","Luke Dicks, David J. Wales","The $K$-means algorithm remains one of the most widely-used clustering
methods due to its simplicity and general utility. The performance of $K$-means
depends upon location of minima low in cost function, amongst a potentially
vast number of solutions. Here, we use the energy landscape approach to map the
change in $K$-means solution space as a result of increasing dataset outliers
and show that the cost function surface becomes more funnelled. Kinetic
analysis reveals that in all cases the overall funnel is composed of shallow
locally-funnelled regions, each of which are separated by areas that do not
support any clustering solutions. These shallow regions correspond to different
types of clustering solution and their increasing number with outliers leads to
longer pathways within the funnel and a reduced correlation between accuracy
and cost function. Finally, we propose that the rates obtained from kinetic
analysis provide a novel measure of clustering similarity that incorporates
information about the paths between them. This measure is robust to outliers
and we illustrate the application to datasets containing multiple outliers.",2306.14346v1,https://arxiv.org/pdf/2306.14346v1
Computational Asymmetries in Robust Classification,"Samuele Marro, Michele Lombardi","In the context of adversarial robustness, we make three strongly related
contributions. First, we prove that while attacking ReLU classifiers is
$\mathit{NP}$-hard, ensuring their robustness at training time is
$\Sigma^2_P$-hard (even on a single example). This asymmetry provides a
rationale for the fact that robust classifications approaches are frequently
fooled in the literature. Second, we show that inference-time robustness
certificates are not affected by this asymmetry, by introducing a
proof-of-concept approach named Counter-Attack (CA). Indeed, CA displays a
reversed asymmetry: running the defense is $\mathit{NP}$-hard, while attacking
it is $\Sigma_2^P$-hard. Finally, motivated by our previous result, we argue
that adversarial attacks can be used in the context of robustness
certification, and provide an empirical evaluation of their effectiveness. As a
byproduct of this process, we also release UG100, a benchmark dataset for
adversarial attacks.",2306.14326v1,https://arxiv.org/pdf/2306.14326v1
"RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated
  Adversarial Perturbations","Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, Dragomir Radev","Despite significant progress having been made in question answering on
tabular data (Table QA), it's unclear whether, and to what extent existing
Table QA models are robust to task-specific perturbations, e.g., replacing key
question entities or shuffling table columns. To systematically study the
robustness of Table QA models, we propose a benchmark called RobuT, which
builds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and
includes human-annotated adversarial perturbations in terms of table header,
table content, and question. Our results indicate that both state-of-the-art
Table QA models and large language models (e.g., GPT-3) with few-shot learning
falter in these adversarial sets. We propose to address this problem by using
large language models to generate adversarial examples to enhance training,
which significantly improves the robustness of Table QA models. Our data and
code is publicly available at https://github.com/yilunzhao/RobuT.",2306.14321v1,https://arxiv.org/pdf/2306.14321v1
Adaptive Sharpness-Aware Pruning for Robust Sparse Networks,"Anna Bair, Hongxu Yin, Maying Shen, Pavlo Molchanov, Jose Alvarez","Robustness and compactness are two essential attributes of deep learning
models that are deployed in the real world. The goals of robustness and
compactness may seem to be at odds, since robustness requires generalization
across domains, while the process of compression exploits specificity in one
domain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies
these goals through the lens of network sharpness. The AdaSAP method produces
sparse networks that are robust to input variations which are unseen at
training time. We achieve this by strategically incorporating weight
perturbations in order to optimize the loss landscape. This allows the model to
be both primed for pruning and regularized for improved robustness. AdaSAP
improves the robust accuracy of pruned models on image classification by up to
+6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a
corrupted Pascal VOC dataset, over a wide range of compression ratios, pruning
criteria, and network architectures, outperforming recent pruning art by large
margins.",2306.14306v2,https://arxiv.org/pdf/2306.14306v2
On Evaluating the Adversarial Robustness of Semantic Segmentation Models,"Levente Halmosi, Mark Jelasity","Achieving robustness against adversarial input perturbation is an important
and intriguing problem in machine learning. In the area of semantic image
segmentation, a number of adversarial training approaches have been proposed as
a defense against adversarial perturbation, but the methodology of evaluating
the robustness of the models is still lacking, compared to image
classification. Here, we demonstrate that, just like in image classification,
it is important to evaluate the models over several different and hard attacks.
We propose a set of gradient based iterative attacks and show that it is
essential to perform a large number of iterations. We include attacks against
the internal representations of the models as well. We apply two types of
attacks: maximizing the error with a bounded perturbation, and minimizing the
perturbation for a given level of error. Using this set of attacks, we show for
the first time that a number of models in previous work that are claimed to be
robust are in fact not robust at all. We then evaluate simple adversarial
training algorithms that produce reasonably robust models even under our set of
strong attacks. Our results indicate that a key design decision to achieve any
robustness is to use only adversarial examples during training. However, this
introduces a trade-off between robustness and accuracy.",2306.14217v1,https://arxiv.org/pdf/2306.14217v1
"Robust Spatiotemporal Traffic Forecasting with Reinforced Dynamic
  Adversarial Training","Fan Liu, Weijia Zhang, Hao Liu","Machine learning-based forecasting models are commonly used in Intelligent
Transportation Systems (ITS) to predict traffic patterns and provide city-wide
services. However, most of the existing models are susceptible to adversarial
attacks, which can lead to inaccurate predictions and negative consequences
such as congestion and delays. Therefore, improving the adversarial robustness
of these models is crucial for ITS. In this paper, we propose a novel framework
for incorporating adversarial training into spatiotemporal traffic forecasting
tasks. We demonstrate that traditional adversarial training methods designated
for static domains cannot be directly applied to traffic forecasting tasks, as
they fail to effectively defend against dynamic adversarial attacks. Then, we
propose a reinforcement learning-based method to learn the optimal node
selection strategy for adversarial examples, which simultaneously strengthens
the dynamic attack defense capability and reduces the model overfitting.
Additionally, we introduce a self-knowledge distillation regularization module
to overcome the ""forgetting issue"" caused by continuously changing adversarial
nodes during training. We evaluate our approach on two real-world traffic
datasets and demonstrate its superiority over other baselines. Our method
effectively enhances the adversarial robustness of spatiotemporal traffic
forecasting models. The source code for our framework is available at
https://github.com/usail-hkust/RDAT.",2306.14126v1,https://arxiv.org/pdf/2306.14126v1
"Decision-Dependent Distributionally Robust Markov Decision Process
  Method in Dynamic Epidemic Control","Jun Song, William Yang, Chaoyue Zhao","In this paper, we present a Distributionally Robust Markov Decision Process
(DRMDP) approach for addressing the dynamic epidemic control problem. The
Susceptible-Exposed-Infectious-Recovered (SEIR) model is widely used to
represent the stochastic spread of infectious diseases, such as COVID-19. While
Markov Decision Processes (MDP) offers a mathematical framework for identifying
optimal actions, such as vaccination and transmission-reducing intervention, to
combat disease spreading according to the SEIR model. However, uncertainties in
these scenarios demand a more robust approach that is less reliant on
error-prone assumptions. The primary objective of our study is to introduce a
new DRMDP framework that allows for an ambiguous distribution of transition
dynamics. Specifically, we consider the worst-case distribution of these
transition probabilities within a decision-dependent ambiguity set. To overcome
the computational complexities associated with policy determination, we propose
an efficient Real-Time Dynamic Programming (RTDP) algorithm that is capable of
computing optimal policies based on the reformulated DRMDP model in an
accurate, timely, and scalable manner. Comparative analysis against the classic
MDP model demonstrates that the DRMDP achieves a lower proportion of infections
and susceptibilities at a reduced cost.",2306.14051v1,https://arxiv.org/pdf/2306.14051v1
Smoothed $f$-Divergence Distributionally Robust Optimization,"Zhenyuan Liu, Bart P. G. Van Parys, Henry Lam","In data-driven optimization, sample average approximation (SAA) is known to
suffer from the so-called optimizer's curse that causes an over-optimistic
evaluation of the solution performance. We argue that a special type of
distributionallly robust optimization (DRO) formulation offers theoretical
advantages in correcting for this optimizer's curse compared to simple
``margin'' adjustments to SAA and other DRO approaches: It attains a
statistical bound on the out-of-sample performance, for a wide class of
objective functions and distributions, that is nearly tightest in terms of
exponential decay rate. This DRO uses an ambiguity set based on a Kullback
Leibler (KL) divergence smoothed by the Wasserstein or L\'evy-Prokhorov (LP)
distance via a suitable distance optimization. Computationally, we also show
that such a DRO, and its generalized versions using smoothed $f$-divergence,
are not harder than DRO problems based on $f$-divergence or Wasserstein
distances, rendering our DRO formulations both statistically optimal and
computationally viable.",2306.14041v2,https://arxiv.org/pdf/2306.14041v2
"Robust Classification of High-Dimensional Data using Data-Adaptive
  Energy Distance","Jyotishka Ray Choudhury, Aytijhya Saha, Sarbojit Roy, Subhajit Dutta","Classification of high-dimensional low sample size (HDLSS) data poses a
challenge in a variety of real-world situations, such as gene expression
studies, cancer research, and medical imaging. This article presents the
development and analysis of some classifiers that are specifically designed for
HDLSS data. These classifiers are free of tuning parameters and are robust, in
the sense that they are devoid of any moment conditions of the underlying data
distributions. It is shown that they yield perfect classification in the HDLSS
asymptotic regime, under some fairly general conditions. The comparative
performance of the proposed classifiers is also investigated. Our theoretical
results are supported by extensive simulation studies and real data analysis,
which demonstrate promising advantages of the proposed classification
techniques over several widely recognized methods.",2306.13985v1,https://arxiv.org/pdf/2306.13985v1
A First Order Meta Stackelberg Method for Robust Federated Learning,"Yunian Pan, Tao Li, Henger Li, Tianyi Xu, Zizhan Zheng, Quanyan Zhu","Previous research has shown that federated learning (FL) systems are exposed
to an array of security risks. Despite the proposal of several defensive
strategies, they tend to be non-adaptive and specific to certain types of
attacks, rendering them ineffective against unpredictable or adaptive threats.
This work models adversarial federated learning as a Bayesian Stackelberg
Markov game (BSMG) to capture the defender's incomplete information of various
attack types. We propose meta-Stackelberg learning (meta-SL), a provably
efficient meta-learning algorithm, to solve the equilibrium strategy in BSMG,
leading to an adaptable FL defense. We demonstrate that meta-SL converges to
the first-order $\varepsilon$-equilibrium point in $O(\varepsilon^{-2})$
gradient iterations, with $O(\varepsilon^{-4})$ samples needed per iteration,
matching the state of the art. Empirical evidence indicates that our
meta-Stackelberg framework performs exceptionally well against potent model
poisoning and backdoor attacks of an uncertain nature.",2306.13800v3,https://arxiv.org/pdf/2306.13800v3
Adversarial Robustness Certification for Bayesian Neural Networks,"Matthew Wicker, Andrea Patane, Luca Laurenti, Marta Kwiatkowska","We study the problem of certifying the robustness of Bayesian neural networks
(BNNs) to adversarial input perturbations. Given a compact set of input points
$T \subseteq \mathbb{R}^m$ and a set of output points $S \subseteq
\mathbb{R}^n$, we define two notions of robustness for BNNs in an adversarial
setting: probabilistic robustness and decision robustness. Probabilistic
robustness is the probability that for all points in $T$ the output of a BNN
sampled from the posterior is in $S$. On the other hand, decision robustness
considers the optimal decision of a BNN and checks if for all points in $T$ the
optimal decision of the BNN for a given loss function lies within the output
set $S$. Although exact computation of these robustness properties is
challenging due to the probabilistic and non-convex nature of BNNs, we present
a unified computational framework for efficiently and formally bounding them.
Our approach is based on weight interval sampling, integration, and bound
propagation techniques, and can be applied to BNNs with a large number of
parameters, and independently of the (approximate) inference method employed to
train the BNN. We evaluate the effectiveness of our methods on various
regression and classification tasks, including an industrial regression
benchmark, MNIST, traffic sign recognition, and airborne collision avoidance,
and demonstrate that our approach enables certification of robustness and
uncertainty of BNN predictions.",2306.13614v1,https://arxiv.org/pdf/2306.13614v1
"TrustGuard: GNN-based Robust and Explainable Trust Evaluation with
  Dynamicity Support","Jie Wang, Zheng Yan, Jiahe Lan, Elisa Bertino, Witold Pedrycz","Trust evaluation assesses trust relationships between entities and
facilitates decision-making. Machine Learning (ML) shows great potential for
trust evaluation owing to its learning capabilities. In recent years, Graph
Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in
dealing with graph data. This has motivated researchers to explore their use in
trust evaluation, as trust relationships among entities can be modeled as a
graph. However, current trust evaluation methods that employ GNNs fail to fully
satisfy the dynamic nature of trust, overlook the adverse effects of
trust-related attacks, and cannot provide convincing explanations on evaluation
results. To address these problems, we propose TrustGuard, a GNN-based accurate
trust evaluation model that supports trust dynamicity, is robust against
typical attacks, and provides explanations through visualization. Specifically,
TrustGuard is designed with a layered architecture that contains a snapshot
input layer, a spatial aggregation layer, a temporal aggregation layer, and a
prediction layer. Among them, the spatial aggregation layer adopts a defense
mechanism to robustly aggregate local trust, and the temporal aggregation layer
applies an attention mechanism for effective learning of temporal patterns.
Extensive experiments on two real-world datasets show that TrustGuard
outperforms state-of-the-art GNN-based trust evaluation models with respect to
trust prediction across single-timeslot and multi-timeslot, even in the
presence of attacks. In addition, TrustGuard can explain its evaluation results
by visualizing both spatial and temporal views.",2306.13339v4,https://arxiv.org/pdf/2306.13339v4
"On Sensitivity and Robustness of Normalization Schemes to Input
  Distribution Shifts in Automatic MR Image Diagnosis","Divyam Madaan, Daniel Sodickson, Kyunghyun Cho, Sumit Chopra","Magnetic Resonance Imaging (MRI) is considered the gold standard of medical
imaging because of the excellent soft-tissue contrast exhibited in the images
reconstructed by the MRI pipeline, which in-turn enables the human radiologist
to discern many pathologies easily. More recently, Deep Learning (DL) models
have also achieved state-of-the-art performance in diagnosing multiple diseases
using these reconstructed images as input. However, the image reconstruction
process within the MRI pipeline, which requires the use of complex hardware and
adjustment of a large number of scanner parameters, is highly susceptible to
noise of various forms, resulting in arbitrary artifacts within the images.
Furthermore, the noise distribution is not stationary and varies within a
machine, across machines, and patients, leading to varying artifacts within the
images. Unfortunately, DL models are quite sensitive to these varying artifacts
as it leads to changes in the input data distribution between the training and
testing phases. The lack of robustness of these models against varying
artifacts impedes their use in medical applications where safety is critical.
In this work, we focus on improving the generalization performance of these
models in the presence of multiple varying artifacts that manifest due to the
complexity of the MR data acquisition. In our experiments, we observe that
Batch Normalization, a widely used technique during the training of DL models
for medical image analysis, is a significant cause of performance degradation
in these changing environments. As a solution, we propose to use other
normalization techniques, such as Group Normalization and Layer Normalization
(LN), to inject robustness into model performance against varying image
artifacts. Through a systematic set of experiments, we show that GN and LN
provide better accuracy for various MR artifacts and distribution shifts.",2306.13276v1,https://arxiv.org/pdf/2306.13276v1
"Towards Reliable Evaluation and Fast Training of Robust Semantic
  Segmentation Models","Francesco Croce, Naman D Singh, Matthias Hein","Adversarial robustness has been studied extensively in image classification,
especially for the $\ell_\infty$-threat model, but significantly less so for
related tasks such as object detection and semantic segmentation, where attacks
turn out to be a much harder optimization problem than for image
classification. We propose several problem-specific novel attacks minimizing
different metrics in accuracy and mIoU. The ensemble of our attacks, SEA, shows
that existing attacks severely overestimate the robustness of semantic
segmentation models. Surprisingly, existing attempts of adversarial training
for semantic segmentation models turn out to be weak or even completely
non-robust. We investigate why previous adaptations of adversarial training to
semantic segmentation failed and show how recently proposed robust ImageNet
backbones can be used to obtain adversarially robust semantic segmentation
models with up to six times less training time for PASCAL-VOC and the more
challenging ADE20k. The associated code and robust models are available at
https://github.com/nmndeep/robust-segmentation",2306.12941v2,https://arxiv.org/pdf/2306.12941v2
"Conditional Generators for Limit Order Book Environments:
  Explainability, Challenges, and Robustness","Andrea Coletta, Joseph Jerome, Rahul Savani, Svitlana Vyetrenko","Limit order books are a fundamental and widespread market mechanism. This
paper investigates the use of conditional generative models for order book
simulation. For developing a trading agent, this approach has drawn recent
attention as an alternative to traditional backtesting due to its ability to
react to the presence of the trading agent. Using a state-of-the-art CGAN (from
Coletta et al. (2022)), we explore its dependence upon input features, which
highlights both strengths and weaknesses. To do this, we use ""adversarial
attacks"" on the model's features and its mechanism. We then show how these
insights can be used to improve the CGAN, both in terms of its realism and
robustness. We finish by laying out a roadmap for future work.",2306.12806v1,https://arxiv.org/pdf/2306.12806v1
"Robust Statistical Comparison of Random Variables with Locally Varying
  Scale of Measurement","Christoph Jansen, Georg Schollmeyer, Hannah Blocher, Julian Rodemann, Thomas Augustin","Spaces with locally varying scale of measurement, like multidimensional
structures with differently scaled dimensions, are pretty common in statistics
and machine learning. Nevertheless, it is still understood as an open question
how to exploit the entire information encoded in them properly. We address this
problem by considering an order based on (sets of) expectations of random
variables mapping into such non-standard spaces. This order contains stochastic
dominance and expectation order as extreme cases when no, or respectively
perfect, cardinal structure is given. We derive a (regularized) statistical
test for our proposed generalized stochastic dominance (GSD) order,
operationalize it by linear optimization, and robustify it by imprecise
probability models. Our findings are illustrated with data from
multidimensional poverty measurement, finance, and medicine.",2306.12803v2,https://arxiv.org/pdf/2306.12803v2
"On the Robustness of Generative Retrieval Models: An Out-of-Distribution
  Perspective","Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Wei Chen, Xueqi Cheng","Recently, we have witnessed generative retrieval increasingly gaining
attention in the information retrieval (IR) field, which retrieves documents by
directly generating their identifiers. So far, much effort has been devoted to
developing effective generative retrieval models. There has been less attention
paid to the robustness perspective. When a new retrieval paradigm enters into
the real-world application, it is also critical to measure the
out-of-distribution (OOD) generalization, i.e., how would generative retrieval
models generalize to new distributions. To answer this question, firstly, we
define OOD robustness from three perspectives in retrieval problems: 1) The
query variations; 2) The unforeseen query types; and 3) The unforeseen tasks.
Based on this taxonomy, we conduct empirical studies to analyze the OOD
robustness of several representative generative retrieval models against dense
retrieval models. The empirical results indicate that the OOD robustness of
generative retrieval models requires enhancement. We hope studying the OOD
robustness of generative retrieval models would be advantageous to the IR
community.",2306.12756v1,https://arxiv.org/pdf/2306.12756v1
Towards quantum enhanced adversarial robustness in machine learning,"Maxwell T. West, Shu-Lok Tsang, Jia S. Low, Charles D. Hill, Christopher Leckie, Lloyd C. L. Hollenberg, Sarah M. Erfani, Muhammad Usman","Machine learning algorithms are powerful tools for data driven tasks such as
image classification and feature detection, however their vulnerability to
adversarial examples - input samples manipulated to fool the algorithm -
remains a serious challenge. The integration of machine learning with quantum
computing has the potential to yield tools offering not only better accuracy
and computational efficiency, but also superior robustness against adversarial
attacks. Indeed, recent work has employed quantum mechanical phenomena to
defend against adversarial attacks, spurring the rapid development of the field
of quantum adversarial machine learning (QAML) and potentially yielding a new
source of quantum advantage. Despite promising early results, there remain
challenges towards building robust real-world QAML tools. In this review we
discuss recent progress in QAML and identify key challenges. We also suggest
future research directions which could determine the route to practicality for
QAML approaches as quantum computing hardware scales up and noise levels are
reduced.",2306.12688v1,https://arxiv.org/pdf/2306.12688v1
Outlier-robust Estimation of a Sparse Linear Model Using Invexity,"Adarsh Barik, Jean Honorio","In this paper, we study problem of estimating a sparse regression vector with
correct support in the presence of outlier samples. The inconsistency of
lasso-type methods is well known in this scenario. We propose a combinatorial
version of outlier-robust lasso which also identifies clean samples.
Subsequently, we use these clean samples to make a good estimation. We also
provide a novel invex relaxation for the combinatorial problem and provide
provable theoretical guarantees for this relaxation. Finally, we conduct
experiments to validate our theory and compare our results against standard
lasso.",2306.12678v1,https://arxiv.org/pdf/2306.12678v1
"RobustNeuralNetworks.jl: a Package for Machine Learning and Data-Driven
  Control with Certified Robustness","Nicholas H. Barbara, Max Revay, Ruigang Wang, Jing Cheng, Ian R. Manchester","Neural networks are typically sensitive to small input perturbations, leading
to unexpected or brittle behaviour. We present RobustNeuralNetworks.jl: a Julia
package for neural network models that are constructed to naturally satisfy a
set of user-defined robustness constraints. The package is based on the
recently proposed Recurrent Equilibrium Network (REN) and Lipschitz-Bounded
Deep Network (LBDN) model classes, and is designed to interface directly with
Julia's most widely-used machine learning package, Flux.jl. We discuss the
theory behind our model parameterization, give an overview of the package, and
provide a tutorial demonstrating its use in image classification, reinforcement
learning, and nonlinear state-observer design.",2306.12612v1,https://arxiv.org/pdf/2306.12612v1
Task-Robust Pre-Training for Worst-Case Downstream Adaptation,"Jianghui Wang, Yang Chen, Xingyu Xie, Cong Fang, Zhouchen Lin","Pre-training has achieved remarkable success when transferred to downstream
tasks. In machine learning, we care about not only the good performance of a
model but also its behavior under reasonable shifts of condition. The same
philosophy holds when pre-training a foundation model. However, the foundation
model may not uniformly behave well for a series of related downstream tasks.
This happens, for example, when conducting mask recovery regression where the
recovery ability or the training instances diverge like pattern features are
extracted dominantly on pre-training, but semantic features are also required
on a downstream task. This paper considers pre-training a model that guarantees
a uniformly good performance over the downstream tasks. We call this goal as
$\textit{downstream-task robustness}$. Our method first separates the upstream
task into several representative ones and applies a simple minimax loss for
pre-training. We then design an efficient algorithm to solve the minimax loss
and prove its convergence in the convex setting. In the experiments, we show
both on large-scale natural language processing and computer vision datasets
our method increases the metrics on worse-case downstream tasks. Additionally,
some theoretical explanations for why our loss is beneficial are provided.
Specifically, we show fewer samples are inherently required for the most
challenging downstream task in some cases.",2306.12070v3,https://arxiv.org/pdf/2306.12070v3
Structure-Aware Robustness Certificates for Graph Classification,"Pierre Osselin, Henry Kenlay, Xiaowen Dong","Certifying the robustness of a graph-based machine learning model poses a
critical challenge for safety. Current robustness certificates for graph
classifiers guarantee output invariance with respect to the total number of
node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$
ball centred on the adjacency matrix. Although theoretically attractive, this
type of isotropic structural noise can be too restrictive in practical
scenarios where some node pairs are more critical than others in determining
the classifier's output. The certificate, in this case, gives a pessimistic
depiction of the robustness of the graph model. To tackle this issue, we
develop a randomised smoothing method based on adding an anisotropic noise
distribution to the input graph structure. We show that our process generates
structural-aware certificates for our classifiers, whereby the magnitude of
robustness certificates can vary across different pre-defined structures of the
graph. We demonstrate the benefits of these certificates in both synthetic and
real-world experiments.",2306.11915v2,https://arxiv.org/pdf/2306.11915v2
"Proactive Human-Robot Co-Assembly: Leveraging Human Intention Prediction
  and Robust Safe Control","Ruixuan Liu, Rui Chen, Abulikemu Abuduweili, Changliu Liu","Human-robot collaboration (HRC) is one key component to achieving flexible
manufacturing to meet the different needs of customers. However, it is
difficult to build intelligent robots that can proactively assist humans in a
safe and efficient way due to several challenges. First, it is challenging to
achieve efficient collaboration due to diverse human behaviors and data
scarcity. Second, it is difficult to ensure interactive safety due to
uncertainty in human behaviors. This paper presents an integrated framework for
proactive HRC. A robust intention prediction module, which leverages prior task
information and human-in-the-loop training, is learned to guide the robot for
efficient collaboration. The proposed framework also uses robust safe control
to ensure interactive safety under uncertainty. The developed framework is
applied to a co-assembly task using a Kinova Gen3 robot. The experiment
demonstrates that our solution is robust to environmental changes as well as
different human preferences and behaviors. In addition, it improves task
efficiency by approximately 15-20%. Moreover, the experiment demonstrates that
our solution can guarantee interactive safety during proactive collaboration.",2306.11862v2,https://arxiv.org/pdf/2306.11862v2
"Towards a robust and reliable deep learning approach for detection of
  compact binary mergers in gravitational wave data","Shreejit Jadhav, Mihir Shrivastava, Sanjit Mitra","The ability of deep learning (DL) approaches to learn generalised signal and
noise models, coupled with their fast inference on GPUs, holds great promise
for enhancing gravitational-wave (GW) searches in terms of speed, parameter
space coverage, and search sensitivity. However, the opaque nature of DL models
severely harms their reliability. In this work, we meticulously develop a DL
model stage-wise and work towards improving its robustness and reliability.
First, we address the problems in maintaining the purity of training data by
deriving a new metric that better reflects the visual strength of the 'chirp'
signal features in the data. Using a reduced, smooth representation obtained
through a variational auto-encoder (VAE), we build a classifier to search for
compact binary coalescence (CBC) signals. Our tests on real LIGO data show an
impressive performance of the model. However, upon probing the robustness of
the model through adversarial attacks, its simple failure modes were
identified, underlining how such models can still be highly fragile. As a first
step towards bringing robustness, we retrain the model in a novel framework
involving a generative adversarial network (GAN). Over the course of training,
the model learns to eliminate the primary modes of failure identified by the
adversaries. Although absolute robustness is practically impossible to achieve,
we demonstrate some fundamental improvements earned through such training, like
sparseness and reduced degeneracy in the extracted features at different layers
inside the model. We show that these gains are achieved at practically zero
loss in terms of model performance on real LIGO data before and after GAN
training. Through a direct search on 8.8 days of LIGO data, we recover two
significant CBC events from GWTC-2.1, GW190519_153544 and GW190521_074359. We
also report the search sensitivity obtained from an injection study.",2306.11797v2,https://arxiv.org/pdf/2306.11797v2
"Harnessing the Power of Adversarial Prompting and Large Language Models
  for Robust Hypothesis Generation in Astronomy","Ioana Ciucă, Yuan-Sen Ting, Sandor Kruk, Kartheik Iyer","This study investigates the application of Large Language Models (LLMs),
specifically GPT-4, within Astronomy. We employ in-context prompting, supplying
the model with up to 1000 papers from the NASA Astrophysics Data System, to
explore the extent to which performance can be improved by immersing the model
in domain-specific literature. Our findings point towards a substantial boost
in hypothesis generation when using in-context prompting, a benefit that is
further accentuated by adversarial prompting. We illustrate how adversarial
prompting empowers GPT-4 to extract essential details from a vast knowledge
base to produce meaningful hypotheses, signaling an innovative step towards
employing LLMs for scientific research in Astronomy.",2306.11648v1,https://arxiv.org/pdf/2306.11648v1
"Soft Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient,
  and Sample Complexity","Runyu Zhang, Yang Hu, Na Li","Robust Markov Decision Processes (MDPs) and risk-sensitive MDPs are both
powerful tools for making decisions in the presence of uncertainties. Previous
efforts have aimed to establish their connections, revealing equivalences in
specific formulations. This paper introduces a new formulation for
risk-sensitive MDPs, which assesses risk in a slightly different manner
compared to the classical Markov risk measure (Ruszczy\'nski 2010), and
establishes its equivalence with a class of soft robust MDP (RMDP) problems,
including the standard RMDP as a special case. Leveraging this equivalence, we
further derive the policy gradient theorem for both problems, proving gradient
domination and global convergence of the exact policy gradient method under the
tabular setting with direct parameterization. This forms a sharp contrast to
the Markov risk measure, known to be potentially non-gradient-dominant (Huang
et al. 2021). We also propose a sample-based offline learning algorithm, namely
the robust fitted-Z iteration (RFZI), for a specific soft RMDP problem with a
KL-divergence regularization term (or equivalently the risk-sensitive MDP with
an entropy risk measure). We showcase its streamlined design and less stringent
assumptions due to the equivalence and analyze its sample complexity",2306.11626v4,https://arxiv.org/pdf/2306.11626v4
Provably Robust Temporal Difference Learning for Heavy-Tailed Rewards,"Semih Cayci, Atilla Eryilmaz","In a broad class of reinforcement learning applications, stochastic rewards
have heavy-tailed distributions, which lead to infinite second-order moments
for stochastic (semi)gradients in policy evaluation and direct policy
optimization. In such instances, the existing RL methods may fail miserably due
to frequent statistical outliers. In this work, we establish that temporal
difference (TD) learning with a dynamic gradient clipping mechanism, and
correspondingly operated natural actor-critic (NAC), can be provably
robustified against heavy-tailed reward distributions. It is shown in the
framework of linear function approximation that a favorable tradeoff between
bias and variability of the stochastic gradients can be achieved with this
dynamic gradient clipping mechanism. In particular, we prove that robust
versions of TD learning achieve sample complexities of order
$\mathcal{O}(\varepsilon^{-\frac{1}{p}})$ and
$\mathcal{O}(\varepsilon^{-1-\frac{1}{p}})$ with and without the full-rank
assumption on the feature matrix, respectively, under heavy-tailed rewards with
finite moments of order $(1+p)$ for some $p\in(0,1]$, both in expectation and
with high probability. We show that a robust variant of NAC based on Robust TD
learning achieves $\tilde{\mathcal{O}}(\varepsilon^{-4-\frac{2}{p}})$ sample
complexity. We corroborate our theoretical results with numerical experiments.",2306.11455v1,https://arxiv.org/pdf/2306.11455v1
Evaluating the Zero-shot Robustness of Instruction-tuned Language Models,"Jiuding Sun, Chantal Shaib, Byron C. Wallace","Instruction fine-tuning has recently emerged as a promising approach for
improving the zero-shot capabilities of Large Language Models (LLMs) on new
tasks. This technique has shown particular strength in improving the
performance of modestly sized LLMs, sometimes inducing performance competitive
with much larger model variants. In this paper we ask two questions: (1) How
sensitive are instruction-tuned models to the particular phrasings of
instructions, and, (2) How can we make them more robust to such natural
language variation? To answer the former, we collect a set of 319 instructions
manually written by NLP practitioners for over 80 unique tasks included in
widely used benchmarks, and we evaluate the variance and average performance of
these instructions as compared to instruction phrasings observed during
instruction fine-tuning. We find that using novel (unobserved) but appropriate
instruction phrasings consistently degrades model performance, sometimes
substantially so. Further, such natural instructions yield a wide variance in
downstream performance, despite their semantic equivalence. Put another way,
instruction-tuned models are not especially robust to instruction re-phrasings.
We propose a simple method to mitigate this issue by introducing ``soft
prompt'' embedding parameters and optimizing these to maximize the similarity
between representations of semantically equivalent instructions. We show that
this method consistently improves the robustness of instruction-tuned models.",2306.11270v2,https://arxiv.org/pdf/2306.11270v2
Simple and Fast Group Robustness by Automatic Feature Reweighting,"Shikai Qiu, Andres Potapczynski, Pavel Izmailov, Andrew Gordon Wilson","A major challenge to out-of-distribution generalization is reliance on
spurious features -- patterns that are predictive of the class label in the
training data distribution, but not causally related to the target. Standard
methods for reducing the reliance on spurious features typically assume that we
know what the spurious feature is, which is rarely true in the real world.
Methods that attempt to alleviate this limitation are complex, hard to tune,
and lead to a significant computational overhead compared to standard training.
In this paper, we propose Automatic Feature Reweighting (AFR), an extremely
simple and fast method for updating the model to reduce the reliance on
spurious features. AFR retrains the last layer of a standard ERM-trained base
model with a weighted loss that emphasizes the examples where the ERM model
predicts poorly, automatically upweighting the minority group without group
labels. With this simple procedure, we improve upon the best reported results
among competing methods trained without spurious attributes on several vision
and natural language classification benchmarks, using only a fraction of their
compute.",2306.11074v1,https://arxiv.org/pdf/2306.11074v1
"Adversarial Robustness of Prompt-based Few-Shot Learning for Natural
  Language Understanding","Venkata Prabhakara Sarath Nookala, Gaurav Verma, Subhabrata Mukherjee, Srijan Kumar","State-of-the-art few-shot learning (FSL) methods leverage prompt-based
fine-tuning to obtain remarkable results for natural language understanding
(NLU) tasks. While much of the prior FSL methods focus on improving downstream
task performance, there is a limited understanding of the adversarial
robustness of such methods. In this work, we conduct an extensive study of
several state-of-the-art FSL methods to assess their robustness to adversarial
perturbations. To better understand the impact of various factors towards
robustness (or the lack of it), we evaluate prompt-based FSL methods against
fully fine-tuned models for aspects such as the use of unlabeled data, multiple
prompts, number of few-shot examples, model size and type. Our results on six
GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL
methods lead to a notable relative drop in task performance (i.e., are less
robust) in the face of adversarial perturbations. However, using (i) unlabeled
data for prompt-based FSL and (ii) multiple prompts flip the trend. We further
demonstrate that increasing the number of few-shot examples and model size lead
to increased adversarial robustness of vanilla FSL methods. Broadly, our work
sheds light on the adversarial robustness evaluation of prompt-based FSL
methods for NLU tasks.",2306.11066v2,https://arxiv.org/pdf/2306.11066v2
"Cross-Modal Attribute Insertions for Assessing the Robustness of
  Vision-and-Language Learning","Shivaen Ramshetty, Gaurav Verma, Srijan Kumar","The robustness of multimodal deep learning models to realistic changes in the
input text is critical for their applicability to important tasks such as
text-to-image retrieval and cross-modal entailment. To measure robustness,
several existing approaches edit the text data, but do so without leveraging
the cross-modal information present in multimodal data. Information from the
visual modality, such as color, size, and shape, provide additional attributes
that users can include in their inputs. Thus, we propose cross-modal attribute
insertions as a realistic perturbation strategy for vision-and-language data
that inserts visual attributes of the objects in the image into the
corresponding text (e.g., ""girl on a chair"" to ""little girl on a wooden
chair""). Our proposed approach for cross-modal attribute insertions is modular,
controllable, and task-agnostic. We find that augmenting input text using
cross-modal insertions causes state-of-the-art approaches for text-to-image
retrieval and cross-modal entailment to perform poorly, resulting in relative
drops of 15% in MRR and 20% in $F_1$ score, respectively. Crowd-sourced
annotations demonstrate that cross-modal insertions lead to higher quality
augmentations for multimodal data than augmentations using text-only data, and
are equivalent in quality to original examples. We release the code to
encourage robustness evaluations of deep vision-and-language models:
https://github.com/claws-lab/multimodal-robustness-xmai.",2306.11065v1,https://arxiv.org/pdf/2306.11065v1
"Benchmarking Robustness of Deep Reinforcement Learning approaches to
  Online Portfolio Management","Marc Velay, Bich-Liên Doan, Arpad Rimmel, Fabrice Popineau, Fabrice Daniel","Deep Reinforcement Learning approaches to Online Portfolio Selection have
grown in popularity in recent years. The sensitive nature of training
Reinforcement Learning agents implies a need for extensive efforts in market
representation, behavior objectives, and training processes, which have often
been lacking in previous works. We propose a training and evaluation process to
assess the performance of classical DRL algorithms for portfolio management. We
found that most Deep Reinforcement Learning algorithms were not robust, with
strategies generalizing poorly and degrading quickly during backtesting.",2306.10950v1,https://arxiv.org/pdf/2306.10950v1
"BNN-DP: Robustness Certification of Bayesian Neural Networks via Dynamic
  Programming","Steven Adams, Andrea Patane, Morteza Lahijanian, Luca Laurenti","In this paper, we introduce BNN-DP, an efficient algorithmic framework for
analysis of adversarial robustness of Bayesian Neural Networks (BNNs). Given a
compact set of input points $T\subset \mathbb{R}^n$, BNN-DP computes lower and
upper bounds on the BNN's predictions for all the points in $T$. The framework
is based on an interpretation of BNNs as stochastic dynamical systems, which
enables the use of Dynamic Programming (DP) algorithms to bound the prediction
range along the layers of the network. Specifically, the method uses bound
propagation techniques and convex relaxations to derive a backward recursion
procedure to over-approximate the prediction range of the BNN with piecewise
affine functions. The algorithm is general and can handle both regression and
classification tasks. On a set of experiments on various regression and
classification tasks and BNN architectures, we show that BNN-DP outperforms
state-of-the-art methods by up to four orders of magnitude in both tightness of
the bounds and computational efficiency.",2306.10742v1,https://arxiv.org/pdf/2306.10742v1
"FDNet: Focal Decomposed Network for Efficient, Robust and Practical Time
  Series Forecasting","Li Shen, Yuning Wei, Yangzhu Wang, Huaxin Qiu","This paper presents FDNet: a Focal Decomposed Network for efficient, robust
and practical time series forecasting. We break away from conventional deep
time series forecasting formulas which obtain prediction results from universal
feature maps of input sequences. In contrary, FDNet neglects universal
correlations of input elements and only extracts fine-grained local features
from input sequence. We show that: (1) Deep time series forecasting with only
fine-grained local feature maps of input sequence is feasible upon theoretical
basis. (2) By abandoning global coarse-grained feature maps, FDNet overcomes
distribution shift problem caused by changing dynamics of time series which is
common in real-world applications. (3) FDNet is not dependent on any inductive
bias of time series except basic auto-regression, making it general and
practical. Moreover, we propose focal input sequence decomposition method which
decomposes input sequence in a focal manner for efficient and robust
forecasting when facing Long Sequence Time series Input (LSTI) problem. FDNet
achieves competitive forecasting performances on six real-world benchmarks and
reduces prediction MSE by 38.4% on average compared with other thirteen SOTA
baselines. The source code is available at https://github.com/OrigamiSL/FDNet.",2306.10703v1,https://arxiv.org/pdf/2306.10703v1
"Dual-view Correlation Hybrid Attention Network for Robust Holistic
  Mammogram Classification","Zhiwei Wang, Junlin Xian, Kangyi Liu, Xin Li, Qiang Li, Xin Yang","Mammogram image is important for breast cancer screening, and typically
obtained in a dual-view form, i.e., cranio-caudal (CC) and mediolateral oblique
(MLO), to provide complementary information. However, previous methods mostly
learn features from the two views independently, which violates the clinical
knowledge and ignores the importance of dual-view correlation. In this paper,
we propose a dual-view correlation hybrid attention network (DCHA-Net) for
robust holistic mammogram classification. Specifically, DCHA-Net is carefully
designed to extract and reinvent deep features for the two views, and meanwhile
to maximize the underlying correlations between them. A hybrid attention
module, consisting of local relation and non-local attention blocks, is
proposed to alleviate the spatial misalignment of the paired views in the
correlation maximization. A dual-view correlation loss is introduced to
maximize the feature similarity between corresponding strip-like regions with
equal distance to the chest wall, motivated by the fact that their features
represent the same breast tissues, and thus should be highly-correlated.
Experimental results on two public datasets, i.e., INbreast and CBIS-DDSM,
demonstrate that DCHA-Net can well preserve and maximize feature correlations
across views, and thus outperforms the state-of-the-arts for classifying a
whole mammogram as malignant or not.",2306.10676v1,https://arxiv.org/pdf/2306.10676v1
"DropCompute: simple and more robust distributed synchronous training via
  compute variance reduction","Niv Giladi, Shahar Gottlieb, Moran Shkolnik, Asaf Karnieli, Ron Banner, Elad Hoffer, Kfir Yehuda Levy, Daniel Soudry","Background: Distributed training is essential for large scale training of
deep neural networks (DNNs). The dominant methods for large scale DNN training
are synchronous (e.g. All-Reduce), but these require waiting for all workers in
each step. Thus, these methods are limited by the delays caused by straggling
workers. Results: We study a typical scenario in which workers are straggling
due to variability in compute time. We find an analytical relation between
compute time properties and scalability limitations, caused by such straggling
workers. With these findings, we propose a simple yet effective decentralized
method to reduce the variation among workers and thus improve the robustness of
synchronous training. This method can be integrated with the widely used
All-Reduce. Our findings are validated on large-scale training tasks using 200
Gaudi Accelerators.",2306.10598v2,https://arxiv.org/pdf/2306.10598v2
"Group Orthogonalization Regularization For Vision Models Adaptation and
  Robustness","Yoav Kurtz, Noga Bar, Raja Giryes","As neural networks become deeper, the redundancy within their parameters
increases. This phenomenon has led to several methods that attempt to reduce
the correlation between convolutional filters. We propose a computationally
efficient regularization technique that encourages orthonormality between
groups of filters within the same layer. Our experiments show that when
incorporated into recent adaptation methods for diffusion models and vision
transformers (ViTs), this regularization improves performance on downstream
tasks. We further show improved robustness when group orthogonality is enforced
during adversarial training. Our code is available at
https://github.com/YoavKurtz/GOR.",2306.10001v2,https://arxiv.org/pdf/2306.10001v2
Adversarially robust clustering with optimality guarantees,"Soham Jana, Kun Yang, Sanjeev Kulkarni","We consider the problem of clustering data points coming from sub-Gaussian
mixtures. Existing methods that provably achieve the optimal mislabeling error,
such as the Lloyd algorithm, are usually vulnerable to outliers. In contrast,
clustering methods seemingly robust to adversarial perturbations are not known
to satisfy the optimal statistical guarantees. We propose a simple robust
algorithm based on the coordinatewise median that obtains the optimal
mislabeling rate even when we allow adversarial outliers to be present. Our
algorithm achieves the optimal error rate in constant iterations when a weak
initialization condition is satisfied. In the absence of outliers, in fixed
dimensions, our theoretical guarantees are similar to that of the Lloyd
algorithm. Extensive experiments on various simulated and public datasets are
conducted to support the theoretical guarantees of our method.",2306.09977v2,https://arxiv.org/pdf/2306.09977v2
"You Don't Need Robust Machine Learning to Manage Adversarial Attack
  Risks","Edward Raff, Michel Benaroch, Andrew L. Farris","The robustness of modern machine learning (ML) models has become an
increasing concern within the community. The ability to subvert a model into
making errant predictions using seemingly inconsequential changes to input is
startling, as is our lack of success in building models robust to this concern.
Existing research shows progress, but current mitigations come with a high cost
and simultaneously reduce the model's accuracy. However, such trade-offs may
not be necessary when other design choices could subvert the risk. In this
survey we review the current literature on attacks and their real-world
occurrences, or limited evidence thereof, to critically evaluate the real-world
risks of adversarial machine learning (AML) for the average entity. This is
done with an eye toward how one would then mitigate these attacks in practice,
the risks for production deployment, and how those risks could be managed. In
doing so we elucidate that many AML threats do not warrant the cost and
trade-offs of robustness due to a low likelihood of attack or availability of
superior non-ML mitigations. Our analysis also recommends cases where an actor
should be concerned about AML to the degree where robust ML models are
necessary for a complete deployment.",2306.09951v1,https://arxiv.org/pdf/2306.09951v1
Wasserstein distributional robustness of neural networks,"Xingjian Bai, Guangyi He, Yifan Jiang, Jan Obloj","Deep neural networks are known to be vulnerable to adversarial attacks (AA).
For an image recognition task, this means that a small perturbation of the
original can result in the image being misclassified. Design of such attacks as
well as methods of adversarial training against them are subject of intense
research. We re-cast the problem using techniques of Wasserstein
distributionally robust optimization (DRO) and obtain novel contributions
leveraging recent insights from DRO sensitivity analysis. We consider a set of
distributional threat models. Unlike the traditional pointwise attacks, which
assume a uniform bound on perturbation of each input data point, distributional
threat models allow attackers to perturb inputs in a non-uniform way. We link
these more general attacks with questions of out-of-sample performance and
Knightian uncertainty. To evaluate the distributional robustness of neural
networks, we propose a first-order AA algorithm and its multi-step version. Our
attack algorithms include Fast Gradient Sign Method (FGSM) and Projected
Gradient Descent (PGD) as special cases. Furthermore, we provide a new
asymptotic estimate of the adversarial accuracy against distributional threat
models. The bound is fast to compute and first-order accurate, offering new
insights even for the pointwise AA. It also naturally yields out-of-sample
performance guarantees. We conduct numerical experiments on the CIFAR-10
dataset using DNNs on RobustBench to illustrate our theoretical results. Our
code is available at https://github.com/JanObloj/W-DRO-Adversarial-Methods.",2306.09844v1,https://arxiv.org/pdf/2306.09844v1
"AI Driven Near Real-time Locational Marginal Pricing Method: A
  Feasibility and Robustness Study","Naga Venkata Sai Jitin Jami, Juraj Kardoš, Olaf Schenk, Harald Köstler","Accurate price predictions are essential for market participants in order to
optimize their operational schedules and bidding strategies, especially in the
current context where electricity prices become more volatile and less
predictable using classical approaches. The Locational Marginal Pricing (LMP)
pricing mechanism is used in many modern power markets, where the traditional
approach utilizes optimal power flow (OPF) solvers. However, for large
electricity grids this process becomes prohibitively time-consuming and
computationally intensive. Machine learning (ML) based predictions could
provide an efficient tool for LMP prediction, especially in energy markets with
intermittent sources like renewable energy. This study evaluates the
performance of popular machine learning and deep learning models in predicting
LMP on multiple electricity grids. The accuracy and robustness of these models
in predicting LMP is assessed considering multiple scenarios. The results show
that ML models can predict LMP 4-5 orders of magnitude faster than traditional
OPF solvers with 5-6\% error rate, highlighting the potential of ML models in
LMP prediction for large-scale power models with the assistance of hardware
infrastructure like multi-core CPUs and GPUs in modern HPC clusters.",2306.10080v2,https://arxiv.org/pdf/2306.10080v2
"Evaluating the Robustness of Text-to-image Diffusion Models against
  Real-world Attacks","Hongcheng Gao, Hao Zhang, Yinpeng Dong, Zhijie Deng","Text-to-image (T2I) diffusion models (DMs) have shown promise in generating
high-quality images from textual descriptions. The real-world applications of
these models require particular attention to their safety and fidelity, but
this has not been sufficiently explored. One fundamental question is whether
existing T2I DMs are robust against variations over input texts. To answer it,
this work provides the first robustness evaluation of T2I DMs against
real-world attacks. Unlike prior studies that focus on malicious attacks
involving apocryphal alterations to the input texts, we consider an attack
space spanned by realistic errors (e.g., typo, glyph, phonetic) that humans can
make, to ensure semantic consistency. Given the inherent randomness of the
generation process, we develop novel distribution-based attack objectives to
mislead T2I DMs. We perform attacks in a black-box manner without any knowledge
of the model. Extensive experiments demonstrate the effectiveness of our method
for attacking popular T2I DMs and simultaneously reveal their non-trivial
robustness issues. Moreover, we provide an in-depth analysis of our method to
show that it is not designed to attack the text encoder in T2I DMs solely.",2306.13103v1,https://arxiv.org/pdf/2306.13103v1
"Span-Selective Linear Attention Transformers for Effective and Robust
  Schema-Guided Dialogue State Tracking","Björn Bebensee, Haejun Lee","In schema-guided dialogue state tracking models estimate the current state of
a conversation using natural language descriptions of the service schema for
generalization to unseen services. Prior generative approaches which decode
slot values sequentially do not generalize well to variations in schema, while
discriminative approaches separately encode history and schema and fail to
account for inter-slot and intent-slot dependencies. We introduce SPLAT, a
novel architecture which achieves better generalization and efficiency than
prior approaches by constraining outputs to a limited prediction space. At the
same time, our model allows for rich attention among descriptions and history
while keeping computation costs constrained by incorporating linear-time
attention. We demonstrate the effectiveness of our model on the Schema-Guided
Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon
existing models achieving 85.3 JGA on the SGD dataset. Further, we show
increased robustness on the SGD-X benchmark: our model outperforms the more
than 30$\times$ larger D3ST-XXL model by 5.0 points.",2306.09340v1,https://arxiv.org/pdf/2306.09340v1
"Stochastic Re-weighted Gradient Descent via Distributionally Robust
  Optimization","Ramnath Kumar, Kushal Majmundar, Dheeraj Nagaraj, Arun Sai Suggala","We present Re-weighted Gradient Descent (RGD), a novel optimization technique
that improves the performance of deep neural networks through dynamic sample
importance weighting. Our method is grounded in the principles of
distributionally robust optimization (DRO) with Kullback-Leibler divergence.
RGD is simple to implement, computationally efficient, and compatible with
widely used optimizers such as SGD and Adam. We demonstrate the broad
applicability and impact of RGD by achieving state-of-the-art results on
diverse benchmarks, including improvements of +0.7% (DomainBed), +1.44%
(tabular classification), +1.94% (GLUE with BERT), and +1.01% (ImageNet-1K with
ViT).",2306.09222v4,https://arxiv.org/pdf/2306.09222v4
Reward-Free Curricula for Training Robust World Models,"Marc Rigter, Minqi Jiang, Ingmar Posner","There has been a recent surge of interest in developing generally-capable
agents that can adapt to new tasks without additional training in the
environment. Learning world models from reward-free exploration is a promising
approach, and enables policies to be trained using imagined experience for new
tasks. However, achieving a general agent requires robustness across different
environments. In this work, we address the novel problem of generating
curricula in the reward-free setting to train robust world models. We consider
robustness in terms of minimax regret over all environment instantiations and
show that the minimax regret can be connected to minimising the maximum error
in the world model across environment instances. This result informs our
algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for
Robustness. WAKER selects environments for data collection based on the
estimated error of the world model for each environment. Our experiments
demonstrate that WAKER outperforms several baselines, resulting in improved
robustness, efficiency, and generalisation.",2306.09205v2,https://arxiv.org/pdf/2306.09205v2
"DiffAug: A Diffuse-and-Denoise Augmentation for Training Robust
  Classifiers","Chandramouli Sastry, Sri Harsha Dumpala, Sageev Oore","We introduce DiffAug, a simple and efficient diffusion-based augmentation
technique to train image classifiers for the crucial yet challenging goal of
improved classifier robustness. Applying DiffAug to a given example consists of
one forward-diffusion step followed by one reverse-diffusion step. Using both
ResNet-50 and Vision Transformer architectures, we comprehensively evaluate
classifiers trained with DiffAug and demonstrate the surprising effectiveness
of single-step reverse diffusion in improving robustness to covariate shifts,
certified adversarial accuracy and out of distribution detection. When we
combine DiffAug with other augmentations such as AugMix and DeepAugment we
demonstrate further improved robustness. Finally, building on this approach, we
also improve classifier-guided diffusion wherein we observe improvements in:
(i) classifier-generalization, (ii) gradient quality (i.e., improved perceptual
alignment) and (iii) image generation performance. We thus introduce a
computationally efficient technique for training with improved robustness that
does not require any additional data, and effectively complements existing
augmentation approaches.",2306.09192v2,https://arxiv.org/pdf/2306.09192v2
"DiAReL: Reinforcement Learning with Disturbance Awareness for Robust
  Sim2Real Policy Transfer in Robot Control","Mohammadhossein Malmir, Josip Josifovski, Noah Klarmann, Alois Knoll","Delayed Markov decision processes fulfill the Markov property by augmenting
the state space of agents with a finite time window of recently committed
actions. In reliance with these state augmentations, delay-resolved
reinforcement learning algorithms train policies to learn optimal interactions
with environments featured with observation or action delays. Although such
methods can directly be trained on the real robots, due to sample inefficiency,
limited resources or safety constraints, a common approach is to transfer
models trained in simulation to the physical robot. However, robotic
simulations rely on approximated models of the physical systems, which hinders
the sim2real transfer. In this work, we consider various uncertainties in the
modelling of the robot's dynamics as unknown intrinsic disturbances applied on
the system input. We introduce a disturbance-augmented Markov decision process
in delayed settings as a novel representation to incorporate disturbance
estimation in training on-policy reinforcement learning algorithms. The
proposed method is validated across several metrics on learning a robotic
reaching task and compared with disturbance-unaware baselines. The results show
that the disturbance-augmented models can achieve higher stabilization and
robustness in the control response, which in turn improves the prospects of
successful sim2real transfer.",2306.09010v1,https://arxiv.org/pdf/2306.09010v1
Modularity Trumps Invariance for Compositional Robustness,"Ian Mason, Anirban Sarkar, Tomotake Sasaki, Xavier Boix","By default neural networks are not robust to changes in data distribution.
This has been demonstrated with simple image corruptions, such as blurring or
adding noise, degrading image classification performance. Many methods have
been proposed to mitigate these issues but for the most part models are
evaluated on single corruptions. In reality, visual space is compositional in
nature, that is, that as well as robustness to elemental corruptions,
robustness to compositions of corruptions is also needed. In this work we
develop a compositional image classification task where, given a few elemental
corruptions, models are asked to generalize to compositions of these
corruptions. That is, to achieve compositional robustness. We experimentally
compare empirical risk minimization with an invariance building pairwise
contrastive loss and, counter to common intuitions in domain generalization,
achieve only marginal improvements in compositional robustness by encouraging
invariance. To move beyond invariance, following previously proposed inductive
biases that model architectures should reflect data structure, we introduce a
modular architecture whose structure replicates the compositional nature of the
task. We then show that this modular approach consistently achieves better
compositional robustness than non-modular approaches. We additionally find
empirical evidence that the degree of invariance between representations of
'in-distribution' elemental corruptions fails to correlate with robustness to
'out-of-distribution' compositions of corruptions.",2306.09005v1,https://arxiv.org/pdf/2306.09005v1
"Augment then Smooth: Reconciling Differential Privacy with Certified
  Robustness","Jiapeng Wu, Atiyeh Ashari Ghomi, David Glukhov, Jesse C. Cresswell, Franziska Boenisch, Nicolas Papernot","Machine learning models are susceptible to a variety of attacks that can
erode trust, including attacks against the privacy of training data, and
adversarial examples that jeopardize model accuracy. Differential privacy and
certified robustness are effective frameworks for combating these two threats
respectively, as they each provide future-proof guarantees. However, we show
that standard differentially private model training is insufficient for
providing strong certified robustness guarantees. Indeed, combining
differential privacy and certified robustness in a single system is
non-trivial, leading previous works to introduce complex training schemes that
lack flexibility. In this work, we present DP-CERT, a simple and effective
method that achieves both privacy and robustness guarantees simultaneously by
integrating randomized smoothing into standard differentially private model
training. Compared to the leading prior work, DP-CERT gives up to a 2.5%
increase in certified accuracy for the same differential privacy guarantee on
CIFAR10. Through in-depth persample metric analysis, we find that larger
certifiable radii correlate with smaller local Lipschitz constants, and show
that DP-CERT effectively reduces Lipschitz constants compared to other
differentially private training methods. The code is available at
github.com/layer6ailabs/dp-cert.",2306.08656v2,https://arxiv.org/pdf/2306.08656v2
"A Unified Framework of Graph Information Bottleneck for Robustness and
  Membership Privacy","Enyan Dai, Limeng Cui, Zhengyang Wang, Xianfeng Tang, Yinghan Wang, Monica Cheng, Bing Yin, Suhang Wang","Graph Neural Networks (GNNs) have achieved great success in modeling
graph-structured data. However, recent works show that GNNs are vulnerable to
adversarial attacks which can fool the GNN model to make desired predictions of
the attacker. In addition, training data of GNNs can be leaked under membership
inference attacks. This largely hinders the adoption of GNNs in high-stake
domains such as e-commerce, finance and bioinformatics. Though investigations
have been made in conducting robust predictions and protecting membership
privacy, they generally fail to simultaneously consider the robustness and
membership privacy. Therefore, in this work, we study a novel problem of
developing robust and membership privacy-preserving GNNs. Our analysis shows
that Information Bottleneck (IB) can help filter out noisy information and
regularize the predictions on labeled samples, which can benefit robustness and
membership privacy. However, structural noises and lack of labels in node
classification challenge the deployment of IB on graph-structured data. To
mitigate these issues, we propose a novel graph information bottleneck
framework that can alleviate structural noises with neighbor bottleneck. Pseudo
labels are also incorporated in the optimization to minimize the gap between
the predictions on the labeled set and unlabeled set for membership privacy.
Extensive experiments on real-world datasets demonstrate that our method can
give robust predictions and simultaneously preserve membership privacy.",2306.08604v1,https://arxiv.org/pdf/2306.08604v1
VIBR: Learning View-Invariant Value Functions for Robust Visual Control,"Tom Dupuis, Jaonary Rabarisoa, Quoc-Cuong Pham, David Filliat","End-to-end reinforcement learning on images showed significant progress in
the recent years. Data-based approach leverage data augmentation and domain
randomization while representation learning methods use auxiliary losses to
learn task-relevant features. Yet, reinforcement still struggles in visually
diverse environments full of distractions and spurious noise. In this work, we
tackle the problem of robust visual control at its core and present VIBR
(View-Invariant Bellman Residuals), a method that combines multi-view training
and invariant prediction to reduce out-of-distribution (OOD) generalization gap
for RL based visuomotor control. Our model-free approach improve baselines
performances without the need of additional representation learning objectives
and with limited additional computational cost. We show that VIBR outperforms
existing methods on complex visuo-motor control environment with high visual
perturbation. Our approach achieves state-of the-art results on the Distracting
Control Suite benchmark, a challenging benchmark still not solved by current
methods, where we evaluate the robustness to a number of visual perturbators,
as well as OOD generalization and extrapolation capabilities.",2306.08537v1,https://arxiv.org/pdf/2306.08537v1
Provably Personalized and Robust Federated Learning,"Mariel Werner, Lie He, Michael Jordan, Martin Jaggi, Sai Praneeth Karimireddy","Identifying clients with similar objectives and learning a model-per-cluster
is an intuitive and interpretable approach to personalization in federated
learning. However, doing so with provable and optimal guarantees has remained
an open challenge. We formalize this problem as a stochastic optimization
problem, achieving optimal convergence rates for a large class of loss
functions. We propose simple iterative algorithms which identify clusters of
similar clients and train a personalized model-per-cluster, using local client
gradients and flexible constraints on the clusters. The convergence rates of
our algorithms asymptotically match those obtained if we knew the true
underlying clustering of the clients and are provably robust in the Byzantine
setting where some fraction of the clients are malicious.",2306.08393v2,https://arxiv.org/pdf/2306.08393v2
Maestro: A Gamified Platform for Teaching AI Robustness,"Margarita Geleta, Jiacen Xu, Manikanta Loya, Junlin Wang, Sameer Singh, Zhou Li, Sergio Gago-Masague","Although the prevention of AI vulnerabilities is critical to preserve the
safety and privacy of users and businesses, educational tools for robust AI are
still underdeveloped worldwide. We present the design, implementation, and
assessment of Maestro. Maestro is an effective open-source game-based platform
that contributes to the advancement of robust AI education. Maestro provides
goal-based scenarios where college students are exposed to challenging
life-inspired assignments in a competitive programming environment. We assessed
Maestro's influence on students' engagement, motivation, and learning success
in robust AI. This work also provides insights into the design features of
online learning tools that promote active learning opportunities in the robust
AI domain. We analyzed the reflection responses (measured with Likert scales)
of 147 undergraduate students using Maestro in two quarterly college courses in
AI. According to the results, students who felt the acquisition of new skills
in robust AI tended to appreciate highly Maestro and scored highly on material
consolidation, curiosity, and mastery in robust AI. Moreover, the leaderboard,
our key gamification element in Maestro, has effectively contributed to
students' engagement and learning. Results also indicate that Maestro can be
effectively adapted to any course length and depth without losing its
educational quality.",2306.08238v1,https://arxiv.org/pdf/2306.08238v1
Uncertainty-Aware Robust Learning on Noisy Graphs,"Shuyi Chen, Kaize Ding, Shixiang Zhu","Graph neural networks have shown impressive capabilities in solving various
graph learning tasks, particularly excelling in node classification. However,
their effectiveness can be hindered by the challenges arising from the
widespread existence of noisy measurements associated with the topological or
nodal information present in real-world graphs. These inaccuracies in
observations can corrupt the crucial patterns within the graph data, ultimately
resulting in undesirable performance in practical applications. To address
these issues, this paper proposes a novel uncertainty-aware graph learning
framework motivated by distributionally robust optimization. Specifically, we
use a graph neural network-based encoder to embed the node features and find
the optimal node embeddings by minimizing the worst-case risk through a minimax
formulation. Such an uncertainty-aware learning process leads to improved node
representations and a more robust graph predictive model that effectively
mitigates the impact of uncertainty arising from data noise. Our experimental
result shows that the proposed framework achieves superior predictive
performance compared to the state-of-the-art baselines under various noisy
settings.",2306.08210v1,https://arxiv.org/pdf/2306.08210v1
Robustly Learning a Single Neuron via Sharpness,"Puqian Wang, Nikos Zarifis, Ilias Diakonikolas, Jelena Diakonikolas","We study the problem of learning a single neuron with respect to the
$L_2^2$-loss in the presence of adversarial label noise. We give an efficient
algorithm that, for a broad family of activations including ReLUs, approximates
the optimal $L_2^2$-error within a constant factor. Our algorithm applies under
much milder distributional assumptions compared to prior work. The key
ingredient enabling our results is a novel connection to local error bounds
from optimization theory.",2306.07892v1,https://arxiv.org/pdf/2306.07892v1
Temporal Gradient Inversion Attacks with Robust Optimization,"Bowen Li, Hanlin Gu, Ruoxin Chen, Jie Li, Chentao Wu, Na Ruan, Xueming Si, Lixin Fan","Federated Learning (FL) has emerged as a promising approach for collaborative
model training without sharing private data. However, privacy concerns
regarding information exchanged during FL have received significant research
attention. Gradient Inversion Attacks (GIAs) have been proposed to reconstruct
the private data retained by local clients from the exchanged gradients. While
recovering private data, the data dimensions and the model complexity increase,
which thwart data reconstruction by GIAs. Existing methods adopt prior
knowledge about private data to overcome those challenges. In this paper, we
first observe that GIAs with gradients from a single iteration fail to
reconstruct private data due to insufficient dimensions of leaked gradients,
complex model architectures, and invalid gradient information. We investigate a
Temporal Gradient Inversion Attack with a Robust Optimization framework, called
TGIAs-RO, which recovers private data without any prior knowledge by leveraging
multiple temporal gradients. To eliminate the negative impacts of outliers,
e.g., invalid gradients for collaborative optimization, robust statistics are
proposed. Theoretical guarantees on the recovery performance and robustness of
TGIAs-RO against invalid gradients are also provided. Extensive empirical
results on MNIST, CIFAR10, ImageNet and Reuters 21578 datasets show that the
proposed TGIAs-RO with 10 temporal gradients improves reconstruction
performance compared to state-of-the-art methods, even for large batch sizes
(up to 128), complex models like ResNet18, and large datasets like ImageNet
(224*224 pixels). Furthermore, the proposed attack method inspires further
exploration of privacy-preserving methods in the context of FL.",2306.07883v1,https://arxiv.org/pdf/2306.07883v1
"Robustness and Generalization Performance of Deep Learning Models on
  Cyber-Physical Systems: A Comparative Study","Alexander Windmann, Henrik Steude, Oliver Niggemann","Deep learning (DL) models have seen increased attention for time series
forecasting, yet the application on cyber-physical systems (CPS) is hindered by
the lacking robustness of these methods. Thus, this study evaluates the
robustness and generalization performance of DL architectures on multivariate
time series data from CPS. Our investigation focuses on the models' ability to
handle a range of perturbations, such as sensor faults and noise, and assesses
their impact on overall performance. Furthermore, we test the generalization
and transfer learning capabilities of these models by exposing them to
out-of-distribution (OOD) samples. These include deviations from standard
system operations, while the core dynamics of the underlying physical system
are preserved. Additionally, we test how well the models respond to several
data augmentation techniques, including added noise and time warping. Our
experimental framework utilizes a simulated three-tank system, proposed as a
novel benchmark for evaluating the robustness and generalization performance of
DL algorithms in CPS data contexts. The findings reveal that certain DL model
architectures and training techniques exhibit superior effectiveness in
handling OOD samples and various perturbations. These insights have significant
implications for the development of DL models that deliver reliable and robust
performance in real-world CPS applications.",2306.07737v1,https://arxiv.org/pdf/2306.07737v1
Theoretical Foundations of Adversarially Robust Learning,Omar Montasser,"Despite extraordinary progress, current machine learning systems have been
shown to be brittle against adversarial examples: seemingly innocuous but
carefully crafted perturbations of test examples that cause machine learning
predictors to misclassify. Can we learn predictors robust to adversarial
examples? and how? There has been much empirical interest in this contemporary
challenge in machine learning, and in this thesis, we address it from a
theoretical perspective.
  In this thesis, we explore what robustness properties can we hope to
guarantee against adversarial examples and develop an understanding of how to
algorithmically guarantee them. We illustrate the need to go beyond traditional
approaches and principles such as empirical risk minimization and uniform
convergence, and make contributions that can be categorized as follows: (1)
introducing problem formulations capturing aspects of emerging practical
challenges in robust learning, (2) designing new learning algorithms with
provable robustness guarantees, and (3) characterizing the complexity of robust
learning and fundamental limitations on the performance of any algorithm.",2306.07723v1,https://arxiv.org/pdf/2306.07723v1
"TopP&R: Robust Support Estimation Approach for Evaluating Fidelity and
  Diversity in Generative Models","Pum Jun Kim, Yoojin Jang, Jisu Kim, Jaejun Yoo","We propose a robust and reliable evaluation metric for generative models by
introducing topological and statistical treatments for rigorous support
estimation. Existing metrics, such as Inception Score (IS), Frechet Inception
Distance (FID), and the variants of Precision and Recall (P&R), heavily rely on
supports that are estimated from sample features. However, the reliability of
their estimation has not been seriously discussed (and overlooked) even though
the quality of the evaluation entirely depends on it. In this paper, we propose
Topological Precision and Recall (TopP&R, pronounced 'topper'), which provides
a systematic approach to estimating supports, retaining only topologically and
statistically important features with a certain level of confidence. This not
only makes TopP&R strong for noisy features, but also provides statistical
consistency. Our theoretical and experimental results show that TopP&R is
robust to outliers and non-independent and identically distributed (Non-IID)
perturbations, while accurately capturing the true trend of change in samples.
To the best of our knowledge, this is the first evaluation metric focused on
the robust estimation of the support and provides its statistical consistency
under noise.",2306.08013v6,https://arxiv.org/pdf/2306.08013v6
On the Robustness of Removal-Based Feature Attributions,"Chris Lin, Ian Covert, Su-In Lee","To explain predictions made by complex machine learning models, many feature
attribution methods have been developed that assign importance scores to input
features. Some recent work challenges the robustness of these methods by
showing that they are sensitive to input and model perturbations, while other
work addresses this issue by proposing robust attribution methods. However,
previous work on attribution robustness has focused primarily on gradient-based
feature attributions, whereas the robustness of removal-based attribution
methods is not currently well understood. To bridge this gap, we theoretically
characterize the robustness properties of removal-based feature attributions.
Specifically, we provide a unified analysis of such methods and derive upper
bounds for the difference between intact and perturbed attributions, under
settings of both input and model perturbations. Our empirical results on
synthetic and real-world data validate our theoretical results and demonstrate
their practical implications, including the ability to increase attribution
robustness by improving the model's Lipschitz regularity.",2306.07462v2,https://arxiv.org/pdf/2306.07462v2
Robust Reinforcement Learning through Efficient Adversarial Herding,"Juncheng Dong, Hao-Lun Hsu, Qitong Gao, Vahid Tarokh, Miroslav Pajic","Although reinforcement learning (RL) is considered the gold standard for
policy design, it may not always provide a robust solution in various
scenarios. This can result in severe performance degradation when the
environment is exposed to potential disturbances. Adversarial training using a
two-player max-min game has been proven effective in enhancing the robustness
of RL agents. In this work, we extend the two-player game by introducing an
adversarial herd, which involves a group of adversaries, in order to address
($\textit{i}$) the difficulty of the inner optimization problem, and
($\textit{ii}$) the potential over pessimism caused by the selection of a
candidate adversary set that may include unlikely scenarios. We first prove
that adversarial herds can efficiently approximate the inner optimization
problem. Then we address the second issue by replacing the worst-case
performance in the inner optimization with the average performance over the
worst-$k$ adversaries. We evaluate the proposed method on multiple MuJoCo
environments. Experimental results demonstrate that our approach consistently
generates more robust policies.",2306.07408v1,https://arxiv.org/pdf/2306.07408v1
"Composing Efficient, Robust Tests for Policy Selection","Dustin Morrill, Thomas J. Walsh, Daniel Hernandez, Peter R. Wurman, Peter Stone","Modern reinforcement learning systems produce many high-quality policies
throughout the learning process. However, to choose which policy to actually
deploy in the real world, they must be tested under an intractable number of
environmental conditions. We introduce RPOSST, an algorithm to select a small
set of test cases from a larger pool based on a relatively small number of
sample evaluations. RPOSST treats the test case selection problem as a
two-player game and optimizes a solution with provable $k$-of-$N$ robustness,
bounding the error relative to a test that used all the test cases in the pool.
Empirical results demonstrate that RPOSST finds a small set of test cases that
identify high quality policies in a toy one-shot game, poker datasets, and a
high-fidelity racing simulator.",2306.07372v1,https://arxiv.org/pdf/2306.07372v1
"AROID: Improving Adversarial Robustness Through Online Instance-Wise
  Data Augmentation","Lin Li, Jianing Qiu, Michael Spratling","Deep neural networks are vulnerable to adversarial examples. Adversarial
training (AT) is an effective defense against adversarial examples. However, AT
is prone to overfitting which degrades robustness substantially. Recently, data
augmentation (DA) was shown to be effective in mitigating robust overfitting if
appropriately designed and optimized for AT. This work proposes a new method to
automatically learn online, instance-wise, DA policies to improve robust
generalization for AT. This is the first automated DA method specific for
robustness. A novel policy learning objective, consisting of Vulnerability,
Affinity and Diversity, is proposed and shown to be sufficiently effective and
efficient to be practical for automatic DA generation during AT. Importantly,
our method dramatically reduces the cost of policy search from the 5000 hours
of AutoAugment and the 412 hours of IDBH to 9 hours, making automated DA more
practical to use for adversarial robustness. This allows our method to
efficiently explore a large search space for a more effective DA policy and
evolve the policy as training progresses. Empirically, our method is shown to
outperform all competitive DA methods across various model architectures and
datasets. Our DA policy reinforced vanilla AT to surpass several
state-of-the-art AT methods regarding both accuracy and robustness. It can also
be combined with those advanced AT methods to further boost robustness. Code
and pre-trained models are available at https://github.com/TreeLLi/AROID.",2306.07197v2,https://arxiv.org/pdf/2306.07197v2
DRCFS: Doubly Robust Causal Feature Selection,"Francesco Quinzan, Ashkan Soleymani, Patrick Jaillet, Cristian R. Rojas, Stefan Bauer","Knowing the features of a complex system that are highly relevant to a
particular target variable is of fundamental interest in many areas of science.
Existing approaches are often limited to linear settings, sometimes lack
guarantees, and in most cases, do not scale to the problem at hand, in
particular to images. We propose DRCFS, a doubly robust feature selection
method for identifying the causal features even in nonlinear and high
dimensional settings. We provide theoretical guarantees, illustrate necessary
conditions for our assumptions, and perform extensive experiments across a wide
range of simulated and semi-synthetic datasets. DRCFS significantly outperforms
existing state-of-the-art methods, selecting robust features even in
challenging highly non-linear and high-dimensional problems.",2306.07024v3,https://arxiv.org/pdf/2306.07024v3
"How robust accuracy suffers from certified training with convex
  relaxations","Piersilvio De Bartolomeis, Jacob Clarysse, Amartya Sanyal, Fanny Yang","Adversarial attacks pose significant threats to deploying state-of-the-art
classifiers in safety-critical applications. Two classes of methods have
emerged to address this issue: empirical defences and certified defences.
Although certified defences come with robustness guarantees, empirical defences
such as adversarial training enjoy much higher popularity among practitioners.
In this paper, we systematically compare the standard and robust error of these
two robust training paradigms across multiple computer vision tasks. We show
that in most tasks and for both $\mathscr{l}_\infty$-ball and
$\mathscr{l}_2$-ball threat models, certified training with convex relaxations
suffers from worse standard and robust error than adversarial training. We
further explore how the error gap between certified and adversarial training
depends on the threat model and the data distribution. In particular, besides
the perturbation budget, we identify as important factors the shape of the
perturbation set and the implicit margin of the data distribution. We support
our arguments with extensive ablations on both synthetic and image datasets.",2306.06995v1,https://arxiv.org/pdf/2306.06995v1
A Graph Transformer-Driven Approach for Network Robustness Learning,"Yu Zhang, Jia Li, Jie Ding, Xiang Li","Learning and analysis of network robustness, including controllability
robustness and connectivity robustness, is critical for various networked
systems against attacks. Traditionally, network robustness is determined by
attack simulations, which is very time-consuming and even incapable for
large-scale networks. Network Robustness Learning, which is dedicated to
learning network robustness with high precision and high speed, provides a
powerful tool to analyze network robustness by replacing simulations. In this
paper, a novel versatile and unified robustness learning approach via graph
transformer (NRL-GT) is proposed, which accomplishes the task of
controllability robustness learning and connectivity robustness learning from
multiple aspects including robustness curve learning, overall robustness
learning, and synthetic network classification. Numerous experiments show that:
1) NRL-GT is a unified learning framework for controllability robustness and
connectivity robustness, demonstrating a strong generalization ability to
ensure high precision when training and test sets are distributed differently;
2) Compared to the cutting-edge methods, NRL-GT can simultaneously perform
network robustness learning from multiple aspects and obtains superior results
in less time. NRL-GT is also able to deal with complex networks of different
size with low learning error and high efficiency; 3) It is worth mentioning
that the backbone of NRL-GT can serve as a transferable feature learning module
for complex networks of different size and different downstream tasks.",2306.06913v2,https://arxiv.org/pdf/2306.06913v2
"Robustifying DARTS by Eliminating Information Bypass Leakage via
  Explicit Sparse Regularization","Jiuling Zhang, Zhiming Ding","Differentiable architecture search (DARTS) is a promising end to end NAS
method which directly optimizes the architecture parameters through general
gradient descent. However, DARTS is brittle to the catastrophic failure
incurred by the skip connection in the search space. Recent studies also cast
doubt on the basic underlying hypotheses of DARTS which are argued to be
inherently prone to the performance discrepancy between the continuous-relaxed
supernet in the training phase and the discretized finalnet in the evaluation
phase. We figure out that the robustness problem and the skepticism can both be
explained by the information bypass leakage during the training of the
supernet. This naturally highlights the vital role of the sparsity of
architecture parameters in the training phase which has not been well developed
in the past. We thus propose a novel sparse-regularized approximation and an
efficient mixed-sparsity training scheme to robustify DARTS by eliminating the
information bypass leakage. We subsequently conduct extensive experiments on
multiple search spaces to demonstrate the effectiveness of our method.",2306.06858v1,https://arxiv.org/pdf/2306.06858v1
"Multimodal Audio-textual Architecture for Robust Spoken Language
  Understanding","Anderson R. Avila, Mehdi Rezagholizadeh, Chao Xing","Recent voice assistants are usually based on the cascade spoken language
understanding (SLU) solution, which consists of an automatic speech recognition
(ASR) engine and a natural language understanding (NLU) system. Because such
approach relies on the ASR output, it often suffers from the so-called ASR
error propagation. In this work, we investigate impacts of this ASR error
propagation on state-of-the-art NLU systems based on pre-trained language
models (PLM), such as BERT and RoBERTa. Moreover, a multimodal language
understanding (MLU) module is proposed to mitigate SLU performance degradation
caused by errors present in the ASR transcript. The MLU benefits from
self-supervised features learned from both audio and text modalities,
specifically Wav2Vec for speech and Bert/RoBERTa for language. Our MLU combines
an encoder network to embed the audio signal and a text encoder to process text
transcripts followed by a late fusion layer to fuse audio and text logits. We
found that the proposed MLU showed to be robust towards poor quality ASR
transcripts, while the performance of BERT and RoBERTa are severely
compromised. Our model is evaluated on five tasks from three SLU datasets and
robustness is tested using ASR transcripts from three ASR engines. Results show
that the proposed approach effectively mitigates the ASR error propagation
problem, surpassing the PLM models' performance across all datasets for the
academic ASR engine.",2306.06819v2,https://arxiv.org/pdf/2306.06819v2
Kepler: Robust Learning for Faster Parametric Query Optimization,"Lyric Doshi, Vincent Zhuang, Gaurav Jain, Ryan Marcus, Haoyu Huang, Deniz Altinbüken, Eugene Brevdo, Campbell Fraser","Most existing parametric query optimization (PQO) techniques rely on
traditional query optimizer cost models, which are often inaccurate and result
in suboptimal query performance. We propose Kepler, an end-to-end
learning-based approach to PQO that demonstrates significant speedups in query
latency over a traditional query optimizer. Central to our method is Row Count
Evolution (RCE), a novel plan generation algorithm based on perturbations in
the sub-plan cardinality space. While previous approaches require accurate cost
models, we bypass this requirement by evaluating candidate plans via actual
execution data and training an ML model to predict the fastest plan given
parameter binding values. Our models leverage recent advances in neural network
uncertainty in order to robustly predict faster plans while avoiding
regressions in query performance. Experimentally, we show that Kepler achieves
significant improvements in query runtime on multiple datasets on PostgreSQL.",2306.06798v2,https://arxiv.org/pdf/2306.06798v2
Precise and Generalized Robustness Certification for Neural Networks,"Yuanyuan Yuan, Shuai Wang, Zhendong Su","The objective of neural network (NN) robustness certification is to determine
if a NN changes its predictions when mutations are made to its inputs. While
most certification research studies pixel-level or a few geometrical-level and
blurring operations over images, this paper proposes a novel framework, GCERT,
which certifies NN robustness under a precise and unified form of diverse
semantic-level image mutations. We formulate a comprehensive set of
semantic-level image mutations uniformly as certain directions in the latent
space of generative models. We identify two key properties, independence and
continuity, that convert the latent space into a precise and analysis-friendly
input space representation for certification. GCERT can be smoothly integrated
with de facto complete, incomplete, or quantitative certification frameworks.
With its precise input space representation, GCERT enables for the first time
complete NN robustness certification with moderate cost under diverse
semantic-level input mutations, such as weather-filter, style transfer, and
perceptual changes (e.g., opening/closing eyes). We show that GCERT enables
certifying NN robustness under various common and security-sensitive scenarios
like autonomous driving.",2306.06747v1,https://arxiv.org/pdf/2306.06747v1
Neural Architecture Design and Robustness: A Dataset,"Steffen Jung, Jovita Lukasik, Margret Keuper","Deep learning models have proven to be successful in a wide range of machine
learning tasks. Yet, they are often highly sensitive to perturbations on the
input data which can lead to incorrect decisions with high confidence,
hampering their deployment for practical use-cases. Thus, finding architectures
that are (more) robust against perturbations has received much attention in
recent years. Just like the search for well-performing architectures in terms
of clean accuracy, this usually involves a tedious trial-and-error process with
one additional challenge: the evaluation of a network's robustness is
significantly more expensive than its evaluation for clean accuracy. Thus, the
aim of this paper is to facilitate better streamlined research on architectural
design choices with respect to their impact on robustness as well as, for
example, the evaluation of surrogate measures for robustness. We therefore
borrow one of the most commonly considered search spaces for neural
architecture search for image classification, NAS-Bench-201, which contains a
manageable size of 6466 non-isomorphic network designs. We evaluate all these
networks on a range of common adversarial attacks and corruption types and
introduce a database on neural architecture design and robustness evaluations.
We further present three exemplary use cases of this dataset, in which we (i)
benchmark robustness measurements based on Jacobian and Hessian matrices for
their robustness predictability, (ii) perform neural architecture search on
robust accuracies, and (iii) provide an initial analysis of how architectural
design choices affect robustness. We find that carefully crafting the topology
of a network can have substantial impact on its robustness, where networks with
the same parameter count range in mean adversarial robust accuracy from
20%-41%. Code and data is available at http://robustness.vision/.",2306.06712v1,https://arxiv.org/pdf/2306.06712v1
"Learning Robust and Consistent Time Series Representations: A Dilated
  Inception-Based Approach","Anh Duy Nguyen, Trang H. Tran, Hieu H. Pham, Phi Le Nguyen, Lam M. Nguyen","Representation learning for time series has been an important research area
for decades. Since the emergence of the foundation models, this topic has
attracted a lot of attention in contrastive self-supervised learning, to solve
a wide range of downstream tasks. However, there have been several challenges
for contrastive time series processing. First, there is no work considering
noise, which is one of the critical factors affecting the efficacy of time
series tasks. Second, there is a lack of efficient yet lightweight encoder
architectures that can learn informative representations robust to various
downstream tasks. To fill in these gaps, we initiate a novel sampling strategy
that promotes consistent representation learning with the presence of noise in
natural time series. In addition, we propose an encoder architecture that
utilizes dilated convolution within the Inception block to create a scalable
and robust network architecture with a wide receptive field. Experiments
demonstrate that our method consistently outperforms state-of-the-art methods
in forecasting, classification, and abnormality detection tasks, e.g. ranks
first over two-thirds of the classification UCR datasets, with only $40\%$ of
the parameters compared to the second-best approach. Our source code for
CoInception framework is accessible at
https://github.com/anhduy0911/CoInception.",2306.06579v1,https://arxiv.org/pdf/2306.06579v1
Boosting Adversarial Robustness using Feature Level Stochastic Smoothing,"Sravanti Addepalli, Samyak Jain, Gaurang Sriramanan, R. Venkatesh Babu","Advances in adversarial defenses have led to a significant improvement in the
robustness of Deep Neural Networks. However, the robust accuracy of present
state-ofthe-art defenses is far from the requirements in critical applications
such as robotics and autonomous navigation systems. Further, in practical use
cases, network prediction alone might not suffice, and assignment of a
confidence value for the prediction can prove crucial. In this work, we propose
a generic method for introducing stochasticity in the network predictions, and
utilize this for smoothing decision boundaries and rejecting low confidence
predictions, thereby boosting the robustness on accepted samples. The proposed
Feature Level Stochastic Smoothing based classification also results in a boost
in robustness without rejection over existing adversarial training methods.
Finally, we combine the proposed method with adversarial detection methods, to
achieve the benefits of both approaches.",2306.06462v1,https://arxiv.org/pdf/2306.06462v1
"Continually learning out-of-distribution spatiotemporal data for robust
  energy forecasting","Arian Prabowo, Kaixuan Chen, Hao Xue, Subbu Sethuvenkatraman, Flora D. Salim","Forecasting building energy usage is essential for promoting sustainability
and reducing waste, as it enables building managers to optimize energy
consumption and reduce costs. This importance is magnified during anomalous
periods, such as the COVID-19 pandemic, which have disrupted occupancy patterns
and made accurate forecasting more challenging. Forecasting energy usage during
anomalous periods is difficult due to changes in occupancy patterns and energy
usage behavior. One of the primary reasons for this is the shift in
distribution of occupancy patterns, with many people working or learning from
home. This has created a need for new forecasting methods that can adapt to
changing occupancy patterns. Online learning has emerged as a promising
solution to this challenge, as it enables building managers to adapt to changes
in occupancy patterns and adjust energy usage accordingly. With online
learning, models can be updated incrementally with each new data point,
allowing them to learn and adapt in real-time. Another solution is to use human
mobility data as a proxy for occupancy, leveraging the prevalence of mobile
devices to track movement patterns and infer occupancy levels. Human mobility
data can be useful in this context as it provides a way to monitor occupancy
patterns without relying on traditional sensors or manual data collection
methods. We have conducted extensive experiments using data from six buildings
to test the efficacy of these approaches. However, deploying these methods in
the real world presents several challenges.",2306.06385v2,https://arxiv.org/pdf/2306.06385v2
"Robust Twin Parametric Margin Support Vector Machine for Multiclass
  Classification","Renato De Leone, Francesca Maggioni, Andrea Spinelli","In this paper, we present novel Twin Parametric Margin Support Vector Machine
(TPMSVM) models to tackle the problem of multiclass classification. We explore
the cases of linear and nonlinear classifiers and propose two possible
alternatives for the final decision function. Since real-world observations are
plagued by measurement errors and noise, data uncertainties need to be
considered in the optimization models. For this reason, we construct
bounded-by-norm uncertainty sets around each sample and derive the robust
counterpart of deterministic models by means of robust optimization techniques.
Finally, we test the proposed TPMSVM methodology on real-world datasets,
showing the good performance of the approach.",2306.06213v2,https://arxiv.org/pdf/2306.06213v2
Robust Data-driven Prescriptiveness Optimization,"Mehran Poursoltani, Erick Delage, Angelos Georghiou","The abundance of data has led to the emergence of a variety of optimization
techniques that attempt to leverage available side information to provide more
anticipative decisions. The wide range of methods and contexts of application
have motivated the design of a universal unitless measure of performance known
as the coefficient of prescriptiveness. This coefficient was designed to
quantify both the quality of contextual decisions compared to a reference one
and the prescriptive power of side information. To identify policies that
maximize the former in a data-driven context, this paper introduces a
distributionally robust contextual optimization model where the coefficient of
prescriptiveness substitutes for the classical empirical risk minimization
objective. We present a bisection algorithm to solve this model, which relies
on solving a series of linear programs when the distributional ambiguity set
has an appropriate nested form and polyhedral structure. Studying a contextual
shortest path problem, we evaluate the robustness of the resulting policies
against alternative methods when the out-of-sample dataset is subject to
varying amounts of distribution shift.",2306.05937v2,https://arxiv.org/pdf/2306.05937v2
"Detecting Adversarial Directions in Deep Reinforcement Learning to Make
  Robust Decisions","Ezgi Korkmaz, Jonah Brown-Cohen","Learning in MDPs with highly complex state representations is currently
possible due to multiple advancements in reinforcement learning algorithm
design. However, this incline in complexity, and furthermore the increase in
the dimensions of the observation came at the cost of volatility that can be
taken advantage of via adversarial attacks (i.e. moving along worst-case
directions in the observation space). To solve this policy instability problem
we propose a novel method to detect the presence of these non-robust directions
via local quadratic approximation of the deep neural policy loss. Our method
provides a theoretical basis for the fundamental cut-off between safe
observations and adversarial observations. Furthermore, our technique is
computationally efficient, and does not depend on the methods used to produce
the worst-case directions. We conduct extensive experiments in the Arcade
Learning Environment with several different adversarial attack techniques. Most
significantly, we demonstrate the effectiveness of our approach even in the
setting where non-robust directions are explicitly optimized to circumvent our
proposed method.",2306.05873v1,https://arxiv.org/pdf/2306.05873v1
"Bring Your Own (Non-Robust) Algorithm to Solve Robust MDPs by Estimating
  The Worst Kernel","Kaixin Wang, Uri Gadot, Navdeep Kumar, Kfir Levy, Shie Mannor","Robust Markov Decision Processes (RMDPs) provide a framework for sequential
decision-making that is robust to perturbations on the transition kernel.
However, current RMDP methods are often limited to small-scale problems,
hindering their use in high-dimensional domains. To bridge this gap, we present
EWoK, a novel online approach to solve RMDP that Estimates the Worst transition
Kernel to learn robust policies. Unlike previous works that regularize the
policy or value updates, EWoK achieves robustness by simulating the worst
scenarios for the agent while retaining complete flexibility in the learning
process. Notably, EWoK can be applied on top of any off-the-shelf {\em
non-robust} RL algorithm, enabling easy scaling to high-dimensional domains.
Our experiments, spanning from simple Cartpole to high-dimensional DeepMind
Control Suite environments, demonstrate the effectiveness and applicability of
the EWoK paradigm as a practical method for learning robust policies.",2306.05859v2,https://arxiv.org/pdf/2306.05859v2
"Extending Kernel PCA through Dualization: Sparsity, Robustness and Fast
  Algorithms","Francesco Tonin, Alex Lambert, Panagiotis Patrinos, Johan A. K. Suykens","The goal of this paper is to revisit Kernel Principal Component Analysis
(KPCA) through dualization of a difference of convex functions. This allows to
naturally extend KPCA to multiple objective functions and leads to efficient
gradient-based algorithms avoiding the expensive SVD of the Gram matrix.
Particularly, we consider objective functions that can be written as Moreau
envelopes, demonstrating how to promote robustness and sparsity within the same
framework. The proposed method is evaluated on synthetic and real-world
benchmarks, showing significant speedup in KPCA training time as well as
highlighting the benefits in terms of robustness and sparsity.",2306.05815v1,https://arxiv.org/pdf/2306.05815v1
"Specifying and Solving Robust Empirical Risk Minimization Problems Using
  CVXPY","Eric Luxenberg, Dhruv Malik, Yuanzhi Li, Aarti Singh, Stephen Boyd","We consider robust empirical risk minimization (ERM), where model parameters
are chosen to minimize the worst-case empirical loss when each data point
varies over a given convex uncertainty set. In some simple cases, such problems
can be expressed in an analytical form. In general the problem can be made
tractable via dualization, which turns a min-max problem into a min-min
problem. Dualization requires expertise and is tedious and error-prone. We
demonstrate how CVXPY can be used to automate this dualization procedure in a
user-friendly manner. Our framework allows practitioners to specify and solve
robust ERM problems with a general class of convex losses, capturing many
standard regression and classification problems. Users can easily specify any
complex uncertainty set that is representable via disciplined convex
programming (DCP) constraints.",2306.05649v2,https://arxiv.org/pdf/2306.05649v2
"Robustness Testing for Multi-Agent Reinforcement Learning: State
  Perturbations on Critical Agents","Ziyuan Zhou, Guanjun Liu","Multi-Agent Reinforcement Learning (MARL) has been widely applied in many
fields such as smart traffic and unmanned aerial vehicles. However, most MARL
algorithms are vulnerable to adversarial perturbations on agent states.
Robustness testing for a trained model is an essential step for confirming the
trustworthiness of the model against unexpected perturbations. This work
proposes a novel Robustness Testing framework for MARL that attacks states of
Critical Agents (RTCA). The RTCA has two innovations: 1) a Differential
Evolution (DE) based method to select critical agents as victims and to advise
the worst-case joint actions on them; and 2) a team cooperation policy
evaluation method employed as the objective function for the optimization of
DE. Then, adversarial state perturbations of the critical agents are generated
based on the worst-case joint actions. This is the first robustness testing
framework with varying victim agents. RTCA demonstrates outstanding performance
in terms of the number of victim agents and destroying cooperation policies.",2306.06136v1,https://arxiv.org/pdf/2306.06136v1
"Robust Brain Age Estimation via Regression Models and MRI-derived
  Features","Mansoor Ahmed, Usama Sardar, Sarwan Ali, Shafiq Alam, Murray Patterson, Imdad Ullah Khan","The determination of biological brain age is a crucial biomarker in the
assessment of neurological disorders and understanding of the morphological
changes that occur during aging. Various machine learning models have been
proposed for estimating brain age through Magnetic Resonance Imaging (MRI) of
healthy controls. However, developing a robust brain age estimation (BAE)
framework has been challenging due to the selection of appropriate MRI-derived
features and the high cost of MRI acquisition. In this study, we present a
novel BAE framework using the Open Big Healthy Brain (OpenBHB) dataset, which
is a new multi-site and publicly available benchmark dataset that includes
region-wise feature metrics derived from T1-weighted (T1-w) brain MRI scans of
3965 healthy controls aged between 6 to 86 years. Our approach integrates three
different MRI-derived region-wise features and different regression models,
resulting in a highly accurate brain age estimation with a Mean Absolute Error
(MAE) of 3.25 years, demonstrating the framework's robustness. We also analyze
our model's regression-based performance on gender-wise (male and female)
healthy test groups. The proposed BAE framework provides a new approach for
estimating brain age, which has important implications for the understanding of
neurological disorders and age-related brain changes.",2306.05514v1,https://arxiv.org/pdf/2306.05514v1
Robust Explainer Recommendation for Time Series Classification,"Thu Trang Nguyen, Thach Le Nguyen, Georgiana Ifrim","Time series classification is a task which deals with temporal sequences, a
prevalent data type common in domains such as human activity recognition,
sports analytics and general sensing. In this area, interest in explainability
has been growing as explanation is key to understand the data and the model
better. Recently, a great variety of techniques have been proposed and adapted
for time series to provide explanation in the form of saliency maps, where the
importance of each data point in the time series is quantified with a numerical
value. However, the saliency maps can and often disagree, so it is unclear
which one to use. This paper provides a novel framework to quantitatively
evaluate and rank explanation methods for time series classification. We show
how to robustly evaluate the informativeness of a given explanation method
(i.e., relevance for the classification task), and how to compare explanations
side-by-side. The goal is to recommend the best explainer for a given time
series classification dataset. We propose AMEE, a Model-Agnostic Explanation
Evaluation framework, for recommending saliency-based explanations for time
series classification. In this approach, data perturbation is added to the
input time series guided by each explanation. Our results show that perturbing
discriminative parts of the time series leads to significant changes in
classification accuracy, which can be used to evaluate each explanation. To be
robust to different types of perturbations and different types of classifiers,
we aggregate the accuracy loss across perturbations and classifiers. This novel
approach allows us to recommend the best explainer among a set of different
explainers, including random and oracle explainers. We provide a quantitative
and qualitative analysis for synthetic datasets, a variety of timeseries
datasets, as well as a real-world case study with known expert ground truth.",2306.05501v4,https://arxiv.org/pdf/2306.05501v4
"Noise-Robust Loss Functions: Enhancing Bounded Losses for Large-Scale
  Noisy Data Learning","Max Staats, Matthias Thamm, Bernd Rosenow","Large annotated datasets inevitably contain noisy labels, which poses a major
challenge for training deep neural networks as they easily memorize the labels.
Noise-robust loss functions have emerged as a notable strategy to counteract
this issue, but it remains challenging to create a robust loss function which
is not susceptible to underfitting. Through a quantitative approach, this paper
explores the limited overlap between the network output at initialization and
regions of non-vanishing gradients of bounded loss functions in the initial
learning phase. Using these insights, we address underfitting of the MAE loss
with a novel method denoted as logit bias, which adds a real number $\epsilon$
to the logit at the position of the correct class. This method enables bounded
losses to learn, even on datasets like WebVision, consisting of over a million
images from 1000 classes. Extensive numerical experiments show that the logit
bias enables MAE to compete with state-of-the-art noise robust loss functions.
In addition, we demonstrate that our method can be used to determine optimal
parameters for other loss functions -- without having to train networks.
Remarkably, our method determines the hyperparameters based on the number of
classes, resulting in loss functions which require zero dataset or
noise-dependent parameters.",2306.05497v2,https://arxiv.org/pdf/2306.05497v2
"Is Attentional Channel Processing Design Required? Comprehensive
  Analysis Of Robustness Between Vision Transformers And Fully Attentional
  Networks","Abhishri Ajit Medewar, Swanand Ashokrao Kavitkar","The robustness testing has been performed for standard CNN models and Vision
Transformers, however there is a lack of comprehensive study between the
robustness of traditional Vision Transformers without an extra attentional
channel design and the latest fully attentional network(FAN) models. So in this
paper, we use the ImageNet dataset to compare the robustness of fully
attentional network(FAN) models with traditional Vision Transformers to
understand the role of an attentional channel processing design using white box
attacks and also study the transferability between the same using black box
attacks.",2306.05495v1,https://arxiv.org/pdf/2306.05495v1
"Enhancing Robustness of AI Offensive Code Generators via Data
  Augmentation","Cristina Improta, Pietro Liguori, Roberto Natella, Bojan Cukic, Domenico Cotroneo","In this work, we present a method to add perturbations to the code
descriptions to create new inputs in natural language (NL) from
well-intentioned developers that diverge from the original ones due to the use
of new words or because they miss part of them. The goal is to analyze how and
to what extent perturbations affect the performance of AI code generators in
the context of security-oriented code. First, we show that perturbed
descriptions preserve the semantics of the original, non-perturbed ones. Then,
we use the method to assess the robustness of three state-of-the-art code
generators against the newly perturbed inputs, showing that the performance of
these AI-based solutions is highly affected by perturbations in the NL
descriptions. To enhance their robustness, we use the method to perform data
augmentation, i.e., to increase the variability and diversity of the NL
descriptions in the training data, proving its effectiveness against both
perturbed and non-perturbed code descriptions.",2306.05079v2,https://arxiv.org/pdf/2306.05079v2
"A Gradient-based Approach for Online Robust Deep Neural Network Training
  with Noisy Labels","Yifan Yang, Alec Koppel, Zheng Zhang","Learning with noisy labels is an important topic for scalable training in
many real-world scenarios. However, few previous research considers this
problem in the online setting, where the arrival of data is streaming. In this
paper, we propose a novel gradient-based approach to enable the detection of
noisy labels for the online learning of model parameters, named Online
Gradient-based Robust Selection (OGRS). In contrast to the previous sample
selection approach for the offline training that requires the estimation of a
clean ratio of the dataset before each epoch of training, OGRS can
automatically select clean samples by steps of gradient update from datasets
with varying clean ratios without changing the parameter setting. During the
training process, the OGRS method selects clean samples at each iteration and
feeds the selected sample to incrementally update the model parameters. We
provide a detailed theoretical analysis to demonstrate data selection process
is converging to the low-loss region of the sample space, by introducing and
proving the sub-linear local Lagrangian regret of the non-convex constrained
optimization problem. Experimental results show that it outperforms
state-of-the-art methods in different settings.",2306.05046v1,https://arxiv.org/pdf/2306.05046v1
"Generalizable Lightweight Proxy for Robust NAS against Diverse
  Perturbations","Hyeonjeong Ha, Minseon Kim, Sung Ju Hwang","Recent neural architecture search (NAS) frameworks have been successful in
finding optimal architectures for given conditions (e.g., performance or
latency). However, they search for optimal architectures in terms of their
performance on clean images only, while robustness against various types of
perturbations or corruptions is crucial in practice. Although there exist
several robust NAS frameworks that tackle this issue by integrating adversarial
training into one-shot NAS, however, they are limited in that they only
consider robustness against adversarial attacks and require significant
computational resources to discover optimal architectures for a single task,
which makes them impractical in real-world scenarios. To address these
challenges, we propose a novel lightweight robust zero-cost proxy that
considers the consistency across features, parameters, and gradients of both
clean and perturbed images at the initialization state. Our approach
facilitates an efficient and rapid search for neural architectures capable of
learning generalizable features that exhibit robustness across diverse
perturbations. The experimental results demonstrate that our proxy can rapidly
and efficiently search for neural architectures that are consistently robust
against various perturbations on multiple benchmark datasets and diverse search
spaces, largely outperforming existing clean zero-shot NAS and robust NAS with
reduced search cost.",2306.05031v2,https://arxiv.org/pdf/2306.05031v2
"Robust Learning with Progressive Data Expansion Against Spurious
  Correlation","Yihe Deng, Yu Yang, Baharan Mirzasoleiman, Quanquan Gu","While deep learning models have shown remarkable performance in various
tasks, they are susceptible to learning non-generalizable spurious features
rather than the core features that are genuinely correlated to the true label.
In this paper, beyond existing analyses of linear models, we theoretically
examine the learning process of a two-layer nonlinear convolutional neural
network in the presence of spurious features. Our analysis suggests that
imbalanced data groups and easily learnable spurious features can lead to the
dominance of spurious features during the learning process. In light of this,
we propose a new training algorithm called PDE that efficiently enhances the
model's robustness for a better worst-group performance. PDE begins with a
group-balanced subset of training data and progressively expands it to
facilitate the learning of the core features. Experiments on synthetic and
real-world benchmark datasets confirm the superior performance of our method on
models such as ResNets and Transformers. On average, our method achieves a 2.8%
improvement in worst-group accuracy compared with the state-of-the-art method,
while enjoying up to 10x faster training efficiency. Codes are available at
https://github.com/uclaml/PDE.",2306.04949v2,https://arxiv.org/pdf/2306.04949v2
"Intrinsic Dimension Estimation for Robust Detection of AI-Generated
  Texts","Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Serguei Barannikov, Irina Piontkovskaya, Sergey Nikolenko, Evgeny Burnaev","Rapidly increasing quality of AI-generated content makes it difficult to
distinguish between human and AI-generated texts, which may lead to undesirable
consequences for society. Therefore, it becomes increasingly important to study
the properties of human texts that are invariant over different text domains
and varying proficiency of human writers, can be easily calculated for any
language, and can robustly separate natural and AI-generated texts regardless
of the generation model and sampling method. In this work, we propose such an
invariant for human-written texts, namely the intrinsic dimensionality of the
manifold underlying the set of embeddings for a given text sample. We show that
the average intrinsic dimensionality of fluent texts in a natural language is
hovering around the value $9$ for several alphabet-based languages and around
$7$ for Chinese, while the average intrinsic dimensionality of AI-generated
texts for each language is $\approx 1.5$ lower, with a clear statistical
separation between human-generated and AI-generated distributions. This
property allows us to build a score-based artificial text detector. The
proposed detector's accuracy is stable over text domains, generator models, and
human writer proficiency levels, outperforming SOTA detectors in model-agnostic
and cross-domain scenarios by a significant margin.",2306.04723v2,https://arxiv.org/pdf/2306.04723v2
"Robust-DefReg: A Robust Deformable Point Cloud Registration Method based
  on Graph Convolutional Neural Networks","Sara Monji-Azad, Marvin Kinz, Jürgen Hesser","Point cloud registration is a fundamental problem in computer vision that
aims to estimate the transformation between corresponding sets of points.
Non-rigid registration, in particular, involves addressing challenges including
various levels of deformation, noise, outliers, and data incompleteness. This
paper introduces Robust-DefReg, a robust non-rigid point cloud registration
method based on graph convolutional networks (GCNNs). Robust-DefReg is a
coarse-to-fine registration approach within an end-to-end pipeline, leveraging
the advantages of both coarse and fine methods. The method learns global
features to find correspondences between source and target point clouds, to
enable appropriate initial alignment, and subsequently fine registration. The
simultaneous achievement of high accuracy and robustness across all challenges
is reported less frequently in existing studies, making it a key objective of
the Robust-DefReg method. The proposed method achieves high accuracy in large
deformations while maintaining computational efficiency. This method possesses
three primary attributes: high accuracy, robustness to different challenges,
and computational efficiency. The experimental results show that the proposed
Robust-DefReg holds significant potential as a foundational architecture for
future investigations in non-rigid point cloud registration. The source code of
Robust-DefReg is available.",2306.04701v1,https://arxiv.org/pdf/2306.04701v1
Multiscale Flow for Robust and Optimal Cosmological Analysis,"Biwei Dai, Uros Seljak","We propose Multiscale Flow, a generative Normalizing Flow that creates
samples and models the field-level likelihood of two-dimensional cosmological
data such as weak lensing. Multiscale Flow uses hierarchical decomposition of
cosmological fields via a wavelet basis, and then models different wavelet
components separately as Normalizing Flows. The log-likelihood of the original
cosmological field can be recovered by summing over the log-likelihood of each
wavelet term. This decomposition allows us to separate the information from
different scales and identify distribution shifts in the data such as unknown
scale-dependent systematics. The resulting likelihood analysis can not only
identify these types of systematics, but can also be made optimal, in the sense
that the Multiscale Flow can learn the full likelihood at the field without any
dimensionality reduction. We apply Multiscale Flow to weak lensing mock
datasets for cosmological inference, and show that it significantly outperforms
traditional summary statistics such as power spectrum and peak counts, as well
as novel Machine Learning based summary statistics such as scattering transform
and convolutional neural networks. We further show that Multiscale Flow is able
to identify distribution shifts not in the training data such as baryonic
effects. Finally, we demonstrate that Multiscale Flow can be used to generate
realistic samples of weak lensing data.",2306.04689v2,https://arxiv.org/pdf/2306.04689v2
"Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis,
  and LLMs Evaluations","Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, Maosong Sun","This paper reexamines the research on out-of-distribution (OOD) robustness in
the field of NLP. We find that the distribution shift settings in previous
studies commonly lack adequate challenges, hindering the accurate evaluation of
OOD robustness. To address these issues, we propose a benchmark construction
protocol that ensures clear differentiation and challenging distribution
shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution
robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we
conduct a series of experiments on pre-trained language models for analysis and
evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the
relationship between in-distribution (ID) and OOD performance. We identify
three typical types that unveil the inner learning mechanism, which could
potentially facilitate the forecasting of OOD robustness, correlating with the
advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and
find that, despite exhibiting some effectiveness in specific cases, they do not
offer significant improvement compared to vanilla fine-tuning. Further, we
evaluate 5 LLMs with various adaptation paradigms and find that when sufficient
ID data is available, fine-tuning domain-specific models outperform LLMs on ID
examples significantly. However, in the case of OOD instances, prioritizing
LLMs with in-context learning yields better results. We identify that both
fine-tuned small models and LLMs face challenges in effectively addressing
downstream tasks. The code is public at
\url{https://github.com/lifan-yuan/OOD_NLP}.",2306.04618v2,https://arxiv.org/pdf/2306.04618v2
"PromptRobust: Towards Evaluating the Robustness of Large Language Models
  on Adversarial Prompts","Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, Xing Xie","The increasing reliance on Large Language Models (LLMs) across academia and
industry necessitates a comprehensive understanding of their robustness to
prompts. In response to this vital need, we introduce PromptRobust, a
robustness benchmark designed to measure LLMs' resilience to adversarial
prompts. This study uses a plethora of adversarial textual attacks targeting
prompts across multiple levels: character, word, sentence, and semantic. The
adversarial prompts, crafted to mimic plausible user errors like typos or
synonyms, aim to evaluate how slight deviations can affect LLM outcomes while
maintaining semantic integrity. These prompts are then employed in diverse
tasks including sentiment analysis, natural language inference, reading
comprehension, machine translation, and math problem-solving. Our study
generates 4,788 adversarial prompts, meticulously evaluated over 8 tasks and 13
datasets. Our findings demonstrate that contemporary LLMs are not robust to
adversarial prompts. Furthermore, we present a comprehensive analysis to
understand the mystery behind prompt robustness and its transferability. We
then offer insightful robustness analysis and pragmatic recommendations for
prompt composition, beneficial to both researchers and everyday users.",2306.04528v5,https://arxiv.org/pdf/2306.04528v5
"Training-Free Neural Active Learning with Initialization-Robustness
  Guarantees","Apivich Hemachandra, Zhongxiang Dai, Jasraj Singh, See-Kiong Ng, Bryan Kian Hsiang Low","Existing neural active learning algorithms have aimed to optimize the
predictive performance of neural networks (NNs) by selecting data for
labelling. However, other than a good predictive performance, being robust
against random parameter initializations is also a crucial requirement in
safety-critical applications. To this end, we introduce our expected variance
with Gaussian processes (EV-GP) criterion for neural active learning, which is
theoretically guaranteed to select data points which lead to trained NNs with
both (a) good predictive performances and (b) initialization robustness.
Importantly, our EV-GP criterion is training-free, i.e., it does not require
any training of the NN during data selection, which makes it computationally
efficient. We empirically demonstrate that our EV-GP criterion is highly
correlated with both initialization robustness and generalization performance,
and show that it consistently outperforms baseline methods in terms of both
desiderata, especially in situations with limited initial data or large batch
sizes.",2306.04454v1,https://arxiv.org/pdf/2306.04454v1
"Label Shift Quantification with Robustness Guarantees via Distribution
  Feature Matching","Bastien Dussap, Gilles Blanchard, Badr-Eddine Chérief-Abdellatif","Quantification learning deals with the task of estimating the target label
distribution under label shift. In this paper, we first present a unifying
framework, distribution feature matching (DFM), that recovers as particular
instances various estimators introduced in previous literature. We derive a
general performance bound for DFM procedures, improving in several key aspects
upon previous bounds derived in particular cases. We then extend this analysis
to study robustness of DFM procedures in the misspecified setting under
departure from the exact label shift hypothesis, in particular in the case of
contamination of the target by an unknown distribution. These theoretical
findings are confirmed by a detailed numerical study on simulated and
real-world datasets. We also introduce an efficient, scalable and robust
version of kernel-based DFM using the Random Fourier Feature principle.",2306.04376v2,https://arxiv.org/pdf/2306.04376v2
"Robust and Efficient Fault Diagnosis of mm-Wave Active Phased Arrays
  using Baseband Signal","Martin H. Nielsen, Yufeng Zhang, Changbin Xue, Jian Ren, Yingzeng Yin, Ming Shen, Gert F. Pedersen","One key communication block in 5G and 6G radios is the active phased array
(APA). To ensure reliable operation, efficient and timely fault diagnosis of
APAs on-site is crucial. To date, fault diagnosis has relied on measurement of
frequency domain radiation patterns using costly equipment and multiple
strictly controlled measurement probes, which are time-consuming, complex, and
therefore infeasible for on-site deployment. This paper proposes a novel method
exploiting a Deep Neural Network (DNN) tailored to extract the features hidden
in the baseband in-phase and quadrature signals for classifying the different
faults. It requires only a single probe in one measurement point for fast and
accurate diagnosis of the faulty elements and components in APAs.
  Validation of the proposed method is done using a commercial 28 GHz APA.
Accuracies of 99% and 80% have been demonstrated for single- and multi-element
failure detection, respectively. Three different test scenarios are
investigated: on-off antenna elements, phase variations, and magnitude
attenuation variations. In a low signal to noise ratio of 4 dB, stable fault
detection accuracy above 90% is maintained. This is all achieved with a
detection time of milliseconds (e.g 6~ms), showing a high potential for on-site
deployment.",2306.04360v1,https://arxiv.org/pdf/2306.04360v1
Optimal Transport Model Distributional Robustness,"Van-Anh Nguyen, Trung Le, Anh Tuan Bui, Thanh-Toan Do, Dinh Phung","Distributional robustness is a promising framework for training deep learning
models that are less vulnerable to adversarial examples and data distribution
shifts. Previous works have mainly focused on exploiting distributional
robustness in the data space. In this work, we explore an optimal
transport-based distributional robustness framework in model spaces.
Specifically, we examine a model distribution within a Wasserstein ball
centered on a given model distribution that maximizes the loss. We have
developed theories that enable us to learn the optimal robust center model
distribution. Interestingly, our developed theories allow us to flexibly
incorporate the concept of sharpness awareness into training, whether it's a
single model, ensemble models, or Bayesian Neural Networks, by considering
specific forms of the center model distribution. These forms include a Dirac
delta distribution over a single model, a uniform distribution over several
models, and a general Bayesian Neural Network. Furthermore, we demonstrate that
Sharpness-Aware Minimization (SAM) is a specific case of our framework when
using a Dirac delta distribution over a single model, while our framework can
be seen as a probabilistic extension of SAM. To validate the effectiveness of
our framework in the aforementioned settings, we conducted extensive
experiments, and the results reveal remarkable improvements compared to the
baselines.",2306.04178v2,https://arxiv.org/pdf/2306.04178v2
"Transferable Adversarial Robustness for Categorical Data via Universal
  Robust Embeddings","Klim Kireev, Maksym Andriushchenko, Carmela Troncoso, Nicolas Flammarion","Research on adversarial robustness is primarily focused on image and text
data. Yet, many scenarios in which lack of robustness can result in serious
risks, such as fraud detection, medical diagnosis, or recommender systems often
do not rely on images or text but instead on tabular data. Adversarial
robustness in tabular data poses two serious challenges. First, tabular
datasets often contain categorical features, and therefore cannot be tackled
directly with existing optimization procedures. Second, in the tabular domain,
algorithms that are not based on deep networks are widely used and offer great
performance, but algorithms to enhance robustness are tailored to neural
networks (e.g. adversarial training).
  In this paper, we tackle both challenges. We present a method that allows us
to train adversarially robust deep networks for tabular data and to transfer
this robustness to other classifiers via universal robust embeddings tailored
to categorical data. These embeddings, created using a bilevel alternating
minimization framework, can be transferred to boosted trees or random forests
making them robust without the need for adversarial training while preserving
their high accuracy on tabular data. We show that our methods outperform
existing techniques within a practical threat model suitable for tabular data.",2306.04064v2,https://arxiv.org/pdf/2306.04064v2
"Deploying a Robust Active Preference Elicitation Algorithm on MTurk:
  Experiment Design, Interface, and Evaluation for COVID-19 Patient
  Prioritization","Caroline M. Johnston, Patrick Vossler, Simon Blessenohl, Phebe Vayanos","Preference elicitation leverages AI or optimization to learn stakeholder
preferences in settings ranging from marketing to public policy. The online
robust preference elicitation procedure of arXiv:2003.01899 has been shown in
simulation to outperform various other elicitation procedures in terms of
effectively learning individuals' true utilities. However, as with any
simulation, the method makes a series of assumptions that cannot easily be
verified to hold true beyond simulation. Thus, we propose to validate the
robust method's performance using real users, focusing on the particular
challenge of selecting policies for prioritizing COVID-19 patients for scarce
hospital resources during the pandemic. To this end, we develop an online
platform for preference elicitation where users report their preferences
between alternatives over a moderate number of pairwise comparisons chosen by a
particular elicitation procedure. We recruit 193 Amazon Mechanical Turk (MTurk)
workers to report their preferences and demonstrate that the robust method
outperforms asking random queries by 21%, the next best performing method in
the simulated results of arXiv:2003.01899, in terms of recommending policies
with a higher utility.",2306.04061v2,https://arxiv.org/pdf/2306.04061v2
"Improving Fairness and Robustness in End-to-End Speech Recognition
  through unsupervised clustering","Irina-Elena Veliche, Pascale Fung","The challenge of fairness arises when Automatic Speech Recognition (ASR)
systems do not perform equally well for all sub-groups of the population. In
the past few years there have been many improvements in overall speech
recognition quality, but without any particular focus on advancing Equality and
Equity for all user groups for whom systems do not perform well. ASR fairness
is therefore also a robustness issue. Meanwhile, data privacy also takes
priority in production systems. In this paper, we present a privacy preserving
approach to improve fairness and robustness of end-to-end ASR without using
metadata, zip codes, or even speaker or utterance embeddings directly in
training. We extract utterance level embeddings using a speaker ID model
trained on a public dataset, which we then use in an unsupervised fashion to
create acoustic clusters. We use cluster IDs instead of speaker utterance
embeddings as extra features during model training, which shows improvements
for all demographic groups and in particular for different accents.",2306.06083v1,https://arxiv.org/pdf/2306.06083v1
"Considering Human Factors in Risk Maps for Robust and Foresighted Driver
  Warning","Tim Puphal, Ryohei Hirano, Malte Probst, Raphael Wenzel, Akihito Kimata","Driver support systems that include human states in the support process is an
active research field. Many recent approaches allow, for example, to sense the
driver's drowsiness or awareness of the driving situation. However, so far,
this rich information has not been utilized much for improving the
effectiveness of support systems. In this paper, we therefore propose a warning
system that uses human states in the form of driver errors and can warn users
in some cases of upcoming risks several seconds earlier than the state of the
art systems not considering human factors. The system consists of a behavior
planner Risk Maps which directly changes its prediction of the surrounding
driving situation based on the sensed driver errors. By checking if this
driver's behavior plan is objectively safe, a more robust and foresighted
driver warning is achieved. In different simulations of a dynamic lane change
and intersection scenarios, we show how the driver's behavior plan can become
unsafe, given the estimate of driver errors, and experimentally validate the
advantages of considering human factors.",2306.03849v1,https://arxiv.org/pdf/2306.03849v1
"Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy
  Learning","Kwangho Kim, José R. Zubizarreta","We propose a simple and general framework for nonparametric estimation of
heterogeneous treatment effects under fairness constraints. Under standard
regularity conditions, we show that the resulting estimators possess the double
robustness property. We use this framework to characterize the trade-off
between fairness and the maximum welfare achievable by the optimal policy. We
evaluate the methods in a simulation study and illustrate them in a real-world
case study.",2306.03625v2,https://arxiv.org/pdf/2306.03625v2
"PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward
  Learning for Robotic Manipulation","Runze Liu, Yali Du, Fengshuo Bai, Jiafei Lyu, Xiu Li","In preference-based Reinforcement Learning (RL), obtaining a large number of
preference labels are both time-consuming and costly. Furthermore, the queried
human preferences cannot be utilized for the new tasks. In this paper, we
propose Zero-shot Cross-task Preference Alignment and Robust Reward Learning
(PEARL), which learns policies from cross-task preference transfer without any
human labels of the target task. Our contributions include two novel components
that facilitate the transfer and learning process. The first is Cross-task
Preference Alignment (CPA), which transfers the preferences between tasks via
optimal transport. The key idea of CPA is to use Gromov-Wasserstein distance to
align the trajectories between tasks, and the solved optimal transport matrix
serves as the correspondence between trajectories. The target task preferences
are computed as the weighted sum of source task preference labels with the
correspondence as weights. Moreover, to ensure robust learning from these
transferred labels, we introduce Robust Reward Learning (RRL), which considers
both reward mean and uncertainty by modeling rewards as Gaussian distributions.
Empirical results on robotic manipulation tasks from Meta-World and Robomimic
demonstrate that our method is capable of transferring preference labels across
tasks accurately and then learns well-behaved policies. Notably, our approach
significantly exceeds existing methods when there are few human preferences.
The code and videos of our method are available at:
https://sites.google.com/view/pearl-preference.",2306.03615v2,https://arxiv.org/pdf/2306.03615v2
"Benchmarking Robustness of AI-Enabled Multi-sensor Fusion Systems:
  Challenges and Opportunities","Xinyu Gao, Zhijie Wang, Yang Feng, Lei Ma, Zhenyu Chen, Baowen Xu","Multi-Sensor Fusion (MSF) based perception systems have been the foundation
in supporting many industrial applications and domains, such as self-driving
cars, robotic arms, and unmanned aerial vehicles. Over the past few years, the
fast progress in data-driven artificial intelligence (AI) has brought a
fast-increasing trend to empower MSF systems by deep learning techniques to
further improve performance, especially on intelligent systems and their
perception systems. Although quite a few AI-enabled MSF perception systems and
techniques have been proposed, up to the present, limited benchmarks that focus
on MSF perception are publicly available. Given that many intelligent systems
such as self-driving cars are operated in safety-critical contexts where
perception systems play an important role, there comes an urgent need for a
more in-depth understanding of the performance and reliability of these MSF
systems. To bridge this gap, we initiate an early step in this direction and
construct a public benchmark of AI-enabled MSF-based perception systems
including three commonly adopted tasks (i.e., object detection, object
tracking, and depth completion). Based on this, to comprehensively understand
MSF systems' robustness and reliability, we design 14 common and realistic
corruption patterns to synthesize large-scale corrupted datasets. We further
perform a systematic evaluation of these systems through our large-scale
evaluation. Our results reveal the vulnerability of the current AI-enabled MSF
perception systems, calling for researchers and practitioners to take
robustness and reliability into account when designing AI-enabled MSF.",2306.03454v2,https://arxiv.org/pdf/2306.03454v2
A Robust Likelihood Model for Novelty Detection,"Ranya Almohsen, Shivang Patel, Donald A. Adjeroh, Gianfranco Doretto","Current approaches to novelty or anomaly detection are based on deep neural
networks. Despite their effectiveness, neural networks are also vulnerable to
imperceptible deformations of the input data. This is a serious issue in
critical applications, or when data alterations are generated by an adversarial
attack. While this is a known problem that has been studied in recent years for
the case of supervised learning, the case of novelty detection has received
very limited attention. Indeed, in this latter setting the learning is
typically unsupervised because outlier data is not available during training,
and new approaches for this case need to be investigated. We propose a new
prior that aims at learning a robust likelihood for the novelty test, as a
defense against attacks. We also integrate the same prior with a
state-of-the-art novelty detection approach. Because of the geometric
properties of that approach, the resulting robust training is computationally
very efficient. An initial evaluation of the method indicates that it is
effective at improving performance with respect to the standard models in the
absence and presence of attacks.",2306.03331v1,https://arxiv.org/pdf/2306.03331v1
Nonlinear Distributionally Robust Optimization,"Mohammed Rayyan Sheriff, Peyman Mohajerin Esfahani","This article focuses on a class of distributionally robust optimization (DRO)
problems where, unlike the growing body of the literature, the objective
function is potentially nonlinear in the distribution. Existing methods to
optimize nonlinear functions in probability space use the Frechet derivatives,
which present both theoretical and computational challenges. Motivated by this,
we propose an alternative notion for the derivative and corresponding
smoothness based on Gateaux (G)-derivative for generic risk measures. These
concepts are explained via three running risk measure examples of variance,
entropic risk, and risk on finite support sets. We then propose a G-derivative
based Frank-Wolfe (FW) algorithm for generic nonlinear optimization problems in
probability spaces and establish its convergence under the proposed notion of
smoothness in a completely norm-independent manner. We use the set-up of the FW
algorithm to devise a methodology to compute a saddle point of the nonlinear
DRO problem. Finally, we validate our theoretical results on two cases of the
entropic and variance risk measures in the context of portfolio selection
problems. In particular, we analyze their regularity conditions and ""sufficient
statistic"", compute the respective FW-oracle in various settings, and confirm
the theoretical outcomes through numerical validation.",2306.03202v2,https://arxiv.org/pdf/2306.03202v2
From Robustness to Explainability and Back Again,"Xuanxiang Huang, Joao Marques-Silva","In contrast with ad-hoc methods for eXplainable Artificial Intelligence
(XAI), formal explainability offers important guarantees of rigor. However,
formal explainability is hindered by poor scalability for some families of
classifiers, the most significant being neural networks. As a result, there are
concerns as to whether formal explainability might serve to complement other
approaches in delivering trustworthy AI. This paper addresses the limitation of
scalability of formal explainability, and proposes novel algorithms for
computing formal explanations. The novel algorithm computes explanations by
answering instead a number of robustness queries, and such that the number of
such queries is at most linear on the number of features. Consequently, the
proposed algorithm establishes a direct relationship between the practical
complexity of formal explainability and that of robustness. More importantly,
the paper generalizes the definition of formal explanation, thereby allowing
the use of robustness tools that are based on different distance norms, and
also by reasoning in terms of some target degree of robustness. The experiments
validate the practical efficiency of the proposed approach.",2306.03048v2,https://arxiv.org/pdf/2306.03048v2
"Evaluating robustness of support vector machines with the Lagrangian
  dual approach","Yuting Liu, Hong Gu, Pan Qin","Adversarial examples bring a considerable security threat to support vector
machines (SVMs), especially those used in safety-critical applications. Thus,
robustness verification is an essential issue for SVMs, which can provide
provable robustness against various kinds of adversary attacks. The evaluation
results obtained through the robustness verification can provide a safe
guarantee for the use of SVMs. The existing verification method does not often
perform well in verifying SVMs with nonlinear kernels. To this end, we propose
a method to improve the verification performance for SVMs with nonlinear
kernels. We first formalize the adversarial robustness evaluation of SVMs as an
optimization problem. Then a lower bound of the original problem is obtained by
solving the Lagrangian dual problem of the original problem. Finally, the
adversarial robustness of SVMs is evaluated concerning the lower bound. We
evaluate the adversarial robustness of SVMs with linear and nonlinear kernels
on the MNIST and Fashion-MNIST datasets. The experimental results show that the
percentage of provable robustness obtained by our method on the test set is
better than that of the state-of-the-art.",2306.02639v1,https://arxiv.org/pdf/2306.02639v1
Enhance Diffusion to Improve Robust Generalization,"Jianhui Sun, Sanchit Sinha, Aidong Zhang","Deep neural networks are susceptible to human imperceptible adversarial
perturbations. One of the strongest defense mechanisms is \emph{Adversarial
Training} (AT). In this paper, we aim to address two predominant problems in
AT. First, there is still little consensus on how to set hyperparameters with a
performance guarantee for AT research, and customized settings impede a fair
comparison between different model designs in AT research. Second, the robustly
trained neural networks struggle to generalize well and suffer from tremendous
overfitting. This paper focuses on the primary AT framework - Projected
Gradient Descent Adversarial Training (PGD-AT). We approximate the dynamic of
PGD-AT by a continuous-time Stochastic Differential Equation (SDE), and show
that the diffusion term of this SDE determines the robust generalization. An
immediate implication of this theoretical finding is that robust generalization
is positively correlated with the ratio between learning rate and batch size.
We further propose a novel approach, \emph{Diffusion Enhanced Adversarial
Training} (DEAT), to manipulate the diffusion term to improve robust
generalization with virtually no extra computational burden. We theoretically
show that DEAT obtains a tighter generalization bound than PGD-AT. Our
empirical investigation is extensive and firmly attests that DEAT universally
outperforms PGD-AT by a significant margin.",2306.02618v2,https://arxiv.org/pdf/2306.02618v2
"Rhythm-controllable Attention with High Robustness for Long Sentence
  Speech Synthesis","Dengfeng Ke, Yayue Deng, Yukang Jia, Jinlong Xue, Qi Luo, Ya Li, Jianqing Sun, Jiaen Liang, Binghuai Lin","Regressive Text-to-Speech (TTS) system utilizes attention mechanism to
generate alignment between text and acoustic feature sequence. Alignment
determines synthesis robustness (e.g, the occurence of skipping, repeating, and
collapse) and rhythm via duration control. However, current attention
algorithms used in speech synthesis cannot control rhythm using external
duration information to generate natural speech while ensuring robustness. In
this study, we propose Rhythm-controllable Attention (RC-Attention) based on
Tracotron2, which improves robustness and naturalness simultaneously. Proposed
attention adopts a trainable scalar learned from four kinds of information to
achieve rhythm control, which makes rhythm control more robust and natural,
even when synthesized sentences are extremely longer than training corpus. We
use word errors counting and AB preference test to measure robustness of
proposed method and naturalness of synthesized speech, respectively. Results
shows that RC-Attention has the lowest word error rate of nearly 0.6%, compared
with 11.8% for baseline system. Moreover, nearly 60% subjects prefer to the
speech synthesized with RC-Attention to that with Forward Attention, because
the former has more natural rhythm.",2306.02593v1,https://arxiv.org/pdf/2306.02593v1
"Incorporating L2 Phonemes Using Articulatory Features for Robust Speech
  Recognition","Jisung Wang, Haram Lee, Myungwoo Oh","The limited availability of non-native speech datasets presents a major
challenge in automatic speech recognition (ASR) to narrow the performance gap
between native and non-native speakers. To address this, the focus of this
study is on the efficient incorporation of the L2 phonemes, which in this work
refer to Korean phonemes, through articulatory feature analysis. This not only
enables accurate modeling of pronunciation variants but also allows for the
utilization of both native Korean and English speech datasets. We employ the
lattice-free maximum mutual information (LF-MMI) objective in an end-to-end
manner, to train the acoustic model to align and predict one of multiple
pronunciation candidates. Experimental results show that the proposed method
improves ASR accuracy for Korean L2 speech by training solely on L1 speech
data. Furthermore, fine-tuning on L2 speech improves recognition accuracy for
both L1 and L2 speech without performance trade-offs.",2306.02534v1,https://arxiv.org/pdf/2306.02534v1
"Towards Robust Feature Learning with t-vFM Similarity for Continual
  Learning","Bilan Gao, YoungBin Kim","Continual learning has been developed using standard supervised contrastive
loss from the perspective of feature learning. Due to the data imbalance during
the training, there are still challenges in learning better representations. In
this work, we suggest using a different similarity metric instead of cosine
similarity in supervised contrastive loss in order to learn more robust
representations. We validate the our method on one of the image classification
datasets Seq-CIFAR-10 and the results outperform recent continual learning
baselines.",2306.02335v1,https://arxiv.org/pdf/2306.02335v1
"Benchmarking Robustness of Adaptation Methods on Pre-trained
  Vision-Language Models","Shuo Chen, Jindong Gu, Zhen Han, Yunpu Ma, Philip Torr, Volker Tresp","Various adaptation methods, such as LoRA, prompts, and adapters, have been
proposed to enhance the performance of pre-trained vision-language models in
specific domains. The robustness of these adaptation methods against
distribution shifts have not been studied. In this study, we assess the
robustness of 11 widely-used adaptation methods across 4 vision-language
datasets under multimodal corruptions. Concretely, we introduce 7 benchmark
datasets, including 96 visual and 87 textual corruptions, to investigate the
robustness of different adaptation methods, the impact of available adaptation
examples, and the influence of trainable parameter size during adaptation. Our
analysis reveals that: 1) Adaptation methods are more sensitive to text
corruptions than visual corruptions. 2) Full fine-tuning does not consistently
provide the highest robustness; instead, adapters can achieve better robustness
with comparable clean performance. 3) Contrary to expectations, our findings
indicate that increasing the number of adaptation data and parameters does not
guarantee enhanced robustness; instead it results in even lower robustness. We
hope this study could benefit future research in the development of robust
multimodal adaptation methods. The benchmark, code, and dataset used in this
study can be accessed at https://adarobustness.github.io .",2306.02080v3,https://arxiv.org/pdf/2306.02080v3
Can Directed Graph Neural Networks be Adversarially Robust?,"Zhichao Hou, Xitong Zhang, Wei Wang, Charu C. Aggarwal, Xiaorui Liu","The existing research on robust Graph Neural Networks (GNNs) fails to
acknowledge the significance of directed graphs in providing rich information
about networks' inherent structure. This work presents the first investigation
into the robustness of GNNs in the context of directed graphs, aiming to
harness the profound trust implications offered by directed graphs to bolster
the robustness and resilience of GNNs. Our study reveals that existing directed
GNNs are not adversarially robust. In pursuit of our goal, we introduce a new
and realistic directed graph attack setting and propose an innovative,
universal, and efficient message-passing framework as a plug-in layer to
significantly enhance the robustness of GNNs. Combined with existing defense
strategies, this framework achieves outstanding clean accuracy and
state-of-the-art robust performance, offering superior defense against both
transfer and adaptive attacks. The findings in this study reveal a novel and
promising direction for this crucial research area. The code will be made
publicly available upon the acceptance of this work.",2306.02002v1,https://arxiv.org/pdf/2306.02002v1
"Improving the generalizability and robustness of large-scale traffic
  signal control","Tianyu Shi, Francois-Xavier Devailly, Denis Larocque, Laurent Charlin","A number of deep reinforcement-learning (RL) approaches propose to control
traffic signals. In this work, we study the robustness of such methods along
two axes. First, sensor failures and GPS occlusions create missing-data
challenges and we show that recent methods remain brittle in the face of these
missing data. Second, we provide a more systematic study of the generalization
ability of RL methods to new networks with different traffic regimes. Again, we
identify the limitations of recent approaches. We then propose using a
combination of distributional and vanilla reinforcement learning through a
policy ensemble. Building upon the state-of-the-art previous model which uses a
decentralized approach for large-scale traffic signal control with graph
convolutional networks (GCNs), we first learn models using a distributional
reinforcement learning (DisRL) approach. In particular, we use implicit
quantile networks (IQN) to model the state-action return distribution with
quantile regression. For traffic signal control problems, an ensemble of
standard RL and DisRL yields superior performance across different scenarios,
including different levels of missing sensor data and traffic flow patterns.
Furthermore, the learning scheme of the resulting model can improve zero-shot
transferability to different road network structures, including both synthetic
networks and real-world networks (e.g., Luxembourg, Manhattan). We conduct
extensive experiments to compare our approach to multi-agent reinforcement
learning and traditional transportation approaches. Results show that the
proposed method improves robustness and generalizability in the face of missing
data, varying road networks, and traffic flows.",2306.01925v2,https://arxiv.org/pdf/2306.01925v2
Robust low-rank training via approximate orthonormal constraints,"Dayana Savostianova, Emanuele Zangrando, Gianluca Ceruti, Francesco Tudisco","With the growth of model and data sizes, a broad effort has been made to
design pruning techniques that reduce the resource demand of deep learning
pipelines, while retaining model performance. In order to reduce both inference
and training costs, a prominent line of work uses low-rank matrix
factorizations to represent the network weights. Although able to retain
accuracy, we observe that low-rank methods tend to compromise model robustness
against adversarial perturbations. By modeling robustness in terms of the
condition number of the neural network, we argue that this loss of robustness
is due to the exploding singular values of the low-rank weight matrices. Thus,
we introduce a robust low-rank training algorithm that maintains the network's
weights on the low-rank matrix manifold while simultaneously enforcing
approximate orthonormal constraints. The resulting model reduces both training
and inference costs while ensuring well-conditioning and thus better
adversarial robustness, without compromising model accuracy. This is shown by
extensive numerical evidence and by our main approximation theorem that shows
the computed robust low-rank network well-approximates the ideal full model,
provided a highly performing low-rank sub-network exists.",2306.01485v1,https://arxiv.org/pdf/2306.01485v1
Towards Robust FastSpeech 2 by Modelling Residual Multimodality,"Fabian Kögel, Bac Nguyen, Fabien Cardinaux","State-of-the-art non-autoregressive text-to-speech (TTS) models based on
FastSpeech 2 can efficiently synthesise high-fidelity and natural speech. For
expressive speech datasets however, we observe characteristic audio
distortions. We demonstrate that such artefacts are introduced to the vocoder
reconstruction by over-smooth mel-spectrogram predictions, which are induced by
the choice of mean-squared-error (MSE) loss for training the mel-spectrogram
decoder. With MSE loss FastSpeech 2 is limited to learn conditional averages of
the training distribution, which might not lie close to a natural sample if the
distribution still appears multimodal after all conditioning signals. To
alleviate this problem, we introduce TVC-GMM, a mixture model of
Trivariate-Chain Gaussian distributions, to model the residual multimodality.
TVC-GMM reduces spectrogram smoothness and improves perceptual audio quality in
particular for expressive datasets as shown by both objective and subjective
evaluation.",2306.01442v1,https://arxiv.org/pdf/2306.01442v1
"Improving Adversarial Robustness of DEQs with Explicit Regulations Along
  the Neural Dynamics","Zonghan Yang, Peng Li, Tianyu Pang, Yang Liu","Deep equilibrium (DEQ) models replace the multiple-layer stacking of
conventional deep networks with a fixed-point iteration of a single-layer
transformation. Having been demonstrated to be competitive in a variety of
real-world scenarios, the adversarial robustness of general DEQs becomes
increasingly crucial for their reliable deployment. Existing works improve the
robustness of general DEQ models with the widely-used adversarial training (AT)
framework, but they fail to exploit the structural uniquenesses of DEQ models.
To this end, we interpret DEQs through the lens of neural dynamics and find
that AT under-regulates intermediate states. Besides, the intermediate states
typically provide predictions with a high prediction entropy. Informed by the
correlation between the entropy of dynamical systems and their stability
properties, we propose reducing prediction entropy by progressively updating
inputs along the neural dynamics. During AT, we also utilize random
intermediate states to compute the loss function. Our methods regulate the
neural dynamics of DEQ models in this manner. Extensive experiments demonstrate
that our methods substantially increase the robustness of DEQ models and even
outperform the strong deep network baselines.",2306.01435v1,https://arxiv.org/pdf/2306.01435v1
A Closer Look at the Adversarial Robustness of Deep Equilibrium Models,"Zonghan Yang, Tianyu Pang, Yang Liu","Deep equilibrium models (DEQs) refrain from the traditional layer-stacking
paradigm and turn to find the fixed point of a single layer. DEQs have achieved
promising performance on different applications with featured memory
efficiency. At the same time, the adversarial vulnerability of DEQs raises
concerns. Several works propose to certify robustness for monotone DEQs.
However, limited efforts are devoted to studying empirical robustness for
general DEQs. To this end, we observe that an adversarially trained DEQ
requires more forward steps to arrive at the equilibrium state, or even
violates its fixed-point structure. Besides, the forward and backward tracks of
DEQs are misaligned due to the black-box solvers. These facts cause gradient
obfuscation when applying the ready-made attacks to evaluate or adversarially
train DEQs. Given this, we develop approaches to estimate the intermediate
gradients of DEQs and integrate them into the attacking pipelines. Our
approaches facilitate fully white-box evaluations and lead to effective
adversarial defense for DEQs. Extensive experiments on CIFAR-10 validate the
adversarial robustness of DEQs competitive with deep networks of similar sizes.",2306.01429v1,https://arxiv.org/pdf/2306.01429v1
"Robust and Generalisable Segmentation of Subtle Epilepsy-causing
  Lesions: a Graph Convolutional Approach","Hannah Spitzer, Mathilde Ripart, Abdulah Fawaz, Logan Z. J. Williams, MELD project, Emma Robinson, Juan Eugenio Iglesias, Sophie Adler, Konrad Wagstyl","Focal cortical dysplasia (FCD) is a leading cause of drug-resistant focal
epilepsy, which can be cured by surgery. These lesions are extremely subtle and
often missed even by expert neuroradiologists. ""Ground truth"" manual lesion
masks are therefore expensive, limited and have large inter-rater variability.
Existing FCD detection methods are limited by high numbers of false positive
predictions, primarily due to vertex- or patch-based approaches that lack
whole-brain context. Here, we propose to approach the problem as semantic
segmentation using graph convolutional networks (GCN), which allows our model
to learn spatial relationships between brain regions. To address the specific
challenges of FCD identification, our proposed model includes an auxiliary loss
to predict distance from the lesion to reduce false positives and a weak
supervision classification loss to facilitate learning from uncertain lesion
masks. On a multi-centre dataset of 1015 participants with surface-based
features and manual lesion masks from structural MRI data, the proposed GCN
achieved an AUC of 0.74, a significant improvement against a previously used
vertex-wise multi-layer perceptron (MLP) classifier (AUC 0.64). With
sensitivity thresholded at 67%, the GCN had a specificity of 71% in comparison
to 49% when using the MLP. This improvement in specificity is vital for
clinical integration of lesion-detection tools into the radiological workflow,
through increasing clinical confidence in the use of AI radiological adjuncts
and reducing the number of areas requiring expert review.",2306.01375v2,https://arxiv.org/pdf/2306.01375v2
"Towards Understanding Clean Generalization and Robust Overfitting in
  Adversarial Training","Binghui Li, Yuanzhi Li","Similar to surprising performance in the standard deep learning, deep nets
trained by adversarial training also generalize well for $\textit{unseen clean
data (natural data)}$. However, despite adversarial training can achieve low
robust training error, there exists a significant $\textit{robust
generalization gap}$. We call this phenomenon the $\textit{Clean Generalization
and Robust Overfitting (CGRO)}$. In this work, we study the CGRO phenomenon in
adversarial training from two views: $\textit{representation complexity}$ and
$\textit{training dynamics}$. Specifically, we consider a binary classification
setting with $N$ separated training data points. $\textit{First}$, we prove
that, based on the assumption that we assume there is
$\operatorname{poly}(D)$-size clean classifier (where $D$ is the data
dimension), ReLU net with only $O(N D)$ extra parameters is able to leverages
robust memorization to achieve the CGRO, while robust classifier still requires
exponential representation complexity in worst case. $\textit{Next}$, we focus
on a structured-data case to analyze training dynamics, where we train a
two-layer convolutional network with $O(N D)$ width against adversarial
perturbation. We then show that a three-stage phase transition occurs during
learning process and the network provably converges to robust memorization
regime, which thereby results in the CGRO. $\textit{Besides}$, we also
empirically verify our theoretical analysis by experiments in real-image
recognition datasets.",2306.01271v2,https://arxiv.org/pdf/2306.01271v2
On the Robustness of Arabic Speech Dialect Identification,"Peter Sullivan, AbdelRahim Elmadany, Muhammad Abdul-Mageed","Arabic dialect identification (ADI) tools are an important part of the
large-scale data collection pipelines necessary for training speech recognition
models. As these pipelines require application of ADI tools to potentially
out-of-domain data, we aim to investigate how vulnerable the tools may be to
this domain shift. With self-supervised learning (SSL) models as a starting
point, we evaluate transfer learning and direct classification from SSL
features. We undertake our evaluation under rich conditions, with a goal to
develop ADI systems from pretrained models and ultimately evaluate performance
on newly collected data. In order to understand what factors contribute to
model decisions, we carry out a careful human study of a subset of our data.
Our analysis confirms that domain shift is a major challenge for ADI models. We
also find that while self-training does alleviate this challenges, it may be
insufficient for realistic conditions.",2306.03789v1,https://arxiv.org/pdf/2306.03789v1
Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes,"Revathy Venkataramanan, Kaushik Roy, Kanak Raj, Renjith Prasad, Yuxin Zi, Vignesh Narayanan, Amit Sheth","As people become more aware of their food choices, food computation models
have become increasingly popular in assisting people in maintaining healthy
eating habits. For example, food recommendation systems analyze recipe
instructions to assess nutritional contents and provide recipe recommendations.
The recent and remarkable successes of generative AI methods, such as
auto-regressive large language models, can lead to robust methods for a more
comprehensive understanding of recipes for healthy food recommendations beyond
surface-level nutrition content assessments. In this study, we explore the use
of generative AI methods to extend current food computation models, primarily
involving the analysis of nutrition and ingredients, to also incorporate
cooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).
Cooking actions are notoriously hard to model using statistical learning
methods due to irregular data patterns - significantly varying natural language
descriptions for the same action (e.g., marinate the meat vs. marinate the meat
and leave overnight) and infrequently occurring patterns (e.g., add salt occurs
far more frequently than marinating the meat). The prototypical approach to
handling irregular data patterns is to increase the volume of data that the
model ingests by orders of magnitude. Unfortunately, in the cooking domain,
these problems are further compounded with larger data volumes presenting a
unique challenge that is not easily handled by simply scaling up. In this work,
we propose novel aggregation-based generative AI methods, Cook-Gen, that
reliably generate cooking actions from recipes, despite difficulties with
irregular data patterns, while also outperforming Large Language Models and
other strong baselines.",2306.01805v1,https://arxiv.org/pdf/2306.01805v1
"Adversarial Robustness in Unsupervised Machine Learning: A Systematic
  Review","Mathias Lundteigen Mohus, Jinyue Li","As the adoption of machine learning models increases, ensuring robust models
against adversarial attacks is increasingly important. With unsupervised
machine learning gaining more attention, ensuring it is robust against attacks
is vital. This paper conducts a systematic literature review on the robustness
of unsupervised learning, collecting 86 papers. Our results show that most
research focuses on privacy attacks, which have effective defenses; however,
many attacks lack effective and general defensive measures. Based on the
results, we formulate a model on the properties of an attack on unsupervised
learning, contributing to future research by providing a model to use.",2306.00687v1,https://arxiv.org/pdf/2306.00687v1
Byzantine-Robust Clustered Federated Learning,"Zhixu Tao, Kun Yang, Sanjeev R. Kulkarni","This paper focuses on the problem of adversarial attacks from Byzantine
machines in a Federated Learning setting where non-Byzantine machines can be
partitioned into disjoint clusters. In this setting, non-Byzantine machines in
the same cluster have the same underlying data distribution, and different
clusters of non-Byzantine machines have different learning tasks. Byzantine
machines can adversarially attack any cluster and disturb the training process
on clusters they attack. In the presence of Byzantine machines, the goal of our
work is to identify cluster membership of non-Byzantine machines and optimize
the models learned by each cluster. We adopt the Iterative Federated Clustering
Algorithm (IFCA) framework of Ghosh et al. (2020) to alternatively estimate
cluster membership and optimize models. In order to make this framework robust
against adversarial attacks from Byzantine machines, we use coordinate-wise
trimmed mean and coordinate-wise median aggregation methods used by Yin et al.
(2018). Specifically, we propose a new Byzantine-Robust Iterative Federated
Clustering Algorithm to improve on the results in Ghosh et al. (2019). We prove
a convergence rate for this algorithm for strongly convex loss functions. We
compare our convergence rate with the convergence rate of an existing
algorithm, and we demonstrate the performance of our algorithm on simulated
data.",2306.00638v1,https://arxiv.org/pdf/2306.00638v1
Faster Robust Tensor Power Method for Arbitrary Order,"Yichuan Deng, Zhao Song, Junze Yin","Tensor decomposition is a fundamental method used in various areas to deal
with high-dimensional data. \emph{Tensor power method} (TPM) is one of the
widely-used techniques in the decomposition of tensors. This paper presents a
novel tensor power method for decomposing arbitrary order tensors, which
overcomes limitations of existing approaches that are often restricted to
lower-order (less than $3$) tensors or require strong assumptions about the
underlying data structure. We apply sketching method, and we are able to
achieve the running time of $\widetilde{O}(n^{p-1})$, on the power $p$ and
dimension $n$ tensor. We provide a detailed analysis for any $p$-th order
tensor, which is never given in previous works.",2306.00406v1,https://arxiv.org/pdf/2306.00406v1
"Efficient and Robust Bayesian Selection of Hyperparameters in Dimension
  Reduction for Visualization","Yin-Ting Liao, Hengrui Luo, Anna Ma","We introduce an efficient and robust auto-tuning framework for hyperparameter
selection in dimension reduction (DR) algorithms, focusing on large-scale
datasets and arbitrary performance metrics. By leveraging Bayesian optimization
(BO) with a surrogate model, our approach enables efficient hyperparameter
selection with multi-objective trade-offs and allows us to perform data-driven
sensitivity analysis. By incorporating normalization and subsampling, the
proposed framework demonstrates versatility and efficiency, as shown in
applications to visualization techniques such as t-SNE and UMAP. We evaluate
our results on various synthetic and real-world datasets using multiple quality
metrics, providing a robust and efficient solution for hyperparameter selection
in DR algorithms.",2306.00357v1,https://arxiv.org/pdf/2306.00357v1
Doubly Robust Self-Training,"Banghua Zhu, Mingyu Ding, Philip Jacobson, Ming Wu, Wei Zhan, Michael Jordan, Jiantao Jiao","Self-training is an important technique for solving semi-supervised learning
problems. It leverages unlabeled data by generating pseudo-labels and combining
them with a limited labeled dataset for training. The effectiveness of
self-training heavily relies on the accuracy of these pseudo-labels. In this
paper, we introduce doubly robust self-training, a novel semi-supervised
algorithm that provably balances between two extremes. When the pseudo-labels
are entirely incorrect, our method reduces to a training process solely using
labeled data. Conversely, when the pseudo-labels are completely accurate, our
method transforms into a training process utilizing all pseudo-labeled data and
labeled data, thus increasing the effective sample size. Through empirical
evaluations on both the ImageNet dataset for image classification and the
nuScenes autonomous driving dataset for 3D object detection, we demonstrate the
superiority of the doubly robust loss over the standard self-training baseline.",2306.00265v3,https://arxiv.org/pdf/2306.00265v3
"Learning for Edge-Weighted Online Bipartite Matching with Robustness
  Guarantees","Pengfei Li, Jianyi Yang, Shaolei Ren","Many problems, such as online ad display, can be formulated as online
bipartite matching. The crucial challenge lies in the nature of
sequentially-revealed online item information, based on which we make
irreversible matching decisions at each step. While numerous expert online
algorithms have been proposed with bounded worst-case competitive ratios, they
may not offer satisfactory performance in average cases. On the other hand,
reinforcement learning (RL) has been applied to improve the average
performance, but it lacks robustness and can perform arbitrarily poorly. In
this paper, we propose a novel RL-based approach to edge-weighted online
bipartite matching with robustness guarantees (LOMAR), achieving both good
average-case and worst-case performance. The key novelty of LOMAR is a new
online switching operation which, based on a judicious condition to hedge
against future uncertainties, decides whether to follow the expert's decision
or the RL decision for each online item. We prove that for any $\rho\in[0,1]$,
LOMAR is $\rho$-competitive against any given expert online algorithm. To
improve the average performance, we train the RL policy by explicitly
considering the online switching operation. Finally, we run empirical
experiments to demonstrate the advantages of LOMAR compared to existing
baselines. Our code is available at: https://github.com/Ren-Research/LOMAR",2306.00172v1,https://arxiv.org/pdf/2306.00172v1
"Tree-Ring Watermarks: Fingerprints for Diffusion Images that are
  Invisible and Robust","Yuxin Wen, John Kirchenbauer, Jonas Geiping, Tom Goldstein","Watermarking the outputs of generative models is a crucial technique for
tracing copyright and preventing potential harm from AI-generated content. In
this paper, we introduce a novel technique called Tree-Ring Watermarking that
robustly fingerprints diffusion model outputs. Unlike existing methods that
perform post-hoc modifications to images after sampling, Tree-Ring Watermarking
subtly influences the entire sampling process, resulting in a model fingerprint
that is invisible to humans. The watermark embeds a pattern into the initial
noise vector used for sampling. These patterns are structured in Fourier space
so that they are invariant to convolutions, crops, dilations, flips, and
rotations. After image generation, the watermark signal is detected by
inverting the diffusion process to retrieve the noise vector, which is then
checked for the embedded signal. We demonstrate that this technique can be
easily applied to arbitrary diffusion models, including text-conditioned Stable
Diffusion, as a plug-in with negligible loss in FID. Our watermark is
semantically hidden in the image space and is far more robust than watermarking
alternatives that are currently deployed. Code is available at
https://github.com/YuxinWenRick/tree-ring-watermark.",2305.20030v3,https://arxiv.org/pdf/2305.20030v3
Investigation of the Robustness of Neural Density Fields,"Jonas Schuhmacher, Fabio Gratl, Dario Izzo, Pablo Gómez","Recent advances in modeling density distributions, so-called neural density
fields, can accurately describe the density distribution of celestial bodies
without, e.g., requiring a shape model - properties of great advantage when
designing trajectories close to these bodies. Previous work introduced this
approach, but several open questions remained. This work investigates neural
density fields and their relative errors in the context of robustness to
external factors like noise or constraints during training, like the maximal
available gravity signal strength due to a certain distance exemplified for 433
Eros and 67P/Churyumov-Gerasimenko. It is found that both models trained on a
polyhedral and mascon ground truth perform similarly, indicating that the
ground truth is not the accuracy bottleneck. The impact of solar radiation
pressure on a typical probe affects training neglectable, with the relative
error being of the same magnitude as without noise. However, limiting the
precision of measurement data by applying Gaussian noise hurts the obtainable
precision. Further, pretraining is shown as practical in order to speed up
network training. Hence, this work demonstrates that training neural networks
for the gravity inversion problem is appropriate as long as the gravity signal
is distinguishable from noise.
  Code and results are available at https://github.com/gomezzz/geodesyNets",2305.19698v1,https://arxiv.org/pdf/2305.19698v1
"Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability
  in Anomaly Detection through Automatic Diffusion Models","Cosmin I. Bercea, Michael Neumayr, Daniel Rueckert, Julia A. Schnabel","The introduction of diffusion models in anomaly detection has paved the way
for more effective and accurate image reconstruction in pathologies. However,
the current limitations in controlling noise granularity hinder diffusion
models' ability to generalize across diverse anomaly types and compromise the
restoration of healthy tissues. To overcome these challenges, we propose
AutoDDPM, a novel approach that enhances the robustness of diffusion models.
AutoDDPM utilizes diffusion models to generate initial likelihood maps of
potential anomalies and seamlessly integrates them with the original image.
Through joint noised distribution re-sampling, AutoDDPM achieves harmonization
and in-painting effects. Our study demonstrates the efficacy of AutoDDPM in
replacing anomalous regions while preserving healthy tissues, considerably
surpassing diffusion models' limitations. It also contributes valuable insights
and analysis on the limitations of current diffusion models, promoting robust
and interpretable anomaly detection in medical imaging - an essential aspect of
building autonomous clinical decision systems with higher interpretability.",2305.19643v1,https://arxiv.org/pdf/2305.19643v1
"Spotlight Attention: Robust Object-Centric Learning With a Spatial
  Locality Prior","Ayush Chakravarthy, Trang Nguyen, Anirudh Goyal, Yoshua Bengio, Michael C. Mozer","The aim of object-centric vision is to construct an explicit representation
of the objects in a scene. This representation is obtained via a set of
interchangeable modules called \emph{slots} or \emph{object files} that compete
for local patches of an image. The competition has a weak inductive bias to
preserve spatial continuity; consequently, one slot may claim patches scattered
diffusely throughout the image. In contrast, the inductive bias of human vision
is strong, to the degree that attention has classically been described with a
spotlight metaphor. We incorporate a spatial-locality prior into
state-of-the-art object-centric vision models and obtain significant
improvements in segmenting objects in both synthetic and real-world datasets.
Similar to human visual attention, the combination of image content and spatial
constraints yield robust unsupervised object-centric learning, including less
sensitivity to model hyperparameters.",2305.19550v1,https://arxiv.org/pdf/2305.19550v1
Contextual Vision Transformers for Robust Representation Learning,"Yujia Bao, Theofanis Karaletsos","We introduce Contextual Vision Transformers (ContextViT), a method designed
to generate robust image representations for datasets experiencing shifts in
latent factors across various groups. Derived from the concept of in-context
learning, ContextViT incorporates an additional context token to encapsulate
group-specific information. This integration allows the model to adjust the
image representation in accordance with the group-specific context.
Specifically, for a given input image, ContextViT maps images with identical
group membership into this context token, which is appended to the input image
tokens. Additionally, we introduce a context inference network to predict such
tokens on-the-fly, given a batch of samples from the group. This enables
ContextViT to adapt to new testing distributions during inference time. We
demonstrate the efficacy of ContextViT across a wide range of applications. In
supervised fine-tuning, we show that augmenting pre-trained ViTs with our
proposed context conditioning mechanism results in consistent improvements in
out-of-distribution generalization on iWildCam and FMoW. We also investigate
self-supervised representation learning with ContextViT. Our experiments on the
Camelyon17 pathology imaging benchmark and the JUMP-CP microscopy imaging
benchmark demonstrate that ContextViT excels in learning stable image
featurizations amidst distribution shift, consistently outperforming its ViT
counterpart.",2305.19402v2,https://arxiv.org/pdf/2305.19402v2
"FERN: Leveraging Graph Attention Networks for Failure Evaluation and
  Robust Network Design","Chenyi Liu, Vaneet Aggarwal, Tian Lan, Nan Geng, Yuan Yang, Mingwei Xu, Qing Li","Robust network design, which aims to guarantee network availability under
various failure scenarios while optimizing performance/cost objectives, has
received significant attention. Existing approaches often rely on model-based
mixed-integer optimization that is hard to scale or employ deep learning to
solve specific engineering problems yet with limited generalizability. In this
paper, we show that failure evaluation provides a common kernel to improve the
tractability and scalability of existing solutions. By providing a neural
network function approximation of this common kernel using graph attention
networks, we develop a unified learning-based framework, FERN, for scalable
Failure Evaluation and Robust Network design. FERN represents rich problem
inputs as a graph and captures both local and global views by attentively
performing feature extraction from the graph. It enables a broad range of
robust network design problems, including robust network validation, network
upgrade optimization, and fault-tolerant traffic engineering that are discussed
in this paper, to be recasted with respect to the common kernel and thus
computed efficiently using neural networks and over a small set of critical
failure scenarios. Extensive experiments on real-world network topologies show
that FERN can efficiently and accurately identify key failure scenarios for
both OSPF and optimal routing scheme, and generalizes well to different
topologies and input traffic patterns. It can speed up multiple robust network
design problems by more than 80x, 200x, 10x, respectively with negligible
performance gap.",2305.19153v1,https://arxiv.org/pdf/2305.19153v1
"Which Models have Perceptually-Aligned Gradients? An Explanation via
  Off-Manifold Robustness","Suraj Srinivas, Sebastian Bordt, Hima Lakkaraju","One of the remarkable properties of robust computer vision models is that
their input-gradients are often aligned with human perception, referred to in
the literature as perceptually-aligned gradients (PAGs). Despite only being
trained for classification, PAGs cause robust models to have rudimentary
generative capabilities, including image generation, denoising, and
in-painting. However, the underlying mechanisms behind these phenomena remain
unknown. In this work, we provide a first explanation of PAGs via
\emph{off-manifold robustness}, which states that models must be more robust
off- the data manifold than they are on-manifold. We first demonstrate
theoretically that off-manifold robustness leads input gradients to lie
approximately on the data manifold, explaining their perceptual alignment. We
then show that Bayes optimal models satisfy off-manifold robustness, and
confirm the same empirically for robust models trained via gradient norm
regularization, randomized smoothing, and adversarial training with projected
gradient descent. Quantifying the perceptual alignment of model gradients via
their similarity with the gradients of generative models, we show that
off-manifold robustness correlates well with perceptual alignment. Finally,
based on the levels of on- and off-manifold robustness, we identify three
different regimes of robustness that affect both perceptual alignment and model
accuracy: weak robustness, bayes-aligned robustness, and excessive robustness.
Code is available at \url{https://github.com/tml-tuebingen/pags}.",2305.19101v2,https://arxiv.org/pdf/2305.19101v2
Solving Robust MDPs through No-Regret Dynamics,Etash Kumar Guha,"Reinforcement Learning is a powerful framework for training agents to
navigate different situations, but it is susceptible to changes in
environmental dynamics. However, solving Markov Decision Processes that are
robust to changes is difficult due to nonconvexity and size of action or state
spaces. While most works have analyzed this problem by taking different
assumptions on the problem, a general and efficient theoretical analysis is
still missing. However, we generate a simple framework for improving robustness
by solving a minimax iterative optimization problem where a policy player and
an environmental dynamics player are playing against each other. Leveraging
recent results in online nonconvex learning and techniques from improving
policy gradient methods, we yield an algorithm that maximizes the robustness of
the Value Function on the order of
$\mathcal{O}\left(\frac{1}{T^{\frac{1}{2}}}\right)$ where $T$ is the number of
iterations of the algorithm.",2305.19035v2,https://arxiv.org/pdf/2305.19035v2
"Policy Gradient Algorithms for Robust MDPs with Non-Rectangular
  Uncertainty Sets","Mengmeng Li, Daniel Kuhn, Tobias Sutter","We propose policy gradient algorithms for robust infinite-horizon Markov
decision processes (MDPs) with non-rectangular uncertainty sets, thereby
addressing an open challenge in the robust MDP literature. Indeed, uncertainty
sets that display statistical optimality properties and make optimal use of
limited data often fail to be rectangular. Unfortunately, the corresponding
robust MDPs cannot be solved with dynamic programming techniques and are in
fact provably intractable. We first present a randomized projected Langevin
dynamics algorithm that solves the robust policy evaluation problem to global
optimality but is inefficient. We also propose a deterministic policy gradient
method that is efficient but solves the robust policy evaluation problem only
approximately, and we prove that the approximation error scales with a new
measure of non-rectangularity of the uncertainty set. Finally, we describe an
actor-critic algorithm that finds an $\epsilon$-optimal solution for the robust
policy improvement problem in $\mathcal{O}(1/\epsilon^4)$ iterations. We thus
present the first complete solution scheme for robust MDPs with non-rectangular
uncertainty sets offering global optimality guarantees. Numerical experiments
show that our algorithms compare favorably against state-of-the-art methods.",2305.19004v3,https://arxiv.org/pdf/2305.19004v3
"Asymptotic Characterisation of Robust Empirical Risk Minimisation
  Performance in the Presence of Outliers","Matteo Vilucchio, Emanuele Troiani, Vittorio Erba, Florent Krzakala","We study robust linear regression in high-dimension, when both the dimension
$d$ and the number of data points $n$ diverge with a fixed ratio $\alpha=n/d$,
and study a data model that includes outliers. We provide exact asymptotics for
the performances of the empirical risk minimisation (ERM) using
$\ell_2$-regularised $\ell_2$, $\ell_1$, and Huber losses, which are the
standard approach to such problems. We focus on two metrics for the
performance: the generalisation error to similar datasets with outliers, and
the estimation error of the original, unpolluted function. Our results are
compared with the information theoretic Bayes-optimal estimation bound. For the
generalization error, we find that optimally-regularised ERM is asymptotically
consistent in the large sample complexity limit if one perform a simple
calibration, and compute the rates of convergence. For the estimation error
however, we show that due to a norm calibration mismatch, the consistency of
the estimator requires an oracle estimate of the optimal norm, or the presence
of a cross-validation set not corrupted by the outliers. We examine in detail
how performance depends on the loss function and on the degree of outlier
corruption in the training set and identify a region of parameters where the
optimal performance of the Huber loss is identical to that of the $\ell_2$
loss, offering insights into the use cases of different loss functions.",2305.18974v2,https://arxiv.org/pdf/2305.18974v2
"Robust Reinforcement Learning Objectives for Sequential Recommender
  Systems","Melissa Mozifian, Tristan Sylvain, Dave Evans, Lili Meng","Attention-based sequential recommendation methods have shown promise in
accurately capturing users' evolving interests from their past interactions.
Recent research has also explored the integration of reinforcement learning
(RL) into these models, in addition to generating superior user
representations. By framing sequential recommendation as an RL problem with
reward signals, we can develop recommender systems that incorporate direct user
feedback in the form of rewards, enhancing personalization for users.
Nonetheless, employing RL algorithms presents challenges, including off-policy
training, expansive combinatorial action spaces, and the scarcity of datasets
with sufficient reward signals. Contemporary approaches have attempted to
combine RL and sequential modeling, incorporating contrastive-based objectives
and negative sampling strategies for training the RL component. In this work,
we further emphasize the efficacy of contrastive-based objectives paired with
augmentation to address datasets with extended horizons. Additionally, we
recognize the potential instability issues that may arise during the
application of negative sampling. These challenges primarily stem from the data
imbalance prevalent in real-world datasets, which is a common issue in offline
RL contexts. Furthermore, we introduce an enhanced methodology aimed at
providing a more effective solution to these challenges. Experimental results
across several real datasets show our method with increased robustness and
state-of-the-art performance.",2305.18820v2,https://arxiv.org/pdf/2305.18820v2
"It begins with a boundary: A geometric view on probabilistically robust
  learning","Leon Bungert, Nicolás García Trillos, Matt Jacobs, Daniel McKenzie, Đorđe Nikolić, Qingsong Wang","Although deep neural networks have achieved super-human performance on many
classification tasks, they often exhibit a worrying lack of robustness towards
adversarially generated examples. Thus, considerable effort has been invested
into reformulating Empirical Risk Minimization (ERM) into an adversarially
robust framework. Recently, attention has shifted towards approaches which
interpolate between the robustness offered by adversarial training and the
higher clean accuracy and faster training times of ERM. In this paper, we take
a fresh and geometric view on one such method -- Probabilistically Robust
Learning (PRL) (Robey et al., ICML, 2022). We propose a geometric framework for
understanding PRL, which allows us to identify a subtle flaw in its original
formulation and to introduce a family of probabilistic nonlocal perimeter
functionals to address this. We prove existence of solutions using novel
relaxation methods and study properties as well as local limits of the
introduced perimeters.",2305.18779v1,https://arxiv.org/pdf/2305.18779v1
Robust Lipschitz Bandits to Adversarial Corruptions,"Yue Kang, Cho-Jui Hsieh, Thomas C. M. Lee","Lipschitz bandit is a variant of stochastic bandits that deals with a
continuous arm set defined on a metric space, where the reward function is
subject to a Lipschitz constraint. In this paper, we introduce a new problem of
Lipschitz bandits in the presence of adversarial corruptions where an adaptive
adversary corrupts the stochastic rewards up to a total budget $C$. The budget
is measured by the sum of corruption levels across the time horizon $T$. We
consider both weak and strong adversaries, where the weak adversary is unaware
of the current action before the attack, while the strong one can observe it.
Our work presents the first line of robust Lipschitz bandit algorithms that can
achieve sub-linear regret under both types of adversary, even when the total
budget of corruption $C$ is unrevealed to the agent. We provide a lower bound
under each type of adversary, and show that our algorithm is optimal under the
strong case. Finally, we conduct experiments to illustrate the effectiveness of
our algorithms against two classic kinds of attacks.",2305.18543v2,https://arxiv.org/pdf/2305.18543v2
Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning,"Yu Wang, Pengchong Qiao, Chang Liu, Guoli Song, Xiawu Zheng, Jie Chen","Recent advances in robust semi-supervised learning (SSL) typically filter
out-of-distribution (OOD) information at the sample level. We argue that an
overlooked problem of robust SSL is its corrupted information on semantic
level, practically limiting the development of the field. In this paper, we
take an initial step to explore and propose a unified framework termed OOD
Semantic Pruning (OSP), which aims at pruning OOD semantics out from
in-distribution (ID) features. Specifically, (i) we propose an aliasing OOD
matching module to pair each ID sample with an OOD sample with semantic
overlap. (ii) We design a soft orthogonality regularization, which first
transforms each ID feature by suppressing its semantic component that is
collinear with paired OOD sample. It then forces the predictions before and
after soft orthogonality decomposition to be consistent. Being practically
simple, our method shows a strong performance in OOD detection and ID
classification on challenging benchmarks. In particular, OSP surpasses the
previous state-of-the-art by 13.7% on accuracy for ID classification and 5.9%
on AUROC for OOD detection on TinyImageNet dataset. The source codes are
publicly available at https://github.com/rain305f/OSP.",2305.18158v2,https://arxiv.org/pdf/2305.18158v2
"From Adversarial Arms Race to Model-centric Evaluation: Motivating a
  Unified Automatic Robustness Evaluation Framework","Yangyi Chen, Hongcheng Gao, Ganqu Cui, Lifan Yuan, Dehan Kong, Hanlu Wu, Ning Shi, Bo Yuan, Longtao Huang, Hui Xue, Zhiyuan Liu, Maosong Sun, Heng Ji","Textual adversarial attacks can discover models' weaknesses by adding
semantic-preserved but misleading perturbations to the inputs. The long-lasting
adversarial attack-and-defense arms race in Natural Language Processing (NLP)
is algorithm-centric, providing valuable techniques for automatic robustness
evaluation. However, the existing practice of robustness evaluation may exhibit
issues of incomprehensive evaluation, impractical evaluation protocol, and
invalid adversarial samples. In this paper, we aim to set up a unified
automatic robustness evaluation framework, shifting towards model-centric
evaluation to further exploit the advantages of adversarial attacks. To address
the above challenges, we first determine robustness evaluation dimensions based
on model capabilities and specify the reasonable algorithm to generate
adversarial samples for each dimension. Then we establish the evaluation
protocol, including evaluation settings and metrics, under realistic demands.
Finally, we use the perturbation degree of adversarial samples to control the
sample validity. We implement a toolkit RobTest that realizes our automatic
robustness evaluation framework. In our experiments, we conduct a robustness
evaluation of RoBERTa models to demonstrate the effectiveness of our evaluation
framework, and further show the rationality of each component in the framework.
The code will be made public at \url{https://github.com/thunlp/RobTest}.",2305.18503v1,https://arxiv.org/pdf/2305.18503v1
"Hardware-aware Training Techniques for Improving Robustness of Ex-Situ
  Neural Network Transfer onto Passive TiO2 ReRAM Crossbars","Philippe Drolet, Raphaël Dawant, Victor Yon, Pierre-Antoine Mouny, Matthieu Valdenaire, Javier Arias Zapata, Pierre Gliech, Sean U. N. Wood, Serge Ecoffey, Fabien Alibart, Yann Beilliard, Dominique Drouin","Passive resistive random access memory (ReRAM) crossbar arrays, a promising
emerging technology used for analog matrix-vector multiplications, are far
superior to their active (1T1R) counterparts in terms of the integration
density. However, current transfers of neural network weights into the
conductance state of the memory devices in the crossbar architecture are
accompanied by significant losses in precision due to hardware variabilities
such as sneak path currents, biasing scheme effects and conductance tuning
imprecision. In this work, training approaches that adapt techniques such as
dropout, the reparametrization trick and regularization to TiO2 crossbar
variabilities are proposed in order to generate models that are better adapted
to their hardware transfers. The viability of this approach is demonstrated by
comparing the outputs and precision of the proposed hardware-aware network with
those of a regular fully connected network over a few thousand weight transfers
using the half moons dataset in a simulation based on experimental data. For
the neural network trained using the proposed hardware-aware method, 79.5% of
the test set's data points can be classified with an accuracy of 95% or higher,
while only 18.5% of the test set's data points can be classified with this
accuracy by the regularly trained neural network.",2305.18495v1,https://arxiv.org/pdf/2305.18495v1
Shift-Robust Molecular Relational Learning with Causal Substructure,"Namkyeong Lee, Kanghoon Yoon, Gyoung S. Na, Sein Kim, Chanyoung Park","Recently, molecular relational learning, whose goal is to predict the
interaction behavior between molecular pairs, got a surge of interest in
molecular sciences due to its wide range of applications. In this work, we
propose CMRL that is robust to the distributional shift in molecular relational
learning by detecting the core substructure that is causally related to
chemical reactions. To do so, we first assume a causal relationship based on
the domain knowledge of molecular sciences and construct a structural causal
model (SCM) that reveals the relationship between variables. Based on the SCM,
we introduce a novel conditional intervention framework whose intervention is
conditioned on the paired molecule. With the conditional intervention
framework, our model successfully learns from the causal substructure and
alleviates the confounding effect of shortcut substructures that are spuriously
correlated to chemical reactions. Extensive experiments on various tasks with
real-world and synthetic datasets demonstrate the superiority of CMRL over
state-of-the-art baseline models. Our code is available at
https://github.com/Namkyeong/CMRL.",2305.18451v3,https://arxiv.org/pdf/2305.18451v3
Sample Complexity of Variance-reduced Distributionally Robust Q-learning,"Shengbo Wang, Nian Si, Jose Blanchet, Zhengyuan Zhou","Dynamic decision making under distributional shifts is of fundamental
interest in theory and applications of reinforcement learning: The distribution
of the environment on which the data is collected can differ from that of the
environment on which the model is deployed. This paper presents two novel
model-free algorithms, namely the distributionally robust Q-learning and its
variance-reduced counterpart, that can effectively learn a robust policy
despite distributional shifts. These algorithms are designed to efficiently
approximate the $q$-function of an infinite-horizon $\gamma$-discounted robust
Markov decision process with Kullback-Leibler uncertainty set to an entry-wise
$\epsilon$-degree of precision. Further, the variance-reduced distributionally
robust Q-learning combines the synchronous Q-learning with variance-reduction
techniques to enhance its performance. Consequently, we establish that it
attains a minmax sample complexity upper bound of $\tilde
O(|S||A|(1-\gamma)^{-4}\epsilon^{-2})$, where $S$ and $A$ denote the state and
action spaces. This is the first complexity result that is independent of the
uncertainty size $\delta$, thereby providing new complexity theoretic insights.
Additionally, a series of numerical experiments confirm the theoretical
findings and the efficiency of the algorithms in handling distributional
shifts.",2305.18420v1,https://arxiv.org/pdf/2305.18420v1
"BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise
  Learning","Jingfeng Zhang, Bo Song, Haohan Wang, Bo Han, Tongliang Liu, Lei Liu, Masashi Sugiyama","Label-noise learning (LNL) aims to increase the model's generalization given
training data with noisy labels. To facilitate practical LNL algorithms,
researchers have proposed different label noise types, ranging from
class-conditional to instance-dependent noises. In this paper, we introduce a
novel label noise type called BadLabel, which can significantly degrade the
performance of existing LNL algorithms by a large margin. BadLabel is crafted
based on the label-flipping attack against standard classification, where
specific samples are selected and their labels are flipped to other labels so
that the loss values of clean and noisy labels become indistinguishable. To
address the challenge posed by BadLabel, we further propose a robust LNL method
that perturbs the labels in an adversarial manner at each epoch to make the
loss values of clean and noisy labels again distinguishable. Once we select a
small set of (mostly) clean labeled data, we can apply the techniques of
semi-supervised learning to train the model accurately. Empirically, our
experimental results demonstrate that existing LNL algorithms are vulnerable to
the newly introduced BadLabel noise type, while our proposed robust LNL method
can effectively improve the generalization performance of the model under
various types of label noise. The new dataset of noisy labels and the source
codes of robust LNL algorithms are available at
https://github.com/zjfheart/BadLabels.",2305.18377v2,https://arxiv.org/pdf/2305.18377v2
Robust Natural Language Understanding with Residual Attention Debiasing,"Fei Wang, James Y. Huang, Tianyi Yan, Wenxuan Zhou, Muhao Chen","Natural language understanding (NLU) models often suffer from unintended
dataset biases. Among bias mitigation methods, ensemble-based debiasing
methods, especially product-of-experts (PoE), have stood out for their
impressive empirical success. However, previous ensemble-based debiasing
methods typically apply debiasing on top-level logits without directly
addressing biased attention patterns. Attention serves as the main media of
feature interaction and aggregation in PLMs and plays a crucial role in
providing robust prediction. In this paper, we propose REsidual Attention
Debiasing (READ), an end-to-end debiasing method that mitigates unintended
biases from attention. Experiments on three NLU tasks show that READ
significantly improves the performance of BERT-based models on OOD data with
shortcuts removed, including +12.9% accuracy on HANS, +11.0% accuracy on
FEVER-Symmetric, and +2.7% F1 on PAWS. Detailed analyses demonstrate the
crucial role of unbiased attention in robust NLU models and that READ
effectively mitigates biases in attention. Code is available at
https://github.com/luka-group/READ.",2305.17627v1,https://arxiv.org/pdf/2305.17627v1
"Faster Margin Maximization Rates for Generic and Adversarially Robust
  Optimization Methods","Guanghui Wang, Zihao Hu, Claudio Gentile, Vidya Muthukumar, Jacob Abernethy","First-order optimization methods tend to inherently favor certain solutions
over others when minimizing an underdetermined training objective that has
multiple global optima. This phenomenon, known as implicit bias, plays a
critical role in understanding the generalization capabilities of optimization
algorithms. Recent research has revealed that in separable binary
classification tasks gradient-descent-based methods exhibit an implicit bias
for the $\ell_2$-maximal margin classifier. Similarly, generic optimization
methods, such as mirror descent and steepest descent, have been shown to
converge to maximal margin classifiers defined by alternative geometries. While
gradient-descent-based algorithms provably achieve fast implicit bias rates,
corresponding rates in the literature for generic optimization methods are
relatively slow. To address this limitation, we present a series of
state-of-the-art implicit bias rates for mirror descent and steepest descent
algorithms. Our primary technique involves transforming a generic optimization
algorithm into an online optimization dynamic that solves a regularized
bilinear game, providing a unified framework for analyzing the implicit bias of
various optimization methods. Our accelerated rates are derived by leveraging
the regret bounds of online learning algorithms within this game framework. We
then show the flexibility of this framework by analyzing the implicit bias in
adversarial training, and again obtain significantly improved convergence
rates.",2305.17544v2,https://arxiv.org/pdf/2305.17544v2
"Two Heads are Better than One: Towards Better Adversarial Robustness by
  Combining Transduction and Rejection","Nils Palumbo, Yang Guo, Xi Wu, Jiefeng Chen, Yingyu Liang, Somesh Jha","Both transduction and rejection have emerged as important techniques for
defending against adversarial perturbations. A recent work by Tram\`er showed
that, in the rejection-only case (no transduction), a strong rejection-solution
can be turned into a strong (but computationally inefficient) non-rejection
solution. This detector-to-classifier reduction has been mostly applied to give
evidence that certain claims of strong selective-model solutions are
susceptible, leaving the benefits of rejection unclear. On the other hand, a
recent work by Goldwasser et al. showed that rejection combined with
transduction can give provable guarantees (for certain problems) that cannot be
achieved otherwise. Nevertheless, under recent strong adversarial attacks
(GMSA, which has been shown to be much more effective than AutoAttack against
transduction), Goldwasser et al.'s work was shown to have low performance in a
practical deep-learning setting. In this paper, we take a step towards
realizing the promise of transduction+rejection in more realistic scenarios.
Theoretically, we show that a novel application of Tram\`er's
classifier-to-detector technique in the transductive setting can give
significantly improved sample-complexity for robust generalization. While our
theoretical construction is computationally inefficient, it guides us to
identify an efficient transductive algorithm to learn a selective model.
Extensive experiments using state of the art attacks (AutoAttack, GMSA) show
that our solutions provide significantly better robust accuracy.",2305.17528v1,https://arxiv.org/pdf/2305.17528v1
"On the Importance of Backbone to the Adversarial Robustness of Object
  Detectors","Xiao Li, Hang Chen, Xiaolin Hu","Object detection is a critical component of various security-sensitive
applications, such as autonomous driving and video surveillance. However,
existing deep learning-based object detectors are vulnerable to adversarial
attacks, which poses a significant challenge to their reliability and safety.
Through experiments, we found that existing works on improving the adversarial
robustness of object detectors have given a false sense of security. We argue
that using adversarially pre-trained backbone networks is essential for
enhancing the adversarial robustness of object detectors. We propose a simple
yet effective recipe for fast adversarial fine-tuning on object detectors with
adversarially pre-trained backbones. Without any modifications to the structure
of object detectors, our recipe achieved significantly better adversarial
robustness than previous works. Moreover, we explore the potential of different
modern object detectors to improve adversarial robustness using our recipe and
demonstrate several interesting findings. Our empirical results set a new
milestone and deepen the understanding of adversarially robust object
detection. Code and trained checkpoints will be publicly available.",2305.17438v1,https://arxiv.org/pdf/2305.17438v1
"Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained
  Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence","Erik C. Johnson, Brian S. Robinson, Gautam K. Vallabha, Justin Joyce, Jordan K. Matelsky, Raphael Norman-Tenazas, Isaac Western, Marisel Villafañe-Delgado, Martha Cervantes, Michael S. Robinette, Arun V. Reddy, Lindsey Kitchell, Patricia K. Rivlin, Elizabeth P. Reilly, Nathan Drenkow, Matthew J. Roos, I-Jeng Wang, Brock A. Wester, William R. Gray-Roncal, Joan A. Hoffmann","Despite the progress in deep learning networks, efficient learning at the
edge (enabling adaptable, low-complexity machine learning solutions) remains a
critical need for defense and commercial applications. We envision a pipeline
to utilize large neuroimaging datasets, including maps of the brain which
capture neuron and synapse connectivity, to improve machine learning
approaches. We have pursued different approaches within this pipeline
structure. First, as a demonstration of data-driven discovery, the team has
developed a technique for discovery of repeated subcircuits, or motifs. These
were incorporated into a neural architecture search approach to evolve network
architectures. Second, we have conducted analysis of the heading direction
circuit in the fruit fly, which performs fusion of visual and angular velocity
features, to explore augmenting existing computational models with new insight.
Our team discovered a novel pattern of connectivity, implemented a new model,
and demonstrated sensor fusion on a robotic platform. Third, the team analyzed
circuitry for memory formation in the fruit fly connectome, enabling the design
of a novel generative replay approach. Finally, the team has begun analysis of
connectivity in mammalian cortex to explore potential improvements to
transformer networks. These constraints increased network robustness on the
most challenging examples in the CIFAR-10-C computer vision robustness
benchmark task, while reducing learnable attention parameters by over an order
of magnitude. Taken together, these results demonstrate multiple potential
approaches to utilize insight from neural systems for developing robust and
efficient machine learning techniques.",2305.17300v1,https://arxiv.org/pdf/2305.17300v1
"Fourier-DeepONet: Fourier-enhanced deep operator networks for full
  waveform inversion with improved accuracy, generalizability, and robustness","Min Zhu, Shihang Feng, Youzuo Lin, Lu Lu","Full waveform inversion (FWI) infers the subsurface structure information
from seismic waveform data by solving a non-convex optimization problem.
Data-driven FWI has been increasingly studied with various neural network
architectures to improve accuracy and computational efficiency. Nevertheless,
the applicability of pre-trained neural networks is severely restricted by
potential discrepancies between the source function used in the field survey
and the one utilized during training. Here, we develop a Fourier-enhanced deep
operator network (Fourier-DeepONet) for FWI with the generalization of seismic
sources, including the frequencies and locations of sources. Specifically, we
employ the Fourier neural operator as the decoder of DeepONet, and we utilize
source parameters as one input of Fourier-DeepONet, facilitating the resolution
of FWI with variable sources. To test Fourier-DeepONet, we develop three new
and realistic FWI benchmark datasets (FWI-F, FWI-L, and FWI-FL) with varying
source frequencies, locations, or both. Our experiments demonstrate that
compared with existing data-driven FWI methods, Fourier-DeepONet obtains more
accurate predictions of subsurface structures in a wide range of source
parameters. Moreover, the proposed Fourier-DeepONet exhibits superior
robustness when handling data with Gaussian noise or missing traces and sources
with Gaussian noise, paving the way for more reliable and accurate subsurface
imaging across diverse real conditions.",2305.17289v2,https://arxiv.org/pdf/2305.17289v2
"Robust Lane Detection through Self Pre-training with Masked Sequential
  Autoencoders and Fine-tuning with Customized PolyLoss","Ruohan Li, Yongqi Dong","Lane detection is crucial for vehicle localization which makes it the
foundation for automated driving and many intelligent and advanced driving
assistant systems. Available vision-based lane detection methods do not make
full use of the valuable features and aggregate contextual information,
especially the interrelationships between lane lines and other regions of the
images in continuous frames. To fill this research gap and upgrade lane
detection performance, this paper proposes a pipeline consisting of self
pre-training with masked sequential autoencoders and fine-tuning with
customized PolyLoss for the end-to-end neural network models using
multi-continuous image frames. The masked sequential autoencoders are adopted
to pre-train the neural network models with reconstructing the missing pixels
from a random masked image as the objective. Then, in the fine-tuning
segmentation phase where lane detection segmentation is performed, the
continuous image frames are served as the inputs, and the pre-trained model
weights are transferred and further updated using the backpropagation mechanism
with customized PolyLoss calculating the weighted errors between the output
lane detection results and the labeled ground truth. Extensive experiment
results demonstrate that, with the proposed pipeline, the lane detection model
performance on both normal and challenging scenes can be advanced beyond the
state-of-the-art, delivering the best testing accuracy (98.38%), precision
(0.937), and F1-measure (0.924) on the normal scene testing set, together with
the best overall accuracy (98.36%) and precision (0.844) in the challenging
scene test set, while the training time can be substantially shortened.",2305.17271v2,https://arxiv.org/pdf/2305.17271v2
"Exact Generalization Guarantees for (Regularized) Wasserstein
  Distributionally Robust Models","Waïss Azizian, Franck Iutzeler, Jérôme Malick","Wasserstein distributionally robust estimators have emerged as powerful
models for prediction and decision-making under uncertainty. These estimators
provide attractive generalization guarantees: the robust objective obtained
from the training distribution is an exact upper bound on the true risk with
high probability. However, existing guarantees either suffer from the curse of
dimensionality, are restricted to specific settings, or lead to spurious error
terms. In this paper, we show that these generalization guarantees actually
hold on general classes of models, do not suffer from the curse of
dimensionality, and can even cover distribution shifts at testing. We also
prove that these results carry over to the newly-introduced regularized
versions of Wasserstein distributionally robust problems.",2305.17076v2,https://arxiv.org/pdf/2305.17076v2
"A Tale of Two Approximations: Tightening Over-Approximation for DNN
  Robustness Verification via Under-Approximation","Zhiyi Xue, Si Liu, Zhaodi Zhang, Yiting Wu, Min Zhang","The robustness of deep neural networks (DNNs) is crucial to the hosting
system's reliability and security. Formal verification has been demonstrated to
be effective in providing provable robustness guarantees. To improve its
scalability, over-approximating the non-linear activation functions in DNNs by
linear constraints has been widely adopted, which transforms the verification
problem into an efficiently solvable linear programming problem. Many efforts
have been dedicated to defining the so-called tightest approximations to reduce
overestimation imposed by over-approximation. In this paper, we study existing
approaches and identify a dominant factor in defining tight approximation,
namely the approximation domain of the activation function. We find out that
tight approximations defined on approximation domains may not be as tight as
the ones on their actual domains, yet existing approaches all rely only on
approximation domains. Based on this observation, we propose a novel
dual-approximation approach to tighten over-approximations, leveraging an
activation function's underestimated domain to define tight approximation
bounds. We implement our approach with two complementary algorithms based
respectively on Monte Carlo simulation and gradient descent into a tool called
DualApp. We assess it on a comprehensive benchmark of DNNs with different
architectures. Our experimental results show that DualApp significantly
outperforms the state-of-the-art approaches with 100% - 1000% improvement on
the verified robustness ratio and 10.64% on average (up to 66.53%) on the
certified lower bound.",2305.16998v1,https://arxiv.org/pdf/2305.16998v1
On Evaluating Adversarial Robustness of Large Vision-Language Models,"Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, Min Lin","Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented
performance in response generation, especially with visual inputs, enabling
more creative and adaptable interaction than large language models such as
ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since
adversaries may successfully evade the entire system by subtly manipulating the
most vulnerable modality (e.g., vision). To this end, we propose evaluating the
robustness of open-source large VLMs in the most realistic and high-risk
setting, where adversaries have only black-box system access and seek to
deceive the model into returning the targeted responses. In particular, we
first craft targeted adversarial examples against pretrained models such as
CLIP and BLIP, and then transfer these adversarial examples to other VLMs such
as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we
observe that black-box queries on these VLMs can further improve the
effectiveness of targeted evasion, resulting in a surprisingly high success
rate for generating targeted responses. Our findings provide a quantitative
understanding regarding the adversarial vulnerability of large VLMs and call
for a more thorough examination of their potential security flaws before
deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",2305.16934v2,https://arxiv.org/pdf/2305.16934v2
A Robust Probabilistic Approach to Stochastic Subspace Identification,"Brandon J. O'Connell, Timothy J. Rogers","Modal parameter estimation of operational structures is often a challenging
task when confronted with unwanted distortions (outliers) in field
measurements. Atypical observations present a problem to operational modal
analysis (OMA) algorithms, such as stochastic subspace identification (SSI),
severely biasing parameter estimates and resulting in misidentification of the
system. Despite this predicament, no simple mechanism currently exists capable
of dealing with such anomalies in SSI. Addressing this problem, this paper
first introduces a novel probabilistic formulation of stochastic subspace
identification (Prob-SSI), realised using probabilistic projections.
Mathematically, the equivalence between this model and the classic algorithm is
demonstrated. This fresh perspective, viewing SSI as a problem in probabilistic
inference, lays the necessary mathematical foundation to enable a plethora of
new, more sophisticated OMA approaches. To this end, a statistically robust SSI
algorithm (robust Prob-SSI) is developed, capable of providing a principled and
automatic way of handling outlying or anomalous data in the measured
timeseries, such as may occur in field recordings, e.g. intermittent sensor
dropout. Robust Prob-SSI is shown to outperform conventional SSI when
confronted with 'corrupted' data, exhibiting improved identification
performance and higher levels of confidence in the found poles when viewing
consistency (stabilisation) diagrams. Similar benefits are also demonstrated on
the Z24 Bridge benchmark dataset, highlighting enhanced performance on measured
systems.",2305.16836v1,https://arxiv.org/pdf/2305.16836v1
Robust Nonparametric Regression under Poisoning Attack,"Puning Zhao, Zhiguo Wan","This paper studies robust nonparametric regression, in which an adversarial
attacker can modify the values of up to $q$ samples from a training dataset of
size $N$. Our initial solution is an M-estimator based on Huber loss
minimization. Compared with simple kernel regression, i.e. the Nadaraya-Watson
estimator, this method can significantly weaken the impact of malicious samples
on the regression performance. We provide the convergence rate as well as the
corresponding minimax lower bound. The result shows that, with proper bandwidth
selection, $\ell_\infty$ error is minimax optimal. The $\ell_2$ error is
optimal with relatively small $q$, but is suboptimal with larger $q$. The
reason is that this estimator is vulnerable if there are many attacked samples
concentrating in a small region. To address this issue, we propose a correction
method by projecting the initial estimate to the space of Lipschitz functions.
The final estimate is nearly minimax optimal for arbitrary $q$, up to a $\ln N$
factor.",2305.16771v2,https://arxiv.org/pdf/2305.16771v2
"The Curious Price of Distributional Robustness in Reinforcement Learning
  with a Generative Model","Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, Matthieu Geist, Yuejie Chi","This paper investigates model robustness in reinforcement learning (RL) to
reduce the sim-to-real gap in practice. We adopt the framework of
distributionally robust Markov decision processes (RMDPs), aimed at learning a
policy that optimizes the worst-case performance when the deployed environment
falls within a prescribed uncertainty set around the nominal MDP. Despite
recent efforts, the sample complexity of RMDPs remained mostly unsettled
regardless of the uncertainty set in use. It was unclear if distributional
robustness bears any statistical consequences when benchmarked against standard
RL. Assuming access to a generative model that draws samples based on the
nominal MDP, we characterize the sample complexity of RMDPs when the
uncertainty set is specified via either the total variation (TV) distance or
$\chi^2$ divergence. The algorithm studied here is a model-based method called
{\em distributionally robust value iteration}, which is shown to be
near-optimal for the full range of uncertainty levels. Somewhat surprisingly,
our results uncover that RMDPs are not necessarily easier or harder to learn
than standard MDPs. The statistical consequence incurred by the robustness
requirement depends heavily on the size and shape of the uncertainty set: in
the case w.r.t.~the TV distance, the minimax sample complexity of RMDPs is
always smaller than that of standard MDPs; in the case w.r.t.~the $\chi^2$
divergence, the sample complexity of RMDPs can often far exceed the standard
MDP counterpart.",2305.16589v2,https://arxiv.org/pdf/2305.16589v2
"Learning Robust Statistics for Simulation-based Inference under Model
  Misspecification","Daolang Huang, Ayush Bharti, Amauri Souza, Luigi Acerbi, Samuel Kaski","Simulation-based inference (SBI) methods such as approximate Bayesian
computation (ABC), synthetic likelihood, and neural posterior estimation (NPE)
rely on simulating statistics to infer parameters of intractable likelihood
models. However, such methods are known to yield untrustworthy and misleading
inference outcomes under model misspecification, thus hindering their
widespread applicability. In this work, we propose the first general approach
to handle model misspecification that works across different classes of SBI
methods. Leveraging the fact that the choice of statistics determines the
degree of misspecification in SBI, we introduce a regularized loss function
that penalises those statistics that increase the mismatch between the data and
the model. Taking NPE and ABC as use cases, we demonstrate the superior
performance of our method on high-dimensional time-series models that are
artificially misspecified. We also apply our method to real data from the field
of radio propagation where the model is known to be misspecified. We show
empirically that the method yields robust inference in misspecified scenarios,
whilst still being accurate when the model is well-specified.",2305.15871v3,https://arxiv.org/pdf/2305.15871v3
"Carefully Blending Adversarial Training and Purification Improves
  Adversarial Robustness","Emanuele Ballarin, Alessio Ansuini, Luca Bortolussi","In this work, we propose a novel adversarial defence mechanism for image
classification - CARSO - blending the paradigms of adversarial training and
adversarial purification in a synergistic robustness-enhancing way. The method
builds upon an adversarially-trained classifier, and learns to map its internal
representation associated with a potentially perturbed input onto a
distribution of tentative clean reconstructions. Multiple samples from such
distribution are classified by the same adversarially-trained model, and an
aggregation of its outputs finally constitutes the robust prediction of
interest. Experimental evaluation by a well-established benchmark of strong
adaptive attacks, across different image datasets, shows that CARSO is able to
defend itself against adaptive end-to-end white-box attacks devised for
stochastic defences. Paying a modest clean accuracy toll, our method improves
by a significant margin the state-of-the-art for CIFAR-10, CIFAR-100, and
TinyImageNet-200 $\ell_\infty$ robust classification accuracy against
AutoAttack. Code, and instructions to obtain pre-trained models are available
at https://github.com/emaballarin/CARSO .",2306.06081v4,https://arxiv.org/pdf/2306.06081v4
"DDDM-VC: Decoupled Denoising Diffusion Models with Disentangled
  Representation and Prior Mixup for Verified Robust Voice Conversion","Ha-Yeong Choi, Sang-Hoon Lee, Seong-Whan Lee","Diffusion-based generative models have exhibited powerful generative
performance in recent years. However, as many attributes exist in the data
distribution and owing to several limitations of sharing the model parameters
across all levels of the generation process, it remains challenging to control
specific styles for each attribute. To address the above problem, this paper
presents decoupled denoising diffusion models (DDDMs) with disentangled
representations, which can control the style for each attribute in generative
models. We apply DDDMs to voice conversion (VC) tasks to address the challenges
of disentangling and controlling each speech attribute (e.g., linguistic
information, intonation, and timbre). First, we use a self-supervised
representation to disentangle the speech representation. Subsequently, the
DDDMs are applied to resynthesize the speech from the disentangled
representations for denoising with respect to each attribute. Moreover, we also
propose the prior mixup for robust voice style transfer, which uses the
converted representation of the mixed style as a prior distribution for the
diffusion models. The experimental results reveal that our method outperforms
publicly available VC models. Furthermore, we show that our method provides
robust generative performance regardless of the model size. Audio samples are
available https://hayeong0.github.io/DDDM-VC-demo/.",2305.15816v1,https://arxiv.org/pdf/2305.15816v1
IDEA: Invariant Defense for Graph Adversarial Robustness,"Shuchang Tao, Qi Cao, Huawei Shen, Yunfan Wu, Bingbing Xu, Xueqi Cheng","Despite the success of graph neural networks (GNNs), their vulnerability to
adversarial attacks poses tremendous challenges for practical applications.
Existing defense methods suffer from severe performance decline under unseen
attacks, due to either limited observed adversarial examples or pre-defined
heuristics. To address these limitations, we analyze the causalities in graph
adversarial attacks and conclude that causal features are key to achieve graph
adversarial robustness, owing to their determinedness for labels and invariance
across attacks. To learn these causal features, we innovatively propose an
Invariant causal DEfense method against adversarial Attacks (IDEA). We derive
node-based and structure-based invariance objectives from an
information-theoretic perspective. IDEA ensures strong predictability for
labels and invariant predictability across attacks, which is provably a
causally invariant defense across various attacks. Extensive experiments
demonstrate that IDEA attains state-of-the-art defense performance under all
five attacks on all five datasets. The implementation of IDEA is available at
https://anonymous.4open.science/r/IDEA.",2305.15792v2,https://arxiv.org/pdf/2305.15792v2
Robust Ante-hoc Graph Explainer using Bilevel Optimization,"Kha-Dinh Luong, Mert Kosan, Arlei Lopes Da Silva, Ambuj Singh","Explaining the decisions made by machine learning models for high-stakes
applications is critical for increasing transparency and guiding improvements
to these decisions. This is particularly true in the case of models for graphs,
where decisions often depend on complex patterns combining rich structural and
attribute data. While recent work has focused on designing so-called post-hoc
explainers, the broader question of what constitutes a good explanation remains
open. One intuitive property is that explanations should be sufficiently
informative to reproduce the predictions given the data. In other words, a good
explainer can be repurposed as a predictor. Post-hoc explainers do not achieve
this goal as their explanations are highly dependent on fixed model parameters
(e.g., learned GNN weights). To address this challenge, we propose RAGE (Robust
Ante-hoc Graph Explainer), a novel and flexible ante-hoc explainer designed to
discover explanations for graph neural networks using bilevel optimization,
with a focus on the chemical domain. RAGE can effectively identify molecular
substructures that contain the full information needed for prediction while
enabling users to rank these explanations in terms of relevance. Our
experiments on various molecular classification tasks show that RAGE
explanations are better than existing post-hoc and ante-hoc approaches.",2305.15745v2,https://arxiv.org/pdf/2305.15745v2
A Robust Classifier Under Missing-Not-At-Random Sample Selection Bias,"Huy Mai, Wen Huang, Wei Du, Xintao Wu","The shift between the training and testing distributions is commonly due to
sample selection bias, a type of bias caused by non-random sampling of examples
to be included in the training set. Although there are many approaches proposed
to learn a classifier under sample selection bias, few address the case where a
subset of labels in the training set are missing-not-at-random (MNAR) as a
result of the selection process. In statistics, Greene's method formulates this
type of sample selection with logistic regression as the prediction model.
However, we find that simply integrating this method into a robust
classification framework is not effective for this bias setting. In this paper,
we propose BiasCorr, an algorithm that improves on Greene's method by modifying
the original training set in order for a classifier to learn under MNAR sample
selection bias. We provide theoretical guarantee for the improvement of
BiasCorr over Greene's method by analyzing its bias. Experimental results on
real-world datasets demonstrate that BiasCorr produces robust classifiers and
can be extended to outperform state-of-the-art classifiers that have been
proposed to train under sample selection bias.",2305.15641v1,https://arxiv.org/pdf/2305.15641v1
"Control invariant set enhanced safe reinforcement learning: improved
  sampling efficiency, guaranteed stability and robustness","Song Bo, Bernard T. Agyeman, Xunyuan Yin, Jinfeng Liu","Reinforcement learning (RL) is an area of significant research interest, and
safe RL in particular is attracting attention due to its ability to handle
safety-driven constraints that are crucial for real-world applications. This
work proposes a novel approach to RL training, called control invariant set
(CIS) enhanced RL, which leverages the advantages of utilizing the explicit
form of CIS to improve stability guarantees and sampling efficiency.
Furthermore, the robustness of the proposed approach is investigated in the
presence of uncertainty. The approach consists of two learning stages: offline
and online. In the offline stage, CIS is incorporated into the reward design,
initial state sampling, and state reset procedures. This incorporation of CIS
facilitates improved sampling efficiency during the offline training process.
In the online stage, RL is retrained whenever the predicted next step state is
outside of the CIS, which serves as a stability criterion, by introducing a
Safety Supervisor to examine the safety of the action and make necessary
corrections. The stability analysis is conducted for both cases, with and
without uncertainty. To evaluate the proposed approach, we apply it to a
simulated chemical reactor. The results show a significant improvement in
sampling efficiency during offline training and closed-loop stability guarantee
in the online implementation, with and without uncertainty.",2305.15602v1,https://arxiv.org/pdf/2305.15602v1
RAND: Robustness Aware Norm Decay For Quantized Seq2seq Models,"David Qiu, David Rim, Shaojin Ding, Oleg Rybakov, Yanzhang He","With the rapid increase in the size of neural networks, model compression has
become an important area of research. Quantization is an effective technique at
decreasing the model size, memory access, and compute load of large models.
Despite recent advances in quantization aware training (QAT) technique, most
papers present evaluations that are focused on computer vision tasks, which
have different training dynamics compared to sequence tasks. In this paper, we
first benchmark the impact of popular techniques such as straight through
estimator, pseudo-quantization noise, learnable scale parameter, clipping, etc.
on 4-bit seq2seq models across a suite of speech recognition datasets ranging
from 1,000 hours to 1 million hours, as well as one machine translation dataset
to illustrate its applicability outside of speech.
  Through the experiments, we report that noise based QAT suffers when there is
insufficient regularization signal flowing back to the quantization scale. We
propose low complexity changes to the QAT process to improve model accuracy
(outperforming popular learnable scale and clipping methods). With the improved
accuracy, it opens up the possibility to exploit some of the other benefits of
noise based QAT: 1) training a single model that performs well in mixed
precision mode and 2) improved generalization on long form speech recognition.",2305.15536v1,https://arxiv.org/pdf/2305.15536v1
"Differentially-Private Decision Trees and Provable Robustness to Data
  Poisoning","Daniël Vos, Jelle Vos, Tianyu Li, Zekeriya Erkin, Sicco Verwer","Decision trees are interpretable models that are well-suited to non-linear
learning problems. Much work has been done on extending decision tree learning
algorithms with differential privacy, a system that guarantees the privacy of
samples within the training data. However, current state-of-the-art algorithms
for this purpose sacrifice much utility for a small privacy benefit. These
solutions create random decision nodes that reduce decision tree accuracy or
spend an excessive share of the privacy budget on labeling leaves. Moreover,
many works do not support continuous features or leak information about them.
We propose a new method called PrivaTree based on private histograms that
chooses good splits while consuming a small privacy budget. The resulting trees
provide a significantly better privacy-utility trade-off and accept mixed
numerical and categorical data without leaking information about numerical
features. Finally, while it is notoriously hard to give robustness guarantees
against data poisoning attacks, we demonstrate bounds for the expected accuracy
and success rates of backdoor attacks against differentially-private learners.
By leveraging the better privacy-utility trade-off of PrivaTree we are able to
train decision trees with significantly better robustness against backdoor
attacks compared to regular decision trees and with meaningful theoretical
guarantees.",2305.15394v2,https://arxiv.org/pdf/2305.15394v2
Robust Sparse Mean Estimation via Incremental Learning,"Jianhao Ma, Rui Ray Chen, Yinghui He, Salar Fattahi, Wei Hu","In this paper, we study the problem of robust sparse mean estimation, where
the goal is to estimate a $k$-sparse mean from a collection of partially
corrupted samples drawn from a heavy-tailed distribution. Existing estimators
face two critical challenges in this setting. First, they are limited by a
conjectured computational-statistical tradeoff, implying that any
computationally efficient algorithm needs $\tilde\Omega(k^2)$ samples, while
its statistically-optimal counterpart only requires $\tilde O(k)$ samples.
Second, the existing estimators fall short of practical use as they scale
poorly with the ambient dimension. This paper presents a simple mean estimator
that overcomes both challenges under moderate conditions: it runs in
near-linear time and memory (both with respect to the ambient dimension) while
requiring only $\tilde O(k)$ samples to recover the true mean. At the core of
our method lies an incremental learning phenomenon: we introduce a simple
nonconvex framework that can incrementally learn the top-$k$ nonzero elements
of the mean while keeping the zero elements arbitrarily small. Unlike existing
estimators, our method does not need any prior knowledge of the sparsity level
$k$. We prove the optimality of our estimator by providing a matching
information-theoretic lower bound. Finally, we conduct a series of simulations
to corroborate our theoretical findings. Our code is available at
https://github.com/huihui0902/Robust_mean_estimation.",2305.15276v1,https://arxiv.org/pdf/2305.15276v1
Robust Classification via a Single Diffusion Model,"Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan, Hang Su, Jun Zhu","Diffusion models have been applied to improve adversarial robustness of image
classifiers by purifying the adversarial noises or generating realistic data
for adversarial training. However, diffusion-based purification can be evaded
by stronger adaptive attacks while adversarial training does not perform well
under unseen threats, exhibiting inevitable limitations of these methods. To
better harness the expressive power of diffusion models, this paper proposes
Robust Diffusion Classifier (RDC), a generative classifier that is constructed
from a pre-trained diffusion model to be adversarially robust. RDC first
maximizes the data likelihood of a given input and then predicts the class
probabilities of the optimized input using the conditional likelihood estimated
by the diffusion model through Bayes' theorem. To further reduce the
computational cost, we propose a new diffusion backbone called multi-head
diffusion and develop efficient sampling strategies. As RDC does not require
training on particular adversarial attacks, we demonstrate that it is more
generalizable to defend against multiple unseen threats. In particular, RDC
achieves $75.67\%$ robust accuracy against various $\ell_\infty$ norm-bounded
adaptive attacks with $\epsilon_\infty=8/255$ on CIFAR-10, surpassing the
previous state-of-the-art adversarial training models by $+4.77\%$. The results
highlight the potential of generative classifiers by employing pre-trained
diffusion models for adversarial robustness compared with the commonly studied
discriminative classifiers. Code is available at
\url{https://github.com/huanranchen/DiffusionClassifier}.",2305.15241v2,https://arxiv.org/pdf/2305.15241v2
ReSync: Riemannian Subgradient-based Robust Rotation Synchronization,"Huikang Liu, Xiao Li, Anthony Man-Cho So","This work presents ReSync, a Riemannian subgradient-based algorithm for
solving the robust rotation synchronization problem, which arises in various
engineering applications. ReSync solves a least-unsquared minimization
formulation over the rotation group, which is nonsmooth and nonconvex, and aims
at recovering the underlying rotations directly. We provide strong theoretical
guarantees for ReSync under the random corruption setting. Specifically, we
first show that the initialization procedure of ReSync yields a proper initial
point that lies in a local region around the ground-truth rotations. We next
establish the weak sharpness property of the aforementioned formulation and
then utilize this property to derive the local linear convergence of ReSync to
the ground-truth rotations. By combining these guarantees, we conclude that
ReSync converges linearly to the ground-truth rotations under appropriate
conditions. Experiment results demonstrate the effectiveness of ReSync.",2305.15136v2,https://arxiv.org/pdf/2305.15136v2
"An Examination of the Robustness of Reference-Free Image Captioning
  Evaluation Metrics","Saba Ahmadi, Aishwarya Agrawal","Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021),
UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for
automatic reference-free evaluation of image captions. Our focus lies in
evaluating the robustness of these metrics in scenarios that require
distinguishing between two captions with high lexical overlap but very
different meanings. Our findings reveal that despite their high correlation
with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify
fine-grained errors. While all metrics exhibit strong sensitivity to visual
grounding errors, their sensitivity to caption implausibility errors is
limited. Furthermore, we found that all metrics are sensitive to variations in
the size of image-relevant objects mentioned in the caption, while CLIPScore
and PAC-S are also sensitive to the number of mentions of image-relevant
objects in the caption. Regarding linguistic aspects of a caption, all metrics
show weak comprehension of negation, and CLIPScore and PAC-S are insensitive to
the structure of the caption to a great extent. We hope our findings will guide
further improvements in reference-free evaluation of image captioning.",2305.14998v2,https://arxiv.org/pdf/2305.14998v2
Non-adversarial Robustness of Deep Learning Methods for Computer Vision,"Gorana Gojić, Vladimir Vincan, Ognjen Kundačina, Dragiša Mišković, Dinu Dragan","Non-adversarial robustness, also known as natural robustness, is a property
of deep learning models that enables them to maintain performance even when
faced with distribution shifts caused by natural variations in data. However,
achieving this property is challenging because it is difficult to predict in
advance the types of distribution shifts that may occur. To address this
challenge, researchers have proposed various approaches, some of which
anticipate potential distribution shifts, while others utilize knowledge about
the shifts that have already occurred to enhance model generalizability. In
this paper, we present a brief overview of the most recent techniques for
improving the robustness of computer vision methods, as well as a summary of
commonly used robustness benchmark datasets for evaluating the model's
performance under data distribution shifts. Finally, we examine the strengths
and limitations of the approaches reviewed and identify general trends in deep
learning robustness improvement for computer vision.",2305.14986v1,https://arxiv.org/pdf/2305.14986v1
Adversarial robustness of amortized Bayesian inference,"Manuel Glöckler, Michael Deistler, Jakob H. Macke","Bayesian inference usually requires running potentially costly inference
procedures separately for every new observation. In contrast, the idea of
amortized Bayesian inference is to initially invest computational cost in
training an inference network on simulated data, which can subsequently be used
to rapidly perform inference (i.e., to return estimates of posterior
distributions) for new observations. This approach has been applied to many
real-world models in the sciences and engineering, but it is unclear how robust
the approach is to adversarial perturbations in the observed data. Here, we
study the adversarial robustness of amortized Bayesian inference, focusing on
simulation-based estimation of multi-dimensional posterior distributions. We
show that almost unrecognizable, targeted perturbations of the observations can
lead to drastic changes in the predicted posterior and highly unrealistic
posterior predictive samples, across several benchmark tasks and a real-world
example from neuroscience. We propose a computationally efficient
regularization scheme based on penalizing the Fisher information of the
conditional density estimator, and show how it improves the adversarial
robustness of amortized Bayesian inference.",2305.14984v1,https://arxiv.org/pdf/2305.14984v1
"Chain-of-Questions Training with Latent Answers for Robust Multistep
  Question Answering","Wang Zhu, Jesse Thomason, Robin Jia","We train a language model (LM) to robustly answer multistep questions by
generating and answering sub-questions. We propose Chain-of-Questions, a
framework that trains a model to generate sub-questions and sub-answers one at
a time by leveraging human annotated question decomposition meaning
representation (QDMR). The key technical challenge is that QDMR only contains
sub-questions but not answers to those sub-questions, so we treat sub-answers
as latent variables and optimize them using a novel dynamic mixture of Hard-EM
and MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methods
by 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA
adversarial set, thus demonstrating the effectiveness and robustness of our
framework.",2305.14901v3,https://arxiv.org/pdf/2305.14901v3
"DialogVCS: Robust Natural Language Understanding in Dialogue System
  Upgrade","Zefan Cai, Xin Zheng, Tianyu Liu, Xu Wang, Haoran Meng, Jiaqi Han, Gang Yuan, Binghuai Lin, Baobao Chang, Yunbo Cao","In the constant updates of the product dialogue systems, we need to retrain
the natural language understanding (NLU) model as new data from the real users
would be merged into the existent data accumulated in the last updates. Within
the newly added data, new intents would emerge and might have semantic
entanglement with the existing intents, e.g. new intents that are semantically
too specific or generic are actually subset or superset of some existing
intents in the semantic space, thus impairing the robustness of the NLU model.
As the first attempt to solve this problem, we setup a new benchmark consisting
of 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent
detection with imperfect data in the system update as a multi-label
classification task with positive but unlabeled intents, which asks the models
to recognize all the proper intents, including the ones with semantic
entanglement, in the inference. We also propose comprehensive baseline models
and conduct in-depth analyses for the benchmark, showing that the semantically
entangled intents can be effectively recognized with an automatic workflow.",2305.14751v1,https://arxiv.org/pdf/2305.14751v1
AdvFunMatch: When Consistent Teaching Meets Adversarial Robustness,"Zihui Wu, Haichang Gao, Bingqian Zhou, Ping Wang","\emph{Consistent teaching} is an effective paradigm for implementing
knowledge distillation (KD), where both student and teacher models receive
identical inputs, and KD is treated as a function matching task (FunMatch).
However, one limitation of FunMatch is that it does not account for the
transfer of adversarial robustness, a model's resistance to adversarial
attacks. To tackle this problem, we propose a simple but effective strategy
called Adversarial Function Matching (AdvFunMatch), which aims to match
distributions for all data points within the $\ell_p$-norm ball of the training
data, in accordance with consistent teaching. Formulated as a min-max
optimization problem, AdvFunMatch identifies the worst-case instances that
maximizes the KL-divergence between teacher and student model outputs, which we
refer to as ""mismatched examples,"" and then matches the outputs on these
mismatched examples. Our experimental results show that AdvFunMatch effectively
produces student models with both high clean accuracy and robustness.
Furthermore, we reveal that strong data augmentations (\emph{e.g.},
AutoAugment) are beneficial in AdvFunMatch, whereas prior works have found them
less effective in adversarial training. Code is available at
\url{https://gitee.com/zihui998/adv-fun-match}.",2305.14700v2,https://arxiv.org/pdf/2305.14700v2
"Negative Feedback Training: A Novel Concept to Improve Robustness of
  NVCIM DNN Accelerators","Yifan Qin, Zheyu Yan, Wujie Wen, Xiaobo Sharon Hu, Yiyu Shi","Compute-in-memory (CIM) accelerators built upon non-volatile memory (NVM)
devices excel in energy efficiency and latency when performing Deep Neural
Network (DNN) inference, thanks to their in-situ data processing capability.
However, the stochastic nature and intrinsic variations of NVM devices often
result in performance degradation in DNN inference. Introducing these non-ideal
device behaviors during DNN training enhances robustness, but drawbacks include
limited accuracy improvement, reduced prediction confidence, and convergence
issues. This arises from a mismatch between the deterministic training and
non-deterministic device variations, as such training, though considering
variations, relies solely on the model's final output. In this work, we draw
inspiration from the control theory and propose a novel training concept:
Negative Feedback Training (NFT) leveraging the multi-scale noisy information
captured from network. We develop two specific NFT instances, Oriented
Variational Forward (OVF) and Intermediate Representation Snapshot (IRS).
Extensive experiments show that our methods outperform existing
state-of-the-art methods with up to a 46.71% improvement in inference accuracy
while reducing epistemic uncertainty, boosting output confidence, and improving
convergence probability. Their effectiveness highlights the generality and
practicality of our NFT concept in enhancing DNN robustness against device
variations.",2305.14561v4,https://arxiv.org/pdf/2305.14561v4
"Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust
  Conversational Understanding","Zheng Chen, Ziyan Jiang, Fan Yang, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Aram Galstyan","Conversational AI systems such as Alexa need to understand defective queries
to ensure robust conversational understanding and reduce user friction. These
defective queries often arise from user ambiguities, mistakes, or errors in
automatic speech recognition (ASR) and natural language understanding (NLU).
  Personalized query rewriting is an approach that focuses on reducing defects
in queries by taking into account the user's individual behavior and
preferences. It typically relies on an index of past successful user
interactions with the conversational AI. However, unseen interactions within
the user's history present additional challenges for personalized query
rewriting. This paper presents our ""Collaborative Query Rewriting"" approach,
which specifically addresses the task of rewriting new user interactions that
have not been previously observed in the user's history. This approach builds a
""User Feedback Interaction Graph"" (FIG) of historical user-entity interactions
and leverages multi-hop graph traversal to enrich each user's index to cover
future unseen defective queries. The enriched user index is called a
Collaborative User Index and contains hundreds of additional entries. To
counteract precision degradation from the enlarged index, we add additional
transformer layers to the L1 retrieval model and incorporate graph-based and
guardrail features into the L2 ranking model.
  Since the user index can be pre-computed, we further investigate the
utilization of a Large Language Model (LLM) to enhance the FIG for user-entity
link prediction in the Video/Music domains. Specifically, this paper
investigates the Dolly-V2 7B model. We found that the user index augmented by
the fine-tuned Dolly-V2 generation significantly enhanced the coverage of
future unseen user interactions, thereby boosting QR performance on unseen
queries compared with the graph traversal only approach.",2305.14449v3,https://arxiv.org/pdf/2305.14449v3
Impact of Light and Shadow on Robustness of Deep Neural Networks,"Chengyin Hu, Weiwen Shi, Chao Li, Jialiang Sun, Donghua Wang, Junqi Wu, Guijian Tang","Deep neural networks (DNNs) have made remarkable strides in various computer
vision tasks, including image classification, segmentation, and object
detection. However, recent research has revealed a vulnerability in advanced
DNNs when faced with deliberate manipulations of input data, known as
adversarial attacks. Moreover, the accuracy of DNNs is heavily influenced by
the distribution of the training dataset. Distortions or perturbations in the
color space of input images can introduce out-of-distribution data, resulting
in misclassification. In this work, we propose a brightness-variation dataset,
which incorporates 24 distinct brightness levels for each image within a subset
of ImageNet. This dataset enables us to simulate the effects of light and
shadow on the images, so as is to investigate the impact of light and shadow on
the performance of DNNs. In our study, we conduct experiments using several
state-of-the-art DNN architectures on the aforementioned dataset. Through our
analysis, we discover a noteworthy positive correlation between the brightness
levels and the loss of accuracy in DNNs. Furthermore, we assess the
effectiveness of recently proposed robust training techniques and strategies,
including AugMix, Revisit, and Free Normalizer, using the ResNet50 architecture
on our brightness-variation dataset. Our experimental results demonstrate that
these techniques can enhance the robustness of DNNs against brightness
variation, leading to improved performance when dealing with images exhibiting
varying brightness levels.",2305.14165v1,https://arxiv.org/pdf/2305.14165v1
"Leveraging Open Information Extraction for More Robust Domain Transfer
  of Event Trigger Detection","David Dukić, Kiril Gashteovski, Goran Glavaš, Jan Šnajder","Event detection is a crucial information extraction task in many domains,
such as Wikipedia or news. The task typically relies on trigger detection (TD)
-- identifying token spans in the text that evoke specific events. While the
notion of triggers should ideally be universal across domains, domain transfer
for TD from high- to low-resource domains results in significant performance
drops. We address the problem of negative transfer in TD by coupling triggers
between domains using subject-object relations obtained from a rule-based open
information extraction (OIE) system. We demonstrate that OIE relations injected
through multi-task training can act as mediators between triggers in different
domains, enhancing zero- and few-shot TD domain transfer and reducing
performance drops, in particular when transferring from a high-resource source
domain (Wikipedia) to a low(er)-resource target domain (news). Additionally, we
combine this improved transfer with masked language modeling on the target
domain, observing further TD transfer gains. Finally, we demonstrate that the
gains are robust to the choice of the OIE system.",2305.14163v2,https://arxiv.org/pdf/2305.14163v2
"Robust Representation Learning with Reliable Pseudo-labels Generation
  via Self-Adaptive Optimal Transport for Short Text Clustering","Xiaolin Zheng, Mengling Hu, Weiming Liu, Chaochao Chen, Xinting Liao","Short text clustering is challenging since it takes imbalanced and noisy data
as inputs. Existing approaches cannot solve this problem well, since (1) they
are prone to obtain degenerate solutions especially on heavy imbalanced
datasets, and (2) they are vulnerable to noises. To tackle the above issues, we
propose a Robust Short Text Clustering (RSTC) model to improve robustness
against imbalanced and noisy data. RSTC includes two modules, i.e.,
pseudo-label generation module and robust representation learning module. The
former generates pseudo-labels to provide supervision for the later, which
contributes to more robust representations and correctly separated clusters. To
provide robustness against the imbalance in data, we propose self-adaptive
optimal transport in the pseudo-label generation module. To improve robustness
against the noise in data, we further introduce both class-wise and
instance-wise contrastive learning in the robust representation learning
module. Our empirical studies on eight short text clustering datasets
demonstrate that RSTC significantly outperforms the state-of-the-art models.
The code is available at: https://github.com/hmllmh/RSTC.",2305.16335v1,https://arxiv.org/pdf/2305.16335v1
Expressive Losses for Verified Robustness via Convex Combinations,"Alessandro De Palma, Rudy Bunel, Krishnamurthy Dvijotham, M. Pawan Kumar, Robert Stanforth, Alessio Lomuscio","In order to train networks for verified adversarial robustness, it is common
to over-approximate the worst-case loss over perturbation regions, resulting in
networks that attain verifiability at the expense of standard performance. As
shown in recent work, better trade-offs between accuracy and robustness can be
obtained by carefully coupling adversarial training with over-approximations.
We hypothesize that the expressivity of a loss function, which we formalize as
the ability to span a range of trade-offs between lower and upper bounds to the
worst-case loss through a single parameter (the over-approximation
coefficient), is key to attaining state-of-the-art performance. To support our
hypothesis, we show that trivial expressive losses, obtained via convex
combinations between adversarial attacks and IBP bounds, yield state-of-the-art
results across a variety of settings in spite of their conceptual simplicity.
We provide a detailed analysis of the relationship between the
over-approximation coefficient and performance profiles across different
expressive losses, showing that, while expressivity is essential, better
approximations of the worst-case loss are not necessarily linked to superior
robustness-accuracy trade-offs.",2305.13991v3,https://arxiv.org/pdf/2305.13991v3
"Preserving Knowledge Invariance: Rethinking Robustness Evaluation of
  Open Information Extraction","Ji Qi, Chuchun Zhang, Xiaozhi Wang, Kaisheng Zeng, Jifan Yu, Jinxin Liu, Jiuding Sun, Yuxiang Chen, Lei Hou, Juanzi Li, Bin Xu","The robustness to distribution changes ensures that NLP models can be
successfully applied in the realistic world, especially for information
extraction tasks. However, most prior evaluation benchmarks have been devoted
to validating pairwise matching correctness, ignoring the crucial measurement
of robustness. In this paper, we present the first benchmark that simulates the
evaluation of open information extraction models in the real world, where the
syntactic and expressive distributions under the same knowledge meaning may
drift variously. We design and annotate a large-scale testbed in which each
example is a knowledge-invariant clique that consists of sentences with
structured knowledge of the same meaning but with different syntactic and
expressive forms. By further elaborating the robustness metric, a model is
judged to be robust if its performance is consistently accurate on the overall
cliques. We perform experiments on typical models published in the last decade
as well as a popular large language model, the results show that the existing
successful models exhibit a frustrating degradation, with a maximum drop of
23.43 F1 score. Our resources and code are available at
https://github.com/qijimrc/ROBUST.",2305.13981v2,https://arxiv.org/pdf/2305.13981v2
"Robust Prompt Optimization for Large Language Models Against
  Distribution Shifts","Moxin Li, Wenjie Wang, Fuli Feng, Yixin Cao, Jizhi Zhang, Tat-Seng Chua","Large Language Model (LLM) has demonstrated significant ability in various
Natural Language Processing tasks. However, their effectiveness is highly
dependent on the phrasing of the task prompt, leading to research on automatic
prompt optimization using labeled task data. We reveal that these prompt
optimization techniques are vulnerable to distribution shifts such as
subpopulation shifts, which are common for LLMs in real-world scenarios such as
customer reviews analysis. In this light, we propose a new problem of robust
prompt optimization for LLMs against distribution shifts, which requires the
prompt optimized over the labeled source group can simultaneously generalize to
an unlabeled target group. To solve this problem, we propose Generalized Prompt
Optimization framework, which incorporates the unlabeled data from the target
group into prompt optimization. Extensive experimental results demonstrate the
effectiveness of the proposed framework with significant performance
improvement on the target group and comparable performance on the source group.",2305.13954v3,https://arxiv.org/pdf/2305.13954v3
On the Optimal Batch Size for Byzantine-Robust Distributed Learning,"Yi-Rui Yang, Chang-Wei Shi, Wu-Jun Li","Byzantine-robust distributed learning (BRDL), in which computing devices are
likely to behave abnormally due to accidental failures or malicious attacks,
has recently become a hot research topic. However, even in the independent and
identically distributed (i.i.d.) case, existing BRDL methods will suffer from a
significant drop on model accuracy due to the large variance of stochastic
gradients. Increasing batch sizes is a simple yet effective way to reduce the
variance. However, when the total number of gradient computation is fixed, a
too-large batch size will lead to a too-small iteration number (update number),
which may also degrade the model accuracy. In view of this challenge, we mainly
study the optimal batch size when the total number of gradient computation is
fixed in this work. In particular, we theoretically and empirically show that
when the total number of gradient computation is fixed, the optimal batch size
in BRDL increases with the fraction of Byzantine workers. Therefore, compared
to the case without attacks, the batch size should be set larger when under
Byzantine attacks. However, for existing BRDL methods, large batch sizes will
lead to a drop on model accuracy, even if there is no Byzantine attack. To deal
with this problem, we propose a novel BRDL method, called Byzantine-robust
stochastic gradient descent with normalized momentum (ByzSGDnm), which can
alleviate the drop on model accuracy in large-batch cases. Moreover, we
theoretically prove the convergence of ByzSGDnm for general non-convex cases
under Byzantine attacks. Empirical results show that ByzSGDnm has a comparable
performance to existing BRDL methods under bit-flipping failure, but can
outperform existing BRDL methods under deliberately crafted attacks.",2305.13856v1,https://arxiv.org/pdf/2305.13856v1
"Enhancing Accuracy and Robustness through Adversarial Training in Class
  Incremental Continual Learning","Minchan Kwon, Kangil Kim","In real life, adversarial attack to deep learning models is a fatal security
issue. However, the issue has been rarely discussed in a widely used
class-incremental continual learning (CICL). In this paper, we address problems
of applying adversarial training to CICL, which is well-known defense method
against adversarial attack. A well-known problem of CICL is class-imbalance
that biases a model to the current task by a few samples of previous tasks.
Meeting with the adversarial training, the imbalance causes another imbalance
of attack trials over tasks. Lacking clean data of a minority class by the
class-imbalance and increasing of attack trials from a majority class by the
secondary imbalance, adversarial training distorts optimal decision boundaries.
The distortion eventually decreases both accuracy and robustness than
adversarial training. To exclude the effects, we propose a straightforward but
significantly effective method, External Adversarial Training (EAT) which can
be applied to methods using experience replay. This method conduct adversarial
training to an auxiliary external model for the current task data at each time
step, and applies generated adversarial examples to train the target model. We
verify the effects on a toy problem and show significance on CICL benchmarks of
image classification. We expect that the results will be used as the first
baseline for robustness research of CICL.",2305.13678v1,https://arxiv.org/pdf/2305.13678v1
On the robust learning mixtures of linear regressions,"Ying Huang, Liang Chen","In this note, we consider the problem of robust learning mixtures of linear
regressions. We connect mixtures of linear regressions and mixtures of
Gaussians with a simple thresholding, so that a quasi-polynomial time algorithm
can be obtained under some mild separation condition. This algorithm has
significantly better robustness than the previous result.",2305.15317v1,https://arxiv.org/pdf/2305.15317v1
Robust Model-Based Optimization for Challenging Fitness Landscapes,"Saba Ghaffari, Ehsan Saleh, Alexander G. Schwing, Yu-Xiong Wang, Martin D. Burke, Saurabh Sinha","Protein design, a grand challenge of the day, involves optimization on a
fitness landscape, and leading methods adopt a model-based approach where a
model is trained on a training set (protein sequences and fitness) and proposes
candidates to explore next. These methods are challenged by sparsity of
high-fitness samples in the training set, a problem that has been in the
literature. A less recognized but equally important problem stems from the
distribution of training samples in the design space: leading methods are not
designed for scenarios where the desired optimum is in a region that is not
only poorly represented in training data, but also relatively far from the
highly represented low-fitness regions. We show that this problem of
""separation"" in the design space is a significant bottleneck in existing
model-based optimization tools and propose a new approach that uses a novel VAE
as its search model to overcome the problem. We demonstrate its advantage over
prior methods in robustly finding improved samples, regardless of the imbalance
and separation between low- and high-fitness samples. Our comprehensive
benchmark on real and semi-synthetic protein datasets as well as solution
design for physics-informed neural networks, showcases the generality of our
approach in discrete and continuous design spaces. Our implementation is
available at https://github.com/sabagh1994/PGVAE.",2305.13650v3,https://arxiv.org/pdf/2305.13650v3
"Improving Classifier Robustness through Active Generation of Pairwise
  Counterfactuals","Ananth Balashankar, Xuezhi Wang, Yao Qin, Ben Packer, Nithum Thain, Jilin Chen, Ed H. Chi, Alex Beutel","Counterfactual Data Augmentation (CDA) is a commonly used technique for
improving robustness in natural language classifiers. However, one fundamental
challenge is how to discover meaningful counterfactuals and efficiently label
them, with minimal human labeling cost. Most existing methods either completely
rely on human-annotated labels, an expensive process which limits the scale of
counterfactual data, or implicitly assume label invariance, which may mislead
the model with incorrect labels. In this paper, we present a novel framework
that utilizes counterfactual generative models to generate a large number of
diverse counterfactuals by actively sampling from regions of uncertainty, and
then automatically label them with a learned pairwise classifier. Our key
insight is that we can more correctly label the generated counterfactuals by
training a pairwise classifier that interpolates the relationship between the
original example and the counterfactual. We demonstrate that with a small
amount of human-annotated counterfactual data (10%), we can generate a
counterfactual augmentation dataset with learned labels, that provides an
18-20% improvement in robustness and a 14-21% reduction in errors on 6
out-of-domain datasets, comparable to that of a fully human-annotated
counterfactual dataset for both sentiment classification and question
paraphrase tasks.",2305.13535v1,https://arxiv.org/pdf/2305.13535v1
"ColMix -- A Simple Data Augmentation Framework to Improve Object
  Detector Performance and Robustness in Aerial Images","Cuong Ly, Grayson Jorgenson, Dan Rosa de Jesus, Henry Kvinge, Adam Attarian, Yijing Watkins","In the last decade, Convolutional Neural Network (CNN) and transformer based
object detectors have achieved high performance on a large variety of datasets.
Though the majority of detection literature has developed this capability on
datasets such as MS COCO, these detectors have still proven effective for
remote sensing applications. Challenges in this particular domain, such as
small numbers of annotated objects and low object density, hinder overall
performance. In this work, we present a novel augmentation method, called
collage pasting, for increasing the object density without a need for
segmentation masks, thereby improving the detector performance. We demonstrate
that collage pasting improves precision and recall beyond related methods, such
as mosaic augmentation, and enables greater control of object density. However,
we find that collage pasting is vulnerable to certain out-of-distribution
shifts, such as image corruptions. To address this, we introduce two simple
approaches for combining collage pasting with PixMix augmentation method, and
refer to our combined techniques as ColMix. Through extensive experiments, we
show that employing ColMix results in detectors with superior performance on
aerial imagery datasets and robust to various corruptions.",2305.13509v1,https://arxiv.org/pdf/2305.13509v1
"Distilling Robustness into Natural Language Inference Models with
  Domain-Targeted Augmentation","Joe Stacey, Marek Rei","Knowledge distillation optimises a smaller student model to behave similarly
to a larger teacher model, retaining some of the performance benefits. While
this method can improve results on in-distribution examples, it does not
necessarily generalise to out-of-distribution (OOD) settings. We investigate
two complementary methods for improving the robustness of the resulting student
models on OOD domains. The first approach augments the distillation with
generated unlabelled examples that match the target distribution. The second
method upsamples data points among the training set that are similar to the
target distribution. When applied on the task of natural language inference
(NLI), our experiments on MNLI show that distillation with these modifications
outperforms previous robustness solutions. We also find that these methods
improve performance on OOD domains even beyond the target domain.",2305.13067v3,https://arxiv.org/pdf/2305.13067v3
"Towards Robust Personalized Dialogue Generation via Order-Insensitive
  Representation Regularization","Liang Chen, Hongru Wang, Yang Deng, Wai-Chung Kwan, Zezhong Wang, Kam-Fai Wong","Generating persona consistent dialogue response is important for developing
an intelligent conversational agent. Recent works typically fine-tune
large-scale pre-trained models on this task by concatenating persona texts and
dialogue history as a single input sequence to generate the target response.
While simple and effective, our analysis shows that this popular practice is
seriously affected by order sensitivity where different input orders of persona
sentences significantly impact the quality and consistency of generated
response, resulting in severe performance fluctuations (i.e., 29.4% on GPT2 and
83.2% on BART). To mitigate the order sensitivity problem, we propose a
model-agnostic framework, ORder Insensitive Generation (ORIG), which enables
dialogue models to learn robust representation under different persona orders
and improve the consistency of response generation. Experiments on the
Persona-Chat dataset justify the effectiveness and superiority of our method
with two dominant pre-trained models (GPT2 and BART).",2305.12782v1,https://arxiv.org/pdf/2305.12782v1
"A Reinforcement Learning Approach for Robust Supervisory Control of UAVs
  Under Disturbances","Ibrahim Ahmed, Marcos Quinones-Grueiro, Gautam Biswas","In this work, we present an approach to supervisory reinforcement learning
control for unmanned aerial vehicles (UAVs). UAVs are dynamic systems where
control decisions in response to disturbances in the environment have to be
made in the order of milliseconds. We formulate a supervisory control
architecture that interleaves with extant embedded control and demonstrates
robustness to environmental disturbances in the form of adverse wind
conditions. We run case studies with a Tarot T-18 Octorotor to demonstrate the
effectiveness of our approach and compare it against a classic cascade control
architecture used in most vehicles. While the results show the performance
difference is marginal for nominal operations, substantial performance
improvement is obtained with the supervisory RL approach under unseen wind
conditions.",2305.12543v1,https://arxiv.org/pdf/2305.12543v1
"On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion
  and Automatic Speech Recognition","Lokesh Bansal, S. Pavankumar Dubagunta, Malolan Chetlur, Pushpak Jagtap, Aravind Ganapathiraju","New-age conversational agent systems perform both speech emotion recognition
(SER) and automatic speech recognition (ASR) using two separate and often
independent approaches for real-world application in noisy environments. In
this paper, we investigate a joint ASR-SER multitask learning approach in a
low-resource setting and show that improvements are observed not only in SER,
but also in ASR. We also investigate the robustness of such jointly trained
models to the presence of background noise, babble, and music. Experimental
results on the IEMOCAP dataset show that joint learning can improve ASR word
error rate (WER) and SER classification accuracy by 10.7% and 2.3% respectively
in clean scenarios. In noisy scenarios, results on data augmented with MUSAN
show that the joint approach outperforms the independent ASR and SER approaches
across many noisy conditions. Overall, the joint ASR-SER approach yielded more
noise-resistant models than the independent ASR and SER approaches.",2305.12540v2,https://arxiv.org/pdf/2305.12540v2
"P-NOC: adversarial training of CAM generating networks for robust weakly
  supervised semantic segmentation priors","Lucas David, Helio Pedrini, Zanoni Dias","Weakly Supervised Semantic Segmentation (WSSS) techniques explore individual
regularization strategies to refine Class Activation Maps (CAMs). In this work,
we first analyze complementary WSSS techniques in the literature, their
segmentation properties, and the conditions in which they are most effective.
Based on these findings, we devise two new techniques: P-NOC and CCAM-H. In the
first, we promote the conjoint training of two adversarial CAM generating
networks: the generator, which progressively learns to erase regions containing
class-specific features, and a discriminator, which is refined to gradually
shift its attention to new class discriminant features. In the latter, we
employ the high quality pseudo-segmentation priors produced by P-NOC to guide
the learning to saliency information in a weakly supervised fashion. Finally,
we employ both pseudo-segmentation priors and pseudo-saliency proposals in the
random walk procedure, resulting in higher quality pseudo-semantic segmentation
masks, and competitive results with the state of the art.",2305.12522v3,https://arxiv.org/pdf/2305.12522v3
"A Novel Framework for Improving the Breakdown Point of Robust Regression
  Algorithms","Zheyi Fan, Szu Hui Ng, Qingpei Hu","We present an effective framework for improving the breakdown point of robust
regression algorithms. Robust regression has attracted widespread attention due
to the ubiquity of outliers, which significantly affect the estimation results.
However, many existing robust least-squares regression algorithms suffer from a
low breakdown point, as they become stuck around local optima when facing
severe attacks. By expanding on the previous work, we propose a novel framework
that enhances the breakdown point of these algorithms by inserting a prior
distribution in each iteration step, and adjusting the prior distribution
according to historical information. We apply this framework to a specific
algorithm and derive the consistent robust regression algorithm with iterative
local search (CORALS). The relationship between CORALS and momentum gradient
descent is described, and a detailed proof of the theoretical convergence of
CORALS is presented. Finally, we demonstrate that the breakdown point of CORALS
is indeed higher than that of the algorithm from which it is derived. We apply
the proposed framework to other robust algorithms, and show that the improved
algorithms achieve better results than the original algorithms, indicating the
effectiveness of the proposed framework.",2305.12220v1,https://arxiv.org/pdf/2305.12220v1
"CARD: Channel Aligned Robust Blend Transformer for Time Series
  Forecasting","Wang Xue, Tian Zhou, Qingsong Wen, Jinyang Gao, Bolin Ding, Rong Jin","Recent studies have demonstrated the great power of Transformer models for
time series forecasting. One of the key elements that lead to the transformer's
success is the channel-independent (CI) strategy to improve the training
robustness. However, the ignorance of the correlation among different channels
in CI would limit the model's forecasting capacity. In this work, we design a
special Transformer, i.e., Channel Aligned Robust Blend Transformer (CARD for
short), that addresses key shortcomings of CI type Transformer in time series
forecasting. First, CARD introduces a channel-aligned attention structure that
allows it to capture both temporal correlations among signals and dynamical
dependence among multiple variables over time. Second, in order to efficiently
utilize the multi-scale knowledge, we design a token blend module to generate
tokens with different resolutions. Third, we introduce a robust loss function
for time series forecasting to alleviate the potential overfitting issue. This
new loss function weights the importance of forecasting over a finite horizon
based on prediction uncertainties. Our evaluation of multiple long-term and
short-term forecasting datasets demonstrates that CARD significantly
outperforms state-of-the-art time series forecasting methods. The code is
available at the following repository:https://github.com/wxie9/CARD",2305.12095v5,https://arxiv.org/pdf/2305.12095v5
"Robust Counterfactual Explanations for Neural Networks With
  Probabilistic Guarantees","Faisal Hamman, Erfaun Noorani, Saumitra Mishra, Daniele Magazzeni, Sanghamitra Dutta","There is an emerging interest in generating robust counterfactual
explanations that would remain valid if the model is updated or changed even
slightly. Towards finding robust counterfactuals, existing literature often
assumes that the original model $m$ and the new model $M$ are bounded in the
parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{<}\Delta$.
However, models can often change significantly in the parameter space with
little to no change in their predictions or accuracy on the given dataset. In
this work, we introduce a mathematical abstraction termed
$\textit{naturally-occurring}$ model change, which allows for arbitrary changes
in the parameter space such that the change in predictions on points that lie
on the data manifold is limited. Next, we propose a measure -- that we call
$\textit{Stability}$ -- to quantify the robustness of counterfactuals to
potential model changes for differentiable models, e.g., neural networks. Our
main contribution is to show that counterfactuals with sufficiently high value
of $\textit{Stability}$ as defined by our measure will remain valid after
potential $\textit{naturally-occurring}$ model changes with high probability
(leveraging concentration bounds for Lipschitz function of independent
Gaussians). Since our quantification depends on the local Lipschitz constant
around a data point which is not always available, we also examine practical
relaxations of our proposed measure and demonstrate experimentally how they can
be incorporated to find robust counterfactuals for neural networks that are
close, realistic, and remain valid after potential model changes. This work
also has interesting connections with model multiplicity, also known as, the
Rashomon effect.",2305.11997v3,https://arxiv.org/pdf/2305.11997v3
"Nonconvex Robust High-Order Tensor Completion Using Randomized Low-Rank
  Approximation","Wenjin Qin, Hailin Wang, Feng Zhang, Weijun Ma, Jianjun Wang, Tingwen Huang","Within the tensor singular value decomposition (T-SVD) framework, existing
robust low-rank tensor completion approaches have made great achievements in
various areas of science and engineering. Nevertheless, these methods involve
the T-SVD based low-rank approximation, which suffers from high computational
costs when dealing with large-scale tensor data. Moreover, most of them are
only applicable to third-order tensors. Against these issues, in this article,
two efficient low-rank tensor approximation approaches fusing randomized
techniques are first devised under the order-d (d >= 3) T-SVD framework. On
this basis, we then further investigate the robust high-order tensor completion
(RHTC) problem, in which a double nonconvex model along with its corresponding
fast optimization algorithms with convergence guarantees are developed. To the
best of our knowledge, this is the first study to incorporate the randomized
low-rank approximation into the RHTC problem. Empirical studies on large-scale
synthetic and real tensor data illustrate that the proposed method outperforms
other state-of-the-art approaches in terms of both computational efficiency and
estimated precision.",2305.11495v1,https://arxiv.org/pdf/2305.11495v1
"A Novel Tensor Factorization-Based Method with Robustness to Inaccurate
  Rank Estimation","Jingjing Zheng, Wenzhe Wang, Xiaoqin Zhang, Xianta Jiang","This study aims to solve the over-reliance on the rank estimation strategy in
the standard tensor factorization-based tensor recovery and the problem of a
large computational cost in the standard t-SVD-based tensor recovery. To this
end, we proposes a new tensor norm with a dual low-rank constraint, which
utilizes the low-rank prior and rank information at the same time. In the
proposed tensor norm, a series of surrogate functions of the tensor tubal rank
can be used to achieve better performance in harness low-rankness within tensor
data. It is proven theoretically that the resulting tensor completion model can
effectively avoid performance degradation caused by inaccurate rank estimation.
Meanwhile, attributed to the proposed dual low-rank constraint, the t-SVD of a
smaller tensor instead of the original big one is computed by using a sample
trick. Based on this, the total cost at each iteration of the optimization
algorithm is reduced to $\mathcal{O}(n^3\log n +kn^3)$ from $\mathcal{O}(n^4)$
achieved with standard methods, where $k$ is the estimation of the true tensor
rank and far less than $n$. Our method was evaluated on synthetic and
real-world data, and it demonstrated superior performance and efficiency over
several existing state-of-the-art tensor completion methods.",2305.11458v1,https://arxiv.org/pdf/2305.11458v1
"Quantifying the robustness of deep multispectral segmentation models
  against natural perturbations and data poisoning","Elise Bishoff, Charles Godfrey, Myles McKay, Eleanor Byler","In overhead image segmentation tasks, including additional spectral bands
beyond the traditional RGB channels can improve model performance. However, it
is still unclear how incorporating this additional data impacts model
robustness to adversarial attacks and natural perturbations. For adversarial
robustness, the additional information could improve the model's ability to
distinguish malicious inputs, or simply provide new attack avenues and
vulnerabilities. For natural perturbations, the additional information could
better inform model decisions and weaken perturbation effects or have no
significant influence at all. In this work, we seek to characterize the
performance and robustness of a multispectral (RGB and near infrared) image
segmentation model subjected to adversarial attacks and natural perturbations.
While existing adversarial and natural robustness research has focused
primarily on digital perturbations, we prioritize on creating realistic
perturbations designed with physical world conditions in mind. For adversarial
robustness, we focus on data poisoning attacks whereas for natural robustness,
we focus on extending ImageNet-C common corruptions for fog and snow that
coherently and self-consistently perturbs the input data. Overall, we find both
RGB and multispectral models are vulnerable to data poisoning attacks
regardless of input or fusion architectures and that while physically
realizable natural perturbations still degrade model performance, the impact
differs based on fusion architecture and input data.",2305.11347v1,https://arxiv.org/pdf/2305.11347v1
"Writing your own book: A method for going from closed to open book QA to
  improve robustness and performance of smaller LLMs","Giorgi Kokaia, Pratyush Sinha, Yutong Jiang, Nozha Boujemaa","We introduce two novel methods, Tree-Search and Self-contextualizing QA,
designed to enhance the performance of large language models (LLMs) in
question-answering tasks. Tree-Search is a sampling technique specifically
created to extract diverse information from an LLM for a given prompt.
Self-contextualizing QA leverages Tree-Search to enable the model to create its
own context using a wide range of information relevant to the prompt, evaluate
it explicitly and return a open book answer to the initial prompt . We
demonstrate that the quality of generated answers improves according to various
metrics, including accuracy, informativeness, coherence, and consistency, as
evaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods
result in increased robustness and that performance is positively correlated
with tree size, benefiting both answer quality and robustness. Finally, we
discuss other promising applications of Tree-Search, highlighting its potential
to enhance a broad range of tasks beyond question-answering.
  \noindent We also discuss several areas for future work, including refining
the Tree-Search and Self-Contextualizing QA methods, improving the coherence of
the generated context, and investigating the impact of bootstrapping on model
robustness",2305.11334v1,https://arxiv.org/pdf/2305.11334v1
"Robust Quantum Controllers: Quantum Information -- Thermodynamic Hidden
  Force Control in Intelligent Robotics based on Quantum Soft Computing","Sergey V. Ulyanov, Viktor S. Ulyanov, Takakhide Hagiwara","A generalized strategy for the design of intelligent robust control systems
based on quantum / soft computing technologies is described. The reliability of
hybrid intelligent controllers increase by providing the ability to
self-organize of imperfect knowledge bases. The main attention is paid to
increasing the level of robustness of intelligent control systems in
unpredictable control situations with the demonstration by illustrative
examples. A SW & HW platform and support tools for a supercomputer accelerator
for modeling quantum algorithms on a classical computer are described.",2305.11254v1,https://arxiv.org/pdf/2305.11254v1
"NODE-ImgNet: a PDE-informed effective and robust model for image
  denoising","Xinheng Xie, Yue Wu, Hao Ni, Cuiyu He","Inspired by the traditional partial differential equation (PDE) approach for
image denoising, we propose a novel neural network architecture, referred as
NODE-ImgNet, that combines neural ordinary differential equations (NODEs) with
convolutional neural network (CNN) blocks. NODE-ImgNet is intrinsically a PDE
model, where the dynamic system is learned implicitly without the explicit
specification of the PDE. This naturally circumvents the typical issues
associated with introducing artifacts during the learning process. By invoking
such a NODE structure, which can also be viewed as a continuous variant of a
residual network (ResNet) and inherits its advantage in image denoising, our
model achieves enhanced accuracy and parameter efficiency. In particular, our
model exhibits consistent effectiveness in different scenarios, including
denoising gray and color images perturbed by Gaussian noise, as well as
real-noisy images, and demonstrates superiority in learning from small image
datasets.",2305.11049v2,https://arxiv.org/pdf/2305.11049v2
"RobustFair: Adversarial Evaluation through Fairness Confusion Directed
  Gradient Search","Xuran Li, Peng Wu, Kaixiang Dong, Zhen Zhang, Yanting Chen","Deep neural networks (DNNs) often face challenges due to their vulnerability
to various adversarial perturbations, including false perturbations that
undermine prediction accuracy and biased perturbations that cause biased
predictions for similar inputs. This paper introduces a novel approach,
RobustFair, to evaluate the accurate fairness of DNNs when subjected to these
false or biased perturbations. RobustFair employs the notion of the fairness
confusion matrix induced in accurate fairness to identify the crucial input
features for perturbations. This matrix categorizes predictions as true fair,
true biased, false fair, and false biased, and the perturbations guided by it
can produce a dual impact on instances and their similar counterparts to either
undermine prediction accuracy (robustness) or cause biased predictions
(individual fairness). RobustFair then infers the ground truth of these
generated adversarial instances based on their loss function values
approximated by the total derivative. To leverage the generated instances for
trustworthiness improvement, RobustFair further proposes a data augmentation
strategy to prioritize adversarial instances resembling the original training
set, for data augmentation and model retraining. Notably, RobustFair excels at
detecting intertwined issues of robustness and individual fairness, which are
frequently overlooked in standard robustness and individual fairness
evaluations. This capability empowers RobustFair to enhance both robustness and
individual fairness evaluations by concurrently identifying defects in either
domain. Empirical case studies and quantile regression analyses on benchmark
datasets demonstrate the effectiveness of the fairness confusion matrix guided
perturbation for false or biased adversarial instance generation.",2305.10906v2,https://arxiv.org/pdf/2305.10906v2
Model-Free Robust Average-Reward Reinforcement Learning,"Yue Wang, Alvaro Velasquez, George Atia, Ashley Prater-Bennette, Shaofeng Zou","Robust Markov decision processes (MDPs) address the challenge of model
uncertainty by optimizing the worst-case performance over an uncertainty set of
MDPs. In this paper, we focus on the robust average-reward MDPs under the
model-free setting. We first theoretically characterize the structure of
solutions to the robust average-reward Bellman equation, which is essential for
our later convergence analysis. We then design two model-free algorithms,
robust relative value iteration (RVI) TD and robust RVI Q-learning, and
theoretically prove their convergence to the optimal solution. We provide
several widely used uncertainty sets as examples, including those defined by
the contamination model, total variation, Chi-squared divergence,
Kullback-Leibler (KL) divergence and Wasserstein distance.",2305.10504v1,https://arxiv.org/pdf/2305.10504v1
"Raising the Bar for Certified Adversarial Robustness with Diffusion
  Models","Thomas Altstidl, David Dobre, Björn Eskofier, Gauthier Gidel, Leo Schwinn","Certified defenses against adversarial attacks offer formal guarantees on the
robustness of a model, making them more reliable than empirical methods such as
adversarial training, whose effectiveness is often later reduced by unseen
attacks. Still, the limited certified robustness that is currently achievable
has been a bottleneck for their practical adoption. Gowal et al. and Wang et
al. have shown that generating additional training data using state-of-the-art
diffusion models can considerably improve the robustness of adversarial
training. In this work, we demonstrate that a similar approach can
substantially improve deterministic certified defenses. In addition, we provide
a list of recommendations to scale the robustness of certified training
approaches. One of our main insights is that the generalization gap, i.e., the
difference between the training and test accuracy of the original model, is a
good predictor of the magnitude of the robustness improvement when using
additional generated data. Our approach achieves state-of-the-art deterministic
robustness certificates on CIFAR-10 for the $\ell_2$ ($\epsilon = 36/255$) and
$\ell_\infty$ ($\epsilon = 8/255$) threat models, outperforming the previous
best results by $+3.95\%$ and $+1.39\%$, respectively. Furthermore, we report
similar improvements for CIFAR-100.",2305.10388v1,https://arxiv.org/pdf/2305.10388v1
"Logit-Based Ensemble Distribution Distillation for Robust Autoregressive
  Sequence Uncertainties","Yassir Fathullah, Guoxuan Xia, Mark Gales","Efficiently and reliably estimating uncertainty is an important objective in
deep learning. It is especially pertinent to autoregressive sequence tasks,
where training and inference costs are typically very high. However, existing
research has predominantly focused on tasks with static data such as image
classification. In this work, we investigate Ensemble Distribution Distillation
(EDD) applied to large-scale natural language sequence-to-sequence data. EDD
aims to compress the superior uncertainty performance of an expensive (teacher)
ensemble into a cheaper (student) single model. Importantly, the ability to
separate knowledge (epistemic) and data (aleatoric) uncertainty is retained.
Existing probability-space approaches to EDD, however, are difficult to scale
to large vocabularies. We show, for modern transformer architectures on
large-scale translation tasks, that modelling the ensemble logits, instead of
softmax probabilities, leads to significantly better students. Moreover, the
students surprisingly even outperform Deep Ensembles by up to ~10% AUROC on
out-of-distribution detection, whilst matching them at in-distribution
translation.",2305.10384v1,https://arxiv.org/pdf/2305.10384v1
"Towards More Robust NLP System Evaluation: Handling Missing Scores in
  Benchmarks","Anas Himmi, Ekhine Irurozki, Nathan Noiry, Stephan Clemencon, Pierre Colombo","The evaluation of natural language processing (NLP) systems is crucial for
advancing the field, but current benchmarking approaches often assume that all
systems have scores available for all tasks, which is not always practical. In
reality, several factors such as the cost of running baseline, private systems,
computational limitations, or incomplete data may prevent some systems from
being evaluated on entire tasks. This paper formalize an existing problem in
NLP research: benchmarking when some systems scores are missing on the task,
and proposes a novel approach to address it. Our method utilizes a compatible
partial ranking approach to impute missing data, which is then aggregated using
the Borda count method. It includes two refinements designed specifically for
scenarios where either task-level or instance-level scores are available. We
also introduce an extended benchmark, which contains over 131 million scores,
an order of magnitude larger than existing benchmarks. We validate our methods
and demonstrate their effectiveness in addressing the challenge of missing
system evaluation on an entire task. This work highlights the need for more
comprehensive benchmarking approaches that can handle real-world scenarios
where not all systems are evaluated on the entire task.",2305.10284v1,https://arxiv.org/pdf/2305.10284v1
"Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace
  Distribution","Yingda Yin, Jiangran Lyu, Yang Wang, He Wang, Baoquan Chen","Estimating the 3DoF rotation from a single RGB image is an important yet
challenging problem. As a popular approach, probabilistic rotation modeling
additionally carries prediction uncertainty information, compared to
single-prediction rotation regression. For modeling probabilistic distribution
over SO(3), it is natural to use Gaussian-like Bingham distribution and matrix
Fisher, however they are shown to be sensitive to outlier predictions, e.g.
$180^\circ$ error and thus are unlikely to converge with optimal performance.
In this paper, we draw inspiration from multivariate Laplace distribution and
propose a novel rotation Laplace distribution on SO(3). Our rotation Laplace
distribution is robust to the disturbance of outliers and enforces much
gradient to the low-error region that it can improve. In addition, we show that
our method also exhibits robustness to small noises and thus tolerates
imperfect annotations. With this benefit, we demonstrate its advantages in
semi-supervised rotation regression, where the pseudo labels are noisy. To
further capture the multi-modal rotation solution space for symmetric objects,
we extend our distribution to rotation Laplace mixture model and demonstrate
its effectiveness. Our extensive experiments show that our proposed
distribution and the mixture model achieve state-of-the-art performance in all
the rotation regression experiments over both probabilistic and
non-probabilistic baselines.",2305.10465v1,https://arxiv.org/pdf/2305.10465v1
"A robust multi-domain network for short-scanning amyloid PET
  reconstruction","Hyoung Suk Park, Young Jin Jeong, Kiwan Jeon","This paper presents a robust multi-domain network designed to restore
low-quality amyloid PET images acquired in a short period of time. The proposed
method is trained on pairs of PET images from short (2 minutes) and standard
(20 minutes) scanning times, sourced from multiple domains. Learning relevant
image features between these domains with a single network is challenging. Our
key contribution is the introduction of a mapping label, which enables
effective learning of specific representations between different domains. The
network, trained with various mapping labels, can efficiently correct amyloid
PET datasets in multiple training domains and unseen domains, such as those
obtained with new radiotracers, acquisition protocols, or PET scanners.
Internal, temporal, and external validations demonstrate the effectiveness of
the proposed method. Notably, for external validation datasets from unseen
domains, the proposed method achieved comparable or superior results relative
to methods trained with these datasets, in terms of quantitative metrics such
as normalized root mean-square error and structure similarity index measure.
Two nuclear medicine physicians evaluated the amyloid status as positive or
negative for the external validation datasets, with accuracies of 0.970 and
0.930 for readers 1 and 2, respectively.",2305.09986v1,https://arxiv.org/pdf/2305.09986v1
"Double Pessimism is Provably Efficient for Distributionally Robust
  Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage","Jose Blanchet, Miao Lu, Tong Zhang, Han Zhong","In this paper, we study distributionally robust offline reinforcement
learning (robust offline RL), which seeks to find an optimal policy purely from
an offline dataset that can perform well in perturbed environments. In
specific, we propose a generic algorithm framework called Doubly Pessimistic
Model-based Policy Optimization ($P^2MPO$), which features a novel combination
of a flexible model estimation subroutine and a doubly pessimistic policy
optimization step. Notably, the double pessimism principle is crucial to
overcome the distributional shifts incurred by (i) the mismatch between the
behavior policy and the target policies; and (ii) the perturbation of the
nominal model. Under certain accuracy conditions on the model estimation
subroutine, we prove that $P^2MPO$ is sample-efficient with robust partial
coverage data, which only requires the offline data to have good coverage of
the distributions induced by the optimal robust policy and the perturbed models
around the nominal model.
  By tailoring specific model estimation subroutines for concrete examples of
RMDPs, including tabular RMDPs, factored RMDPs, kernel and neural RMDPs, we
prove that $P^2MPO$ enjoys a $\tilde{\mathcal{O}}(n^{-1/2})$ convergence rate,
where $n$ is the dataset size. We highlight that all these examples, except
tabular RMDPs, are first identified and proven tractable by this work.
Furthermore, we continue our study of robust offline RL in the robust Markov
games (RMGs). By extending the double pessimism principle identified for
single-agent RMDPs, we propose another algorithm framework that can efficiently
find the robust Nash equilibria among players using only robust unilateral
(partial) coverage data. To our best knowledge, this work proposes the first
general learning principle -- double pessimism -- for robust offline RL and
shows that it is provably efficient with general function approximation.",2305.09659v2,https://arxiv.org/pdf/2305.09659v2
Noise robust neural network architecture,"Xiong Yunuo, Xiong Hongwei","In which we propose neural network architecture (dune neural network) for
recognizing general noisy image without adding any artificial noise in the
training data. By representing each free parameter of the network as an
uncertainty interval, and applying a linear transformation to each input
element, we show that the resulting architecture achieves decent noise
robustness when faced with input data with white noise. We apply simple dune
neural networks for MNIST dataset and demonstrate that even for very noisy
input images which are hard for human to recognize, our approach achieved
better test set accuracy than human without dataset augmentation. We also find
that our method is robust for many other examples with various background
patterns added.",2305.09276v1,https://arxiv.org/pdf/2305.09276v1
"Ortho-ODE: Enhancing Robustness and of Neural ODEs against Adversarial
  Attacks",Vishal Purohit,"Neural Ordinary Differential Equations (NODEs) probed the usage of numerical
solvers to solve the differential equation characterized by a Neural Network
(NN), therefore initiating a new paradigm of deep learning models with infinite
depth. NODEs were designed to tackle the irregular time series problem.
However, NODEs have demonstrated robustness against various noises and
adversarial attacks. This paper is about the natural robustness of NODEs and
examines the cause behind such surprising behaviour. We show that by
controlling the Lipschitz constant of the ODE dynamics the robustness can be
significantly improved. We derive our approach from Grownwall's inequality.
Further, we draw parallels between contractivity theory and Grownwall's
inequality. Experimentally we corroborate the enhanced robustness on numerous
datasets - MNIST, CIFAR-10, and CIFAR 100. We also present the impact of
adaptive and non-adaptive solvers on the robustness of NODEs.",2305.09179v1,https://arxiv.org/pdf/2305.09179v1
Scalable and Robust Tensor Ring Decomposition for Large-scale Data,"Yicong He, George K. Atia","Tensor ring (TR) decomposition has recently received increased attention due
to its superior expressive performance for high-order tensors. However, the
applicability of traditional TR decomposition algorithms to real-world
applications is hindered by prevalent large data sizes, missing entries, and
corruption with outliers. In this work, we propose a scalable and robust TR
decomposition algorithm capable of handling large-scale tensor data with
missing entries and gross corruptions. We first develop a novel auto-weighted
steepest descent method that can adaptively fill the missing entries and
identify the outliers during the decomposition process. Further, taking
advantage of the tensor ring model, we develop a novel fast Gram matrix
computation (FGMC) approach and a randomized subtensor sketching (RStS)
strategy which yield significant reduction in storage and computational
complexity. Experimental results demonstrate that the proposed method
outperforms existing TR decomposition methods in the presence of outliers, and
runs significantly faster than existing robust tensor completion algorithms.",2305.09044v1,https://arxiv.org/pdf/2305.09044v1
Causal Analysis for Robust Interpretability of Neural Networks,"Ola Ahmad, Nicolas Bereux, Loïc Baret, Vahid Hashemi, Freddy Lecue","Interpreting the inner function of neural networks is crucial for the
trustworthy development and deployment of these black-box models. Prior
interpretability methods focus on correlation-based measures to attribute model
decisions to individual examples. However, these measures are susceptible to
noise and spurious correlations encoded in the model during the training phase
(e.g., biased inputs, model overfitting, or misspecification). Moreover, this
process has proven to result in noisy and unstable attributions that prevent
any transparent understanding of the model's behavior. In this paper, we
develop a robust interventional-based method grounded by causal analysis to
capture cause-effect mechanisms in pre-trained neural networks and their
relation to the prediction. Our novel approach relies on path interventions to
infer the causal mechanisms within hidden layers and isolate relevant and
necessary information (to model prediction), avoiding noisy ones. The result is
task-specific causal explanatory graphs that can audit model behavior and
express the actual causes underlying its performance. We apply our method to
vision models trained on classification tasks. On image classification tasks,
we provide extensive quantitative experiments to show that our approach can
capture more stable and faithful explanations than standard attribution-based
methods. Furthermore, the underlying causal graphs reveal the neural
interactions in the model, making it a valuable tool in other applications
(e.g., model repair).",2305.08950v2,https://arxiv.org/pdf/2305.08950v2
"Assessing Hidden Risks of LLMs: An Empirical Study on Robustness,
  Consistency, and Credibility","Wentao Ye, Mingfeng Ou, Tianyi Li, Yipeng chen, Xuetao Ma, Yifan Yanggong, Sai Wu, Jie Fu, Gang Chen, Haobo Wang, Junbo Zhao","The recent popularity of large language models (LLMs) has brought a
significant impact to boundless fields, particularly through their open-ended
ecosystem such as the APIs, open-sourced models, and plugins. However, with
their widespread deployment, there is a general lack of research that
thoroughly discusses and analyzes the potential risks concealed. In that case,
we intend to conduct a preliminary but pioneering study covering the
robustness, consistency, and credibility of LLMs systems. With most of the
related literature in the era of LLM uncharted, we propose an automated
workflow that copes with an upscaled number of queries/responses. Overall, we
conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA,
and OPT. Core to our workflow consists of a data primitive, followed by an
automated interpreter that evaluates these LLMs under different adversarial
metrical systems. As a result, we draw several, and perhaps unfortunate,
conclusions that are quite uncommon from this trendy community. Briefly, they
are: (i)-the minor but inevitable error occurrence in the user-generated query
input may, by chance, cause the LLM to respond unexpectedly; (ii)-LLMs possess
poor consistency when processing semantically similar query input. In addition,
as a side finding, we find that ChatGPT is still capable to yield the correct
answer even when the input is polluted at an extreme level. While this
phenomenon demonstrates the powerful memorization of the LLMs, it raises
serious concerns about using such data for LLM-involved evaluation in academic
development. To deal with it, we propose a novel index associated with a
dataset that roughly decides the feasibility of using such data for
LLM-involved evaluation. Extensive empirical studies are tagged to support the
aforementioned claims.",2305.10235v4,https://arxiv.org/pdf/2305.10235v4
"Sensitivity and Robustness of Large Language Models to Prompt Template
  in Japanese Text Classification Tasks","Chengguang Gan, Tatsunori Mori","Prompt engineering relevance research has seen a notable surge in recent
years, primarily driven by advancements in pre-trained language models and
large language models. However, a critical issue has been identified within
this domain: the inadequate of sensitivity and robustness of these models
towards Prompt Templates, particularly in lesser-studied languages such as
Japanese. This paper explores this issue through a comprehensive evaluation of
several representative Large Language Models (LLMs) and a widely-utilized
pre-trained model(PLM). These models are scrutinized using a benchmark dataset
in Japanese, with the aim to assess and analyze the performance of the current
multilingual models in this context. Our experimental results reveal startling
discrepancies. A simple modification in the sentence structure of the Prompt
Template led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44.
This observation underscores the fact that even the highly performance GPT-4
model encounters significant stability issues when dealing with diverse
Japanese prompt templates, rendering the consistency of the model's output
results questionable. In light of these findings, we conclude by proposing
potential research trajectories to further enhance the development and
performance of Large Language Models in their current stage.",2305.08714v2,https://arxiv.org/pdf/2305.08714v2
"Beqi: Revitalize the Senegalese Wolof Language with a Robust Spelling
  Corrector","Derguene Mbaye, Moussa Diallo","The progress of Natural Language Processing (NLP), although fast in recent
years, is not at the same pace for all languages. African languages in
particular are still behind and lack automatic processing tools. Some of these
tools are very important for the development of these languages but also have
an important role in many NLP applications. This is particularly the case for
automatic spell checkers. Several approaches have been studied to address this
task and the one modeling spelling correction as a translation task from
misspelled (noisy) text to well-spelled (correct) text shows promising results.
However, this approach requires a parallel corpus of noisy data on the one hand
and correct data on the other hand, whereas Wolof is a low-resource language
and does not have such a corpus. In this paper, we present a way to address the
constraint related to the lack of data by generating synthetic data and we
present sequence-to-sequence models using Deep Learning for spelling correction
in Wolof. We evaluated these models in three different scenarios depending on
the subwording method applied to the data and showed that the latter had a
significant impact on the performance of the models, which opens the way for
future research in Wolof spelling correction.",2305.08518v1,https://arxiv.org/pdf/2305.08518v1
Label Smoothing is Robustification against Model Misspecification,"Ryoya Yamasaki, Toshiyuki Tanaka","Label smoothing (LS) adopts smoothed targets in classification tasks. For
example, in binary classification, instead of the one-hot target $(1,0)^\top$
used in conventional logistic regression (LR), LR with LS (LSLR) uses the
smoothed target $(1-\frac{\alpha}{2},\frac{\alpha}{2})^\top$ with a smoothing
level $\alpha\in(0,1)$, which causes squeezing of values of the logit. Apart
from the common regularization-based interpretation of LS that leads to an
inconsistent probability estimator, we regard LSLR as modifying the loss
function and consistent estimator for probability estimation. In order to study
the significance of each of these two modifications by LSLR, we introduce a
modified LSLR (MLSLR) that uses the same loss function as LSLR and the same
consistent estimator as LR, while not squeezing the logits. For the loss
function modification, we theoretically show that MLSLR with a larger smoothing
level has lower efficiency with correctly-specified models, while it exhibits
higher robustness against model misspecification than LR. Also, for the
modification of the probability estimator, an experimental comparison between
LSLR and MLSLR showed that this modification and squeezing of the logits in
LSLR have negative effects on the probability estimation and classification
performance. The understanding of the properties of LS provided by these
comparisons allows us to propose MLSLR as an improvement over LSLR.",2305.08501v1,https://arxiv.org/pdf/2305.08501v1
t-RAIN: Robust generalization under weather-aliasing label shift attacks,"Aboli Marathe, Sanjana Prabhu","In the classical supervised learning settings, classifiers are fit with the
assumption of balanced label distributions and produce remarkable results on
the same. In the real world, however, these assumptions often bend and in turn
adversely impact model performance. Identifying bad learners in skewed target
distributions is even more challenging. Thus achieving model robustness under
these ""label shift"" settings is an important task in autonomous perception. In
this paper, we analyze the impact of label shift on the task of multi-weather
classification for autonomous vehicles. We use this information as a prior to
better assess pedestrian detection in adverse weather. We model the
classification performance as an indicator of robustness under 4 label shift
scenarios and study the behavior of multiple classes of models. We propose
t-RAIN a similarity mapping technique for synthetic data augmentation using
large scale generative models and evaluate the performance on DAWN dataset.
This mapping boosts model test accuracy by 2.1, 4.4, 1.9, 2.7 % in no-shift,
fog, snow, dust shifts respectively. We present state-of-the-art pedestrian
detection results on real and synthetic weather domains with best performing
82.69 AP (snow) and 62.31 AP (fog) respectively.",2305.08302v1,https://arxiv.org/pdf/2305.08302v1
On enhancing the robustness of Vision Transformers: Defensive Diffusion,"Raza Imam, Muhammad Huzaifa, Mohammed El-Amine Azz","Privacy and confidentiality of medical data are of utmost importance in
healthcare settings. ViTs, the SOTA vision model, rely on large amounts of
patient data for training, which raises concerns about data security and the
potential for unauthorized access. Adversaries may exploit vulnerabilities in
ViTs to extract sensitive patient information and compromising patient privacy.
This work address these vulnerabilities to ensure the trustworthiness and
reliability of ViTs in medical applications. In this work, we introduced a
defensive diffusion technique as an adversarial purifier to eliminate
adversarial noise introduced by attackers in the original image. By utilizing
the denoising capabilities of the diffusion model, we employ a reverse
diffusion process to effectively eliminate the adversarial noise from the
attack sample, resulting in a cleaner image that is then fed into the ViT
blocks. Our findings demonstrate the effectiveness of the diffusion model in
eliminating attack-agnostic adversarial noise from images. Additionally, we
propose combining knowledge distillation with our framework to obtain a
lightweight student model that is both computationally efficient and robust
against gray box attacks. Comparison of our method with a SOTA baseline method,
SEViT, shows that our work is able to outperform the baseline. Extensive
experiments conducted on a publicly available Tuberculosis X-ray dataset
validate the computational efficiency and improved robustness achieved by our
proposed architecture.",2305.08031v1,https://arxiv.org/pdf/2305.08031v1
SPP-CNN: An Efficient Framework for Network Robustness Prediction,"Chengpei Wu, Yang Lou, Lin Wang, Junli Li, Xiang Li, Guanrong Chen","This paper addresses the robustness of a network to sustain its connectivity
and controllability against malicious attacks. This kind of network robustness
is typically measured by the time-consuming attack simulation, which returns a
sequence of values that record the remaining connectivity and controllability
after a sequence of node- or edge-removal attacks. For improvement, this paper
develops an efficient framework for network robustness prediction, the spatial
pyramid pooling convolutional neural network (SPP-CNN). The new framework
installs a spatial pyramid pooling layer between the convolutional and
fully-connected layers, overcoming the common mismatch issue in the CNN-based
prediction approaches and extending its generalizability. Extensive experiments
are carried out by comparing SPP-CNN with three state-of-the-art robustness
predictors, namely a CNN-based and two graph neural networks-based frameworks.
Synthetic and real-world networks, both directed and undirected, are
investigated. Experimental results demonstrate that the proposed SPP-CNN
achieves better prediction performances and better generalizability to unknown
datasets, with significantly lower time-consumption, than its counterparts.",2305.07872v1,https://arxiv.org/pdf/2305.07872v1
"Device-Robust Acoustic Scene Classification via Impulse Response
  Augmentation","Tobias Morocutti, Florian Schmid, Khaled Koutini, Gerhard Widmer","The ability to generalize to a wide range of recording devices is a crucial
performance factor for audio classification models. The characteristics of
different types of microphones introduce distributional shifts in the digitized
audio signals due to their varying frequency responses. If this domain shift is
not taken into account during training, the model's performance could degrade
severely when it is applied to signals recorded by unseen devices. In
particular, training a model on audio signals recorded with a small number of
different microphones can make generalization to unseen devices difficult. To
tackle this problem, we convolve audio signals in the training set with
pre-recorded device impulse responses (DIRs) to artificially increase the
diversity of recording devices. We systematically study the effect of DIR
augmentation on the task of Acoustic Scene Classification using CNNs and Audio
Spectrogram Transformers. The results show that DIR augmentation in isolation
performs similarly to the state-of-the-art method Freq-MixStyle. However, we
also show that DIR augmentation and Freq-MixStyle are complementary, achieving
a new state-of-the-art performance on signals recorded by devices unseen during
training.",2305.07499v2,https://arxiv.org/pdf/2305.07499v2
Gallery Sampling for Robust and Fast Face Identification,"Myung-cheol Roh, Pyoung-gang Lim, Jongju Shin","Deep learning methods have been achieved brilliant results in face
recognition. One of the important tasks to improve the performance is to
collect and label images as many as possible. However, labeling identities and
checking qualities of large image data are difficult task and mistakes cannot
be avoided in processing large data. Previous works have been trying to deal
with the problem only in training domain, however it can cause much serious
problem if the mistakes are in gallery data of face identification. We proposed
gallery data sampling methods which are robust to outliers including wrong
labeled, low quality, and less-informative images and reduce searching time.
The proposed sampling-by-pruning and sampling-by-generating methods
significantly improved face identification performance on our 5.4M web image
dataset of celebrities. The proposed method achieved 0.0975 in terms of FNIR at
FPIR=0.01, while conventional method showed 0.3891. The average number of
feature vectors for each individual gallery was reduced to 17.1 from 115.9 and
it can provide much faster search. We also made experiments on public datasets
and our method achieved 0.1314 and 0.0668 FNIRs at FPIR=0.01 on the
CASIA-WebFace and MS1MV2, while the convectional method did 0.5446, and 0.1327,
respectively.",2305.07495v1,https://arxiv.org/pdf/2305.07495v1
"Parameterized Approximation for Robust Clustering in Discrete Geometric
  Spaces","Fateme Abbasi, Sandip Banerjee, Jarosław Byrka, Parinya Chalermsook, Ameet Gadekar, Kamyar Khodamoradi, Dániel Marx, Roohani Sharma, Joachim Spoerhase","We consider the well-studied Robust $(k, z)$-Clustering problem, which
generalizes the classic $k$-Median, $k$-Means, and $k$-Center problems. Given a
constant $z\ge 1$, the input to Robust $(k, z)$-Clustering is a set $P$ of $n$
weighted points in a metric space $(M,\delta)$ and a positive integer $k$.
Further, each point belongs to one (or more) of the $m$ many different groups
$S_1,S_2,\ldots,S_m$. Our goal is to find a set $X$ of $k$ centers such that
$\max_{i \in [m]} \sum_{p \in S_i} w(p) \delta(p,X)^z$ is minimized.
  This problem arises in the domains of robust optimization [Anthony, Goyal,
Gupta, Nagarajan, Math. Oper. Res. 2010] and in algorithmic fairness. For
polynomial time computation, an approximation factor of $O(\log m/\log\log m)$
is known [Makarychev, Vakilian, COLT $2021$], which is tight under a plausible
complexity assumption even in the line metrics. For FPT time, there is a
$(3^z+\epsilon)$-approximation algorithm, which is tight under GAP-ETH [Goyal,
Jaiswal, Inf. Proc. Letters, 2023].
  Motivated by the tight lower bounds for general discrete metrics, we focus on
\emph{geometric} spaces such as the (discrete) high-dimensional Euclidean
setting and metrics of low doubling dimension, which play an important role in
data analysis applications. First, for a universal constant $\eta_0 >0.0006$,
we devise a $3^z(1-\eta_{0})$-factor FPT approximation algorithm for discrete
high-dimensional Euclidean spaces thereby bypassing the lower bound for general
metrics. We complement this result by showing that even the special case of
$k$-Center in dimension $\Theta(\log n)$ is $(\sqrt{3/2}- o(1))$-hard to
approximate for FPT algorithms. Finally, we complete the FPT approximation
landscape by designing an FPT $(1+\epsilon)$-approximation scheme (EPAS) for
the metric of sub-logarithmic doubling dimension.",2305.07316v1,https://arxiv.org/pdf/2305.07316v1
Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models,"Yichi Zhang, Mihai Cucuringu, Alexander Y. Shestopaloff, Stefan Zohren","In multivariate time series systems, key insights can be obtained by
discovering lead-lag relationships inherent in the data, which refer to the
dependence between two time series shifted in time relative to one another, and
which can be leveraged for the purposes of control, forecasting or clustering.
We develop a clustering-driven methodology for robust detection of lead-lag
relationships in lagged multi-factor models. Within our framework, the
envisioned pipeline takes as input a set of time series, and creates an
enlarged universe of extracted subsequence time series from each input time
series, via a sliding window approach. This is then followed by an application
of various clustering techniques, (such as k-means++ and spectral clustering),
employing a variety of pairwise similarity measures, including nonlinear ones.
Once the clusters have been extracted, lead-lag estimates across clusters are
robustly aggregated to enhance the identification of the consistent
relationships in the original universe. We establish connections to the
multireference alignment problem for both the homogeneous and heterogeneous
settings. Since multivariate time series are ubiquitous in a wide range of
domains, we demonstrate that our method is not only able to robustly detect
lead-lag relationships in financial markets, but can also yield insightful
results when applied to an environmental data set.",2305.06704v3,https://arxiv.org/pdf/2305.06704v3
"On Practical Robust Reinforcement Learning: Practical Uncertainty Set
  and Double-Agent Algorithm","Ukjo Hwang, Songnam Hong","Robust reinforcement learning (RRL) aims at seeking a robust policy to
optimize the worst case performance over an uncertainty set of Markov decision
processes (MDPs). This set contains some perturbed MDPs from a nominal MDP
(N-MDP) that generate samples for training, which reflects some potential
mismatches between training (i.e., N-MDP) and true environments. In this paper
we present an elaborated uncertainty set by excluding some implausible MDPs
from the existing sets. Under this uncertainty set, we develop a sample-based
RRL algorithm (named ARQ-Learning) for tabular setting and characterize its
finite-time error bound. Also, it is proved that ARQ-Learning converges as fast
as the standard Q-Learning and robust Q-Learning while ensuring better
robustness. We introduce an additional pessimistic agent which can tackle the
major bottleneck for the extension of ARQ-Learning into the cases with larger
or continuous state spaces. Incorporating this idea into RL algorithms, we
propose double-agent algorithms for model-free RRL. Via experiments, we
demonstrate the effectiveness of the proposed algorithms.",2305.06657v3,https://arxiv.org/pdf/2305.06657v3
"PPGenCDR: A Stable and Robust Framework for Privacy-Preserving
  Cross-Domain Recommendation","Xinting Liao, Weiming Liu, Xiaolin Zheng, Binhui Yao, Chaochao Chen","Privacy-preserving cross-domain recommendation (PPCDR) refers to preserving
the privacy of users when transferring the knowledge from source domain to
target domain for better performance, which is vital for the long-term
development of recommender systems. Existing work on cross-domain
recommendation (CDR) reaches advanced and satisfying recommendation
performance, but mostly neglects preserving privacy. To fill this gap, we
propose a privacy-preserving generative cross-domain recommendation (PPGenCDR)
framework for PPCDR. PPGenCDR includes two main modules, i.e., stable
privacy-preserving generator module, and robust cross-domain recommendation
module. Specifically, the former isolates data from different domains with a
generative adversarial network (GAN) based model, which stably estimates the
distribution of private data in the source domain with Renyi differential
privacy (RDP) technique. Then the latter aims to robustly leverage the
perturbed but effective knowledge from the source domain with the raw data in
target domain to improve recommendation performance. Three key modules, i.e.,
(1) selective privacy preserver, (2) GAN stabilizer, and (3) robustness
conductor, guarantee the cost-effective trade-off between utility and privacy,
the stability of GAN when using RDP, and the robustness of leveraging
transferable knowledge accordingly. The extensive empirical studies on Douban
and Amazon datasets demonstrate that PPGenCDR significantly outperforms the
state-of-the-art recommendation models while preserving privacy.",2305.16163v1,https://arxiv.org/pdf/2305.16163v1
"Randomized Smoothing with Masked Inference for Adversarially Robust Text
  Classifications","Han Cheol Moon, Shafiq Joty, Ruochen Zhao, Megh Thakkar, Xu Chi","Large-scale pre-trained language models have shown outstanding performance in
a variety of NLP tasks. However, they are also known to be significantly
brittle against specifically crafted adversarial examples, leading to
increasing interest in probing the adversarial robustness of NLP systems. We
introduce RSMI, a novel two-stage framework that combines randomized smoothing
(RS) with masked inference (MI) to improve the adversarial robustness of NLP
systems. RS transforms a classifier into a smoothed classifier to obtain robust
representations, whereas MI forces a model to exploit the surrounding context
of a masked token in an input sequence. RSMI improves adversarial robustness by
2 to 3 times over existing state-of-the-art methods on benchmark datasets. We
also perform in-depth qualitative analysis to validate the effectiveness of the
different stages of RSMI and probe the impact of its components through
extensive ablations. By empirically proving the stability of RSMI, we put it
forward as a practical method to robustly train large-scale NLP models. Our
code and datasets are available at https://github.com/Han8931/rsmi_nlp",2305.06522v1,https://arxiv.org/pdf/2305.06522v1
"Robust multi-agent coordination via evolutionary generation of auxiliary
  adversarial attackers","Lei Yuan, Zi-Qian Zhang, Ke Xue, Hao Yin, Feng Chen, Cong Guan, Li-He Li, Chao Qian, Yang Yu","Cooperative multi-agent reinforcement learning (CMARL) has shown to be
promising for many real-world applications. Previous works mainly focus on
improving coordination ability via solving MARL-specific challenges (e.g.,
non-stationarity, credit assignment, scalability), but ignore the policy
perturbation issue when testing in a different environment. This issue hasn't
been considered in problem formulation or efficient algorithm design. To
address this issue, we firstly model the problem as a limited policy adversary
Dec-POMDP (LPA-Dec-POMDP), where some coordinators from a team might
accidentally and unpredictably encounter a limited number of malicious action
attacks, but the regular coordinators still strive for the intended goal. Then,
we propose Robust Multi-Agent Coordination via Evolutionary Generation of
Auxiliary Adversarial Attackers (ROMANCE), which enables the trained policy to
encounter diversified and strong auxiliary adversarial attacks during training,
thus achieving high robustness under various policy perturbations. Concretely,
to avoid the ego-system overfitting to a specific attacker, we maintain a set
of attackers, which is optimized to guarantee the attackers high attacking
quality and behavior diversity. The goal of quality is to minimize the
ego-system coordination effect, and a novel diversity regularizer based on
sparse action is applied to diversify the behaviors among attackers. The
ego-system is then paired with a population of attackers selected from the
maintained attacker set, and alternately trained against the constantly
evolving attackers. Extensive experiments on multiple scenarios from SMAC
indicate our ROMANCE provides comparable or better robustness and
generalization ability than other baselines.",2305.05909v1,https://arxiv.org/pdf/2305.05909v1
"Causal Information Splitting: Engineering Proxy Features for Robustness
  to Distribution Shifts","Bijan Mazaheri, Atalanti Mastakouri, Dominik Janzing, Michaela Hardt","Statistical prediction models are often trained on data from different
probability distributions than their eventual use cases. One approach to
proactively prepare for these shifts harnesses the intuition that causal
mechanisms should remain invariant between environments. Here we focus on a
challenging setting in which the causal and anticausal variables of the target
are unobserved. Leaning on information theory, we develop feature selection and
engineering techniques for the observed downstream variables that act as
proxies. We identify proxies that help to build stable models and moreover
utilize auxiliary training tasks to answer counterfactual questions that
extract stability-enhancing information from proxies. We demonstrate the
effectiveness of our techniques on synthetic and real data.",2305.05832v3,https://arxiv.org/pdf/2305.05832v3
Ranking & Reweighting Improves Group Distributional Robustness,"Yachuan Liu, Bohan Zhang, Qiaozhu Mei, Paramveer Dhillon","Recent work has shown that standard training via empirical risk minimization
(ERM) can produce models that achieve high accuracy on average but low accuracy
on underrepresented groups due to the prevalence of spurious features. A
predominant approach to tackle this group robustness problem minimizes the
worst group error (akin to a minimax strategy) on the training data, hoping it
will generalize well on the testing data. However, this is often suboptimal,
especially when the out-of-distribution (OOD) test data contains previously
unseen groups. Inspired by ideas from the information retrieval and
learning-to-rank literature, this paper first proposes to use Discounted
Cumulative Gain (DCG) as a metric of model quality for facilitating better
hyperparameter tuning and model selection. Being a ranking-based metric, DCG
weights multiple poorly-performing groups (instead of considering just the
group with the worst performance). As a natural next step, we build on our
results to propose a ranking-based training method called Discounted Rank
Upweighting (DRU), which differentially reweights a ranked list of
poorly-performing groups in the training data to learn models that exhibit
strong OOD performance on the test data. Results on several synthetic and
real-world datasets highlight the superior generalization ability of our
group-ranking-based (akin to soft-minimax) approach in selecting and learning
models that are robust to group distributional shifts.",2305.05759v1,https://arxiv.org/pdf/2305.05759v1
Robust Implicit Regularization via Weight Normalization,"Hung-Hsu Chou, Holger Rauhut, Rachel Ward","Overparameterized models may have many interpolating solutions; implicit
regularization refers to the hidden preference of a particular optimization
method towards a certain interpolating solution among the many. A by now
established line of work has shown that (stochastic) gradient descent tends to
have an implicit bias towards low rank and/or sparse solutions when used to
train deep linear networks, explaining to some extent why overparameterized
neural network models trained by gradient descent tend to have good
generalization performance in practice.However, existing theory for square-loss
objectives often requires very small initialization of the trainable weights,
which is at odds with the larger scale at which weights are initialized in
practice for faster convergence and better generalization performance. In this
paper, we aim to close this gap by incorporating and analyzing gradient flow
(continuous-time version of gradient descent) with weight normalization, where
the weight vector is reparameterized in terms of polar coordinates, and
gradient flow is applied to the polar coordinates. By analyzing key invariants
of the gradient flow and using Lojasiewicz Theorem, we show that weight
normalization also has an implicit bias towards sparse solutions in the
diagonal linear model, but that in contrast to plain gradient flow, weight
normalization enables a robust bias that persists even if the weights are
initialized at practically large scale. Experiments suggest that the gains in
both convergence speed and robustness of the implicit bias are improved
dramatically by using weight normalization in overparameterized diagonal linear
network models.",2305.05448v3,https://arxiv.org/pdf/2305.05448v3
"Investigating the Corruption Robustness of Image Classifiers with Random
  Lp-norm Corruptions","Georg Siedel, Weijia Shao, Silvia Vock, Andrey Morozov","Robustness is a fundamental property of machine learning classifiers required
to achieve safety and reliability. In the field of adversarial robustness of
image classifiers, robustness is commonly defined as the stability of a model
to all input changes within a p-norm distance. However, in the field of random
corruption robustness, variations observed in the real world are used, while
p-norm corruptions are rarely considered. This study investigates the use of
random p-norm corruptions to augment the training and test data of image
classifiers. We evaluate the model robustness against imperceptible random
p-norm corruptions and propose a novel robustness metric. We empirically
investigate whether robustness transfers across different p-norms and derive
conclusions on which p-norm corruptions a model should be trained and
evaluated. We find that training data augmentation with a combination of p-norm
corruptions significantly improves corruption robustness, even on top of
state-of-the-art data augmentation schemes.",2305.05400v4,https://arxiv.org/pdf/2305.05400v4
Sharpness-Aware Minimization Alone can Improve Adversarial Robustness,"Zeming Wei, Jingyu Zhu, Yihao Zhang","Sharpness-Aware Minimization (SAM) is an effective method for improving
generalization ability by regularizing loss sharpness. In this paper, we
explore SAM in the context of adversarial robustness. We find that using only
SAM can achieve superior adversarial robustness without sacrificing clean
accuracy compared to standard training, which is an unexpected benefit. We also
discuss the relation between SAM and adversarial training (AT), a popular
method for improving the adversarial robustness of DNNs. In particular, we show
that SAM and AT differ in terms of perturbation strength, leading to different
accuracy and robustness trade-offs. We provide theoretical evidence for these
claims in a simplified model. Finally, while AT suffers from decreased clean
accuracy and computational overhead, we suggest that SAM can be regarded as a
lightweight substitute for AT under certain requirements. Code is available at
https://github.com/weizeming/SAM_AT.",2305.05392v2,https://arxiv.org/pdf/2305.05392v2
"Robust Acoustic and Semantic Contextual Biasing in Neural Transducers
  for Speech Recognition","Xuandi Fu, Kanthashree Mysore Sathyendra, Ankur Gandhe, Jing Liu, Grant P. Strimel, Ross McGowan, Athanasios Mouchtaris","Attention-based contextual biasing approaches have shown significant
improvements in the recognition of generic and/or personal rare-words in
End-to-End Automatic Speech Recognition (E2E ASR) systems like neural
transducers. These approaches employ cross-attention to bias the model towards
specific contextual entities injected as bias-phrases to the model. Prior
approaches typically relied on subword encoders for encoding the bias phrases.
However, subword tokenizations are coarse and fail to capture granular
pronunciation information which is crucial for biasing based on acoustic
similarity. In this work, we propose to use lightweight character
representations to encode fine-grained pronunciation features to improve
contextual biasing guided by acoustic similarity between the audio and the
contextual entities (termed acoustic biasing). We further integrate pretrained
neural language model (NLM) based encoders to encode the utterance's semantic
context along with contextual entities to perform biasing informed by the
utterance's semantic context (termed semantic biasing). Experiments using a
Conformer Transducer model on the Librispeech dataset show a 4.62% - 9.26%
relative WER improvement on different biasing list sizes over the baseline
contextual model when incorporating our proposed acoustic and semantic biasing
approach. On a large-scale in-house dataset, we observe 7.91% relative WER
improvement compared to our baseline model. On tail utterances, the
improvements are even more pronounced with 36.80% and 23.40% relative WER
improvements on Librispeech rare words and an in-house testset respectively.",2305.05271v1,https://arxiv.org/pdf/2305.05271v1
"FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class
  Imbalance and Label Noise Heterogeneity","Nannan Wu, Li Yu, Xuefeng Jiang, Kwang-Ting Cheng, Zengqiang Yan","Federated noisy label learning (FNLL) is emerging as a promising tool for
privacy-preserving multi-source decentralized learning. Existing research,
relying on the assumption of class-balanced global data, might be incapable to
model complicated label noise, especially in medical scenarios. In this paper,
we first formulate a new and more realistic federated label noise problem where
global data is class-imbalanced and label noise is heterogeneous, and then
propose a two-stage framework named FedNoRo for noise-robust federated
learning. Specifically, in the first stage of FedNoRo, per-class loss
indicators followed by Gaussian Mixture Model are deployed for noisy client
identification. In the second stage, knowledge distillation and a
distance-aware aggregation function are jointly adopted for noise-robust
federated model updating. Experimental results on the widely-used ICH and
ISIC2019 datasets demonstrate the superiority of FedNoRo against the
state-of-the-art FNLL methods for addressing class imbalance and label noise
heterogeneity in real-world FL scenarios.",2305.05230v2,https://arxiv.org/pdf/2305.05230v2
"Communication-Robust Multi-Agent Learning by Adaptable Auxiliary
  Multi-Agent Adversary Generation","Lei Yuan, Feng Chen, Zhongzhang Zhang, Yang Yu","Communication can promote coordination in cooperative Multi-Agent
Reinforcement Learning (MARL). Nowadays, existing works mainly focus on
improving the communication efficiency of agents, neglecting that real-world
communication is much more challenging as there may exist noise or potential
attackers. Thus the robustness of the communication-based policies becomes an
emergent and severe issue that needs more exploration. In this paper, we posit
that the ego system trained with auxiliary adversaries may handle this
limitation and propose an adaptable method of Multi-Agent Auxiliary Adversaries
Generation for robust Communication, dubbed MA3C, to obtain a robust
communication-based policy. In specific, we introduce a novel message-attacking
approach that models the learning of the auxiliary attacker as a cooperative
problem under a shared goal to minimize the coordination ability of the ego
system, with which every information channel may suffer from distinct message
attacks. Furthermore, as naive adversarial training may impede the
generalization ability of the ego system, we design an attacker population
generation approach based on evolutionary learning. Finally, the ego system is
paired with an attacker population and then alternatively trained against the
continuously evolving attackers to improve its robustness, meaning that both
the ego system and the attackers are adaptable. Extensive experiments on
multiple benchmarks indicate that our proposed MA3C provides comparable or
better robustness and generalization ability than other baselines.",2305.05116v1,https://arxiv.org/pdf/2305.05116v1
"Less is More: Removing Text-regions Improves CLIP Training Efficiency
  and Robustness","Liangliang Cao, Bowen Zhang, Chen Chen, Yinfei Yang, Xianzhi Du, Wencong Zhang, Zhiyun Lu, Yantao Zheng","The CLIP (Contrastive Language-Image Pre-training) model and its variants are
becoming the de facto backbone in many applications. However, training a CLIP
model from hundreds of millions of image-text pairs can be prohibitively
expensive. Furthermore, the conventional CLIP model doesn't differentiate
between the visual semantics and meaning of text regions embedded in images.
This can lead to non-robustness when the text in the embedded region doesn't
match the image's visual appearance. In this paper, we discuss two effective
approaches to improve the efficiency and robustness of CLIP training: (1)
augmenting the training dataset while maintaining the same number of
optimization steps, and (2) filtering out samples that contain text regions in
the image. By doing so, we significantly improve the classification and
retrieval accuracy on public benchmarks like ImageNet and CoCo. Filtering out
images with text regions also protects the model from typographic attacks. To
verify this, we build a new dataset named ImageNet with Adversarial Text
Regions (ImageNet-Attr). Our filter-based CLIP model demonstrates a top-1
accuracy of 68.78\%, outperforming previous models whose accuracy was all below
50\%.",2305.05095v1,https://arxiv.org/pdf/2305.05095v1
Explanation-based Finetuning Makes Models More Robust to Spurious Cues,"Josh Magnus Ludan, Yixuan Meng, Tai Nguyen, Saurabh Shah, Qing Lyu, Marianna Apidianaki, Chris Callison-Burch","Large Language Models (LLMs) are so powerful that they sometimes learn
correlations between labels and features that are irrelevant to the task,
leading to poor generalization on out-of-distribution data. We propose
explanation-based finetuning as a general approach to mitigate LLMs' reliance
on spurious correlations. Unlike standard finetuning where the model only
predicts the answer given the input, we finetune the model to additionally
generate a free-text explanation supporting its answer. To evaluate our method,
we finetune the model on artificially constructed training sets containing
different types of spurious cues, and test it on a test set without these cues.
Compared to standard finetuning, our method makes GPT-3 (davinci) remarkably
more robust against spurious cues in terms of accuracy drop across four
classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC
(+6.5). The efficacy generalizes across multiple model families and scales,
with greater gains for larger models. Finally, our method also works well with
explanations generated by the model, implying its applicability to more
datasets without human-written explanations.",2305.04990v3,https://arxiv.org/pdf/2305.04990v3
"Robust Traffic Light Detection Using Salience-Sensitive Loss:
  Computational Framework and Evaluations","Ross Greer, Akshay Gopalkrishnan, Jacob Landgren, Lulua Rakla, Anish Gopalan, Mohan Trivedi","One of the most important tasks for ensuring safe autonomous driving systems
is accurately detecting road traffic lights and accurately determining how they
impact the driver's actions. In various real-world driving situations, a scene
may have numerous traffic lights with varying levels of relevance to the
driver, and thus, distinguishing and detecting the lights that are relevant to
the driver and influence the driver's actions is a critical safety task. This
paper proposes a traffic light detection model which focuses on this task by
first defining salient lights as the lights that affect the driver's future
decisions. We then use this salience property to construct the LAVA Salient
Lights Dataset, the first US traffic light dataset with an annotated salience
property. Subsequently, we train a Deformable DETR object detection transformer
model using Salience-Sensitive Focal Loss to emphasize stronger performance on
salient traffic lights, showing that a model trained with this loss function
has stronger recall than one trained without.",2305.04516v1,https://arxiv.org/pdf/2305.04516v1
"Pick your Poison: Undetectability versus Robustness in Data Poisoning
  Attacks","Nils Lukas, Florian Kerschbaum","Deep image classification models trained on vast amounts of web-scraped data
are susceptible to data poisoning - a mechanism for backdooring models. A small
number of poisoned samples seen during training can severely undermine a
model's integrity during inference. Existing work considers an effective
defense as one that either (i) restores a model's integrity through repair or
(ii) detects an attack. We argue that this approach overlooks a crucial
trade-off: Attackers can increase robustness at the expense of detectability
(over-poisoning) or decrease detectability at the cost of robustness
(under-poisoning). In practice, attacks should remain both undetectable and
robust. Detectable but robust attacks draw human attention and rigorous model
evaluation or cause the model to be re-trained or discarded. In contrast,
attacks that are undetectable but lack robustness can be repaired with minimal
impact on model accuracy. Our research points to intrinsic flaws in current
attack evaluation methods and raises the bar for all data poisoning attackers
who must delicately balance this trade-off to remain robust and undetectable.
To demonstrate the existence of more potent defenders, we propose defenses
designed to (i) detect or (ii) repair poisoned models using a limited amount of
trusted image-label pairs. Our results show that an attacker who needs to be
robust and undetectable is substantially less threatening. Our defenses
mitigate all tested attacks with a maximum accuracy decline of 2% using only 1%
of clean data on CIFAR-10 and 2.5% on ImageNet. We demonstrate the scalability
of our defenses by evaluating large vision-language models, such as CLIP.
Attackers who can manipulate the model's parameters pose an elevated risk as
they can achieve higher robustness at low detectability compared to data
poisoning attackers.",2305.09671v2,https://arxiv.org/pdf/2305.09671v2
Robust Multi-agent Communication via Multi-view Message Certification,"Lei Yuan, Tao Jiang, Lihe Li, Feng Chen, Zongzhang Zhang, Yang Yu","Many multi-agent scenarios require message sharing among agents to promote
coordination, hastening the robustness of multi-agent communication when
policies are deployed in a message perturbation environment. Major relevant
works tackle this issue under specific assumptions, like a limited number of
message channels would sustain perturbations, limiting the efficiency in
complex scenarios. In this paper, we take a further step addressing this issue
by learning a robust multi-agent communication policy via multi-view message
certification, dubbed CroMAC. Agents trained under CroMAC can obtain guaranteed
lower bounds on state-action values to identify and choose the optimal action
under a worst-case deviation when the received messages are perturbed.
Concretely, we first model multi-agent communication as a multi-view problem,
where every message stands for a view of the state. Then we extract a
certificated joint message representation by a multi-view variational
autoencoder (MVAE) that uses a product-of-experts inference network. For the
optimization phase, we do perturbations in the latent space of the state for a
certificate guarantee. Then the learned joint message representation is used to
approximate the certificated state representation during training. Extensive
experiments in several cooperative multi-agent benchmarks validate the
effectiveness of the proposed CroMAC.",2305.13936v1,https://arxiv.org/pdf/2305.13936v1
"Robust Tensor CUR Decompositions: Rapid Low-Tucker-Rank Tensor Recovery
  with Sparse Corruption","HanQin Cai, Zehan Chao, Longxiu Huang, Deanna Needell","We study the tensor robust principal component analysis (TRPCA) problem, a
tensorial extension of matrix robust principal component analysis (RPCA), that
aims to split the given tensor into an underlying low-rank component and a
sparse outlier component. This work proposes a fast algorithm, called Robust
Tensor CUR Decompositions (RTCUR), for large-scale non-convex TRPCA problems
under the Tucker rank setting. RTCUR is developed within a framework of
alternating projections that projects between the set of low-rank tensors and
the set of sparse tensors. We utilize the recently developed tensor CUR
decomposition to substantially reduce the computational complexity in each
projection. In addition, we develop four variants of RTCUR for different
application settings. We demonstrate the effectiveness and computational
advantages of RTCUR against state-of-the-art methods on both synthetic and
real-world datasets.",2305.04080v2,https://arxiv.org/pdf/2305.04080v2
Robust A-Optimal Experimental Design for Bayesian Inverse Problems,"Ahmed Attia, Sven Leyffer, Todd Munson","Optimal design of experiments for Bayesian inverse problems has recently
gained wide popularity and attracted much attention, especially in the
computational science and Bayesian inversion communities. An optimal design
maximizes a predefined utility function that is formulated in terms of the
elements of an inverse problem, an example being optimal sensor placement for
parameter identification. The state-of-the-art algorithmic approaches following
this simple formulation generally overlook misspecification of the elements of
the inverse problem, such as the prior or the measurement uncertainties. This
work presents an efficient algorithmic approach for designing optimal
experimental design schemes for Bayesian inverse problems such that the optimal
design is robust to misspecification of elements of the inverse problem.
Specifically, we consider a worst-case scenario approach for the uncertain or
misspecified parameters, formulate robust objectives, and propose an
algorithmic approach for optimizing such objectives. Both relaxation and
stochastic solution approaches are discussed with detailed analysis and insight
into the interpretation of the problem and the proposed algorithmic approach.
Extensive numerical experiments to validate and analyze the proposed approach
are carried out for sensor placement in a parameter identification problem.",2305.03855v1,https://arxiv.org/pdf/2305.03855v1
"On the Effectiveness of Equivariant Regularization for Robust Online
  Continual Learning","Lorenzo Bonicelli, Matteo Boschini, Emanuele Frascaroli, Angelo Porrello, Matteo Pennisi, Giovanni Bellitto, Simone Palazzo, Concetto Spampinato, Simone Calderara","Humans can learn incrementally, whereas neural networks forget previously
acquired information catastrophically. Continual Learning (CL) approaches seek
to bridge this gap by facilitating the transfer of knowledge to both previous
tasks (backward transfer) and future ones (forward transfer) during training.
  Recent research has shown that self-supervision can produce versatile models
that can generalize well to diverse downstream tasks. However, contrastive
self-supervised learning (CSSL), a popular self-supervision technique, has
limited effectiveness in online CL (OCL). OCL only permits one iteration of the
input dataset, and CSSL's low sample efficiency hinders its use on the input
data-stream.
  In this work, we propose Continual Learning via Equivariant Regularization
(CLER), an OCL approach that leverages equivariant tasks for self-supervision,
avoiding CSSL's limitations. Our method represents the first attempt at
combining equivariant knowledge with CL and can be easily integrated with
existing OCL methods. Extensive ablations shed light on how equivariant pretext
tasks affect the network's information flow and its impact on CL dynamics.",2305.03648v1,https://arxiv.org/pdf/2305.03648v1
Verifiable Learning for Robust Tree Ensembles,"Stefano Calzavara, Lorenzo Cazzaro, Giulio Ermanno Pibiri, Nicola Prezza","Verifying the robustness of machine learning models against evasion attacks
at test time is an important research problem. Unfortunately, prior work
established that this problem is NP-hard for decision tree ensembles, hence
bound to be intractable for specific inputs. In this paper, we identify a
restricted class of decision tree ensembles, called large-spread ensembles,
which admit a security verification algorithm running in polynomial time. We
then propose a new approach called verifiable learning, which advocates the
training of such restricted model classes which are amenable for efficient
verification. We show the benefits of this idea by designing a new training
algorithm that automatically learns a large-spread decision tree ensemble from
labelled data, thus enabling its security verification in polynomial time.
Experimental results on public datasets confirm that large-spread ensembles
trained using our algorithm can be verified in a matter of seconds, using
standard commercial hardware. Moreover, large-spread ensembles are more robust
than traditional ensembles against evasion attacks, at the cost of an
acceptable loss of accuracy in the non-adversarial setting.",2305.03626v4,https://arxiv.org/pdf/2305.03626v4
"A Comprehensive Study on Dataset Distillation: Performance, Privacy,
  Robustness and Fairness","Zongxiong Chen, Jiahui Geng, Derui Zhu, Herbert Woisetschlaeger, Qing Li, Sonja Schimmler, Ruben Mayer, Chunming Rong","The aim of dataset distillation is to encode the rich features of an original
dataset into a tiny dataset. It is a promising approach to accelerate neural
network training and related studies. Different approaches have been proposed
to improve the informativeness and generalization performance of distilled
images. However, no work has comprehensively analyzed this technique from a
security perspective and there is a lack of systematic understanding of
potential risks. In this work, we conduct extensive experiments to evaluate
current state-of-the-art dataset distillation methods. We successfully use
membership inference attacks to show that privacy risks still remain. Our work
also demonstrates that dataset distillation can cause varying degrees of impact
on model robustness and amplify model unfairness across classes when making
predictions. This work offers a large-scale benchmarking framework for dataset
distillation evaluation.",2305.03355v3,https://arxiv.org/pdf/2305.03355v3
"Generic and Robust Root Cause Localization for Multi-Dimensional Data in
  Online Service Systems","Zeyan Li, Junjie Chen, Yihao Chen, Chengyang Luo, Yiwei Zhao, Yongqian Sun, Kaixin Sui, Xiping Wang, Dapeng Liu, Xing Jin, Qi Wang, Dan Pei","Localizing root causes for multi-dimensional data is critical to ensure
online service systems' reliability. When a fault occurs, only the measure
values within specific attribute combinations are abnormal. Such attribute
combinations are substantial clues to the underlying root causes and thus are
called root causes of multidimensional data. This paper proposes a generic and
robust root cause localization approach for multi-dimensional data, PSqueeze.
We propose a generic property of root cause for multi-dimensional data,
generalized ripple effect (GRE). Based on it, we propose a novel probabilistic
cluster method and a robust heuristic search method. Moreover, we identify the
importance of determining external root causes and propose an effective method
for the first time in literature. Our experiments on two real-world datasets
with 5400 faults show that the F1-score of PSqueeze outperforms baselines by
32.89%, while the localization time is around 10 seconds across all cases. The
F1-score in determining external root causes of PSqueeze achieves 0.90.
Furthermore, case studies in several production systems demonstrate that
PSqueeze is helpful to fault diagnosis in the real world.",2305.03331v1,https://arxiv.org/pdf/2305.03331v1
"ReMask: A Robust Information-Masking Approach for Domain Counterfactual
  Generation","Pengfei Hong, Rishabh Bhardwaj, Navonil Majumdar, Somak Aditya, Soujanya Poria","Domain shift is a big challenge in NLP, thus, many approaches resort to
learning domain-invariant features to mitigate the inference phase domain
shift. Such methods, however, fail to leverage the domain-specific nuances
relevant to the task at hand. To avoid such drawbacks, domain counterfactual
generation aims to transform a text from the source domain to a given target
domain. However, due to the limited availability of data, such frequency-based
methods often miss and lead to some valid and spurious domain-token
associations. Hence, we employ a three-step domain obfuscation approach that
involves frequency and attention norm-based masking, to mask domain-specific
cues, and unmasking to regain the domain generic context. Our experiments
empirically show that the counterfactual samples sourced from our masked text
lead to improved domain transfer on 10 out of 12 domain sentiment
classification settings, with an average of 2% accuracy improvement over the
state-of-the-art for unsupervised domain adaptation (UDA). Further, our model
outperforms the state-of-the-art by achieving 1.4% average accuracy improvement
in the adversarial domain adaptation (ADA) setting. Moreover, our model also
shows its domain adaptation efficacy on a large multi-domain intent
classification dataset where it attains state-of-the-art results. We release
the codes publicly at \url{https://github.com/declare-lab/remask}.",2305.02858v1,https://arxiv.org/pdf/2305.02858v1
BranchNorm: Robustly Scaling Extremely Deep Transformers,"Yijin Liu, Xianfeng Zeng, Fandong Meng, Jie Zhou","Recently, DeepNorm scales Transformers into extremely deep (i.e., 1000
layers) and reveals the promising potential of deep scaling. To stabilize the
training of deep models, DeepNorm (Wang et al., 2022) attempts to constrain the
model update to a constant value. Although applying such a constraint can
benefit the early stage of model training, it may lead to undertrained models
during the whole training procedure. In this paper, we propose BranchNorm,
which dynamically rescales the non-residual branch of Transformer in accordance
with the training period. BranchNorm not only theoretically stabilizes the
training with smooth gradient norms at the early stage, but also encourages
better convergence in the subsequent training stage. Experiment results on
multiple translation tasks demonstrate that BranchNorm achieves a better
trade-off between training stability and converge performance.",2305.02790v1,https://arxiv.org/pdf/2305.02790v1
"Toward Evaluating Robustness of Reinforcement Learning with Adversarial
  Policy","Xiang Zheng, Xingjun Ma, Shengjie Wang, Xinyu Wang, Chao Shen, Cong Wang","Reinforcement learning agents are susceptible to evasion attacks during
deployment. In single-agent environments, these attacks can occur through
imperceptible perturbations injected into the inputs of the victim policy
network. In multi-agent environments, an attacker can manipulate an adversarial
opponent to influence the victim policy's observations indirectly. While
adversarial policies offer a promising technique to craft such attacks, current
methods are either sample-inefficient due to poor exploration strategies or
require extra surrogate model training under the black-box assumption. To
address these challenges, in this paper, we propose Intrinsically Motivated
Adversarial Policy (IMAP) for efficient black-box adversarial policy learning
in both single- and multi-agent environments. We formulate four types of
adversarial intrinsic regularizers -- maximizing the adversarial state
coverage, policy coverage, risk, or divergence -- to discover potential
vulnerabilities of the victim policy in a principled way. We also present a
novel bias-reduction method to balance the extrinsic objective and the
adversarial intrinsic regularizers adaptively. Our experiments validate the
effectiveness of the four types of adversarial intrinsic regularizers and the
bias-reduction method in enhancing black-box adversarial policy learning across
a variety of environments. Our IMAP successfully evades two types of defense
methods, adversarial training and robust regularizer, decreasing the
performance of the state-of-the-art robust WocaR-PPO agents by 34\%-54\% across
four single-agent tasks. IMAP also achieves a state-of-the-art attacking
success rate of 83.91\% in the multi-agent game YouShallNotPass. Our code is
available at \url{https://github.com/x-zheng16/IMAP}.",2305.02605v3,https://arxiv.org/pdf/2305.02605v3
Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA,"Ilias Diakonikolas, Daniel M. Kane, Ankit Pensia, Thanasis Pittas","We study principal component analysis (PCA), where given a dataset in
$\mathbb{R}^d$ from a distribution, the task is to find a unit vector $v$ that
approximately maximizes the variance of the distribution after being projected
along $v$. Despite being a classical task, standard estimators fail drastically
if the data contains even a small fraction of outliers, motivating the problem
of robust PCA. Recent work has developed computationally-efficient algorithms
for robust PCA that either take super-linear time or have sub-optimal error
guarantees. Our main contribution is to develop a nearly-linear time algorithm
for robust PCA with near-optimal error guarantees. We also develop a
single-pass streaming algorithm for robust PCA with memory usage nearly-linear
in the dimension.",2305.02544v1,https://arxiv.org/pdf/2305.02544v1
A Curriculum View of Robust Loss Functions,"Zebin Ou, Yue Zhang","Robust loss functions are designed to combat the adverse impacts of label
noise, whose robustness is typically supported by theoretical bounds agnostic
to the training dynamics. However, these bounds may fail to characterize the
empirical performance as it remains unclear why robust loss functions can
underfit. We show that most loss functions can be rewritten into a form with
the same class-score margin and different sample-weighting functions. The
resulting curriculum view provides a straightforward analysis of the training
dynamics, which helps attribute underfitting to diminished average sample
weights and noise robustness to larger weights for clean samples. We show that
simple fixes to the curriculums can make underfitting robust loss functions
competitive with the state-of-the-art, and training schedules can substantially
affect the noise robustness even with robust loss functions. Code is available
at \url{github}.",2305.02139v1,https://arxiv.org/pdf/2305.02139v1
"Robust Multi-bit Natural Language Watermarking through Invariant
  Features","KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, Nojun Kwak","Recent years have witnessed a proliferation of valuable original natural
language contents found in subscription-based media outlets, web novel
platforms, and outputs of large language models. However, these contents are
susceptible to illegal piracy and potential misuse without proper security
measures. This calls for a secure watermarking system to guarantee copyright
protection through leakage tracing or ownership identification. To effectively
combat piracy and protect copyrights, a multi-bit watermarking framework should
be able to embed adequate bits of information and extract the watermarks in a
robust manner despite possible corruption. In this work, we explore ways to
advance both payload and robustness by following a well-known proposition from
image watermarking and identify features in natural language that are invariant
to minor corruption. Through a systematic analysis of the possible sources of
errors, we further propose a corruption-resistant infill model. Our full method
improves upon the previous work on robustness by +16.8% point on average on
four datasets, three corruption types, and two corruption ratios. Code
available at https://github.com/bangawayoo/nlp-watermarking.",2305.01904v2,https://arxiv.org/pdf/2305.01904v2
"Out-of-distribution detection algorithms for robust insect
  classification","Mojdeh Saadati, Aditya Balu, Shivani Chiranjeevi, Talukder Zaki Jubery, Asheesh K Singh, Soumik Sarkar, Arti Singh, Baskar Ganapathysubramanian","Deep learning-based approaches have produced models with good insect
classification accuracy; Most of these models are conducive for application in
controlled environmental conditions. One of the primary emphasis of researchers
is to implement identification and classification models in the real
agriculture fields, which is challenging because input images that are wildly
out of the distribution (e.g., images like vehicles, animals, humans, or a
blurred image of an insect or insect class that is not yet trained on) can
produce an incorrect insect classification. Out-of-distribution (OOD) detection
algorithms provide an exciting avenue to overcome these challenge as it ensures
that a model abstains from making incorrect classification prediction of
non-insect and/or untrained insect class images. We generate and evaluate the
performance of state-of-the-art OOD algorithms on insect detection classifiers.
These algorithms represent a diversity of methods for addressing an OOD
problem. Specifically, we focus on extrusive algorithms, i.e., algorithms that
wrap around a well-trained classifier without the need for additional
co-training. We compared three OOD detection algorithms: (i) Maximum Softmax
Probability, which uses the softmax value as a confidence score, (ii)
Mahalanobis distance-based algorithm, which uses a generative classification
approach; and (iii) Energy-Based algorithm that maps the input data to a scalar
value, called energy. We performed an extensive series of evaluations of these
OOD algorithms across three performance axes: (a) \textit{Base model accuracy}:
How does the accuracy of the classifier impact OOD performance? (b) How does
the \textit{level of dissimilarity to the domain} impact OOD performance? and
(c) \textit{Data imbalance}: How sensitive is OOD performance to the imbalance
in per-class sample size?",2305.01823v1,https://arxiv.org/pdf/2305.01823v1
"Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models
  against Counterfactual Noise","Giwon Hong, Jeonghwan Kim, Junmo Kang, Sung-Hyon Myaeng, Joyce Jiyoung Whang","Most existing retrieval-augmented language models (LMs) assume a naive
dichotomy within a retrieved document set: query-relevance and irrelevance. Our
work investigates a more challenging scenario in which even the ""relevant""
documents may contain misleading or incorrect information, causing conflict
among the retrieved documents and thereby negatively influencing model
decisions as noise. We observe that existing LMs are highly brittle to the
presence of conflicting information in both the fine-tuning and in-context
few-shot learning scenarios. We propose approaches for handling knowledge
conflicts among retrieved documents by explicitly fine-tuning a discriminator
or prompting GPT-3.5 to elicit its discriminative capability. Our empirical
results on open-domain QA show that these approaches significantly enhance
model robustness. We also provide our findings on incorporating the fine-tuned
discriminator's decision into the in-context learning process, proposing a way
to exploit the benefits of two disparate learning schemes. Alongside our
findings, we provide MacNoise, a machine-generated, conflict-induced dataset to
further encourage research in this direction.",2305.01579v3,https://arxiv.org/pdf/2305.01579v3
"ARBEx: Attentive Feature Extraction with Reliability Balancing for
  Robust Facial Expression Learning","Azmine Toushik Wasi, Karlo Šerbetar, Raima Islam, Taki Hasan Rafi, Dong-Kyu Chae","In this paper, we introduce a framework ARBEx, a novel attentive feature
extraction framework driven by Vision Transformer with reliability balancing to
cope against poor class distributions, bias, and uncertainty in the facial
expression learning (FEL) task. We reinforce several data pre-processing and
refinement methods along with a window-based cross-attention ViT to squeeze the
best of the data. We also employ learnable anchor points in the embedding space
with label distributions and multi-head self-attention mechanism to optimize
performance against weak predictions with reliability balancing, which is a
strategy that leverages anchor points, attention scores, and confidence values
to enhance the resilience of label predictions. To ensure correct label
classification and improve the models' discriminative power, we introduce
anchor loss, which encourages large margins between anchor points.
Additionally, the multi-head self-attention mechanism, which is also trainable,
plays an integral role in identifying accurate labels. This approach provides
critical elements for improving the reliability of predictions and has a
substantial positive effect on final prediction capabilities. Our adaptive
model can be integrated with any deep neural network to forestall challenges in
various recognition tasks. Our strategy outperforms current state-of-the-art
methodologies, according to extensive experiments conducted in a variety of
contexts.",2305.01486v3,https://arxiv.org/pdf/2305.01486v3
Get Back Here: Robust Imitation by Return-to-Distribution Planning,"Geoffrey Cideron, Baruch Tabanpour, Sebastian Curi, Sertan Girgin, Leonard Hussenot, Gabriel Dulac-Arnold, Matthieu Geist, Olivier Pietquin, Robert Dadashi","We consider the Imitation Learning (IL) setup where expert data are not
collected on the actual deployment environment but on a different version. To
address the resulting distribution shift, we combine behavior cloning (BC) with
a planner that is tasked to bring the agent back to states visited by the
expert whenever the agent deviates from the demonstration distribution. The
resulting algorithm, POIR, can be trained offline, and leverages online
interactions to efficiently fine-tune its planner to improve performance over
time. We test POIR on a variety of human-generated manipulation demonstrations
in a realistic robotic manipulation simulator and show robustness of the
learned policy to different initial state distributions and noisy dynamics.",2305.01400v1,https://arxiv.org/pdf/2305.01400v1
Stratified Adversarial Robustness with Rejection,"Jiefeng Chen, Jayaram Raghuram, Jihye Choi, Xi Wu, Yingyu Liang, Somesh Jha","Recently, there is an emerging interest in adversarially training a
classifier with a rejection option (also known as a selective classifier) for
boosting adversarial robustness. While rejection can incur a cost in many
applications, existing studies typically associate zero cost with rejecting
perturbed inputs, which can result in the rejection of numerous
slightly-perturbed inputs that could be correctly classified. In this work, we
study adversarially-robust classification with rejection in the stratified
rejection setting, where the rejection cost is modeled by rejection loss
functions monotonically non-increasing in the perturbation magnitude. We
theoretically analyze the stratified rejection setting and propose a novel
defense method -- Adversarial Training with Consistent Prediction-based
Rejection (CPR) -- for building a robust selective classifier. Experiments on
image datasets demonstrate that the proposed method significantly outperforms
existing methods under strong adaptive attacks. For instance, on CIFAR-10, CPR
reduces the total robust loss (for different rejection losses) by at least 7.3%
under both seen and unseen attacks.",2305.01139v2,https://arxiv.org/pdf/2305.01139v2
Revisiting Robustness in Graph Machine Learning,"Lukas Gosch, Daniel Sturm, Simon Geisler, Stephan Günnemann","Many works show that node-level predictions of Graph Neural Networks (GNNs)
are unrobust to small, often termed adversarial, changes to the graph
structure. However, because manual inspection of a graph is difficult, it is
unclear if the studied perturbations always preserve a core assumption of
adversarial examples: that of unchanged semantic content. To address this
problem, we introduce a more principled notion of an adversarial graph, which
is aware of semantic content change. Using Contextual Stochastic Block Models
(CSBMs) and real-world graphs, our results uncover: $i)$ for a majority of
nodes the prevalent perturbation models include a large fraction of perturbed
graphs violating the unchanged semantics assumption; $ii)$ surprisingly, all
assessed GNNs show over-robustness - that is robustness beyond the point of
semantic change. We find this to be a complementary phenomenon to adversarial
examples and show that including the label-structure of the training graph into
the inference process of GNNs significantly reduces over-robustness, while
having a positive effect on test accuracy and adversarial robustness.
Theoretically, leveraging our new semantics-aware notion of robustness, we
prove that there is no robustness-accuracy tradeoff for inductively classifying
a newly added node.",2305.00851v2,https://arxiv.org/pdf/2305.00851v2
Efficient Sensitivity Analysis for Parametric Robust Markov Chains,"Thom Badings, Sebastian Junges, Ahmadreza Marandi, Ufuk Topcu, Nils Jansen","We provide a novel method for sensitivity analysis of parametric robust
Markov chains. These models incorporate parameters and sets of probability
distributions to alleviate the often unrealistic assumption that precise
probabilities are available. We measure sensitivity in terms of partial
derivatives with respect to the uncertain transition probabilities regarding
measures such as the expected reward. As our main contribution, we present an
efficient method to compute these partial derivatives. To scale our approach to
models with thousands of parameters, we present an extension of this method
that selects the subset of $k$ parameters with the highest partial derivative.
Our methods are based on linear programming and differentiating these programs
around a given value for the parameters. The experiments show the applicability
of our approach on models with over a million states and thousands of
parameters. Moreover, we embed the results within an iterative learning scheme
that profits from having access to a dedicated sensitivity analysis.",2305.01473v1,https://arxiv.org/pdf/2305.01473v1
Robustified Learning for Online Optimization with Memory Costs,"Pengfei Li, Jianyi Yang, Shaolei Ren","Online optimization with memory costs has many real-world applications, where
sequential actions are made without knowing the future input. Nonetheless, the
memory cost couples the actions over time, adding substantial challenges.
Conventionally, this problem has been approached by various expert-designed
online algorithms with the goal of achieving bounded worst-case competitive
ratios, but the resulting average performance is often unsatisfactory. On the
other hand, emerging machine learning (ML) based optimizers can improve the
average performance, but suffer from the lack of worst-case performance
robustness. In this paper, we propose a novel expert-robustified learning (ERL)
approach, achieving {both} good average performance and robustness. More
concretely, for robustness, ERL introduces a novel projection operator that
robustifies ML actions by utilizing an expert online algorithm; for average
performance, ERL trains the ML optimizer based on a recurrent architecture by
explicitly considering downstream expert robustification. We prove that, for
any $\lambda\geq1$, ERL can achieve $\lambda$-competitive against the expert
algorithm and $\lambda\cdot C$-competitive against the optimal offline
algorithm (where $C$ is the expert's competitive ratio). Additionally, we
extend our analysis to a novel setting of multi-step memory costs. Finally, our
analysis is supported by empirical experiments for an energy scheduling
application.",2305.00677v1,https://arxiv.org/pdf/2305.00677v1
The ART of Transfer Learning: An Adaptive and Robust Pipeline,"Boxiang Wang, Yunan Wu, Chenglong Ye","Transfer learning is an essential tool for improving the performance of
primary tasks by leveraging information from auxiliary data resources. In this
work, we propose Adaptive Robust Transfer Learning (ART), a flexible pipeline
of performing transfer learning with generic machine learning algorithms. We
establish the non-asymptotic learning theory of ART, providing a provable
theoretical guarantee for achieving adaptive transfer while preventing negative
transfer. Additionally, we introduce an ART-integrated-aggregating machine that
produces a single final model when multiple candidate algorithms are
considered. We demonstrate the promising performance of ART through extensive
empirical studies on regression, classification, and sparse learning. We
further present a real-data analysis for a mortality study.",2305.00520v1,https://arxiv.org/pdf/2305.00520v1
"Efficient MILP Decomposition in Quantum Computing for ReLU Network
  Robustness","Nicola Franco, Tom Wollschläger, Benedikt Poggel, Stephan Günnemann, Jeanette Miriam Lorenz","Emerging quantum computing technologies, such as Noisy Intermediate-Scale
Quantum (NISQ) devices, offer potential advancements in solving mathematical
optimization problems. However, limitations in qubit availability, noise, and
errors pose challenges for practical implementation. In this study, we examine
two decomposition methods for Mixed-Integer Linear Programming (MILP) designed
to reduce the original problem size and utilize available NISQ devices more
efficiently. We concentrate on breaking down the original problem into smaller
subproblems, which are then solved iteratively using a combined
quantum-classical hardware approach. We conduct a detailed analysis for the
decomposition of MILP with Benders and Dantzig-Wolfe methods. In our analysis,
we show that the number of qubits required to solve Benders is exponentially
large in the worst-case, while remains constant for Dantzig-Wolfe.
Additionally, we leverage Dantzig-Wolfe decomposition on the use-case of
certifying the robustness of ReLU networks. Our experimental results
demonstrate that this approach can save up to 90\% of qubits compared to
existing methods on quantum annealing and gate-based quantum computers.",2305.00472v2,https://arxiv.org/pdf/2305.00472v2
"Adversarial Representation Learning for Robust Privacy Preservation in
  Audio","Shayan Gharib, Minh Tran, Diep Luong, Konstantinos Drossos, Tuomas Virtanen","Sound event detection systems are widely used in various applications such as
surveillance and environmental monitoring where data is automatically
collected, processed, and sent to a cloud for sound recognition. However, this
process may inadvertently reveal sensitive information about users or their
surroundings, hence raising privacy concerns. In this study, we propose a novel
adversarial training method for learning representations of audio recordings
that effectively prevents the detection of speech activity from the latent
features of the recordings. The proposed method trains a model to generate
invariant latent representations of speech-containing audio recordings that
cannot be distinguished from non-speech recordings by a speech classifier. The
novelty of our work is in the optimization algorithm, where the speech
classifier's weights are regularly replaced with the weights of classifiers
trained in a supervised manner. This increases the discrimination power of the
speech classifier constantly during the adversarial training, motivating the
model to generate latent representations in which speech is not
distinguishable, even using new speech classifiers trained outside the
adversarial training loop. The proposed method is evaluated against a baseline
approach with no privacy measures and a prior adversarial training method,
demonstrating a significant reduction in privacy violations compared to the
baseline approach. Additionally, we show that the prior adversarial method is
practically ineffective for this purpose.",2305.00011v2,https://arxiv.org/pdf/2305.00011v2
"Evaluating the Stability of Semantic Concept Representations in CNNs for
  Robust Explainability","Georgii Mikriukov, Gesina Schwalbe, Christian Hellert, Korinna Bade","Analysis of how semantic concepts are represented within Convolutional Neural
Networks (CNNs) is a widely used approach in Explainable Artificial
Intelligence (XAI) for interpreting CNNs. A motivation is the need for
transparency in safety-critical AI-based systems, as mandated in various
domains like automated driving. However, to use the concept representations for
safety-relevant purposes, like inspection or error retrieval, these must be of
high quality and, in particular, stable. This paper focuses on two stability
goals when working with concept representations in computer vision CNNs:
stability of concept retrieval and of concept attribution. The guiding use-case
is a post-hoc explainability framework for object detection (OD) CNNs, towards
which existing concept analysis (CA) methods are successfully adapted. To
address concept retrieval stability, we propose a novel metric that considers
both concept separation and consistency, and is agnostic to layer and concept
representation dimensionality. We then investigate impacts of concept
abstraction level, number of concept training samples, CNN size, and concept
representation dimensionality on stability. For concept attribution stability
we explore the effect of gradient instability on gradient-based explainability
methods. The results on various CNNs for classification and object detection
yield the main findings that (1) the stability of concept retrieval can be
enhanced through dimensionality reduction via data aggregation, and (2) in
shallow layers where gradient instability is more pronounced, gradient
smoothing techniques are advised. Finally, our approach provides valuable
insights into selecting the appropriate layer and concept representation
dimensionality, paving the way towards CA in safety-critical XAI applications.",2304.14864v1,https://arxiv.org/pdf/2304.14864v1
"A noise-robust acoustic method for recognizing foraging activities of
  grazing cattle","Luciano S. Martinez-Rau, José O. Chelotti, Mariano Ferrero, Julio R. Galli, Santiago A. Utsumi, Alejandra M. Planisich, H. Leonardo Rufiner, Leonardo L. Giovanini","Farmers must continuously improve their livestock production systems to
remain competitive in the growing dairy market. Precision livestock farming
technologies provide individualized monitoring of animals on commercial farms,
optimizing livestock production. Continuous acoustic monitoring is a widely
accepted sensing technique used to estimate the daily rumination and grazing
time budget of free-ranging cattle. However, typical environmental and natural
noises on pastures noticeably affect the performance limiting the practical
application of current acoustic methods. In this study, we present the
operating principle and generalization capability of an acoustic method called
Noise-Robust Foraging Activity Recognizer (NRFAR). The proposed method
determines foraging activity bouts by analyzing fixed-length segments of
identified jaw movement events produced during grazing and rumination. The
additive noise robustness of the NRFAR was evaluated for several
signal-to-noise ratios using stationary Gaussian white noise and four different
nonstationary natural noise sources. In noiseless conditions, NRFAR reached an
average balanced accuracy of 86.4%, outperforming two previous acoustic methods
by more than 7.5%. Furthermore, NRFAR performed better than previous acoustic
methods in 77 of 80 evaluated noisy scenarios (53 cases with p<0.05). NRFAR has
been shown to be effective in harsh free-ranging environments and could be used
as a reliable solution to improve pasture management and monitor the health and
welfare of dairy cows. The instrumentation and computational algorithms
presented in this publication are protected by a pending patent application: AR
P20220100910. Web demo available at: https://sinc.unl.edu.ar/web-demo/nrfar",2304.14824v3,https://arxiv.org/pdf/2304.14824v3
"A feature selection method based on Shapley values robust to concept
  shift in regression","Carlos Sebastián, Carlos E. González-Guillén","Feature selection is one of the most relevant processes in any methodology
for creating a statistical learning model. Usually, existing algorithms
establish some criterion to select the most influential variables, discarding
those that do not contribute to the model with any relevant information. This
methodology makes sense in a static situation where the joint distribution of
the data does not vary over time. However, when dealing with real data, it is
common to encounter the problem of the dataset shift and, specifically, changes
in the relationships between variables (concept shift). In this case, the
influence of a variable cannot be the only indicator of its quality as a
regressor of the model, since the relationship learned in the training phase
may not correspond to the current situation. In tackling this problem, our
approach establishes a direct relationship between the Shapley values and
prediction errors, operating at a more local level to effectively detect the
individual biases introduced by each variable. The proposed methodology is
evaluated through various examples, including synthetic scenarios mimicking
sudden and incremental shift situations, as well as two real-world cases
characterized by concept shifts. Additionally, we perform three analyses of
standard situations to assess the algorithm's robustness in the absence of
shifts. The results demonstrate that our proposed algorithm significantly
outperforms state-of-the-art feature selection methods in concept shift
scenarios, while matching the performance of existing methodologies in static
situations.",2304.14774v3,https://arxiv.org/pdf/2304.14774v3
"Graph Neural Networks on Factor Graphs for Robust, Fast, and Scalable
  Linear State Estimation with PMUs","Ognjen Kundacina, Mirsad Cosovic, Dragisa Miskovic, Dejan Vukobratovic","As phasor measurement units (PMUs) become more widely used in transmission
power systems, a fast state estimation (SE) algorithm that can take advantage
of their high sample rates is needed. To accomplish this, we present a method
that uses graph neural networks (GNNs) to learn complex bus voltage estimates
from PMU voltage and current measurements. We propose an original
implementation of GNNs over the power system's factor graph to simplify the
integration of various types and quantities of measurements on power system
buses and branches. Furthermore, we augment the factor graph to improve the
robustness of GNN predictions. This model is highly efficient and scalable, as
its computational complexity is linear with respect to the number of nodes in
the power system. Training and test examples were generated by randomly
sampling sets of power system measurements and annotated with the exact
solutions of linear SE with PMUs. The numerical results demonstrate that the
GNN model provides an accurate approximation of the SE solutions. Furthermore,
errors caused by PMU malfunctions or communication failures that would normally
make the SE problem unobservable have a local effect and do not deteriorate the
results in the rest of the power system.",2304.14680v1,https://arxiv.org/pdf/2304.14680v1
Robust and Fast Vehicle Detection using Augmented Confidence Map,"Hamam Mokayed, Palaiahnakote Shivakumara, Lama Alkhaled, Rajkumar Saini, Muhammad Zeshan Afzal, Yan Chai Hum, Marcus Liwicki","Vehicle detection in real-time scenarios is challenging because of the time
constraints and the presence of multiple types of vehicles with different
speeds, shapes, structures, etc. This paper presents a new method relied on
generating a confidence map-for robust and faster vehicle detection. To reduce
the adverse effect of different speeds, shapes, structures, and the presence of
several vehicles in a single image, we introduce the concept of augmentation
which highlights the region of interest containing the vehicles. The augmented
map is generated by exploring the combination of multiresolution analysis and
maximally stable extremal regions (MR-MSER). The output of MR-MSER is supplied
to fast CNN to generate a confidence map, which results in candidate regions.
Furthermore, unlike existing models that implement complicated models for
vehicle detection, we explore the combination of a rough set and fuzzy-based
models for robust vehicle detection. To show the effectiveness of the proposed
method, we conduct experiments on our dataset captured by drones and on several
vehicle detection benchmark datasets, namely, KITTI and UA-DETRAC. The results
on our dataset and the benchmark datasets show that the proposed method
outperforms the existing methods in terms of time efficiency and achieves a
good detection rate.",2304.14462v1,https://arxiv.org/pdf/2304.14462v1
"Attacks on Robust Distributed Learning Schemes via Sensitivity Curve
  Maximization","Christian A. Schroth, Stefan Vlaski, Abdelhak M. Zoubir","Distributed learning paradigms, such as federated or decentralized learning,
allow a collection of agents to solve global learning and optimization problems
through limited local interactions. Most such strategies rely on a mixture of
local adaptation and aggregation steps, either among peers or at a central
fusion center. Classically, aggregation in distributed learning is based on
averaging, which is statistically efficient, but susceptible to attacks by even
a small number of malicious agents. This observation has motivated a number of
recent works, which develop robust aggregation schemes by employing robust
variations of the mean. We present a new attack based on sensitivity curve
maximization (SCM), and demonstrate that it is able to disrupt existing robust
aggregation schemes by injecting small, but effective perturbations.",2304.14024v1,https://arxiv.org/pdf/2304.14024v1
Unsupervised Learning of Robust Spectral Shape Matching,"Dongliang Cao, Paul Roetzer, Florian Bernard","We propose a novel learning-based approach for robust 3D shape matching. Our
method builds upon deep functional maps and can be trained in a fully
unsupervised manner. Previous deep functional map methods mainly focus on
predicting optimised functional maps alone, and then rely on off-the-shelf
post-processing to obtain accurate point-wise maps during inference. However,
this two-stage procedure for obtaining point-wise maps often yields sub-optimal
performance. In contrast, building upon recent insights about the relation
between functional maps and point-wise maps, we propose a novel unsupervised
loss to couple the functional maps and point-wise maps, and thereby directly
obtain point-wise maps without any post-processing. Our approach obtains
accurate correspondences not only for near-isometric shapes, but also for more
challenging non-isometric shapes and partial shapes, as well as shapes with
different discretisation or topological noise. Using a total of nine diverse
datasets, we extensively evaluate the performance and demonstrate that our
method substantially outperforms previous state-of-the-art methods, even
compared to recent supervised methods. Our code is available at
https://github.com/dongliangcao/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching.",2304.14419v1,https://arxiv.org/pdf/2304.14419v1
"Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot
  Encoding and Regularization","Shijie Cui, Agus Sudjianto, Aijun Zhang, Runze Li","Gradient-boosted decision trees (GBDT) are widely used and highly effective
machine learning approach for tabular data modeling. However, their complex
structure may lead to low robustness against small covariate perturbation in
unseen data. In this study, we apply one-hot encoding to convert a GBDT model
into a linear framework, through encoding of each tree leaf to one dummy
variable. This allows for the use of linear regression techniques, plus a novel
risk decomposition for assessing the robustness of a GBDT model against
covariate perturbations. We propose to enhance the robustness of GBDT models by
refitting their linear regression forms with $L_1$ or $L_2$ regularization.
Theoretical results are obtained about the effect of regularization on the
model performance and robustness. It is demonstrated through numerical
experiments that the proposed regularization approach can enhance the
robustness of the one-hot-encoded GBDT models.",2304.13761v3,https://arxiv.org/pdf/2304.13761v3
"CROP: Towards Distributional-Shift Robust Reinforcement Learning using
  Compact Reshaped Observation Processing","Philipp Altmann, Fabian Ritz, Leonard Feuchtinger, Jonas Nüßlein, Claudia Linnhoff-Popien, Thomy Phan","The safe application of reinforcement learning (RL) requires generalization
from limited training data to unseen scenarios. Yet, fulfilling tasks under
changing circumstances is a key challenge in RL. Current state-of-the-art
approaches for generalization apply data augmentation techniques to increase
the diversity of training data. Even though this prevents overfitting to the
training environment(s), it hinders policy optimization. Crafting a suitable
observation, only containing crucial information, has been shown to be a
challenging task itself. To improve data efficiency and generalization
capabilities, we propose Compact Reshaped Observation Processing (CROP) to
reduce the state information used for policy optimization. By providing only
relevant information, overfitting to a specific training layout is precluded
and generalization to unseen environments is improved. We formulate three CROPs
that can be applied to fully observable observation- and action-spaces and
provide methodical foundation. We empirically show the improvements of CROP in
a distributionally shifted safety gridworld. We furthermore provide benchmark
comparisons to full observability and data-augmentation in two different-sized
procedurally generated mazes.",2304.13616v2,https://arxiv.org/pdf/2304.13616v2
Robust Non-Linear Feedback Coding via Power-Constrained Deep Learning,"Junghoon Kim, Taejoon Kim, David Love, Christopher Brinton","The design of codes for feedback-enabled communications has been a
long-standing open problem. Recent research on non-linear, deep learning-based
coding schemes have demonstrated significant improvements in communication
reliability over linear codes, but are still vulnerable to the presence of
forward and feedback noise over the channel. In this paper, we develop a new
family of non-linear feedback codes that greatly enhance robustness to channel
noise. Our autoencoder-based architecture is designed to learn codes based on
consecutive blocks of bits, which obtains de-noising advantages over bit-by-bit
processing to help overcome the physical separation between the encoder and
decoder over a noisy channel. Moreover, we develop a power control layer at the
encoder to explicitly incorporate hardware constraints into the learning
optimization, and prove that the resulting average power constraint is
satisfied asymptotically. Numerical experiments demonstrate that our scheme
outperforms state-of-the-art feedback codes by wide margins over practical
forward and feedback noise regimes, and provide information-theoretic insights
on the behavior of our non-linear codes. Moreover, we observe that, in a long
blocklength regime, canonical error correction codes are still preferable to
feedback codes when the feedback noise becomes high.",2304.13178v2,https://arxiv.org/pdf/2304.13178v2
"LSTM-based Load Forecasting Robustness Against Noise Injection Attack in
  Microgrid","Amirhossein Nazeri, Pierluigi Pisu","In this paper, we investigate the robustness of an LSTM neural network
against noise injection attacks for electric load forecasting in an ideal
microgrid. The performance of the LSTM model is investigated under a black-box
Gaussian noise attack with different SNRs. It is assumed that attackers have
just access to the input data of the LSTM model. The results show that the
noise attack affects the performance of the LSTM model. The load prediction
means absolute error (MAE) is 0.047 MW for a healthy prediction, while this
value increases up to 0.097 MW for a Gaussian noise insertion with SNR= 6 dB.
To robustify the LSTM model against noise attack, a low-pass filter with
optimal cut-off frequency is applied at the model's input to remove the noise
attack. The filter performs better in case of noise with lower SNR and is less
promising for small noises.",2304.13104v1,https://arxiv.org/pdf/2304.13104v1
Making Video Quality Assessment Models Robust to Bit Depth,"Joshua P. Ebenezer, Zaixi Shang, Yongjun Wu, Hai Wei, Sriram Sethuraman, Alan C. Bovik","We introduce a novel feature set, which we call HDRMAX features, that when
included into Video Quality Assessment (VQA) algorithms designed for Standard
Dynamic Range (SDR) videos, sensitizes them to distortions of High Dynamic
Range (HDR) videos that are inadequately accounted for by these algorithms.
While these features are not specific to HDR, and also augment the equality
prediction performances of VQA models on SDR content, they are especially
effective on HDR. HDRMAX features modify powerful priors drawn from Natural
Video Statistics (NVS) models by enhancing their measurability where they
visually impact the brightest and darkest local portions of videos, thereby
capturing distortions that are often poorly accounted for by existing VQA
models. As a demonstration of the efficacy of our approach, we show that, while
current state-of-the-art VQA models perform poorly on 10-bit HDR databases,
their performances are greatly improved by the inclusion of HDRMAX features
when tested on HDR and 10-bit distorted videos.",2304.13092v1,https://arxiv.org/pdf/2304.13092v1
"Improving Robustness Against Adversarial Attacks with Deeply Quantized
  Neural Networks","Ferheen Ayaz, Idris Zakariyya, José Cano, Sye Loong Keoh, Jeremy Singer, Danilo Pau, Mounia Kharbouche-Harrari","Reducing the memory footprint of Machine Learning (ML) models, particularly
Deep Neural Networks (DNNs), is essential to enable their deployment into
resource-constrained tiny devices. However, a disadvantage of DNN models is
their vulnerability to adversarial attacks, as they can be fooled by adding
slight perturbations to the inputs. Therefore, the challenge is how to create
accurate, robust, and tiny DNN models deployable on resource-constrained
embedded devices. This paper reports the results of devising a tiny DNN model,
robust to adversarial black and white box attacks, trained with an automatic
quantizationaware training framework, i.e. QKeras, with deep quantization loss
accounted in the learning loop, thereby making the designed DNNs more accurate
for deployment on tiny devices. We investigated how QKeras and an adversarial
robustness technique, Jacobian Regularization (JR), can provide a
co-optimization strategy by exploiting the DNN topology and the per layer JR
approach to produce robust yet tiny deeply quantized DNN models. As a result, a
new DNN model implementing this cooptimization strategy was conceived,
developed and tested on three datasets containing both images and audio inputs,
as well as compared its performance with existing benchmarks against various
white-box and black-box attacks. Experimental results demonstrated that on
average our proposed DNN model resulted in 8.3% and 79.5% higher accuracy than
MLCommons/Tiny benchmarks in the presence of white-box and black-box attacks on
the CIFAR-10 image dataset and a subset of the Google Speech Commands audio
dataset respectively. It was also 6.5% more accurate for black-box attacks on
the SVHN image dataset.",2304.12829v1,https://arxiv.org/pdf/2304.12829v1
Differential Privacy via Distributionally Robust Optimization,"Aras Selvi, Huikang Liu, Wolfram Wiesemann","In recent years, differential privacy has emerged as the de facto standard
for sharing statistics of datasets while limiting the disclosure of private
information about the involved individuals. This is achieved by randomly
perturbing the statistics to be published, which in turn leads to a
privacy-accuracy trade-off: larger perturbations provide stronger privacy
guarantees, but they result in less accurate statistics that offer lower
utility to the recipients. Of particular interest are therefore optimal
mechanisms that provide the highest accuracy for a pre-selected level of
privacy. To date, work in this area has focused on specifying families of
perturbations a priori and subsequently proving their asymptotic and/or
best-in-class optimality. In this paper, we develop a class of mechanisms that
enjoy non-asymptotic and unconditional optimality guarantees. To this end, we
formulate the mechanism design problem as an infinite-dimensional
distributionally robust optimization problem. We show that the problem affords
a strong dual, and we exploit this duality to develop converging hierarchies of
finite-dimensional upper and lower bounding problems. Our upper (primal) bounds
correspond to implementable perturbations whose suboptimality can be bounded by
our lower (dual) bounds. Both bounding problems can be solved within seconds
via cutting plane techniques that exploit the inherent problem structure. Our
numerical experiments demonstrate that our perturbations can outperform the
previously best results from the literature on artificial as well as standard
benchmark problems.",2304.12681v2,https://arxiv.org/pdf/2304.12681v2
"A Multi-Task Approach to Robust Deep Reinforcement Learning for Resource
  Allocation","Steffen Gracla, Carsten Bockelmann, Armin Dekorsy","With increasing complexity of modern communication systems, machine learning
algorithms have become a focal point of research. However, performance demands
have tightened in parallel to complexity. For some of the key applications
targeted by future wireless, such as the medical field, strict and reliable
performance guarantees are essential, but vanilla machine learning methods have
been shown to struggle with these types of requirements. Therefore, the
question is raised whether these methods can be extended to better deal with
the demands imposed by such applications. In this paper, we look at a
combinatorial resource allocation challenge with rare, significant events which
must be handled properly. We propose to treat this as a multi-task learning
problem, select two methods from this domain, Elastic Weight Consolidation and
Gradient Episodic Memory, and integrate them into a vanilla actor-critic
scheduler. We compare their performance in dealing with Black Swan Events with
the state-of-the-art of augmenting the training data distribution and report
that the multi-task approach proves highly effective.",2304.12660v1,https://arxiv.org/pdf/2304.12660v1
"A New Inexact Proximal Linear Algorithm with Adaptive Stopping Criteria
  for Robust Phase Retrieval","Zhong Zheng, Shiqian Ma, Lingzhou Xue","This paper considers the robust phase retrieval problem, which can be cast as
a nonsmooth and nonconvex optimization problem. We propose a new inexact
proximal linear algorithm with the subproblem being solved inexactly. Our
contributions are two adaptive stopping criteria for the subproblem. The
convergence behavior of the proposed methods is analyzed. Through experiments
on both synthetic and real datasets, we demonstrate that our methods are much
more efficient than existing methods, such as the original proximal linear
algorithm and the subgradient method.",2304.12522v2,https://arxiv.org/pdf/2304.12522v2
Evaluating Adversarial Robustness on Document Image Classification,"Timothée Fronteau, Arnaud Paran, Aymen Shabou","Adversarial attacks and defenses have gained increasing interest on computer
vision systems in recent years, but as of today, most investigations are
limited to images. However, many artificial intelligence models actually handle
documentary data, which is very different from real world images. Hence, in
this work, we try to apply the adversarial attack philosophy on documentary and
natural data and to protect models against such attacks. We focus our work on
untargeted gradient-based, transfer-based and score-based attacks and evaluate
the impact of adversarial training, JPEG input compression and grey-scale input
transformation on the robustness of ResNet50 and EfficientNetB0 model
architectures. To the best of our knowledge, no such work has been conducted by
the community in order to study the impact of these attacks on the document
image classification task.",2304.12486v2,https://arxiv.org/pdf/2304.12486v2
Generating robust counterfactual explanations,"Victor Guyomard, Françoise Fessant, Thomas Guyet, Tassadit Bouadi, Alexandre Termier","Counterfactual explanations have become a mainstay of the XAI field. This
particularly intuitive statement allows the user to understand what small but
necessary changes would have to be made to a given situation in order to change
a model prediction. The quality of a counterfactual depends on several
criteria: realism, actionability, validity, robustness, etc. In this paper, we
are interested in the notion of robustness of a counterfactual. More precisely,
we focus on robustness to counterfactual input changes. This form of robustness
is particularly challenging as it involves a trade-off between the robustness
of the counterfactual and the proximity with the example to explain. We propose
a new framework, CROCO, that generates robust counterfactuals while managing
effectively this trade-off, and guarantees the user a minimal robustness. An
empirical evaluation on tabular datasets confirms the relevance and
effectiveness of our approach.",2304.12943v1,https://arxiv.org/pdf/2304.12943v1
"Robust Tickets Can Transfer Better: Drawing More Transferable
  Subnetworks in Transfer Learning","Yonggan Fu, Ye Yuan, Shang Wu, Jiayi Yuan, Yingyan Lin","Transfer learning leverages feature representations of deep neural networks
(DNNs) pretrained on source tasks with rich data to empower effective
finetuning on downstream tasks. However, the pretrained models are often
prohibitively large for delivering generalizable representations, which limits
their deployment on edge devices with constrained resources. To close this gap,
we propose a new transfer learning pipeline, which leverages our finding that
robust tickets can transfer better, i.e., subnetworks drawn with properly
induced adversarial robustness can win better transferability over vanilla
lottery ticket subnetworks. Extensive experiments and ablation studies validate
that our proposed transfer learning pipeline can achieve enhanced
accuracy-sparsity trade-offs across both diverse downstream tasks and sparsity
patterns, further enriching the lottery ticket hypothesis.",2304.11834v2,https://arxiv.org/pdf/2304.11834v2
Robust and differentially private stochastic linear bandits,"Vasileios Charisopoulos, Hossein Esfandiari, Vahab Mirrokni","In this paper, we study the stochastic linear bandit problem under the
additional requirements of differential privacy, robustness and batched
observations. In particular, we assume an adversary randomly chooses a constant
fraction of the observed rewards in each batch, replacing them with arbitrary
numbers. We present differentially private and robust variants of the arm
elimination algorithm using logarithmic batch queries under two privacy models
and provide regret bounds in both settings. In the first model, every reward in
each round is reported by a potentially different client, which reduces to
standard local differential privacy (LDP). In the second model, every action is
""owned"" by a different client, who may aggregate the rewards over multiple
queries and privatize the aggregate response instead. To the best of our
knowledge, our algorithms are the first simultaneously providing differential
privacy and adversarial robustness in the stochastic linear bandits problem.",2304.11741v1,https://arxiv.org/pdf/2304.11741v1
Benchmarking Low-Shot Robustness to Natural Distribution Shifts,"Aaditya Singh, Kartik Sarangmath, Prithvijit Chattopadhyay, Judy Hoffman","Robustness to natural distribution shifts has seen remarkable progress thanks
to recent pre-training strategies combined with better fine-tuning methods.
However, such fine-tuning assumes access to large amounts of labelled data, and
the extent to which the observations hold when the amount of training data is
not as high remains unknown. We address this gap by performing the first
in-depth study of robustness to various natural distribution shifts in
different low-shot regimes: spanning datasets, architectures, pre-trained
initializations, and state-of-the-art robustness interventions. Most
importantly, we find that there is no single model of choice that is often more
robust than others, and existing interventions can fail to improve robustness
on some datasets even if they do so in the full-shot regime. We hope that our
work will motivate the community to focus on this problem of practical
importance.",2304.11263v2,https://arxiv.org/pdf/2304.11263v2
"RSBA: Robust Statistical Backdoor Attack under Privilege-Constrained
  Scenarios","Xiaolei Liu, Ming Yi, Kangyi Ding, Bangzhou Xin, Yixiao Xu, Li Yan, Chao Shen","Learning-based systems have been demonstrated to be vulnerable to backdoor
attacks, wherein malicious users manipulate model performance by injecting
backdoors into the target model and activating them with specific triggers.
Previous backdoor attack methods primarily focused on two key metrics: attack
success rate and stealthiness. However, these methods often necessitate
significant privileges over the target model, such as control over the training
process, making them challenging to implement in real-world scenarios.
Moreover, the robustness of existing backdoor attacks is not guaranteed, as
they prove sensitive to defenses such as image augmentations and model
distillation. In this paper, we address these two limitations and introduce
RSBA (Robust Statistical Backdoor Attack under Privilege-constrained
Scenarios). The key insight of RSBA is that statistical features can naturally
divide images into different groups, offering a potential implementation of
triggers. This type of trigger is more robust than manually designed ones, as
it is widely distributed in normal images. By leveraging these statistical
triggers, RSBA enables attackers to conduct black-box attacks by solely
poisoning the labels or the images. We empirically and theoretically
demonstrate the robustness of RSBA against image augmentations and model
distillation. Experimental results show that RSBA achieves a 99.83\% attack
success rate in black-box scenarios. Remarkably, it maintains a high success
rate even after model distillation, where attackers lack access to the training
dataset of the student model (1.39\% success rate for baseline methods on
average).",2304.10985v2,https://arxiv.org/pdf/2304.10985v2
"Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild","Yikai Wang, Zheyuan Jiang, Jianyu Chen","Recently, reinforcement learning has become a promising and polular solution
for robot legged locomotion. Compared to model-based control, reinforcement
learning based controllers can achieve better robustness against uncertainties
of environments through sim-to-real learning. However, the corresponding
learned gaits are in general overly conservative and unatural. In this paper,
we propose a new framework for learning robust, agile and natural legged
locomotion skills over challenging terrain. We incorporate an adversarial
training branch based on real animal locomotion data upon a teacher-student
training pipeline for robust sim-to-real transfer. Empirical results on both
simulation and real world of a quadruped robot demonstrate that our proposed
algorithm enables robustly traversing challenging terrains such as stairs,
rocky ground and slippery floor with only proprioceptive perception. Meanwhile,
the gaits are more agile, natural, and energy efficient compared to the
baselines. Both qualitative and quantitative results are presented in this
paper.",2304.10888v3,https://arxiv.org/pdf/2304.10888v3
RPLKG: Robust Prompt Learning with Knowledge Graph,"Yewon Kim, YongTaek Lim, Dokyung Yoon, KyungWoo Song","Large-scale pre-trained models have been known that they are transferable,
and they generalize well on the unseen dataset. Recently, multimodal
pre-trained models such as CLIP show significant performance improvement in
diverse experiments. However, when the labeled dataset is limited, the
generalization of a new dataset or domain is still challenging. To improve the
generalization performance on few-shot learning, there have been diverse
efforts, such as prompt learning and adapter. However, the current few-shot
adaptation methods are not interpretable, and they require a high computation
cost for adaptation. In this study, we propose a new method, robust prompt
learning with knowledge graph (RPLKG). Based on the knowledge graph, we
automatically design diverse interpretable and meaningful prompt sets. Our
model obtains cached embeddings of prompt sets after one forwarding from a
large pre-trained model. After that, model optimizes the prompt selection
processes with GumbelSoftmax. In this way, our model is trained using
relatively little memory and learning time. Also, RPLKG selects the optimal
interpretable prompt automatically, depending on the dataset. In summary, RPLKG
is i) interpretable, ii) requires small computation resources, and iii) easy to
incorporate prior human knowledge. To validate the RPLKG, we provide
comprehensive experimental results on few-shot learning, domain generalization
and new class generalization setting. RPLKG shows a significant performance
improvement compared to zero-shot learning and competitive performance against
several prompt learning methods using much lower resources.",2304.10805v1,https://arxiv.org/pdf/2304.10805v1
"DEIR: Efficient and Robust Exploration through
  Discriminative-Model-Based Episodic Intrinsic Rewards","Shanchuan Wan, Yujin Tang, Yingtao Tian, Tomoyuki Kaneko","Exploration is a fundamental aspect of reinforcement learning (RL), and its
effectiveness is a deciding factor in the performance of RL algorithms,
especially when facing sparse extrinsic rewards. Recent studies have shown the
effectiveness of encouraging exploration with intrinsic rewards estimated from
novelties in observations. However, there is a gap between the novelty of an
observation and an exploration, as both the stochasticity in the environment
and the agent's behavior may affect the observation. To evaluate exploratory
behaviors accurately, we propose DEIR, a novel method in which we theoretically
derive an intrinsic reward with a conditional mutual information term that
principally scales with the novelty contributed by agent explorations, and then
implement the reward with a discriminative forward model. Extensive experiments
on both standard and advanced exploration tasks in MiniGrid show that DEIR
quickly learns a better policy than the baselines. Our evaluations on ProcGen
demonstrate both the generalization capability and the general applicability of
our intrinsic reward. Our source code is available at
https://github.com/swan-utokyo/deir.",2304.10770v2,https://arxiv.org/pdf/2304.10770v2
"Missing Modality Robustness in Semi-Supervised Multi-Modal Semantic
  Segmentation","Harsh Maheshwari, Yen-Cheng Liu, Zsolt Kira","Using multiple spatial modalities has been proven helpful in improving
semantic segmentation performance. However, there are several real-world
challenges that have yet to be addressed: (a) improving label efficiency and
(b) enhancing robustness in realistic scenarios where modalities are missing at
the test time. To address these challenges, we first propose a simple yet
efficient multi-modal fusion mechanism Linear Fusion, that performs better than
the state-of-the-art multi-modal models even with limited supervision. Second,
we propose M3L: Multi-modal Teacher for Masked Modality Learning, a
semi-supervised framework that not only improves the multi-modal performance
but also makes the model robust to the realistic missing modality scenario
using unlabeled data. We create the first benchmark for semi-supervised
multi-modal semantic segmentation and also report the robustness to missing
modalities. Our proposal shows an absolute improvement of up to 10% on robust
mIoU above the most competitive baselines. Our code is available at
https://github.com/harshm121/M3L",2304.10756v1,https://arxiv.org/pdf/2304.10756v1
Interpretable and Robust AI in EEG Systems: A Survey,"Xinliang Zhou, Chenyu Liu, Liming Zhai, Ziyu Jia, Cuntai Guan, Yang Liu","The close coupling of artificial intelligence (AI) and electroencephalography
(EEG) has substantially advanced human-computer interaction (HCI) technologies
in the AI era. Different from traditional EEG systems, the interpretability and
robustness of AI-based EEG systems are becoming particularly crucial. The
interpretability clarifies the inner working mechanisms of AI models and thus
can gain the trust of users. The robustness reflects the AI's reliability
against attacks and perturbations, which is essential for sensitive and fragile
EEG signals. Thus the interpretability and robustness of AI in EEG systems have
attracted increasing attention, and their research has achieved great progress
recently. However, there is still no survey covering recent advances in this
field. In this paper, we present the first comprehensive survey and summarize
the interpretable and robust AI techniques for EEG systems. Specifically, we
first propose a taxonomy of interpretability by characterizing it into three
types: backpropagation, perturbation, and inherently interpretable methods.
Then we classify the robustness mechanisms into four classes: noise and
artifacts, human variability, data acquisition instability, and adversarial
attacks. Finally, we identify several critical and unresolved challenges for
interpretable and robust AI in EEG systems and further discuss their future
directions.",2304.10755v2,https://arxiv.org/pdf/2304.10755v2
"RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text
  Matching Models","Seulki Park, Daeho Um, Hajung Yoon, Sanghyuk Chun, Sangdoo Yun, Jin Young Choi","In this paper, we propose a robustness benchmark for image-text matching
models to assess their vulnerabilities. To this end, we insert adversarial
texts and images into the search pool (i.e., gallery set) and evaluate models
with the adversarial data. Specifically, we replace a word in the text to
change the meaning of the text and mix images with different images to create
perceptible changes in pixels. We assume that such explicit alterations would
not deceive a robust model, as they should understand the holistic meaning of
texts and images simultaneously. However, in our evaluations on the proposed
benchmark, many state-of-the-art models show significant performance
degradation, e.g., Recall@1: 81.9% $\rightarrow$ 64.5% in BLIP, 66.1%
$\rightarrow$ 37.5% in VSE$\infty$, where the models favor adversarial
texts/images over the original ones. This reveals the current vision-language
models may not account for subtle changes or understand the overall context of
texts and images. Our findings can provide insights for improving the
robustness of the vision-language models and devising more diverse stress-test
methods in cross-modal retrieval task. Source code and dataset will be
available at https://github.com/pseulki/rococo.",2304.10727v2,https://arxiv.org/pdf/2304.10727v2
"Granular-ball computing: an efficient, robust, and interpretable
  adaptive multi-granularity representation and computation method","Shuyin Xia, Guoyin Wang, Xinbo Gao, Xiaoyu Lian","Human cognition operates on a ""Global-first"" cognitive mechanism,
prioritizing information processing based on coarse-grained details. This
mechanism inherently possesses an adaptive multi-granularity description
capacity, resulting in computational traits such as efficiency, robustness, and
interpretability. The analysis pattern reliance on the finest granularity and
single-granularity makes most existing computational methods less efficient,
robust, and interpretable, which is an important reason for the current lack of
interpretability in neural networks. Multi-granularity granular-ball computing
employs granular-balls of varying sizes to daptively represent and envelop the
sample space, facilitating learning based on these granular-balls. Given that
the number of coarse-grained ""granular-balls"" is fewer than sample points,
granular-ball computing proves more efficient. Moreover, the inherent
coarse-grained nature of granular-balls reduces susceptibility to fine-grained
sample disturbances, enhancing robustness. The multi-granularity construct of
granular-balls generates topological structures and coarse-grained
descriptions, naturally augmenting interpretability. Granular-ball computing
has successfully ventured into diverse AI domains, fostering the development of
innovative theoretical methods, including granular-ball classifiers, clustering
techniques, neural networks, rough sets, and evolutionary computing. This has
notably ameliorated the efficiency, noise robustness, and interpretability of
traditional methods. Overall, granular-ball computing is a rare and innovative
theoretical approach in AI that can adaptively and simultaneously enhance
efficiency, robustness, and interpretability. This article delves into the main
application landscapes for granular-ball computing, aiming to equip future
researchers with references and insights to refine and expand this promising
theory.",2304.11171v4,https://arxiv.org/pdf/2304.11171v4
Certified Adversarial Robustness Within Multiple Perturbation Bounds,"Soumalya Nandi, Sravanti Addepalli, Harsh Rangwani, R. Venkatesh Babu","Randomized smoothing (RS) is a well known certified defense against
adversarial attacks, which creates a smoothed classifier by predicting the most
likely class under random noise perturbations of inputs during inference. While
initial work focused on robustness to $\ell_2$ norm perturbations using noise
sampled from a Gaussian distribution, subsequent works have shown that
different noise distributions can result in robustness to other $\ell_p$ norm
bounds as well. In general, a specific noise distribution is optimal for
defending against a given $\ell_p$ norm based attack. In this work, we aim to
improve the certified adversarial robustness against multiple perturbation
bounds simultaneously. Towards this, we firstly present a novel
\textit{certification scheme}, that effectively combines the certificates
obtained using different noise distributions to obtain optimal results against
multiple perturbation bounds. We further propose a novel \textit{training noise
distribution} along with a \textit{regularized training scheme} to improve the
certification within both $\ell_1$ and $\ell_2$ perturbation norms
simultaneously. Contrary to prior works, we compare the certified robustness of
different training algorithms across the same natural (clean) accuracy, rather
than across fixed noise levels used for training and certification. We also
empirically invalidate the argument that training and certifying the classifier
with the same amount of noise gives the best results. The proposed approach
achieves improvements on the ACR (Average Certified Radius) metric across both
$\ell_1$ and $\ell_2$ perturbation bounds.",2304.10446v1,https://arxiv.org/pdf/2304.10446v1
Using Z3 for Formal Modeling and Verification of FNN Global Robustness,"Yihao Zhang, Zeming Wei, Xiyue Zhang, Meng Sun","While Feedforward Neural Networks (FNNs) have achieved remarkable success in
various tasks, they are vulnerable to adversarial examples. Several techniques
have been developed to verify the adversarial robustness of FNNs, but most of
them focus on robustness verification against the local perturbation
neighborhood of a single data point. There is still a large research gap in
global robustness analysis. The global-robustness verifiable framework
DeepGlobal has been proposed to identify \textit{all} possible Adversarial
Dangerous Regions (ADRs) of FNNs, not limited to data samples in a test set. In
this paper, we propose a complete specification and implementation of
DeepGlobal utilizing the SMT solver Z3 for more explicit definition, and
propose several improvements to DeepGlobal for more efficient verification. To
evaluate the effectiveness of our implementation and improvements, we conduct
extensive experiments on a set of benchmark datasets. Visualization of our
experiment results shows the validity and effectiveness of the approach.",2304.10558v2,https://arxiv.org/pdf/2304.10558v2
Robust nonlinear set-point control with reinforcement learning,"Ruoqi Zhang, Per Mattsson, Torbjörn Wigren","There has recently been an increased interest in reinforcement learning for
nonlinear control problems. However standard reinforcement learning algorithms
can often struggle even on seemingly simple set-point control problems. This
paper argues that three ideas can improve reinforcement learning methods even
for highly nonlinear set-point control problems: 1) Make use of a prior
feedback controller to aid amplitude exploration. 2) Use integrated errors. 3)
Train on model ensembles. Together these ideas lead to more efficient training,
and a trained set-point controller that is more robust to modelling errors and
thus can be directly deployed to real-world nonlinear systems. The claim is
supported by experiments with a real-world nonlinear cascaded tank process and
a simulated strongly nonlinear pH-control system.",2304.10277v1,https://arxiv.org/pdf/2304.10277v1
Robust Deep Reinforcement Learning Scheduling via Weight Anchoring,"Steffen Gracla, Edgar Beck, Carsten Bockelmann, Armin Dekorsy","Questions remain on the robustness of data-driven learning methods when
crossing the gap from simulation to reality. We utilize weight anchoring, a
method known from continual learning, to cultivate and fixate desired behavior
in Neural Networks. Weight anchoring may be used to find a solution to a
learning problem that is nearby the solution of another learning problem.
Thereby, learning can be carried out in optimal environments without neglecting
or unlearning desired behavior. We demonstrate this approach on the example of
learning mixed QoS-efficient discrete resource scheduling with infrequent
priority messages. Results show that this method provides performance
comparable to the state of the art of augmenting a simulation environment,
alongside significantly increased robustness and steerability.",2304.10176v1,https://arxiv.org/pdf/2304.10176v1
Optimality of Robust Online Learning,"Zheng-Chu Guo, Andreas Christmann, Lei Shi","In this paper, we study an online learning algorithm with a robust loss
function $\mathcal{L}_{\sigma}$ for regression over a reproducing kernel
Hilbert space (RKHS). The loss function $\mathcal{L}_{\sigma}$ involving a
scaling parameter $\sigma>0$ can cover a wide range of commonly used robust
losses. The proposed algorithm is then a robust alternative for online least
squares regression aiming to estimate the conditional mean function. For
properly chosen $\sigma$ and step size, we show that the last iterate of this
online algorithm can achieve optimal capacity independent convergence in the
mean square distance. Moreover, if additional information on the underlying
function space is known, we also establish optimal capacity dependent rates for
strong convergence in RKHS. To the best of our knowledge, both of the two
results are new to the existing literature of online learning.",2304.10060v1,https://arxiv.org/pdf/2304.10060v1
"An XAI framework for robust and transparent data-driven wind turbine
  power curve models","Simon Letzgus, Klaus-Robert Müller","Wind turbine power curve models translate ambient conditions into turbine
power output. They are essential for energy yield prediction and turbine
performance monitoring. In recent years, increasingly complex machine learning
methods have become state-of-the-art for this task. Nevertheless, they
frequently encounter criticism due to their apparent lack of transparency,
which raises concerns regarding their performance in non-stationary
environments, such as those faced by wind turbines. We, therefore, introduce an
explainable artificial intelligence (XAI) framework to investigate and validate
strategies learned by data-driven power curve models from operational wind
turbine data. With the help of simple, physics-informed baseline models it
enables an automated evaluation of machine learning models beyond standard
error metrics. Alongside this novel tool, we present its efficacy for a more
informed model selection. We show, for instance, that learned strategies can be
meaningful indicators for a model's generalization ability in addition to test
set errors, especially when only little data is available. Moreover, the
approach facilitates an understanding of how decisions along the machine
learning pipeline, such as data selection, pre-processing, or training
parameters, affect learned strategies. In a practical example, we demonstrate
the framework's utilisation to obtain more physically meaningful models, a
prerequisite not only for robustness but also for insights into turbine
operation by domain experts. The latter, we demonstrate in the context of wind
turbine performance monitoring. Alongside this paper, we publish a Python
implementation of the presented framework and hope this can guide researchers
and practitioners alike toward training, selecting and utilizing more
transparent and robust data-driven wind turbine power curve models.",2304.09835v2,https://arxiv.org/pdf/2304.09835v2
"GREAT Score: Global Robustness Evaluation of Adversarial Perturbation
  using Generative Models","Zaitang Li, Pin-Yu Chen, Tsung-Yi Ho","Current studies on adversarial robustness mainly focus on aggregating local
robustness results from a set of data samples to evaluate and rank different
models. However, the local statistics may not well represent the true global
robustness of the underlying unknown data distribution. To address this
challenge, this paper makes the first attempt to present a new framework,
called GREAT Score , for global robustness evaluation of adversarial
perturbation using generative models. Formally, GREAT Score carries the
physical meaning of a global statistic capturing a mean certified attack-proof
perturbation level over all samples drawn from a generative model. For
finite-sample evaluation, we also derive a probabilistic guarantee on the
sample complexity and the difference between the sample mean and the true mean.
GREAT Score has several advantages: (1) Robustness evaluations using GREAT
Score are efficient and scalable to large models, by sparing the need of
running adversarial attacks. In particular, we show high correlation and
significantly reduced computation cost of GREAT Score when compared to the
attack-based model ranking on RobustBench (Croce,et. al. 2021). (2) The use of
generative models facilitates the approximation of the unknown data
distribution. In our ablation study with different generative adversarial
networks (GANs), we observe consistency between global robustness evaluation
and the quality of GANs. (3) GREAT Score can be used for remote auditing of
privacy-sensitive black-box models, as demonstrated by our robustness
evaluation on several online facial recognition services.",2304.09875v2,https://arxiv.org/pdf/2304.09875v2
Wavelets Beat Monkeys at Adversarial Robustness,"Jingtong Su, Julia Kempe","Research on improving the robustness of neural networks to adversarial noise
- imperceptible malicious perturbations of the data - has received significant
attention. The currently uncontested state-of-the-art defense to obtain robust
deep neural networks is Adversarial Training (AT), but it consumes
significantly more resources compared to standard training and trades off
accuracy for robustness. An inspiring recent work [Dapello et al.] aims to
bring neurobiological tools to the question: How can we develop Neural Nets
that robustly generalize like human vision? [Dapello et al.] design a network
structure with a neural hidden first layer that mimics the primate primary
visual cortex (V1), followed by a back-end structure adapted from current CNN
vision models. It seems to achieve non-trivial adversarial robustness on
standard vision benchmarks when tested on small perturbations. Here we revisit
this biologically inspired work, and ask whether a principled parameter-free
representation with inspiration from physics is able to achieve the same goal.
We discover that the wavelet scattering transform can replace the complex
V1-cortex and simple uniform Gaussian noise can take the role of neural
stochasticity, to achieve adversarial robustness. In extensive experiments on
the CIFAR-10 benchmark with adaptive adversarial attacks we show that: 1)
Robustness of VOneBlock architectures is relatively weak (though non-zero) when
the strength of the adversarial attack radius is set to commonly used
benchmarks. 2) Replacing the front-end VOneBlock by an off-the-shelf
parameter-free Scatternet followed by simple uniform Gaussian noise can achieve
much more substantial adversarial robustness without adversarial training. Our
work shows how physically inspired structures yield new insights into
robustness that were previously only thought possible by meticulously mimicking
the human cortex.",2304.09403v1,https://arxiv.org/pdf/2304.09403v1
The Adaptive $τ$-Lasso: Robustness and Oracle Properties,"Emadaldin Mozafari-Majd, Visa Koivunen","This paper introduces a new regularized version of the robust
$\tau$-regression estimator for analyzing high-dimensional datasets subject to
gross contamination in the response variables and covariates (explanatory
variables). The resulting estimator, termed adaptive $\tau$-Lasso, is robust to
outliers and high-leverage points. It also incorporates an adaptive
$\ell_1$-norm penalty term, which enables the selection of relevant variables
and reduces the bias associated with large true regression coefficients. More
specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each
regression coefficient. For a fixed number of predictors $p$, we show that the
adaptive $\tau$-Lasso has the oracle property, ensuring both variable-selection
consistency and asymptotic normality. Asymptotic normality applies only to the
entries of the regression vector corresponding to the true support, assuming
knowledge of the true regression vector support. We characterize its robustness
by establishing the finite-sample breakdown point and the influence function.
We carry out extensive simulations and observe that the class of $\tau$-Lasso
estimators exhibits robustness and reliable performance in both contaminated
and uncontaminated data settings. We also validate our theoretical findings on
robustness properties through simulations. In the face of outliers and
high-leverage points, the adaptive $\tau$-Lasso and $\tau$-Lasso estimators
achieve the best performance or close-to-best performance in terms of
prediction and variable selection accuracy compared to other competing
regularized estimators for all scenarios considered in this study. Therefore,
the adaptive $\tau$-Lasso and $\tau$-Lasso estimators provide attractive tools
for a variety of sparse linear regression problems, particularly in
high-dimensional settings and when the data is contaminated by outliers and
high-leverage points.",2304.09310v3,https://arxiv.org/pdf/2304.09310v3
"Variational Relational Point Completion Network for Robust 3D
  Classification","Liang Pan, Xinyi Chen, Zhongang Cai, Junzhe Zhang, Haiyu Zhao, Shuai Yi, Ziwei Liu","Real-scanned point clouds are often incomplete due to viewpoint, occlusion,
and noise, which hampers 3D geometric modeling and perception. Existing point
cloud completion methods tend to generate global shape skeletons and hence lack
fine local details. Furthermore, they mostly learn a deterministic
partial-to-complete mapping, but overlook structural relations in man-made
objects. To tackle these challenges, this paper proposes a variational
framework, Variational Relational point Completion Network (VRCNet) with two
appealing properties: 1) Probabilistic Modeling. In particular, we propose a
dual-path architecture to enable principled probabilistic modeling across
partial and complete clouds. One path consumes complete point clouds for
reconstruction by learning a point VAE. The other path generates complete
shapes for partial point clouds, whose embedded distribution is guided by
distribution obtained from the reconstruction path during training. 2)
Relational Enhancement. Specifically, we carefully design point self-attention
kernel and point selective kernel module to exploit relational point features,
which refines local shape details conditioned on the coarse completion. In
addition, we contribute multi-view partial point cloud datasets (MVP and MVP-40
dataset) containing over 200,000 high-quality scans, which render partial 3D
shapes from 26 uniformly distributed camera poses for each 3D CAD model.
Extensive experiments demonstrate that VRCNet outperforms state-of-the-art
methods on all standard point cloud completion benchmarks. Notably, VRCNet
shows great generalizability and robustness on real-world point cloud scans.
Moreover, we can achieve robust 3D classification for partial point clouds with
the help of VRCNet, which can highly increase classification accuracy.",2304.09131v1,https://arxiv.org/pdf/2304.09131v1
Robustness of Visual Explanations to Common Data Augmentation,"Lenka Tětková, Lars Kai Hansen","As the use of deep neural networks continues to grow, understanding their
behaviour has become more crucial than ever. Post-hoc explainability methods
are a potential solution, but their reliability is being called into question.
Our research investigates the response of post-hoc visual explanations to
naturally occurring transformations, often referred to as augmentations. We
anticipate explanations to be invariant under certain transformations, such as
changes to the colour map while responding in an equivariant manner to
transformations like translation, object scaling, and rotation. We have found
remarkable differences in robustness depending on the type of transformation,
with some explainability methods (such as LRP composites and Guided Backprop)
being more stable than others. We also explore the role of training with data
augmentation. We provide evidence that explanations are typically less robust
to augmentation than classification performance, regardless of whether data
augmentation is used in training or not.",2304.08984v1,https://arxiv.org/pdf/2304.08984v1
"A Domain-Region Based Evaluation of ML Performance Robustness to
  Covariate Shift","Firas Bayram, Bestoun S. Ahmed","Most machine learning methods assume that the input data distribution is the
same in the training and testing phases. However, in practice, this
stationarity is usually not met and the distribution of inputs differs, leading
to unexpected performance of the learned model in deployment. The issue in
which the training and test data inputs follow different probability
distributions while the input-output relationship remains unchanged is referred
to as covariate shift. In this paper, the performance of conventional machine
learning models was experimentally evaluated in the presence of covariate
shift. Furthermore, a region-based evaluation was performed by decomposing the
domain of probability density function of the input data to assess the
classifier's performance per domain region. Distributional changes were
simulated in a two-dimensional classification problem. Subsequently, a higher
four-dimensional experiments were conducted. Based on the experimental
analysis, the Random Forests algorithm is the most robust classifier in the
two-dimensional case, showing the lowest degradation rate for accuracy and
F1-score metrics, with a range between 0.1% and 2.08%. Moreover, the results
reveal that in higher-dimensional experiments, the performance of the models is
predominantly influenced by the complexity of the classification function,
leading to degradation rates exceeding 25% in most cases. It is also concluded
that the models exhibit high bias towards the region with high density in the
input space domain of the training samples.",2304.08855v1,https://arxiv.org/pdf/2304.08855v1
"RS2G: Data-Driven Scene-Graph Extraction and Embedding for Robust
  Autonomous Perception and Scenario Understanding","Junyao Wang, Arnav Vaibhav Malawade, Junhong Zhou, Shih-Yuan Yu, Mohammad Abdullah Al Faruque","Effectively capturing intricate interactions among road users is of critical
importance to achieving safe navigation for autonomous vehicles. While graph
learning (GL) has emerged as a promising approach to tackle this challenge,
existing GL models rely on predefined domain-specific graph extraction rules
that often fail in real-world drastically changing scenarios. Additionally,
these graph extraction rules severely impede the capability of existing GL
methods to generalize knowledge across domains. To address this issue, we
propose RoadScene2Graph (RS2G), an innovative autonomous scenario understanding
framework with a novel data-driven graph extraction and modeling approach that
dynamically captures the diverse relations among road users. Our evaluations
demonstrate that on average RS2G outperforms the state-of-the-art (SOTA)
rule-based graph extraction method by 4.47% and the SOTA deep learning model by
22.19% in subjective risk assessment. More importantly, RS2G delivers notably
better performance in transferring knowledge gained from simulation
environments to unseen real-world scenarios.",2304.08600v2,https://arxiv.org/pdf/2304.08600v2
SplitAMC: Split Learning for Robust Automatic Modulation Classification,"Jihoon Park, Seungeun Oh, Seong-Lyun Kim","Automatic modulation classification (AMC) is a technology that identifies a
modulation scheme without prior signal information and plays a vital role in
various applications, including cognitive radio and link adaptation. With the
development of deep learning (DL), DL-based AMC methods have emerged, while
most of them focus on reducing computational complexity in a centralized
structure. This centralized learning-based AMC (CentAMC) violates data privacy
in the aspect of direct transmission of client-side raw data. Federated
learning-based AMC (FedeAMC) can bypass this issue by exchanging model
parameters, but causes large resultant latency and client-side computational
load. Moreover, both CentAMC and FedeAMC are vulnerable to large-scale noise
occured in the wireless channel between the client and the server. To this end,
we develop a novel AMC method based on a split learning (SL) framework, coined
SplitAMC, that can achieve high accuracy even in poor channel conditions, while
guaranteeing data privacy and low latency. In SplitAMC, each client can benefit
from data privacy leakage by exchanging smashed data and its gradient instead
of raw data, and has robustness to noise with the help of high scale of smashed
data. Numerical evaluations validate that SplitAMC outperforms CentAMC and
FedeAMC in terms of accuracy for all SNRs as well as latency.",2304.12200v1,https://arxiv.org/pdf/2304.12200v1
"RNN-Guard: Certified Robustness Against Multi-frame Attacks for
  Recurrent Neural Networks","Yunruo Zhang, Tianyu Du, Shouling Ji, Peng Tang, Shanqing Guo","It is well-known that recurrent neural networks (RNNs), although widely used,
are vulnerable to adversarial attacks including one-frame attacks and
multi-frame attacks. Though a few certified defenses exist to provide
guaranteed robustness against one-frame attacks, we prove that defending
against multi-frame attacks remains a challenging problem due to their enormous
perturbation space. In this paper, we propose the first certified defense
against multi-frame attacks for RNNs called RNN-Guard. To address the above
challenge, we adopt the perturb-all-frame strategy to construct perturbation
spaces consistent with those in multi-frame attacks. However, the
perturb-all-frame strategy causes a precision issue in linear relaxations. To
address this issue, we introduce a novel abstract domain called InterZono and
design tighter relaxations. We prove that InterZono is more precise than
Zonotope yet carries the same time complexity. Experimental evaluations across
various datasets and model structures show that the certified robust accuracy
calculated by RNN-Guard with InterZono is up to 2.18 times higher than that
with Zonotope. In addition, we extend RNN-Guard as the first certified training
method against multi-frame attacks to directly enhance RNNs' robustness. The
results show that the certified robust accuracy of models trained with
RNN-Guard against multi-frame attacks is 15.47 to 67.65 percentage points
higher than those with other training methods.",2304.07980v1,https://arxiv.org/pdf/2304.07980v1
"Robust Educational Dialogue Act Classifiers with Low-Resource and
  Imbalanced Datasets","Jionghao Lin, Wei Tan, Ngoc Dang Nguyen, David Lang, Lan Du, Wray Buntine, Richard Beare, Guanliang Chen, Dragan Gasevic","Dialogue acts (DAs) can represent conversational actions of tutors or
students that take place during tutoring dialogues. Automating the
identification of DAs in tutoring dialogues is significant to the design of
dialogue-based intelligent tutoring systems. Many prior studies employ machine
learning models to classify DAs in tutoring dialogues and invest much effort to
optimize the classification accuracy by using limited amounts of training data
(i.e., low-resource data scenario). However, beyond the classification
accuracy, the robustness of the classifier is also important, which can reflect
the capability of the classifier on learning the patterns from different class
distributions. We note that many prior studies on classifying educational DAs
employ cross entropy (CE) loss to optimize DA classifiers on low-resource data
with imbalanced DA distribution. The DA classifiers in these studies tend to
prioritize accuracy on the majority class at the expense of the minority class
which might not be robust to the data with imbalanced ratios of different DA
classes. To optimize the robustness of classifiers on imbalanced class
distributions, we propose to optimize the performance of the DA classifier by
maximizing the area under the ROC curve (AUC) score (i.e., AUC maximization).
Through extensive experiments, our study provides evidence that (i) by
maximizing AUC in the training process, the DA classifier achieves significant
performance improvement compared to the CE approach under low-resource data,
and (ii) AUC maximization approaches can improve the robustness of the DA
classifier under different class imbalance ratios.",2304.07499v1,https://arxiv.org/pdf/2304.07499v1
"Critical Sampling for Robust Evolution Operator Learning of Unknown
  Dynamical Systems","Ce Zhang, Kailiang Wu, Zhihai He","Given an unknown dynamical system, what is the minimum number of samples
needed for effective learning of its governing laws and accurate prediction of
its future evolution behavior, and how to select these critical samples? In
this work, we propose to explore this problem based on a design approach.
Starting from a small initial set of samples, we adaptively discover critical
samples to achieve increasingly accurate learning of the system evolution. One
central challenge here is that we do not know the network modeling error since
the ground-truth system state is unknown, which is however needed for critical
sampling. To address this challenge, we introduce a multi-step reciprocal
prediction network where forward and backward evolution networks are designed
to learn the temporal evolution behavior in the forward and backward time
directions, respectively. Very interestingly, we find that the desired network
modeling error is highly correlated with the multi-step reciprocal prediction
error, which can be directly computed from the current system state. This
allows us to perform a dynamic selection of critical samples from regions with
high network modeling errors for dynamical systems. Additionally, a joint
spatial-temporal evolution network is introduced which incorporates spatial
dynamics modeling into the temporal evolution prediction for robust learning of
the system evolution operator with few samples. Our extensive experimental
results demonstrate that our proposed method is able to dramatically reduce the
number of samples needed for effective learning and accurate prediction of
evolution behaviors of unknown dynamical systems by up to hundreds of times.",2304.07485v3,https://arxiv.org/pdf/2304.07485v3
"A Distributionally Robust Approach to Regret Optimal Control using the
  Wasserstein Distance","Feras Al Taha, Shuhao Yan, Eilyan Bitar","This paper proposes a distributionally robust approach to regret optimal
control of discrete-time linear dynamical systems with quadratic costs subject
to a stochastic additive disturbance on the state process. The underlying
probability distribution of the disturbance process is unknown, but assumed to
lie in a given ball of distributions defined in terms of the type-2 Wasserstein
distance. In this framework, strictly causal linear disturbance feedback
controllers are designed to minimize the worst-case expected regret. The regret
incurred by a controller is defined as the difference between the cost it
incurs in response to a realization of the disturbance process and the cost
incurred by the optimal noncausal controller which has perfect knowledge of the
disturbance process realization at the outset. Building on a well-established
duality theory for optimal transport problems, we derive a reformulation of the
minimax regret optimal control problem as a tractable semidefinite program.
Using the equivalent dual reformulation, we characterize a worst-case
distribution achieving the worst-case expected regret in relation to the
distribution at the center of the Wasserstein ball. We compare the minimax
regret optimal control design method with the distributionally robust optimal
control approach using an illustrative example and numerical experiments.",2304.06783v2,https://arxiv.org/pdf/2304.06783v2
"Evaluating the Robustness of Interpretability Methods through
  Explanation Invariance and Equivariance","Jonathan Crabbé, Mihaela van der Schaar","Interpretability methods are valuable only if their explanations faithfully
describe the explained model. In this work, we consider neural networks whose
predictions are invariant under a specific symmetry group. This includes
popular architectures, ranging from convolutional to graph neural networks. Any
explanation that faithfully explains this type of model needs to be in
agreement with this invariance property. We formalize this intuition through
the notion of explanation invariance and equivariance by leveraging the
formalism from geometric deep learning. Through this rigorous formalism, we
derive (1) two metrics to measure the robustness of any interpretability method
with respect to the model symmetry group; (2) theoretical robustness guarantees
for some popular interpretability methods and (3) a systematic approach to
increase the invariance of any interpretability method with respect to a
symmetry group. By empirically measuring our metrics for explanations of models
associated with various modalities and symmetry groups, we derive a set of 5
guidelines to allow users and developers of interpretability methods to produce
robust explanations.",2304.06715v3,https://arxiv.org/pdf/2304.06715v3
Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser,"Astha Verma, A V Subramanyam, Siddhesh Bangar, Naman Lal, Rajiv Ratn Shah, Shin'ichi Satoh","Certified defense methods against adversarial perturbations have been
recently investigated in the black-box setting with a zeroth-order (ZO)
perspective. However, these methods suffer from high model variance with low
performance on high-dimensional datasets due to the ineffective design of the
denoiser and are limited in their utilization of ZO techniques. To this end, we
propose a certified ZO preprocessing technique for removing adversarial
perturbations from the attacked image in the black-box setting using only model
queries. We propose a robust UNet denoiser (RDUNet) that ensures the robustness
of black-box models trained on high-dimensional datasets. We propose a novel
black-box denoised smoothing (DS) defense mechanism, ZO-RUDS, by prepending our
RDUNet to the black-box model, ensuring black-box defense. We further propose
ZO-AE-RUDS in which RDUNet followed by autoencoder (AE) is prepended to the
black-box model. We perform extensive experiments on four classification
datasets, CIFAR-10, CIFAR-10, Tiny Imagenet, STL-10, and the MNIST dataset for
image reconstruction tasks. Our proposed defense methods ZO-RUDS and ZO-AE-RUDS
beat SOTA with a huge margin of $35\%$ and $9\%$, for low dimensional
(CIFAR-10) and with a margin of $20.61\%$ and $23.51\%$ for high-dimensional
(STL-10) datasets, respectively.",2304.06430v2,https://arxiv.org/pdf/2304.06430v2
"Learning Robust and Correct Controllers from Signal Temporal Logic
  Specifications Using BarrierNet","Wenliang Liu, Wei Xiao, Calin Belta","In this paper, we consider the problem of learning a neural network
controller for a system required to satisfy a Signal Temporal Logic (STL)
specification. We exploit STL quantitative semantics to define a notion of
robust satisfaction. Guaranteeing the correctness of a neural network
controller, i.e., ensuring the satisfaction of the specification by the
controlled system, is a difficult problem that received a lot of attention
recently. We provide a general procedure to construct a set of trainable High
Order Control Barrier Functions (HOCBFs) enforcing the satisfaction of formulas
in a fragment of STL. We use the BarrierNet, implemented by a differentiable
Quadratic Program (dQP) with HOCBF constraints, as the last layer of the neural
network controller, to guarantee the satisfaction of the STL formulas. We train
the HOCBFs together with other neural network parameters to further improve the
robustness of the controller. Simulation results demonstrate that our approach
ensures satisfaction and outperforms existing algorithms.",2304.06160v1,https://arxiv.org/pdf/2304.06160v1
"Towards More Robust and Accurate Sequential Recommendation with
  Cascade-guided Adversarial Training","Juntao Tan, Shelby Heinecke, Zhiwei Liu, Yongjun Chen, Yongfeng Zhang, Huan Wang","Sequential recommendation models, models that learn from chronological
user-item interactions, outperform traditional recommendation models in many
settings. Despite the success of sequential recommendation models, their
robustness has recently come into question. Two properties unique to the nature
of sequential recommendation models may impair their robustness - the cascade
effects induced during training and the model's tendency to rely too heavily on
temporal information. To address these vulnerabilities, we propose
Cascade-guided Adversarial training, a new adversarial training procedure that
is specifically designed for sequential recommendation models. Our approach
harnesses the intrinsic cascade effects present in sequential modeling to
produce strategic adversarial perturbations to item embeddings during training.
Experiments on training state-of-the-art sequential models on four public
datasets from different domains show that our training approach produces
superior model ranking accuracy and superior model robustness to real item
replacement perturbations when compared to both standard model training and
generic adversarial training.",2304.05492v2,https://arxiv.org/pdf/2304.05492v2
"RELS-DQN: A Robust and Efficient Local Search Framework for
  Combinatorial Optimization","Yuanhang Shao, Tonmoy Dey, Nikola Vuckovic, Luke Van Popering, Alan Kuhnle","Combinatorial optimization (CO) aims to efficiently find the best solution to
NP-hard problems ranging from statistical physics to social media marketing. A
wide range of CO applications can benefit from local search methods because
they allow reversible action over greedy policies. Deep Q-learning (DQN) using
message-passing neural networks (MPNN) has shown promise in replicating the
local search behavior and obtaining comparable results to the local search
algorithms. However, the over-smoothing and the information loss during the
iterations of message passing limit its robustness across applications, and the
large message vectors result in memory inefficiency. Our paper introduces
RELS-DQN, a lightweight DQN framework that exhibits the local search behavior
while providing practical scalability. Using the RELS-DQN model trained on one
application, it can generalize to various applications by providing solution
values higher than or equal to both the local search algorithms and the
existing DQN models while remaining efficient in runtime and memory.",2304.06048v1,https://arxiv.org/pdf/2304.06048v1
"Selecting Robust Features for Machine Learning Applications using
  Multidata Causal Discovery","Saranya Ganesh S., Tom Beucler, Frederick Iat-Hin Tam, Milton S. Gomez, Jakob Runge, Andreas Gerhardus","Robust feature selection is vital for creating reliable and interpretable
Machine Learning (ML) models. When designing statistical prediction models in
cases where domain knowledge is limited and underlying interactions are
unknown, choosing the optimal set of features is often difficult. To mitigate
this issue, we introduce a Multidata (M) causal feature selection approach that
simultaneously processes an ensemble of time series datasets and produces a
single set of causal drivers. This approach uses the causal discovery
algorithms PC1 or PCMCI that are implemented in the Tigramite Python package.
These algorithms utilize conditional independence tests to infer parts of the
causal graph. Our causal feature selection approach filters out
causally-spurious links before passing the remaining causal features as inputs
to ML models (Multiple linear regression, Random Forest) that predict the
targets. We apply our framework to the statistical intensity prediction of
Western Pacific Tropical Cyclones (TC), for which it is often difficult to
accurately choose drivers and their dimensionality reduction (time lags,
vertical levels, and area-averaging). Using more stringent significance
thresholds in the conditional independence tests helps eliminate spurious
causal relationships, thus helping the ML model generalize better to unseen TC
cases. M-PC1 with a reduced number of features outperforms M-PCMCI, non-causal
ML, and other feature selection methods (lagged correlation, random), even
slightly outperforming feature selection based on eXplainable Artificial
Intelligence. The optimal causal drivers obtained from our causal feature
selection help improve our understanding of underlying relationships and
suggest new potential drivers of TC intensification.",2304.05294v5,https://arxiv.org/pdf/2304.05294v5
"The Capacity and Robustness Trade-off: Revisiting the Channel
  Independent Strategy for Multivariate Time Series Forecasting","Lu Han, Han-Jia Ye, De-Chuan Zhan","Multivariate time series data comprises various channels of variables. The
multivariate forecasting models need to capture the relationship between the
channels to accurately predict future values. However, recently, there has been
an emergence of methods that employ the Channel Independent (CI) strategy.
These methods view multivariate time series data as separate univariate time
series and disregard the correlation between channels. Surprisingly, our
empirical results have shown that models trained with the CI strategy
outperform those trained with the Channel Dependent (CD) strategy, usually by a
significant margin. Nevertheless, the reasons behind this phenomenon have not
yet been thoroughly explored in the literature. This paper provides
comprehensive empirical and theoretical analyses of the characteristics of
multivariate time series datasets and the CI/CD strategy. Our results conclude
that the CD approach has higher capacity but often lacks robustness to
accurately predict distributionally drifted time series. In contrast, the CI
approach trades capacity for robust prediction. Practical measures inspired by
these analyses are proposed to address the capacity and robustness dilemma,
including a modified CD method called Predict Residuals with Regularization
(PRReg) that can surpass the CI strategy. We hope our findings can raise
awareness among researchers about the characteristics of multivariate time
series and inspire the construction of better forecasting models.",2304.05206v1,https://arxiv.org/pdf/2304.05206v1
"Wav2code: Restore Clean Speech Representations via Codebook Lookup for
  Noise-Robust ASR","Yuchen Hu, Chen Chen, Qiushi Zhu, Eng Siong Chng","Automatic speech recognition (ASR) has gained remarkable successes thanks to
recent advances of deep learning, but it usually degrades significantly under
real-world noisy conditions. Recent works introduce speech enhancement (SE) as
front-end to improve speech quality, which is proved effective but may not be
optimal for downstream ASR due to speech distortion problem. Based on that,
latest works combine SE and currently popular self-supervised learning (SSL) to
alleviate distortion and improve noise robustness. Despite the effectiveness,
the speech distortion caused by conventional SE still cannot be cleared out. In
this paper, we propose a self-supervised framework named Wav2code to implement
a feature-level SE with reduced distortions for noise-robust ASR. First, in
pre-training stage the clean speech representations from SSL model are sent to
lookup a discrete codebook via nearest-neighbor feature matching, the resulted
code sequence are then exploited to reconstruct the original clean
representations, in order to store them in codebook as prior. Second, during
finetuning we propose a Transformer-based code predictor to accurately predict
clean codes by modeling the global dependency of input noisy representations,
which enables discovery and restoration of high-quality clean representations
with reduced distortions. Furthermore, we propose an interactive feature fusion
network to combine original noisy and the restored clean representations to
consider both fidelity and quality, resulting in more informative features for
downstream ASR. Finally, experiments on both synthetic and real noisy datasets
demonstrate that Wav2code can solve the speech distortion and improve ASR
performance under various noisy conditions, resulting in stronger robustness.",2304.04974v3,https://arxiv.org/pdf/2304.04974v3
"Robust Dequantization of the Quantum Singular value Transformation and
  Quantum Machine Learning Algorithms",François Le Gall,"Several quantum algorithms for linear algebra problems, and in particular
quantum machine learning problems, have been ""dequantized"" in the past few
years. These dequantization results typically hold when classical algorithms
can access the data via length-squared sampling. In this work we investigate
how robust these dequantization results are. We introduce the notion of
approximate length-squared sampling, where classical algorithms are only able
to sample from a distribution close to the ideal distribution in total
variation distance. While quantum algorithms are natively robust against small
perturbations, current techniques in dequantization are not. Our main technical
contribution is showing how many techniques from randomized linear algebra can
be adapted to work under this weaker assumption as well. We then use these
techniques to show that the recent low-rank dequantization framework by Chia,
Gily\'en, Li, Lin, Tang and Wang (JACM 2022) and the dequantization framework
for sparse matrices by Gharibian and Le Gall (STOC 2022), which are both based
on the Quantum Singular Value Transformation, can be generalized to the case of
approximate length-squared sampling access to the input. We also apply these
results to obtain a robust dequantization of many quantum machine learning
algorithms, including quantum algorithms for recommendation systems, supervised
clustering and low-rank matrix inversion.",2304.04932v1,https://arxiv.org/pdf/2304.04932v1
Are Visual Recognition Models Robust to Image Compression?,"João Maria Janeiro, Stanislav Frolov, Alaaeldin El-Nouby, Jakob Verbeek","Reducing the data footprint of visual content via image compression is
essential to reduce storage requirements, but also to reduce the bandwidth and
latency requirements for transmission. In particular, the use of compressed
images allows for faster transfer of data, and faster response times for visual
recognition in edge devices that rely on cloud-based services. In this paper,
we first analyze the impact of image compression using traditional codecs, as
well as recent state-of-the-art neural compression approaches, on three visual
recognition tasks: image classification, object detection, and semantic
segmentation. We consider a wide range of compression levels, ranging from 0.1
to 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition
ability is significantly impacted when using strong compression. For example,
for segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1
bpp using the best compression model we evaluated. Second, we test to what
extent this performance drop can be ascribed to a loss of relevant information
in the compressed image, or to a lack of generalization of visual recognition
models to images with compression artefacts. We find that to a large extent the
performance loss is due to the latter: by finetuning the recognition models on
compressed training images, most of the performance loss is recovered. For
example, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%
of the original drop in accuracy.",2304.04518v1,https://arxiv.org/pdf/2304.04518v1
On Robustness in Multimodal Learning,"Brandon McKinzie, Joseph Cheng, Vaishaal Shankar, Yinfei Yang, Jonathon Shlens, Alexander Toshev","Multimodal learning is defined as learning over multiple heterogeneous input
modalities such as video, audio, and text. In this work, we are concerned with
understanding how models behave as the type of modalities differ between
training and deployment, a situation that naturally arises in many applications
of multimodal learning to hardware platforms. We present a multimodal
robustness framework to provide a systematic analysis of common multimodal
representation learning methods. Further, we identify robustness short-comings
of these approaches and propose two intervention techniques leading to
$1.5\times$-$4\times$ robustness improvements on three datasets, AudioSet,
Kinetics-400 and ImageNet-Captions. Finally, we demonstrate that these
interventions better utilize additional modalities, if present, to achieve
competitive results of $44.2$ mAP on AudioSet 20K.",2304.04385v2,https://arxiv.org/pdf/2304.04385v2
"Ensemble Modeling for Time Series Forecasting: an Adaptive Robust
  Optimization Approach","Dimitris Bertsimas, Leonard Boussioux","Accurate time series forecasting is critical for a wide range of problems
with temporal data. Ensemble modeling is a well-established technique for
leveraging multiple predictive models to increase accuracy and robustness, as
the performance of a single predictor can be highly variable due to shifts in
the underlying data distribution. This paper proposes a new methodology for
building robust ensembles of time series forecasting models. Our approach
utilizes Adaptive Robust Optimization (ARO) to construct a linear regression
ensemble in which the models' weights can adapt over time. We demonstrate the
effectiveness of our method through a series of synthetic experiments and
real-world applications, including air pollution management, energy consumption
forecasting, and tropical cyclone intensity forecasting. Our results show that
our adaptive ensembles outperform the best ensemble member in hindsight by
16-26% in root mean square error and 14-28% in conditional value at risk and
improve over competitive ensemble techniques.",2304.04308v1,https://arxiv.org/pdf/2304.04308v1
"Adversarially Robust Neural Architecture Search for Graph Neural
  Networks","Beini Xie, Heng Chang, Ziwei Zhang, Xin Wang, Daixin Wang, Zhiqiang Zhang, Rex Ying, Wenwu Zhu","Graph Neural Networks (GNNs) obtain tremendous success in modeling relational
data. Still, they are prone to adversarial attacks, which are massive threats
to applying GNNs to risk-sensitive domains. Existing defensive methods neither
guarantee performance facing new data/tasks or adversarial attacks nor provide
insights to understand GNN robustness from an architectural perspective. Neural
Architecture Search (NAS) has the potential to solve this problem by automating
GNN architecture designs. Nevertheless, current graph NAS approaches lack
robust design and are vulnerable to adversarial attacks. To tackle these
challenges, we propose a novel Robust Neural Architecture search framework for
GNNs (G-RNA). Specifically, we design a robust search space for the
message-passing mechanism by adding graph structure mask operations into the
search space, which comprises various defensive operation candidates and allows
us to search for defensive GNNs. Furthermore, we define a robustness metric to
guide the search procedure, which helps to filter robust architectures. In this
way, G-RNA helps understand GNN robustness from an architectural perspective
and effectively searches for optimal adversarial robust GNNs. Extensive
experimental results on benchmark datasets show that G-RNA significantly
outperforms manually designed robust GNNs and vanilla graph NAS baselines by
12.1% to 23.4% under adversarial attacks.",2304.04168v1,https://arxiv.org/pdf/2304.04168v1
Exploring the Connection between Robust and Generative Models,"Senad Beadini, Iacopo Masi","We offer a study that connects robust discriminative classifiers trained with
adversarial training (AT) with generative modeling in the form of Energy-based
Models (EBM). We do so by decomposing the loss of a discriminative classifier
and showing that the discriminative model is also aware of the input data
density. Though a common assumption is that adversarial points leave the
manifold of the input data, our study finds out that, surprisingly, untargeted
adversarial points in the input space are very likely under the generative
model hidden inside the discriminative classifier -- have low energy in the
EBM. We present two evidence: untargeted attacks are even more likely than the
natural data and their likelihood increases as the attack strength increases.
This allows us to easily detect them and craft a novel attack called
High-Energy PGD that fools the classifier yet has energy similar to the data
set. The code is available at github.com/senad96/Robust-Generative",2304.04033v4,https://arxiv.org/pdf/2304.04033v4
"RobCaps: Evaluating the Robustness of Capsule Networks against Affine
  Transformations and Adversarial Attacks","Alberto Marchisio, Antonio De Marco, Alessio Colucci, Maurizio Martina, Muhammad Shafique","Capsule Networks (CapsNets) are able to hierarchically preserve the pose
relationships between multiple objects for image classification tasks. Other
than achieving high accuracy, another relevant factor in deploying CapsNets in
safety-critical applications is the robustness against input transformations
and malicious adversarial attacks.
  In this paper, we systematically analyze and evaluate different factors
affecting the robustness of CapsNets, compared to traditional Convolutional
Neural Networks (CNNs). Towards a comprehensive comparison, we test two CapsNet
models and two CNN models on the MNIST, GTSRB, and CIFAR10 datasets, as well as
on the affine-transformed versions of such datasets. With a thorough analysis,
we show which properties of these architectures better contribute to increasing
the robustness and their limitations. Overall, CapsNets achieve better
robustness against adversarial examples and affine transformations, compared to
a traditional CNN with a similar number of parameters. Similar conclusions have
been derived for deeper versions of CapsNets and CNNs. Moreover, our results
unleash a key finding that the dynamic routing does not contribute much to
improving the CapsNets' robustness. Indeed, the main generalization
contribution is due to the hierarchical feature learning through capsules.",2304.03973v2,https://arxiv.org/pdf/2304.03973v2
Benchmarking the Robustness of Quantized Models,"Yisong Xiao, Tianyuan Zhang, Shunchang Liu, Haotong Qin","Quantization has emerged as an essential technique for deploying deep neural
networks (DNNs) on devices with limited resources. However, quantized models
exhibit vulnerabilities when exposed to various noises in real-world
applications. Despite the importance of evaluating the impact of quantization
on robustness, existing research on this topic is limited and often disregards
established principles of robustness evaluation, resulting in incomplete and
inconclusive findings. To address this gap, we thoroughly evaluated the
robustness of quantized models against various noises (adversarial attacks,
natural corruptions, and systematic noises) on ImageNet. Extensive experiments
demonstrate that lower-bit quantization is more resilient to adversarial
attacks but is more susceptible to natural corruptions and systematic noises.
Notably, our investigation reveals that impulse noise (in natural corruptions)
and the nearest neighbor interpolation (in systematic noises) have the most
significant impact on quantized models. Our research contributes to advancing
the robust quantization of models and their deployment in real-world scenarios.",2304.03968v1,https://arxiv.org/pdf/2304.03968v1
"Robust Deep Learning Models Against Semantic-Preserving Adversarial
  Attack","Dashan Gao, Yunce Zhao, Yinghua Yao, Zeqi Zhang, Bifei Mao, Xin Yao","Deep learning models can be fooled by small $l_p$-norm adversarial
perturbations and natural perturbations in terms of attributes. Although the
robustness against each perturbation has been explored, it remains a challenge
to address the robustness against joint perturbations effectively. In this
paper, we study the robustness of deep learning models against joint
perturbations by proposing a novel attack mechanism named Semantic-Preserving
Adversarial (SPA) attack, which can then be used to enhance adversarial
training. Specifically, we introduce an attribute manipulator to generate
natural and human-comprehensible perturbations and a noise generator to
generate diverse adversarial noises. Based on such combined noises, we optimize
both the attribute value and the diversity variable to generate
jointly-perturbed samples. For robust training, we adversarially train the deep
learning model against the generated joint perturbations. Empirical results on
four benchmarks show that the SPA attack causes a larger performance decline
with small $l_{\infty}$ norm-ball constraints compared to existing approaches.
Furthermore, our SPA-enhanced training outperforms existing defense methods
against such joint perturbations.",2304.03955v1,https://arxiv.org/pdf/2304.03955v1
Improving Identity-Robustness for Face Models,"Qi Qi, Shervin Ardeshir","Despite the success of deep-learning models in many tasks, there have been
concerns about such models learning shortcuts, and their lack of robustness to
irrelevant confounders. When it comes to models directly trained on human
faces, a sensitive confounder is that of human identities. Many face-related
tasks should ideally be identity-independent, and perform uniformly across
different individuals (i.e. be fair). One way to measure and enforce such
robustness and performance uniformity is through enforcing it during training,
assuming identity-related information is available at scale. However, due to
privacy concerns and also the cost of collecting such information, this is
often not the case, and most face datasets simply contain input images and
their corresponding task-related labels. Thus, improving identity-related
robustness without the need for such annotations is of great importance. Here,
we explore using face-recognition embedding vectors, as proxies for identities,
to enforce such robustness. We propose to use the structure in the
face-recognition embedding space, to implicitly emphasize rare samples within
each class. We do so by weighting samples according to their conditional
inverse density (CID) in the proxy embedding space. Our experiments suggest
that such a simple sample weighting scheme, not only improves the training
robustness, it often improves the overall performance as a result of such
robustness. We also show that employing such constraints during training
results in models that are significantly less sensitive to different levels of
bias in the dataset.",2304.03838v2,https://arxiv.org/pdf/2304.03838v2
Domain Generalization In Robust Invariant Representation,"Gauri Gupta, Ritvik Kapila, Keshav Gupta, Ramesh Raskar","Unsupervised approaches for learning representations invariant to common
transformations are used quite often for object recognition. Learning
invariances makes models more robust and practical to use in real-world
scenarios. Since data transformations that do not change the intrinsic
properties of the object cause the majority of the complexity in recognition
tasks, models that are invariant to these transformations help reduce the
amount of training data required. This further increases the model's efficiency
and simplifies training. In this paper, we investigate the generalization of
invariant representations on out-of-distribution data and try to answer the
question: Do model representations invariant to some transformations in a
particular seen domain also remain invariant in previously unseen domains?
Through extensive experiments, we demonstrate that the invariant model learns
unstructured latent representations that are robust to distribution shifts,
thus making invariance a desirable property for training in
resource-constrained settings.",2304.03431v2,https://arxiv.org/pdf/2304.03431v2
Noise-Robust Dense Retrieval via Contrastive Alignment Post Training,"Daniel Campos, ChengXiang Zhai, Alessandro Magnani","The success of contextual word representations and advances in neural
information retrieval have made dense vector-based retrieval a standard
approach for passage and document ranking. While effective and efficient,
dual-encoders are brittle to variations in query distributions and noisy
queries. Data augmentation can make models more robust but introduces overhead
to training set generation and requires retraining and index regeneration. We
present Contrastive Alignment POst Training (CAPOT), a highly efficient
finetuning method that improves model robustness without requiring index
regeneration, the training set optimization, or alteration. CAPOT enables
robust retrieval by freezing the document encoder while the query encoder
learns to align noisy queries with their unaltered root. We evaluate CAPOT
noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,
finding CAPOT has a similar impact as data augmentation with none of its
overhead.",2304.03401v2,https://arxiv.org/pdf/2304.03401v2
"Improving Visual Question Answering Models through Robustness Analysis
  and In-Context Learning with a Chain of Basic Questions","Jia-Hong Huang, Modar Alfadly, Bernard Ghanem, Marcel Worring","Deep neural networks have been critical in the task of Visual Question
Answering (VQA), with research traditionally focused on improving model
accuracy. Recently, however, there has been a trend towards evaluating the
robustness of these models against adversarial attacks. This involves assessing
the accuracy of VQA models under increasing levels of noise in the input, which
can target either the image or the proposed query question, dubbed the main
question. However, there is currently a lack of proper analysis of this aspect
of VQA. This work proposes a new method that utilizes semantically related
questions, referred to as basic questions, acting as noise to evaluate the
robustness of VQA models. It is hypothesized that as the similarity of a basic
question to the main question decreases, the level of noise increases. To
generate a reasonable noise level for a given main question, a pool of basic
questions is ranked based on their similarity to the main question, and this
ranking problem is cast as a LASSO optimization problem. Additionally, this
work proposes a novel robustness measure, R_score, and two basic question
datasets to standardize the analysis of VQA model robustness. The experimental
results demonstrate that the proposed evaluation method effectively analyzes
the robustness of VQA models. Moreover, the experiments show that in-context
learning with a chain of basic questions can enhance model accuracy.",2304.03147v1,https://arxiv.org/pdf/2304.03147v1
"Robustmix: Improving Robustness by Regularizing the Frequency Bias of
  Deep Nets","Jonas Ngnawe, Marianne ABEMGNIGNI NJIFON, Jonathan Heek, Yann Dauphin","Deep networks have achieved impressive results on a range of well-curated
benchmark datasets. Surprisingly, their performance remains sensitive to
perturbations that have little effect on human performance. In this work, we
propose a novel extension of Mixup called Robustmix that regularizes networks
to classify based on lower-frequency spatial features. We show that this type
of regularization improves robustness on a range of benchmarks such as
Imagenet-C and Stylized Imagenet. It adds little computational overhead and,
furthermore, does not require a priori knowledge of a large set of image
transformations. We find that this approach further complements recent advances
in model architecture and data augmentation, attaining a state-of-the-art mCE
of 44.8 with an EfficientNet-B8 model and RandAugment, which is a reduction of
16 mCE compared to the baseline.",2304.02847v1,https://arxiv.org/pdf/2304.02847v1
Robust Neural Architecture Search,"Xunyu Zhu, Jian Li, Yong Liu, Weiping Wang","Neural Architectures Search (NAS) becomes more and more popular over these
years. However, NAS-generated models tends to suffer greater vulnerability to
various malicious attacks. Lots of robust NAS methods leverage adversarial
training to enhance the robustness of NAS-generated models, however, they
neglected the nature accuracy of NAS-generated models. In our paper, we propose
a novel NAS method, Robust Neural Architecture Search (RNAS). To design a
regularization term to balance accuracy and robustness, RNAS generates
architectures with both high accuracy and good robustness. To reduce search
cost, we further propose to use noise examples instead adversarial examples as
input to search architectures. Extensive experiments show that RNAS achieves
state-of-the-art (SOTA) performance on both image classification and
adversarial attacks, which illustrates the proposed RNAS achieves a good
tradeoff between robustness and accuracy.",2304.02845v2,https://arxiv.org/pdf/2304.02845v2
Robust Calibrate Proxy Loss for Deep Metric Learning,"Xinyue Li, Jian Wang, Wei Song, Yanling Du, Zhixiang Liu","The mainstream researche in deep metric learning can be divided into two
genres: proxy-based and pair-based methods. Proxy-based methods have attracted
extensive attention due to the lower training complexity and fast network
convergence. However, these methods have limitations as the poxy optimization
is done by network, which makes it challenging for the proxy to accurately
represent the feature distrubtion of the real class of data. In this paper, we
propose a Calibrate Proxy (CP) structure, which uses the real sample
information to improve the similarity calculation in proxy-based loss and
introduces a calibration loss to constraint the proxy optimization towards the
center of the class features. At the same time, we set a small number of
proxies for each class to alleviate the impact of intra-class differences on
retrieval performance. The effectiveness of our method is evaluated by
extensive experiments on three public datasets and multiple synthetic
label-noise datasets. The results show that our approach can effectively
improve the performance of commonly used proxy-based losses on both regular and
noisy datasets.",2304.09162v1,https://arxiv.org/pdf/2304.09162v1
Hyper-parameter Tuning for Adversarially Robust Models,"Pedro Mendes, Paolo Romano, David Garlan","This work focuses on the problem of hyper-parameter tuning (HPT) for robust
(i.e., adversarially trained) models, shedding light on the new challenges and
opportunities arising during the HPT process for robust models. To this end, we
conduct an extensive experimental study based on 3 popular deep models, in
which we explore exhaustively 9 (discretized) HPs, 2 fidelity dimensions, and 2
attack bounds, for a total of 19208 configurations (corresponding to 50
thousand GPU hours). Through this study, we show that the complexity of the HPT
problem is further exacerbated in adversarial settings due to the need to
independently tune the HPs used during standard and adversarial training:
succeeding in doing so (i.e., adopting different HP settings in both phases)
can lead to a reduction of up to 80% and 43% of the error for clean and
adversarial inputs, respectively. On the other hand, we also identify new
opportunities to reduce the cost of HPT for robust models. Specifically, we
propose to leverage cheap adversarial training methods to obtain inexpensive,
yet highly correlated, estimations of the quality achievable using
state-of-the-art methods. We show that, by exploiting this novel idea in
conjunction with a recent multi-fidelity optimizer (taKG), the efficiency of
the HPT process can be enhanced by up to 2.1x.",2304.02497v3,https://arxiv.org/pdf/2304.02497v3
"Robustness Benchmark of Road User Trajectory Prediction Models for
  Automated Driving","Manuel Muñoz Sánchez, Emilia Silvas, Jos Elfring, René van de Molengraft","Accurate and robust trajectory predictions of road users are needed to enable
safe automated driving. To do this, machine learning models are often used,
which can show erratic behavior when presented with previously unseen inputs.
In this work, two environment-aware models (MotionCNN and MultiPath++) and two
common baselines (Constant Velocity and an LSTM) are benchmarked for robustness
against various perturbations that simulate functional insufficiencies observed
during model deployment in a vehicle: unavailability of road information, late
detections, and noise. Results show significant performance degradation under
the presence of these perturbations, with errors increasing up to +1444.8\% in
commonly used trajectory prediction evaluation metrics. Training the models
with similar perturbations effectively reduces performance degradation, with
error increases of up to +87.5\%. We argue that despite being an effective
mitigation strategy, data augmentation through perturbations during training
does not guarantee robustness towards unforeseen perturbations, since
identification of all possible on-road complications is unfeasible.
Furthermore, degrading the inputs sometimes leads to more accurate predictions,
suggesting that the models are unable to learn the true relationships between
the different elements in the data.",2304.01895v1,https://arxiv.org/pdf/2304.01895v1
Learning Stable and Robust Linear Parameter-Varying State-Space Models,"Chris Verhoek, Ruigang Wang, Roland Tóth","This paper presents two direct parameterizations of stable and robust linear
parameter-varying state-space (LPV-SS) models. The model parametrizations
guarantee a priori that for all parameter values during training, the allowed
models are stable in the contraction sense or have their Lipschitz constant
bounded by a user-defined value $\gamma$. Furthermore, since the
parametrizations are direct, the models can be trained using unconstrained
optimization. The fact that the trained models are of the LPV-SS class makes
them useful for, e.g., further convex analysis or controller design. The
effectiveness of the approach is demonstrated on an LPV identification problem.",2304.01828v2,https://arxiv.org/pdf/2304.01828v2
"Learning Invariant Representation via Contrastive Feature Alignment for
  Clutter Robust SAR Target Recognition","Bowen Peng, Jianyue Xie, Bo Peng, Li Liu","The deep neural networks (DNNs) have freed the synthetic aperture radar
automatic target recognition (SAR ATR) from expertise-based feature designing
and demonstrated superiority over conventional solutions. There has been shown
the unique deficiency of ground vehicle benchmarks in shapes of strong
background correlation results in DNNs overfitting the clutter and being
non-robust to unfamiliar surroundings. However, the gap between fixed
background model training and varying background application remains
underexplored. Inspired by contrastive learning, this letter proposes a
solution called Contrastive Feature Alignment (CFA) aiming to learn invariant
representation for robust recognition. The proposed method contributes a mixed
clutter variants generation strategy and a new inference branch equipped with
channel-weighted mean square error (CWMSE) loss for invariant representation
learning. In specific, the generation strategy is delicately designed to better
attract clutter-sensitive deviation in feature space. The CWMSE loss is further
devised to better contrast this deviation and align the deep features activated
by the original images and corresponding clutter variants. The proposed CFA
combines both classification and CWMSE losses to train the model jointly, which
allows for the progressive learning of invariant target representation.
Extensive evaluations on the MSTAR dataset and six DNN models prove the
effectiveness of our proposal. The results demonstrated that the CFA-trained
models are capable of recognizing targets among unfamiliar surroundings that
are not included in the dataset, and are robust to varying signal-to-clutter
ratios.",2304.01747v1,https://arxiv.org/pdf/2304.01747v1
RARE: Robust Masked Graph Autoencoder,"Wenxuan Tu, Qing Liao, Sihang Zhou, Xin Peng, Chuan Ma, Zhe Liu, Xinwang Liu, Zhiping Cai","Masked graph autoencoder (MGAE) has emerged as a promising self-supervised
graph pre-training (SGP) paradigm due to its simplicity and effectiveness.
However, existing efforts perform the mask-then-reconstruct operation in the
raw data space as is done in computer vision (CV) and natural language
processing (NLP) areas, while neglecting the important non-Euclidean property
of graph data. As a result, the highly unstable local connection structures
largely increase the uncertainty in inferring masked data and decrease the
reliability of the exploited self-supervision signals, leading to inferior
representations for downstream evaluations. To address this issue, we propose a
novel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve the
certainty in inferring masked data and the reliability of the self-supervision
mechanism by further masking and reconstructing node samples in the high-order
latent feature space. Through both theoretical and empirical analyses, we have
discovered that performing a joint mask-then-reconstruct strategy in both
latent feature and raw data spaces could yield improved stability and
performance. To this end, we elaborately design a masked latent feature
completion scheme, which predicts latent features of masked nodes under the
guidance of high-order sample correlations that are hard to be observed from
the raw data perspective. Specifically, we first adopt a latent feature
predictor to predict the masked latent features from the visible ones. Next, we
encode the raw data of masked samples with a momentum graph encoder and
subsequently employ the resulting representations to improve predicted results
through latent feature matching. Extensive experiments on seventeen datasets
have demonstrated the effectiveness and robustness of RARE against
state-of-the-art (SOTA) competitors across three downstream tasks.",2304.01507v2,https://arxiv.org/pdf/2304.01507v2
"Accelerated parallel MRI using memory efficient and robust monotone
  operator learning (MOL)","Aniket Pramanik, Mathews Jacob","Model-based deep learning methods that combine imaging physics with learned
regularization priors have been emerging as powerful tools for parallel MRI
acceleration. The main focus of this paper is to determine the utility of the
monotone operator learning (MOL) framework in the parallel MRI setting. The MOL
algorithm alternates between a gradient descent step using a monotone
convolutional neural network (CNN) and a conjugate gradient algorithm to
encourage data consistency. The benefits of this approach include similar
guarantees as compressive sensing algorithms including uniqueness, convergence,
and stability, while being significantly more memory efficient than unrolled
methods. We validate the proposed scheme by comparing it with different
unrolled algorithms in the context of accelerated parallel MRI for static and
dynamic settings.",2304.01351v1,https://arxiv.org/pdf/2304.01351v1
"CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception","Youngseok Kim, Juyeb Shin, Sanmin Kim, In-Jae Lee, Jun Won Choi, Dongsuk Kum","Autonomous driving requires an accurate and fast 3D perception system that
includes 3D object detection, tracking, and segmentation. Although recent
low-cost camera-based approaches have shown promising results, they are
susceptible to poor illumination or bad weather conditions and have a large
localization error. Hence, fusing camera with low-cost radar, which provides
precise long-range measurement and operates reliably in all environments, is
promising but has not yet been thoroughly investigated. In this paper, we
propose Camera Radar Net (CRN), a novel camera-radar fusion framework that
generates a semantically rich and spatially accurate bird's-eye-view (BEV)
feature map for various tasks. To overcome the lack of spatial information in
an image, we transform perspective view image features to BEV with the help of
sparse but accurate radar points. We further aggregate image and radar feature
maps in BEV using multi-modal deformable attention designed to tackle the
spatial misalignment between inputs. CRN with real-time setting operates at 20
FPS while achieving comparable performance to LiDAR detectors on nuScenes, and
even outperforms at a far distance on 100m setting. Moreover, CRN with offline
setting yields 62.4% NDS, 57.5% mAP on nuScenes test set and ranks first among
all camera and camera-radar 3D object detectors.",2304.00670v3,https://arxiv.org/pdf/2304.00670v3
"Risk-Sensitive and Robust Model-Based Reinforcement Learning and
  Planning",Marc Rigter,"Many sequential decision-making problems that are currently automated, such
as those in manufacturing or recommender systems, operate in an environment
where there is either little uncertainty, or zero risk of catastrophe. As
companies and researchers attempt to deploy autonomous systems in less
constrained environments, it is increasingly important that we endow sequential
decision-making algorithms with the ability to reason about uncertainty and
risk.
  In this thesis, we will address both planning and reinforcement learning (RL)
approaches to sequential decision-making. In the planning setting, it is
assumed that a model of the environment is provided, and a policy is optimised
within that model. Reinforcement learning relies upon extensive random
exploration, and therefore usually requires a simulator in which to perform
training. In many real-world domains, it is impossible to construct a perfectly
accurate model or simulator. Therefore, the performance of any policy is
inevitably uncertain due to the incomplete knowledge about the environment.
Furthermore, in stochastic domains, the outcome of any given run is also
uncertain due to the inherent randomness of the environment. These two sources
of uncertainty are usually classified as epistemic, and aleatoric uncertainty,
respectively. The over-arching goal of this thesis is to contribute to
developing algorithms that mitigate both sources of uncertainty in sequential
decision-making problems.
  We make a number of contributions towards this goal, with a focus on
model-based algorithms...",2304.00573v1,https://arxiv.org/pdf/2304.00573v1
To be Robust and to be Fair: Aligning Fairness with Robustness,"Junyi Chai, Xiaoqian Wang","Adversarial training has been shown to be reliable in improving robustness
against adversarial samples. However, the problem of adversarial training in
terms of fairness has not yet been properly studied, and the relationship
between fairness and accuracy attack still remains unclear. Can we
simultaneously improve robustness w.r.t. both fairness and accuracy? To tackle
this topic, in this paper, we study the problem of adversarial training and
adversarial attack w.r.t. both metrics. We propose a unified structure for
fairness attack which brings together common notions in group fairness, and we
theoretically prove the equivalence of fairness attack against different
notions. Moreover, we show the alignment of fairness and accuracy attack, and
theoretically demonstrate that robustness w.r.t. one metric benefits from
robustness w.r.t. the other metric. Our study suggests a novel way to unify
adversarial training and attack w.r.t. fairness and accuracy, and experimental
results show that our proposed method achieves better performance in terms of
robustness w.r.t. both metrics.",2304.00061v1,https://arxiv.org/pdf/2304.00061v1
"A robust deep learning-based damage identification approach for SHM
  considering missing data","Fan Deng, Xiaoming Tao, Pengxiang Wei, Shiyin Wei","Data-driven method for Structural Health Monitoring (SHM), that mine the
hidden structural performance from the correlations among monitored time series
data, has received widely concerns recently. However, missing data
significantly impacts the conduction of this method. Missing data is a
frequently encountered issue in time series data in SHM and many other
real-world applications, that harms to the standardized data mining and
downstream tasks, such as condition assessment. Imputation approaches based on
spatiotemporal relations among monitoring data are developed to handle this
issue, however, no additional information is added during imputation. This
paper thus develops a robust method for damage identification that considers
the missing data occasions, based on long-short term memory (LSTM) model and
dropout mechanism in the autoencoder (AE) framework. Inputs channels are
randomly dropped to simulate the missing data in training, and reconstruction
errors are used as the loss function and the damage indicator. Quasi-static
response (cable tension) of a cable-stayed bridge released in 1st IPC-SHM is
employed to verify this proposed method, and results show that the missing data
imputation and damage identification can be implemented together in a unified
way.",2304.00040v1,https://arxiv.org/pdf/2304.00040v1
"Learning from Similar Linear Representations: Adaptivity, Minimaxity,
  and Robustness","Ye Tian, Yuqi Gu, Yang Feng","Representation multi-task learning (MTL) has achieved tremendous success in
practice. However, the theoretical understanding of these methods is still
lacking. Most existing theoretical works focus on cases where all tasks share
the same representation, and claim that MTL almost always improves performance.
Nevertheless, as the number of tasks grows, assuming all tasks share the same
representation is unrealistic. Furthermore, empirical findings often indicate
that a shared representation does not necessarily improve single-task learning
performance. In this paper, we aim to understand how to learn from tasks with
\textit{similar but not exactly the same} linear representations, while dealing
with outlier tasks. Assuming a known intrinsic dimension, we proposed a
penalized empirical risk minimization method and a spectral method that are
\textit{adaptive} to the similarity structure and \textit{robust} to outlier
tasks. Both algorithms outperform single-task learning when representations
across tasks are sufficiently similar and the proportion of outlier tasks is
small. Moreover, they always perform at least as well as single-task learning,
even when the representations are dissimilar. We provided information-theoretic
lower bounds to demonstrate that both methods are nearly \textit{minimax}
optimal in a large regime, with the spectral method being optimal in the
absence of outlier tasks. Additionally, we introduce a thresholding algorithm
to adapt to an unknown intrinsic dimension. We conducted extensive numerical
experiments to validate our theoretical findings.",2303.17765v3,https://arxiv.org/pdf/2303.17765v3
Towards Adversarially Robust Continual Learning,"Tao Bai, Chen Chen, Lingjuan Lyu, Jun Zhao, Bihan Wen","Recent studies show that models trained by continual learning can achieve the
comparable performances as the standard supervised learning and the learning
flexibility of continual learning models enables their wide applications in the
real world. Deep learning models, however, are shown to be vulnerable to
adversarial attacks. Though there are many studies on the model robustness in
the context of standard supervised learning, protecting continual learning from
adversarial attacks has not yet been investigated. To fill in this research
gap, we are the first to study adversarial robustness in continual learning and
propose a novel method called \textbf{T}ask-\textbf{A}ware \textbf{B}oundary
\textbf{A}ugmentation (TABA) to boost the robustness of continual learning
models. With extensive experiments on CIFAR-10 and CIFAR-100, we show the
efficacy of adversarial training and TABA in defending adversarial attacks.",2303.17764v1,https://arxiv.org/pdf/2303.17764v1
"Generating Adversarial Samples in Mini-Batches May Be Detrimental To
  Adversarial Robustness","Timothy Redgrave, Colton Crum","Neural networks have been proven to be both highly effective within computer
vision, and highly vulnerable to adversarial attacks. Consequently, as the use
of neural networks increases due to their unrivaled performance, so too does
the threat posed by adversarial attacks. In this work, we build towards
addressing the challenge of adversarial robustness by exploring the
relationship between the mini-batch size used during adversarial sample
generation and the strength of the adversarial samples produced. We demonstrate
that an increase in mini-batch size results in a decrease in the efficacy of
the samples produced, and we draw connections between these observations and
the phenomenon of vanishing gradients. Next, we formulate loss functions such
that adversarial sample strength is not degraded by mini-batch size. Our
findings highlight a potential risk for underestimating the true (practical)
strength of adversarial attacks, and a risk of overestimating a model's
robustness. We share our codes to let others replicate our experiments and to
facilitate further exploration of the connections between batch size and
adversarial sample strength.",2303.17720v1,https://arxiv.org/pdf/2303.17720v1
Robust Multi-Agent Pickup and Delivery with Delays,"Giacomo Lodigiani, Nicola Basilico, Francesco Amigoni","Multi-Agent Pickup and Delivery (MAPD) is the problem of computing
collision-free paths for a group of agents such that they can safely reach
delivery locations from pickup ones. These locations are provided at runtime,
making MAPD a combination between classical Multi-Agent Path Finding (MAPF) and
online task assignment. Current algorithms for MAPD do not consider many of the
practical issues encountered in real applications: real agents often do not
follow the planned paths perfectly, and may be subject to delays and failures.
In this paper, we study the problem of MAPD with delays, and we present two
solution approaches that provide robustness guarantees by planning paths that
limit the effects of imperfect execution. In particular, we introduce two
algorithms, k-TP and p-TP, both based on a decentralized algorithm typically
used to solve MAPD, Token Passing (TP), which offer deterministic and
probabilistic guarantees, respectively. Experimentally, we compare our
algorithms against a version of TP enriched with online replanning. k-TP and
p-TP provide robust solutions, significantly reducing the number of replans
caused by delays, with little or no increase in solution cost and running time.",2303.17422v1,https://arxiv.org/pdf/2303.17422v1
"Beyond Empirical Risk Minimization: Local Structure Preserving
  Regularization for Improving Adversarial Robustness","Wei Wei, Jiahuan Zhou, Ying Wu","It is broadly known that deep neural networks are susceptible to being fooled
by adversarial examples with perturbations imperceptible by humans. Various
defenses have been proposed to improve adversarial robustness, among which
adversarial training methods are most effective. However, most of these methods
treat the training samples independently and demand a tremendous amount of
samples to train a robust network, while ignoring the latent structural
information among these samples. In this work, we propose a novel Local
Structure Preserving (LSP) regularization, which aims to preserve the local
structure of the input space in the learned embedding space. In this manner,
the attacking effect of adversarial samples lying in the vicinity of clean
samples can be alleviated. We show strong empirical evidence that with or
without adversarial training, our method consistently improves the performance
of adversarial robustness on several image classification datasets compared to
the baselines and some state-of-the-art approaches, thus providing promising
direction for future research.",2303.16861v1,https://arxiv.org/pdf/2303.16861v1
Are Data-driven Explanations Robust against Out-of-distribution Data?,"Tang Li, Fengchun Qiao, Mengmeng Ma, Xi Peng","As black-box models increasingly power high-stakes applications, a variety of
data-driven explanation methods have been introduced. Meanwhile, machine
learning models are constantly challenged by distributional shifts. A question
naturally arises: Are data-driven explanations robust against
out-of-distribution data? Our empirical results show that even though predict
correctly, the model might still yield unreliable explanations under
distributional shifts. How to develop robust explanations against
out-of-distribution data? To address this problem, we propose an end-to-end
model-agnostic learning framework Distributionally Robust Explanations (DRE).
The key idea is, inspired by self-supervised learning, to fully utilizes the
inter-distribution information to provide supervisory signals for the learning
of explanations without human annotation. Can robust explanations benefit the
model's generalization capability? We conduct extensive experiments on a wide
range of tasks and data types, including classification and regression on image
and scientific tabular data. Our results demonstrate that the proposed method
significantly improves the model's performance in terms of explanation and
prediction robustness against distributional shifts.",2303.16390v1,https://arxiv.org/pdf/2303.16390v1
Provable Robustness for Streaming Models with a Sliding Window,"Aounon Kumar, Vinu Sankar Sadasivan, Soheil Feizi","The literature on provable robustness in machine learning has primarily
focused on static prediction problems, such as image classification, in which
input samples are assumed to be independent and model performance is measured
as an expectation over the input distribution. Robustness certificates are
derived for individual input instances with the assumption that the model is
evaluated on each instance separately. However, in many deep learning
applications such as online content recommendation and stock market analysis,
models use historical data to make predictions. Robustness certificates based
on the assumption of independent input samples are not directly applicable in
such scenarios. In this work, we focus on the provable robustness of machine
learning models in the context of data streams, where inputs are presented as a
sequence of potentially correlated items. We derive robustness certificates for
models that use a fixed-size sliding window over the input stream. Our
guarantees hold for the average model performance across the entire stream and
are independent of stream size, making them suitable for large data streams. We
perform experiments on speech detection and human activity recognition tasks
and show that our certificates can produce meaningful performance guarantees
against adversarial perturbations.",2303.16308v1,https://arxiv.org/pdf/2303.16308v1
"Robust and IP-Protecting Vertical Federated Learning against Unexpected
  Quitting of Parties","Jingwei Sun, Zhixu Du, Anna Dai, Saleh Baghersalimi, Alireza Amirshahi, David Atienza, Yiran Chen","Vertical federated learning (VFL) enables a service provider (i.e., active
party) who owns labeled features to collaborate with passive parties who
possess auxiliary features to improve model performance. Existing VFL
approaches, however, have two major vulnerabilities when passive parties
unexpectedly quit in the deployment phase of VFL - severe performance
degradation and intellectual property (IP) leakage of the active party's
labels. In this paper, we propose \textbf{Party-wise Dropout} to improve the
VFL model's robustness against the unexpected exit of passive parties and a
defense method called \textbf{DIMIP} to protect the active party's IP in the
deployment phase. We evaluate our proposed methods on multiple datasets against
different inference attacks. The results show that Party-wise Dropout
effectively maintains model performance after the passive party quits, and
DIMIP successfully disguises label information from the passive party's feature
extractor, thereby mitigating IP leakage.",2303.18178v1,https://arxiv.org/pdf/2303.18178v1
"Denoising Autoencoder-based Defensive Distillation as an Adversarial
  Robustness Algorithm","Bakary Badjie, José Cecílio, António Casimiro","Adversarial attacks significantly threaten the robustness of deep neural
networks (DNNs). Despite the multiple defensive methods employed, they are
nevertheless vulnerable to poison attacks, where attackers meddle with the
initial training data. In order to defend DNNs against such adversarial
attacks, this work proposes a novel method that combines the defensive
distillation mechanism with a denoising autoencoder (DAE). This technique tries
to lower the sensitivity of the distilled model to poison attacks by spotting
and reconstructing poisonous adversarial inputs in the training data. We added
carefully created adversarial samples to the initial training data to assess
the proposed method's performance. Our experimental findings demonstrate that
our method successfully identified and reconstructed the poisonous inputs while
also considering enhancing the DNN's resilience. The proposed approach provides
a potent and robust defense mechanism for DNNs in various applications where
data poisoning attacks are a concern. Thus, the defensive distillation
technique's limitation posed by poisonous adversarial attacks is overcome.",2303.15901v1,https://arxiv.org/pdf/2303.15901v1
"Conditional Generative Models are Provably Robust: Pointwise Guarantees
  for Bayesian Inverse Problems","Fabian Altekrüger, Paul Hagemann, Gabriele Steidl","Conditional generative models became a very powerful tool to sample from
Bayesian inverse problem posteriors. It is well-known in classical Bayesian
literature that posterior measures are quite robust with respect to
perturbations of both the prior measure and the negative log-likelihood, which
includes perturbations of the observations. However, to the best of our
knowledge, the robustness of conditional generative models with respect to
perturbations of the observations has not been investigated yet. In this paper,
we prove for the first time that appropriately learned conditional generative
models provide robust results for single observations.",2303.15845v3,https://arxiv.org/pdf/2303.15845v3
"RobustSwap: A Simple yet Robust Face Swapping Model against Attribute
  Leakage","Jaeseong Lee, Taewoo Kim, Sunghyun Park, Younggun Lee, Jaegul Choo","Face swapping aims at injecting a source image's identity (i.e., facial
features) into a target image, while strictly preserving the target's
attributes, which are irrelevant to identity. However, we observed that
previous approaches still suffer from source attribute leakage, where the
source image's attributes interfere with the target image's. In this paper, we
analyze the latent space of StyleGAN and find the adequate combination of the
latents geared for face swapping task. Based on the findings, we develop a
simple yet robust face swapping model, RobustSwap, which is resistant to the
potential source attribute leakage. Moreover, we exploit the coordination of
3DMM's implicit and explicit information as a guidance to incorporate the
structure of the source image and the precise pose of the target image. Despite
our method solely utilizing an image dataset without identity labels for
training, our model has the capability to generate high-fidelity and temporally
consistent videos. Through extensive qualitative and quantitative evaluations,
we demonstrate that our method shows significant improvements compared with the
previous face swapping models in synthesizing both images and videos. Project
page is available at https://robustswap.github.io/",2303.15768v1,https://arxiv.org/pdf/2303.15768v1
"Efficient Deep Learning of Robust, Adaptive Policies using Tube
  MPC-Guided Data Augmentation","Tong Zhao, Andrea Tagliabue, Jonathan P. How","The deployment of agile autonomous systems in challenging, unstructured
environments requires adaptation capabilities and robustness to uncertainties.
Existing robust and adaptive controllers, such as those based on model
predictive control (MPC), can achieve impressive performance at the cost of
heavy online onboard computations. Strategies that efficiently learn robust and
onboard-deployable policies from MPC have emerged, but they still lack
fundamental adaptation capabilities. In this work, we extend an existing
efficient Imitation Learning (IL) algorithm for robust policy learning from MPC
with the ability to learn policies that adapt to challenging model/environment
uncertainties. The key idea of our approach consists in modifying the IL
procedure by conditioning the policy on a learned lower-dimensional
model/environment representation that can be efficiently estimated online. We
tailor our approach to the task of learning an adaptive position and attitude
control policy to track trajectories under challenging disturbances on a
multirotor. Evaluations in simulation show that a high-quality adaptive policy
can be obtained in about $1.3$ hours. We additionally empirically demonstrate
rapid adaptation to in- and out-of-training-distribution uncertainties,
achieving a $6.1$ cm average position error under wind disturbances that
correspond to about $50\%$ of the weight of the robot, and that are $36\%$
larger than the maximum wind seen during training.",2303.15688v2,https://arxiv.org/pdf/2303.15688v2
"Adjusted Wasserstein Distributionally Robust Estimator in Statistical
  Learning","Yiling Xie, Xiaoming Huo","We propose an adjusted Wasserstein distributionally robust estimator -- based
on a nonlinear transformation of the Wasserstein distributionally robust (WDRO)
estimator in statistical learning. The classic WDRO estimator is asymptotically
biased, while our adjusted WDRO estimator is asymptotically unbiased, resulting
in a smaller asymptotic mean squared error. Further, under certain conditions,
our proposed adjustment technique provides a general principle to de-bias
asymptotically biased estimators. Specifically, we will investigate how the
adjusted WDRO estimator is developed in the generalized linear model, including
logistic regression, linear regression, and Poisson regression. Numerical
experiments demonstrate the favorable practical performance of the adjusted
estimator over the classic one.",2303.15579v3,https://arxiv.org/pdf/2303.15579v3
"CoRe-Sleep: A Multimodal Fusion Framework for Time Series Robust to
  Imperfect Modalities","Konstantinos Kontras, Christos Chatzichristos, Huy Phan, Johan Suykens, Maarten De Vos","Sleep abnormalities can have severe health consequences. Automated sleep
staging, i.e. labelling the sequence of sleep stages from the patient's
physiological recordings, could simplify the diagnostic process. Previous work
on automated sleep staging has achieved great results, mainly relying on the
EEG signal. However, often multiple sources of information are available beyond
EEG. This can be particularly beneficial when the EEG recordings are noisy or
even missing completely. In this paper, we propose CoRe-Sleep, a Coordinated
Representation multimodal fusion network that is particularly focused on
improving the robustness of signal analysis on imperfect data. We demonstrate
how appropriately handling multimodal information can be the key to achieving
such robustness. CoRe-Sleep tolerates noisy or missing modalities segments,
allowing training on incomplete data. Additionally, it shows state-of-the-art
performance when testing on both multimodal and unimodal data using a single
model on SHHS-1, the largest publicly available study that includes sleep stage
labels. The results indicate that training the model on multimodal data does
positively influence performance when tested on unimodal data. This work aims
at bridging the gap between automated analysis tools and their clinical
utility.",2304.06485v1,https://arxiv.org/pdf/2304.06485v1
Robust Risk-Aware Option Hedging,"David Wu, Sebastian Jaimungal","The objectives of option hedging/trading extend beyond mere protection
against downside risks, with a desire to seek gains also driving agent's
strategies. In this study, we showcase the potential of robust risk-aware
reinforcement learning (RL) in mitigating the risks associated with
path-dependent financial derivatives. We accomplish this by leveraging a policy
gradient approach that optimises robust risk-aware performance criteria. We
specifically apply this methodology to the hedging of barrier options, and
highlight how the optimal hedging strategy undergoes distortions as the agent
moves from being risk-averse to risk-seeking. As well as how the agent
robustifies their strategy. We further investigate the performance of the hedge
when the data generating process (DGP) varies from the training DGP, and
demonstrate that the robust strategies outperform the non-robust ones.",2303.15216v3,https://arxiv.org/pdf/2303.15216v3
"Diffusion Denoised Smoothing for Certified and Adversarial Robust
  Out-Of-Distribution Detection","Nicola Franco, Daniel Korth, Jeanette Miriam Lorenz, Karsten Roscher, Stephan Guennemann","As the use of machine learning continues to expand, the importance of
ensuring its safety cannot be overstated. A key concern in this regard is the
ability to identify whether a given sample is from the training distribution,
or is an ""Out-Of-Distribution"" (OOD) sample. In addition, adversaries can
manipulate OOD samples in ways that lead a classifier to make a confident
prediction. In this study, we present a novel approach for certifying the
robustness of OOD detection within a $\ell_2$-norm around the input, regardless
of network architecture and without the need for specific components or
additional training. Further, we improve current techniques for detecting
adversarial attacks on OOD samples, while providing high levels of certified
and adversarial robustness on in-distribution samples. The average of all OOD
detection metrics on CIFAR10/100 shows an increase of $\sim 13 \% / 5\%$
relative to previous approaches.",2303.14961v3,https://arxiv.org/pdf/2303.14961v3
"Detecting Backdoors During the Inference Stage Based on Corruption
  Robustness Consistency","Xiaogeng Liu, Minghui Li, Haoyu Wang, Shengshan Hu, Dengpan Ye, Hai Jin, Libing Wu, Chaowei Xiao","Deep neural networks are proven to be vulnerable to backdoor attacks.
Detecting the trigger samples during the inference stage, i.e., the test-time
trigger sample detection, can prevent the backdoor from being triggered.
However, existing detection methods often require the defenders to have high
accessibility to victim models, extra clean data, or knowledge about the
appearance of backdoor triggers, limiting their practicality. In this paper, we
propose the test-time corruption robustness consistency evaluation (TeCo), a
novel test-time trigger sample detection method that only needs the hard-label
outputs of the victim models without any extra information. Our journey begins
with the intriguing observation that the backdoor-infected models have similar
performance across different image corruptions for the clean images, but
perform discrepantly for the trigger samples. Based on this phenomenon, we
design TeCo to evaluate test-time robustness consistency by calculating the
deviation of severity that leads to predictions' transition across different
corruptions. Extensive experiments demonstrate that compared with
state-of-the-art defenses, which even require either certain information about
the trigger types or accessibility of clean data, TeCo outperforms them on
different backdoor attacks, datasets, and model architectures, enjoying a
higher AUROC by 10% and 5 times of stability.",2303.18191v1,https://arxiv.org/pdf/2303.18191v1
BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning,"Changdae Oh, Hyeji Hwang, Hee-young Lee, YongTaek Lim, Geunyoung Jung, Jiyoung Jung, Hosik Choi, Kyungwoo Song","With the surge of large-scale pre-trained models (PTMs), fine-tuning these
models to numerous downstream tasks becomes a crucial problem. Consequently,
parameter efficient transfer learning (PETL) of large models has grasped huge
attention. While recent PETL methods showcase impressive performance, they rely
on optimistic assumptions: 1) the entire parameter set of a PTM is available,
and 2) a sufficiently large memory capacity for the fine-tuning is equipped.
However, in most real-world applications, PTMs are served as a black-box API or
proprietary software without explicit parameter accessibility. Besides, it is
hard to meet a large memory requirement for modern PTMs. In this work, we
propose black-box visual prompting (BlackVIP), which efficiently adapts the
PTMs without knowledge about model architectures and parameters. BlackVIP has
two components; 1) Coordinator and 2) simultaneous perturbation stochastic
approximation with gradient correction (SPSA-GC). The Coordinator designs
input-dependent image-shaped visual prompts, which improves few-shot adaptation
and robustness on distribution/location shift. SPSA-GC efficiently estimates
the gradient of a target model to update Coordinator. Extensive experiments on
16 datasets demonstrate that BlackVIP enables robust adaptation to diverse
domains without accessing PTMs' parameters, with minimal memory requirements.
Code: \url{https://github.com/changdaeoh/BlackVIP}",2303.14773v2,https://arxiv.org/pdf/2303.14773v2
"CeFlow: A Robust and Efficient Counterfactual Explanation Framework for
  Tabular Data using Normalizing Flows","Tri Dung Duong, Qian Li, Guandong Xu","Counterfactual explanation is a form of interpretable machine learning that
generates perturbations on a sample to achieve the desired outcome. The
generated samples can act as instructions to guide end users on how to observe
the desired results by altering samples. Although state-of-the-art
counterfactual explanation methods are proposed to use variational autoencoder
(VAE) to achieve promising improvements, they suffer from two major
limitations: 1) the counterfactuals generation is prohibitively slow, which
prevents algorithms from being deployed in interactive environments; 2) the
counterfactual explanation algorithms produce unstable results due to the
randomness in the sampling procedure of variational autoencoder. In this work,
to address the above limitations, we design a robust and efficient
counterfactual explanation framework, namely CeFlow, which utilizes normalizing
flows for the mixed-type of continuous and categorical features. Numerical
experiments demonstrate that our technique compares favorably to
state-of-the-art methods. We release our source at
https://github.com/tridungduong16/fairCE.git for reproducing the results.",2303.14668v1,https://arxiv.org/pdf/2303.14668v1
PORE: Provably Robust Recommender Systems against Data Poisoning Attacks,"Jinyuan Jia, Yupei Liu, Yuepeng Hu, Neil Zhenqiang Gong","Data poisoning attacks spoof a recommender system to make arbitrary,
attacker-desired recommendations via injecting fake users with carefully
crafted rating scores into the recommender system. We envision a cat-and-mouse
game for such data poisoning attacks and their defenses, i.e., new defenses are
designed to defend against existing attacks and new attacks are designed to
break them. To prevent such a cat-and-mouse game, we propose PORE, the first
framework to build provably robust recommender systems in this work. PORE can
transform any existing recommender system to be provably robust against any
untargeted data poisoning attacks, which aim to reduce the overall performance
of a recommender system. Suppose PORE recommends top-$N$ items to a user when
there is no attack. We prove that PORE still recommends at least $r$ of the $N$
items to the user under any data poisoning attack, where $r$ is a function of
the number of fake users in the attack. Moreover, we design an efficient
algorithm to compute $r$ for each user. We empirically evaluate PORE on popular
benchmark datasets.",2303.14601v1,https://arxiv.org/pdf/2303.14601v1
"Improving robustness of jet tagging algorithms with adversarial
  training: exploring the loss surface",Annika Stein,"In the field of high-energy physics, deep learning algorithms continue to
gain in relevance and provide performance improvements over traditional
methods, for example when identifying rare signals or finding complex patterns.
From an analyst's perspective, obtaining highest possible performance is
desirable, but recently, some attention has been shifted towards studying
robustness of models to investigate how well these perform under slight
distortions of input features. Especially for tasks that involve many
(low-level) inputs, the application of deep neural networks brings new
challenges. In the context of jet flavor tagging, adversarial attacks are used
to probe a typical classifier's vulnerability and can be understood as a model
for systematic uncertainties. A corresponding defense strategy, adversarial
training, improves robustness, while maintaining high performance.
Investigating the loss surface corresponding to the inputs and models in
question reveals geometric interpretations of robustness, taking correlations
into account.",2303.14511v1,https://arxiv.org/pdf/2303.14511v1
"Robust Path Following on Rivers Using Bootstrapped Reinforcement
  Learning","Niklas Paulig, Ostap Okhrin","This paper develops a Deep Reinforcement Learning (DRL)-agent for navigation
and control of autonomous surface vessels (ASV) on inland waterways. Spatial
restrictions due to waterway geometry and the resulting challenges, such as
high flow velocities or shallow banks, require controlled and precise movement
of the ASV. A state-of-the-art bootstrapped Q-learning algorithm in combination
with a versatile training environment generator leads to a robust and accurate
rudder controller. To validate our results, we compare the path-following
capabilities of the proposed approach to a vessel-specific PID controller on
real-world river data from the lower- and middle Rhine, indicating that the DRL
algorithm could effectively prove generalizability even in never-seen scenarios
while simultaneously attaining high navigational accuracy.",2303.15178v1,https://arxiv.org/pdf/2303.15178v1
Generalist: Decoupling Natural and Robust Generalization,"Hongjun Wang, Yisen Wang","Deep neural networks obtained by standard training have been constantly
plagued by adversarial examples. Although adversarial training demonstrates its
capability to defend against adversarial examples, unfortunately, it leads to
an inevitable drop in the natural generalization. To address the issue, we
decouple the natural generalization and the robust generalization from joint
training and formulate different training strategies for each one.
Specifically, instead of minimizing a global loss on the expectation over these
two generalization errors, we propose a bi-expert framework called
\emph{Generalist} where we simultaneously train base learners with task-aware
strategies so that they can specialize in their own fields. The parameters of
base learners are collected and combined to form a global learner at intervals
during the training process. The global learner is then distributed to the base
learners as initialized parameters for continued training. Theoretically, we
prove that the risks of Generalist will get lower once the base learners are
well trained. Extensive experiments verify the applicability of Generalist to
achieve high accuracy on natural examples while maintaining considerable
robustness to adversarial ones. Code is available at
https://github.com/PKU-ML/Generalist.",2303.13813v1,https://arxiv.org/pdf/2303.13813v1
"Adversarial Robustness and Feature Impact Analysis for Driver Drowsiness
  Detection","João Vitorino, Lourenço Rodrigues, Eva Maia, Isabel Praça, André Lourenço","Drowsy driving is a major cause of road accidents, but drivers are dismissive
of the impact that fatigue can have on their reaction times. To detect
drowsiness before any impairment occurs, a promising strategy is using Machine
Learning (ML) to monitor Heart Rate Variability (HRV) signals. This work
presents multiple experiments with different HRV time windows and ML models, a
feature impact analysis using Shapley Additive Explanations (SHAP), and an
adversarial robustness analysis to assess their reliability when processing
faulty input data and perturbed HRV signals. The most reliable model was
Extreme Gradient Boosting (XGB) and the optimal time window had between 120 and
150 seconds. Furthermore, SHAP enabled the selection of the 18 most impactful
features and the training of new smaller models that achieved a performance as
good as the initial ones. Despite the susceptibility of all models to
adversarial attacks, adversarial training enabled them to preserve
significantly higher results, especially XGB. Therefore, ML models can
significantly benefit from realistic adversarial training to provide a more
robust driver drowsiness detection.",2303.13649v1,https://arxiv.org/pdf/2303.13649v1
Optimization and Optimizers for Adversarial Robustness,"Hengyue Liang, Buyun Liang, Le Peng, Ying Cui, Tim Mitchell, Ju Sun","Empirical robustness evaluation (RE) of deep learning models against
adversarial perturbations entails solving nontrivial constrained optimization
problems. Existing numerical algorithms that are commonly used to solve them in
practice predominantly rely on projected gradient, and mostly handle
perturbations modeled by the $\ell_1$, $\ell_2$ and $\ell_\infty$ distances. In
this paper, we introduce a novel algorithmic framework that blends a
general-purpose constrained-optimization solver PyGRANSO with Constraint
Folding (PWCF), which can add more reliability and generality to the
state-of-the-art RE packages, e.g., AutoAttack. Regarding reliability, PWCF
provides solutions with stationarity measures and feasibility tests to assess
the solution quality. For generality, PWCF can handle perturbation models that
are typically inaccessible to the existing projected gradient methods; the main
requirement is the distance metric to be almost everywhere differentiable.
Taking advantage of PWCF and other existing numerical algorithms, we further
explore the distinct patterns in the solutions found for solving these
optimization problems using various combinations of losses, perturbation
models, and optimization algorithms. We then discuss the implications of these
patterns on the current robustness evaluation and adversarial training.",2303.13401v1,https://arxiv.org/pdf/2303.13401v1
"AI Models Close to your Chest: Robust Federated Learning Strategies for
  Multi-site CT","Edward H. Lee, Brendan Kelly, Emre Altinmakas, Hakan Dogan, Maryam Mohammadzadeh, Errol Colak, Steve Fu, Olivia Choudhury, Ujjwal Ratan, Felipe Kitamura, Hernan Chaves, Jimmy Zheng, Mourad Said, Eduardo Reis, Jaekwang Lim, Patricia Yokoo, Courtney Mitchell, Golnaz Houshmand, Marzyeh Ghassemi, Ronan Killeen, Wendy Qiu, Joel Hayden, Farnaz Rafiee, Chad Klochko, Nicholas Bevins, Faeze Sazgara, S. Simon Wong, Michael Moseley, Safwan Halabi, Kristen W. Yeom","While it is well known that population differences from genetics, sex, race,
and environmental factors contribute to disease, AI studies in medicine have
largely focused on locoregional patient cohorts with less diverse data sources.
Such limitation stems from barriers to large-scale data share and ethical
concerns over data privacy. Federated learning (FL) is one potential pathway
for AI development that enables learning across hospitals without data share.
In this study, we show the results of various FL strategies on one of the
largest and most diverse COVID-19 chest CT datasets: 21 participating hospitals
across five continents that comprise >10,000 patients with >1 million images.
We also propose an FL strategy that leverages synthetically generated data to
overcome class and size imbalances. We also describe the sources of data
heterogeneity in the context of FL, and show how even among the correctly
labeled populations, disparities can arise due to these biases.",2303.13567v2,https://arxiv.org/pdf/2303.13567v2
"Robust Consensus in Ranking Data Analysis: Definitions, Properties and
  Computational Issues","Morgane Goibert, Clément Calauzènes, Ekhine Irurozki, Stéphan Clémençon","As the issue of robustness in AI systems becomes vital, statistical learning
techniques that are reliable even in presence of partly contaminated data have
to be developed. Preference data, in the form of (complete) rankings in the
simplest situations, are no exception and the demand for appropriate concepts
and tools is all the more pressing given that technologies fed by or producing
this type of data (e.g. search engines, recommending systems) are now massively
deployed. However, the lack of vector space structure for the set of rankings
(i.e. the symmetric group $\mathfrak{S}_n$) and the complex nature of
statistics considered in ranking data analysis make the formulation of
robustness objectives in this domain challenging. In this paper, we introduce
notions of robustness, together with dedicated statistical methods, for
Consensus Ranking the flagship problem in ranking data analysis, aiming at
summarizing a probability distribution on $\mathfrak{S}_n$ by a median ranking.
Precisely, we propose specific extensions of the popular concept of breakdown
point, tailored to consensus ranking, and address the related computational
issues. Beyond the theoretical contributions, the relevance of the approach
proposed is supported by an experimental study.",2303.12878v1,https://arxiv.org/pdf/2303.12878v1
"Reliable and Efficient Evaluation of Adversarial Robustness for Deep
  Hashing-Based Retrieval","Xunguang Wang, Jiawang Bai, Xinyue Xu, Xiaomeng Li","Deep hashing has been extensively applied to massive image retrieval due to
its efficiency and effectiveness. Recently, several adversarial attacks have
been presented to reveal the vulnerability of deep hashing models against
adversarial examples. However, existing attack methods suffer from degraded
performance or inefficiency because they underutilize the semantic relations
between original samples or spend a lot of time learning these relations with a
deep neural network. In this paper, we propose a novel Pharos-guided Attack,
dubbed PgA, to evaluate the adversarial robustness of deep hashing networks
reliably and efficiently. Specifically, we design pharos code to represent the
semantics of the benign image, which preserves the similarity to semantically
relevant samples and dissimilarity to irrelevant ones. It is proven that we can
quickly calculate the pharos code via a simple math formula. Accordingly, PgA
can directly conduct a reliable and efficient attack on deep hashing-based
retrieval by maximizing the similarity between the hash code of the adversarial
example and the pharos code. Extensive experiments on the benchmark datasets
verify that the proposed algorithm outperforms the prior state-of-the-arts in
both attack strength and speed.",2303.12658v1,https://arxiv.org/pdf/2303.12658v1
"Disturbance Injection under Partial Automation: Robust Imitation
  Learning for Long-horizon Tasks","Hirotaka Tahara, Hikaru Sasaki, Hanbit Oh, Edgar Anarossi, Takamitsu Matsubara","Partial Automation (PA) with intelligent support systems has been introduced
in industrial machinery and advanced automobiles to reduce the burden of long
hours of human operation. Under PA, operators perform manual operations
(providing actions) and operations that switch to automatic/manual mode
(mode-switching). Since PA reduces the total duration of manual operation,
these two action and mode-switching operations can be replicated by imitation
learning with high sample efficiency. To this end, this paper proposes
Disturbance Injection under Partial Automation (DIPA) as a novel imitation
learning framework. In DIPA, mode and actions (in the manual mode) are assumed
to be observables in each state and are used to learn both action and
mode-switching policies. The above learning is robustified by injecting
disturbances into the operator's actions to optimize the disturbance's level
for minimizing the covariate shift under PA. We experimentally validated the
effectiveness of our method for long-horizon tasks in two simulations and a
real robot environment and confirmed that our method outperformed the previous
methods and reduced the demonstration burden.",2303.12375v1,https://arxiv.org/pdf/2303.12375v1
Distribution-restrained Softmax Loss for the Model Robustness,"Hao Wang, Chen Li, Jinzhe Jiang, Xin Zhang, Yaqian Zhao, Weifeng Gong","Recently, the robustness of deep learning models has received widespread
attention, and various methods for improving model robustness have been
proposed, including adversarial training, model architecture modification,
design of loss functions, certified defenses, and so on. However, the principle
of the robustness to attacks is still not fully understood, also the related
research is still not sufficient. Here, we have identified a significant factor
that affects the robustness of models: the distribution characteristics of
softmax values for non-real label samples. We found that the results after an
attack are highly correlated with the distribution characteristics, and thus we
proposed a loss function to suppress the distribution diversity of softmax. A
large number of experiments have shown that our method can improve robustness
without significant time consumption.",2303.12363v1,https://arxiv.org/pdf/2303.12363v1
"TsSHAP: Robust model agnostic feature-based explainability for time
  series forecasting","Vikas C. Raykar, Arindam Jati, Sumanta Mukherjee, Nupur Aggarwal, Kanthi Sarpatwar, Giridhar Ganapavarapu, Roman Vaculin","A trustworthy machine learning model should be accurate as well as
explainable. Understanding why a model makes a certain decision defines the
notion of explainability. While various flavors of explainability have been
well-studied in supervised learning paradigms like classification and
regression, literature on explainability for time series forecasting is
relatively scarce.
  In this paper, we propose a feature-based explainability algorithm, TsSHAP,
that can explain the forecast of any black-box forecasting model. The method is
agnostic of the forecasting model and can provide explanations for a forecast
in terms of interpretable features defined by the user a prior.
  The explanations are in terms of the SHAP values obtained by applying the
TreeSHAP algorithm on a surrogate model that learns a mapping between the
interpretable feature space and the forecast of the black-box model.
  Moreover, we formalize the notion of local, semi-local, and global
explanations in the context of time series forecasting, which can be useful in
several scenarios. We validate the efficacy and robustness of TsSHAP through
extensive experiments on multiple datasets.",2303.12316v1,https://arxiv.org/pdf/2303.12316v1
EPiC: Ensemble of Partial Point Clouds for Robust Classification,"Meir Yossef Levi, Guy Gilboa","Robust point cloud classification is crucial for real-world applications, as
consumer-type 3D sensors often yield partial and noisy data, degraded by
various artifacts. In this work we propose a general ensemble framework, based
on partial point cloud sampling. Each ensemble member is exposed to only
partial input data. Three sampling strategies are used jointly, two local ones,
based on patches and curves, and a global one of random sampling. We
demonstrate the robustness of our method to various local and global
degradations. We show that our framework significantly improves the robustness
of top classification netowrks by a large margin. Our experimental setting uses
the recently introduced ModelNet-C database by Ren et al.[24], where we reach
SOTA both on unaugmented and on augmented data. Our unaugmented mean Corruption
Error (mCE) is 0.64 (current SOTA is 0.86) and 0.50 for augmented data (current
SOTA is 0.57). We analyze and explain these remarkable results through
diversity analysis. Our code is available at:
https://github.com/yossilevii100/EPiC",2303.11419v2,https://arxiv.org/pdf/2303.11419v2
"DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified
  Robustness","Shoumik Saha, Wenxiao Wang, Yigitcan Kaya, Soheil Feizi, Tudor Dumitras","Machine Learning (ML) models have been utilized for malware detection for
over two decades. Consequently, this ignited an ongoing arms race between
malware authors and antivirus systems, compelling researchers to propose
defenses for malware-detection models against evasion attacks. However, most if
not all existing defenses against evasion attacks suffer from sizable
performance degradation and/or can defend against only specific attacks, which
makes them less practical in real-world settings. In this work, we develop a
certified defense, DRSM (De-Randomized Smoothed MalConv), by redesigning the
de-randomized smoothing technique for the domain of malware detection.
Specifically, we propose a window ablation scheme to provably limit the impact
of adversarial bytes while maximally preserving local structures of the
executables. After showing how DRSM is theoretically robust against attacks
with contiguous adversarial bytes, we verify its performance and certified
robustness experimentally, where we observe only marginal accuracy drops as the
cost of robustness. To our knowledge, we are the first to offer certified
robustness in the realm of static detection of malware executables. More
surprisingly, through evaluating DRSM against 9 empirical attacks of different
types, we observe that the proposed defense is empirically robust to some
extent against a diverse set of attacks, some of which even fall out of the
scope of its original threat model. In addition, we collected 15.5K recent
benign raw executables from diverse sources, which will be made public as a
dataset called PACE (Publicly Accessible Collection(s) of Executables) to
alleviate the scarcity of publicly available benign datasets for studying
malware detection and provide future research with more representative data of
the time.",2303.13372v3,https://arxiv.org/pdf/2303.13372v3
"TWINS: A Fine-Tuning Framework for Improved Transferability of
  Adversarial Robustness and Generalization","Ziquan Liu, Yi Xu, Xiangyang Ji, Antoni B. Chan","Recent years have seen the ever-increasing importance of pre-trained models
and their downstream training in deep learning research and applications. At
the same time, the defense for adversarial examples has been mainly
investigated in the context of training from random initialization on simple
classification tasks. To better exploit the potential of pre-trained models in
adversarial robustness, this paper focuses on the fine-tuning of an
adversarially pre-trained model in various classification tasks. Existing
research has shown that since the robust pre-trained model has already learned
a robust feature extractor, the crucial question is how to maintain the
robustness in the pre-trained model when learning the downstream task. We study
the model-based and data-based approaches for this goal and find that the two
common approaches cannot achieve the objective of improving both generalization
and adversarial robustness. Thus, we propose a novel statistics-based approach,
Two-WIng NormliSation (TWINS) fine-tuning framework, which consists of two
neural networks where one of them keeps the population means and variances of
pre-training data in the batch normalization layers. Besides the robust
information transfer, TWINS increases the effective learning rate without
hurting the training stability since the relationship between a weight norm and
its gradient norm in standard batch normalization layer is broken, resulting in
a faster escape from the sub-optimal initialization and alleviating the robust
overfitting. Finally, TWINS is shown to be effective on a wide range of image
classification datasets in terms of both generalization and robustness. Our
code is available at https://github.com/ziquanliu/CVPR2023-TWINS.",2303.11135v1,https://arxiv.org/pdf/2303.11135v1
"Benchmarking Robustness of 3D Object Detection to Common Corruptions in
  Autonomous Driving","Yinpeng Dong, Caixin Kang, Jinlai Zhang, Zijian Zhu, Yikai Wang, Xiao Yang, Hang Su, Xingxing Wei, Jun Zhu","3D object detection is an important task in autonomous driving to perceive
the surroundings. Despite the excellent performance, the existing 3D detectors
lack the robustness to real-world corruptions caused by adverse weathers,
sensor noises, etc., provoking concerns about the safety and reliability of
autonomous driving systems. To comprehensively and rigorously benchmark the
corruption robustness of 3D detectors, in this paper we design 27 types of
common corruptions for both LiDAR and camera inputs considering real-world
driving scenarios. By synthesizing these corruptions on public datasets, we
establish three corruption robustness benchmarks -- KITTI-C, nuScenes-C, and
Waymo-C. Then, we conduct large-scale experiments on 24 diverse 3D object
detection models to evaluate their corruption robustness. Based on the
evaluation results, we draw several important findings, including: 1)
motion-level corruptions are the most threatening ones that lead to significant
performance drop of all models; 2) LiDAR-camera fusion models demonstrate
better robustness; 3) camera-only models are extremely vulnerable to image
corruptions, showing the indispensability of LiDAR point clouds. We release the
benchmarks and codes at https://github.com/kkkcx/3D_Corruptions_AD. We hope
that our benchmarks and findings can provide insights for future research on
developing robust 3D object detection models.",2303.11040v1,https://arxiv.org/pdf/2303.11040v1
"Recursive Euclidean Distance Based Robust Aggregation Technique For
  Federated Learning","Charuka Herath, Yogachandran Rahulamathavan, Xiaolan Liu","Federated learning has gained popularity as a solution to data availability
and privacy challenges in machine learning. However, the aggregation process of
local model updates to obtain a global model in federated learning is
susceptible to malicious attacks, such as backdoor poisoning, label-flipping,
and membership inference. Malicious users aim to sabotage the collaborative
learning process by training the local model with malicious data. In this
paper, we propose a novel robust aggregation approach based on recursive
Euclidean distance calculation. Our approach measures the distance of the local
models from the previous global model and assigns weights accordingly. Local
models far away from the global model are assigned smaller weights to minimize
the data poisoning effect during aggregation. Our experiments demonstrate that
the proposed algorithm outperforms state-of-the-art algorithms by at least
$5\%$ in accuracy while reducing time complexity by less than $55\%$. Our
contribution is significant as it addresses the critical issue of malicious
attacks in federated learning while improving the accuracy of the global model.",2303.11337v1,https://arxiv.org/pdf/2303.11337v1
"A Global Model Approach to Robust Few-Shot SAR Automatic Target
  Recognition",Nathan Inkawhich,"In real-world scenarios, it may not always be possible to collect hundreds of
labeled samples per class for training deep learning-based SAR Automatic Target
Recognition (ATR) models. This work specifically tackles the few-shot SAR ATR
problem, where only a handful of labeled samples may be available to support
the task of interest. Our approach is composed of two stages. In the first, a
global representation model is trained via self-supervised learning on a large
pool of diverse and unlabeled SAR data. In the second stage, the global model
is used as a fixed feature extractor and a classifier is trained to partition
the feature space given the few-shot support samples, while simultaneously
being calibrated to detect anomalous inputs. Unlike competing approaches which
require a pristine labeled dataset for pretraining via meta-learning, our
approach learns highly transferable features from unlabeled data that have
little-to-no relation to the downstream task. We evaluate our method in
standard and extended MSTAR operating conditions and find it to achieve high
accuracy and robust out-of-distribution detection in many different few-shot
settings. Our results are particularly significant because they show the merit
of a global model approach to SAR ATR, which makes minimal assumptions, and
provides many axes for extendability.",2303.10800v1,https://arxiv.org/pdf/2303.10800v1
Trainable Projected Gradient Method for Robust Fine-tuning,"Junjiao Tian, Xiaoliang Dai, Chih-Yao Ma, Zecheng He, Yen-Cheng Liu, Zsolt Kira","Recent studies on transfer learning have shown that selectively fine-tuning a
subset of layers or customizing different learning rates for each layer can
greatly improve robustness to out-of-distribution (OOD) data and retain
generalization capability in the pre-trained models. However, most of these
methods employ manually crafted heuristics or expensive hyper-parameter
searches, which prevent them from scaling up to large datasets and neural
networks. To solve this problem, we propose Trainable Projected Gradient Method
(TPGM) to automatically learn the constraint imposed for each layer for a
fine-grained fine-tuning regularization. This is motivated by formulating
fine-tuning as a bi-level constrained optimization problem. Specifically, TPGM
maintains a set of projection radii, i.e., distance constraints between the
fine-tuned model and the pre-trained model, for each layer, and enforces them
through weight projections. To learn the constraints, we propose a bi-level
optimization to automatically learn the best set of projection radii in an
end-to-end manner. Theoretically, we show that the bi-level optimization
formulation could explain the regularization capability of TPGM. Empirically,
with little hyper-parameter search cost, TPGM outperforms existing fine-tuning
methods in OOD performance while matching the best in-distribution (ID)
performance. For example, when fine-tuned on DomainNet-Real and ImageNet,
compared to vanilla fine-tuning, TPGM shows $22\%$ and $10\%$ relative OOD
improvement respectively on their sketch counterparts. Code is available at
\url{https://github.com/PotatoTian/TPGM}.",2303.10720v2,https://arxiv.org/pdf/2303.10720v2
"Study of Robust Adaptive Beamforming with Covariance Matrix
  Reconstruction Based on Power Spectral Estimation and Uncertainty Region","S. Mohammadzadeh, V. H. Nascimento, R. C. de Lamare, O. Kukrer","In this work, a simple and effective robust adaptive beamforming technique is
proposed for uniform linear arrays, which is based on the power spectral
estimation and uncertainty region (PSEUR) of the interference plus noise (IPN)
components. In particular, two algorithms are presented to find the angular
sector of interference in every snapshot based on the adopted spatial
uncertainty region of the interference direction. Moreover, a power spectrum is
introduced based on the estimation of the power of interference and noise
components, which allows the development of a robust approach to IPN covariance
matrix reconstruction. The proposed method has two main advantages. First, an
angular region that contains the interference direction is updated based on the
statistics of the array data. Secondly, the proposed IPN-PSEUR method avoids
estimating the power spectrum of the whole range of possible directions of the
interference sector. Simulation results show that the performance of the
proposed IPN-PSEUR beamformer is almost always close to the optimal value
across a wide range of signal-to-noise ratios.",2304.10502v1,https://arxiv.org/pdf/2304.10502v1
"Robust Mode Connectivity-Oriented Adversarial Defense: Enhancing Neural
  Network Robustness Against Diversified $\ell_p$ Attacks","Ren Wang, Yuxuan Li, Sijia Liu","Adversarial robustness is a key concept in measuring the ability of neural
networks to defend against adversarial attacks during the inference phase.
Recent studies have shown that despite the success of improving adversarial
robustness against a single type of attack using robust training techniques,
models are still vulnerable to diversified $\ell_p$ attacks. To achieve
diversified $\ell_p$ robustness, we propose a novel robust mode connectivity
(RMC)-oriented adversarial defense that contains two population-based learning
phases. The first phase, RMC, is able to search the model parameter space
between two pre-trained models and find a path containing points with high
robustness against diversified $\ell_p$ attacks. In light of the effectiveness
of RMC, we develop a second phase, RMC-based optimization, with RMC serving as
the basic unit for further enhancement of neural network diversified $\ell_p$
robustness. To increase computational efficiency, we incorporate learning with
a self-robust mode connectivity (SRMC) module that enables the fast
proliferation of the population used for endpoints of RMC. Furthermore, we draw
parallels between SRMC and the human immune system. Experimental results on
various datasets and model architectures demonstrate that the proposed defense
methods can achieve high diversified $\ell_p$ robustness against $\ell_\infty$,
$\ell_2$, $\ell_1$, and hybrid attacks. Codes are available at
\url{https://github.com/wangren09/MCGR}.",2303.10225v1,https://arxiv.org/pdf/2303.10225v1
Robust probabilistic inference via a constrained transport metric,"Abhisek Chakraborty, Anirban Bhattacharya, Debdeep Pati","Flexible Bayesian models are typically constructed using limits of large
parametric models with a multitude of parameters that are often
uninterpretable. In this article, we offer a novel alternative by constructing
an exponentially tilted empirical likelihood carefully designed to concentrate
near a parametric family of distributions of choice with respect to a novel
variant of the Wasserstein metric, which is then combined with a prior
distribution on model parameters to obtain a robustified posterior. The
proposed approach finds applications in a wide variety of robust inference
problems, where we intend to perform inference on the parameters associated
with the centering distribution in presence of outliers. Our proposed transport
metric enjoys great computational simplicity, exploiting the Sinkhorn
regularization for discrete optimal transport problems, and being inherently
parallelizable. We demonstrate superior performance of our methodology when
compared against state-of-the-art robust Bayesian inference methods. We also
demonstrate equivalence of our approach with a nonparametric Bayesian
formulation under a suitable asymptotic framework, testifying to its
flexibility. The constrained entropy maximization that sits at the heart of our
likelihood formulation finds its utility beyond robust Bayesian inference; an
illustration is provided in a trustworthy machine learning application.",2303.10085v1,https://arxiv.org/pdf/2303.10085v1
"How robust is randomized blind deconvolution via nuclear norm
  minimization against adversarial noise?","Julia Kostin, Felix Krahmer, Dominik Stöger","In this paper, we study the problem of recovering two unknown signals from
their convolution, which is commonly referred to as blind deconvolution.
Reformulation of blind deconvolution as a low-rank recovery problem has led to
multiple theoretical recovery guarantees in the past decade due to the success
of the nuclear norm minimization heuristic. In particular, in the absence of
noise, exact recovery has been established for sufficiently incoherent signals
contained in lower-dimensional subspaces. However, if the convolution is
corrupted by additive bounded noise, the stability of the recovery problem
remains much less understood. In particular, existing reconstruction bounds
involve large dimension factors and therefore fail to explain the empirical
evidence for dimension-independent robustness of nuclear norm minimization.
Recently, theoretical evidence has emerged for ill-posed behavior of low-rank
matrix recovery for sufficiently small noise levels. In this work, we develop
improved recovery guarantees for blind deconvolution with adversarial noise
which exhibit square-root scaling in the noise level. Hence, our results are
consistent with existing counterexamples which speak against linear scaling in
the noise level as demonstrated for related low-rank matrix recovery problems.",2303.10030v1,https://arxiv.org/pdf/2303.10030v1
"Deep Nonparametric Estimation of Intrinsic Data Structures by Chart
  Autoencoders: Generalization Error and Robustness","Hao Liu, Alex Havrilla, Rongjie Lai, Wenjing Liao","Autoencoders have demonstrated remarkable success in learning low-dimensional
latent features of high-dimensional data across various applications. Assuming
that data are sampled near a low-dimensional manifold, we employ chart
autoencoders, which encode data into low-dimensional latent features on a
collection of charts, preserving the topology and geometry of the data
manifold. Our paper establishes statistical guarantees on the generalization
error of chart autoencoders, and we demonstrate their denoising capabilities by
considering $n$ noisy training samples, along with their noise-free
counterparts, on a $d$-dimensional manifold. By training autoencoders, we show
that chart autoencoders can effectively denoise the input data with normal
noise. We prove that, under proper network architectures, chart autoencoders
achieve a squared generalization error in the order of $\displaystyle
n^{-\frac{2}{d+2}}\log^4 n$, which depends on the intrinsic dimension of the
manifold and only weakly depends on the ambient dimension and noise level. We
further extend our theory on data with noise containing both normal and
tangential components, where chart autoencoders still exhibit a denoising
effect for the normal component. As a special case, our theory also applies to
classical autoencoders, as long as the data manifold has a global
parametrization. Our results provide a solid theoretical foundation for the
effectiveness of autoencoders, which is further validated through several
numerical experiments.",2303.09863v3,https://arxiv.org/pdf/2303.09863v3
"DORIC : Domain Robust Fine-Tuning for Open Intent Clustering through
  Dependency Parsing","Jihyun Lee, Seungyeon Seo, Yunsu Kim, Gary Geunbae Lee","We present our work on Track 2 in the Dialog System Technology Challenges 11
(DSTC11). DSTC11-Track2 aims to provide a benchmark for zero-shot,
cross-domain, intent-set induction. In the absence of in-domain training
dataset, robust utterance representation that can be used across domains is
necessary to induce users' intentions. To achieve this, we leveraged a
multi-domain dialogue dataset to fine-tune the language model and proposed
extracting Verb-Object pairs to remove the artifacts of unnecessary
information. Furthermore, we devised the method that generates each cluster's
name for the explainability of clustered results. Our approach achieved 3rd
place in the precision score and showed superior accuracy and normalized mutual
information (NMI) score than the baseline model on various domain datasets.",2303.09827v1,https://arxiv.org/pdf/2303.09827v1
"It Is All About Data: A Survey on the Effects of Data on Adversarial
  Robustness","Peiyu Xiong, Michael Tegegn, Jaskeerat Singh Sarin, Shubhraneel Pal, Julia Rubin","Adversarial examples are inputs to machine learning models that an attacker
has intentionally designed to confuse the model into making a mistake. Such
examples pose a serious threat to the applicability of machine-learning-based
systems, especially in life- and safety-critical domains. To address this
problem, the area of adversarial robustness investigates mechanisms behind
adversarial attacks and defenses against these attacks. This survey reviews a
particular subset of this literature that focuses on investigating properties
of training data in the context of model robustness under evasion attacks. It
first summarizes the main properties of data leading to adversarial
vulnerability. It then discusses guidelines and techniques for improving
adversarial robustness by enhancing the data representation and learning
procedures, as well as techniques for estimating robustness guarantees given
particular data. Finally, it discusses gaps of knowledge and promising future
research directions in this area.",2303.09767v3,https://arxiv.org/pdf/2303.09767v3
Among Us: Adversarially Robust Collaborative Perception by Consensus,"Yiming Li, Qi Fang, Jiamu Bai, Siheng Chen, Felix Juefei-Xu, Chen Feng","Multiple robots could perceive a scene (e.g., detect objects) collaboratively
better than individuals, although easily suffer from adversarial attacks when
using deep learning. This could be addressed by the adversarial defense, but
its training requires the often-unknown attacking mechanism. Differently, we
propose ROBOSAC, a novel sampling-based defense strategy generalizable to
unseen attackers. Our key idea is that collaborative perception should lead to
consensus rather than dissensus in results compared to individual perception.
This leads to our hypothesize-and-verify framework: perception results with and
without collaboration from a random subset of teammates are compared until
reaching a consensus. In such a framework, more teammates in the sampled subset
often entail better perception performance but require longer sampling time to
reject potential attackers. Thus, we derive how many sampling trials are needed
to ensure the desired size of an attacker-free subset, or equivalently, the
maximum size of such a subset that we can successfully sample within a given
number of trials. We validate our method on the task of collaborative 3D object
detection in autonomous driving scenarios.",2303.09495v3,https://arxiv.org/pdf/2303.09495v3
BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition,"HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam","Named Entity Recognition (NER) is a fundamental task in natural language
processing that involves identifying and classifying named entities in text.
But much work hasn't been done for complex named entity recognition in Bangla,
despite being the seventh most spoken language globally. CNER is a more
challenging task than traditional NER as it involves identifying and
classifying complex and compound entities, which are not common in Bangla
language. In this paper, we present the winning solution of Bangla Complex
Named Entity Recognition Challenge - addressing the CNER task on BanglaCoNER
dataset using two different approaches, namely Conditional Random Fields (CRF)
and finetuning transformer based Deep Learning models such as BanglaBERT.
  The dataset consisted of 15300 sentences for training and 800 sentences for
validation, in the .conll format. Exploratory Data Analysis (EDA) on the
dataset revealed that the dataset had 7 different NER tags, with notable
presence of English words, suggesting that the dataset is synthetic and likely
a product of translation.
  We experimented with a variety of feature combinations including Part of
Speech (POS) tags, word suffixes, Gazetteers, and cluster information from
embeddings, while also finetuning the BanglaBERT (large) model for NER. We
found that not all linguistic patterns are immediately apparent or even
intuitive to humans, which is why Deep Learning based models has proved to be
the more effective model in NLP, including CNER task. Our fine tuned BanglaBERT
(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our
study highlights the importance of Bangla Complex Named Entity Recognition,
particularly in the context of synthetic datasets. Our findings also
demonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in
Bangla language.",2303.09306v2,https://arxiv.org/pdf/2303.09306v2
Robust Evaluation of Diffusion-Based Adversarial Purification,"Minjong Lee, Dongwoo Kim","We question the current evaluation practice on diffusion-based purification
methods. Diffusion-based purification methods aim to remove adversarial effects
from an input data point at test time. The approach gains increasing attention
as an alternative to adversarial training due to the disentangling between
training and testing. Well-known white-box attacks are often employed to
measure the robustness of the purification. However, it is unknown whether
these attacks are the most effective for the diffusion-based purification since
the attacks are often tailored for adversarial training. We analyze the current
practices and provide a new guideline for measuring the robustness of
purification methods against adversarial attacks. Based on our analysis, we
further propose a new purification strategy improving robustness compared to
the current diffusion-based purification methods.",2303.09051v3,https://arxiv.org/pdf/2303.09051v3
"Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness
  with Dataset Reinforcement","Fartash Faghri, Hadi Pouransari, Sachin Mehta, Mehrdad Farajtabar, Ali Farhadi, Mohammad Rastegari, Oncel Tuzel","We propose Dataset Reinforcement, a strategy to improve a dataset once such
that the accuracy of any model architecture trained on the reinforced dataset
is improved at no additional training cost for users. We propose a Dataset
Reinforcement strategy based on data augmentation and knowledge distillation.
Our generic strategy is designed based on extensive analysis across CNN- and
transformer-based models and performing large-scale study of distillation with
state-of-the-art models with various data augmentations. We create a reinforced
version of the ImageNet training dataset, called ImageNet+, as well as
reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained
with ImageNet+ are more accurate, robust, and calibrated, and transfer well to
downstream tasks (e.g., segmentation and detection). As an example, the
accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on
ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the
ImageNet validation set is also reduced by 9.9%. Using this backbone with
Mask-RCNN for object detection on MS-COCO, the mean average precision improves
by 0.8%. We reach similar gains for MobileNets, ViTs, and Swin-Transformers.
For MobileNetV3 and Swin-Tiny, we observe significant improvements on
ImageNet-R/A/C of up to 20% improved robustness. Models pretrained on ImageNet+
and fine-tuned on CIFAR-100+, Flowers-102+, and Food-101+, reach up to 3.4%
improved accuracy. The code, datasets, and pretrained models are available at
https://github.com/apple/ml-dr.",2303.08983v3,https://arxiv.org/pdf/2303.08983v3
Robust Pivoting Manipulation using Contact Implicit Bilevel Optimization,"Yuki Shirai, Devesh K. Jha, Arvind U. Raghunathan","Generalizable manipulation requires that robots be able to interact with
novel objects and environment. This requirement makes manipulation extremely
challenging as a robot has to reason about complex frictional interactions with
uncertainty in physical properties of the object and the environment. In this
paper, we study robust optimization for planning of pivoting manipulation in
the presence of uncertainties. We present insights about how friction can be
exploited to compensate for inaccuracies in the estimates of the physical
properties during manipulation. Under certain assumptions, we derive analytical
expressions for stability margin provided by friction during pivoting
manipulation. This margin is then used in a Contact Implicit Bilevel
Optimization (CIBO) framework to optimize a trajectory that maximizes this
stability margin to provide robustness against uncertainty in several physical
parameters of the object. We present analysis of the stability margin with
respect to several parameters involved in the underlying bilevel optimization
problem. We demonstrate our proposed method using a 6 DoF manipulator for
manipulating several different objects. We also design and validate an MPC
controller using the proposed algorithm which can track and regulate the
position of the object during manipulation.",2303.08965v2,https://arxiv.org/pdf/2303.08965v2
Agnostic Multi-Robust Learning Using ERM,"Saba Ahmadi, Avrim Blum, Omar Montasser, Kevin Stangl","A fundamental problem in robust learning is asymmetry: a learner needs to
correctly classify every one of exponentially-many perturbations that an
adversary might make to a test-time natural example. In contrast, the attacker
only needs to find one successful perturbation. Xiang et al.[2022] proposed an
algorithm that in the context of patch attacks for image classification,
reduces the effective number of perturbations from an exponential to a
polynomial number of perturbations and learns using an ERM oracle. However, to
achieve its guarantee, their algorithm requires the natural examples to be
robustly realizable. This prompts the natural question; can we extend their
approach to the non-robustly-realizable case where there is no classifier with
zero robust error?
  Our first contribution is to answer this question affirmatively by reducing
this problem to a setting in which an algorithm proposed by Feige et al.[2015]
can be applied, and in the process extend their guarantees. Next, we extend our
results to a multi-group setting and introduce a novel agnostic multi-robust
learning problem where the goal is to learn a predictor that achieves low
robust loss on a (potentially) rich collection of subgroups.",2303.08944v2,https://arxiv.org/pdf/2303.08944v2
"EvalAttAI: A Holistic Approach to Evaluating Attribution Maps in Robust
  and Non-Robust Models","Ian E. Nielsen, Ravi P. Ramachandran, Nidhal Bouaynaya, Hassan M. Fathallah-Shaykh, Ghulam Rasool","The expansion of explainable artificial intelligence as a field of research
has generated numerous methods of visualizing and understanding the black box
of a machine learning model. Attribution maps are generally used to highlight
the parts of the input image that influence the model to make a specific
decision. On the other hand, the robustness of machine learning models to
natural noise and adversarial attacks is also being actively explored. This
paper focuses on evaluating methods of attribution mapping to find whether
robust neural networks are more explainable. We explore this problem within the
application of classification for medical imaging. Explainability research is
at an impasse. There are many methods of attribution mapping, but no current
consensus on how to evaluate them and determine the ones that are the best. Our
experiments on multiple datasets (natural and medical imaging) and various
attribution methods reveal that two popular evaluation metrics, Deletion and
Insertion, have inherent limitations and yield contradictory results. We
propose a new explainability faithfulness metric (called EvalAttAI) that
addresses the limitations of prior metrics. Using our novel evaluation, we
found that Bayesian deep neural networks using the Variational Density
Propagation technique were consistently more explainable when used with the
best performing attribution method, the Vanilla Gradient. However, in general,
various types of robust neural networks may not be more explainable, despite
these models producing more visually plausible attribution maps.",2303.08866v1,https://arxiv.org/pdf/2303.08866v1
"Watch or Listen: Robust Audio-Visual Speech Recognition with Visual
  Corruption Modeling and Reliability Scoring","Joanna Hong, Minsu Kim, Jeongsoo Choi, Yong Man Ro","This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal
input corruption situations where audio inputs and visual inputs are both
corrupted, which is not well addressed in previous research directions.
Previous studies have focused on how to complement the corrupted audio inputs
with the clean visual inputs with the assumption of the availability of clean
visual inputs. However, in real life, clean visual inputs are not always
accessible and can even be corrupted by occluded lip regions or noises. Thus,
we firstly analyze that the previous AVSR models are not indeed robust to the
corruption of multimodal input streams, the audio and the visual inputs,
compared to uni-modal models. Then, we design multimodal input corruption
modeling to develop robust AVSR models. Lastly, we propose a novel AVSR
framework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that
is robust to the corrupted multimodal inputs. The AV-RelScore can determine
which input modal stream is reliable or not for the prediction and also can
exploit the more reliable streams in prediction. The effectiveness of the
proposed method is evaluated with comprehensive experiments on popular
benchmark databases, LRS2 and LRS3. We also show that the reliability scores
obtained by AV-RelScore well reflect the degree of corruption and make the
proposed model focus on the reliable multimodal representations.",2303.08536v2,https://arxiv.org/pdf/2303.08536v2
Bayesian Learning for the Robust Verification of Autonomous Robots,"Xingyu Zhao, Simos Gerasimou, Radu Calinescu, Calum Imrie, Valentin Robu, David Flynn","Autonomous robots used in infrastructure inspection, space exploration and
other critical missions operate in highly dynamic environments. As such, they
must continually verify their ability to complete the tasks associated with
these missions safely and effectively. Here we present a Bayesian learning
framework that enables this runtime verification of autonomous robots. The
framework uses prior knowledge and observations of the verified robot to learn
expected ranges for the occurrence rates of regular and singular (e.g.,
catastrophic failure) events. Interval continuous-time Markov models defined
using these ranges are then analysed to obtain expected intervals of variation
for system properties such as mission duration and success probability. We
apply the framework to an autonomous robotic mission for underwater
infrastructure inspection and repair. The formal proofs and experiments
presented in the paper show that our framework produces results that reflect
the uncertainty intrinsic to many real-world systems, enabling the robust
verification of their quantitative properties under parametric uncertainty.",2303.08476v2,https://arxiv.org/pdf/2303.08476v2
"Improving Adversarial Robustness with Hypersphere Embedding and
  Angular-based Regularizations","Olukorede Fakorede, Ashutosh Nirala, Modeste Atsague, Jin Tian","Adversarial training (AT) methods have been found to be effective against
adversarial attacks on deep neural networks. Many variants of AT have been
proposed to improve its performance. Pang et al. [1] have recently shown that
incorporating hypersphere embedding (HE) into the existing AT procedures
enhances robustness. We observe that the existing AT procedures are not
designed for the HE framework, and thus fail to adequately learn the angular
discriminative information available in the HE framework. In this paper, we
propose integrating HE into AT with regularization terms that exploit the rich
angular information available in the HE framework. Specifically, our method,
termed angular-AT, adds regularization terms to AT that explicitly enforce
weight-feature compactness and inter-class separation; all expressed in terms
of angular features. Experimental results show that angular-AT further improves
adversarial robustness.",2303.08289v1,https://arxiv.org/pdf/2303.08289v1
RODD: Robust Outlier Detection in Data Cubes,"Lara Kuhlmann, Daniel Wilmes, Emmanuel Müller, Markus Pauly, Daniel Horn","Data cubes are multidimensional databases, often built from several separate
databases, that serve as flexible basis for data analysis. Surprisingly,
outlier detection on data cubes has not yet been treated extensively. In this
work, we provide the first framework to evaluate robust outlier detection
methods in data cubes (RODD). We introduce a novel random forest-based outlier
detection approach (RODD-RF) and compare it with more traditional methods based
on robust location estimators. We propose a general type of test data and
examine all methods in a simulation study. Moreover, we apply ROOD-RF to real
world data. The results show that RODD-RF can lead to improved outlier
detection.",2303.08193v1,https://arxiv.org/pdf/2303.08193v1
"Allegro-Legato: Scalable, Fast, and Robust Neural-Network Quantum
  Molecular Dynamics via Sharpness-Aware Minimization","Hikaru Ibayashi, Taufeq Mohammed Razakh, Liqiu Yang, Thomas Linker, Marco Olguin, Shinnosuke Hattori, Ye Luo, Rajiv K. Kalia, Aiichiro Nakano, Ken-ichi Nomura, Priya Vashishta","Neural-network quantum molecular dynamics (NNQMD) simulations based on
machine learning are revolutionizing atomistic simulations of materials by
providing quantum-mechanical accuracy but orders-of-magnitude faster,
illustrated by ACM Gordon Bell prize (2020) and finalist (2021).
State-of-the-art (SOTA) NNQMD model founded on group theory featuring
rotational equivariance and local descriptors has provided much higher accuracy
and speed than those models, thus named Allegro (meaning fast). On massively
parallel supercomputers, however, it suffers a fidelity-scaling problem, where
growing number of unphysical predictions of interatomic forces prohibits
simulations involving larger numbers of atoms for longer times. Here, we solve
this problem by combining the Allegro model with sharpness aware minimization
(SAM) for enhancing the robustness of model through improved smoothness of the
loss landscape. The resulting Allegro-Legato (meaning fast and ""smooth"") model
was shown to elongate the time-to-failure $t_\textrm{failure}$, without
sacrificing computational speed or accuracy. Specifically, Allegro-Legato
exhibits much weaker dependence of timei-to-failure on the problem size,
$t_{\textrm{failure}} \propto N^{-0.14}$ ($N$ is the number of atoms) compared
to the SOTA Allegro model $\left(t_{\textrm{failure}} \propto
N^{-0.29}\right)$, i.e., systematically delayed time-to-failure, thus allowing
much larger and longer NNQMD simulations without failure. The model also
exhibits excellent computational scalability and GPU acceleration on the
Polaris supercomputer at Argonne Leadership Computing Facility. Such scalable,
accurate, fast and robust NNQMD models will likely find broad applications in
NNQMD simulations on emerging exaflop/s computers, with a specific example of
accounting for nuclear quantum effects in the dynamics of ammonia.",2303.08169v1,https://arxiv.org/pdf/2303.08169v1
"ISimDL: Importance Sampling-Driven Acceleration of Fault Injection
  Simulations for Evaluating the Robustness of Deep Learning","Alessio Colucci, Andreas Steininger, Muhammad Shafique","Deep Learning (DL) systems have proliferated in many applications, requiring
specialized hardware accelerators and chips. In the nano-era, devices have
become increasingly more susceptible to permanent and transient faults.
Therefore, we need an efficient methodology for analyzing the resilience of
advanced DL systems against such faults, and understand how the faults in
neural accelerator chips manifest as errors at the DL application level, where
faults can lead to undetectable and unrecoverable errors. Using fault
injection, we can perform resilience investigations of the DL system by
modifying neuron weights and outputs at the software-level, as if the hardware
had been affected by a transient fault. Existing fault models reduce the search
space, allowing faster analysis, but requiring a-priori knowledge on the model,
and not allowing further analysis of the filtered-out search space. Therefore,
we propose ISimDL, a novel methodology that employs neuron sensitivity to
generate importance sampling-based fault-scenarios. Without any a-priori
knowledge of the model-under-test, ISimDL provides an equivalent reduction of
the search space as existing works, while allowing long simulations to cover
all the possible faults, improving on existing model requirements. Our
experiments show that the importance sampling provides up to 15x higher
precision in selecting critical faults than the random uniform sampling,
reaching such precision in less than 100 faults. Additionally, we showcase
another practical use-case for importance sampling for reliable DNN design,
namely Fault Aware Training (FAT). By using ISimDL to select the faults leading
to errors, we can insert the faults during the DNN training process to harden
the DNN against such faults. Using importance sampling in FAT reduces the
overhead required for finding faults that lead to a predetermined drop in
accuracy by more than 12x.",2303.08035v2,https://arxiv.org/pdf/2303.08035v2
Verifying the Robustness of Automatic Credibility Assessment,"Piotr Przybyła, Alexander Shvets, Horacio Saggion","Text classification methods have been widely investigated as a way to detect
content of low credibility: fake news, social media bots, propaganda, etc.
Quite accurate models (likely based on deep neural networks) help in moderating
public electronic platforms and often cause content creators to face rejection
of their submissions or removal of already published texts. Having the
incentive to evade further detection, content creators try to come up with a
slightly modified version of the text (known as an attack with an adversarial
example) that exploit the weaknesses of classifiers and result in a different
output. Here we systematically test the robustness of popular text classifiers
against available attacking techniques and discover that, indeed, in some cases
insignificant changes in input text can mislead the models. We also introduce
BODEGA: a benchmark for testing both victim models and attack methods on four
misinformation detection tasks in an evaluation framework designed to simulate
real use-cases of content moderation. Finally, we manually analyse a subset
adversarial examples and check what kinds of modifications are used in
successful attacks. The BODEGA code and data is openly shared in hope of
enhancing the comparability and replicability of further research in this area",2303.08032v2,https://arxiv.org/pdf/2303.08032v2
SMUG: Towards robust MRI reconstruction by smoothed unrolling,"Hui Li, Jinghan Jia, Shijun Liang, Yuguang Yao, Saiprasad Ravishankar, Sijia Liu","Although deep learning (DL) has gained much popularity for accelerated
magnetic resonance imaging (MRI), recent studies have shown that DL-based MRI
reconstruction models could be oversensitive to tiny input perturbations (that
are called 'adversarial perturbations'), which cause unstable, low-quality
reconstructed images. This raises the question of how to design robust DL
methods for MRI reconstruction. To address this problem, we propose a novel
image reconstruction framework, termed SMOOTHED UNROLLING (SMUG), which
advances a deep unrolling-based MRI reconstruction model using a randomized
smoothing (RS)-based robust learning operation. RS, which improves the
tolerance of a model against input noises, has been widely used in the design
of adversarial defense for image classification. Yet, we find that the
conventional design that applies RS to the entire DL process is ineffective for
MRI reconstruction. We show that SMUG addresses the above issue by customizing
the RS operation based on the unrolling architecture of the DL-based MRI
reconstruction model. Compared to the vanilla RS approach and several variants
of SMUG, we show that SMUG improves the robustness of MRI reconstruction with
respect to a diverse set of perturbation sources, including perturbations to
the input measurements, different measurement sampling rates, and different
unrolling steps. Code for SMUG will be available at
https://github.com/LGM70/SMUG.",2303.12735v1,https://arxiv.org/pdf/2303.12735v1
Model-tuning Via Prompts Makes NLP Models Adversarially Robust,"Mrigank Raman, Pratyush Maini, J. Zico Kolter, Zachary C. Lipton, Danish Pruthi","In recent years, NLP practitioners have converged on the following practice:
(i) import an off-the-shelf pretrained (masked) language model; (ii) append a
multilayer perceptron atop the CLS token's hidden representation (with randomly
initialized weights); and (iii) fine-tune the entire model on a downstream task
(MLP-FT). This procedure has produced massive gains on standard NLP benchmarks,
but these models remain brittle, even to mild adversarial perturbations. In
this work, we demonstrate surprising gains in adversarial robustness enjoyed by
Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream
tasks. Rather than appending an MLP head to make output prediction, MVP appends
a prompt template to the input, and makes prediction via text
infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3
different models, MVP improves performance against adversarial substitutions by
an average of 8% over standard methods and even outperforms adversarial
training-based state-of-art defenses by 3.5%. By combining MVP with adversarial
training, we achieve further improvements in adversarial robustness while
maintaining performance on unperturbed examples. Finally, we conduct ablations
to investigate the mechanism underlying these gains. Notably, we find that the
main causes of vulnerability of MLP-FT can be attributed to the misalignment
between pre-training and fine-tuning tasks, and the randomly initialized MLP
parameters.",2303.07320v2,https://arxiv.org/pdf/2303.07320v2
"Learning Model-Free Robust Precoding for Cooperative Multibeam Satellite
  Communications","Steffen Gracla, Alea Schröder, Maik Röper, Carsten Bockelmann, Dirk Wübben, Armin Dekorsy","Direct Low Earth Orbit satellite-to-handheld links are expected to be part of
a new era in satellite communications. Space-Division Multiple Access precoding
is a technique that reduces interference among satellite beams, therefore
increasing spectral efficiency by allowing cooperating satellites to reuse
frequency. Over the past decades, optimal precoding solutions with perfect
channel state information have been proposed for several scenarios, whereas
robust precoding with only imperfect channel state information has been mostly
studied for simplified models. In particular, for Low Earth Orbit satellite
applications such simplified models might not be accurate. In this paper, we
use the function approximation capabilities of the Soft Actor-Critic deep
Reinforcement Learning algorithm to learn robust precoding with no knowledge of
the system imperfections.",2303.11427v1,https://arxiv.org/pdf/2303.11427v1
"Robust Contrastive Language-Image Pre-training against Data Poisoning
  and Backdoor Attacks","Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman","Contrastive vision-language representation learning has achieved
state-of-the-art performance for zero-shot classification, by learning from
millions of image-caption pairs crawled from the internet. However, the massive
data that powers large multimodal models such as CLIP, makes them extremely
vulnerable to various types of targeted data poisoning and backdoor attacks.
Despite this vulnerability, robust contrastive vision-language pre-training
against such attacks has remained unaddressed. In this work, we propose ROCLIP,
the first effective method for robust pre-training multimodal vision-language
models against targeted data poisoning and backdoor attacks. ROCLIP effectively
breaks the association between poisoned image-caption pairs by considering a
relatively large and varying pool of random captions, and matching every image
with the text that is most similar to it in the pool instead of its own
caption, every few epochs.It also leverages image and text augmentations to
further strengthen the defense and improve the performance of the model. Our
extensive experiments show that ROCLIP renders state-of-the-art targeted data
poisoning and backdoor attacks ineffective during pre-training CLIP models. In
particular, ROCLIP decreases the success rate for targeted data poisoning
attacks from 93.75% to 12.5% and that of backdoor attacks down to 0%, while
improving the model's linear probe performance by 10% and maintains a similar
zero shot performance compared to CLIP. By increasing the frequency of
matching, ROCLIP is able to defend strong attacks, which add up to 1% poisoned
examples to the data, and successfully maintain a low attack success rate of
12.5%, while trading off the performance on some tasks.",2303.06854v2,https://arxiv.org/pdf/2303.06854v2
"Twice Regularized Markov Decision Processes: The Equivalence between
  Robustness and Regularization","Esther Derman, Yevgeniy Men, Matthieu Geist, Shie Mannor","Robust Markov decision processes (MDPs) aim to handle changing or partially
known system dynamics. To solve them, one typically resorts to robust
optimization methods. However, this significantly increases computational
complexity and limits scalability in both learning and planning. On the other
hand, regularized MDPs show more stability in policy learning without impairing
time complexity. Yet, they generally do not encompass uncertainty in the model
dynamics. In this work, we aim to learn robust MDPs using regularization. We
first show that regularized MDPs are a particular instance of robust MDPs with
uncertain reward. We thus establish that policy iteration on reward-robust MDPs
can have the same time complexity as on regularized MDPs. We further extend
this relationship to MDPs with uncertain transitions: this leads to a
regularization term with an additional dependence on the value function. We
then generalize regularized MDPs to twice regularized MDPs ($\text{R}^2$ MDPs),
i.e., MDPs with $\textit{both}$ value and policy regularization. The
corresponding Bellman operators enable us to derive planning and learning
schemes with convergence and generalization guarantees, thus reducing
robustness to regularization. We numerically show this two-fold advantage on
tabular and physical domains, highlighting the fact that $\text{R}^2$ preserves
its efficacy in continuous environments.",2303.06654v1,https://arxiv.org/pdf/2303.06654v1
Ignorance is Bliss: Robust Control via Information Gating,"Manan Tomar, Riashat Islam, Matthew E. Taylor, Sergey Levine, Philip Bachman","Informational parsimony provides a useful inductive bias for learning
representations that achieve better generalization by being robust to noise and
spurious correlations. We propose \textit{information gating} as a way to learn
parsimonious representations that identify the minimal information required for
a task. When gating information, we can learn to reveal as little information
as possible so that a task remains solvable, or hide as little information as
possible so that a task becomes unsolvable. We gate information using a
differentiable parameterization of the signal-to-noise ratio, which can be
applied to arbitrary values in a network, e.g., erasing pixels at the input
layer or activations in some intermediate layer. When gating at the input
layer, our models learn which visual cues matter for a given task. When gating
intermediate layers, our models learn which activations are needed for
subsequent stages of computation. We call our approach \textit{InfoGating}. We
apply InfoGating to various objectives such as multi-step forward and inverse
dynamics models, Q-learning, and behavior cloning, highlighting how InfoGating
can naturally help in discarding information not relevant for control. Results
show that learning to identify and use minimal information can improve
generalization in downstream tasks. Policies based on InfoGating are
considerably more robust to irrelevant visual features, leading to improved
pretraining and finetuning of RL models.",2303.06121v2,https://arxiv.org/pdf/2303.06121v2
Distributionally Robust Optimization with Probabilistic Group,"Soumya Suvra Ghosal, Yixuan Li","Modern machine learning models may be susceptible to learning spurious
correlations that hold on average but not for the atypical group of samples. To
address the problem, previous approaches minimize the empirical worst-group
risk. Despite the promise, they often assume that each sample belongs to one
and only one group, which does not allow expressing the uncertainty in group
labeling. In this paper, we propose a novel framework PG-DRO, which explores
the idea of probabilistic group membership for distributionally robust
optimization. Key to our framework, we consider soft group membership instead
of hard group annotations. The group probabilities can be flexibly generated
using either supervised learning or zero-shot approaches. Our framework
accommodates samples with group membership ambiguity, offering stronger
flexibility and generality than the prior art. We comprehensively evaluate
PG-DRO on both image classification and natural language processing benchmarks,
establishing superior performance",2303.05809v1,https://arxiv.org/pdf/2303.05809v1
"Variance-aware robust reinforcement learning with linear function
  approximation under heavy-tailed rewards","Xiang Li, Qiang Sun","This paper presents two algorithms, AdaOFUL and VARA, for online sequential
decision-making in the presence of heavy-tailed rewards with only finite
variances. For linear stochastic bandits, we address the issue of heavy-tailed
rewards by modifying the adaptive Huber regression and proposing AdaOFUL.
AdaOFUL achieves a state-of-the-art regret bound of
$\widetilde{O}\big(d\big(\sum_{t=1}^T \nu_{t}^2\big)^{1/2}+d\big)$ as if the
rewards were uniformly bounded, where $\nu_{t}^2$ is the observed conditional
variance of the reward at round $t$, $d$ is the feature dimension, and
$\widetilde{O}(\cdot)$ hides logarithmic dependence. Building upon AdaOFUL, we
propose VARA for linear MDPs, which achieves a tighter variance-aware regret
bound of $\widetilde{O}(d\sqrt{HG^*K})$. Here, $H$ is the length of episodes,
$K$ is the number of episodes, and $G^*$ is a smaller instance-dependent
quantity that can be bounded by other instance-dependent quantities when
additional structural conditions on the MDP are satisfied. Our regret bound is
superior to the current state-of-the-art bounds in three ways: (1) it depends
on a tighter instance-dependent quantity and has optimal dependence on $d$ and
$H$, (2) we can obtain further instance-dependent bounds of $G^*$ under
additional structural conditions on the MDP, and (3) our regret bound is valid
even when rewards have only finite variances, achieving a level of generality
unmatched by previous works. Overall, our modified adaptive Huber regression
algorithm may serve as a useful building block in the design of algorithms for
online problems with heavy-tailed rewards.",2303.05606v2,https://arxiv.org/pdf/2303.05606v2
"Evaluating the Robustness of Conversational Recommender Systems by
  Adversarial Examples","Ali Montazeralghaem, James Allan","Conversational recommender systems (CRSs) are improving rapidly, according to
the standard recommendation accuracy metrics. However, it is essential to make
sure that these systems are robust in interacting with users including regular
and malicious users who want to attack the system by feeding the system
modified input data. In this paper, we propose an adversarial evaluation scheme
including four scenarios in two categories and automatically generate
adversarial examples to evaluate the robustness of these systems in the face of
different input data. By executing these adversarial examples we can compare
the ability of different conversational recommender systems to satisfy the
user's preferences. We evaluate three CRSs by the proposed adversarial examples
on two datasets. Our results show that none of these systems are robust and
reliable to the adversarial examples.",2303.05575v1,https://arxiv.org/pdf/2303.05575v1
On the Robustness of Text Vectorizers,"Rémi Catellier, Samuel Vaiter, Damien Garreau","A fundamental issue in machine learning is the robustness of the model with
respect to changes in the input. In natural language processing, models
typically contain a first embedding layer, transforming a sequence of tokens
into vector representations. While the robustness with respect to changes of
continuous inputs is well-understood, the situation is less clear when
considering discrete changes, for instance replacing a word by another in an
input sentence. Our work formally proves that popular embedding schemes, such
as concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit
robustness in the H\""older or Lipschitz sense with respect to the Hamming
distance. We provide quantitative bounds for these schemes and demonstrate how
the constants involved are affected by the length of the document. These
findings are exemplified through a series of numerical examples.",2303.07203v2,https://arxiv.org/pdf/2303.07203v2
Efficient Certified Training and Robustness Verification of Neural ODEs,"Mustafa Zeqiri, Mark Niklas Müller, Marc Fischer, Martin Vechev","Neural Ordinary Differential Equations (NODEs) are a novel neural
architecture, built around initial value problems with learned dynamics which
are solved during inference. Thought to be inherently more robust against
adversarial perturbations, they were recently shown to be vulnerable to strong
adversarial attacks, highlighting the need for formal guarantees. However,
despite significant progress in robustness verification for standard
feed-forward architectures, the verification of high dimensional NODEs remains
an open problem. In this work, we address this challenge and propose GAINS, an
analysis framework for NODEs combining three key ideas: (i) a novel class of
ODE solvers, based on variable but discrete time steps, (ii) an efficient graph
representation of solver trajectories, and (iii) a novel abstraction algorithm
operating on this graph representation. Together, these advances enable the
efficient analysis and certified training of high-dimensional NODEs, by
reducing the runtime from an intractable $O(\exp(d)+\exp(T))$ to ${O}(d+T^2
\log^2T)$ in the dimensionality $d$ and integration time $T$. In an extensive
evaluation on computer vision (MNIST and FMNIST) and time-series forecasting
(PHYSIO-NET) problems, we demonstrate the effectiveness of both our certified
training and verification methods.",2303.05246v1,https://arxiv.org/pdf/2303.05246v1
"FedREP: A Byzantine-Robust, Communication-Efficient and
  Privacy-Preserving Framework for Federated Learning","Yi-Rui Yang, Kun Wang, Wu-Jun Li","Federated learning (FL) has recently become a hot research topic, in which
Byzantine robustness, communication efficiency and privacy preservation are
three important aspects. However, the tension among these three aspects makes
it hard to simultaneously take all of them into account. In view of this
challenge, we theoretically analyze the conditions that a communication
compression method should satisfy to be compatible with existing
Byzantine-robust methods and privacy-preserving methods. Motivated by the
analysis results, we propose a novel communication compression method called
consensus sparsification (ConSpar). To the best of our knowledge, ConSpar is
the first communication compression method that is designed to be compatible
with both Byzantine-robust methods and privacy-preserving methods. Based on
ConSpar, we further propose a novel FL framework called FedREP, which is
Byzantine-robust, communication-efficient and privacy-preserving. We
theoretically prove the Byzantine robustness and the convergence of FedREP.
Empirical results show that FedREP can significantly outperform
communication-efficient privacy-preserving baselines. Furthermore, compared
with Byzantine-robust communication-efficient baselines, FedREP can achieve
comparable accuracy with the extra advantage of privacy preservation.",2303.05206v1,https://arxiv.org/pdf/2303.05206v1
Robust Millimeter Beamforming via Self-Supervised Hybrid Deep Learning,"Fenghao Zhu, Bohao Wang, Zhaohui Yang, Chongwen Huang, Zhaoyang Zhang, George C. Alexandropoulos, Chau Yuen, Merouane Debbah","Beamforming with large-scale antenna arrays has been widely used in recent
years, which is acknowledged as an important part in 5G and incoming 6G. Thus,
various techniques are leveraged to improve its performance, e.g., deep
learning, advanced optimization algorithms, etc. Although its performance in
many previous research scenarios with deep learning is quite attractive,
usually it drops rapidly when the environment or dataset is changed. Therefore,
designing effective beamforming network with strong robustness is an open issue
for the intelligent wireless communications. In this paper, we propose a robust
beamforming self-supervised network, and verify it in two kinds of different
datasets with various scenarios. Simulation results show that the proposed
self-supervised network with hybrid learning performs well in both classic
DeepMIMO and new WAIR-D dataset with the strong robustness under the various
environments. Also, we present the principle to explain the rationality of this
kind of hybrid learning, which is instructive to apply with more kinds of
datasets.",2303.12653v3,https://arxiv.org/pdf/2303.12653v3
The Robustness Verification of Linear Sound Quantum Classifiers,Su Bonan,"I present a quick and sound method for the robustness verification of a sort
of quantum classifiers who are Linear Sound. Since quantum machine learning has
been put into practice in relevant fields and Linear Sound Property, LSP is a
pervasive property, the method could be universally applied. I implemented my
method with a Quantum Convolutional Neural Network, QCNN using MindQuantum,
Huawei and successfully verified its robustness when classifying MNIST dataset.",2303.04982v1,https://arxiv.org/pdf/2303.04982v1
LMI-based Data-Driven Robust Model Predictive Control,"Hoang Hai Nguyen, Maurice Friedel, Rolf Findeisen","Predictive control, which is based on a model of the system to compute the
applied input optimizing the future system behavior, is by now widely used. If
the nominal models are not given or are very uncertain, data-driven model
predictive control approaches can be employed, where the system model or input
is directly obtained from past measured trajectories. Using a data
informativity framework and Finsler's lemma, we propose a data-driven robust
linear matrix inequality-based model predictive control scheme that considers
input and state constraints. Using these data, we formulate the problem as a
semi-definite optimization problem, whose solution provides the matrix gain for
the linear feedback, while the decisive variables are independent of the length
of the measurement data. The designed controller stabilizes the closed-loop
system asymptotically and guarantees constraint satisfaction. Numerical
examples are conducted to illustrate the method.",2303.04777v1,https://arxiv.org/pdf/2303.04777v1
Robust Multimodal Fusion for Human Activity Recognition,"Sanju Xaviar, Xin Yang, Omid Ardakanian","The proliferation of IoT and mobile devices equipped with heterogeneous
sensors has enabled new applications that rely on the fusion of time-series
data generated by multiple sensors with different modalities. While there are
promising deep neural network architectures for multimodal fusion, their
performance falls apart quickly in the presence of consecutive missing data and
noise across multiple modalities/sensors, the issues that are prevalent in
real-world settings. We propose Centaur, a multimodal fusion model for human
activity recognition (HAR) that is robust to these data quality issues. Centaur
combines a data cleaning module, which is a denoising autoencoder with
convolutional layers, and a multimodal fusion module, which is a deep
convolutional neural network with the self-attention mechanism to capture
cross-sensor correlation. We train Centaur using a stochastic data corruption
scheme and evaluate it on three datasets that contain data generated by
multiple inertial measurement units. Centaur's data cleaning module outperforms
2 state-of-the-art autoencoder-based models and its multimodal fusion module
outperforms 4 strong baselines. Compared to 2 related robust fusion
architectures, Centaur is more robust, achieving 11.59-17.52% higher accuracy
in HAR, especially in the presence of consecutive missing data in multiple
sensor channels.",2303.04636v1,https://arxiv.org/pdf/2303.04636v1
Byzantine-Robust Loopless Stochastic Variance-Reduced Gradient,"Nikita Fedin, Eduard Gorbunov","Distributed optimization with open collaboration is a popular field since it
provides an opportunity for small groups/companies/universities, and
individuals to jointly solve huge-scale problems. However, standard
optimization algorithms are fragile in such settings due to the possible
presence of so-called Byzantine workers -- participants that can send
(intentionally or not) incorrect information instead of the one prescribed by
the protocol (e.g., send anti-gradient instead of stochastic gradients). Thus,
the problem of designing distributed methods with provable robustness to
Byzantine workers has been receiving a lot of attention recently. In
particular, several works consider a very promising way to achieve Byzantine
tolerance via exploiting variance reduction and robust aggregation. The
existing approaches use SAGA- and SARAH-type variance-reduced estimators, while
another popular estimator -- SVRG -- is not studied in the context of
Byzantine-robustness. In this work, we close this gap in the literature and
propose a new method -- Byzantine-Robust Loopless Stochastic Variance Reduced
Gradient (BR-LSVRG). We derive non-asymptotic convergence guarantees for the
new method in the strongly convex case and compare its performance with
existing approaches in numerical experiments.",2303.04560v1,https://arxiv.org/pdf/2303.04560v1
"A robust method for reliability updating with equality information using
  sequential adaptive importance sampling","Xiong Xiao, Zeyu Wang, Quanwang Li","Reliability updating refers to a problem that integrates Bayesian updating
technique with structural reliability analysis and cannot be directly solved by
structural reliability methods (SRMs) when it involves equality information.
The state-of-the-art approaches transform equality information into inequality
information by introducing an auxiliary standard normal parameter. These
methods, however, encounter the loss of computational efficiency due to the
difficulty in finding the maximum of the likelihood function, the large
coefficient of variation (COV) associated with the posterior failure
probability and the inapplicability to dynamic updating problems where new
information is constantly available. To overcome these limitations, this paper
proposes an innovative method called RU-SAIS (reliability updating using
sequential adaptive importance sampling), which combines elements of sequential
importance sampling and K-means clustering to construct a series of important
sampling densities (ISDs) using Gaussian mixture. The last ISD of the sequence
is further adaptively modified through application of the cross entropy method.
The performance of RU-SAIS is demonstrated by three examples. Results show that
RU-SAIS achieves a more accurate and robust estimator of the posterior failure
probability than the existing methods such as subset simulation.",2303.04545v1,https://arxiv.org/pdf/2303.04545v1
"The Novel Adaptive Fractional Order Gradient Decent Algorithms Design
  via Robust Control","Jiaxu Liu, Song Chen, Shengze Cai, Chao Xu","The vanilla fractional order gradient descent may oscillatively converge to a
region around the global minimum instead of converging to the exact minimum
point, or even diverge, in the case where the objective function is strongly
convex. To address this problem, a novel adaptive fractional order gradient
descent (AFOGD) method and a novel adaptive fractional order accelerated
gradient descent (AFOAGD) method are proposed in this paper. Inspired by the
quadratic constraints and Lyapunov stability analysis from robust control
theory, we establish a linear matrix inequality to analyse the convergence of
our proposed algorithms. We prove that the proposed algorithms can achieve
R-linear convergence when the objective function is $\textbf{L-}$smooth and
$\textbf{m-}$strongly-convex. Several numerical simulations are demonstrated to
verify the effectiveness and superiority of our proposed algorithms.",2303.04328v1,https://arxiv.org/pdf/2303.04328v1
"DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep
  Adversarial Learning for Counterfactual Prediction and Treatment Effect
  Estimation on Real World Data","Shantanu Ghosh, Zheng Feng, Jiang Bian, Kevin Butler, Mattia Prosperi","Determining causal effects of interventions onto outcomes from real-world,
observational (non-randomized) data, e.g., treatment repurposing using
electronic health records, is challenging due to underlying bias. Causal deep
learning has improved over traditional techniques for estimating individualized
treatment effects (ITE). We present the Doubly Robust Variational
Information-theoretic Deep Adversarial Learning (DR-VIDAL), a novel generative
framework that combines two joint models of treatment and outcome, ensuring an
unbiased ITE estimation even when one of the two is misspecified. DR-VIDAL
integrates: (i) a variational autoencoder (VAE) to factorize confounders into
latent variables according to causal assumptions; (ii) an information-theoretic
generative adversarial network (Info-GAN) to generate counterfactuals; (iii) a
doubly robust block incorporating treatment propensities for outcome
predictions. On synthetic and real-world datasets (Infant Health and
Development Program, Twin Birth Registry, and National Supported Work Program),
DR-VIDAL achieves better performance than other non-generative and generative
methods. In conclusion, DR-VIDAL uniquely fuses causal assumptions, VAE,
Info-GAN, and doubly robustness into a comprehensive, performant framework.
Code is available at: https://github.com/Shantanu48114860/DR-VIDAL-AMIA-22
under MIT license.",2303.04201v3,https://arxiv.org/pdf/2303.04201v3
Robustness-preserving Lifelong Learning via Dataset Condensation,"Jinghan Jia, Yihua Zhang, Dogyoon Song, Sijia Liu, Alfred Hero","Lifelong learning (LL) aims to improve a predictive model as the data source
evolves continuously. Most work in this learning paradigm has focused on
resolving the problem of 'catastrophic forgetting,' which refers to a notorious
dilemma between improving model accuracy over new data and retaining accuracy
over previous data. Yet, it is also known that machine learning (ML) models can
be vulnerable in the sense that tiny, adversarial input perturbations can
deceive the models into producing erroneous predictions. This motivates the
research objective of this paper - specification of a new LL framework that can
salvage model robustness (against adversarial attacks) from catastrophic
forgetting. Specifically, we propose a new memory-replay LL strategy that
leverages modern bi-level optimization techniques to determine the 'coreset' of
the current data (i.e., a small amount of data to be memorized) for ease of
preserving adversarial robustness over time. We term the resulting LL framework
'Data-Efficient Robustness-Preserving LL' (DERPLL). The effectiveness of DERPLL
is evaluated for class-incremental image classification using ResNet-18 over
the CIFAR-10 dataset. Experimental results show that DERPLL outperforms the
conventional coreset-guided LL baseline and achieves a substantial improvement
in both standard accuracy and robust accuracy.",2303.04183v1,https://arxiv.org/pdf/2303.04183v1
"Domain Randomization for Robust, Affordable and Effective Closed-loop
  Control of Soft Robots","Gabriele Tiboni, Andrea Protopapa, Tatiana Tommasi, Giuseppe Averta","Soft robots are gaining popularity thanks to their intrinsic safety to
contacts and adaptability. However, the potentially infinite number of Degrees
of Freedom makes their modeling a daunting task, and in many cases only an
approximated description is available. This challenge makes reinforcement
learning (RL) based approaches inefficient when deployed on a realistic
scenario, due to the large domain gap between models and the real platform. In
this work, we demonstrate, for the first time, how Domain Randomization (DR)
can solve this problem by enhancing RL policies for soft robots with: i)
robustness w.r.t. unknown dynamics parameters; ii) reduced training times by
exploiting drastically simpler dynamic models for learning; iii) better
environment exploration, which can lead to exploitation of environmental
constraints for optimal performance. Moreover, we introduce a novel algorithmic
extension to previous adaptive domain randomization methods for the automatic
inference of dynamics parameters for deformable objects. We provide an
extensive evaluation in simulation on four different tasks and two soft robot
designs, opening interesting perspectives for future research on Reinforcement
Learning for closed-loop soft robot control.",2303.04136v2,https://arxiv.org/pdf/2303.04136v2
"New Perspectives on Regularization and Computation in Optimal
  Transport-Based Distributionally Robust Optimization","Soroosh Shafieezadeh-Abadeh, Liviu Aolaritei, Florian Dörfler, Daniel Kuhn","We study optimal transport-based distributionally robust optimization
problems where a fictitious adversary, often envisioned as nature, can choose
the distribution of the uncertain problem parameters by reshaping a prescribed
reference distribution at a finite transportation cost. In this framework, we
show that robustification is intimately related to various forms of variation
and Lipschitz regularization even if the transportation cost function fails to
be (some power of) a metric. We also derive conditions for the existence and
the computability of a Nash equilibrium between the decision-maker and nature,
and we demonstrate numerically that nature's Nash strategy can be viewed as a
distribution that is supported on remarkably deceptive adversarial samples.
Finally, we identify practically relevant classes of optimal transport-based
distributionally robust optimization problems that can be addressed with
efficient gradient descent algorithms even if the loss function or the
transportation cost function are nonconvex (but not both at the same time).",2303.03900v1,https://arxiv.org/pdf/2303.03900v1
Can Decentralized Learning be more robust than Federated Learning?,"Mathilde Raynal, Dario Pasquini, Carmela Troncoso","Decentralized Learning (DL) is a peer--to--peer learning approach that allows
a group of users to jointly train a machine learning model. To ensure
correctness, DL should be robust, i.e., Byzantine users must not be able to
tamper with the result of the collaboration. In this paper, we introduce two
\textit{new} attacks against DL where a Byzantine user can: make the network
converge to an arbitrary model of their choice, and exclude an arbitrary user
from the learning process. We demonstrate our attacks' efficiency against
Self--Centered Clipping, the state--of--the--art robust DL protocol. Finally,
we show that the capabilities decentralization grants to Byzantine users result
in decentralized learning \emph{always} providing less robustness than
federated learning.",2303.03829v1,https://arxiv.org/pdf/2303.03829v1
Robust Dominant Periodicity Detection for Time Series with Missing Data,"Qingsong Wen, Linxiao Yang, Liang Sun","Periodicity detection is an important task in time series analysis, but still
a challenging problem due to the diverse characteristics of time series data
like abrupt trend change, outlier, noise, and especially block missing data. In
this paper, we propose a robust and effective periodicity detection algorithm
for time series with block missing data. We first design a robust trend filter
to remove the interference of complicated trend patterns under missing data.
Then, we propose a robust autocorrelation function (ACF) that can handle
missing values and outliers effectively. We rigorously prove that the proposed
robust ACF can still work well when the length of the missing block is less
than $1/3$ of the period length. Last, by combining the time-frequency
information, our algorithm can generate the period length accurately. The
experimental results demonstrate that our algorithm outperforms existing
periodicity detection algorithms on real-world time series datasets.",2303.03553v1,https://arxiv.org/pdf/2303.03553v1
Robustness of Utilizing Feedback in Embodied Visual Navigation,"Jenny Zhang, Samson Yu, Jiafei Duan, Cheston Tan","This paper presents a framework for training an agent to actively request
help in object-goal navigation tasks, with feedback indicating the location of
the target object in its field of view. To make the agent more robust in
scenarios where a teacher may not always be available, the proposed training
curriculum includes a mix of episodes with and without feedback. The results
show that this approach improves the agent's performance, even in the absence
of feedback.",2303.15453v1,https://arxiv.org/pdf/2303.15453v1
MetaPhysiCa: OOD Robustness in Physics-informed Machine Learning,"S Chandra Mouli, Muhammad Ashraful Alam, Bruno Ribeiro","A fundamental challenge in physics-informed machine learning (PIML) is the
design of robust PIML methods for out-of-distribution (OOD) forecasting tasks.
These OOD tasks require learning-to-learn from observations of the same (ODE)
dynamical system with different unknown ODE parameters, and demand accurate
forecasts even under out-of-support initial conditions and out-of-support ODE
parameters. In this work we propose a solution for such tasks, which we define
as a meta-learning procedure for causal structure discovery (including
invariant risk minimization). Using three different OOD tasks, we empirically
observe that the proposed approach significantly outperforms existing
state-of-the-art PIML and deep learning methods.",2303.03181v1,https://arxiv.org/pdf/2303.03181v1
Masked Images Are Counterfactual Samples for Robust Fine-tuning,"Yao Xiao, Ziyi Tang, Pengxu Wei, Cong Liu, Liang Lin","Deep learning models are challenged by the distribution shift between the
training data and test data. Recently, the large models pre-trained on diverse
data have demonstrated unprecedented robustness to various distribution shifts.
However, fine-tuning these models can lead to a trade-off between
in-distribution (ID) performance and out-of-distribution (OOD) robustness.
Existing methods for tackling this trade-off do not explicitly address the OOD
robustness problem. In this paper, based on causal analysis of the
aforementioned problems, we propose a novel fine-tuning method, which uses
masked images as counterfactual samples that help improve the robustness of the
fine-tuning model. Specifically, we mask either the semantics-related or
semantics-unrelated patches of the images based on class activation map to
break the spurious correlation, and refill the masked patches with patches from
other images. The resulting counterfactual samples are used in feature-based
distillation with the pre-trained model. Extensive experiments verify that
regularizing the fine-tuning with the proposed masked images can achieve a
better trade-off between ID and OOD performance, surpassing previous methods on
the OOD performance. Our code is available at
https://github.com/Coxy7/robust-finetuning.",2303.03052v3,https://arxiv.org/pdf/2303.03052v3
Robust Autoencoders for Collective Corruption Removal,"Taihui Li, Hengkang Wang, Peng Le, XianE Tang, Ju Sun","Robust PCA is a standard tool for learning a linear subspace in the presence
of sparse corruption or rare outliers. What about robustly learning manifolds
that are more realistic models for natural data, such as images? There have
been several recent attempts to generalize robust PCA to manifold settings. In
this paper, we propose $\ell_1$- and scaling-invariant $\ell_1/\ell_2$-robust
autoencoders based on a surprisingly compact formulation built on the intuition
that deep autoencoders perform manifold learning. We demonstrate on several
standard image datasets that the proposed formulation significantly outperforms
all previous methods in collectively removing sparse corruption, without clean
images for training. Moreover, we also show that the learned manifold
structures can be generalized to unseen data samples effectively.",2303.02828v1,https://arxiv.org/pdf/2303.02828v1
"Improved Sample Complexity Bounds for Distributionally Robust
  Reinforcement Learning","Zaiyan Xu, Kishan Panaganti, Dileep Kalathil","We consider the problem of learning a control policy that is robust against
the parameter mismatches between the training environment and testing
environment. We formulate this as a distributionally robust reinforcement
learning (DR-RL) problem where the objective is to learn the policy which
maximizes the value function against the worst possible stochastic model of the
environment in an uncertainty set. We focus on the tabular episodic learning
setting where the algorithm has access to a generative model of the nominal
(training) environment around which the uncertainty set is defined. We propose
the Robust Phased Value Learning (RPVL) algorithm to solve this problem for the
uncertainty sets specified by four different divergences: total variation,
chi-square, Kullback-Leibler, and Wasserstein. We show that our algorithm
achieves $\tilde{\mathcal{O}}(|\mathcal{S}||\mathcal{A}| H^{5})$ sample
complexity, which is uniformly better than the existing results by a factor of
$|\mathcal{S}|$, where $|\mathcal{S}|$ is number of states, $|\mathcal{A}|$ is
the number of actions, and $H$ is the horizon length. We also provide the
first-ever sample complexity result for the Wasserstein uncertainty set.
Finally, we demonstrate the performance of our algorithm using simulation
experiments.",2303.02783v2,https://arxiv.org/pdf/2303.02783v2
"Robustness, Evaluation and Adaptation of Machine Learning Models in the
  Wild",Vihari Piratla,"Our goal is to improve reliability of Machine Learning (ML) systems deployed
in the wild. ML models perform exceedingly well when test examples are similar
to train examples. However, real-world applications are required to perform on
any distribution of test examples. Current ML systems can fail silently on test
examples with distribution shifts. In order to improve reliability of ML models
due to covariate or domain shift, we propose algorithms that enable models to:
(a) generalize to a larger family of test distributions, (b) evaluate accuracy
under distribution shifts, (c) adapt to a target distribution. We study causes
of impaired robustness to domain shifts and present algorithms for training
domain robust models. A key source of model brittleness is due to domain
overfitting, which our new training algorithms suppress and instead encourage
domain-general hypotheses. While we improve robustness over standard training
methods for certain problem settings, performance of ML systems can still vary
drastically with domain shifts. It is crucial for developers and stakeholders
to understand model vulnerabilities and operational ranges of input, which
could be assessed on the fly during the deployment, albeit at a great cost.
Instead, we advocate for proactively estimating accuracy surfaces over any
combination of prespecified and interpretable domain shifts for performance
forecasting. We present a label-efficient estimation to address estimation over
a combinatorial space of domain shifts. Further, when a model's performance on
a target domain is found to be poor, traditional approaches adapt the model
using the target domain's resources. Standard adaptation methods assume access
to sufficient labeled resources, which may be impractical for deployed models.
We initiate a study of lightweight adaptation techniques with only unlabeled
data resources with a focus on language applications.",2303.02781v1,https://arxiv.org/pdf/2303.02781v1
"Investigating Group Distributionally Robust Optimization for Deep
  Imbalanced Learning: A Case Study of Binary Tabular Data Classification","Ismail. B. Mustapha, Shafaatunnur Hasan, Hatem S Y Nabbus, Mohamed Mostafa Ali Montaser, Sunday Olusanya Olatunji, Siti Maryam Shamsuddin","One of the most studied machine learning challenges that recent studies have
shown the susceptibility of deep neural networks to is the class imbalance
problem. While concerted research efforts in this direction have been notable
in recent years, findings have shown that the canonical learning objective,
empirical risk minimization (ERM), is unable to achieve optimal imbalance
learning in deep neural networks given its bias to the majority class. An
alternative learning objective, group distributionally robust optimization
(gDRO), is investigated in this study for imbalance learning, focusing on
tabular imbalanced data as against image data that has dominated deep imbalance
learning research. Contrary to minimizing average per instance loss as in ERM,
gDRO seeks to minimize the worst group loss over the training data.
Experimental findings in comparison with ERM and classical imbalance methods
using four popularly used evaluation metrics in imbalance learning across
several benchmark imbalance binary tabular data of varying imbalance ratios
reveal impressive performance of gDRO, outperforming other compared methods in
terms of g-mean and roc-auc.",2303.02505v1,https://arxiv.org/pdf/2303.02505v1
RoLNiP: Robust Learning Using Noisy Pairwise Comparisons,"Samartha S Maheshwara, Naresh Manwani","This paper presents a robust approach for learning from noisy pairwise
comparisons. We propose sufficient conditions on the loss function under which
the risk minimization framework becomes robust to noise in the pairwise similar
dissimilar data. Our approach does not require the knowledge of noise rate in
the uniform noise case. In the case of conditional noise, the proposed method
depends on the noise rates. For such cases, we offer a provably correct
approach for estimating the noise rates. Thus, we propose an end-to-end
approach to learning robust classifiers in this setting. We experimentally show
that the proposed approach RoLNiP outperforms the robust state-of-the-art
methods for learning with noisy pairwise comparisons.",2303.02341v1,https://arxiv.org/pdf/2303.02341v1
"Improved Robustness Against Adaptive Attacks With Ensembles and
  Error-Correcting Output Codes","Thomas Philippon, Christian Gagné","Neural network ensembles have been studied extensively in the context of
adversarial robustness and most ensemble-based approaches remain vulnerable to
adaptive attacks. In this paper, we investigate the robustness of
Error-Correcting Output Codes (ECOC) ensembles through architectural
improvements and ensemble diversity promotion. We perform a comprehensive
robustness assessment against adaptive attacks and investigate the relationship
between ensemble diversity and robustness. Our results demonstrate the benefits
of ECOC ensembles for adversarial robustness compared to regular ensembles of
convolutional neural networks (CNNs) and show why the robustness of previous
implementations is limited. We also propose an adversarial training method
specific to ECOC ensembles that allows to further improve robustness to
adaptive attacks.",2303.02322v1,https://arxiv.org/pdf/2303.02322v1
"Certified Robust Neural Networks: Generalization and Corruption
  Resistance","Amine Bennouna, Ryan Lucas, Bart Van Parys","Recent work have demonstrated that robustness (to ""corruption"") can be at
odds with generalization. Adversarial training, for instance, aims to reduce
the problematic susceptibility of modern neural networks to small data
perturbations. Surprisingly, overfitting is a major concern in adversarial
training despite being mostly absent in standard training. We provide here
theoretical evidence for this peculiar ""robust overfitting"" phenomenon.
Subsequently, we advance a novel distributionally robust loss function bridging
robustness and generalization. We demonstrate both theoretically as well as
empirically the loss to enjoy a certified level of robustness against two
common types of corruption--data evasion and poisoning attacks--while ensuring
guaranteed generalization. We show through careful numerical experiments that
our resulting holistic robust (HR) training procedure yields SOTA performance.
Finally, we indicate that HR training can be interpreted as a direct extension
of adversarial training and comes with a negligible additional computational
burden. A ready-to-use python library implementing our algorithm is available
at https://github.com/RyanLucas3/HR_Neural_Networks.",2303.02251v2,https://arxiv.org/pdf/2303.02251v2
"Pre-trained Model Representations and their Robustness against Noise for
  Speech Emotion Analysis","Vikramjit Mitra, Vasudha Kowtha, Hsiang-Yun Sherry Chien, Erdrin Azemi, Carlos Avendano","Pre-trained model representations have demonstrated state-of-the-art
performance in speech recognition, natural language processing, and other
applications. Speech models, such as Bidirectional Encoder Representations from
Transformers (BERT) and Hidden units BERT (HuBERT), have enabled generating
lexical and acoustic representations to benefit speech recognition
applications. We investigated the use of pre-trained model representations for
estimating dimensional emotions, such as activation, valence, and dominance,
from speech. We observed that while valence may rely heavily on lexical
representations, activation and dominance rely mostly on acoustic information.
In this work, we used multi-modal fusion representations from pre-trained
models to generate state-of-the-art speech emotion estimation, and we showed a
100% and 30% relative improvement in concordance correlation coefficient (CCC)
on valence estimation compared to standard acoustic and lexical baselines.
Finally, we investigated the robustness of pre-trained model representations
against noise and reverberation degradation and noticed that lexical and
acoustic representations are impacted differently. We discovered that lexical
representations are more robust to distortions compared to acoustic
representations, and demonstrated that knowledge distillation from a
multi-modal model helps to improve the noise-robustness of acoustic-based
models.",2303.03177v1,https://arxiv.org/pdf/2303.03177v1
"PointCert: Point Cloud Classification with Deterministic Certified
  Robustness Guarantees","Jinghuai Zhang, Jinyuan Jia, Hongbin Liu, Neil Zhenqiang Gong","Point cloud classification is an essential component in many
security-critical applications such as autonomous driving and augmented
reality. However, point cloud classifiers are vulnerable to adversarially
perturbed point clouds. Existing certified defenses against adversarial point
clouds suffer from a key limitation: their certified robustness guarantees are
probabilistic, i.e., they produce an incorrect certified robustness guarantee
with some probability. In this work, we propose a general framework, namely
PointCert, that can transform an arbitrary point cloud classifier to be
certifiably robust against adversarial point clouds with deterministic
guarantees. PointCert certifiably predicts the same label for a point cloud
when the number of arbitrarily added, deleted, and/or modified points is less
than a threshold. Moreover, we propose multiple methods to optimize the
certified robustness guarantees of PointCert in three application scenarios. We
systematically evaluate PointCert on ModelNet and ScanObjectNN benchmark
datasets. Our results show that PointCert substantially outperforms
state-of-the-art certified defenses even though their robustness guarantees are
probabilistic.",2303.01959v1,https://arxiv.org/pdf/2303.01959v1
"Convex Bounds on the Softmax Function with Applications to Robustness
  Verification","Dennis Wei, Haoze Wu, Min Wu, Pin-Yu Chen, Clark Barrett, Eitan Farchi","The softmax function is a ubiquitous component at the output of neural
networks and increasingly in intermediate layers as well. This paper provides
convex lower bounds and concave upper bounds on the softmax function, which are
compatible with convex optimization formulations for characterizing neural
networks and other ML models. We derive bounds using both a natural
exponential-reciprocal decomposition of the softmax as well as an alternative
decomposition in terms of the log-sum-exp function. The new bounds are provably
and/or numerically tighter than linear bounds obtained in previous work on
robustness verification of transformers. As illustrations of the utility of the
bounds, we apply them to verification of transformers as well as of the
robustness of predictive uncertainty estimates of deep ensembles.",2303.01713v1,https://arxiv.org/pdf/2303.01713v1
"Streaming Algorithms for Learning with Experts: Deterministic Versus
  Robust","David P. Woodruff, Fred Zhang, Samson Zhou","In the online learning with experts problem, an algorithm must make a
prediction about an outcome on each of $T$ days (or times), given a set of $n$
experts who make predictions on each day (or time). The algorithm is given
feedback on the outcomes of each day, including the cost of its prediction and
the cost of the expert predictions, and the goal is to make a prediction with
the minimum cost, specifically compared to the best expert in the set. Recent
work by Srinivas, Woodruff, Xu, and Zhou (STOC 2022) introduced the study of
the online learning with experts problem under memory constraints.
  However, often the predictions made by experts or algorithms at some time
influence future outcomes, so that the input is adaptively chosen. Whereas
deterministic algorithms would be robust to adaptive inputs, existing
algorithms all crucially use randomization to sample a small number of experts.
  In this paper, we study deterministic and robust algorithms for the experts
problem. We first show a space lower bound of
$\widetilde{\Omega}\left(\frac{nM}{RT}\right)$ for any deterministic algorithm
that achieves regret $R$ when the best expert makes $M$ mistakes. Our result
shows that the natural deterministic algorithm, which iterates through pools of
experts until each expert in the pool has erred, is optimal up to
polylogarithmic factors. On the positive side, we give a randomized algorithm
that is robust to adaptive inputs that uses
$\widetilde{O}\left(\frac{n}{R\sqrt{T}}\right)$ space for $M=O\left(\frac{R^2
T}{\log^2 n}\right)$, thereby showing a smooth space-regret trade-off.",2303.01709v1,https://arxiv.org/pdf/2303.01709v1
"Miipher: A Robust Speech Restoration Model Integrating Self-Supervised
  Speech and Text Representations","Yuma Koizumi, Heiga Zen, Shigeki Karita, Yifan Ding, Kohei Yatabe, Nobuyuki Morioka, Yu Zhang, Wei Han, Ankur Bapna, Michiel Bacchiani","Speech restoration (SR) is a task of converting degraded speech signals into
high-quality ones. In this study, we propose a robust SR model called Miipher,
and apply Miipher to a new SR application: increasing the amount of
high-quality training data for speech generation by converting speech samples
collected from the Web to studio-quality. To make our SR model robust against
various degradation, we use (i) a speech representation extracted from w2v-BERT
for the input feature, and (ii) a text representation extracted from
transcripts via PnG-BERT as a linguistic conditioning feature. Experiments show
that Miipher (i) is robust against various audio degradation and (ii) enable us
to train a high-quality text-to-speech (TTS) model from restored speech samples
collected from the Web. Audio samples are available at our demo page:
google.github.io/df-conformer/miipher/",2303.01664v2,https://arxiv.org/pdf/2303.01664v2
"Robust Semi-Supervised Anomaly Detection via Adversarially Learned
  Continuous Noise Corruption","Jack W Barker, Neelanjan Bhowmik, Yona Falinie A Gaus, Toby P Breckon","Anomaly detection is the task of recognising novel samples which deviate
significantly from pre-establishednormality. Abnormal classes are not present
during training meaning that models must learn effective rep-resentations
solely across normal class data samples. Deep Autoencoders (AE) have been
widely used foranomaly detection tasks, but suffer from overfitting to a null
identity function. To address this problem, weimplement a training scheme
applied to a Denoising Autoencoder (DAE) which introduces an efficient methodof
producing Adversarially Learned Continuous Noise (ALCN) to maximally globally
corrupt the input priorto denoising. Prior methods have applied similar
approaches of adversarial training to increase the robustnessof DAE, however
they exhibit limitations such as slow inference speed reducing their real-world
applicabilityor producing generalised obfuscation which is more trivial to
denoise. We show through rigorous evaluationthat our ALCN method of
regularisation during training improves AUC performance during inference
whileremaining efficient over both classical, leave-one-out novelty detection
tasks with the variations-: 9 (normal)vs. 1 (abnormal) & 1 (normal) vs. 9
(abnormal); MNIST - AUCavg: 0.890 & 0.989, CIFAR-10 - AUCavg: 0.670& 0.742, in
addition to challenging real-world anomaly detection tasks: industrial
inspection (MVTEC-AD -AUCavg: 0.780) and plant disease detection (Plant Village
- AUC: 0.770) when compared to prior approaches.",2303.03925v1,https://arxiv.org/pdf/2303.03925v1
"The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness
  in ReLU Networks","Spencer Frei, Gal Vardi, Peter L. Bartlett, Nathan Srebro","In this work, we study the implications of the implicit bias of gradient flow
on generalization and adversarial robustness in ReLU networks. We focus on a
setting where the data consists of clusters and the correlations between
cluster means are small, and show that in two-layer ReLU networks gradient flow
is biased towards solutions that generalize well, but are highly vulnerable to
adversarial examples. Our results hold even in cases where the network has many
more parameters than training examples. Despite the potential for harmful
overfitting in such overparameterized settings, we prove that the implicit bias
of gradient flow prevents it. However, the implicit bias also leads to
non-robust solutions (susceptible to small adversarial $\ell_2$-perturbations),
even though robust networks that fit the data exist.",2303.01456v2,https://arxiv.org/pdf/2303.01456v2
"ArCL: Enhancing Contrastive Learning with Augmentation-Robust
  Representations","Xuyang Zhao, Tianqi Du, Yisen Wang, Jun Yao, Weiran Huang","Self-Supervised Learning (SSL) is a paradigm that leverages unlabeled data
for model training. Empirical studies show that SSL can achieve promising
performance in distribution shift scenarios, where the downstream and training
distributions differ. However, the theoretical understanding of its
transferability remains limited. In this paper, we develop a theoretical
framework to analyze the transferability of self-supervised contrastive
learning, by investigating the impact of data augmentation on it. Our results
reveal that the downstream performance of contrastive learning depends largely
on the choice of data augmentation. Moreover, we show that contrastive learning
fails to learn domain-invariant features, which limits its transferability.
Based on these theoretical insights, we propose a novel method called
Augmentation-robust Contrastive Learning (ArCL), which guarantees to learn
domain-invariant features and can be easily integrated with existing
contrastive learning algorithms. We conduct experiments on several datasets and
show that ArCL significantly improves the transferability of contrastive
learning.",2303.01092v2,https://arxiv.org/pdf/2303.01092v2
"Demystifying Causal Features on Adversarial Examples and Causal
  Inoculation for Robust Network by Adversarial Instrumental Variable
  Regression","Junho Kim, Byung-Kwan Lee, Yong Man Ro","The origin of adversarial examples is still inexplicable in research fields,
and it arouses arguments from various viewpoints, albeit comprehensive
investigations. In this paper, we propose a way of delving into the unexpected
vulnerability in adversarially trained networks from a causal perspective,
namely adversarial instrumental variable (IV) regression. By deploying it, we
estimate the causal relation of adversarial prediction under an unbiased
environment dissociated from unknown confounders. Our approach aims to
demystify inherent causal features on adversarial examples by leveraging a
zero-sum optimization game between a casual feature estimator (i.e., hypothesis
model) and worst-case counterfactuals (i.e., test function) disturbing to find
causal features. Through extensive analyses, we demonstrate that the estimated
causal features are highly related to the correct prediction for adversarial
robustness, and the counterfactuals exhibit extreme features significantly
deviating from the correct prediction. In addition, we present how to
effectively inoculate CAusal FEatures (CAFE) into defense networks for
improving adversarial robustness.",2303.01052v1,https://arxiv.org/pdf/2303.01052v1
"Re-weighting Based Group Fairness Regularization via Classwise Robust
  Optimization","Sangwon Jung, Taeeon Park, Sanghyuk Chun, Taesup Moon","Many existing group fairness-aware training methods aim to achieve the group
fairness by either re-weighting underrepresented groups based on certain rules
or using weakly approximated surrogates for the fairness metrics in the
objective as regularization terms. Although each of the learning schemes has
its own strength in terms of applicability or performance, respectively, it is
difficult for any method in the either category to be considered as a gold
standard since their successful performances are typically limited to specific
cases. To that end, we propose a principled method, dubbed as \ours, which
unifies the two learning schemes by incorporating a well-justified group
fairness metric into the training objective using a class wise distributionally
robust optimization (DRO) framework. We then develop an iterative optimization
algorithm that minimizes the resulting objective by automatically producing the
correct re-weights for each group. Our experiments show that FairDRO is
scalable and easily adaptable to diverse applications, and consistently
achieves the state-of-the-art performance on several benchmark datasets in
terms of the accuracy-fairness trade-off, compared to recent strong baselines.",2303.00442v1,https://arxiv.org/pdf/2303.00442v1
"Leveraging SO(3)-steerable convolutions for pose-robust semantic
  segmentation in 3D medical data","Ivan Diaz, Mario Geiger, Richard Iain McKinley","Convolutional neural networks (CNNs) allow for parameter sharing and
translational equivariance by using convolutional kernels in their linear
layers. By restricting these kernels to be SO(3)-steerable, CNNs can further
improve parameter sharing. These rotationally-equivariant convolutional layers
have several advantages over standard convolutional layers, including increased
robustness to unseen poses, smaller network size, and improved sample
efficiency. Despite this, most segmentation networks used in medical image
analysis continue to rely on standard convolutional kernels. In this paper, we
present a new family of segmentation networks that use equivariant voxel
convolutions based on spherical harmonics. These networks are robust to data
poses not seen during training, and do not require rotation-based data
augmentation during training. In addition, we demonstrate improved segmentation
performance in MRI brain tumor and healthy brain structure segmentation tasks,
with enhanced robustness to reduced amounts of training data and improved
parameter efficiency. Code to reproduce our results, and to implement the
equivariant segmentation networks for other tasks is available at
http://github.com/SCAN-NRAD/e3nn_Unet",2303.00351v3,https://arxiv.org/pdf/2303.00351v3
"Combating Exacerbated Heterogeneity for Robust Models in Federated
  Learning","Jianing Zhu, Jiangchao Yao, Tongliang Liu, Quanming Yao, Jianliang Xu, Bo Han","Privacy and security concerns in real-world applications have led to the
development of adversarially robust federated models. However, the
straightforward combination between adversarial training and federated learning
in one framework can lead to the undesired robustness deterioration. We
discover that the attribution behind this phenomenon is that the generated
adversarial data could exacerbate the data heterogeneity among local clients,
making the wrapped federated learning perform poorly. To deal with this
problem, we propose a novel framework called Slack Federated Adversarial
Training (SFAT), assigning the client-wise slack during aggregation to combat
the intensified heterogeneity. Theoretically, we analyze the convergence of the
proposed method to properly relax the objective when combining federated
learning and adversarial training. Experimentally, we verify the rationality
and effectiveness of SFAT on various benchmarked and real-world datasets with
different adversarial training and federated optimization methods. The code is
publicly available at https://github.com/ZFancy/SFAT.",2303.00250v1,https://arxiv.org/pdf/2303.00250v1
Convolutional Visual Prompt for Robust Visual Perception,"Yun-Yun Tsai, Chengzhi Mao, Junfeng Yang","Vision models are often vulnerable to out-of-distribution (OOD) samples
without adapting. While visual prompts offer a lightweight method of
input-space adaptation for large-scale vision models, they rely on a
high-dimensional additive vector and labeled data. This leads to overfitting
when adapting models in a self-supervised test-time setting without labels. We
introduce convolutional visual prompts (CVP) for label-free test-time
adaptation for robust visual perception. The structured nature of CVP demands
fewer trainable parameters, less than 1\% compared to standard visual prompts,
combating overfitting. Extensive experiments and analysis on a wide variety of
OOD visual perception tasks show that our approach is effective, improving
robustness by up to 5.87% over several large-scale models.",2303.00198v2,https://arxiv.org/pdf/2303.00198v2
"Transformed Low-Rank Parameterization Can Help Robust Generalization for
  Tensor Neural Networks","Andong Wang, Chao Li, Mingyuan Bai, Zhong Jin, Guoxu Zhou, Qibin Zhao","Achieving efficient and robust multi-channel data learning is a challenging
task in data science. By exploiting low-rankness in the transformed domain,
i.e., transformed low-rankness, tensor Singular Value Decomposition (t-SVD) has
achieved extensive success in multi-channel data representation and has
recently been extended to function representation such as Neural Networks with
t-product layers (t-NNs). However, it still remains unclear how t-SVD
theoretically affects the learning behavior of t-NNs. This paper is the first
to answer this question by deriving the upper bounds of the generalization
error of both standard and adversarially trained t-NNs. It reveals that the
t-NNs compressed by exact transformed low-rank parameterization can achieve a
sharper adversarial generalization bound. In practice, although t-NNs rarely
have exactly transformed low-rank weights, our analysis further shows that by
adversarial training with gradient flow (GF), the over-parameterized t-NNs with
ReLU activations are trained with implicit regularization towards transformed
low-rank parameterization under certain conditions. We also establish
adversarial generalization bounds for t-NNs with approximately transformed
low-rank weights. Our analysis indicates that the transformed low-rank
parameterization can promisingly enhance robust generalization for t-NNs.",2303.00196v3,https://arxiv.org/pdf/2303.00196v3
"Edit at your own risk: evaluating the robustness of edited models to
  distribution shifts","Davis Brown, Charles Godfrey, Cody Nizinski, Jonathan Tu, Henry Kvinge","The current trend toward ever-larger models makes standard retraining
procedures an ever-more expensive burden. For this reason, there is growing
interest in model editing, which enables computationally inexpensive,
interpretable, post-hoc model modifications. While many model editing
techniques are promising, research on the properties of edited models is
largely limited to evaluation of validation accuracy. The robustness of edited
models is an important and yet mostly unexplored topic. In this paper, we
employ recently developed techniques from the field of deep learning robustness
to investigate both how model editing affects the general robustness of a
model, as well as the robustness of the specific behavior targeted by the edit.
We find that edits tend to reduce general robustness, but that the degree of
degradation depends on the editing algorithm and layers chosen. Motivated by
these observations we introduce a new model editing algorithm, 1-layer
interpolation (1-LI), which uses weight-space interpolation to navigate the
trade-off between editing task accuracy and general robustness.",2303.00046v2,https://arxiv.org/pdf/2303.00046v2
Toward Robust Uncertainty Estimation with Random Activation Functions,"Yana Stoyanova, Soroush Ghandi, Maryam Tavakol","Deep neural networks are in the limelight of machine learning with their
excellent performance in many data-driven applications. However, they can lead
to inaccurate predictions when queried in out-of-distribution data points,
which can have detrimental effects especially in sensitive domains, such as
healthcare and transportation, where erroneous predictions can be very costly
and/or dangerous. Subsequently, quantifying the uncertainty of the output of a
neural network is often leveraged to evaluate the confidence of its
predictions, and ensemble models have proved to be effective in measuring the
uncertainty by utilizing the variance of predictions over a pool of models. In
this paper, we propose a novel approach for uncertainty quantification via
ensembles, called Random Activation Functions (RAFs) Ensemble, that aims at
improving the ensemble diversity toward a more robust estimation, by
accommodating each neural network with a different (random) activation
function. Extensive empirical study demonstrates that RAFs Ensemble outperforms
state-of-the-art ensemble uncertainty quantification methods on both synthetic
and real-world datasets in a series of regression tasks.",2302.14552v1,https://arxiv.org/pdf/2302.14552v1
"RoPAWS: Robust Semi-supervised Representation Learning from Uncurated
  Data","Sangwoo Mo, Jong-Chyi Su, Chih-Yao Ma, Mido Assran, Ishan Misra, Licheng Yu, Sean Bell","Semi-supervised learning aims to train a model using limited labels.
State-of-the-art semi-supervised methods for image classification such as PAWS
rely on self-supervised representations learned with large-scale unlabeled but
curated data. However, PAWS is often less effective when using real-world
unlabeled data that is uncurated, e.g., contains out-of-class data. We propose
RoPAWS, a robust extension of PAWS that can work with real-world unlabeled
data. We first reinterpret PAWS as a generative classifier that models
densities using kernel density estimation. From this probabilistic perspective,
we calibrate its prediction based on the densities of labeled and unlabeled
data, which leads to a simple closed-form solution from the Bayes' rule. We
demonstrate that RoPAWS significantly improves PAWS for uncurated Semi-iNat by
+5.3% and curated ImageNet by +0.4%.",2302.14483v1,https://arxiv.org/pdf/2302.14483v1
Robust Field-level Likelihood-free Inference with Galaxies,"Natalí S. M. de Santi, Helen Shao, Francisco Villaescusa-Navarro, L. Raul Abramo, Romain Teyssier, Pablo Villanueva-Domingo, Yueying Ni, Daniel Anglés-Alcázar, Shy Genel, Elena Hernandez-Martinez, Ulrich P. Steinwandel, Christopher C. Lovell, Klaus Dolag, Tiago Castro, Mark Vogelsberger","We train graph neural networks to perform field-level likelihood-free
inference using galaxy catalogs from state-of-the-art hydrodynamic simulations
of the CAMELS project. Our models are rotational, translational, and
permutation invariant and do not impose any cut on scale. From galaxy catalogs
that only contain $3$D positions and radial velocities of $\sim 1, 000$
galaxies in tiny $(25~h^{-1}{\rm Mpc})^3$ volumes our models can infer the
value of $\Omega_{\rm m}$ with approximately $12$ % precision. More
importantly, by testing the models on galaxy catalogs from thousands of
hydrodynamic simulations, each having a different efficiency of supernova and
AGN feedback, run with five different codes and subgrid models - IllustrisTNG,
SIMBA, Astrid, Magneticum, SWIFT-EAGLE -, we find that our models are robust to
changes in astrophysics, subgrid physics, and subhalo/galaxy finder.
Furthermore, we test our models on $1,024$ simulations that cover a vast region
in parameter space - variations in $5$ cosmological and $23$ astrophysical
parameters - finding that the model extrapolates really well. Our results
indicate that the key to building a robust model is the use of both galaxy
positions and velocities, suggesting that the network have likely learned an
underlying physical relation that does not depend on galaxy formation and is
valid on scales larger than $\sim10~h^{-1}{\rm kpc}$.",2302.14101v2,https://arxiv.org/pdf/2302.14101v2
Diversity matters: Robustness of bias measurements in Wikidata,"Paramita Das, Sai Keerthana Karnam, Anirban Panda, Bhanu Prakash Reddy Guda, Soumya Sarkar, Animesh Mukherjee","With the widespread use of knowledge graphs (KG) in various automated AI
systems and applications, it is very important to ensure that information
retrieval algorithms leveraging them are free from societal biases. Previous
works have depicted biases that persist in KGs, as well as employed several
metrics for measuring the biases. However, such studies lack the systematic
exploration of the sensitivity of the bias measurements, through varying
sources of data, or the embedding algorithms used. To address this research
gap, in this work, we present a holistic analysis of bias measurement on the
knowledge graph. First, we attempt to reveal data biases that surface in
Wikidata for thirteen different demographics selected from seven continents.
Next, we attempt to unfold the variance in the detection of biases by two
different knowledge graph embedding algorithms - TransE and ComplEx. We conduct
our extensive experiments on a large number of occupations sampled from the
thirteen demographics with respect to the sensitive attribute, i.e., gender.
Our results show that the inherent data bias that persists in KG can be altered
by specific algorithm bias as incorporated by KG embedding learning algorithms.
Further, we show that the choice of the state-of-the-art KG embedding algorithm
has a strong impact on the ranking of biased occupations irrespective of
gender. We observe that the similarity of the biased occupations across
demographics is minimal which reflects the socio-cultural differences around
the globe. We believe that this full-scale audit of the bias measurement
pipeline will raise awareness among the community while deriving insights
related to design choices of data and algorithms both and refrain from the
popular dogma of ``one-size-fits-all''.",2302.14027v1,https://arxiv.org/pdf/2302.14027v1
Robust Robot Planning for Human-Robot Collaboration,"Yang You, Vincent Thomas, Francis Colas, Rachid Alami, Olivier Buffet","In human-robot collaboration, the objectives of the human are often unknown
to the robot. Moreover, even assuming a known objective, the human behavior is
also uncertain. In order to plan a robust robot behavior, a key preliminary
question is then: How to derive realistic human behaviors given a known
objective? A major issue is that such a human behavior should itself account
for the robot behavior, otherwise collaboration cannot happen. In this paper,
we rely on Markov decision models, representing the uncertainty over the human
objective as a probability distribution over a finite set of objective
functions (inducing a distribution over human behaviors). Based on this, we
propose two contributions: 1) an approach to automatically generate an
uncertain human behavior (a policy) for each given objective function while
accounting for possible robot behaviors; and 2) a robot planning algorithm that
is robust to the above-mentioned uncertainties and relies on solving a
partially observable Markov decision process (POMDP) obtained by reasoning on a
distribution over human behaviors. A co-working scenario allows conducting
experiments and presenting qualitative and quantitative results to evaluate our
approach.",2302.13916v1,https://arxiv.org/pdf/2302.13916v1
"Evaluating Robustness and Uncertainty of Graph Models Under Structural
  Distributional Shifts","Gleb Bazhenov, Denis Kuznedelev, Andrey Malinin, Artem Babenko, Liudmila Prokhorenkova","In reliable decision-making systems based on machine learning, models have to
be robust to distributional shifts or provide the uncertainty of their
predictions. In node-level problems of graph learning, distributional shifts
can be especially complex since the samples are interdependent. To evaluate the
performance of graph models, it is important to test them on diverse and
meaningful distributional shifts. However, most graph benchmarks considering
distributional shifts for node-level problems focus mainly on node features,
while structural properties are also essential for graph problems. In this
work, we propose a general approach for inducing diverse distributional shifts
based on graph structure. We use this approach to create data splits according
to several structural node properties: popularity, locality, and density. In
our experiments, we thoroughly evaluate the proposed distributional shifts and
show that they can be quite challenging for existing graph models. We also
reveal that simple models often outperform more sophisticated methods on the
considered structural shifts. Finally, our experiments provide evidence that
there is a trade-off between the quality of learned representations for the
base classification task under structural distributional shift and the ability
to separate the nodes from different distributions using these representations.",2302.13875v4,https://arxiv.org/pdf/2302.13875v4
"Self-Supervised Pre-Training for Deep Image Prior-Based Robust PET Image
  Denoising","Yuya Onishi, Fumio Hashimoto, Kibo Ote, Keisuke Matsubara, Masanobu Ibaraki","Deep image prior (DIP) has been successfully applied to positron emission
tomography (PET) image restoration, enabling represent implicit prior using
only convolutional neural network architecture without training dataset,
whereas the general supervised approach requires massive low- and high-quality
PET image pairs. To answer the increased need for PET imaging with DIP, it is
indispensable to improve the performance of the underlying DIP itself. Here, we
propose a self-supervised pre-training model to improve the DIP-based PET image
denoising performance. Our proposed pre-training model acquires transferable
and generalizable visual representations from only unlabeled PET images by
restoring various degraded PET images in a self-supervised approach. We
evaluated the proposed method using clinical brain PET data with various
radioactive tracers ($^{18}$F-florbetapir, $^{11}$C-Pittsburgh compound-B,
$^{18}$F-fluoro-2-deoxy-D-glucose, and $^{15}$O-CO$_{2}$) acquired from
different PET scanners. The proposed method using the self-supervised
pre-training model achieved robust and state-of-the-art denoising performance
while retaining spatial details and quantification accuracy compared to other
unsupervised methods and pre-training model. These results highlight the
potential that the proposed method is particularly effective against rare
diseases and probes and helps reduce the scan time or the radiotracer dose
without affecting the patients.",2302.13546v1,https://arxiv.org/pdf/2302.13546v1
Kernel Conditional Moment Constraints for Confounding Robust Inference,"Kei Ishikawa, Niao He","We study policy evaluation of offline contextual bandits subject to
unobserved confounders. Sensitivity analysis methods are commonly used to
estimate the policy value under the worst-case confounding over a given
uncertainty set. However, existing work often resorts to some coarse relaxation
of the uncertainty set for the sake of tractability, leading to overly
conservative estimation of the policy value. In this paper, we propose a
general estimator that provides a sharp lower bound of the policy value. It can
be shown that our estimator contains the recently proposed sharp estimator by
Dorn and Guo (2022) as a special case, and our method enables a novel extension
of the classical marginal sensitivity model using f-divergence. To construct
our estimator, we leverage the kernel method to obtain a tractable
approximation to the conditional moment constraints, which traditional
non-sharp estimators failed to take into account. In the theoretical analysis,
we provide a condition for the choice of the kernel which guarantees no
specification error that biases the lower bound estimation. Furthermore, we
provide consistency guarantees of policy evaluation and learning. In the
experiments with synthetic and real-world data, we demonstrate the
effectiveness of the proposed method.",2302.13348v2,https://arxiv.org/pdf/2302.13348v2
A Finite Sample Complexity Bound for Distributionally Robust Q-learning,"Shengbo Wang, Nian Si, Jose Blanchet, Zhengyuan Zhou","We consider a reinforcement learning setting in which the deployment
environment is different from the training environment. Applying a robust
Markov decision processes formulation, we extend the distributionally robust
$Q$-learning framework studied in Liu et al. [2022]. Further, we improve the
design and analysis of their multi-level Monte Carlo estimator. Assuming access
to a simulator, we prove that the worst-case expected sample complexity of our
algorithm to learn the optimal robust $Q$-function within an $\epsilon$ error
in the sup norm is upper bounded by $\tilde
O(|S||A|(1-\gamma)^{-5}\epsilon^{-2}p_{\wedge}^{-6}\delta^{-4})$, where
$\gamma$ is the discount rate, $p_{\wedge}$ is the non-zero minimal support
probability of the transition kernels and $\delta$ is the uncertainty size.
This is the first sample complexity result for the model-free robust RL
problem. Simulation studies further validate our theoretical results.",2302.13203v3,https://arxiv.org/pdf/2302.13203v3
"UnbiasedNets: A Dataset Diversification Framework for Robustness Bias
  Alleviation in Neural Networks","Mahum Naseer, Bharath Srinivas Prabakaran, Osman Hasan, Muhammad Shafique","Performance of trained neural network (NN) models, in terms of testing
accuracy, has improved remarkably over the past several years, especially with
the advent of deep learning. However, even the most accurate NNs can be biased
toward a specific output classification due to the inherent bias in the
available training datasets, which may propagate to the real-world
implementations. This paper deals with the robustness bias, i.e., the bias
exhibited by the trained NN by having a significantly large robustness to noise
for a certain output class, as compared to the remaining output classes. The
bias is shown to result from imbalanced datasets, i.e., the datasets where all
output classes are not equally represented. Towards this, we propose the
UnbiasedNets framework, which leverages K-means clustering and the NN's noise
tolerance to diversify the given training dataset, even from relatively smaller
datasets. This generates balanced datasets and reduces the bias within the
datasets themselves. To the best of our knowledge, this is the first framework
catering to the robustness bias problem in NNs. We use real-world datasets to
demonstrate the efficacy of the UnbiasedNets for data diversification, in case
of both binary and multi-label classifiers. The results are compared to
well-known tools aimed at generating balanced datasets, and illustrate how
existing works have limited success while addressing the robustness bias. In
contrast, UnbiasedNets provides a notable improvement over existing works,
while even reducing the robustness bias significantly in some cases, as
observed by comparing the NNs trained on the diversified and original datasets.",2302.12538v2,https://arxiv.org/pdf/2302.12538v2
"Robust Weight Signatures: Gaining Robustness as Easy as Patching
  Weights?","Ruisi Cai, Zhenyu Zhang, Zhangyang Wang","Given a robust model trained to be resilient to one or multiple types of
distribution shifts (e.g., natural image corruptions), how is that ""robustness""
encoded in the model weights, and how easily can it be disentangled and/or
""zero-shot"" transferred to some other models? This paper empirically suggests a
surprisingly simple answer: linearly - by straightforward model weight
arithmetic! We start by drawing several key observations: (1)assuming that we
train the same model architecture on both a clean dataset and its corrupted
version, resultant weights mostly differ in shallow layers; (2)the weight
difference after projection, which we call ""Robust Weight Signature"" (RWS),
appears to be discriminative and indicative of different corruption types;
(3)for the same corruption type, the RWSs obtained by one model architecture
are highly consistent and transferable across different datasets.
  We propose a minimalistic model robustness ""patching"" framework that carries
a model trained on clean data together with its pre-extracted RWSs. In this
way, injecting certain robustness to the model is reduced to directly adding
the corresponding RWS to its weight. We verify our proposed framework to be
remarkably (1)lightweight. since RWSs concentrate on the shallowest few layers
and we further show they can be painlessly quantized, storing an RWS is up to
13 x more compact than storing the full weight copy; (2)in-situ adjustable.
RWSs can be appended as needed and later taken off to restore the intact clean
model. We further demonstrate one can linearly re-scale the RWS to control the
patched robustness strength; (3)composable. Multiple RWSs can be added
simultaneously to patch more comprehensive robustness at once; and
(4)transferable. Even when the clean model backbone is continually adapted or
updated, RWSs remain as effective patches due to their outstanding
cross-dataset transferability.",2302.12480v1,https://arxiv.org/pdf/2302.12480v1
"On the Hardness of Robustness Transfer: A Perspective from Rademacher
  Complexity over Symmetric Difference Hypothesis Space","Yuyang Deng, Nidham Gazagnadou, Junyuan Hong, Mehrdad Mahdavi, Lingjuan Lyu","Recent studies demonstrated that the adversarially robust learning under
$\ell_\infty$ attack is harder to generalize to different domains than standard
domain adaptation. How to transfer robustness across different domains has been
a key question in domain adaptation field. To investigate the fundamental
difficulty behind adversarially robust domain adaptation (or robustness
transfer), we propose to analyze a key complexity measure that controls the
cross-domain generalization: the adversarial Rademacher complexity over {\em
symmetric difference hypothesis space} $\mathcal{H} \Delta \mathcal{H}$. For
linear models, we show that adversarial version of this complexity is always
greater than the non-adversarial one, which reveals the intrinsic hardness of
adversarially robust domain adaptation. We also establish upper bounds on this
complexity measure. Then we extend them to the ReLU neural network class by
upper bounding the adversarial Rademacher complexity in the binary
classification setting. Finally, even though the robust domain adaptation is
provably harder, we do find positive relation between robust learning and
standard domain adaptation. We explain \emph{how adversarial training helps
domain adaptation in terms of standard risk}. We believe our results initiate
the study of the generalization theory of adversarially robust domain
adaptation, and could shed lights on distributed adversarially robust learning
from heterogeneous sources, e.g., federated learning scenario.",2302.12351v1,https://arxiv.org/pdf/2302.12351v1
Uncertainty Injection: A Deep Learning Method for Robust Optimization,"Wei Cui, Wei Yu","This paper proposes a paradigm of uncertainty injection for training deep
learning model to solve robust optimization problems. The majority of existing
studies on deep learning focus on the model learning capability, while assuming
the quality and accuracy of the inputs data can be guaranteed. However, in
realistic applications of deep learning for solving optimization problems, the
accuracy of inputs, which are the problem parameters in this case, plays a
large role. This is because, in many situations, it is often costly or sometime
impossible to obtain the problem parameters accurately, and correspondingly, it
is highly desirable to develop learning algorithms that can account for the
uncertainties in the input and produce solutions that are robust against these
uncertainties. This paper presents a novel uncertainty injection scheme for
training machine learning models that are capable of implicitly accounting for
the uncertainties and producing statistically robust solutions. We further
identify the wireless communications as an application field where
uncertainties are prevalent in problem parameters such as the channel
coefficients. We show the effectiveness of the proposed training scheme in two
applications: the robust power loading for multiuser
multiple-input-multiple-output (MIMO) downlink transmissions; and the robust
power control for device-to-device (D2D) networks.",2302.12304v2,https://arxiv.org/pdf/2302.12304v2
"Beyond Moments: Robustly Learning Affine Transformations with
  Asymptotically Optimal Error","He Jia, Pravesh K . Kothari, Santosh S. Vempala","We present a polynomial-time algorithm for robustly learning an unknown
affine transformation of the standard hypercube from samples, an important and
well-studied setting for independent component analysis (ICA). Specifically,
given an $\epsilon$-corrupted sample from a distribution $D$ obtained by
applying an unknown affine transformation $x \rightarrow Ax+s$ to the uniform
distribution on a $d$-dimensional hypercube $[-1,1]^d$, our algorithm
constructs $\hat{A}, \hat{s}$ such that the total variation distance of the
distribution $\hat{D}$ from $D$ is $O(\epsilon)$ using poly$(d)$ time and
samples. Total variation distance is the information-theoretically strongest
possible notion of distance in our setting and our recovery guarantees in this
distance are optimal up to the absolute constant factor multiplying $\epsilon$.
In particular, if the columns of $A$ are normalized to be unit length, our
total variation distance guarantee implies a bound on the sum of the $\ell_2$
distances between the column vectors of $A$ and $A'$, $\sum_{i =1}^d
\|a_i-\hat{a}_i\|_2 = O(\epsilon)$. In contrast, the strongest known prior
results only yield a $\epsilon^{O(1)}$ (relative) bound on the distance between
individual $a_i$'s and their estimates and translate into an $O(d\epsilon)$
bound on the total variation distance. Our key innovation is a new approach to
ICA (even to outlier-free ICA) that circumvents the difficulties in the
classical method of moments and instead relies on a new geometric certificate
of correctness of an affine transformation. Our algorithm is based on a new
method that iteratively improves an estimate of the unknown affine
transformation whenever the requirements of the certificate are not met.",2302.12289v1,https://arxiv.org/pdf/2302.12289v1
"Testing Stationarity Concepts for ReLU Networks: Hardness, Regularity,
  and Robust Algorithms","Lai Tian, Anthony Man-Cho So","We study the computational problem of the stationarity test for the empirical
loss of neural networks with ReLU activation functions. Our contributions are:
  Hardness: We show that checking a certain first-order approximate
stationarity concept for a piecewise linear function is co-NP-hard. This
implies that testing a certain stationarity concept for a modern nonsmooth
neural network is in general computationally intractable. As a corollary, we
prove that testing so-called first-order minimality for functions in abs-normal
form is co-NP-complete, which was conjectured by Griewank and Walther (2019,
SIAM J. Optim., vol. 29, p284).
  Regularity: We establish a necessary and sufficient condition for the
validity of an equality-type subdifferential chain rule in terms of Clarke,
Fr\'echet, and limiting subdifferentials of the empirical loss of two-layer
ReLU networks. This new condition is simple and efficiently checkable.
  Robust algorithms: We introduce an algorithmic scheme to test
near-approximate stationarity in terms of both Clarke and Fr\'echet
subdifferentials. Our scheme makes no false positive or false negative error
when the tested point is sufficiently close to a stationary one and a certain
qualification is satisfied. This is the first practical and robust stationarity
test approach for two-layer ReLU networks.",2302.12261v1,https://arxiv.org/pdf/2302.12261v1
"Comparative Study of Coupling and Autoregressive Flows through Robust
  Statistical Tests","Andrea Coccaro, Marco Letizia, Humberto Reyes-Gonzalez, Riccardo Torre","Normalizing Flows have emerged as a powerful brand of generative models, as
they not only allow for efficient sampling of complicated target distributions,
but also deliver density estimation by construction. We propose here an
in-depth comparison of coupling and autoregressive flows, both of the affine
and rational quadratic spline type, considering four different architectures:
Real-valued Non-Volume Preserving (RealNVP), Masked Autoregressive Flow (MAF),
Coupling Rational Quadratic Spline (C-RQS), and Autoregressive Rational
Quadratic Spline (A-RQS). We focus on a set of multimodal target distributions
of increasing dimensionality ranging from 4 to 400. The performances are
compared by means of different test-statistics for two-sample tests, built from
known distance measures: the sliced Wasserstein distance, the
dimension-averaged one-dimensional Kolmogorov-Smirnov test, and the Frobenius
norm of the difference between correlation matrices. Furthermore, we include
estimations of the variance of both the metrics and the trained models. Our
results indicate that the A-RQS algorithm stands out both in terms of accuracy
and training speed. Nonetheless, all the algorithms are generally able, without
too much fine-tuning, to learn complicated distributions with limited training
data and in a reasonable time, of the order of hours on a Tesla A40 GPU. The
only exception is the C-RQS, which takes significantly longer to train, does
not always provide good accuracy, and becomes unstable for large
dimensionalities. All algorithms have been implemented using TensorFlow2 and
TensorFlow Probability and made available on
\href{https://github.com/NF4HEP/NormalizingFlowsHD}{GitHub}.",2302.12024v2,https://arxiv.org/pdf/2302.12024v2
"ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for
  Generalizable and Robust Synthetic Image Detection","Md Awsafur Rahman, Bishmoy Paul, Najibul Haque Sarker, Zaber Ibn Abdul Hakim, Shaikh Anowarul Fattah","Synthetic image generation has opened up new opportunities but has also
created threats in regard to privacy, authenticity, and security. Detecting
fake images is of paramount importance to prevent illegal activities, and
previous research has shown that generative models leave unique patterns in
their synthetic images that can be exploited to detect them. However, the
fundamental problem of generalization remains, as even state-of-the-art
detectors encounter difficulty when facing generators never seen during
training. To assess the generalizability and robustness of synthetic image
detectors in the face of real-world impairments, this paper presents a
large-scale dataset named ArtiFact, comprising diverse generators, object
categories, and real-world challenges. Moreover, the proposed multi-class
classification scheme, combined with a filter stride reduction strategy
addresses social platform impairments and effectively detects synthetic images
from both seen and unseen generators. The proposed solution significantly
outperforms other top teams by 8.34% on Test 1, 1.26% on Test 2, and 15.08% on
Test 3 in the IEEE VIP Cup challenge at ICIP 2022, as measured by the accuracy
metric.",2302.11970v2,https://arxiv.org/pdf/2302.11970v2
Out-of-Domain Robustness via Targeted Augmentations,"Irena Gao, Shiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto, Percy Liang","Models trained on one set of domains often suffer performance drops on unseen
domains, e.g., when wildlife monitoring models are deployed in new camera
locations. In this work, we study principles for designing data augmentations
for out-of-domain (OOD) generalization. In particular, we focus on real-world
scenarios in which some domain-dependent features are robust, i.e., some
features that vary across domains are predictive OOD. For example, in the
wildlife monitoring application above, image backgrounds vary across camera
locations but indicate habitat type, which helps predict the species of
photographed animals. Motivated by theoretical analysis on a linear setting, we
propose targeted augmentations, which selectively randomize spurious
domain-dependent features while preserving robust ones. We prove that targeted
augmentations improve OOD performance, allowing models to generalize better
with fewer domains. In contrast, existing approaches such as generic
augmentations, which fail to randomize domain-dependent features, and
domain-invariant augmentations, which randomize all domain-dependent features,
both perform poorly OOD. In experiments on three real-world datasets, we show
that targeted augmentations set new states-of-the-art for OOD performance by
3.2-15.2 percentage points.",2302.11861v3,https://arxiv.org/pdf/2302.11861v3
Provable Robustness Against a Union of $\ell_0$ Adversarial Attacks,"Zayd Hammoudeh, Daniel Lowd","Sparse or $\ell_0$ adversarial attacks arbitrarily perturb an unknown subset
of the features. $\ell_0$ robustness analysis is particularly well-suited for
heterogeneous (tabular) data where features have different types or scales.
State-of-the-art $\ell_0$ certified defenses are based on randomized smoothing
and apply to evasion attacks only. This paper proposes feature partition
aggregation (FPA) -- a certified defense against the union of $\ell_0$ evasion,
backdoor, and poisoning attacks. FPA generates its stronger robustness
guarantees via an ensemble whose submodels are trained on disjoint feature
sets. Compared to state-of-the-art $\ell_0$ defenses, FPA is up to
3,000${\times}$ faster and provides larger median robustness guarantees (e.g.,
median certificates of 13 pixels over 10 for CIFAR10, 12 pixels over 10 for
MNIST, 4 features over 1 for Weather, and 3 features over 1 for Ames), meaning
FPA provides the additional dimensions of robustness essentially for free.",2302.11628v4,https://arxiv.org/pdf/2302.11628v4
"ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep
  Learning Paradigms","Minzhou Pan, Yi Zeng, Lingjuan Lyu, Xue Lin, Ruoxi Jia","Backdoor data detection is traditionally studied in an end-to-end supervised
learning (SL) setting. However, recent years have seen the proliferating
adoption of self-supervised learning (SSL) and transfer learning (TL), due to
their lesser need for labeled data. Successful backdoor attacks have also been
demonstrated in these new settings. However, we lack a thorough understanding
of the applicability of existing detection methods across a variety of learning
settings. By evaluating 56 attack settings, we show that the performance of
most existing detection methods varies significantly across different attacks
and poison ratios, and all fail on the state-of-the-art clean-label attack. In
addition, they either become inapplicable or suffer large performance losses
when applied to SSL and TL. We propose a new detection method called Active
Separation via Offset (ASSET), which actively induces different model behaviors
between the backdoor and clean samples to promote their separation. We also
provide procedures to adaptively select the number of suspicious points to
remove. In the end-to-end SL setting, ASSET is superior to existing methods in
terms of consistency of defensive performance across different attacks and
robustness to changes in poison ratios; in particular, it is the only method
that can detect the state-of-the-art clean-label attack. Moreover, ASSET's
average detection rates are higher than the best existing methods in SSL and
TL, respectively, by 69.3% and 33.2%, thus providing the first practical
backdoor defense for these new DL settings. We open-source the project to drive
further development and encourage engagement:
https://github.com/ruoxi-jia-group/ASSET.",2302.11408v2,https://arxiv.org/pdf/2302.11408v2
"Gradient Remedy for Multi-Task Learning in End-to-End Noise-Robust
  Speech Recognition","Yuchen Hu, Chen Chen, Ruizhe Li, Qiushi Zhu, Eng Siong Chng","Speech enhancement (SE) is proved effective in reducing noise from noisy
speech signals for downstream automatic speech recognition (ASR), where
multi-task learning strategy is employed to jointly optimize these two tasks.
However, the enhanced speech learned by SE objective may not always yield good
ASR results. From the optimization view, there sometimes exists interference
between the gradients of SE and ASR tasks, which could hinder the multi-task
learning and finally lead to sub-optimal ASR performance. In this paper, we
propose a simple yet effective approach called gradient remedy (GR) to solve
interference between task gradients in noise-robust speech recognition, from
perspectives of both angle and magnitude. Specifically, we first project the SE
task's gradient onto a dynamic surface that is at acute angle to ASR gradient,
in order to remove the conflict between them and assist in ASR optimization.
Furthermore, we adaptively rescale the magnitude of two gradients to prevent
the dominant ASR task from being misled by SE gradient. Experimental results
show that the proposed approach well resolves the gradient interference and
achieves relative word error rate (WER) reductions of 9.3% and 11.1% over
multi-task learning baseline, on RATS and CHiME-4 datasets, respectively. Our
code is available at GitHub.",2302.11362v2,https://arxiv.org/pdf/2302.11362v2
"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution
  Perspective","Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue Zhang, Xing Xie","ChatGPT is a recent chatbot service released by OpenAI and is receiving
increasing attention over the past few months. While evaluations of various
aspects of ChatGPT have been done, its robustness, i.e., the performance to
unexpected inputs, is still unclear to the public. Robustness is of particular
concern in responsible AI, especially for safety-critical applications. In this
paper, we conduct a thorough evaluation of the robustness of ChatGPT from the
adversarial and out-of-distribution (OOD) perspective. To do so, we employ the
AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart
review and DDXPlus medical diagnosis datasets for OOD evaluation. We select
several popular foundation models as baselines. Results show that ChatGPT shows
consistent advantages on most adversarial and OOD classification and
translation tasks. However, the absolute performance is far from perfection,
which suggests that adversarial and OOD robustness remains a significant threat
to foundation models. Moreover, ChatGPT shows astounding performance in
understanding dialogue-related texts and we find that it tends to provide
informal suggestions for medical tasks instead of definitive answers. Finally,
we present in-depth discussions of possible research directions.",2302.12095v5,https://arxiv.org/pdf/2302.12095v5
Distributionally Robust Recourse Action,"Duy Nguyen, Ngoc Bui, Viet Anh Nguyen","A recourse action aims to explain a particular algorithmic decision by
showing one specific way in which the instance could be modified to receive an
alternate outcome. Existing recourse generation methods often assume that the
machine learning model does not change over time. However, this assumption does
not always hold in practice because of data distribution shifts, and in this
case, the recourse action may become invalid. To redress this shortcoming, we
propose the Distributionally Robust Recourse Action (DiRRAc) framework, which
generates a recourse action that has a high probability of being valid under a
mixture of model shifts. We formulate the robustified recourse setup as a
min-max optimization problem, where the max problem is specified by Gelbrich
distance over an ambiguity set around the distribution of model parameters.
Then we suggest a projected gradient descent algorithm to find a robust
recourse according to the min-max objective. We show that our DiRRAc framework
can be extended to hedge against the misspecification of the mixture weights.
Numerical experiments with both synthetic and three real-world datasets
demonstrate the benefits of our proposed framework over state-of-the-art
recourse methods.",2302.11211v1,https://arxiv.org/pdf/2302.11211v1
"What Are Effective Labels for Augmented Data? Improving Calibration and
  Robustness with AutoLabel","Yao Qin, Xuezhi Wang, Balaji Lakshminarayanan, Ed H. Chi, Alex Beutel","A wide breadth of research has devised data augmentation approaches that can
improve both accuracy and generalization performance for neural networks.
However, augmented data can end up being far from the clean training data and
what is the appropriate label is less clear. Despite this, most existing work
simply uses one-hot labels for augmented data. In this paper, we show re-using
one-hot labels for highly distorted data might run the risk of adding noise and
degrading accuracy and calibration. To mitigate this, we propose a generic
method AutoLabel to automatically learn the confidence in the labels for
augmented data, based on the transformation distance between the clean
distribution and augmented distribution. AutoLabel is built on label smoothing
and is guided by the calibration-performance over a hold-out validation set. We
successfully apply AutoLabel to three different data augmentation techniques:
the state-of-the-art RandAug, AugMix, and adversarial training. Experiments on
CIFAR-10, CIFAR-100 and ImageNet show that AutoLabel significantly improves
existing data augmentation techniques over models' calibration and accuracy,
especially under distributional shift.",2302.11188v1,https://arxiv.org/pdf/2302.11188v1
"Unifying Speech Enhancement and Separation with Gradient Modulation for
  End-to-End Noise-Robust Speech Separation","Yuchen Hu, Chen Chen, Heqing Zou, Xionghu Zhong, Eng Siong Chng","Recent studies in neural network-based monaural speech separation (SS) have
achieved a remarkable success thanks to increasing ability of long sequence
modeling. However, they would degrade significantly when put under realistic
noisy conditions, as the background noise could be mistaken for speaker's
speech and thus interfere with the separated sources. To alleviate this
problem, we propose a novel network to unify speech enhancement and separation
with gradient modulation to improve noise-robustness. Specifically, we first
build a unified network by combining speech enhancement (SE) and separation
modules, with multi-task learning for optimization, where SE is supervised by
parallel clean mixture to reduce noise for downstream speech separation.
Furthermore, in order to avoid suppressing valid speaker information when
reducing noise, we propose a gradient modulation (GM) strategy to harmonize the
SE and SS tasks from optimization view. Experimental results show that our
approach achieves the state-of-the-art on large-scale Libri2Mix- and
Libri3Mix-noisy datasets, with SI-SNRi results of 16.0 dB and 15.8 dB
respectively. Our code is available at GitHub.",2302.11131v1,https://arxiv.org/pdf/2302.11131v1
"Low Rank Matrix Completion via Robust Alternating Minimization in Nearly
  Linear Time","Yuzhou Gu, Zhao Song, Junze Yin, Lichen Zhang","Given a matrix $M\in \mathbb{R}^{m\times n}$, the low rank matrix completion
problem asks us to find a rank-$k$ approximation of $M$ as $UV^\top$ for $U\in
\mathbb{R}^{m\times k}$ and $V\in \mathbb{R}^{n\times k}$ by only observing a
few entries specified by a set of entries $\Omega\subseteq [m]\times [n]$. In
particular, we examine an approach that is widely used in practice -- the
alternating minimization framework. Jain, Netrapalli, and Sanghavi [JNS13]
showed that if $M$ has incoherent rows and columns, then alternating
minimization provably recovers the matrix $M$ by observing a nearly linear in
$n$ number of entries. While the sample complexity has been subsequently
improved [GLZ17], alternating minimization steps are required to be computed
exactly. This hinders the development of more efficient algorithms and fails to
depict the practical implementation of alternating minimization, where the
updates are usually performed approximately in favor of efficiency.
  In this paper, we take a major step towards a more efficient and error-robust
alternating minimization framework. To this end, we develop an analytical
framework for alternating minimization that can tolerate a moderate amount of
errors caused by approximate updates. Moreover, our algorithm runs in time
$\widetilde O(|\Omega| k)$, which is nearly linear in the time to verify the
solution while preserving the sample complexity. This improves upon all prior
known alternating minimization approaches which require $\widetilde O(|\Omega|
k^2)$ time.",2302.11068v3,https://arxiv.org/pdf/2302.11068v3
MultiRobustBench: Benchmarking Robustness Against Multiple Attacks,"Sihui Dai, Saeed Mahloujifar, Chong Xiang, Vikash Sehwag, Pin-Yu Chen, Prateek Mittal","The bulk of existing research in defending against adversarial examples
focuses on defending against a single (typically bounded Lp-norm) attack, but
for a practical setting, machine learning (ML) models should be robust to a
wide variety of attacks. In this paper, we present the first unified framework
for considering multiple attacks against ML models. Our framework is able to
model different levels of learner's knowledge about the test-time adversary,
allowing us to model robustness against unforeseen attacks and robustness
against unions of attacks. Using our framework, we present the first
leaderboard, MultiRobustBench, for benchmarking multiattack evaluation which
captures performance across attack types and attack strengths. We evaluate the
performance of 16 defended models for robustness against a set of 9 different
attack types, including Lp-based threat models, spatial transformations, and
color changes, at 20 different attack strengths (180 attacks total).
Additionally, we analyze the state of current defenses against multiple
attacks. Our analysis shows that while existing defenses have made progress in
terms of average robustness across the set of attacks used, robustness against
the worst-case attack is still a big open problem as all existing models
perform worse than random guessing.",2302.10980v3,https://arxiv.org/pdf/2302.10980v3
Robust Mean Estimation Without Moments for Symmetric Distributions,"Gleb Novikov, David Steurer, Stefan Tiegel","We study the problem of robustly estimating the mean or location parameter
without moment assumptions. We show that for a large class of symmetric
distributions, the same error as in the Gaussian setting can be achieved
efficiently. The distributions we study include products of arbitrary symmetric
one-dimensional distributions, such as product Cauchy distributions, as well as
elliptical distributions.
  For product distributions and elliptical distributions with known scatter
(covariance) matrix, we show that given an $\varepsilon$-corrupted sample, we
can with probability at least $1-\delta$ estimate its location up to error
$O(\varepsilon \sqrt{\log(1/\varepsilon)})$ using $\tfrac{d\log(d) +
\log(1/\delta)}{\varepsilon^2 \log(1/\varepsilon)}$ samples. This result
matches the best-known guarantees for the Gaussian distribution and known SQ
lower bounds (up to the $\log(d)$ factor). For elliptical distributions with
unknown scatter (covariance) matrix, we propose a sequence of efficient
algorithms that approaches this optimal error. Specifically, for every $k \in
\mathbb{N}$, we design an estimator using time and samples $\tilde{O}({d^k})$
achieving error $O(\varepsilon^{1-\frac{1}{2k}})$. This matches the error and
running time guarantees when assuming certifiably bounded moments of order up
to $k$. For unknown covariance, such error bounds of $o(\sqrt{\varepsilon})$
are not even known for (general) sub-Gaussian distributions.
  Our algorithms are based on a generalization of the well-known filtering
technique. We show how this machinery can be combined with Huber-loss-based
techniques to work with projections of the noise that behave more nicely than
the initial noise. Moreover, we show how SoS proofs can be used to obtain
algorithmic guarantees even for distributions without a first moment. We
believe that this approach may find other applications in future works.",2302.10844v2,https://arxiv.org/pdf/2302.10844v2
"A Survey of Trustworthy Federated Learning with Perspectives on
  Security, Robustness, and Privacy","Yifei Zhang, Dun Zeng, Jinglong Luo, Zenglin Xu, Irwin King","Trustworthy artificial intelligence (AI) technology has revolutionized daily
life and greatly benefited human society. Among various AI technologies,
Federated Learning (FL) stands out as a promising solution for diverse
real-world scenarios, ranging from risk evaluation systems in finance to
cutting-edge technologies like drug discovery in life sciences. However,
challenges around data isolation and privacy threaten the trustworthiness of FL
systems. Adversarial attacks against data privacy, learning algorithm
stability, and system confidentiality are particularly concerning in the
context of distributed training in federated learning. Therefore, it is crucial
to develop FL in a trustworthy manner, with a focus on security, robustness,
and privacy. In this survey, we propose a comprehensive roadmap for developing
trustworthy FL systems and summarize existing efforts from three key aspects:
security, robustness, and privacy. We outline the threats that pose
vulnerabilities to trustworthy federated learning across different stages of
development, including data processing, model training, and deployment. To
guide the selection of the most appropriate defense methods, we discuss
specific technical solutions for realizing each aspect of Trustworthy FL (TFL).
Our approach differs from previous work that primarily discusses TFL from a
legal perspective or presents FL from a high-level, non-technical viewpoint.",2302.10637v1,https://arxiv.org/pdf/2302.10637v1
"Multiobjective Evolutionary Pruning of Deep Neural Networks with
  Transfer Learning for improving their Performance and Robustness","Javier Poyatos, Daniel Molina, Aitor Martínez, Javier Del Ser, Francisco Herrera","Evolutionary Computation algorithms have been used to solve optimization
problems in relation with architectural, hyper-parameter or training
configuration, forging the field known today as Neural Architecture Search.
These algorithms have been combined with other techniques such as the pruning
of Neural Networks, which reduces the complexity of the network, and the
Transfer Learning, which lets the import of knowledge from another problem
related to the one at hand. The usage of several criteria to evaluate the
quality of the evolutionary proposals is also a common case, in which the
performance and complexity of the network are the most used criteria. This work
proposes MO-EvoPruneDeepTL, a multi-objective evolutionary pruning algorithm.
MO-EvoPruneDeepTL uses Transfer Learning to adapt the last layers of Deep
Neural Networks, by replacing them with sparse layers evolved by a genetic
algorithm, which guides the evolution based in the performance, complexity and
robustness of the network, being the robustness a great quality indicator for
the evolved models. We carry out different experiments with several datasets to
assess the benefits of our proposal. Results show that our proposal achieves
promising results in all the objectives, and direct relation are presented
among them. The experiments also show that the most influential neurons help us
explain which parts of the input images are the most relevant for the
prediction of the pruned neural network. Lastly, by virtue of the diversity
within the Pareto front of pruning patterns produced by the proposal, it is
shown that an ensemble of differently pruned models improves the overall
performance and robustness of the trained networks.",2302.10253v2,https://arxiv.org/pdf/2302.10253v2
"Seasoning Model Soups for Robustness to Adversarial and Natural
  Distribution Shifts","Francesco Croce, Sylvestre-Alvise Rebuffi, Evan Shelhamer, Sven Gowal","Adversarial training is widely used to make classifiers robust to a specific
threat or adversary, such as $\ell_p$-norm bounded perturbations of a given
$p$-norm. However, existing methods for training classifiers robust to multiple
threats require knowledge of all attacks during training and remain vulnerable
to unseen distribution shifts. In this work, we describe how to obtain
adversarially-robust model soups (i.e., linear combinations of parameters) that
smoothly trade-off robustness to different $\ell_p$-norm bounded adversaries.
We demonstrate that such soups allow us to control the type and level of
robustness, and can achieve robustness to all threats without jointly training
on all of them. In some cases, the resulting model soups are more robust to a
given $\ell_p$-norm adversary than the constituent model specialized against
that same adversary. Finally, we show that adversarially-robust model soups can
be a viable tool to adapt to distribution shifts from a few examples.",2302.10164v1,https://arxiv.org/pdf/2302.10164v1
"A DNN based Normalized Time-frequency Weighted Criterion for Robust
  Wideband DoA Estimation","Kuan-Lin Chen, Ching-Hua Lee, Bhaskar D. Rao, Harinath Garudadri","Deep neural networks (DNNs) have greatly benefited direction of arrival (DoA)
estimation methods for speech source localization in noisy environments.
However, their localization accuracy is still far from satisfactory due to the
vulnerability to nonspeech interference. To improve the robustness against
interference, we propose a DNN based normalized time-frequency (T-F) weighted
criterion which minimizes the distance between the candidate steering vectors
and the filtered snapshots in the T-F domain. Our method requires no
eigendecomposition and uses a simple normalization to prevent the optimization
objective from being misled by noisy filtered snapshots. We also study
different designs of T-F weights guided by a DNN. We find that duplicating the
Hadamard product of speech ratio masks is highly effective and better than
other techniques such as direct masking and taking the mean in the proposed
approach. However, the best-performing design of T-F weights is
criterion-dependent in general. Experiments show that the proposed method
outperforms popular DNN based DoA estimation methods including widely used
subspace methods in noisy and reverberant environments.",2302.10147v1,https://arxiv.org/pdf/2302.10147v1
Stationary Point Losses for Robust Model,"Weiwei Gao, Dazhi Zhang, Yao Li, Zhichang Guo, Ovanes Petrosian","The inability to guarantee robustness is one of the major obstacles to the
application of deep learning models in security-demanding domains. We identify
that the most commonly used cross-entropy (CE) loss does not guarantee robust
boundary for neural networks. CE loss sharpens the neural network at the
decision boundary to achieve a lower loss, rather than pushing the boundary to
a more robust position. A robust boundary should be kept in the middle of
samples from different classes, thus maximizing the margins from the boundary
to the samples. We think this is due to the fact that CE loss has no stationary
point. In this paper, we propose a family of new losses, called stationary
point (SP) loss, which has at least one stationary point on the correct
classification side. We proved that robust boundary can be guaranteed by SP
loss without losing much accuracy. With SP loss, larger perturbations are
required to generate adversarial examples. We demonstrate that robustness is
improved under a variety of adversarial attacks by applying SP loss. Moreover,
robust boundary learned by SP loss also performs well on imbalanced datasets.",2302.09575v1,https://arxiv.org/pdf/2302.09575v1
Delving into the Adversarial Robustness of Federated Learning,"Jie Zhang, Bo Li, Chen Chen, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chao Wu","In Federated Learning (FL), models are as fragile as centrally trained models
against adversarial examples. However, the adversarial robustness of federated
learning remains largely unexplored. This paper casts light on the challenge of
adversarial robustness of federated learning. To facilitate a better
understanding of the adversarial vulnerability of the existing FL methods, we
conduct comprehensive robustness evaluations on various attacks and adversarial
training methods. Moreover, we reveal the negative impacts induced by directly
adopting adversarial training in FL, which seriously hurts the test accuracy,
especially in non-IID settings. In this work, we propose a novel algorithm
called Decision Boundary based Federated Adversarial Training (DBFAT), which
consists of two components (local re-weighting and global regularization) to
improve both accuracy and robustness of FL systems. Extensive experiments on
multiple datasets demonstrate that DBFAT consistently outperforms other
baselines under both IID and non-IID settings.",2302.09479v1,https://arxiv.org/pdf/2302.09479v1
"Robust and Versatile Bipedal Jumping Control through Reinforcement
  Learning","Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, Koushil Sreenath","This work aims to push the limits of agility for bipedal robots by enabling a
torque-controlled bipedal robot to perform robust and versatile dynamic jumps
in the real world. We present a reinforcement learning framework for training a
robot to accomplish a large variety of jumping tasks, such as jumping to
different locations and directions. To improve performance on these challenging
tasks, we develop a new policy structure that encodes the robot's long-term
input/output (I/O) history while also providing direct access to a short-term
I/O history. In order to train a versatile jumping policy, we utilize a
multi-stage training scheme that includes different training stages for
different objectives. After multi-stage training, the policy can be directly
transferred to a real bipedal Cassie robot. Training on different tasks and
exploring more diverse scenarios lead to highly robust policies that can
exploit the diverse set of learned maneuvers to recover from perturbations or
poor landings during real-world deployment. Such robustness in the proposed
policy enables Cassie to succeed in completing a variety of challenging jump
tasks in the real world, such as standing long jumps, jumping onto elevated
platforms, and multi-axes jumps.",2302.09450v2,https://arxiv.org/pdf/2302.09450v2
"Stochastic Approximation Approaches to Group Distributionally Robust
  Optimization","Lijun Zhang, Peng Zhao, Zhen-Hua Zhuang, Tianbao Yang, Zhi-Hua Zhou","This paper investigates group distributionally robust optimization (GDRO),
with the purpose to learn a model that performs well over $m$ different
distributions. First, we formulate GDRO as a stochastic convex-concave
saddle-point problem, and demonstrate that stochastic mirror descent (SMD),
using $m$ samples in each iteration, achieves an $O(m (\log m)/\epsilon^2)$
sample complexity for finding an $\epsilon$-optimal solution, which matches the
$\Omega(m/\epsilon^2)$ lower bound up to a logarithmic factor. Then, we make
use of techniques from online learning to reduce the number of samples required
in each round from $m$ to $1$, keeping the same sample complexity.
Specifically, we cast GDRO as a two-players game where one player simply
performs SMD and the other executes an online algorithm for non-oblivious
multi-armed bandits. Next, we consider a more practical scenario where the
number of samples that can be drawn from each distribution is different, and
propose a novel formulation of weighted GDRO, which allows us to derive
distribution-dependent convergence rates. Denote by $n_i$ the sample budget for
the $i$-th distribution, and assume $n_1 \geq n_2 \geq \cdots \geq n_m$. In the
first approach, we incorporate non-uniform sampling into SMD such that the
sample budget is satisfied in expectation, and prove that the excess risk of
the $i$-th distribution decreases at an $O(\sqrt{n_1 \log m}/n_i)$ rate. In the
second approach, we use mini-batches to meet the budget exactly and also reduce
the variance in stochastic gradients, and then leverage stochastic mirror-prox
algorithm, which can exploit small variances, to optimize a carefully designed
weighted GDRO problem. Under appropriate conditions, it attains an $O((\log
m)/\sqrt{n_i})$ convergence rate, which almost matches the optimal
$O(\sqrt{1/n_i})$ rate of only learning from the $i$-th distribution with $n_i$
samples.",2302.09267v4,https://arxiv.org/pdf/2302.09267v4
Smoothly Giving up: Robustness for Simple Models,"Tyler Sypherd, Nathan Stromberg, Richard Nock, Visar Berisha, Lalitha Sankar","There is a growing need for models that are interpretable and have reduced
energy and computational cost (e.g., in health care analytics and federated
learning). Examples of algorithms to train such models include logistic
regression and boosting. However, one challenge facing these algorithms is that
they provably suffer from label noise; this has been attributed to the joint
interaction between oft-used convex loss functions and simpler hypothesis
classes, resulting in too much emphasis being placed on outliers. In this work,
we use the margin-based $\alpha$-loss, which continuously tunes between
canonical convex and quasi-convex losses, to robustly train simple models. We
show that the $\alpha$ hyperparameter smoothly introduces non-convexity and
offers the benefit of ""giving up"" on noisy training examples. We also provide
results on the Long-Servedio dataset for boosting and a COVID-19 survey dataset
for logistic regression, highlighting the efficacy of our approach across
multiple relevant domains.",2302.09114v1,https://arxiv.org/pdf/2302.09114v1
"Virtualization of Tiny Embedded Systems with a robust real-time capable
  and extensible Stack Virtual Machine REXAVM supporting Material-integrated
  Intelligent Systems and Tiny Machine Learning","Stefan Bosse, Sarah Bornemann, Björn Lüssem","In the past decades, there has been a significant increase in sensor density
and sensor deployment, driven by a significant miniaturization and decrease in
size down to the chip level, addressing ubiquitous computing, edge computing,
as well as distributed sensor networks. Material-integrated and intelligent
systems (MIIS) provide the next integration and application level, but they
create new challenges and introduce hard constraints (resources, energy supply,
communication, resilience, and security). Commonly, low-resource systems are
statically programmed processors with application-specific software or
application-specific hardware (FPGA). This work demonstrates the need for and
solution to virtualization in such low-resource and constrained systems towards
resilient distributed sensor and cyber-physical networks using a unified
low-resource, customizable, and real-time capable embedded and extensible stack
virtual machine (REXAVM) that can be implemented and cooperate in both software
and hardware. In a holistic architecture approach, the VM specifically
addresses digital signal processing and tiny machine learning. The REXAVM is
highly customizable through the use of VM program code generators at compile
time and incremental code processing at run time. The VM uses an integrated,
highly efficient just-in-time compiler to create Bytecode from text code. This
paper shows and evaluates the suitability of the proposed VM architecture for
operationally equivalent software and hardware (FPGA) implementations. Specific
components supporting tiny ML and DSP using fixed-point arithmetic with respect
to efficiency and accuracy are discussed. An extended use-case section
demonstrates the usability of the introduced VM architecture for a broad range
of applications.",2302.09002v1,https://arxiv.org/pdf/2302.09002v1
Quantile LSTM: A Robust LSTM for Anomaly Detection In Time Series Data,"Snehanshu Saha, Jyotirmoy Sarkar, Soma Dhavala, Santonu Sarkar, Preyank Mota","Anomalies refer to the departure of systems and devices from their normal
behaviour in standard operating conditions. An anomaly in an industrial device
can indicate an upcoming failure, often in the temporal direction. In this
paper, we make two contributions: 1) we estimate conditional quantiles and
consider three different ways to define anomalies based on the estimated
quantiles. 2) we use a new learnable activation function in the popular Long
Short Term Memory networks (LSTM) architecture to model temporal long-range
dependency. In particular, we propose Parametric Elliot Function (PEF) as an
activation function (AF) inside LSTM, which saturates lately compared to
sigmoid and tanh. The proposed algorithms are compared with other well-known
anomaly detection algorithms, such as Isolation Forest (iForest), Elliptic
Envelope, Autoencoder, and modern Deep Learning models such as Deep
Autoencoding Gaussian Mixture Model (DAGMM), Generative Adversarial Networks
(GAN). The algorithms are evaluated in terms of various performance metrics,
such as Precision and Recall. The algorithms have been tested on multiple
industrial time-series datasets such as Yahoo, AWS, GE, and machine sensors. We
have found that the LSTM-based quantile algorithms are very effective and
outperformed the existing algorithms in identifying anomalies.",2302.08712v1,https://arxiv.org/pdf/2302.08712v1
"A Novel Noise Injection-based Training Scheme for Better Model
  Robustness","Zeliang Zhang, Jinyang Jiang, Minjie Chen, Zhiyuan Wang, Yijie Peng, Zhaofei Yu","Noise injection-based method has been shown to be able to improve the
robustness of artificial neural networks in previous work. In this work, we
propose a novel noise injection-based training scheme for better model
robustness. Specifically, we first develop a likelihood ratio method to
estimate the gradient with respect to both synaptic weights and noise levels
for stochastic gradient descent training. Then, we design an approximation for
the vanilla noise injection-based training method to reduce memory and improve
computational efficiency. Next, we apply our proposed scheme to spiking neural
networks and evaluate the performance of classification accuracy and robustness
on MNIST and Fashion-MNIST datasets. Experiment results show that our proposed
method achieves a much better performance on adversarial robustness and
slightly better performance on original accuracy, compared with the
conventional gradient-based training method.",2302.10802v2,https://arxiv.org/pdf/2302.10802v2
"AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust
  Autonomous Driving","Tianyue Zheng, Ang Li, Zhe Chen, Hongbo Wang, Jun Luo","Object detection with on-board sensors (e.g., lidar, radar, and camera) play
a crucial role in autonomous driving (AD), and these sensors complement each
other in modalities. While crowdsensing may potentially exploit these sensors
(of huge quantity) to derive more comprehensive knowledge, \textit{federated
learning} (FL) appears to be the necessary tool to reach this potential: it
enables autonomous vehicles (AVs) to train machine learning models without
explicitly sharing raw sensory data. However, the multimodal sensors introduce
various data heterogeneity across distributed AVs (e.g., label quantity skews
and varied modalities), posing critical challenges to effective FL. To this
end, we present AutoFed as a heterogeneity-aware FL framework to fully exploit
multimodal sensory data on AVs and thus enable robust AD. Specifically, we
first propose a novel model leveraging pseudo-labeling to avoid mistakenly
treating unlabeled objects as the background. We also propose an
autoencoder-based data imputation method to fill missing data modality (of
certain AVs) with the available ones. To further reconcile the heterogeneity,
we finally present a client selection mechanism exploiting the similarities
among client models to improve both training stability and convergence rate.
Our experiments on benchmark dataset confirm that AutoFed substantially
improves over status quo approaches in both precision and recall, while
demonstrating strong robustness to adverse weather conditions.",2302.08646v3,https://arxiv.org/pdf/2302.08646v3
Robust expected improvement for Bayesian optimization,"Ryan B. Christianson, Robert B. Gramacy","Bayesian Optimization (BO) links Gaussian Process (GP) surrogates with
sequential design toward optimizing expensive-to-evaluate black-box functions.
Example design heuristics, or so-called acquisition functions, like expected
improvement (EI), balance exploration and exploitation to furnish global
solutions under stringent evaluation budgets. However, they fall short when
solving for robust optima, meaning a preference for solutions in a wider domain
of attraction. Robust solutions are useful when inputs are imprecisely
specified, or where a series of solutions is desired. A common mathematical
programming technique in such settings involves an adversarial objective,
biasing a local solver away from ``sharp'' troughs. Here we propose a surrogate
modeling and active learning technique called robust expected improvement (REI)
that ports adversarial methodology into the BO/GP framework. After describing
the methods, we illustrate and draw comparisons to several competitors on
benchmark synthetic exercises and real problems of varying complexity.",2302.08612v2,https://arxiv.org/pdf/2302.08612v2
On the Role of Memory in Robust Opinion Dynamics,"Luca Becchetti, Andrea Clementi, Amos Korman, Francesco Pasquale, Luca Trevisan, Robin Vacus","We investigate opinion dynamics in a fully-connected system, consisting of
$n$ identical and anonymous agents, where one of the opinions (which is called
correct) represents a piece of information to disseminate. In more detail, one
source agent initially holds the correct opinion and remains with this opinion
throughout the execution. The goal for non-source agents is to quickly agree on
this correct opinion, and do that robustly, i.e., from any initial
configuration. The system evolves in rounds. In each round, one agent chosen
uniformly at random is activated: unless it is the source, the agent pulls the
opinions of $\ell$ random agents and then updates its opinion according to some
rule. We consider a restricted setting, in which agents have no memory and they
only revise their opinions on the basis of those of the agents they currently
sample. As restricted as it is, this setting encompasses very popular opinion
dynamics, such as the voter model and best-of-$k$ majority rules.
  Qualitatively speaking, we show that lack of memory prevents efficient
convergence. Specifically, we prove that no dynamics can achieve correct
convergence in an expected number of steps that is sub-quadratic in $n$, even
under a strong version of the model in which activated agents have complete
access to the current configuration of the entire system, i.e., the case
$\ell=n$. Conversely, we prove that the simple voter model (in which $\ell=1$)
correctly solves the problem, while almost matching the aforementioned lower
bound.
  These results suggest that, in contrast to symmetric consensus problems (that
do not involve a notion of correct opinion), fast convergence on the correct
opinion using stochastic opinion dynamics may indeed require the use of memory.
This insight may reflect on natural information dissemination processes that
rely on a few knowledgeable individuals.",2302.08600v1,https://arxiv.org/pdf/2302.08600v1
"A Bayesian Perspective for Determinant Minimization Based Robust
  Structured Matrix Factorizatio","Gokcan Tatli, Alper T. Erdogan","We introduce a Bayesian perspective for the structured matrix factorization
problem. The proposed framework provides a probabilistic interpretation for
existing geometric methods based on determinant minimization. We model input
data vectors as linear transformations of latent vectors drawn from a
distribution uniform over a particular domain reflecting structural
assumptions, such as the probability simplex in Nonnegative Matrix
Factorization and polytopes in Polytopic Matrix Factorization. We represent the
rows of the linear transformation matrix as vectors generated independently
from a normal distribution whose covariance matrix is inverse Wishart
distributed. We show that the corresponding maximum a posteriori estimation
problem boils down to the robust determinant minimization approach for
structured matrix factorization, providing insights about parameter selections
and potential algorithmic extensions.",2302.08416v1,https://arxiv.org/pdf/2302.08416v1
Graph Adversarial Immunization for Certifiable Robustness,"Shuchang Tao, Huawei Shen, Qi Cao, Yunfan Wu, Liang Hou, Xueqi Cheng","Despite achieving great success, graph neural networks (GNNs) are vulnerable
to adversarial attacks. Existing defenses focus on developing adversarial
training or model modification. In this paper, we propose and formulate graph
adversarial immunization, i.e., vaccinating part of graph structure to improve
certifiable robustness of graph against any admissible adversarial attack. We
first propose edge-level immunization to vaccinate node pairs. Unfortunately,
such edge-level immunization cannot defend against emerging node injection
attacks, since it only immunizes existing node pairs. To this end, we further
propose node-level immunization. To avoid computationally intensive
combinatorial optimization associated with adversarial immunization, we develop
AdvImmune-Edge and AdvImmune-Node algorithms to effectively obtain the immune
node pairs or nodes. Extensive experiments demonstrate the superiority of
AdvImmune methods. In particular, AdvImmune-Node remarkably improves the ratio
of robust nodes by 79%, 294%, and 100%, after immunizing only 5% of nodes.
Furthermore, AdvImmune methods show excellent defensive performance against
various attacks, outperforming state-of-the-art defenses. To the best of our
knowledge, this is the first attempt to improve certifiable robustness from
graph data perspective without losing performance on clean graphs, providing
new insights into graph adversarial learning.",2302.08051v2,https://arxiv.org/pdf/2302.08051v2
Robust Mid-Pass Filtering Graph Convolutional Networks,"Jincheng Huang, Lun Du, Xu Chen, Qiang Fu, Shi Han, Dongmei Zhang","Graph convolutional networks (GCNs) are currently the most promising paradigm
for dealing with graph-structure data, while recent studies have also shown
that GCNs are vulnerable to adversarial attacks. Thus developing GCN models
that are robust to such attacks become a hot research topic. However, the
structural purification learning-based or robustness constraints-based defense
GCN methods are usually designed for specific data or attacks, and introduce
additional objective that is not for classification. Extra training overhead is
also required in their design. To address these challenges, we conduct in-depth
explorations on mid-frequency signals on graphs and propose a simple yet
effective Mid-pass filter GCN (Mid-GCN). Theoretical analyses guarantee the
robustness of signals through the mid-pass filter, and we also shed light on
the properties of different frequency signals under adversarial attacks.
Extensive experiments on six benchmark graph data further verify the
effectiveness of our designed Mid-GCN in node classification accuracy compared
to state-of-the-art GCNs under various adversarial attack strategies.",2302.08048v1,https://arxiv.org/pdf/2302.08048v1
"XploreNAS: Explore Adversarially Robust & Hardware-efficient Neural
  Architectures for Non-ideal Xbars","Abhiroop Bhattacharjee, Abhishek Moitra, Priyadarshini Panda","Compute In-Memory platforms such as memristive crossbars are gaining focus as
they facilitate acceleration of Deep Neural Networks (DNNs) with high area and
compute-efficiencies. However, the intrinsic non-idealities associated with the
analog nature of computing in crossbars limits the performance of the deployed
DNNs. Furthermore, DNNs are shown to be vulnerable to adversarial attacks
leading to severe security threats in their large-scale deployment. Thus,
finding adversarially robust DNN architectures for non-ideal crossbars is
critical to the safe and secure deployment of DNNs on the edge. This work
proposes a two-phase algorithm-hardware co-optimization approach called
XploreNAS that searches for hardware-efficient & adversarially robust neural
architectures for non-ideal crossbar platforms. We use the one-shot Neural
Architecture Search (NAS) approach to train a large Supernet with
crossbar-awareness and sample adversarially robust Subnets therefrom,
maintaining competitive hardware-efficiency. Our experiments on crossbars with
benchmark datasets (SVHN, CIFAR10 & CIFAR100) show upto ~8-16% improvement in
the adversarial robustness of the searched Subnets against a baseline ResNet-18
model subjected to crossbar-aware adversarial training. We benchmark our robust
Subnets for Energy-Delay-Area-Products (EDAPs) using the Neurosim tool and find
that with additional hardware-efficiency driven optimizations, the Subnets
attain ~1.5-1.6x lower EDAPs than ResNet-18 baseline.",2302.07769v2,https://arxiv.org/pdf/2302.07769v2
Cauchy Loss Function: Robustness Under Gaussian and Cauchy Noise,"Thamsanqa Mlotshwa, Heinrich van Deventer, Anna Sergeevna Bosman","In supervised machine learning, the choice of loss function implicitly
assumes a particular noise distribution over the data. For example, the
frequently used mean squared error (MSE) loss assumes a Gaussian noise
distribution. The choice of loss function during training and testing affects
the performance of artificial neural networks (ANNs). It is known that MSE may
yield substandard performance in the presence of outliers. The Cauchy loss
function (CLF) assumes a Cauchy noise distribution, and is therefore
potentially better suited for data with outliers. This papers aims to determine
the extent of robustness and generalisability of the CLF as compared to MSE.
CLF and MSE are assessed on a few handcrafted regression problems, and a
real-world regression problem with artificially simulated outliers, in the
context of ANN training. CLF yielded results that were either comparable to or
better than the results yielded by MSE, with a few notable exceptions.",2302.07238v1,https://arxiv.org/pdf/2302.07238v1
On the Role of Randomization in Adversarially Robust Classification,"Lucas Gnecco-Heredia, Yann Chevaleyre, Benjamin Negrevergne, Laurent Meunier, Muni Sreenivas Pydi","Deep neural networks are known to be vulnerable to small adversarial
perturbations in test data. To defend against adversarial attacks,
probabilistic classifiers have been proposed as an alternative to deterministic
ones. However, literature has conflicting findings on the effectiveness of
probabilistic classifiers in comparison to deterministic ones. In this paper,
we clarify the role of randomization in building adversarially robust
classifiers. Given a base hypothesis set of deterministic classifiers, we show
the conditions under which a randomized ensemble outperforms the hypothesis set
in adversarial risk, extending previous results. Additionally, we show that for
any probabilistic binary classifier (including randomized ensembles), there
exists a deterministic classifier that outperforms it. Finally, we give an
explicit description of the deterministic hypothesis set that contains such a
deterministic classifier for many types of commonly used probabilistic
classifiers, i.e. randomized ensembles and parametric/input noise injection.",2302.07221v3,https://arxiv.org/pdf/2302.07221v3
"An Experimental Study of Byzantine-Robust Aggregation Schemes in
  Federated Learning","Shenghui Li, Edith C. -H. Ngai, Thiemo Voigt","Byzantine-robust federated learning aims at mitigating Byzantine failures
during the federated training process, where malicious participants may upload
arbitrary local updates to the central server to degrade the performance of the
global model. In recent years, several robust aggregation schemes have been
proposed to defend against malicious updates from Byzantine clients and improve
the robustness of federated learning. These solutions were claimed to be
Byzantine-robust, under certain assumptions. Other than that, new attack
strategies are emerging, striving to circumvent the defense schemes. However,
there is a lack of systematic comparison and empirical study thereof. In this
paper, we conduct an experimental study of Byzantine-robust aggregation schemes
under different attacks using two popular algorithms in federated learning,
FedSGD and FedAvg . We first survey existing Byzantine attack strategies and
Byzantine-robust aggregation schemes that aim to defend against Byzantine
attacks. We also propose a new scheme, ClippedClustering , to enhance the
robustness of a clustering-based scheme by automatically clipping the updates.
Then we provide an experimental evaluation of eight aggregation schemes in the
scenario of five different Byzantine attacks. Our results show that these
aggregation schemes sustain relatively high accuracy in some cases but are
ineffective in others. In particular, our proposed ClippedClustering
successfully defends against most attacks under independent and IID local
datasets. However, when the local datasets are Non-IID, the performance of all
the aggregation schemes significantly decreases. With Non-IID data, some of
these aggregation schemes fail even in the complete absence of Byzantine
clients. We conclude that the robustness of all the aggregation schemes is
limited, highlighting the need for new defense strategies, in particular for
Non-IID datasets.",2302.07173v1,https://arxiv.org/pdf/2302.07173v1
"DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical
  Awareness for Robust Fovea Localization","Sifan Song, Jinfeng Wang, Zilong Wang, Jionglong Su, Xiaowei Ding, Kang Dang","Accurate fovea localization is essential for analyzing retinal diseases to
prevent irreversible vision loss. While current deep learning-based methods
outperform traditional ones, they still face challenges such as the lack of
local anatomical landmarks around the fovea, the inability to robustly handle
diseased retinal images, and the variations in image conditions. In this paper,
we propose a novel transformer-based architecture called DualStreamFoveaNet
(DSFN) for multi-cue fusion. This architecture explicitly incorporates
long-range connections and global features using retina and vessel
distributions for robust fovea localization. We introduce a spatial attention
mechanism in the dual-stream encoder to extract and fuse self-learned
anatomical information, focusing more on features distributed along blood
vessels and significantly reducing computational costs by decreasing token
numbers. Our extensive experiments show that the proposed architecture achieves
state-of-the-art performance on two public datasets and one large-scale private
dataset. Furthermore, we demonstrate that the DSFN is more robust on both
normal and diseased retina images and has better generalization capacity in
cross-dataset experiments.",2302.06961v4,https://arxiv.org/pdf/2302.06961v4
"Density-Softmax: Efficient Test-time Model for Uncertainty Estimation
  and Robustness under Distribution Shifts","Ha Manh Bui, Anqi Liu","Sampling-based methods, e.g., Deep Ensembles and Bayesian Neural Nets have
become promising approaches to improve the quality of uncertainty estimation
and robust generalization. However, they suffer from a large model size and
high latency at test-time, which limits the scalability needed for low-resource
devices and real-time applications. To resolve these computational issues, we
propose Density-Softmax, a sampling-free deterministic framework via combining
a density function built on a Lipschitz-constrained feature extractor with the
softmax layer. Theoretically, we show that our model is the solution of minimax
uncertainty risk and is distance-aware on feature space, thus reducing the
over-confidence of the standard softmax under distribution shifts. Empirically,
our method enjoys competitive results with state-of-the-art techniques in terms
of uncertainty and robustness, while having a lower number of model parameters
and a lower latency at test-time.",2302.06495v3,https://arxiv.org/pdf/2302.06495v3
Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting,"Yuchen Liu, Chen Chen, Lingjuan Lyu, Fangzhao Wu, Sai Wu, Gang Chen","Federated learning has exhibited vulnerabilities to Byzantine attacks, where
the Byzantine attackers can send arbitrary gradients to a central server to
destroy the convergence and performance of the global model. A wealth of robust
AGgregation Rules (AGRs) have been proposed to defend against Byzantine
attacks. However, Byzantine clients can still circumvent robust AGRs when data
is non-Identically and Independently Distributed (non-IID). In this paper, we
first reveal the root causes of performance degradation of current robust AGRs
in non-IID settings: the curse of dimensionality and gradient heterogeneity. In
order to address this issue, we propose GAS, a \shorten approach that can
successfully adapt existing robust AGRs to non-IID settings. We also provide a
detailed convergence analysis when the existing robust AGRs are combined with
GAS. Experiments on various real-world datasets verify the efficacy of our
proposed GAS. The implementation code is provided in
https://github.com/YuchenLiu-a/byzantine-gas.",2302.06079v2,https://arxiv.org/pdf/2302.06079v2
"Robust Representation Learning by Clustering with Bisimulation Metrics
  for Visual Reinforcement Learning with Distractions","Qiyuan Liu, Qi Zhou, Rui Yang, Jie Wang","Recent work has shown that representation learning plays a critical role in
sample-efficient reinforcement learning (RL) from pixels. Unfortunately, in
real-world scenarios, representation learning is usually fragile to
task-irrelevant distractions such as variations in background or viewpoint. To
tackle this problem, we propose a novel clustering-based approach, namely
Clustering with Bisimulation Metrics (CBM), which learns robust representations
by grouping visual observations in the latent space. Specifically, CBM
alternates between two steps: (1) grouping observations by measuring their
bisimulation distances to the learned prototypes; (2) learning a set of
prototypes according to the current cluster assignments. Computing cluster
assignments with bisimulation metrics enables CBM to capture task-relevant
information, as bisimulation metrics quantify the behavioral similarity between
observations. Moreover, CBM encourages the consistency of representations
within each group, which facilitates filtering out task-irrelevant information
and thus induces robust representations against distractions. An appealing
feature is that CBM can achieve sample-efficient representation learning even
if multiple distractions exist simultaneously.Experiments demonstrate that CBM
significantly improves the sample efficiency of popular visual RL algorithms
and achieves state-of-the-art performance on both multiple and single
distraction settings. The code is available at
https://github.com/MIRALab-USTC/RL-CBM.",2302.12003v2,https://arxiv.org/pdf/2302.12003v2
USER: Unsupervised Structural Entropy-based Robust Graph Neural Network,"Yifei Wang, Yupan Wang, Zeyu Zhang, Song Yang, Kaiqi Zhao, Jiamou Liu","Unsupervised/self-supervised graph neural networks (GNN) are vulnerable to
inherent randomness in the input graph data which greatly affects the
performance of the model in downstream tasks. In this paper, we alleviate the
interference of graph randomness and learn appropriate representations of nodes
without label information. To this end, we propose USER, an unsupervised robust
version of graph neural networks that is based on structural entropy. We
analyze the property of intrinsic connectivity and define intrinsic
connectivity graph. We also identify the rank of the adjacency matrix as a
crucial factor in revealing a graph that provides the same embeddings as the
intrinsic connectivity graph. We then introduce structural entropy in the
objective function to capture such a graph. Extensive experiments conducted on
clustering and link prediction tasks under random-noises and meta-attack over
three datasets show USER outperforms benchmarks and is robust to heavier
randomness.",2302.05889v1,https://arxiv.org/pdf/2302.05889v1
"Pushing the Accuracy-Group Robustness Frontier with Introspective
  Self-play","Jeremiah Zhe Liu, Krishnamurthy Dj Dvijotham, Jihyeon Lee, Quan Yuan, Martin Strobel, Balaji Lakshminarayanan, Deepak Ramachandran","Standard empirical risk minimization (ERM) training can produce deep neural
network (DNN) models that are accurate on average but under-perform in
under-represented population subgroups, especially when there are imbalanced
group distributions in the long-tailed training data. Therefore, approaches
that improve the accuracy-group robustness trade-off frontier of a DNN model
(i.e. improving worst-group accuracy without sacrificing average accuracy, or
vice versa) is of crucial importance. Uncertainty-based active learning (AL)
can potentially improve the frontier by preferentially sampling
underrepresented subgroups to create a more balanced training dataset. However,
the quality of uncertainty estimates from modern DNNs tend to degrade in the
presence of spurious correlations and dataset bias, compromising the
effectiveness of AL for sampling tail groups. In this work, we propose
Introspective Self-play (ISP), a simple approach to improve the uncertainty
estimation of a deep neural network under dataset bias, by adding an auxiliary
introspection task requiring a model to predict the bias for each data point in
addition to the label. We show that ISP provably improves the bias-awareness of
the model representation and the resulting uncertainty estimates. On two
real-world tabular and language tasks, ISP serves as a simple ""plug-in"" for AL
model training, consistently improving both the tail-group sampling rate and
the final accuracy-fairness trade-off frontier of popular AL methods.",2302.05807v1,https://arxiv.org/pdf/2302.05807v1
HateProof: Are Hateful Meme Detection Systems really Robust?,"Piush Aggarwal, Pranit Chawla, Mithun Das, Punyajoy Saha, Binny Mathew, Torsten Zesch, Animesh Mukherjee","Exploiting social media to spread hate has tremendously increased over the
years. Lately, multi-modal hateful content such as memes has drawn relatively
more traction than uni-modal content. Moreover, the availability of implicit
content payloads makes them fairly challenging to be detected by existing
hateful meme detection systems. In this paper, we present a use case study to
analyze such systems' vulnerabilities against external adversarial attacks. We
find that even very simple perturbations in uni-modal and multi-modal settings
performed by humans with little knowledge about the model can make the existing
detection models highly vulnerable. Empirically, we find a noticeable
performance drop of as high as 10% in the macro-F1 score for certain attacks.
As a remedy, we attempt to boost the model's robustness using contrastive
learning as well as an adversarial training-based method - VILLA. Using an
ensemble of the above two approaches, in two of our high resolution datasets,
we are able to (re)gain back the performance to a large extent for certain
attacks. We believe that ours is a first step toward addressing this crucial
problem in an adversarial setting and would inspire more such investigations in
the future.",2302.05703v1,https://arxiv.org/pdf/2302.05703v1
Evaluating the Robustness of Discrete Prompts,"Yoichi Ishibashi, Danushka Bollegala, Katsuhito Sudoh, Satoshi Nakamura","Discrete prompts have been used for fine-tuning Pre-trained Language Models
for diverse NLP tasks. In particular, automatic methods that generate discrete
prompts from a small set of training instances have reported superior
performance. However, a closer look at the learnt prompts reveals that they
contain noisy and counter-intuitive lexical constructs that would not be
encountered in manually-written prompts. This raises an important yet
understudied question regarding the robustness of automatically learnt discrete
prompts when used in downstream tasks. To address this question, we conduct a
systematic study of the robustness of discrete prompts by applying carefully
designed perturbations into an application using AutoPrompt and then measure
their performance in two Natural Language Inference (NLI) datasets. Our
experimental results show that although the discrete prompt-based method
remains relatively robust against perturbations to NLI inputs, they are highly
sensitive to other types of perturbations such as shuffling and deletion of
prompt tokens. Moreover, they generalize poorly across different NLI datasets.
We hope our findings will inspire future work on robust discrete prompt
learning.",2302.05619v1,https://arxiv.org/pdf/2302.05619v1
Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis,"Zhu Wang, Sourav Medya, Sathya N. Ravi","Often, deep network models are purely inductive during training and while
performing inference on unseen data. Thus, when such models are used for
predictions, it is well known that they often fail to capture the semantic
information and implicit dependencies that exist among objects (or concepts) on
a population level. Moreover, it is still unclear how domain or prior modal
knowledge can be specified in a backpropagation friendly manner, especially in
large-scale and noisy settings. In this work, we propose an end-to-end vision
and language model incorporating explicit knowledge graphs. We also introduce
an interactive out-of-distribution (OOD) layer using implicit network operator.
The layer is used to filter noise that is brought by external knowledge base.
In practice, we apply our model on several vision and language downstream tasks
including visual question answering, visual reasoning, and image-text retrieval
on different datasets. Our experiments show that it is possible to design
models that perform similarly to state-of-art results but with significantly
fewer samples and training time.",2302.05608v1,https://arxiv.org/pdf/2302.05608v1
Robust Knowledge Transfer in Tiered Reinforcement Learning,"Jiawei Huang, Niao He","In this paper, we study the Tiered Reinforcement Learning setting, a parallel
transfer learning framework, where the goal is to transfer knowledge from the
low-tier (source) task to the high-tier (target) task to reduce the exploration
risk of the latter while solving the two tasks in parallel. Unlike previous
work, we do not assume the low-tier and high-tier tasks share the same dynamics
or reward functions, and focus on robust knowledge transfer without prior
knowledge on the task similarity. We identify a natural and necessary condition
called the ``Optimal Value Dominance'' for our objective. Under this condition,
we propose novel online learning algorithms such that, for the high-tier task,
it can achieve constant regret on partial states depending on the task
similarity and retain near-optimal regret when the two tasks are dissimilar,
while for the low-tier task, it can keep near-optimal without making sacrifice.
Moreover, we further study the setting with multiple low-tier tasks, and
propose a novel transfer source selection mechanism, which can ensemble the
information from all low-tier tasks and allow provable benefits on a much
larger state-action space.",2302.05534v3,https://arxiv.org/pdf/2302.05534v3
Towards Minimax Optimality of Model-based Robust Reinforcement Learning,"Pierre Clavier, Erwan Le Pennec, Matthieu Geist","We study the sample complexity of obtaining an $\epsilon$-optimal policy in
\emph{Robust} discounted Markov Decision Processes (RMDPs), given only access
to a generative model of the nominal kernel. This problem is widely studied in
the non-robust case, and it is known that any planning approach applied to an
empirical MDP estimated with $\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A
\mid}{\epsilon^2})$ samples provides an $\epsilon$-optimal policy, which is
minimax optimal. Results in the robust case are much more scarce. For $sa$-
(resp $s$-)rectangular uncertainty sets, the best known sample complexity is
$\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid}{\epsilon^2})$ (resp.
$\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid^2}{\epsilon^2})$), for
specific algorithms and when the uncertainty set is based on the total
variation (TV), the KL or the Chi-square divergences. In this paper, we
consider uncertainty sets defined with an $L_p$-ball (recovering the TV case),
and study the sample complexity of \emph{any} planning algorithm (with high
accuracy guarantee on the solution) applied to an empirical RMDP estimated
using the generative model. In the general case, we prove a sample complexity
of $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid\mid A \mid}{\epsilon^2})$ for
both the $sa$- and $s$-rectangular cases (improvements of $\mid S \mid$ and
$\mid S \mid\mid A \mid$ respectively). When the size of the uncertainty is
small enough, we improve the sample complexity to
$\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A \mid }{\epsilon^2})$,
recovering the lower-bound for the non-robust case for the first time and a
robust lower-bound when the size of the uncertainty is small enough.",2302.05372v3,https://arxiv.org/pdf/2302.05372v3
Beyond In-Domain Scenarios: Robust Density-Aware Calibration,"Christian Tomani, Futa Waseda, Yuesong Shen, Daniel Cremers","Calibrating deep learning models to yield uncertainty-aware predictions is
crucial as deep neural networks get increasingly deployed in safety-critical
applications. While existing post-hoc calibration methods achieve impressive
results on in-domain test datasets, they are limited by their inability to
yield reliable uncertainty estimates in domain-shift and out-of-domain (OOD)
scenarios. We aim to bridge this gap by proposing DAC, an accuracy-preserving
as well as Density-Aware Calibration method based on k-nearest-neighbors (KNN).
In contrast to existing post-hoc methods, we utilize hidden layers of
classifiers as a source for uncertainty-related information and study their
importance. We show that DAC is a generic method that can readily be combined
with state-of-the-art post-hoc methods. DAC boosts the robustness of
calibration performance in domain-shift and OOD, while maintaining excellent
in-domain predictive uncertainty estimates. We demonstrate that DAC leads to
consistently better calibration across a large number of model architectures,
datasets, and metrics. Additionally, we show that DAC improves calibration
substantially on recent large-scale neural networks pre-trained on vast amounts
of data.",2302.05118v2,https://arxiv.org/pdf/2302.05118v2
"CCDN: Checkerboard Corner Detection Network for Robust Camera
  Calibration","Ben Chen, Caihua Xiong, Qi Zhang","Aiming to improve the checkerboard corner detection robustness against the
images with poor quality, such as lens distortion, extreme poses, and noise, we
propose a novel detection algorithm which can maintain high accuracy on inputs
under multiply scenarios without any prior knowledge of the checkerboard
pattern. This whole algorithm includes a checkerboard corner detection network
and some post-processing techniques. The network model is a fully convolutional
network with improvements of loss function and learning rate, which can deal
with the images of arbitrary size and produce correspondingly-sized output with
a corner score on each pixel by efficient inference and learning. Besides, in
order to remove the false positives, we employ three post-processing techniques
including threshold related to maximum response, non-maximum suppression, and
clustering. Evaluations on two different datasets show its superior robustness,
accuracy and wide applicability in quantitative comparisons with the
state-of-the-art methods, like MATE, ChESS, ROCHADE and OCamCalib.",2302.05097v1,https://arxiv.org/pdf/2302.05097v1
On the Privacy-Robustness-Utility Trilemma in Distributed Learning,"Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot, John Stephan","The ubiquity of distributed machine learning (ML) in sensitive public domain
applications calls for algorithms that protect data privacy, while being robust
to faults and adversarial behaviors. Although privacy and robustness have been
extensively studied independently in distributed ML, their synthesis remains
poorly understood. We present the first tight analysis of the error incurred by
any algorithm ensuring robustness against a fraction of adversarial machines,
as well as differential privacy (DP) for honest machines' data against any
other curious entity. Our analysis exhibits a fundamental trade-off between
privacy, robustness, and utility. To prove our lower bound, we consider the
case of mean estimation, subject to distributed DP and robustness constraints,
and devise reductions to centralized estimation of one-way marginals. We prove
our matching upper bound by presenting a new distributed ML algorithm using a
high-dimensional robust aggregation rule. The latter amortizes the dependence
on the dimension in the error (caused by adversarial workers and DP), while
being agnostic to the statistical properties of the data.",2302.04787v2,https://arxiv.org/pdf/2302.04787v2
Robust and Scalable Bayesian Online Changepoint Detection,"Matias Altamirano, François-Xavier Briol, Jeremias Knoblauch","This paper proposes an online, provably robust, and scalable Bayesian
approach for changepoint detection. The resulting algorithm has key advantages
over previous work: it provides provable robustness by leveraging the
generalised Bayesian perspective, and also addresses the scalability issues of
previous attempts. Specifically, the proposed generalised Bayesian formalism
leads to conjugate posteriors whose parameters are available in closed form by
leveraging diffusion score matching. The resulting algorithm is exact, can be
updated through simple algebra, and is more than 10 times faster than its
closest competitor.",2302.04759v2,https://arxiv.org/pdf/2302.04759v2
Outlier-Robust Gromov-Wasserstein for Graph Data,"Lemin Kong, Jiajin Li, Jianheng Tang, Anthony Man-Cho So","Gromov-Wasserstein (GW) distance is a powerful tool for comparing and
aligning probability distributions supported on different metric spaces.
Recently, GW has become the main modeling technique for aligning heterogeneous
data for a wide range of graph learning tasks. However, the GW distance is
known to be highly sensitive to outliers, which can result in large
inaccuracies if the outliers are given the same weight as other samples in the
objective function. To mitigate this issue, we introduce a new and robust
version of the GW distance called RGW. RGW features optimistically perturbed
marginal constraints within a Kullback-Leibler divergence-based ambiguity set.
To make the benefits of RGW more accessible in practice, we develop a
computationally efficient and theoretically provable procedure using Bregman
proximal alternating linearized minimization algorithm. Through extensive
experimentation, we validate our theoretical results and demonstrate the
effectiveness of RGW on real-world graph learning tasks, such as subgraph
matching and partial shape correspondence.",2302.04610v2,https://arxiv.org/pdf/2302.04610v2
"FLAC: A Robust Failure-Aware Atomic Commit Protocol for Distributed
  Transactions","Hexiang Pan, Quang-Trung Ta, Meihui Zhang, Yeow Meng Chee, Gang Chen, Beng Chin Ooi","In distributed transaction processing, atomic commit protocol (ACP) is used
to ensure database consistency. With the use of commodity compute nodes and
networks, failures such as system crashes and network partitioning are common.
It is therefore important for ACP to dynamically adapt to the operating
condition for efficiency while ensuring the consistency of the database.
Existing ACPs often assume stable operating conditions, hence, they are either
non-generalizable to different environments or slow in practice.
  In this paper, we propose a novel and practical ACP, called Failure-Aware
Atomic Commit (FLAC). In essence, FLAC includes three protocols, which are
specifically designed for three different environments: (i) no failure occurs,
(ii) participant nodes might crash but there is no delayed connection, or (iii)
both crashed nodes and delayed connection can occur. It models these
environments as the failure-free, crash-failure, and network-failure robustness
levels. During its operation, FLAC can monitor if any failure occurs and
dynamically switch to operate the most suitable protocol, using a robustness
level state machine, whose parameters are fine-tuned by reinforcement learning.
Consequently, it improves both the response time and throughput, and
effectively handles nodes distributed across the Internet where crash and
network failures might occur. We implement FLAC in a distributed transactional
key-value storage system based on Google Percolator and evaluate its
performance with both a micro benchmark and a macro benchmark of real workload.
The results show that FLAC achieves up to 2.22x throughput improvement and
2.82x latency speedup, compared to existing ACPs for high-contention workloads.",2302.04500v3,https://arxiv.org/pdf/2302.04500v3
IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness,"Xiaoyun Xu, Guilherme Perin, Stjepan Picek","In this paper, we propose a novel method, IB-RAR, which uses Information
Bottleneck (IB) to strengthen adversarial robustness for both adversarial
training and non-adversarial-trained methods. We first use the IB theory to
build regularizers as learning objectives in the loss function. Then, we filter
out unnecessary features of intermediate representation according to their
mutual information (MI) with labels, as the network trained with IB provides
easily distinguishable MI for its features. Experimental results show that our
method can be naturally combined with adversarial training and provides
consistently better accuracy on new adversarial examples. Our method improves
the accuracy by an average of 3.07% against five adversarial attacks for the
VGG16 network, trained with three adversarial training benchmarks and the
CIFAR-10 dataset. In addition, our method also provides good robustness for
undefended methods, such as training with cross-entropy loss only. Finally, in
the absence of adversarial training, the VGG16 network trained using our method
and the CIFAR-10 dataset reaches an accuracy of 35.86% against PGD examples,
while using all layers reaches 25.61% accuracy.",2302.10896v2,https://arxiv.org/pdf/2302.10896v2
"Et Tu Certifications: Robustness Certificates Yield Better Adversarial
  Examples","Andrew C. Cullen, Shijie Liu, Paul Montague, Sarah M. Erfani, Benjamin I. P. Rubinstein","In guaranteeing the absence of adversarial examples in an instance's
neighbourhood, certification mechanisms play an important role in demonstrating
neural net robustness. In this paper, we ask if these certifications can
compromise the very models they help to protect? Our new \emph{Certification
Aware Attack} exploits certifications to produce computationally efficient
norm-minimising adversarial examples $74 \%$ more often than comparable
attacks, while reducing the median perturbation norm by more than $10\%$. While
these attacks can be used to assess the tightness of certification bounds, they
also highlight that releasing certifications can paradoxically reduce security.",2302.04379v4,https://arxiv.org/pdf/2302.04379v4
"Robustness to Spurious Correlations Improves Semantic
  Out-of-Distribution Detection","Lily H. Zhang, Rajesh Ranganath","Methods which utilize the outputs or feature representations of predictive
models have emerged as promising approaches for out-of-distribution (OOD)
detection of image inputs. However, these methods struggle to detect OOD inputs
that share nuisance values (e.g. background) with in-distribution inputs. The
detection of shared-nuisance out-of-distribution (SN-OOD) inputs is
particularly relevant in real-world applications, as anomalies and
in-distribution inputs tend to be captured in the same settings during
deployment. In this work, we provide a possible explanation for SN-OOD
detection failures and propose nuisance-aware OOD detection to address them.
Nuisance-aware OOD detection substitutes a classifier trained via empirical
risk minimization and cross-entropy loss with one that 1. is trained under a
distribution where the nuisance-label relationship is broken and 2. yields
representations that are independent of the nuisance under this distribution,
both marginally and conditioned on the label. We can train a classifier to
achieve these objectives using Nuisance-Randomized Distillation (NuRD), an
algorithm developed for OOD generalization under spurious correlations. Output-
and feature-based nuisance-aware OOD detection perform substantially better
than their original counterparts, succeeding even when detection based on
domain generalization algorithms fails to improve performance.",2302.04132v1,https://arxiv.org/pdf/2302.04132v1
WAT: Improve the Worst-class Robustness in Adversarial Training,"Boqi Li, Weiwei Liu","Deep Neural Networks (DNN) have been shown to be vulnerable to adversarial
examples. Adversarial training (AT) is a popular and effective strategy to
defend against adversarial attacks. Recent works (Benz et al., 2020; Xu et al.,
2021; Tian et al., 2021) have shown that a robust model well-trained by AT
exhibits a remarkable robustness disparity among classes, and propose various
methods to obtain consistent robust accuracy across classes. Unfortunately,
these methods sacrifice a good deal of the average robust accuracy.
Accordingly, this paper proposes a novel framework of worst-class adversarial
training and leverages no-regret dynamics to solve this problem. Our goal is to
obtain a classifier with great performance on worst-class and sacrifice just a
little average robust accuracy at the same time. We then rigorously analyze the
theoretical properties of our proposed algorithm, and the generalization error
bound in terms of the worst-class robust risk. Furthermore, we propose a
measurement to evaluate the proposed method in terms of both the average and
worst-class accuracies. Experiments on various datasets and networks show that
our proposed method outperforms the state-of-the-art approaches.",2302.04025v1,https://arxiv.org/pdf/2302.04025v1
"Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset
  Selection","Xilie Xu, Jingfeng Zhang, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli","Adversarial contrastive learning (ACL) does not require expensive data
annotations but outputs a robust representation that withstands adversarial
attacks and also generalizes to a wide range of downstream tasks. However, ACL
needs tremendous running time to generate the adversarial variants of all
training data, which limits its scalability to large datasets. To speed up ACL,
this paper proposes a robustness-aware coreset selection (RCS) method. RCS does
not require label information and searches for an informative subset that
minimizes a representational divergence, which is the distance of the
representation between natural data and their virtual adversarial variants. The
vanilla solution of RCS via traversing all possible subsets is computationally
prohibitive. Therefore, we theoretically transform RCS into a surrogate problem
of submodular maximization, of which the greedy search is an efficient solution
with an optimality guarantee for the original problem. Empirically, our
comprehensive results corroborate that RCS can speed up ACL by a large margin
without significantly hurting the robustness transferability. Notably, to the
best of our knowledge, we are the first to conduct ACL efficiently on the
large-scale ImageNet-1K dataset to obtain an effective robust representation
via RCS. Our source code is at
https://github.com/GodXuxilie/Efficient_ACL_via_RCS.",2302.03857v5,https://arxiv.org/pdf/2302.03857v5
Temporal Robustness against Data Poisoning,"Wenxiao Wang, Soheil Feizi","Data poisoning considers cases when an adversary manipulates the behavior of
machine learning algorithms through malicious training data. Existing threat
models of data poisoning center around a single metric, the number of poisoned
samples. In consequence, if attackers can poison more samples than expected
with affordable overhead, as in many practical scenarios, they may be able to
render existing defenses ineffective in a short time. To address this issue, we
leverage timestamps denoting the birth dates of data, which are often available
but neglected in the past. Benefiting from these timestamps, we propose a
temporal threat model of data poisoning with two novel metrics, earliness and
duration, which respectively measure how long an attack started in advance and
how long an attack lasted. Using these metrics, we define the notions of
temporal robustness against data poisoning, providing a meaningful sense of
protection even with unbounded amounts of poisoned samples when the attacks are
temporally bounded. We present a benchmark with an evaluation protocol
simulating continuous data collection and periodic deployments of updated
models, thus enabling empirical evaluation of temporal robustness. Lastly, we
develop and also empirically verify a baseline defense, namely temporal
aggregation, offering provable temporal robustness and highlighting the
potential of our temporal threat model for data poisoning.",2302.03684v3,https://arxiv.org/pdf/2302.03684v3
"Med-NCA: Robust and Lightweight Segmentation with Neural Cellular
  Automata","John Kalkhof, Camila González, Anirban Mukhopadhyay","Access to the proper infrastructure is critical when performing medical image
segmentation with Deep Learning. This requirement makes it difficult to run
state-of-the-art segmentation models in resource-constrained scenarios like
primary care facilities in rural areas and during crises. The recently emerging
field of Neural Cellular Automata (NCA) has shown that locally interacting
one-cell models can achieve competitive results in tasks such as image
generation or segmentations in low-resolution inputs. However, they are
constrained by high VRAM requirements and the difficulty of reaching
convergence for high-resolution images. To counteract these limitations we
propose Med-NCA, an end-to-end NCA training pipeline for high-resolution image
segmentation. Our method follows a two-step process. Global knowledge is first
communicated between cells across the downscaled image. Following that,
patch-based segmentation is performed. Our proposed Med-NCA outperforms the
classic UNet by 2% and 3% Dice for hippocampus and prostate segmentation,
respectively, while also being 500 times smaller. We also show that Med-NCA is
by design invariant with respect to image scale, shape and translation,
experiencing only slight performance degradation even with strong shifts; and
is robust against MRI acquisition artefacts. Med-NCA enables high-resolution
medical image segmentation even on a Raspberry Pi B+, arguably the smallest
device able to run PyTorch and that can be powered by a standard power bank.",2302.03473v1,https://arxiv.org/pdf/2302.03473v1
Robustness Implies Fairness in Causal Algorithmic Recourse,"Ahmad-Reza Ehyaei, Amir-Hossein Karimi, Bernhard Schölkopf, Setareh Maghsudi","Algorithmic recourse aims to disclose the inner workings of the black-box
decision process in situations where decisions have significant consequences,
by providing recommendations to empower beneficiaries to achieve a more
favorable outcome. To ensure an effective remedy, suggested interventions must
not only be low-cost but also robust and fair. This goal is accomplished by
providing similar explanations to individuals who are alike. This study
explores the concept of individual fairness and adversarial robustness in
causal algorithmic recourse and addresses the challenge of achieving both. To
resolve the challenges, we propose a new framework for defining adversarially
robust recourse. The new setting views the protected feature as a pseudometric
and demonstrates that individual fairness is a special case of adversarial
robustness. Finally, we introduce the fair robust recourse problem to achieve
both desirable properties and show how it can be satisfied both theoretically
and empirically.",2302.03465v2,https://arxiv.org/pdf/2302.03465v2
"Hebbian and Gradient-based Plasticity Enables Robust Memory and Rapid
  Learning in RNNs","Yu Duan, Zhongfan Jia, Qian Li, Yi Zhong, Kaisheng Ma","Rapidly learning from ongoing experiences and remembering past events with a
flexible memory system are two core capacities of biological intelligence.
While the underlying neural mechanisms are not fully understood, various
evidence supports that synaptic plasticity plays a critical role in memory
formation and fast learning. Inspired by these results, we equip Recurrent
Neural Networks (RNNs) with plasticity rules to enable them to adapt their
parameters according to ongoing experiences. In addition to the traditional
local Hebbian plasticity, we propose a global, gradient-based plasticity rule,
which allows the model to evolve towards its self-determined target. Our models
show promising results on sequential and associative memory tasks, illustrating
their ability to robustly form and retain memories. In the meantime, these
models can cope with many challenging few-shot learning problems. Comparing
different plasticity rules under the same framework shows that Hebbian
plasticity is well-suited for several memory and associative learning tasks;
however, it is outperformed by gradient-based plasticity on few-shot regression
tasks which require the model to infer the underlying mapping. Code is
available at https://github.com/yuvenduan/PlasticRNNs.",2302.03235v1,https://arxiv.org/pdf/2302.03235v1
"Exploring and Exploiting Decision Boundary Dynamics for Adversarial
  Robustness","Yuancheng Xu, Yanchao Sun, Micah Goldblum, Tom Goldstein, Furong Huang","The robustness of a deep classifier can be characterized by its margins: the
decision boundary's distances to natural data points. However, it is unclear
whether existing robust training methods effectively increase the margin for
each vulnerable point during training. To understand this, we propose a
continuous-time framework for quantifying the relative speed of the decision
boundary with respect to each individual point. Through visualizing the moving
speed of the decision boundary under Adversarial Training, one of the most
effective robust training algorithms, a surprising moving-behavior is revealed:
the decision boundary moves away from some vulnerable points but simultaneously
moves closer to others, decreasing their margins. To alleviate these
conflicting dynamics of the decision boundary, we propose Dynamics-aware Robust
Training (DyART), which encourages the decision boundary to engage in movement
that prioritizes increasing smaller margins. In contrast to prior works, DyART
directly operates on the margins rather than their indirect approximations,
allowing for more targeted and effective robustness improvement. Experiments on
the CIFAR-10 and Tiny-ImageNet datasets verify that DyART alleviates the
conflicting dynamics of the decision boundary and obtains improved robustness
under various perturbation sizes compared to the state-of-the-art defenses. Our
code is available at
https://github.com/Yuancheng-Xu/Dynamics-Aware-Robust-Training.",2302.03015v2,https://arxiv.org/pdf/2302.03015v2
Robust Subtask Learning for Compositional Generalization,"Kishor Jothimurugan, Steve Hsu, Osbert Bastani, Rajeev Alur","Compositional reinforcement learning is a promising approach for training
policies to perform complex long-horizon tasks. Typically, a high-level task is
decomposed into a sequence of subtasks and a separate policy is trained to
perform each subtask. In this paper, we focus on the problem of training
subtask policies in a way that they can be used to perform any task; here, a
task is given by a sequence of subtasks. We aim to maximize the worst-case
performance over all tasks as opposed to the average-case performance. We
formulate the problem as a two agent zero-sum game in which the adversary picks
the sequence of subtasks. We propose two RL algorithms to solve this game: one
is an adaptation of existing multi-agent RL algorithms to our setting and the
other is an asynchronous version which enables parallel training of subtask
policies. We evaluate our approach on two multi-task environments with
continuous states and actions and demonstrate that our algorithms outperform
state-of-the-art baselines.",2302.02984v2,https://arxiv.org/pdf/2302.02984v2
"Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group
  Shifts","Amrith Setlur, Don Dennis, Benjamin Eysenbach, Aditi Raghunathan, Chelsea Finn, Virginia Smith, Sergey Levine","Training machine learning models robust to distribution shifts is critical
for real-world applications. Some robust training algorithms (e.g., Group DRO)
specialize to group shifts and require group information on all training
points. Other methods (e.g., CVaR DRO) that do not need group annotations can
be overly conservative, since they naively upweight high loss points which may
form a contrived set that does not correspond to any meaningful group in the
real world (e.g., when the high loss points are randomly mislabeled training
points). In this work, we address limitations in prior approaches by assuming a
more nuanced form of group shift: conditioned on the label, we assume that the
true group function (indicator over group) is simple. For example, we may
expect that group shifts occur along low bitrate features (e.g., image
background, lighting). Thus, we aim to learn a model that maintains high
accuracy on simple group functions realized by these low bitrate features, that
need not spend valuable model capacity achieving high accuracy on contrived
groups of examples. Based on this, we consider the two-player game formulation
of DRO where the adversary's capacity is bitrate-constrained. Our resulting
practical algorithm, Bitrate-Constrained DRO (BR-DRO), does not require group
information on training samples yet matches the performance of Group DRO on
datasets that have training group annotations and that of CVaR DRO on
long-tailed distributions. Our theoretical analysis reveals that in some
settings BR-DRO objective can provably yield statistically efficient and less
conservative solutions than unconstrained CVaR DRO.",2302.02931v2,https://arxiv.org/pdf/2302.02931v2
"Collective Robustness Certificates: Exploiting Interdependence in Graph
  Neural Networks","Jan Schuchardt, Aleksandar Bojchevski, Johannes Gasteiger, Stephan Günnemann","In tasks like node classification, image segmentation, and named-entity
recognition we have a classifier that simultaneously outputs multiple
predictions (a vector of labels) based on a single input, i.e. a single graph,
image, or document respectively. Existing adversarial robustness certificates
consider each prediction independently and are thus overly pessimistic for such
tasks. They implicitly assume that an adversary can use different perturbed
inputs to attack different predictions, ignoring the fact that we have a single
shared input. We propose the first collective robustness certificate which
computes the number of predictions that are simultaneously guaranteed to remain
stable under perturbation, i.e. cannot be attacked. We focus on Graph Neural
Networks and leverage their locality property - perturbations only affect the
predictions in a close neighborhood - to fuse multiple single-node certificates
into a drastically stronger collective certificate. For example, on the
Citeseer dataset our collective certificate for node classification increases
the average number of certifiable feature perturbations from $7$ to $351$.",2302.02829v1,https://arxiv.org/pdf/2302.02829v1
On Private and Robust Bandits,"Yulian Wu, Xingyu Zhou, Youming Tao, Di Wang","We study private and robust multi-armed bandits (MABs), where the agent
receives Huber's contaminated heavy-tailed rewards and meanwhile needs to
ensure differential privacy. We first present its minimax lower bound,
characterizing the information-theoretic limit of regret with respect to
privacy budget, contamination level and heavy-tailedness. Then, we propose a
meta-algorithm that builds on a private and robust mean estimation sub-routine
\texttt{PRM} that essentially relies on reward truncation and the Laplace
mechanism only. For two different heavy-tailed settings, we give specific
schemes of \texttt{PRM}, which enable us to achieve nearly-optimal regret. As
by-products of our main results, we also give the first minimax lower bound for
private heavy-tailed MABs (i.e., without contamination). Moreover, our two
proposed truncation-based \texttt{PRM} achieve the optimal trade-off between
estimation accuracy, privacy and robustness. Finally, we support our
theoretical results with experimental studies.",2302.02526v2,https://arxiv.org/pdf/2302.02526v2
"Leaving Reality to Imagination: Robust Classification via Generated
  Datasets","Hritik Bansal, Aditya Grover","Recent research on robustness has revealed significant performance gaps
between neural image classifiers trained on datasets that are similar to the
test set, and those that are from a naturally shifted distribution, such as
sketches, paintings, and animations of the object categories observed during
training. Prior work focuses on reducing this gap by designing engineered
augmentations of training data or through unsupervised pretraining of a single
large model on massive in-the-wild training datasets scraped from the Internet.
However, the notion of a dataset is also undergoing a paradigm shift in recent
years. With drastic improvements in the quality, ease-of-use, and access to
modern generative models, generated data is pervading the web. In this light,
we study the question: How do these generated datasets influence the natural
robustness of image classifiers? We find that Imagenet classifiers trained on
real data augmented with generated data achieve higher accuracy and effective
robustness than standard training and popular augmentation strategies in the
presence of natural distribution shifts. We analyze various factors influencing
these results, including the choice of conditioning strategies and the amount
of generated data. Additionally, we find that the standard ImageNet classifiers
suffer a performance degradation of upto 20\% on the generated data, indicating
their fragility at accurately classifying the objects under novel variations.
Lastly, we demonstrate that the image classifiers, which have been trained on
real data augmented with generated data from the base generative model, exhibit
greater resilience to natural distribution shifts compared to the classifiers
trained on real data augmented with generated data from the finetuned
generative model on the real data. The code, models, and datasets are available
at https://github.com/Hritikbansal/generative-robustness.",2302.02503v2,https://arxiv.org/pdf/2302.02503v2
Rethinking Robust Contrastive Learning from the Adversarial Perspective,"Fatemeh Ghofrani, Mehdi Yaghouti, Pooyan Jamshidi","To advance the understanding of robust deep learning, we delve into the
effects of adversarial training on self-supervised and supervised contrastive
learning alongside supervised learning. Our analysis uncovers significant
disparities between adversarial and clean representations in standard-trained
networks across various learning algorithms. Remarkably, adversarial training
mitigates these disparities and fosters the convergence of representations
toward a universal set, regardless of the learning scheme used. Additionally,
increasing the similarity between adversarial and clean representations,
particularly near the end of the network, enhances network robustness. These
findings offer valuable insights for designing and training effective and
robust deep learning networks. Our code is released at
\textcolor{magenta}{\url{https://github.com/softsys4ai/CL-Robustness}}.",2302.02502v2,https://arxiv.org/pdf/2302.02502v2
"Achieving Robust Generalization for Wireless Channel Estimation Neural
  Networks by Designed Training Data","Dianxin Luan, John Thompson","In this paper, we propose a method to design the training data that can
support robust generalization of trained neural networks to unseen channels.
The proposed design that improves the generalization is described and analysed.
It avoids the requirement of online training for previously unseen channels, as
this is a memory and processing intensive solution, especially for battery
powered mobile terminals. To prove the validity of the proposed method, we use
the channels modelled by different standards and fading modelling for
simulation. We also use an attention-based structure and a convolutional neural
network to evaluate the generalization results achieved. Simulation results
show that the trained neural networks maintain almost identical performance on
the unseen channels.",2302.02302v1,https://arxiv.org/pdf/2302.02302v1
On Robust Numerical Solver for ODE via Self-Attention Mechanism,"Zhongzhan Huang, Mingfu Liang, Liang Lin","With the development of deep learning techniques, AI-enhanced numerical
solvers are expected to become a new paradigm for solving differential
equations due to their versatility and effectiveness in alleviating the
accuracy-speed trade-off in traditional numerical solvers. However, this
paradigm still inevitably requires a large amount of high-quality data, whose
acquisition is often very expensive in natural science and engineering
problems. Therefore, in this paper, we explore training efficient and robust
AI-enhanced numerical solvers with a small data size by mitigating intrinsic
noise disturbances. We first analyze the ability of the self-attention
mechanism to regulate noise in supervised learning and then propose a
simple-yet-effective numerical solver, AttSolver, which introduces an additive
self-attention mechanism to the numerical solution of differential equations
based on the dynamical system perspective of the residual neural network. Our
results on benchmarks, ranging from high-dimensional problems to chaotic
systems, demonstrate the effectiveness of AttSolver in generally improving the
performance of existing traditional numerical solvers without any elaborated
model crafting. Finally, we analyze the convergence, generalization, and
robustness of the proposed method experimentally and theoretically.",2302.10184v1,https://arxiv.org/pdf/2302.10184v1
Certified Robust Control under Adversarial Perturbations,"Jinghan Yang, Hunmin Kim, Wenbin Wan, Naira Hovakimyan, Yevgeniy Vorobeychik","Autonomous systems increasingly rely on machine learning techniques to
transform high-dimensional raw inputs into predictions that are then used for
decision-making and control. However, it is often easy to maliciously
manipulate such inputs and, as a result, predictions. While effective
techniques have been proposed to certify the robustness of predictions to
adversarial input perturbations, such techniques have been disembodied from
control systems that make downstream use of the predictions. We propose the
first approach for composing robustness certification of predictions with
respect to raw input perturbations with robust control to obtain certified
robustness of control to adversarial input perturbations. We use a case study
of adaptive vehicle control to illustrate our approach and show the value of
the resulting end-to-end certificates through extensive experiments.",2302.02208v1,https://arxiv.org/pdf/2302.02208v1
"Interpolation for Robust Learning: Data Augmentation on Wasserstein
  Geodesics","Jiacheng Zhu, Jielin Qiu, Aritra Guha, Zhuolin Yang, Xuanlong Nguyen, Bo Li, Ding Zhao","We propose to study and promote the robustness of a model as per its
performance through the interpolation of training data distributions.
Specifically, (1) we augment the data by finding the worst-case Wasserstein
barycenter on the geodesic connecting subpopulation distributions of different
categories. (2) We regularize the model for smoother performance on the
continuous geodesic path connecting subpopulation distributions. (3)
Additionally, we provide a theoretical guarantee of robustness improvement and
investigate how the geodesic location and the sample size contribute,
respectively. Experimental validations of the proposed strategy on
\textit{four} datasets, including CIFAR-100 and ImageNet, establish the
efficacy of our method, e.g., our method improves the baselines' certifiable
robustness on CIFAR10 up to $7.7\%$, with $16.8\%$ on empirical robustness on
CIFAR-100. Our work provides a new perspective of model robustness through the
lens of Wasserstein geodesic-based interpolation with a practical off-the-shelf
strategy that can be combined with existing robust training methods.",2302.02092v3,https://arxiv.org/pdf/2302.02092v3
Robust Budget Pacing with a Single Sample,"Santiago Balseiro, Rachitesh Kumar, Vahab Mirrokni, Balasubramanian Sivan, Di Wang","Major Internet advertising platforms offer budget pacing tools as a standard
service for advertisers to manage their ad campaigns. Given the inherent
non-stationarity in an advertiser's value and also competing advertisers'
values over time, a commonly used approach is to learn a target expenditure
plan that specifies a target spend as a function of time, and then run a
controller that tracks this plan. This raises the question: how many historical
samples are required to learn a good expenditure plan? We study this question
by considering an advertiser repeatedly participating in $T$ second-price
auctions, where the tuple of her value and the highest competing bid is drawn
from an unknown time-varying distribution. The advertiser seeks to maximize her
total utility subject to her budget constraint. Prior work has shown the
sufficiency of $T\log T$ samples per distribution to achieve the optimal
$O(\sqrt{T})$-regret. We dramatically improve this state-of-the-art and show
that just one sample per distribution is enough to achieve the near-optimal
$\tilde O(\sqrt{T})$-regret, while still being robust to noise in the sampling
distributions.",2302.02006v1,https://arxiv.org/pdf/2302.02006v1
Asymmetric Certified Robustness via Feature-Convex Neural Networks,"Samuel Pfrommer, Brendon G. Anderson, Julien Piet, Somayeh Sojoudi","Recent works have introduced input-convex neural networks (ICNNs) as learning
models with advantageous training, inference, and generalization properties
linked to their convex structure. In this paper, we propose a novel
feature-convex neural network architecture as the composition of an ICNN with a
Lipschitz feature map in order to achieve adversarial robustness. We consider
the asymmetric binary classification setting with one ""sensitive"" class, and
for this class we prove deterministic, closed-form, and easily-computable
certified robust radii for arbitrary $\ell_p$-norms. We theoretically justify
the use of these models by characterizing their decision region geometry,
extending the universal approximation theorem for ICNN regression to the
classification setting, and proving a lower bound on the probability that such
models perfectly fit even unstructured uniformly distributed data in
sufficiently high dimensions. Experiments on Malimg malware classification and
subsets of MNIST, Fashion-MNIST, and CIFAR-10 datasets show that feature-convex
classifiers attain state-of-the-art certified $\ell_1$-radii as well as
substantial $\ell_2$- and $\ell_{\infty}$-radii while being far more
computationally efficient than any competitive baseline.",2302.01961v2,https://arxiv.org/pdf/2302.01961v2
From Robustness to Privacy and Back,"Hilal Asi, Jonathan Ullman, Lydia Zakynthinou","We study the relationship between two desiderata of algorithms in statistical
inference and machine learning: differential privacy and robustness to
adversarial data corruptions. Their conceptual similarity was first observed by
Dwork and Lei (STOC 2009), who observed that private algorithms satisfy
robustness, and gave a general method for converting robust algorithms to
private ones. However, all general methods for transforming robust algorithms
into private ones lead to suboptimal error rates. Our work gives the first
black-box transformation that converts any adversarially robust algorithm into
one that satisfies pure differential privacy. Moreover, we show that for any
low-dimensional estimation task, applying our transformation to an optimal
robust estimator results in an optimal private estimator. Thus, we conclude
that for any low-dimensional task, the optimal error rate for
$\varepsilon$-differentially private estimators is essentially the same as the
optimal error rate for estimators that are robust to adversarially corrupting
$1/\varepsilon$ training samples. We apply our transformation to obtain new
optimal private estimators for several high-dimensional tasks, including
Gaussian (sparse) linear regression and PCA. Finally, we present an extension
of our transformation that leads to approximate differentially private
algorithms whose error does not depend on the range of the output space, which
is impossible under pure differential privacy.",2302.01855v1,https://arxiv.org/pdf/2302.01855v1
AIROGS: Artificial Intelligence for RObust Glaucoma Screening Challenge,"Coen de Vente, Koenraad A. Vermeer, Nicolas Jaccard, He Wang, Hongyi Sun, Firas Khader, Daniel Truhn, Temirgali Aimyshev, Yerkebulan Zhanibekuly, Tien-Dung Le, Adrian Galdran, Miguel Ángel González Ballester, Gustavo Carneiro, Devika R G, Hrishikesh P S, Densen Puthussery, Hong Liu, Zekang Yang, Satoshi Kondo, Satoshi Kasai, Edward Wang, Ashritha Durvasula, Jónathan Heras, Miguel Ángel Zapata, Teresa Araújo, Guilherme Aresta, Hrvoje Bogunović, Mustafa Arikan, Yeong Chan Lee, Hyun Bin Cho, Yoon Ho Choi, Abdul Qayyum, Imran Razzak, Bram van Ginneken, Hans G. Lemij, Clara I. Sánchez","The early detection of glaucoma is essential in preventing visual impairment.
Artificial intelligence (AI) can be used to analyze color fundus photographs
(CFPs) in a cost-effective manner, making glaucoma screening more accessible.
While AI models for glaucoma screening from CFPs have shown promising results
in laboratory settings, their performance decreases significantly in real-world
scenarios due to the presence of out-of-distribution and low-quality images. To
address this issue, we propose the Artificial Intelligence for Robust Glaucoma
Screening (AIROGS) challenge. This challenge includes a large dataset of around
113,000 images from about 60,000 patients and 500 different screening centers,
and encourages the development of algorithms that are robust to ungradable and
unexpected input data. We evaluated solutions from 14 teams in this paper, and
found that the best teams performed similarly to a set of 20 expert
ophthalmologists and optometrists. The highest-scoring team achieved an area
under the receiver operating characteristic curve of 0.99 (95% CI: 0.98-0.99)
for detecting ungradable images on-the-fly. Additionally, many of the
algorithms showed robust performance when tested on three other publicly
available datasets. These results demonstrate the feasibility of robust
AI-enabled glaucoma screening.",2302.01738v2,https://arxiv.org/pdf/2302.01738v2
"Revisiting Personalized Federated Learning: Robustness Against Backdoor
  Attacks","Zeyu Qin, Liuyi Yao, Daoyuan Chen, Yaliang Li, Bolin Ding, Minhao Cheng","In this work, besides improving prediction accuracy, we study whether
personalization could bring robustness benefits to backdoor attacks. We conduct
the first study of backdoor attacks in the pFL framework, testing 4 widely used
backdoor attacks against 6 pFL methods on benchmark datasets FEMNIST and
CIFAR-10, a total of 600 experiments. The study shows that pFL methods with
partial model-sharing can significantly boost robustness against backdoor
attacks. In contrast, pFL methods with full model-sharing do not show
robustness. To analyze the reasons for varying robustness performances, we
provide comprehensive ablation studies on different pFL methods. Based on our
findings, we further propose a lightweight defense method, Simple-Tuning, which
empirically improves defense performance against backdoor attacks. We believe
that our work could provide both guidance for pFL application in terms of its
robustness and offer valuable insights to design more robust FL methods in the
future. We open-source our code to establish the first benchmark for black-box
backdoor attacks in pFL:
https://github.com/alibaba/FederatedScope/tree/backdoor-bench.",2302.01677v2,https://arxiv.org/pdf/2302.01677v2
"Beyond the Universal Law of Robustness: Sharper Laws for Random Features
  and Neural Tangent Kernels","Simone Bombari, Shayan Kiyani, Marco Mondelli","Machine learning models are vulnerable to adversarial perturbations, and a
thought-provoking paper by Bubeck and Sellke has analyzed this phenomenon
through the lens of over-parameterization: interpolating smoothly the data
requires significantly more parameters than simply memorizing it. However, this
""universal"" law provides only a necessary condition for robustness, and it is
unable to discriminate between models. In this paper, we address these gaps by
focusing on empirical risk minimization in two prototypical settings, namely,
random features and the neural tangent kernel (NTK). We prove that, for random
features, the model is not robust for any degree of over-parameterization, even
when the necessary condition coming from the universal law of robustness is
satisfied. In contrast, for even activations, the NTK model meets the universal
lower bound, and it is robust as soon as the necessary condition on
over-parameterization is fulfilled. This also addresses a conjecture in prior
work by Bubeck, Li and Nagaraj. Our analysis decouples the effect of the kernel
of the model from an ""interaction matrix"", which describes the interaction with
the test data and captures the effect of the activation. Our theoretical
results are corroborated by numerical evidence on both synthetic and standard
datasets (MNIST, CIFAR-10).",2302.01629v2,https://arxiv.org/pdf/2302.01629v2
Robust Camera Pose Refinement for Multi-Resolution Hash Encoding,"Hwan Heo, Taekyung Kim, Jiyoung Lee, Jaewon Lee, Soohyun Kim, Hyunwoo J. Kim, Jin-Hwa Kim","Multi-resolution hash encoding has recently been proposed to reduce the
computational cost of neural renderings, such as NeRF. This method requires
accurate camera poses for the neural renderings of given scenes. However,
contrary to previous methods jointly optimizing camera poses and 3D scenes, the
naive gradient-based camera pose refinement method using multi-resolution hash
encoding severely deteriorates performance. We propose a joint optimization
algorithm to calibrate the camera pose and learn a geometric representation
using efficient multi-resolution hash encoding. Showing that the oscillating
gradient flows of hash encoding interfere with the registration of camera
poses, our method addresses the issue by utilizing smooth interpolation
weighting to stabilize the gradient oscillation for the ray samplings across
hash grids. Moreover, the curriculum training procedure helps to learn the
level-wise hash encoding, further increasing the pose refinement. Experiments
on the novel-view synthesis datasets validate that our learning frameworks
achieve state-of-the-art performance and rapid convergence of neural rendering,
even when initial camera poses are unknown.",2302.01571v1,https://arxiv.org/pdf/2302.01571v1
"Effective Robustness against Natural Distribution Shifts for Models with
  Different Training Data","Zhouxing Shi, Nicholas Carlini, Ananth Balashankar, Ludwig Schmidt, Cho-Jui Hsieh, Alex Beutel, Yao Qin","""Effective robustness"" measures the extra out-of-distribution (OOD)
robustness beyond what can be predicted from the in-distribution (ID)
performance. Existing effective robustness evaluations typically use a single
test set such as ImageNet to evaluate the ID accuracy. This becomes problematic
when evaluating models trained on different data distributions, e.g., comparing
models trained on ImageNet vs. zero-shot language-image pre-trained models
trained on LAION. In this paper, we propose a new evaluation metric to evaluate
and compare the effective robustness of models trained on different data. To do
this, we control for the accuracy on multiple ID test sets that cover the
training distributions for all the evaluated models. Our new evaluation metric
provides a better estimate of effective robustness when there are models with
different training data. It may also explain the surprising effective
robustness gains of zero-shot CLIP-like models exhibited in prior works that
used ImageNet as the only ID test set, while the gains diminish under our new
evaluation. Additional artifacts including interactive visualizations are
provided at https://shizhouxing.github.io/effective-robustness.",2302.01381v2,https://arxiv.org/pdf/2302.01381v2
On the Robustness of Randomized Ensembles to Adversarial Perturbations,"Hassan Dbouk, Naresh R. Shanbhag","Randomized ensemble classifiers (RECs), where one classifier is randomly
selected during inference, have emerged as an attractive alternative to
traditional ensembling methods for realizing adversarially robust classifiers
with limited compute requirements. However, recent works have shown that
existing methods for constructing RECs are more vulnerable than initially
claimed, casting major doubts on their efficacy and prompting fundamental
questions such as: ""When are RECs useful?"", ""What are their limits?"", and ""How
do we train them?"". In this work, we first demystify RECs as we derive
fundamental results regarding their theoretical limits, necessary and
sufficient conditions for them to be useful, and more. Leveraging this new
understanding, we propose a new boosting algorithm (BARRE) for training robust
RECs, and empirically demonstrate its effectiveness at defending against strong
$\ell_\infty$ norm-bounded adversaries across various network architectures and
datasets. Our code can be found at https://github.com/hsndbk4/BARRE.",2302.01375v3,https://arxiv.org/pdf/2302.01375v3
Robust Markov Decision Processes without Model Estimation,"Wenhao Yang, Han Wang, Tadashi Kozuno, Scott M. Jordan, Zhihua Zhang","Robust Markov Decision Processes (MDPs) are receiving much attention in
learning a robust policy which is less sensitive to environment changes. There
are an increasing number of works analyzing sample-efficiency of robust MDPs.
However, there are two major barriers to applying robust MDPs in practice.
First, most works study robust MDPs in a model-based regime, where the
transition probability needs to be estimated and requires a large amount of
memories $\mathcal{O}(|\mathcal{S}|^2|\mathcal{A}|)$. Second, prior work
typically assumes a strong oracle to obtain the optimal solution as an
intermediate step to solve robust MDPs. However, in practice, such an oracle
does not exist usually. To remove the oracle, we transform the original robust
MDPs into an alternative form, which allows us to use stochastic gradient
methods to solve the robust MDPs. Moreover, we prove the alternative form still
plays a similar role as the original form. With this new formulation, we devise
a sample-efficient algorithm to solve the robust MDPs in a model-free regime,
which does not require an oracle and trades off a lower storage requirement
$\mathcal{O}(|\mathcal{S}||\mathcal{A}|)$ with being able to generate samples
from a generative model or Markovian chain. Finally, we validate our
theoretical findings via numerical experiments, showing the efficiency with the
alternative form of robust MDPs.",2302.01248v2,https://arxiv.org/pdf/2302.01248v2
Robust Estimation under the Wasserstein Distance,"Sloan Nietert, Rachel Cummings, Ziv Goldfeld","We study the problem of robust distribution estimation under the Wasserstein
metric, a popular discrepancy measure between probability distributions rooted
in optimal transport (OT) theory. We introduce a new outlier-robust Wasserstein
distance $\mathsf{W}_p^\varepsilon$ which allows for $\varepsilon$ outlier mass
to be removed from its input distributions, and show that minimum distance
estimation under $\mathsf{W}_p^\varepsilon$ achieves minimax optimal robust
estimation risk. Our analysis is rooted in several new results for partial OT,
including an approximate triangle inequality, which may be of independent
interest. To address computational tractability, we derive a dual formulation
for $\mathsf{W}_p^\varepsilon$ that adds a simple penalty term to the classic
Kantorovich dual objective. As such, $\mathsf{W}_p^\varepsilon$ can be
implemented via an elementary modification to standard, duality-based OT
solvers. Our results are extended to sliced OT, where distributions are
projected onto low-dimensional subspaces, and applications to homogeneity and
independence testing are explored. We illustrate the virtues of our framework
via applications to generative modeling with contaminated datasets.",2302.01237v1,https://arxiv.org/pdf/2302.01237v1
Convolutional Neural Operators for robust and accurate learning of PDEs,"Bogdan Raonić, Roberto Molinaro, Tim De Ryck, Tobias Rohner, Francesca Bartolucci, Rima Alaifari, Siddhartha Mishra, Emmanuel de Bézenac","Although very successfully used in conventional machine learning, convolution
based neural network architectures -- believed to be inconsistent in function
space -- have been largely ignored in the context of learning solution
operators of PDEs. Here, we present novel adaptations for convolutional neural
networks to demonstrate that they are indeed able to process functions as
inputs and outputs. The resulting architecture, termed as convolutional neural
operators (CNOs), is designed specifically to preserve its underlying
continuous nature, even when implemented in a discretized form on a computer.
We prove a universality theorem to show that CNOs can approximate operators
arising in PDEs to desired accuracy. CNOs are tested on a novel suite of
benchmarks, encompassing a diverse set of PDEs with possibly multi-scale
solutions and are observed to significantly outperform baselines, paving the
way for an alternative framework for robust and accurate operator learning. Our
code is publicly available at
https://github.com/bogdanraonic3/ConvolutionalNeuralOperator",2302.01178v3,https://arxiv.org/pdf/2302.01178v3
"Vehicle Fault-Tolerant Robust Power Transmission Line Inspection
  Planning","František Nekovář, Jan Faigl, Martin Saska","This paper concerns fault-tolerant power transmission line inspection
planning as a generalization of the multiple traveling salesmen problem. The
addressed inspection planning problem is formulated as a single-depot
multiple-vehicle scenario, where the inspection vehicles are constrained by the
battery budget limiting their inspection time. The inspection vehicle is
assumed to be an autonomous multi-copter with a wide range of possible flight
speeds influencing battery consumption. The inspection plan is represented by
multiple routes for vehicles providing full coverage over inspection target
power lines. On an inspection vehicle mission interruption, which might happen
at any time during the execution of the inspection plan, the inspection is
re-planned using the remaining vehicles and their remaining battery budgets.
Robustness is introduced by choosing a suitable cost function for the initial
plan that maximizes the time window for successful re-planning. It enables the
remaining vehicles to successfully finish all the inspection targets using
their respective remaining battery budgets. A combinatorial metaheuristic
algorithm with various cost functions is used for planning and fast re-planning
during the inspection.",2302.01163v1,https://arxiv.org/pdf/2302.01163v1
RobustNeRF: Ignoring Distractors with Robust Losses,"Sara Sabour, Suhani Vora, Daniel Duckworth, Ivan Krasin, David J. Fleet, Andrea Tagliasacchi","Neural radiance fields (NeRF) excel at synthesizing new views given
multi-view, calibrated images of a static scene. When scenes include
distractors, which are not persistent during image capture (moving objects,
lighting variations, shadows), artifacts appear as view-dependent effects or
'floaters'. To cope with distractors, we advocate a form of robust estimation
for NeRF training, modeling distractors in training data as outliers of an
optimization problem. Our method successfully removes outliers from a scene and
improves upon our baselines, on synthetic and real-world scenes. Our technique
is simple to incorporate in modern NeRF frameworks, with few hyper-parameters.
It does not assume a priori knowledge of the types of distractors, and is
instead focused on the optimization problem rather than pre-processing or
modeling transient objects. More results on our page
https://robustnerf.github.io.",2302.00833v2,https://arxiv.org/pdf/2302.00833v2
"Model Monitoring and Robustness of In-Use Machine Learning Models:
  Quantifying Data Distribution Shifts Using Population Stability Index","Aria Khademi, Michael Hopka, Devesh Upadhyay","Safety goes first. Meeting and maintaining industry safety standards for
robustness of artificial intelligence (AI) and machine learning (ML) models
require continuous monitoring for faults and performance drops. Deep learning
models are widely used in industrial applications, e.g., computer vision, but
the susceptibility of their performance to environment changes (e.g., noise)
\emph{after deployment} on the product, are now well-known. A major challenge
is detecting data distribution shifts that happen, comparing the following:
{\bf (i)} development stage of AI and ML models, i.e., train/validation/test,
to {\bf (ii)} deployment stage on the product (i.e., even after `testing') in
the environment. We focus on a computer vision example related to autonomous
driving and aim at detecting shifts that occur as a result of adding noise to
images. We use the population stability index (PSI) as a measure of presence
and intensity of shift and present results of our empirical experiments showing
a promising potential for the PSI. We further discuss multiple aspects of model
monitoring and robustness that need to be analyzed \emph{simultaneously} to
achieve robustness for industry safety standards. We propose the need for and
the research direction toward \emph{categorizations} of problem classes and
examples where monitoring for robustness is required and present challenges and
pointers for future work from a \emph{practical} perspective.",2302.00775v1,https://arxiv.org/pdf/2302.00775v1
"Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous
  Unobserved Confounders","David Bruns-Smith, Angela Zhou","Offline reinforcement learning is important in domains such as medicine,
economics, and e-commerce where online experimentation is costly, dangerous or
unethical, and where the true model is unknown. However, most methods assume
all covariates used in the behavior policy's action decisions are observed.
Though this assumption, sequential ignorability/unconfoundedness, likely does
not hold in observational data, most of the data that accounts for selection
into treatment may be observed, motivating sensitivity analysis. We study
robust policy evaluation and policy optimization in the presence of
sequentially-exogenous unobserved confounders under a sensitivity model. We
propose and analyze orthogonalized robust fitted-Q-iteration that uses
closed-form solutions of the robust Bellman operator to derive a loss
minimization problem for the robust Q function, and adds a bias-correction to
quantile estimation. Our algorithm enjoys the computational ease of
fitted-Q-iteration and statistical improvements (reduced dependence on quantile
estimation error) from orthogonalization. We provide sample complexity bounds,
insights, and show effectiveness both in simulations and on real-world
longitudinal healthcare data of treating sepsis. In particular, our model of
sequential unobserved confounders yields an online Markov decision process,
rather than partially observed Markov decision process: we illustrate how this
can enable warm-starting optimistic reinforcement learning algorithms with
valid robust bounds from observational data.",2302.00662v2,https://arxiv.org/pdf/2302.00662v2
Robust online active learning,"Davide Cacciarelli, Murat Kulahci, John Sølve Tyssedal","In many industrial applications, obtaining labeled observations is not
straightforward as it often requires the intervention of human experts or the
use of expensive testing equipment. In these circumstances, active learning can
be highly beneficial in suggesting the most informative data points to be used
when fitting a model. Reducing the number of observations needed for model
development alleviates both the computational burden required for training and
the operational expenses related to labeling. Online active learning, in
particular, is useful in high-volume production processes where the decision
about the acquisition of the label for a data point needs to be taken within an
extremely short time frame. However, despite the recent efforts to develop
online active learning strategies, the behavior of these methods in the
presence of outliers has not been thoroughly examined. In this work, we
investigate the performance of online active linear regression in contaminated
data streams. Our study shows that the currently available query strategies are
prone to sample outliers, whose inclusion in the training set eventually
degrades the predictive performance of the models. To address this issue, we
propose a solution that bounds the search area of a conditional D-optimal
algorithm and uses a robust estimator. Our approach strikes a balance between
exploring unseen regions of the input space and protecting against outliers.
Through numerical simulations, we show that the proposed method is effective in
improving the performance of online active learning in the presence of
outliers, thus expanding the potential applications of this powerful tool.",2302.00422v6,https://arxiv.org/pdf/2302.00422v6
"The Impacts of Unanswerable Questions on the Robustness of Machine
  Reading Comprehension Models","Son Quoc Tran, Phong Nguyen-Thuan Do, Uyen Le, Matt Kretchmar","Pretrained language models have achieved super-human performances on many
Machine Reading Comprehension (MRC) benchmarks. Nevertheless, their relative
inability to defend against adversarial attacks has spurred skepticism about
their natural language understanding. In this paper, we ask whether training
with unanswerable questions in SQuAD 2.0 can help improve the robustness of MRC
models against adversarial attacks. To explore that question, we fine-tune
three state-of-the-art language models on either SQuAD 1.1 or SQuAD 2.0 and
then evaluate their robustness under adversarial attacks. Our experiments
reveal that current models fine-tuned on SQuAD 2.0 do not initially appear to
be any more robust than ones fine-tuned on SQuAD 1.1, yet they reveal a measure
of hidden robustness that can be leveraged to realize actual performance gains.
Furthermore, we find that the robustness of models fine-tuned on SQuAD 2.0
extends to additional out-of-domain datasets. Finally, we introduce a new
adversarial attack to reveal artifacts of SQuAD 2.0 that current MRC models are
learning.",2302.00094v1,https://arxiv.org/pdf/2302.00094v1
"Adaptive sparseness for correntropy-based robust regression via
  automatic relevance determination","Yuanhao Li, Badong Chen, Okito Yamashita, Natsue Yoshimura, Yasuharu Koike","Sparseness and robustness are two important properties for many machine
learning scenarios. In the present study, regarding the maximum correntropy
criterion (MCC) based robust regression algorithm, we investigate to integrate
the MCC method with the automatic relevance determination (ARD) technique in a
Bayesian framework, so that MCC-based robust regression could be implemented
with adaptive sparseness. To be specific, we use an inherent noise assumption
from the MCC to derive an explicit likelihood function, and realize the maximum
a posteriori (MAP) estimation with the ARD prior by variational Bayesian
inference. Compared to the existing robust and sparse L1-regularized MCC
regression, the proposed MCC-ARD regression can eradicate the troublesome
tuning for the regularization hyper-parameter which controls the regularization
strength. Further, MCC-ARD achieves superior prediction performance and feature
selection capability than L1-regularized MCC, as demonstrated by a noisy and
high-dimensional simulation study.",2302.00082v1,https://arxiv.org/pdf/2302.00082v1
Interpreting Robustness Proofs of Deep Neural Networks,"Debangshu Banerjee, Avaljot Singh, Gagandeep Singh","In recent years numerous methods have been developed to formally verify the
robustness of deep neural networks (DNNs). Though the proposed techniques are
effective in providing mathematical guarantees about the DNNs behavior, it is
not clear whether the proofs generated by these methods are
human-interpretable. In this paper, we bridge this gap by developing new
concepts, algorithms, and representations to generate human understandable
interpretations of the proofs. Leveraging the proposed method, we show that the
robustness proofs of standard DNNs rely on spurious input features, while the
proofs of DNNs trained to be provably robust filter out even the semantically
meaningful features. The proofs for the DNNs combining adversarial and provably
robust training are the most effective at selectively filtering out spurious
features as well as relying on human-understandable input features.",2301.13845v1,https://arxiv.org/pdf/2301.13845v1
Are Defenses for Graph Neural Networks Robust?,"Felix Mujkanovic, Simon Geisler, Stephan Günnemann, Aleksandar Bojchevski","A cursory reading of the literature suggests that we have made a lot of
progress in designing effective adversarial defenses for Graph Neural Networks
(GNNs). Yet, the standard methodology has a serious flaw - virtually all of the
defenses are evaluated against non-adaptive attacks leading to overly
optimistic robustness estimates. We perform a thorough robustness analysis of 7
of the most popular defenses spanning the entire spectrum of strategies, i.e.,
aimed at improving the graph, the architecture, or the training. The results
are sobering - most defenses show no or only marginal improvement compared to
an undefended baseline. We advocate using custom adaptive attacks as a gold
standard and we outline the lessons we learned from successfully designing such
attacks. Moreover, our diverse collection of perturbed graphs forms a
(black-box) unit test offering a first glance at a model's robustness.",2301.13694v1,https://arxiv.org/pdf/2301.13694v1
An Efficient Solution to s-Rectangular Robust Markov Decision Processes,"Navdeep Kumar, Kfir Levy, Kaixin Wang, Shie Mannor","We present an efficient robust value iteration for \texttt{s}-rectangular
robust Markov Decision Processes (MDPs) with a time complexity comparable to
standard (non-robust) MDPs which is significantly faster than any existing
method. We do so by deriving the optimal robust Bellman operator in concrete
forms using our $L_p$ water filling lemma. We unveil the exact form of the
optimal policies, which turn out to be novel threshold policies with the
probability of playing an action proportional to its advantage.",2301.13642v1,https://arxiv.org/pdf/2301.13642v1
Policy Gradient for Rectangular Robust Markov Decision Processes,"Navdeep Kumar, Esther Derman, Matthieu Geist, Kfir Levy, Shie Mannor","Policy gradient methods have become a standard for training reinforcement
learning agents in a scalable and efficient manner. However, they do not
account for transition uncertainty, whereas learning robust policies can be
computationally expensive. In this paper, we introduce robust policy gradient
(RPG), a policy-based method that efficiently solves rectangular robust Markov
decision processes (MDPs). We provide a closed-form expression for the worst
occupation measure. Incidentally, we find that the worst kernel is a rank-one
perturbation of the nominal. Combining the worst occupation measure with a
robust Q-value estimation yields an explicit form of the robust gradient. Our
resulting RPG can be estimated from data with the same time complexity as its
non-robust equivalent. Hence, it relieves the computational burden of convex
optimization problems required for training robust policies by current policy
gradient approaches.",2301.13589v2,https://arxiv.org/pdf/2301.13589v2
"Learning Against Distributional Uncertainty: On the Trade-off Between
  Robustness and Specificity","Shixiong Wang, Haowei Wang, Jean Honorio","Trustworthy machine learning aims at combating distributional uncertainties
in training data distributions compared to population distributions. Typical
treatment frameworks include the Bayesian approach, (min-max) distributionally
robust optimization (DRO), and regularization. However, two issues have to be
raised: 1) All these methods are biased estimators of the true optimal cost; 2)
the prior distribution in the Bayesian method, the radius of the distributional
ball in the DRO method, and the regularizer in the regularization method are
difficult to specify. This paper studies a new framework that unifies the three
approaches and that addresses the two challenges mentioned above. The
asymptotic properties (e.g., consistency and asymptotic normalities),
non-asymptotic properties (e.g., unbiasedness and generalization error bound),
and a Monte--Carlo-based solution method of the proposed model are studied. The
new model reveals the trade-off between the robustness to the unseen data and
the specificity to the training data.",2301.13565v1,https://arxiv.org/pdf/2301.13565v1
"Robust Linear Regression: Gradient-descent, Early-stopping, and Beyond","Meyer Scetbon, Elvis Dohmatob","In this work we study the robustness to adversarial attacks, of
early-stopping strategies on gradient-descent (GD) methods for linear
regression. More precisely, we show that early-stopped GD is optimally robust
(up to an absolute constant) against Euclidean-norm adversarial attacks.
However, we show that this strategy can be arbitrarily sub-optimal in the case
of general Mahalanobis attacks. This observation is compatible with recent
findings in the case of classification~\cite{Vardi2022GradientMP} that show
that GD provably converges to non-robust models. To alleviate this issue, we
propose to apply instead a GD scheme on a transformation of the data adapted to
the attack. This data transformation amounts to apply feature-depending
learning rates and we show that this modified GD is able to handle any
Mahalanobis attack, as well as more general attacks under some conditions.
Unfortunately, choosing such adapted transformations can be hard for general
attacks. To the rescue, we design a simple and tractable estimator whose
adversarial risk is optimal up to within a multiplicative constant of 1.1124 in
the population regime, and works for any norm.",2301.13486v1,https://arxiv.org/pdf/2301.13486v1
"Optimal Transport Perturbations for Safe Reinforcement Learning with
  Robustness Guarantees","James Queeney, Erhan Can Ozcan, Ioannis Ch. Paschalidis, Christos G. Cassandras","Robustness and safety are critical for the trustworthy deployment of deep
reinforcement learning. Real-world decision making applications require
algorithms that can guarantee robust performance and safety in the presence of
general environment disturbances, while making limited assumptions on the data
collection process during training. In order to accomplish this goal, we
introduce a safe reinforcement learning framework that incorporates robustness
through the use of an optimal transport cost uncertainty set. We provide an
efficient implementation based on applying Optimal Transport Perturbations to
construct worst-case virtual state transitions, which does not impact data
collection during training and does not require detailed simulator access. In
experiments on continuous control tasks with safety constraints, our approach
demonstrates robust performance while significantly improving safety at
deployment time compared to standard safe reinforcement learning.",2301.13375v2,https://arxiv.org/pdf/2301.13375v2
"Misspecification-robust Sequential Neural Likelihood for
  Simulation-based Inference","Ryan P. Kelly, David J. Nott, David T. Frazier, David J. Warne, Chris Drovandi","Simulation-based inference techniques are indispensable for parameter
estimation of mechanistic and simulable models with intractable likelihoods.
While traditional statistical approaches like approximate Bayesian computation
and Bayesian synthetic likelihood have been studied under well-specified and
misspecified settings, they often suffer from inefficiencies due to wasted
model simulations. Neural approaches, such as sequential neural likelihood
(SNL) avoid this wastage by utilising all model simulations to train a neural
surrogate for the likelihood function. However, the performance of SNL under
model misspecification is unreliable and can result in overconfident posteriors
centred around an inaccurate parameter estimate. In this paper, we propose a
novel SNL method, which through the incorporation of additional adjustment
parameters, is robust to model misspecification and capable of identifying
features of the data that the model is not able to recover. We demonstrate the
efficacy of our approach through several illustrative examples, where our
method gives more accurate point estimates and uncertainty quantification than
SNL.",2301.13368v2,https://arxiv.org/pdf/2301.13368v2
"RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers
  via Randomized Deletion","Zhuoqun Huang, Neil G. Marchant, Keane Lucas, Lujo Bauer, Olga Ohrimenko, Benjamin I. P. Rubinstein","Randomized smoothing is a leading approach for constructing classifiers that
are certifiably robust against adversarial examples. Existing work on
randomized smoothing has focused on classifiers with continuous inputs, such as
images, where $\ell_p$-norm bounded adversaries are commonly studied. However,
there has been limited work for classifiers with discrete or variable-size
inputs, such as for source code, which require different threat models and
smoothing mechanisms. In this work, we adapt randomized smoothing for discrete
sequence classifiers to provide certified robustness against edit
distance-bounded adversaries. Our proposed smoothing mechanism randomized
deletion (RS-Del) applies random deletion edits, which are (perhaps
surprisingly) sufficient to confer robustness against adversarial deletion,
insertion and substitution edits. Our proof of certification deviates from the
established Neyman-Pearson approach, which is intractable in our setting, and
is instead organized around longest common subsequences. We present a case
study on malware detection--a binary classification problem on byte sequences
where classifier evasion is a well-established threat model. When applied to
the popular MalConv malware detection model, our smoothing mechanism RS-Del
achieves a certified accuracy of 91% at an edit distance radius of 128 bytes.",2302.01757v3,https://arxiv.org/pdf/2302.01757v3
Near Optimal Private and Robust Linear Regression,"Xiyang Liu, Prateek Jain, Weihao Kong, Sewoong Oh, Arun Sai Suggala","We study the canonical statistical estimation problem of linear regression
from $n$ i.i.d.~examples under $(\varepsilon,\delta)$-differential privacy when
some response variables are adversarially corrupted. We propose a variant of
the popular differentially private stochastic gradient descent (DP-SGD)
algorithm with two innovations: a full-batch gradient descent to improve sample
complexity and a novel adaptive clipping to guarantee robustness. When there is
no adversarial corruption, this algorithm improves upon the existing
state-of-the-art approach and achieves a near optimal sample complexity. Under
label-corruption, this is the first efficient linear regression algorithm to
guarantee both $(\varepsilon,\delta)$-DP and robustness. Synthetic experiments
confirm the superiority of our approach.",2301.13273v1,https://arxiv.org/pdf/2301.13273v1
Robust empirical risk minimization via Newton's method,"Eirini Ioannou, Muni Sreenivas Pydi, Po-Ling Loh","A new variant of Newton's method for empirical risk minimization is studied,
where at each iteration of the optimization algorithm, the gradient and Hessian
of the objective function are replaced by robust estimators taken from existing
literature on robust mean estimation for multivariate data. After proving a
general theorem about the convergence of successive iterates to a small ball
around the population-level minimizer, consequences of the theory in
generalized linear models are studied when data are generated from Huber's
epsilon-contamination model and/or heavytailed distributions. An algorithm for
obtaining robust Newton directions based on the conjugate gradient method is
also proposed, which may be more appropriate for high-dimensional settings, and
conjectures about the convergence of the resulting algorithm are offered.
Compared to robust gradient descent, the proposed algorithm enjoys the faster
rates of convergence for successive iterates often achieved by second-order
algorithms for convex problems, i.e., quadratic convergence in a neighborhood
of the optimum, with a stepsize that may be chosen adaptively via backtracking
linesearch.",2301.13192v2,https://arxiv.org/pdf/2301.13192v2
"Towards Adversarial Realism and Robust Learning for IoT Intrusion
  Detection and Classification","João Vitorino, Isabel Praça, Eva Maia","The Internet of Things (IoT) faces tremendous security challenges. Machine
learning models can be used to tackle the growing number of cyber-attack
variations targeting IoT systems, but the increasing threat posed by
adversarial attacks restates the need for reliable defense strategies. This
work describes the types of constraints required for a realistic adversarial
cyber-attack example and proposes a methodology for a trustworthy adversarial
robustness analysis with a realistic adversarial evasion attack vector. The
proposed methodology was used to evaluate three supervised algorithms, Random
Forest (RF), Extreme Gradient Boosting (XGB), and Light Gradient Boosting
Machine (LGBM), and one unsupervised algorithm, Isolation Forest (IFOR).
Constrained adversarial examples were generated with the Adaptative
Perturbation Pattern Method (A2PM), and evasion attacks were performed against
models created with regular and adversarial training. Even though RF was the
least affected in binary classification, XGB consistently achieved the highest
accuracy in multi-class classification. The obtained results evidence the
inherent susceptibility of tree-based algorithms and ensembles to adversarial
evasion attacks and demonstrates the benefits of adversarial training and a
security by design approach for a more robust IoT network intrusion detection
and cyber-attack classification.",2301.13122v3,https://arxiv.org/pdf/2301.13122v3
Language-Driven Anchors for Zero-Shot Adversarial Robustness,"Xiao Li, Wei Zhang, Yining Liu, Zhanhao Hu, Bo Zhang, Xiaolin Hu","Deep Neural Networks (DNNs) are known to be susceptible to adversarial
attacks. Previous researches mainly focus on improving adversarial robustness
in the fully supervised setting, leaving the challenging domain of zero-shot
adversarial robustness an open question. In this work, we investigate this
domain by leveraging the recent advances in large vision-language models, such
as CLIP, to introduce zero-shot adversarial robustness to DNNs. We propose
LAAT, a Language-driven, Anchor-based Adversarial Training strategy. LAAT
utilizes the features of a text encoder for each category as fixed anchors
(normalized feature embeddings) for each category, which are then employed for
adversarial training. By leveraging the semantic consistency of the text
encoders, LAAT aims to enhance the adversarial robustness of the image model on
novel categories. However, naively using text encoders leads to poor results.
Through analysis, we identified the issue to be the high cosine similarity
between text encoders. We then design an expansion algorithm and an alignment
cross-entropy loss to alleviate the problem. Our experimental results
demonstrated that LAAT significantly improves zero-shot adversarial robustness
over state-of-the-art methods. LAAT has the potential to enhance adversarial
robustness by large-scale multimodal models, especially when labeled data is
unavailable during training.",2301.13096v3,https://arxiv.org/pdf/2301.13096v3
Benchmarking Robustness to Adversarial Image Obfuscations,"Florian Stimberg, Ayan Chakrabarti, Chun-Ta Lu, Hussein Hazimeh, Otilia Stretcu, Wei Qiao, Yintao Liu, Merve Kaya, Cyrus Rashtchian, Ariel Fuxman, Mehmet Tek, Sven Gowal","Automated content filtering and moderation is an important tool that allows
online platforms to build striving user communities that facilitate cooperation
and prevent abuse. Unfortunately, resourceful actors try to bypass automated
filters in a bid to post content that violate platform policies and codes of
conduct. To reach this goal, these malicious actors may obfuscate policy
violating images (e.g. overlay harmful images by carefully selected benign
images or visual patterns) to prevent machine learning models from reaching the
correct decision. In this paper, we invite researchers to tackle this specific
issue and present a new image benchmark. This benchmark, based on ImageNet,
simulates the type of obfuscations created by malicious actors. It goes beyond
ImageNet-$\textrm{C}$ and ImageNet-$\bar{\textrm{C}}$ by proposing general,
drastic, adversarial modifications that preserve the original content intent.
It aims to tackle a more common adversarial threat than the one considered by
$\ell_p$-norm bounded adversaries. We evaluate 33 pretrained models on the
benchmark and train models with different augmentations, architectures and
training methods on subsets of the obfuscations to measure generalization. We
hope this benchmark will encourage researchers to test their models and methods
and try to find new approaches that are more robust to these obfuscations.",2301.12993v2,https://arxiv.org/pdf/2301.12993v2
"ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion
  Probabilistic Models","Shengmeng Li, Luping Liu, Zenghao Chai, Runnan Li, Xu Tan","Though denoising diffusion probabilistic models (DDPMs) have achieved
remarkable generation results, the low sampling efficiency of DDPMs still
limits further applications. Since DDPMs can be formulated as diffusion
ordinary differential equations (ODEs), various fast sampling methods can be
derived from solving diffusion ODEs. However, we notice that previous sampling
methods with fixed analytical form are not robust with the error in the noise
estimated from pretrained diffusion models. In this work, we construct an
error-robust Adams solver (ERA-Solver), which utilizes the implicit Adams
numerical method that consists of a predictor and a corrector. Different from
the traditional predictor based on explicit Adams methods, we leverage a
Lagrange interpolation function as the predictor, which is further enhanced
with an error-robust strategy to adaptively select the Lagrange bases with
lower error in the estimated noise. Experiments on Cifar10, LSUN-Church, and
LSUN-Bedroom datasets demonstrate that our proposed ERA-Solver achieves 5.14,
9.42, and 9.69 Fenchel Inception Distance (FID) for image generation, with only
10 network evaluations.",2301.12935v3,https://arxiv.org/pdf/2301.12935v3
Identifying Adversarially Attackable and Robust Samples,"Vyas Raina, Mark Gales","Adversarial attacks insert small, imperceptible perturbations to input
samples that cause large, undesired changes to the output of deep learning
models. Despite extensive research on generating adversarial attacks and
building defense systems, there has been limited research on understanding
adversarial attacks from an input-data perspective. This work introduces the
notion of sample attackability, where we aim to identify samples that are most
susceptible to adversarial attacks (attackable samples) and conversely also
identify the least susceptible samples (robust samples). We propose a
deep-learning-based detector to identify the adversarially attackable and
robust samples in an unseen dataset for an unseen target model. Experiments on
standard image classification datasets enables us to assess the portability of
the deep attackability detector across a range of architectures. We find that
the deep attackability detector performs better than simple model
uncertainty-based measures for identifying the attackable/robust samples. This
suggests that uncertainty is an inadequate proxy for measuring sample distance
to a decision boundary. In addition to better understanding adversarial attack
theory, it is found that the ability to identify the adversarially attackable
and robust samples has implications for improving the efficiency of
sample-selection tasks.",2301.12896v3,https://arxiv.org/pdf/2301.12896v3
"Robust Attributed Graph Alignment via Joint Structure Learning and
  Optimal Transport","Jianheng Tang, Weiqi Zhang, Jiajin Li, Kangfei Zhao, Fugee Tsung, Jia Li","Graph alignment, which aims at identifying corresponding entities across
multiple networks, has been widely applied in various domains. As the graphs to
be aligned are usually constructed from different sources, the inconsistency
issues of structures and features between two graphs are ubiquitous in
real-world applications. Most existing methods follow the
``embed-then-cross-compare'' paradigm, which computes node embeddings in each
graph and then processes node correspondences based on cross-graph embedding
comparison. However, we find these methods are unstable and sub-optimal when
structure or feature inconsistency appears. To this end, we propose SLOTAlign,
an unsupervised graph alignment framework that jointly performs Structure
Learning and Optimal Transport Alignment. We convert graph alignment to an
optimal transport problem between two intra-graph matrices without the
requirement of cross-graph comparison. We further incorporate multi-view
structure learning to enhance graph representation power and reduce the effect
of structure and feature inconsistency inherited across graphs. Moreover, an
alternating scheme based algorithm has been developed to address the joint
optimization problem in SLOTAlign, and the provable convergence result is also
established. Finally, we conduct extensive experiments on six unsupervised
graph alignment datasets and the DBP15K knowledge graph (KG) alignment
benchmark dataset. The proposed SLOTAlign shows superior performance and
strongest robustness over seven unsupervised graph alignment methods and five
specialized KG alignment methods.",2301.12721v2,https://arxiv.org/pdf/2301.12721v2
Robust Meta Learning for Image based tasks,"Penghao Jiang, Xin Ke, ZiFeng Wang, Chunxi Li","A machine learning model that generalizes well should obtain low errors on
unseen test examples. Thus, if we learn an optimal model in training data, it
could have better generalization performance in testing tasks. However,
learning such a model is not possible in standard machine learning frameworks
as the distribution of the test data is unknown. To tackle this challenge, we
propose a novel robust meta-learning method, which is more robust to the
image-based testing tasks which is unknown and has distribution shifts with
training tasks. Our robust meta-learning method can provide robust optimal
models even when data from each distribution are scarce. In experiments, we
demonstrate that our algorithm not only has better generalization performance
but also robust to different unknown testing tasks.",2301.12698v2,https://arxiv.org/pdf/2301.12698v2
"Risk-Averse Model Uncertainty for Distributionally Robust Safe
  Reinforcement Learning","James Queeney, Mouhacine Benosman","Many real-world domains require safe decision making in uncertain
environments. In this work, we introduce a deep reinforcement learning
framework for approaching this important problem. We consider a distribution
over transition models, and apply a risk-averse perspective towards model
uncertainty through the use of coherent distortion risk measures. We provide
robustness guarantees for this framework by showing it is equivalent to a
specific class of distributionally robust safe reinforcement learning problems.
Unlike existing approaches to robustness in deep reinforcement learning,
however, our formulation does not involve minimax optimization. This leads to
an efficient, model-free implementation of our approach that only requires
standard data collection from a single training environment. In experiments on
continuous control tasks with safety constraints, we demonstrate that our
framework produces robust performance and safety at deployment time across a
range of perturbed test environments.",2301.12593v2,https://arxiv.org/pdf/2301.12593v2
"Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive
  Smoothing","Yatong Bai, Brendon G. Anderson, Aerin Kim, Somayeh Sojoudi","While prior research has proposed a plethora of methods that build neural
classifiers robust against adversarial robustness, practitioners are still
reluctant to adopt them due to their unacceptably severe clean accuracy
penalties. This paper significantly alleviates this accuracy-robustness
trade-off by mixing the output probabilities of a standard classifier and a
robust classifier, where the standard network is optimized for clean accuracy
and is not robust in general. We show that the robust base classifier's
confidence difference for correct and incorrect examples is the key to this
improvement. In addition to providing intuitions and empirical evidence, we
theoretically certify the robustness of the mixed classifier under realistic
assumptions. Furthermore, we adapt an adversarial input detector into a mixing
network that adaptively adjusts the mixture of the two base models, further
reducing the accuracy penalty of achieving robustness. The proposed flexible
method, termed ""adaptive smoothing"", can work in conjunction with existing or
even future methods that improve clean accuracy, robustness, or adversary
detection. Our empirical evaluation considers strong attack methods, including
AutoAttack and adaptive attack. On the CIFAR-100 dataset, our method achieves
an 85.21% clean accuracy while maintaining a 38.72% $\ell_\infty$-AutoAttacked
($\epsilon = 8/255$) accuracy, becoming the second most robust method on the
RobustBench CIFAR-100 benchmark as of submission, while improving the clean
accuracy by ten percentage points compared with all listed models. The code
that implements our method is available at
https://github.com/Bai-YT/AdaptiveSmoothing.",2301.12554v5,https://arxiv.org/pdf/2301.12554v5
Unlocking Deterministic Robustness Certification on ImageNet,"Kai Hu, Andy Zou, Zifan Wang, Klas Leino, Matt Fredrikson","Despite the promise of Lipschitz-based methods for provably-robust deep
learning with deterministic guarantees, current state-of-the-art results are
limited to feed-forward Convolutional Networks (ConvNets) on low-dimensional
data, such as CIFAR-10. This paper investigates strategies for expanding
certifiably robust training to larger, deeper models. A key challenge in
certifying deep networks is efficient calculation of the Lipschitz bound for
residual blocks found in ResNet and ViT architectures. We show that fast ways
of bounding the Lipschitz constant for conventional ResNets are loose, and show
how to address this by designing a new residual block, leading to the
\emph{Linear ResNet} (LiResNet) architecture. We then introduce \emph{Efficient
Margin MAximization} (EMMA), a loss function that stabilizes robust training by
simultaneously penalizing worst-case adversarial examples from \emph{all}
classes. Together, these contributions yield new \emph{state-of-the-art} robust
accuracy on CIFAR-10/100 and Tiny-ImageNet under $\ell_2$ perturbations.
Moreover, for the first time, we are able to scale up fast deterministic
robustness guarantees to ImageNet, demonstrating that this approach to robust
learning can be applied to real-world applications.
  We release our code on Github: \url{https://github.com/klasleino/gloro}.",2301.12549v3,https://arxiv.org/pdf/2301.12549v3
"Towards Verifying the Geometric Robustness of Large-scale Neural
  Networks","Fu Wang, Peipei Xu, Wenjie Ruan, Xiaowei Huang","Deep neural networks (DNNs) are known to be vulnerable to adversarial
geometric transformation. This paper aims to verify the robustness of
large-scale DNNs against the combination of multiple geometric transformations
with a provable guarantee. Given a set of transformations (e.g., rotation,
scaling, etc.), we develop GeoRobust, a black-box robustness analyser built
upon a novel global optimisation strategy, for locating the worst-case
combination of transformations that affect and even alter a network's output.
GeoRobust can provide provable guarantees on finding the worst-case combination
based on recent advances in Lipschitzian theory. Due to its black-box nature,
GeoRobust can be deployed on large-scale DNNs regardless of their
architectures, activation functions, and the number of neurons. In practice,
GeoRobust can locate the worst-case geometric transformation with high
precision for the ResNet50 model on ImageNet in a few seconds on average. We
examined 18 ImageNet classifiers, including the ResNet family and vision
transformers, and found a positive correlation between the geometric robustness
of the networks and the parameter numbers. We also observe that increasing the
depth of DNN is more beneficial than increasing its width in terms of improving
its geometric robustness. Our tool GeoRobust is available at
https://github.com/TrustAI/GeoRobust.",2301.12456v2,https://arxiv.org/pdf/2301.12456v2
"FedRC: Tackling Diverse Distribution Shifts Challenge in Federated
  Learning by Robust Clustering","Yongxin Guo, Xiaoying Tang, Tao Lin","Federated Learning (FL) is a machine learning paradigm that safeguards
privacy by retaining client data on edge devices. However, optimizing FL in
practice can be challenging due to the diverse and heterogeneous nature of the
learning system. Though recent research has focused on improving the
optimization of FL when distribution shifts occur among clients, ensuring
global performance when multiple types of distribution shifts occur
simultaneously among clients -- such as feature distribution shift, label
distribution shift, and concept shift -- remain under-explored. In this paper,
we identify the learning challenges posed by the simultaneous occurrence of
diverse distribution shifts and propose a clustering principle to overcome
these challenges. Through our research, we find that existing methods fail to
address the clustering principle. Therefore, we propose a novel clustering
algorithm framework, dubbed as FedRC, which adheres to our proposed clustering
principle by incorporating a bi-level optimization problem and a novel
objective function. Extensive experiments demonstrate that FedRC significantly
outperforms other SOTA cluster-based FL methods. Our code is available at
\url{https://github.com/LINs-lab/FedRC}.",2301.12379v4,https://arxiv.org/pdf/2301.12379v4
"Analyzing Robustness of the Deep Reinforcement Learning Algorithm in
  Ramp Metering Applications Considering False Data Injection Attack and
  Defense","Diyi Liu, Lanmin Liu, Lee D Han","Ramp metering is the act of controlling on-going vehicles to the highway
mainlines. Decades of practices of ramp metering have proved that ramp metering
can decrease total travel time, mitigate shockwaves, decrease rear-end
collisions by smoothing the traffic interweaving process, etc. Besides
traditional control algorithm like ALINEA, Deep Reinforcement Learning (DRL)
algorithms have been introduced to build a finer control. However, two
remaining challenges still hinder DRL from being implemented in the real world:
(1) some assumptions of algorithms are hard to be matched in the real world;
(2) the rich input states may make the model vulnerable to attacks and data
noises. To investigate these issues, we propose a Deep Q-Learning algorithm
using only loop detectors information as inputs in this study. Then, a set of
False Data Injection attacks and random noise attack are designed to
investigate the robustness of the model. The major benefit of the model is that
it can be applied to almost any ramp metering sites regardless of the road
geometries and layouts. Besides outcompeting the ALINEA method, the Deep
Q-Learning method also shows a good robustness through training among very
different demands and geometries. For example, during the testing case in I-24
near Murfreesboro, TN, the model shows its robustness as it still outperforms
ALINEA algorithm under Fast Gradient Sign Method attacks. Unlike many previous
studies, the model is trained and tested in completely different environments
to show the capabilities of the model.",2301.12036v2,https://arxiv.org/pdf/2301.12036v2
Alignment with human representations supports robust few-shot learning,"Ilia Sucholutsky, Thomas L. Griffiths","Should we care whether AI systems have representations of the world that are
similar to those of humans? We provide an information-theoretic analysis that
suggests that there should be a U-shaped relationship between the degree of
representational alignment with humans and performance on few-shot learning
tasks. We confirm this prediction empirically, finding such a relationship in
an analysis of the performance of 491 computer vision models. We also show that
highly-aligned models are more robust to both natural adversarial attacks and
domain shifts. Our results suggest that human-alignment is often a sufficient,
but not necessary, condition for models to make effective use of limited data,
be robust, and generalize well.",2301.11990v3,https://arxiv.org/pdf/2301.11990v3
"OccRob: Efficient SMT-Based Occlusion Robustness Verification of Deep
  Neural Networks","Xingwu Guo, Ziwei Zhou, Yueling Zhang, Guy Katz, Min Zhang","Occlusion is a prevalent and easily realizable semantic perturbation to deep
neural networks (DNNs). It can fool a DNN into misclassifying an input image by
occluding some segments, possibly resulting in severe errors. Therefore, DNNs
planted in safety-critical systems should be verified to be robust against
occlusions prior to deployment. However, most existing robustness verification
approaches for DNNs are focused on non-semantic perturbations and are not
suited to the occlusion case. In this paper, we propose the first efficient,
SMT-based approach for formally verifying the occlusion robustness of DNNs. We
formulate the occlusion robustness verification problem and prove it is
NP-complete. Then, we devise a novel approach for encoding occlusions as a part
of neural networks and introduce two acceleration techniques so that the
extended neural networks can be efficiently verified using off-the-shelf,
SMT-based neural network verification tools. We implement our approach in a
prototype called OccRob and extensively evaluate its performance on benchmark
datasets with various occlusion variants. The experimental results demonstrate
our approach's effectiveness and efficiency in verifying DNNs' robustness
against various occlusions, and its ability to generate counterexamples when
these DNNs are not robust.",2301.11912v1,https://arxiv.org/pdf/2301.11912v1
"Policy-Value Alignment and Robustness in Search-based Multi-Agent
  Learning","Niko A. Grupen, Michael Hanlon, Alexis Hao, Daniel D. Lee, Bart Selman","Large-scale AI systems that combine search and learning have reached
super-human levels of performance in game-playing, but have also been shown to
fail in surprising ways. The brittleness of such models limits their efficacy
and trustworthiness in real-world deployments. In this work, we systematically
study one such algorithm, AlphaZero, and identify two phenomena related to the
nature of exploration. First, we find evidence of policy-value misalignment --
for many states, AlphaZero's policy and value predictions contradict each
other, revealing a tension between accurate move-selection and value estimation
in AlphaZero's objective. Further, we find inconsistency within AlphaZero's
value function, which causes it to generalize poorly, despite its policy
playing an optimal strategy. From these insights we derive VISA-VIS: a novel
method that improves policy-value alignment and value robustness in AlphaZero.
Experimentally, we show that our method reduces policy-value misalignment by up
to 76%, reduces value generalization error by up to 50%, and reduces average
value error by up to 55%.",2301.11857v2,https://arxiv.org/pdf/2301.11857v2
Single-Trajectory Distributionally Robust Reinforcement Learning,"Zhipeng Liang, Xiaoteng Ma, Jose Blanchet, Jiheng Zhang, Zhengyuan Zhou","As a framework for sequential decision-making, Reinforcement Learning (RL)
has been regarded as an essential component leading to Artificial General
Intelligence (AGI). However, RL is often criticized for having the same
training environment as the test one, which also hinders its application in the
real world. To mitigate this problem, Distributionally Robust RL (DRRL) is
proposed to improve the worst performance in a set of environments that may
contain the unknown test environment. Due to the nonlinearity of the robustness
goal, most of the previous work resort to the model-based approach, learning
with either an empirical distribution learned from the data or a simulator that
can be sampled infinitely, which limits their applications in simple dynamics
environments. In contrast, we attempt to design a DRRL algorithm that can be
trained along a single trajectory, i.e., no repeated sampling from a state.
Based on the standard Q-learning, we propose distributionally robust Q-learning
with the single trajectory (DRQ) and its average-reward variant named
differential DRQ. We provide asymptotic convergence guarantees and experiments
for both settings, demonstrating their superiority in the perturbed
environments against the non-robust ones.",2301.11721v1,https://arxiv.org/pdf/2301.11721v1
Robust variance-regularized risk minimization with concomitant scaling,Matthew J. Holland,"Under losses which are potentially heavy-tailed, we consider the task of
minimizing sums of the loss mean and standard deviation, without trying to
accurately estimate the variance. By modifying a technique for variance-free
robust mean estimation to fit our problem setting, we derive a simple learning
procedure which can be easily combined with standard gradient-based solvers to
be used in traditional machine learning workflows. Empirically, we verify that
our proposed approach, despite its simplicity, performs as well or better than
even the best-performing candidates derived from alternative criteria such as
CVaR or DRO risks on a variety of datasets.",2301.11584v2,https://arxiv.org/pdf/2301.11584v2
"Certifiably Robust Reinforcement Learning through Model-Based Abstract
  Interpretation","Chenxi Yang, Greg Anderson, Swarat Chaudhuri","We present a reinforcement learning (RL) framework in which the learned
policy comes with a machine-checkable certificate of provable adversarial
robustness. Our approach, called CAROL, learns a model of the environment. In
each learning iteration, it uses the current version of this model and an
external abstract interpreter to construct a differentiable signal for provable
robustness. This signal is used to guide learning, and the abstract
interpretation used to construct it directly leads to the robustness
certificate returned at convergence. We give a theoretical analysis that bounds
the worst-case accumulative reward of CAROL. We also experimentally evaluate
CAROL on four MuJoCo environments with continuous state and action spaces. On
these tasks, CAROL learns policies that, when contrasted with policies from the
state-of-the-art robust RL algorithms, exhibit: (i) markedly enhanced certified
performance lower bounds; and (ii) comparable performance under empirical
adversarial attacks.",2301.11374v2,https://arxiv.org/pdf/2301.11374v2
"A Robust Optimisation Perspective on Counterexample-Guided Repair of
  Neural Networks","David Boetius, Stefan Leue, Tobias Sutter","Counterexample-guided repair aims at creating neural networks with
mathematical safety guarantees, facilitating the application of neural networks
in safety-critical domains. However, whether counterexample-guided repair is
guaranteed to terminate remains an open question. We approach this question by
showing that counterexample-guided repair can be viewed as a robust
optimisation algorithm. While termination guarantees for neural network repair
itself remain beyond our reach, we prove termination for more restrained
machine learning models and disprove termination in a general setting. We
empirically study the practical implications of our theoretical results,
demonstrating the suitability of common verifiers and falsifiers for repair
despite a disadvantageous theoretical result. Additionally, we use our
theoretical insights to devise a novel algorithm for repairing linear
regression models based on quadratic programming, surpassing existing
approaches.",2301.11342v2,https://arxiv.org/pdf/2301.11342v2
Certified Interpretability Robustness for Class Activation Mapping,"Alex Gu, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, Luca Daniel","Interpreting machine learning models is challenging but crucial for ensuring
the safety of deep networks in autonomous driving systems. Due to the
prevalence of deep learning based perception models in autonomous vehicles,
accurately interpreting their predictions is crucial. While a variety of such
methods have been proposed, most are shown to lack robustness. Yet, little has
been done to provide certificates for interpretability robustness. Taking a
step in this direction, we present CORGI, short for Certifiably prOvable
Robustness Guarantees for Interpretability mapping. CORGI is an algorithm that
takes in an input image and gives a certifiable lower bound for the robustness
of the top k pixels of its CAM interpretability map. We show the effectiveness
of CORGI via a case study on traffic sign data, certifying lower bounds on the
minimum adversarial perturbation not far from (4-5x) state-of-the-art attack
methods.",2301.11324v1,https://arxiv.org/pdf/2301.11324v1
"Robust One-Class Classification with Signed Distance Function using
  1-Lipschitz Neural Networks","Louis Bethune, Paul Novello, Thibaut Boissin, Guillaume Coiffier, Mathieu Serrurier, Quentin Vincenot, Andres Troya-Galvis","We propose a new method, dubbed One Class Signed Distance Function (OCSDF),
to perform One Class Classification (OCC) by provably learning the Signed
Distance Function (SDF) to the boundary of the support of any distribution. The
distance to the support can be interpreted as a normality score, and its
approximation using 1-Lipschitz neural networks provides robustness bounds
against $l2$ adversarial attacks, an under-explored weakness of deep
learning-based OCC algorithms. As a result, OCSDF comes with a new metric,
certified AUROC, that can be computed at the same cost as any classical AUROC.
We show that OCSDF is competitive against concurrent methods on tabular and
image data while being way more robust to adversarial attacks, illustrating its
theoretical properties. Finally, as exploratory research perspectives, we
theoretically and empirically show how OCSDF connects OCC with image generation
and implicit neural surface parametrization. Our code is available at
https://github.com/Algue-Rythme/OneClassMetricLearning",2303.01978v2,https://arxiv.org/pdf/2303.01978v2
"Train Hard, Fight Easy: Robust Meta Reinforcement Learning","Ido Greenberg, Shie Mannor, Gal Chechik, Eli Meirom","A major challenge of reinforcement learning (RL) in real-world applications
is the variation between environments, tasks or clients. Meta-RL (MRL)
addresses this issue by learning a meta-policy that adapts to new tasks.
Standard MRL methods optimize the average return over tasks, but often suffer
from poor results in tasks of high risk or difficulty. This limits system
reliability since test tasks are not known in advance. In this work, we define
a robust MRL objective with a controlled robustness level. Optimization of
analogous robust objectives in RL is known to lead to both *biased gradients*
and *data inefficiency*. We prove that the gradient bias disappears in our
proposed MRL framework. The data inefficiency is addressed via the novel Robust
Meta RL algorithm (RoML). RoML is a meta-algorithm that generates a robust
version of any given MRL algorithm, by identifying and over-sampling harder
tasks throughout training. We demonstrate that RoML achieves robust returns on
multiple navigation and continuous control benchmarks.",2301.11147v2,https://arxiv.org/pdf/2301.11147v2
Finding Regions of Counterfactual Explanations via Robust Optimization,"Donato Maragno, Jannis Kurtz, Tabea E. Röber, Rob Goedhart, Ş. Ilker Birbil, Dick den Hertog","Counterfactual explanations play an important role in detecting bias and
improving the explainability of data-driven classification models. A
counterfactual explanation (CE) is a minimal perturbed data point for which the
decision of the model changes. Most of the existing methods can only provide
one CE, which may not be achievable for the user. In this work we derive an
iterative method to calculate robust CEs, i.e. CEs that remain valid even after
the features are slightly perturbed. To this end, our method provides a whole
region of CEs allowing the user to choose a suitable recourse to obtain a
desired outcome. We use algorithmic ideas from robust optimization and prove
convergence results for the most common machine learning methods including
logistic regression, decision trees, random forests, and neural networks. Our
experiments show that our method can efficiently generate globally optimal
robust CEs for a variety of common data sets and classification models.",2301.11113v3,https://arxiv.org/pdf/2301.11113v3
"RobustPdM: Designing Robust Predictive Maintenance against Adversarial
  Attacks","Ayesha Siddique, Ripan Kumar Kundu, Gautam Raj Mode, Khaza Anuarul Hoque","The state-of-the-art predictive maintenance (PdM) techniques have shown great
success in reducing maintenance costs and downtime of complicated machines
while increasing overall productivity through extensive utilization of
Internet-of-Things (IoT) and Deep Learning (DL). Unfortunately, IoT sensors and
DL algorithms are both prone to cyber-attacks. For instance, DL algorithms are
known for their susceptibility to adversarial examples. Such adversarial
attacks are vastly under-explored in the PdM domain. This is because the
adversarial attacks in the computer vision domain for classification tasks
cannot be directly applied to the PdM domain for multivariate time series (MTS)
regression tasks. In this work, we propose an end-to-end methodology to design
adversarially robust PdM systems by extensively analyzing the effect of
different types of adversarial attacks and proposing a novel adversarial
defense technique for DL-enabled PdM models. First, we propose novel MTS
Projected Gradient Descent (PGD) and MTS PGD with random restarts (PGD_r)
attacks. Then, we evaluate the impact of MTS PGD and PGD_r along with MTS Fast
Gradient Sign Method (FGSM) and MTS Basic Iterative Method (BIM) on Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Convolutional Neural
Network (CNN), and Bi-directional LSTM based PdM system. Our results using
NASA's turbofan engine dataset show that adversarial attacks can cause a severe
defect (up to 11X) in the RUL prediction, outperforming the effectiveness of
the state-of-the-art PdM attacks by 3X. Furthermore, we present a novel
approximate adversarial training method to defend against adversarial attacks.
We observe that approximate adversarial training can significantly improve the
robustness of PdM models (up to 54X) and outperforms the state-of-the-art PdM
defense methods by offering 3X more robustness.",2301.10822v2,https://arxiv.org/pdf/2301.10822v2
On the Adversarial Robustness of Camera-based 3D Object Detection,"Shaoyuan Xie, Zichao Li, Zeyu Wang, Cihang Xie","In recent years, camera-based 3D object detection has gained widespread
attention for its ability to achieve high performance with low computational
cost. However, the robustness of these methods to adversarial attacks has not
been thoroughly examined, especially when considering their deployment in
safety-critical domains like autonomous driving. In this study, we conduct the
first comprehensive investigation of the robustness of leading camera-based 3D
object detection approaches under various adversarial conditions. We
systematically analyze the resilience of these models under two attack
settings: white-box and black-box; focusing on two primary objectives:
classification and localization. Additionally, we delve into two types of
adversarial attack techniques: pixel-based and patch-based. Our experiments
yield four interesting findings: (a) bird's-eye-view-based representations
exhibit stronger robustness against localization attacks; (b)
depth-estimation-free approaches have the potential to show stronger
robustness; (c) accurate depth estimation effectively improves robustness for
depth-estimation-based methods; (d) incorporating multi-frame benign inputs can
effectively mitigate adversarial attacks. We hope our findings can steer the
development of future camera-based object detection models with enhanced
adversarial robustness.",2301.10766v2,https://arxiv.org/pdf/2301.10766v2
Towards Robust Metrics for Concept Representation Evaluation,"Mateo Espinosa Zarlenga, Pietro Barbiero, Zohreh Shams, Dmitry Kazhdan, Umang Bhatt, Adrian Weller, Mateja Jamnik","Recent work on interpretability has focused on concept-based explanations,
where deep learning models are explained in terms of high-level units of
information, referred to as concepts. Concept learning models, however, have
been shown to be prone to encoding impurities in their representations, failing
to fully capture meaningful features of their inputs. While concept learning
lacks metrics to measure such phenomena, the field of disentanglement learning
has explored the related notion of underlying factors of variation in the data,
with plenty of metrics to measure the purity of such factors. In this paper, we
show that such metrics are not appropriate for concept learning and propose
novel metrics for evaluating the purity of concept representations in both
approaches. We show the advantage of these metrics over existing ones and
demonstrate their utility in evaluating the robustness of concept
representations and interventions performed on them. In addition, we show their
utility for benchmarking state-of-the-art methods from both families and find
that, contrary to common assumptions, supervision alone may not be sufficient
for pure concept representations.",2301.10367v1,https://arxiv.org/pdf/2301.10367v1
"Minimal Value-Equivalent Partial Models for Scalable and Robust Planning
  in Lifelong Reinforcement Learning","Safa Alver, Doina Precup","Learning models of the environment from pure interaction is often considered
an essential component of building lifelong reinforcement learning agents.
However, the common practice in model-based reinforcement learning is to learn
models that model every aspect of the agent's environment, regardless of
whether they are important in coming up with optimal decisions or not. In this
paper, we argue that such models are not particularly well-suited for
performing scalable and robust planning in lifelong reinforcement learning
scenarios and we propose new kinds of models that only model the relevant
aspects of the environment, which we call ""minimal value-equivalent partial
models"". After providing a formal definition for these models, we provide
theoretical results demonstrating the scalability advantages of performing
planning with such models and then perform experiments to empirically
illustrate our theoretical results. Then, we provide some useful heuristics on
how to learn these kinds of models with deep learning architectures and
empirically demonstrate that models learned in such a way can allow for
performing planning that is robust to distribution shifts and compounding model
errors. Overall, both our theoretical and empirical results suggest that
minimal value-equivalent partial models can provide significant benefits to
performing scalable and robust planning in lifelong reinforcement learning
scenarios.",2301.10119v2,https://arxiv.org/pdf/2301.10119v2
A Robust Hypothesis Test for Tree Ensemble Pruning,"Daniel de Marchi, Matthew Welch, Michael Kosorok","Gradient boosted decision trees are some of the most popular algorithms in
applied machine learning. They are a flexible and powerful tool that can
robustly fit to any tabular dataset in a scalable and computationally efficient
way. One of the most critical parameters to tune when fitting these models are
the various penalty terms used to distinguish signal from noise in the current
model. These penalties are effective in practice, but are lacking in robust
theoretical justifications. In this paper we develop and present a novel
theoretically justified hypothesis test of split quality for gradient boosted
tree ensembles and demonstrate that using this method instead of the common
penalty terms leads to a significant reduction in out of sample loss.
Additionally, this method provides a theoretically well-justified stopping
condition for the tree growing algorithm. We also present several innovative
extensions to the method, opening the door for a wide variety of novel tree
pruning algorithms.",2301.10115v2,https://arxiv.org/pdf/2301.10115v2
"Large Language Models as Fiduciaries: A Case Study Toward Robustly
  Communicating With Artificial Intelligence Through Legal Standards",John J. Nay,"Artificial Intelligence (AI) is taking on increasingly autonomous roles,
e.g., browsing the web as a research assistant and managing money. But
specifying goals and restrictions for AI behavior is difficult. Similar to how
parties to a legal contract cannot foresee every potential ""if-then""
contingency of their future relationship, we cannot specify desired AI behavior
for all circumstances. Legal standards facilitate robust communication of
inherently vague and underspecified goals. Instructions (in the case of
language models, ""prompts"") that employ legal standards will allow AI agents to
develop shared understandings of the spirit of a directive that generalize
expectations regarding acceptable actions to take in unspecified states of the
world. Standards have built-in context that is lacking from other goal
specification languages, such as plain language and programming languages.
Through an empirical study on thousands of evaluation labels we constructed
from U.S. court opinions, we demonstrate that large language models (LLMs) are
beginning to exhibit an ""understanding"" of one of the most relevant legal
standards for AI agents: fiduciary obligations. Performance comparisons across
models suggest that, as LLMs continue to exhibit improved core capabilities,
their legal standards understanding will also continue to improve. OpenAI's
latest LLM has 78% accuracy on our data, their previous release has 73%
accuracy, and a model from their 2020 GPT-3 paper has 27% accuracy (worse than
random). Our research is an initial step toward a framework for evaluating AI
understanding of legal standards more broadly, and for conducting reinforcement
learning with legal feedback (RLLF).",2301.10095v2,https://arxiv.org/pdf/2301.10095v2
Huber-Robust Confidence Sequences,"Hongjian Wang, Aaditya Ramdas","Confidence sequences are confidence intervals that can be sequentially
tracked, and are valid at arbitrary data-dependent stopping times. This paper
presents confidence sequences for a univariate mean of an unknown distribution
with a known upper bound on the $p$-th central moment ($p$ > 1), but allowing
for (at most) $\epsilon$ fraction of arbitrary distribution corruption, as in
Huber's contamination model. We do this by designing new robust exponential
supermartingales, and show that the resulting confidence sequences attain the
optimal width achieved in the nonsequential setting. Perhaps surprisingly, the
constant margin between our sequential result and the lower bound is smaller
than even fixed-time robust confidence intervals based on the trimmed mean, for
example. Since confidence sequences are a common tool used within A/B/n testing
and bandits, these results open the door to sequential experimentation that is
robust to outliers and adversarial corruptions.",2301.09573v2,https://arxiv.org/pdf/2301.09573v2
"Utilizing Domain Knowledge: Robust Machine Learning for Building Energy
  Prediction with Small, Inconsistent Datasets","Xia Chen, Manav Mahan Singh, Philipp Geyer","The demand for a huge amount of data for machine learning (ML) applications
is currently a bottleneck in an empirically dominated field. We propose a
method to combine prior knowledge with data-driven methods to significantly
reduce their data dependency. In this study, component-based machine learning
(CBML) as the knowledge-encoded data-driven method is examined in the context
of energy-efficient building engineering. It encodes the abstraction of
building structural knowledge as semantic information in the model
organization. We design a case experiment to understand the efficacy of
knowledge-encoded ML in sparse data input (1% - 0.0125% sampling rate). The
result reveals its three advanced features compared with pure ML methods: 1.
Significant improvement in the robustness of ML to extremely small-size and
inconsistent datasets; 2. Efficient data utilization from different entities'
record collections; 3. Characteristics of accepting incomplete data with high
interpretability and reduced training time. All these features provide a
promising path to alleviating the deployment bottleneck of data-intensive
methods and contribute to efficient real-world data usage. Moreover, four
necessary prerequisites are summarized in this study that ensures the target
scenario benefits by combining prior knowledge and ML generalization.",2302.10784v2,https://arxiv.org/pdf/2302.10784v2
"Statistically Optimal Robust Mean and Covariance Estimation for
  Anisotropic Gaussians","Arshak Minasyan, Nikita Zhivotovskiy","Assume that $X_{1}, \ldots, X_{N}$ is an $\varepsilon$-contaminated sample of
$N$ independent Gaussian vectors in $\mathbb{R}^d$ with mean $\mu$ and
covariance $\Sigma$. In the strong $\varepsilon$-contamination model we assume
that the adversary replaced an $\varepsilon$ fraction of vectors in the
original Gaussian sample by any other vectors. We show that there is an
estimator $\widehat \mu$ of the mean satisfying, with probability at least $1 -
\delta$, a bound of the form \[ \|\widehat{\mu} - \mu\|_2 \le
c\left(\sqrt{\frac{\operatorname{Tr}(\Sigma)}{N}} +
\sqrt{\frac{\|\Sigma\|\log(1/\delta)}{N}} +
\varepsilon\sqrt{\|\Sigma\|}\right), \] where $c > 0$ is an absolute constant
and $\|\Sigma\|$ denotes the operator norm of $\Sigma$. In the same
contaminated Gaussian setup, we construct an estimator $\widehat \Sigma$ of the
covariance matrix $\Sigma$ that satisfies, with probability at least $1 -
\delta$, \[ \left\|\widehat{\Sigma} - \Sigma\right\| \le
c\left(\sqrt{\frac{\|\Sigma\|\operatorname{Tr}(\Sigma)}{N}} +
\|\Sigma\|\sqrt{\frac{\log(1/\delta)}{N}} + \varepsilon\|\Sigma\|\right). \]
Both results are optimal up to multiplicative constant factors. Despite the
recent significant interest in robust statistics, achieving both dimension-free
bounds in the canonical Gaussian case remained open. In fact, several
previously known results were either dimension-dependent and required $\Sigma$
to be close to identity, or had a sub-optimal dependence on the contamination
level $\varepsilon$.
  As a part of the analysis, we derive sharp concentration inequalities for
central order statistics of Gaussian, folded normal, and chi-squared
distributions.",2301.09024v1,https://arxiv.org/pdf/2301.09024v1
"Limitations of Piecewise Linearity for Efficient Robustness
  Certification",Klas Leino,"Certified defenses against small-norm adversarial examples have received
growing attention in recent years; though certified accuracies of
state-of-the-art methods remain far below their non-robust counterparts,
despite the fact that benchmark datasets have been shown to be well-separated
at far larger radii than the literature generally attempts to certify. In this
work, we offer insights that identify potential factors in this performance
gap. Specifically, our analysis reveals that piecewise linearity imposes
fundamental limitations on the tightness of leading certification techniques.
These limitations are felt in practical terms as a greater need for capacity in
models hoped to be certified efficiently. Moreover, this is in addition to the
capacity necessary to learn a robust boundary, studied in prior work. However,
we argue that addressing the limitations of piecewise linearity through scaling
up model capacity may give rise to potential difficulties -- particularly
regarding robust generalization -- therefore, we conclude by suggesting that
developing smooth activation functions may be the way forward for advancing the
performance of certified neural networks.",2301.08842v1,https://arxiv.org/pdf/2301.08842v1
"RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge
  Distillation","Utkarsh Nath, Yancheng Wang, Yingzhen Yang","Deep Neural Networks are vulnerable to adversarial attacks. Neural
Architecture Search (NAS), one of the driving tools of deep neural networks,
demonstrates superior performance in prediction accuracy in various machine
learning applications. However, it is unclear how it performs against
adversarial attacks. Given the presence of a robust teacher, it would be
interesting to investigate if NAS would produce robust neural architecture by
inheriting robustness from the teacher. In this paper, we propose Robust Neural
Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL), a novel
NAS algorithm that improves the robustness of NAS by learning from a robust
teacher through cross-layer knowledge distillation. Unlike previous knowledge
distillation methods that encourage close student/teacher output only in the
last layer, RNAS-CL automatically searches for the best teacher layer to
supervise each student layer. Experimental result evidences the effectiveness
of RNAS-CL and shows that RNAS-CL produces small and robust neural
architecture.",2301.08092v1,https://arxiv.org/pdf/2301.08092v1
Improve Noise Tolerance of Robust Loss via Noise-Awareness,"Kehui Ding, Jun Shu, Deyu Meng, Zongben Xu","Robust loss minimization is an important strategy for handling robust
learning issue on noisy labels. Current approaches for designing robust losses
involve the introduction of noise-robust factors, i.e., hyperparameters, to
control the trade-off between noise robustness and learnability. However,
finding suitable hyperparameters for different datasets with noisy labels is a
challenging and time-consuming task. Moreover, existing robust loss methods
usually assume that all training samples share common hyperparameters, which
are independent of instances. This limits the ability of these methods to
distinguish the individual noise properties of different samples and overlooks
the varying contributions of diverse training samples in helping models
understand underlying patterns. To address above issues, we propose to assemble
robust loss with instance-dependent hyperparameters to improve their noise
tolerance with theoretical guarantee. To achieve setting such
instance-dependent hyperparameters for robust loss, we propose a meta-learning
method which is capable of adaptively learning a hyperparameter prediction
function, called Noise-Aware-Robust-Loss-Adjuster (NARL-Adjuster for brevity).
Through mutual amelioration between hyperparameter prediction function and
classifier parameters in our method, both of them can be simultaneously finely
ameliorated and coordinated to attain solutions with good generalization
capability. Four SOTA robust loss functions are attempted to be integrated with
our algorithm, and comprehensive experiments substantiate the general
availability and effectiveness of the proposed method in both its noise
tolerance and performance.",2301.07306v2,https://arxiv.org/pdf/2301.07306v2
Robust Scheduling with GFlowNets,"David W. Zhang, Corrado Rainone, Markus Peschl, Roberto Bondesan","Finding the best way to schedule operations in a computation graph is a
classical NP-hard problem which is central to compiler optimization. However,
evaluating the goodness of a schedule on the target hardware can be very
time-consuming. Traditional approaches as well as previous machine learning
ones typically optimize proxy metrics, which are fast to evaluate but can lead
to bad schedules when tested on the target hardware. In this work, we propose a
new approach to scheduling by sampling proportionally to the proxy metric using
a novel GFlowNet method. We introduce a technique to control the trade-off
between diversity and goodness of the proposed schedules at inference time and
demonstrate empirically that the pure optimization baselines can lead to subpar
performance with respect to our approach when tested on a target model.
Furthermore, we show that conditioning the GFlowNet on the computation graph
enables generalization to unseen scheduling problems for both synthetic and
real-world compiler datasets.",2302.05446v2,https://arxiv.org/pdf/2302.05446v2
"Adversarial Robust Deep Reinforcement Learning Requires Redefining
  Robustness",Ezgi Korkmaz,"Learning from raw high dimensional data via interaction with a given
environment has been effectively achieved through the utilization of deep
neural networks. Yet the observed degradation in policy performance caused by
imperceptible worst-case policy dependent translations along high sensitivity
directions (i.e. adversarial perturbations) raises concerns on the robustness
of deep reinforcement learning policies. In our paper, we show that these high
sensitivity directions do not lie only along particular worst-case directions,
but rather are more abundant in the deep neural policy landscape and can be
found via more natural means in a black-box setting. Furthermore, we show that
vanilla training techniques intriguingly result in learning more robust
policies compared to the policies learnt via the state-of-the-art adversarial
training techniques. We believe our work lays out intriguing properties of the
deep reinforcement learning policy manifold and our results can help to build
robust and generalizable deep reinforcement learning policies.",2301.07487v1,https://arxiv.org/pdf/2301.07487v1
"Syntactically Robust Training on Partially-Observed Data for Open
  Information Extraction","Ji Qi, Yuxiang Chen, Lei Hou, Juanzi Li, Bin Xu","Open Information Extraction models have shown promising results with
sufficient supervision. However, these models face a fundamental challenge that
the syntactic distribution of training data is partially observable in
comparison to the real world. In this paper, we propose a syntactically robust
training framework that enables models to be trained on a syntactic-abundant
distribution based on diverse paraphrase generation. To tackle the intrinsic
problem of knowledge deformation of paraphrasing, two algorithms based on
semantic similarity matching and syntactic tree walking are used to restore the
expressionally transformed knowledge. The training framework can be generally
applied to other syntactic partial observable domains. Based on the proposed
framework, we build a new evaluation set called CaRB-AutoPara, a syntactically
diverse dataset consistent with the real-world setting for validating the
robustness of the models. Experiments including a thorough analysis show that
the performance of the model degrades with the increase of the difference in
syntactic distribution, while our framework gives a robust boundary. The source
code is publicly available at https://github.com/qijimrc/RobustOIE.",2301.06841v1,https://arxiv.org/pdf/2301.06841v1
"Async-HFL: Efficient and Robust Asynchronous Federated Learning in
  Hierarchical IoT Networks","Xiaofan Yu, Ludmila Cherkasova, Harsh Vardhan, Quanling Zhao, Emily Ekaireb, Xiyuan Zhang, Arya Mazumdar, Tajana Rosing","Federated Learning (FL) has gained increasing interest in recent years as a
distributed on-device learning paradigm. However, multiple challenges remain to
be addressed for deploying FL in real-world Internet-of-Things (IoT) networks
with hierarchies. Although existing works have proposed various approaches to
account data heterogeneity, system heterogeneity, unexpected stragglers and
scalibility, none of them provides a systematic solution to address all of the
challenges in a hierarchical and unreliable IoT network. In this paper, we
propose an asynchronous and hierarchical framework (Async-HFL) for performing
FL in a common three-tier IoT network architecture. In response to the largely
varied delays, Async-HFL employs asynchronous aggregations at both the gateway
and the cloud levels thus avoids long waiting time. To fully unleash the
potential of Async-HFL in converging speed under system heterogeneities and
stragglers, we design device selection at the gateway level and device-gateway
association at the cloud level. Device selection chooses edge devices to
trigger local training in real-time while device-gateway association determines
the network topology periodically after several cloud epochs, both satisfying
bandwidth limitation. We evaluate Async-HFL's convergence speedup using
large-scale simulations based on ns-3 and a network topology from NYCMesh. Our
results show that Async-HFL converges 1.08-1.31x faster in wall-clock time and
saves up to 21.6% total communication cost compared to state-of-the-art
asynchronous FL algorithms (with client selection). We further validate
Async-HFL on a physical deployment and observe robust convergence under
unexpected stragglers.",2301.06646v4,https://arxiv.org/pdf/2301.06646v4
"$β$-DARTS++: Bi-level Regularization for Proxy-robust Differentiable
  Architecture Search","Peng Ye, Tong He, Baopu Li, Tao Chen, Lei Bai, Wanli Ouyang","Neural Architecture Search has attracted increasing attention in recent
years. Among them, differential NAS approaches such as DARTS, have gained
popularity for the search efficiency. However, they still suffer from three
main issues, that are, the weak stability due to the performance collapse, the
poor generalization ability of the searched architectures, and the inferior
robustness to different kinds of proxies. To solve the stability and
generalization problems, a simple-but-effective regularization method, termed
as Beta-Decay, is proposed to regularize the DARTS-based NAS searching process
(i.e., $\beta$-DARTS). Specifically, Beta-Decay regularization can impose
constraints to keep the value and variance of activated architecture parameters
from being too large, thereby ensuring fair competition among architecture
parameters and making the supernet less sensitive to the impact of input on the
operation set. In-depth theoretical analyses on how it works and why it works
are provided. Comprehensive experiments validate that Beta-Decay regularization
can help to stabilize the searching process and makes the searched network more
transferable across different datasets. To address the robustness problem, we
first benchmark different NAS methods under a wide range of proxy data, proxy
channels, proxy layers and proxy epochs, since the robustness of NAS under
different kinds of proxies has not been explored before. We then conclude some
interesting findings and find that $\beta$-DARTS always achieves the best
result among all compared NAS methods under almost all proxies. We further
introduce the novel flooding regularization to the weight optimization of
$\beta$-DARTS (i.e., Bi-level regularization), and experimentally and
theoretically verify its effectiveness for improving the proxy robustness of
differentiable NAS.",2301.06393v1,https://arxiv.org/pdf/2301.06393v1
"A Robust Classification Framework for Byzantine-Resilient Stochastic
  Gradient Descent","Shashank Reddy Chirra, Kalyan Varma Nadimpalli, Shrisha Rao","This paper proposes a Robust Gradient Classification Framework (RGCF) for
Byzantine fault tolerance in distributed stochastic gradient descent. The
framework consists of a pattern recognition filter which we train to be able to
classify individual gradients as Byzantine by using their direction alone. This
filter is robust to an arbitrary number of Byzantine workers for convex as well
as non-convex optimisation settings, which is a significant improvement on the
prior work that is robust to Byzantine faults only when up to 50% of the
workers are Byzantine. This solution does not require an estimate of the number
of Byzantine workers; its running time is not dependent on the number of
workers and can scale up to training instances with a large number of workers
without a loss in performance. We validate our solution by training
convolutional neural networks on the MNIST dataset in the presence of Byzantine
workers.",2301.07498v1,https://arxiv.org/pdf/2301.07498v1
Doubly Robust Counterfactual Classification,"Kwangho Kim, Edward H. Kennedy, José R. Zubizarreta","We study counterfactual classification as a new tool for decision-making
under hypothetical (contrary to fact) scenarios. We propose a doubly-robust
nonparametric estimator for a general counterfactual classifier, where we can
incorporate flexible constraints by casting the classification problem as a
nonlinear mathematical program involving counterfactuals. We go on to analyze
the rates of convergence of the estimator and provide a closed-form expression
for its asymptotic distribution. Our analysis shows that the proposed estimator
is robust against nuisance model misspecification, and can attain fast
$\sqrt{n}$ rates with tractable inference even when using nonparametric machine
learning approaches. We study the empirical performance of our methods by
simulation and apply them for recidivism risk prediction.",2301.06199v1,https://arxiv.org/pdf/2301.06199v1
"Enabling Astronaut Self-Scheduling using a Robust Advanced Modelling and
  Scheduling system: an assessment during a Mars analogue mission","Michael Saint-Guillain, Jean Vanderdonckt, Nicolas Burny, Vladimir Pletser, Tiago Vaquero, Steve Chien, Alexander Karl, Jessica Marquez, John Karasinski, Cyril Wain, Audrey Comein, Ignacio S. Casla, Jean Jacobs, Julien Meert, Cheyenne Chamart, Sirga Drouet, Julie Manon","Human long duration exploration missions (LDEMs) raise a number of
technological challenges. This paper addresses the question of the crew
autonomy: as the distances increase, the communication delays and constraints
tend to prevent the astronauts from being monitored and supported by a real
time ground control. Eventually, future planetary missions will necessarily
require a form of astronaut self-scheduling. We study the usage of a computer
decision-support tool by a crew of analog astronauts, during a Mars simulation
mission conducted at the Mars Desert Research Station (MDRS, Mars Society) in
Utah. The proposed tool, called Romie, belongs to the new category of Robust
Advanced Modelling and Scheduling (RAMS) systems. It allows the crew members
(i) to visually model their scientific objectives and constraints, (ii) to
compute near-optimal operational schedules while taking uncertainty into
account, (iii) to monitor the execution of past and current activities, and
(iv) to modify scientific objectives/constraints w.r.t. unforeseen events and
opportunistic science. In this study, we empirically measure how the
astronauts, who are novice planners, perform at using such a tool when
self-scheduling under the realistic assumptions of a simulated Martian
planetary habitat.",2301.08248v1,https://arxiv.org/pdf/2301.08248v1
Robust Bayesian Target Value Optimization,"Johannes G. Hoffer, Sascha Ranftl, Bernhard C. Geiger","We consider the problem of finding an input to a stochastic black box
function such that the scalar output of the black box function is as close as
possible to a target value in the sense of the expected squared error. While
the optimization of stochastic black boxes is classic in (robust) Bayesian
optimization, the current approaches based on Gaussian processes predominantly
focus either on i) maximization/minimization rather than target value
optimization or ii) on the expectation, but not the variance of the output,
ignoring output variations due to stochasticity in uncontrollable environmental
variables. In this work, we fill this gap and derive acquisition functions for
common criteria such as the expected improvement, the probability of
improvement, and the lower confidence bound, assuming that aleatoric effects
are Gaussian with known variance. Our experiments illustrate that this setting
is compatible with certain extensions of Gaussian processes, and show that the
thus derived acquisition functions can outperform classical Bayesian
optimization even if the latter assumptions are violated. An industrial use
case in billet forging is presented.",2301.04344v1,https://arxiv.org/pdf/2301.04344v1
On the Robustness of AlphaFold: A COVID-19 Case Study,"Ismail Alkhouri, Sumit Jha, Andre Beckus, George Atia, Alvaro Velasquez, Rickard Ewetz, Arvind Ramanathan, Susmit Jha","Protein folding neural networks (PFNNs) such as AlphaFold predict remarkably
accurate structures of proteins compared to other approaches. However, the
robustness of such networks has heretofore not been explored. This is
particularly relevant given the broad social implications of such technologies
and the fact that biologically small perturbations in the protein sequence do
not generally lead to drastic changes in the protein structure. In this paper,
we demonstrate that AlphaFold does not exhibit such robustness despite its high
accuracy. This raises the challenge of detecting and quantifying the extent to
which these predicted protein structures can be trusted. To measure the
robustness of the predicted structures, we utilize (i) the root-mean-square
deviation (RMSD) and (ii) the Global Distance Test (GDT) similarity measure
between the predicted structure of the original sequence and the structure of
its adversarially perturbed version. We prove that the problem of minimally
perturbing protein sequences to fool protein folding neural networks is
NP-complete. Based on the well-established BLOSUM62 sequence alignment scoring
matrix, we generate adversarial protein sequences and show that the RMSD
between the predicted protein structure and the structure of the original
sequence are very large when the adversarial changes are bounded by (i) 20
units in the BLOSUM62 distance, and (ii) five residues (out of hundreds or
thousands of residues) in the given protein sequence. In our experimental
evaluation, we consider 111 COVID-19 proteins in the Universal Protein resource
(UniProt), a central resource for protein data managed by the European
Bioinformatics Institute, Swiss Institute of Bioinformatics, and the US Protein
Information Resource. These result in an overall GDT similarity test score
average of around 34%, demonstrating a substantial drop in the performance of
AlphaFold.",2301.04093v2,https://arxiv.org/pdf/2301.04093v2
"On the Susceptibility and Robustness of Time Series Models through
  Adversarial Attack and Defense","Asadullah Hill Galib, Bidhan Bashyal","Under adversarial attacks, time series regression and classification are
vulnerable. Adversarial defense, on the other hand, can make the models more
resilient. It is important to evaluate how vulnerable different time series
models are to attacks and how well they recover using defense. The sensitivity
to various attacks and the robustness using the defense of several time series
models are investigated in this study. Experiments are run on seven-time series
models with three adversarial attacks and one adversarial defense. According to
the findings, all models, particularly GRU and RNN, appear to be vulnerable.
LSTM and GRU also have better defense recovery. FGSM exceeds the competitors in
terms of attacks. PGD attacks are more difficult to recover from than other
sorts of attacks.",2301.03703v1,https://arxiv.org/pdf/2301.03703v1
"On adversarial robustness and the use of Wasserstein ascent-descent
  dynamics to enforce it","Camilo Garcia Trillos, Nicolas Garcia Trillos","We propose iterative algorithms to solve adversarial problems in a variety of
supervised learning settings of interest. Our algorithms, which can be
interpreted as suitable ascent-descent dynamics in Wasserstein spaces, take the
form of a system of interacting particles. These interacting particle dynamics
are shown to converge toward appropriate mean-field limit equations in certain
large number of particles regimes. In turn, we prove that, under certain
regularity assumptions, these mean-field equations converge, in the large time
limit, toward approximate Nash equilibria of the original adversarial learning
problems. We present results for nonconvex-nonconcave settings, as well as for
nonconvex-concave ones. Numerical experiments illustrate our results.",2301.03662v1,https://arxiv.org/pdf/2301.03662v1
"A Robust Multilabel Method Integrating Rule-based Transparent Model,
  Soft Label Correlation Learning and Label Noise Resistance","Qiongdan Lou, Zhaohong Deng, Kup-Sze Choi, Shitong Wang","Model transparency, label correlation learning and the robust-ness to label
noise are crucial for multilabel learning. However, few existing methods study
these three characteristics simultaneously. To address this challenge, we
propose the robust multilabel Takagi-Sugeno-Kang fuzzy system (R-MLTSK-FS) with
three mechanisms. First, we design a soft label learning mechanism to reduce
the effect of label noise by explicitly measuring the interactions between
labels, which is also the basis of the other two mechanisms. Second, the
rule-based TSK FS is used as the base model to efficiently model the inference
relationship be-tween features and soft labels in a more transparent way than
many existing multilabel models. Third, to further improve the performance of
multilabel learning, we build a correlation enhancement learning mechanism
based on the soft label space and the fuzzy feature space. Extensive
experiments are conducted to demonstrate the superiority of the proposed
method.",2301.03283v3,https://arxiv.org/pdf/2301.03283v3
RobArch: Designing Robust Architectures against Adversarial Attacks,"ShengYun Peng, Weilin Xu, Cory Cornelius, Kevin Li, Rahul Duggal, Duen Horng Chau, Jason Martin","Adversarial Training is the most effective approach for improving the
robustness of Deep Neural Networks (DNNs). However, compared to the large body
of research in optimizing the adversarial training process, there are few
investigations into how architecture components affect robustness, and they
rarely constrain model capacity. Thus, it is unclear where robustness precisely
comes from. In this work, we present the first large-scale systematic study on
the robustness of DNN architecture components under fixed parameter budgets.
Through our investigation, we distill 18 actionable robust network design
guidelines that empower model developers to gain deep insights. We demonstrate
these guidelines' effectiveness by introducing the novel Robust Architecture
(RobArch) model that instantiates the guidelines to build a family of
top-performing models across parameter capacities against strong adversarial
attacks. RobArch achieves the new state-of-the-art AutoAttack accuracy on the
RobustBench ImageNet leaderboard. The code is available at
$\href{https://github.com/ShengYun-Peng/RobArch}{\text{this url}}$.",2301.03110v1,https://arxiv.org/pdf/2301.03110v1
AI Maintenance: A Robustness Perspective,"Pin-Yu Chen, Payel Das","With the advancements in machine learning (ML) methods and compute resources,
artificial intelligence (AI) empowered systems are becoming a prevailing
technology. However, current AI technology such as deep learning is not
flawless. The significantly increased model complexity and data scale incur
intensified challenges when lacking trustworthiness and transparency, which
could create new risks and negative impacts. In this paper, we carve out AI
maintenance from the robustness perspective. We start by introducing some
highlighted robustness challenges in the AI lifecycle and motivating AI
maintenance by making analogies to car maintenance. We then propose an AI model
inspection framework to detect and mitigate robustness risks. We also draw
inspiration from vehicle autonomy to define the levels of AI robustness
automation. Our proposal for AI maintenance facilitates robustness assessment,
status tracking, risk scanning, model hardening, and regulation throughout the
AI lifecycle, which is an essential milestone toward building sustainable and
trustworthy AI ecosystems.",2301.03052v1,https://arxiv.org/pdf/2301.03052v1
"MoreauGrad: Sparse and Robust Interpretation of Neural Networks via
  Moreau Envelope","Jingwei Zhang, Farzan Farnia","Explaining the predictions of deep neural nets has been a topic of great
interest in the computer vision literature. While several gradient-based
interpretation schemes have been proposed to reveal the influential variables
in a neural net's prediction, standard gradient-based interpretation frameworks
have been commonly observed to lack robustness to input perturbations and
flexibility for incorporating prior knowledge of sparsity and group-sparsity
structures. In this work, we propose MoreauGrad as an interpretation scheme
based on the classifier neural net's Moreau envelope. We demonstrate that
MoreauGrad results in a smooth and robust interpretation of a multi-layer
neural network and can be efficiently computed through first-order optimization
methods. Furthermore, we show that MoreauGrad can be naturally combined with
$L_1$-norm regularization techniques to output a sparse or group-sparse
explanation which are prior conditions applicable to a wide range of deep
learning applications. We empirically evaluate the proposed MoreauGrad scheme
on standard computer vision datasets, showing the qualitative and quantitative
success of the MoreauGrad approach in comparison to standard gradient-based
interpretation methods.",2302.05294v1,https://arxiv.org/pdf/2302.05294v1
"REaaS: Enabling Adversarially Robust Downstream Classifiers via Robust
  Encoder as a Service","Wenjie Qu, Jinyuan Jia, Neil Zhenqiang Gong","Encoder as a service is an emerging cloud service. Specifically, a service
provider first pre-trains an encoder (i.e., a general-purpose feature
extractor) via either supervised learning or self-supervised learning and then
deploys it as a cloud service API. A client queries the cloud service API to
obtain feature vectors for its training/testing inputs when training/testing
its classifier (called downstream classifier). A downstream classifier is
vulnerable to adversarial examples, which are testing inputs with carefully
crafted perturbation that the downstream classifier misclassifies. Therefore,
in safety and security critical applications, a client aims to build a robust
downstream classifier and certify its robustness guarantees against adversarial
examples.
  What APIs should the cloud service provide, such that a client can use any
certification method to certify the robustness of its downstream classifier
against adversarial examples while minimizing the number of queries to the
APIs? How can a service provider pre-train an encoder such that clients can
build more certifiably robust downstream classifiers? We aim to answer the two
questions in this work. For the first question, we show that the cloud service
only needs to provide two APIs, which we carefully design, to enable a client
to certify the robustness of its downstream classifier with a minimal number of
queries to the APIs. For the second question, we show that an encoder
pre-trained using a spectral-norm regularization term enables clients to build
more robust downstream classifiers.",2301.02905v1,https://arxiv.org/pdf/2301.02905v1
"gRoMA: a Tool for Measuring the Global Robustness of Deep Neural
  Networks","Natan Levy, Raz Yerushalmi, Guy Katz","Deep neural networks (DNNs) are at the forefront of cutting-edge technology,
and have been achieving remarkable performance in a variety of complex tasks.
Nevertheless, their integration into safety-critical systems, such as in the
aerospace or automotive domains, poses a significant challenge due to the
threat of adversarial inputs: perturbations in inputs that might cause the DNN
to make grievous mistakes. Multiple studies have demonstrated that even modern
DNNs are susceptible to adversarial inputs, and this risk must thus be measured
and mitigated to allow the deployment of DNNs in critical settings. Here, we
present gRoMA (global Robustness Measurement and Assessment), an innovative and
scalable tool that implements a probabilistic approach to measure the global
categorial robustness of a DNN. Specifically, gRoMA measures the probability of
encountering adversarial inputs for a specific output category. Our tool
operates on pre-trained, black-box classification DNNs, and generates input
samples belonging to an output category of interest. It measures the DNN's
susceptibility to adversarial inputs around these inputs, and aggregates the
results to infer the overall global categorial robustness of the DNN up to some
small bounded statistical error.
  We evaluate our tool on the popular Densenet DNN model over the CIFAR10
dataset. Our results reveal significant gaps in the robustness of the different
output categories. This experiment demonstrates the usefulness and scalability
of our approach and its potential for allowing DNNs to be deployed within
critical systems of interest.",2301.02288v3,https://arxiv.org/pdf/2301.02288v3
"Value Enhancement of Reinforcement Learning via Efficient and Robust
  Trust Region Optimization","Chengchun Shi, Zhengling Qi, Jianing Wang, Fan Zhou","Reinforcement learning (RL) is a powerful machine learning technique that
enables an intelligent agent to learn an optimal policy that maximizes the
cumulative rewards in sequential decision making. Most of methods in the
existing literature are developed in \textit{online} settings where the data
are easy to collect or simulate. Motivated by high stake domains such as mobile
health studies with limited and pre-collected data, in this paper, we study
\textit{offline} reinforcement learning methods. To efficiently use these
datasets for policy optimization, we propose a novel value enhancement method
to improve the performance of a given initial policy computed by existing
state-of-the-art RL algorithms. Specifically, when the initial policy is not
consistent, our method will output a policy whose value is no worse and often
better than that of the initial policy. When the initial policy is consistent,
under some mild conditions, our method will yield a policy whose value
converges to the optimal one at a faster rate than the initial policy,
achieving the desired ``value enhancement"" property. The proposed method is
generally applicable to any parametrized policy that belongs to certain
pre-specified function class (e.g., deep neural networks). Extensive numerical
studies are conducted to demonstrate the superior performance of our method.",2301.02220v1,https://arxiv.org/pdf/2301.02220v1
"Robust Control for Dynamical Systems With Non-Gaussian Noise via Formal
  Abstractions","Thom Badings, Licio Romao, Alessandro Abate, David Parker, Hasan A. Poonawala, Marielle Stoelinga, Nils Jansen","Controllers for dynamical systems that operate in safety-critical settings
must account for stochastic disturbances. Such disturbances are often modeled
as process noise in a dynamical system, and common assumptions are that the
underlying distributions are known and/or Gaussian. In practice, however, these
assumptions may be unrealistic and can lead to poor approximations of the true
noise distribution. We present a novel controller synthesis method that does
not rely on any explicit representation of the noise distributions. In
particular, we address the problem of computing a controller that provides
probabilistic guarantees on safely reaching a target, while also avoiding
unsafe regions of the state space. First, we abstract the continuous control
system into a finite-state model that captures noise by probabilistic
transitions between discrete states. As a key contribution, we adapt tools from
the scenario approach to compute probably approximately correct (PAC) bounds on
these transition probabilities, based on a finite number of samples of the
noise. We capture these bounds in the transition probability intervals of a
so-called interval Markov decision process (iMDP). This iMDP is, with a
user-specified confidence probability, robust against uncertainty in the
transition probabilities, and the tightness of the probability intervals can be
controlled through the number of samples. We use state-of-the-art verification
techniques to provide guarantees on the iMDP and compute a controller for which
these guarantees carry over to the original control system. In addition, we
develop a tailored computational scheme that reduces the complexity of the
synthesis of these guarantees on the iMDP. Benchmarks on realistic control
systems show the practical applicability of our method, even when the iMDP has
hundreds of millions of transitions.",2301.01526v1,https://arxiv.org/pdf/2301.01526v1
"Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus
  on Videos","Wei Xingxing, Wang Songping, Yan Huanqian","Adversarial robustness assessment for video recognition models has raised
concerns owing to their wide applications on safety-critical tasks. Compared
with images, videos have much high dimension, which brings huge computational
costs when generating adversarial videos. This is especially serious for the
query-based black-box attacks where gradient estimation for the threat models
is usually utilized, and high dimensions will lead to a large number of
queries. To mitigate this issue, we propose to simultaneously eliminate the
temporal and spatial redundancy within the video to achieve an effective and
efficient gradient estimation on the reduced searching space, and thus query
number could decrease. To implement this idea, we design the novel Adversarial
spatial-temporal Focus (AstFocus) attack on videos, which performs attacks on
the simultaneously focused key frames and key regions from the inter-frames and
intra-frames in the video. AstFocus attack is based on the cooperative
Multi-Agent Reinforcement Learning (MARL) framework. One agent is responsible
for selecting key frames, and another agent is responsible for selecting key
regions. These two agents are jointly trained by the common rewards received
from the black-box threat models to perform a cooperative prediction. By
continuously querying, the reduced searching space composed of key frames and
key regions is becoming precise, and the whole query number becomes less than
that on the original video. Extensive experiments on four mainstream video
recognition models and three widely used action recognition datasets
demonstrate that the proposed AstFocus attack outperforms the SOTA methods,
which is prevenient in fooling rate, query number, time, and perturbation
magnitude at the same.",2301.00896v2,https://arxiv.org/pdf/2301.00896v2
Robust Average-Reward Markov Decision Processes,"Yue Wang, Alvaro Velasquez, George Atia, Ashley Prater-Bennette, Shaofeng Zou","In robust Markov decision processes (MDPs), the uncertainty in the transition
kernel is addressed by finding a policy that optimizes the worst-case
performance over an uncertainty set of MDPs. While much of the literature has
focused on discounted MDPs, robust average-reward MDPs remain largely
unexplored. In this paper, we focus on robust average-reward MDPs, where the
goal is to find a policy that optimizes the worst-case average reward over an
uncertainty set. We first take an approach that approximates average-reward
MDPs using discounted MDPs. We prove that the robust discounted value function
converges to the robust average-reward as the discount factor $\gamma$ goes to
$1$, and moreover, when $\gamma$ is large, any optimal policy of the robust
discounted MDP is also an optimal policy of the robust average-reward. We
further design a robust dynamic programming approach, and theoretically
characterize its convergence to the optimum. Then, we investigate robust
average-reward MDPs directly without using discounted MDPs as an intermediate
step. We derive the robust Bellman equation for robust average-reward MDPs,
prove that the optimal policy can be derived from its solution, and further
design a robust relative value iteration algorithm that provably finds its
solution, or equivalently, the optimal robust policy.",2301.00858v2,https://arxiv.org/pdf/2301.00858v2
"DGFont++: Robust Deformable Generative Networks for Unsupervised Font
  Generation","Xinyuan Chen, Yangchen Xie, Li Sun, Yue Lu","Automatic font generation without human experts is a practical and
significant problem, especially for some languages that consist of a large
number of characters. Existing methods for font generation are often in
supervised learning. They require a large number of paired data, which are
labor-intensive and expensive to collect. In contrast, common unsupervised
image-to-image translation methods are not applicable to font generation, as
they often define style as the set of textures and colors. In this work, we
propose a robust deformable generative network for unsupervised font generation
(abbreviated as DGFont++). We introduce a feature deformation skip connection
(FDSC) to learn local patterns and geometric transformations between fonts. The
FDSC predicts pairs of displacement maps and employs the predicted maps to
apply deformable convolution to the low-level content feature maps. The outputs
of FDSC are fed into a mixer to generate final results. Moreover, we introduce
contrastive self-supervised learning to learn a robust style representation for
fonts by understanding the similarity and dissimilarities of fonts. To
distinguish different styles, we train our model with a multi-task
discriminator, which ensures that each style can be discriminated
independently. In addition to adversarial loss, another two reconstruction
losses are adopted to constrain the domain-invariant characteristics between
generated images and content images. Taking advantage of FDSC and the adopted
loss functions, our model is able to maintain spatial information and generates
high-quality character images in an unsupervised manner. Experiments
demonstrate that our model is able to generate character images of higher
quality than state-of-the-art methods.",2212.14742v1,https://arxiv.org/pdf/2212.14742v1
"Robust representations of oil wells' intervals via sparse attention
  mechanism","Alina Ermilova, Nikita Baramiia, Valerii Kornilov, Sergey Petrakov, Alexey Zaytsev","Transformer-based neural network architectures achieve state-of-the-art
results in different domains, from natural language processing (NLP) to
computer vision (CV). The key idea of Transformers, the attention mechanism,
has already led to significant breakthroughs in many areas. The attention has
found their implementation for time series data as well. However, due to the
quadratic complexity of the attention calculation regarding input sequence
length, the application of Transformers is limited by high resource demands.
Moreover, their modifications for industrial time series need to be robust to
missing or noised values, which complicates the expansion of the horizon of
their application. To cope with these issues, we introduce the class of
efficient Transformers named Regularized Transformers (Reguformers). We
implement the regularization technique inspired by the dropout ideas to improve
robustness and reduce computational expenses. The focus in our experiments is
on oil&gas data, namely, well logs, a prominent example of multivariate time
series. The goal is to solve the problems of similarity and representation
learning for them. To evaluate our models for such problems, we work with an
industry-scale open dataset consisting of well logs of more than 20 wells. The
experiments show that all variations of Reguformers outperform the previously
developed RNNs, classical Transformer model, and robust modifications of it
like Informer and Performer in terms of well-intervals' classification and the
quality of the obtained well-intervals' representations. Moreover, the
sustainability to missing and incorrect data in our models exceeds that of
others by a significant margin. The best result that the Reguformer achieves on
well-interval similarity task is the mean PR~AUC score equal to 0.983, which is
comparable to the classical Transformer and outperforms the previous models.",2212.14246v3,https://arxiv.org/pdf/2212.14246v3
Provable Robust Saliency-based Explanations,"Chao Chen, Chenghua Guo, Guixiang Ma, Ming Zeng, Xi Zhang, Sihong Xie","Robust explanations of machine learning models are critical to establishing
human trust in the models. The top-$k$ intersection is widely used to evaluate
the robustness of explanations. However, most existing attacking and defense
strategies are based on $\ell_p$ norms, thus creating a mismatch between the
evaluation and optimization objectives. To this end, we define explanation
thickness for measuring top-$k$ salient features ranking stability, and design
the \textit{R2ET} algorithm based on a novel tractable surrogate to maximize
the thickness and stabilize the top salient features efficiently.
Theoretically, we prove a connection between R2ET and adversarial training;
using a novel multi-objective optimization formulation and a generalization
error bound, we further prove that the surrogate objective can improve both the
numerical and statistical stability of the explanations. Experiments with a
wide spectrum of network architectures and data modalities demonstrate that
R2ET attains higher explanation robustness under stealthy attacks while
retaining model accuracy.",2212.14106v3,https://arxiv.org/pdf/2212.14106v3
Robustifying Markowitz,"Wolfgang Karl Härdle, Yegor Klochkov, Alla Petukhina, Nikita Zhivotovskiy","Markowitz mean-variance portfolios with sample mean and covariance as input
parameters feature numerous issues in practice. They perform poorly out of
sample due to estimation error, they experience extreme weights together with
high sensitivity to change in input parameters. The heavy-tail characteristics
of financial time series are in fact the cause for these erratic fluctuations
of weights that consequently create substantial transaction costs. In
robustifying the weights we present a toolbox for stabilizing costs and weights
for global minimum Markowitz portfolios. Utilizing a projected gradient descent
(PGD) technique, we avoid the estimation and inversion of the covariance
operator as a whole and concentrate on robust estimation of the gradient
descent increment. Using modern tools of robust statistics we construct a
computationally efficient estimator with almost Gaussian properties based on
median-of-means uniformly over weights. This robustified Markowitz approach is
confirmed by empirical studies on equity markets. We demonstrate that
robustified portfolios reach the lowest turnover compared to shrinkage-based
and constrained portfolios while preserving or slightly improving out-of-sample
performance.",2212.13996v1,https://arxiv.org/pdf/2212.13996v1
Differentiable Search of Accurate and Robust Architectures,"Yuwei Ou, Xiangning Xie, Shangce Gao, Yanan Sun, Kay Chen Tan, Jiancheng Lv","Deep neural networks (DNNs) are found to be vulnerable to adversarial
attacks, and various methods have been proposed for the defense. Among these
methods, adversarial training has been drawing increasing attention because of
its simplicity and effectiveness. However, the performance of the adversarial
training is greatly limited by the architectures of target DNNs, which often
makes the resulting DNNs with poor accuracy and unsatisfactory robustness. To
address this problem, we propose DSARA to automatically search for the neural
architectures that are accurate and robust after adversarial training. In
particular, we design a novel cell-based search space specially for adversarial
training, which improves the accuracy and the robustness upper bound of the
searched architectures by carefully designing the placement of the cells and
the proportional relationship of the filter numbers. Then we propose a
two-stage search strategy to search for both accurate and robust neural
architectures. At the first stage, the architecture parameters are optimized to
minimize the adversarial loss, which makes full use of the effectiveness of the
adversarial training in enhancing the robustness. At the second stage, the
architecture parameters are optimized to minimize both the natural loss and the
adversarial loss utilizing the proposed multi-objective adversarial training
method, so that the searched neural architectures are both accurate and robust.
We evaluate the proposed algorithm under natural data and various adversarial
attacks, which reveals the superiority of the proposed method in terms of both
accurate and robust architectures. We also conclude that accurate and robust
neural architectures tend to deploy very different structures near the input
and the output, which has great practical significance on both hand-crafting
and automatically designing of accurate and robust neural architectures.",2212.14049v2,https://arxiv.org/pdf/2212.14049v2
Robust Sequence Networked Submodular Maximization,"Qihao Shi, Bingyang Fu, Can Wang, Jiawei Chen, Sheng Zhou, Yan Feng, Chun Chen","In this paper, we study the \underline{R}obust \underline{o}ptimization for
\underline{se}quence \underline{Net}worked \underline{s}ubmodular maximization
(RoseNets) problem. We interweave the robust optimization with the sequence
networked submodular maximization. The elements are connected by a directed
acyclic graph and the objective function is not submodular on the elements but
on the edges in the graph. Under such networked submodular scenario, the impact
of removing an element from a sequence depends both on its position in the
sequence and in the network. This makes the existing robust algorithms
inapplicable. In this paper, we take the first step to study the RoseNets
problem. We design a robust greedy algorithm, which is robust against the
removal of an arbitrary subset of the selected elements. The approximation
ratio of the algorithm depends both on the number of the removed elements and
the network topology. We further conduct experiments on real applications of
recommendation and link prediction. The experimental results demonstrate the
effectiveness of the proposed algorithm.",2212.13725v2,https://arxiv.org/pdf/2212.13725v2
"Optimal algorithms for group distributionally robust optimization and
  beyond","Tasuku Soma, Khashayar Gatmiry, Stefanie Jegelka","Distributionally robust optimization (DRO) can improve the robustness and
fairness of learning methods. In this paper, we devise stochastic algorithms
for a class of DRO problems including group DRO, subpopulation fairness, and
empirical conditional value at risk (CVaR) optimization. Our new algorithms
achieve faster convergence rates than existing algorithms for multiple DRO
settings. We also provide a new information-theoretic lower bound that implies
our bounds are tight for group DRO. Empirically, too, our algorithms outperform
known methods",2212.13669v1,https://arxiv.org/pdf/2212.13669v1
"Robust Consensus Clustering and its Applications for Advertising
  Forecasting","Deguang Kong, Miao Lu, Konstantin Shmakov, Jian Yang","Consensus clustering aggregates partitions in order to find a better fit by
reconciling clustering results from different sources/executions. In practice,
there exist noise and outliers in clustering task, which, however, may
significantly degrade the performance. To address this issue, we propose a
novel algorithm -- robust consensus clustering that can find common ground
truth among experts' opinions, which tends to be minimally affected by the bias
caused by the outliers. In particular, we formalize the robust consensus
clustering problem as a constraint optimization problem, and then derive an
effective algorithm upon alternating direction method of multipliers (ADMM)
with rigorous convergence guarantee. Our method outperforms the baselines on
benchmarks. We apply the proposed method to the real-world advertising campaign
segmentation and forecasting tasks using the proposed consensus clustering
results based on the similarity computed via Kolmogorov-Smirnov Statistics. The
accurate clustering result is helpful for building the advertiser profiles so
as to perform the forecasting.",2301.00717v1,https://arxiv.org/pdf/2301.00717v1
"Robust computation of optimal transport by $β$-potential
  regularization","Shintaro Nakamura, Han Bao, Masashi Sugiyama","Optimal transport (OT) has become a widely used tool in the machine learning
field to measure the discrepancy between probability distributions. For
instance, OT is a popular loss function that quantifies the discrepancy between
an empirical distribution and a parametric model. Recently, an entropic penalty
term and the celebrated Sinkhorn algorithm have been commonly used to
approximate the original OT in a computationally efficient way. However, since
the Sinkhorn algorithm runs a projection associated with the Kullback-Leibler
divergence, it is often vulnerable to outliers. To overcome this problem, we
propose regularizing OT with the \beta-potential term associated with the
so-called $\beta$-divergence, which was developed in robust statistics. Our
theoretical analysis reveals that the $\beta$-potential can prevent the mass
from being transported to outliers. We experimentally demonstrate that the
transport matrix computed with our algorithm helps estimate a probability
distribution robustly even in the presence of outliers. In addition, our
proposed method can successfully detect outliers from a contaminated dataset",2212.13251v1,https://arxiv.org/pdf/2212.13251v1
A Bayesian Robust Regression Method for Corrupted Data Reconstruction,"Zheyi Fan, Zhaohui Li, Jingyan Wang, Dennis K. J. Lin, Xiao Xiong, Qingpei Hu","Because of the widespread existence of noise and data corruption, recovering
the true regression parameters with a certain proportion of corrupted response
variables is an essential task. Methods to overcome this problem often involve
robust least-squares regression, but few methods perform well when confronted
with severe adaptive adversarial attacks. In many applications, prior knowledge
is often available from historical data or engineering experience, and by
incorporating prior information into a robust regression method, we develop an
effective robust regression method that can resist adaptive adversarial
attacks. First, we propose the novel TRIP (hard Thresholding approach to Robust
regression with sImple Prior) algorithm, which improves the breakdown point
when facing adaptive adversarial attacks. Then, to improve the robustness and
reduce the estimation error caused by the inclusion of priors, we use the idea
of Bayesian reweighting to construct the more robust BRHT (robust Bayesian
Reweighting regression via Hard Thresholding) algorithm. We prove the
theoretical convergence of the proposed algorithms under mild conditions, and
extensive experiments show that under different types of dataset attacks, our
algorithms outperform other benchmark ones. Finally, we apply our methods to a
data-recovery problem in a real-world application involving a space solar
array, demonstrating their good applicability.",2212.12787v2,https://arxiv.org/pdf/2212.12787v2
Benchmark for Uncertainty & Robustness in Self-Supervised Learning,"Ha Manh Bui, Iliana Maifeld-Carucci","Self-Supervised Learning (SSL) is crucial for real-world applications,
especially in data-hungry domains such as healthcare and self-driving cars. In
addition to a lack of labeled data, these applications also suffer from
distributional shifts. Therefore, an SSL method should provide robust
generalization and uncertainty estimation in the test dataset to be considered
a reliable model in such high-stakes domains. However, existing approaches
often focus on generalization, without evaluating the model's uncertainty. The
ability to compare SSL techniques for improving these estimates is therefore
critical for research on the reliability of self-supervision models. In this
paper, we explore variants of SSL methods, including Jigsaw Puzzles, Context,
Rotation, Geometric Transformations Prediction for vision, as well as BERT and
GPT for language tasks. We train SSL in auxiliary learning for vision and
pre-training for language model, then evaluate the generalization (in-out
classification accuracy) and uncertainty (expected calibration error) across
different distribution covariate shift datasets, including MNIST-C, CIFAR-10-C,
CIFAR-10.1, and MNLI. Our goal is to create a benchmark with outputs from
experiments, providing a starting point for new SSL methods in Reliable Machine
Learning. All source code to reproduce results is available at
https://github.com/hamanhbui/reliable_ssl_baselines.",2212.12411v1,https://arxiv.org/pdf/2212.12411v1
"Robust Meta-Representation Learning via Global Label Inference and
  Classification","Ruohan Wang, Isak Falk, Massimiliano Pontil, Carlo Ciliberto","Few-shot learning (FSL) is a central problem in meta-learning, where learners
must efficiently learn from few labeled examples. Within FSL, feature
pre-training has recently become an increasingly popular strategy to
significantly improve generalization performance. However, the contribution of
pre-training is often overlooked and understudied, with limited theoretical
understanding of its impact on meta-learning performance. Further, pre-training
requires a consistent set of global labels shared across training tasks, which
may be unavailable in practice. In this work, we address the above issues by
first showing the connection between pre-training and meta-learning. We discuss
why pre-training yields more robust meta-representation and connect the
theoretical analysis to existing works and empirical results. Secondly, we
introduce Meta Label Learning (MeLa), a novel meta-learning algorithm that
learns task relations by inferring global labels across tasks. This allows us
to exploit pre-training for FSL even when global labels are unavailable or
ill-defined. Lastly, we introduce an augmented pre-training procedure that
further improves the learned meta-representation. Empirically, MeLa outperforms
existing methods across a diverse range of benchmarks, in particular under a
more challenging setting where the number of training tasks is limited and
labels are task-specific. We also provide extensive ablation study to highlight
its key properties.",2212.11702v2,https://arxiv.org/pdf/2212.11702v2
"Imitation Is Not Enough: Robustifying Imitation with Reinforcement
  Learning for Challenging Driving Scenarios","Yiren Lu, Justin Fu, George Tucker, Xinlei Pan, Eli Bronstein, Rebecca Roelofs, Benjamin Sapp, Brandyn White, Aleksandra Faust, Shimon Whiteson, Dragomir Anguelov, Sergey Levine","Imitation learning (IL) is a simple and powerful way to use high-quality
human driving data, which can be collected at scale, to produce human-like
behavior. However, policies based on imitation learning alone often fail to
sufficiently account for safety and reliability concerns. In this paper, we
show how imitation learning combined with reinforcement learning using simple
rewards can substantially improve the safety and reliability of driving
policies over those learned from imitation alone. In particular, we train a
policy on over 100k miles of urban driving data, and measure its effectiveness
in test scenarios grouped by different levels of collision likelihood. Our
analysis shows that while imitation can perform well in low-difficulty
scenarios that are well-covered by the demonstration data, our proposed
approach significantly improves robustness on the most challenging scenarios
(over 38% reduction in failures). To our knowledge, this is the first
application of a combined imitation and reinforcement learning approach in
autonomous driving that utilizes large amounts of real-world human driving
data.",2212.11419v2,https://arxiv.org/pdf/2212.11419v2
Deep Unfolded Tensor Robust PCA with Self-supervised Learning,"Harry Dong, Megna Shah, Sean Donegan, Yuejie Chi","Tensor robust principal component analysis (RPCA), which seeks to separate a
low-rank tensor from its sparse corruptions, has been crucial in data science
and machine learning where tensor structures are becoming more prevalent. While
powerful, existing tensor RPCA algorithms can be difficult to use in practice,
as their performance can be sensitive to the choice of additional
hyperparameters, which are not straightforward to tune. In this paper, we
describe a fast and simple self-supervised model for tensor RPCA using deep
unfolding by only learning four hyperparameters. Despite its simplicity, our
model expunges the need for ground truth labels while maintaining competitive
or even greater performance compared to supervised deep unfolding. Furthermore,
our model is capable of operating in extreme data-starved scenarios. We
demonstrate these claims on a mix of synthetic data and real-world tasks,
comparing performance against previously studied supervised deep unfolding
methods and Bayesian optimization baselines.",2212.11346v1,https://arxiv.org/pdf/2212.11346v1
"Robust Path Selection in Software-defined WANs using Deep Reinforcement
  Learning","Shahrooz Pouryousef, Lixin Gao, Don Towsley","In the context of an efficient network traffic engineering process where the
network continuously measures a new traffic matrix and updates the set of paths
in the network, an automated process is required to quickly and efficiently
identify when and what set of paths should be used. Unfortunately, the burden
of finding the optimal solution for the network updating process in each given
time interval is high since the computation complexity of optimization
approaches using linear programming increases significantly as the size of the
network increases. In this paper, we use deep reinforcement learning to derive
a data-driven algorithm that does the path selection in the network considering
the overhead of route computation and path updates. Our proposed scheme
leverages information about past network behavior to identify a set of robust
paths to be used for multiple future time intervals to avoid the overhead of
updating the forwarding behavior of routers frequently. We compare the results
of our approach to other traffic engineering solutions through extensive
simulations across real network topologies. Our results demonstrate that our
scheme fares well by a factor of 40% with respect to reducing link utilization
compared to traditional TE schemes such as ECMP. Our scheme provides a slightly
higher link utilization (around 25%) compared to schemes that only minimize
link utilization and do not care about path updating overhead.",2212.11155v2,https://arxiv.org/pdf/2212.11155v2
"Revisiting Residual Networks for Adversarial Robustness: An
  Architectural Perspective","Shihua Huang, Zhichao Lu, Kalyanmoy Deb, Vishnu Naresh Boddeti","Efforts to improve the adversarial robustness of convolutional neural
networks have primarily focused on developing more effective adversarial
training methods. In contrast, little attention was devoted to analyzing the
role of architectural elements (such as topology, depth, and width) on
adversarial robustness. This paper seeks to bridge this gap and present a
holistic study on the impact of architectural design on adversarial robustness.
We focus on residual networks and consider architecture design at the block
level, i.e., topology, kernel size, activation, and normalization, as well as
at the network scaling level, i.e., depth and width of each block in the
network. In both cases, we first derive insights through systematic ablative
experiments. Then we design a robust residual block, dubbed RobustResBlock, and
a compound scaling rule, dubbed RobustScaling, to distribute depth and width at
the desired FLOP count. Finally, we combine RobustResBlock and RobustScaling
and present a portfolio of adversarially robust residual networks,
RobustResNets, spanning a broad spectrum of model capacities. Experimental
validation across multiple datasets and adversarial attacks demonstrate that
RobustResNets consistently outperform both the standard WRNs and other existing
robust architectures, achieving state-of-the-art AutoAttack robust accuracy of
61.1% without additional data and 63.7% with 500K external data while being
$2\times$ more compact in terms of parameters. Code is available at \url{
https://github.com/zhichao-lu/robust-residual-network}",2212.11005v1,https://arxiv.org/pdf/2212.11005v1
Audio Denoising for Robust Audio Fingerprinting,Kamil Akesbi,"Music discovery services let users identify songs from short mobile
recordings. These solutions are often based on Audio Fingerprinting, and rely
more specifically on the extraction of spectral peaks in order to be robust to
a number of distortions. Few works have been done to study the robustness of
these algorithms to background noise captured in real environments. In
particular, AFP systems still struggle when the signal to noise ratio is low,
i.e when the background noise is strong. In this project, we tackle this
problematic with Deep Learning. We test a new hybrid strategy which consists of
inserting a denoising DL model in front of a peak-based AFP algorithm. We
simulate noisy music recordings using a realistic data augmentation pipeline,
and train a DL model to denoise them. The denoising model limits the impact of
background noise on the AFP system's extracted peaks, improving its robustness
to noise. We further propose a novel loss function to adapt the DL model to the
considered AFP system, increasing its precision in terms of retrieved spectral
peaks. To the best of our knowledge, this hybrid strategy has not been tested
before.",2212.11277v1,https://arxiv.org/pdf/2212.11277v1
"Testing Occupational Gender Bias in Language Models: Towards Robust
  Measurement and Zero-Shot Debiasing","Yuen Chen, Vethavikashini Chithrra Raghuram, Justus Mattern, Mrinmaya Sachan, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin","Generated texts from large language models (LLMs) have been shown to exhibit
a variety of harmful, human-like biases against various demographics. These
findings motivate research efforts aiming to understand and measure such
effects. Prior works have proposed benchmarks for identifying and techniques
for mitigating these stereotypical associations. However, as recent research
pointed out, existing benchmarks lack a robust experimental setup, hindering
the inference of meaningful conclusions from their evaluation metrics. In this
paper, we introduce a list of desiderata for robustly measuring biases in
generative language models. Building upon these design principles, we propose a
benchmark called OCCUGENDER, with a bias-measuring procedure to investigate
occupational gender bias. We then use this benchmark to test several
state-of-the-art open-source LLMs, including Llama, Mistral, and their
instruction-tuned versions. The results show that these models exhibit
substantial occupational gender bias. We further propose prompting techniques
to mitigate these biases without requiring fine-tuning. Finally, we validate
the effectiveness of our methods through experiments on the same set of models.",2212.10678v2,https://arxiv.org/pdf/2212.10678v2
Policy Gradient in Robust MDPs with Global Convergence Guarantee,"Qiuhao Wang, Chin Pang Ho, Marek Petrik","Robust Markov decision processes (RMDPs) provide a promising framework for
computing reliable policies in the face of model errors. Many successful
reinforcement learning algorithms build on variations of policy-gradient
methods, but adapting these methods to RMDPs has been challenging. As a result,
the applicability of RMDPs to large, practical domains remains limited. This
paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first
generic policy gradient method for RMDPs. In contrast with prior robust policy
gradient algorithms, DRPG monotonically reduces approximation errors to
guarantee convergence to a globally optimal policy in tabular RMDPs. We
introduce a novel parametric transition kernel and solve the inner loop robust
policy via a gradient-based method. Finally, our numerical results demonstrate
the utility of our new algorithm and confirm its global convergence properties.",2212.10439v2,https://arxiv.org/pdf/2212.10439v2
"Walking Noise: On Layer-Specific Robustness of Neural Architectures
  against Noisy Computations and Associated Characteristic Learning Dynamics","Hendrik Borras, Bernhard Klein, Holger Fröning","Deep neural networks are extremely successful in various applications,
however they exhibit high computational demands and energy consumption. This is
exacerbated by stuttering technology scaling, prompting the need for novel
approaches to handle increasingly complex neural architectures. At the same
time, alternative computing technologies such as analog computing, which
promise groundbreaking improvements in energy efficiency, are inevitably
fraught with noise and inaccurate calculations. Such noisy computations are
more energy efficient, and, given a fixed power budget, also more time
efficient. However, like any kind of unsafe optimization, they require
countermeasures to ensure functionally correct results.
  This work considers noisy computations in an abstract form, and gears to
understand the implications of such noise on the accuracy of neural network
classifiers as an exemplary workload. We propose a methodology called Walking
Noise which injects layer-specific noise to measure the robustness and to
provide insights on the learning dynamics. In more detail, we investigate the
implications of additive, multiplicative and mixed noise for different
classification tasks and model architectures. While noisy training
significantly increases robustness for all noise types, we observe in
particular that it results in increased weight magnitudes and thus inherently
improves the signal-to-noise ratio for additive noise injection. Contrarily,
training with multiplicative noise can lead to a form of self-binarization of
the model parameters, leading to extreme robustness. We conclude with a
discussion of the use of this methodology in practice, among others, discussing
its use for tailored multi-execution in noisy environments.",2212.10430v2,https://arxiv.org/pdf/2212.10430v2
ReCode: Robustness Evaluation of Code Generation Models,"Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, Bing Xiang","Code generation models have achieved impressive performance. However, they
tend to be brittle as slight edits to a prompt could lead to very different
generations; these robustness properties, critical for user experience when
deployed in real-life applications, are not well understood. Most existing
works on robustness in text or code tasks have focused on classification, while
robustness in generation tasks is an uncharted area and to date there is no
comprehensive benchmark for robustness in code generation. In this paper, we
propose ReCode, a comprehensive robustness evaluation benchmark for code
generation models. We customize over 30 transformations specifically for code
on docstrings, function and variable names, code syntax, and code format. They
are carefully designed to be natural in real-life coding practice, preserve the
original semantic meaning, and thus provide multifaceted assessments of a
model's robustness performance. With human annotators, we verified that over
90% of the perturbed prompts do not alter the semantic meaning of the original
prompt. In addition, we define robustness metrics for code generation models
considering the worst-case behavior under each type of perturbation, taking
advantage of the fact that executing the generated code can serve as objective
evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well
as function completion tasks derived from them. Interesting observations
include: better robustness for CodeGen over InCoder and GPT-J; models are most
sensitive to syntax perturbations; more challenging robustness evaluation on
MBPP over HumanEval.",2212.10264v1,https://arxiv.org/pdf/2212.10264v1
In and Out-of-Domain Text Adversarial Robustness via Label Smoothing,"Yahan Yang, Soham Dan, Dan Roth, Insup Lee","Recently it has been shown that state-of-the-art NLP models are vulnerable to
adversarial attacks, where the predictions of a model can be drastically
altered by slight modifications to the input (such as synonym substitutions).
While several defense techniques have been proposed, and adapted, to the
discrete nature of text adversarial attacks, the benefits of general-purpose
regularization methods such as label smoothing for language models, have not
been studied. In this paper, we study the adversarial robustness provided by
various label smoothing strategies in foundational models for diverse NLP tasks
in both in-domain and out-of-domain settings. Our experiments show that label
smoothing significantly improves adversarial robustness in pre-trained models
like BERT, against various popular attacks. We also analyze the relationship
between prediction confidence and robustness, showing that label smoothing
reduces over-confident errors on adversarial examples.",2212.10258v2,https://arxiv.org/pdf/2212.10258v2
Distributional Robustness Bounds Generalization Errors,"Shixiong Wang, Haowei Wang","Bayesian methods, distributionally robust optimization methods, and
regularization methods are three pillars of trustworthy machine learning
combating distributional uncertainty, e.g., the uncertainty of an empirical
distribution compared to the true underlying distribution. This paper
investigates the connections among the three frameworks and, in particular,
explores why these frameworks tend to have smaller generalization errors.
Specifically, first, we suggest a quantitative definition for ""distributional
robustness"", propose the concept of ""robustness measure"", and formalize several
philosophical concepts in distributionally robust optimization. Second, we show
that Bayesian methods are distributionally robust in the probably approximately
correct (PAC) sense; in addition, by constructing a Dirichlet-process-like
prior in Bayesian nonparametrics, it can be proven that any regularized
empirical risk minimization method is equivalent to a Bayesian method. Third,
we show that generalization errors of machine learning models can be
characterized using the distributional uncertainty of the nominal distribution
and the robustness measures of these machine learning models, which is a new
perspective to bound generalization errors, and therefore, explain the reason
why distributionally robust machine learning models, Bayesian models, and
regularization models tend to have smaller generalization errors in a unified
manner.",2212.09962v3,https://arxiv.org/pdf/2212.09962v3
"Robust and Resource-efficient Machine Learning Aided Viewport Prediction
  in Virtual Reality","Yuang Jiang, Konstantinos Poularakis, Diego Kiedanski, Sastry Kompella, Leandros Tassiulas","360-degree panoramic videos have gained considerable attention in recent
years due to the rapid development of head-mounted displays (HMDs) and
panoramic cameras. One major problem in streaming panoramic videos is that
panoramic videos are much larger in size compared to traditional ones.
Moreover, the user devices are often in a wireless environment, with limited
battery, computation power, and bandwidth. To reduce resource consumption,
researchers have proposed ways to predict the users' viewports so that only
part of the entire video needs to be transmitted from the server. However, the
robustness of such prediction approaches has been overlooked in the literature:
it is usually assumed that only a few models, pre-trained on past users'
experiences, are applied for prediction to all users. We observe that those
pre-trained models can perform poorly for some users because they might have
drastically different behaviors from the majority, and the pre-trained models
cannot capture the features in unseen videos. In this work, we propose a novel
meta learning based viewport prediction paradigm to alleviate the worst
prediction performance and ensure the robustness of viewport prediction. This
paradigm uses two machine learning models, where the first model predicts the
viewing direction, and the second model predicts the minimum video prefetch
size that can include the actual viewport. We first train two meta models so
that they are sensitive to new training data, and then quickly adapt them to
users while they are watching the videos. Evaluation results reveal that the
meta models can adapt quickly to each user, and can significantly increase the
prediction accuracy, especially for the worst-performing predictions.",2212.09945v1,https://arxiv.org/pdf/2212.09945v1
"Improving the Robustness of Summarization Models by Detecting and
  Removing Input Noise","Kundan Krishna, Yao Zhao, Jie Ren, Balaji Lakshminarayanan, Jiaming Luo, Mohammad Saleh, Peter J. Liu","The evaluation of abstractive summarization models typically uses test data
that is identically distributed as training data. In real-world practice,
documents to be summarized may contain input noise caused by text extraction
artifacts or data pipeline bugs. The robustness of model performance under
distribution shift caused by such noise is relatively under-studied. We present
a large empirical study quantifying the sometimes severe loss in performance
(up to 12 ROUGE-1 points) from different types of input noise for a range of
datasets and model sizes. We then propose a light-weight method for detecting
and removing such noise in the input during model inference without requiring
any extra training, auxiliary models, or even prior knowledge of the type of
noise. Our proposed approach effectively mitigates the loss in performance,
recovering a large fraction of the performance drop, sometimes as large as 11
ROUGE-1 points.",2212.09928v2,https://arxiv.org/pdf/2212.09928v2
"Robust Design and Evaluation of Predictive Algorithms under Unobserved
  Confounding","Ashesh Rambachan, Amanda Coston, Edward Kennedy","Predictive algorithms inform consequential decisions in settings where the
outcome is selectively observed given choices made by human decision makers. We
propose a unified framework for the robust design and evaluation of predictive
algorithms in selectively observed data. We impose general assumptions on how
much the outcome may vary on average between unselected and selected units
conditional on observed covariates and identified nuisance parameters,
formalizing popular empirical strategies for imputing missing data such as
proxy outcomes and instrumental variables. We develop debiased machine learning
estimators for the bounds on a large class of predictive performance estimands,
such as the conditional likelihood of the outcome, a predictive algorithm's
mean square error, true/false positive rate, and many others, under these
assumptions. In an administrative dataset from a large Australian financial
institution, we illustrate how varying assumptions on unobserved confounding
leads to meaningful changes in default risk predictions and evaluations of
credit scores across sensitive groups.",2212.09844v5,https://arxiv.org/pdf/2212.09844v5
"Robust Anomaly Map Assisted Multiple Defect Detection with Supervised
  Classification Techniques","Jože M. Rožanec, Patrik Zajec, Spyros Theodoropoulos, Erik Koehorst, Blaž Fortuna, Dunja Mladenić","Industry 4.0 aims to optimize the manufacturing environment by leveraging new
technological advances, such as new sensing capabilities and artificial
intelligence. The DRAEM technique has shown state-of-the-art performance for
unsupervised classification. The ability to create anomaly maps highlighting
areas where defects probably lie can be leveraged to provide cues to supervised
classification models and enhance their performance. Our research shows that
the best performance is achieved when training a defect detection model by
providing an image and the corresponding anomaly map as input. Furthermore,
such a setting provides consistent performance when framing the defect
detection as a binary or multiclass classification problem and is not affected
by class balancing policies. We performed the experiments on three datasets
with real-world data provided by Philips Consumer Lifestyle BV.",2212.09352v1,https://arxiv.org/pdf/2212.09352v1
"Estimating the Adversarial Robustness of Attributions in Text with
  Transformers","Adam Ivankay, Mattia Rigotti, Ivan Girardi, Chiara Marchiori, Pascal Frossard","Explanations are crucial parts of deep neural network (DNN) classifiers. In
high stakes applications, faithful and robust explanations are important to
understand and gain trust in DNN classifiers. However, recent work has shown
that state-of-the-art attribution methods in text classifiers are susceptible
to imperceptible adversarial perturbations that alter explanations
significantly while maintaining the correct prediction outcome. If undetected,
this can critically mislead the users of DNNs. Thus, it is crucial to
understand the influence of such adversarial perturbations on the networks'
explanations and their perceptibility. In this work, we establish a novel
definition of attribution robustness (AR) in text classification, based on
Lipschitz continuity. Crucially, it reflects both attribution change induced by
adversarial input alterations and perceptibility of such alterations. Moreover,
we introduce a wide set of text similarity measures to effectively capture
locality between two text samples and imperceptibility of adversarial
perturbations in text. We then propose our novel TransformerExplanationAttack
(TEA), a strong adversary that provides a tight estimation for attribution
robustness in text classification. TEA uses state-of-the-art language models to
extract word substitutions that result in fluent, contextual adversarial
samples. Finally, with experiments on several text classification
architectures, we show that TEA consistently outperforms current
state-of-the-art AR estimators, yielding perturbations that alter explanations
to a greater extent while being more fluent and less perceptible.",2212.09155v1,https://arxiv.org/pdf/2212.09155v1
"Confidence-aware Training of Smoothed Classifiers for Certified
  Robustness","Jongheon Jeong, Seojin Kim, Jinwoo Shin","Any classifier can be ""smoothed out"" under Gaussian noise to build a new
classifier that is provably robust to $\ell_2$-adversarial perturbations, viz.,
by averaging its predictions over the noise via randomized smoothing. Under the
smoothed classifiers, the fundamental trade-off between accuracy and
(adversarial) robustness has been well evidenced in the literature: i.e.,
increasing the robustness of a classifier for an input can be at the expense of
decreased accuracy for some other inputs. In this paper, we propose a simple
training method leveraging this trade-off to obtain robust smoothed
classifiers, in particular, through a sample-wise control of robustness over
the training samples. We make this control feasible by using ""accuracy under
Gaussian noise"" as an easy-to-compute proxy of adversarial robustness for an
input. Specifically, we differentiate the training objective depending on this
proxy to filter out samples that are unlikely to benefit from the worst-case
(adversarial) objective. Our experiments show that the proposed method, despite
its simplicity, consistently exhibits improved certified robustness upon
state-of-the-art training methods. Somewhat surprisingly, we find these
improvements persist even for other notions of robustness, e.g., to various
types of common corruptions.",2212.09000v2,https://arxiv.org/pdf/2212.09000v2
"A Robust Semantic Frame Parsing Pipeline on a New Complex Twitter
  Dataset","Yu Wang, Hongxia Jin","Most recent semantic frame parsing systems for spoken language understanding
(SLU) are designed based on recurrent neural networks. These systems display
decent performance on benchmark SLU datasets such as ATIS or SNIPS, which
contain short utterances with relatively simple patterns. However, the current
semantic frame parsing models lack a mechanism to handle out-of-distribution
(\emph{OOD}) patterns and out-of-vocabulary (\emph{OOV}) tokens. In this paper,
we introduce a robust semantic frame parsing pipeline that can handle both
\emph{OOD} patterns and \emph{OOV} tokens in conjunction with a new complex
Twitter dataset that contains long tweets with more \emph{OOD} patterns and
\emph{OOV} tokens. The new pipeline demonstrates much better results in
comparison to state-of-the-art baseline SLU models on both the SNIPS dataset
and the new Twitter dataset (Our new Twitter dataset can be downloaded from
https://1drv.ms/u/s!AroHb-W6_OAlavK4begsDsMALfE?e=c8f2XX ). Finally, we also
build an E2E application to demo the feasibility of our algorithm and show why
it is useful in real application.",2212.08987v1,https://arxiv.org/pdf/2212.08987v1
Language model acceptability judgements are not always robust to context,"Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adina Williams","Targeted syntactic evaluations of language models ask whether models show
stable preferences for syntactically acceptable content over minimal-pair
unacceptable inputs. Most targeted syntactic evaluation datasets ask models to
make these judgements with just a single context-free sentence as input. This
does not match language models' training regime, in which input sentences are
always highly contextualized by the surrounding corpus. This mismatch raises an
important question: how robust are models' syntactic judgements in different
contexts? In this paper, we investigate the stability of language models'
performance on targeted syntactic evaluations as we vary properties of the
input context: the length of the context, the types of syntactic phenomena it
contains, and whether or not there are violations of grammaticality. We find
that model judgements are generally robust when placed in randomly sampled
linguistic contexts. However, they are substantially unstable for contexts
containing syntactic structures matching those in the critical test content.
Among all tested models (GPT-2 and five variants of OPT), we significantly
improve models' judgements by providing contexts with matching syntactic
structures, and conversely significantly worsen them using unacceptable
contexts with matching but violated syntactic structures. This effect is
amplified by the length of the context, except for unrelated inputs. We show
that these changes in model performance are not explainable by simple features
matching the context and the test inputs, such as lexical overlap and
dependency overlap. This sensitivity to highly specific syntactic features of
the context can only be explained by the models' implicit in-context learning
abilities.",2212.08979v1,https://arxiv.org/pdf/2212.08979v1
Robust Explanation Constraints for Neural Networks,"Matthew Wicker, Juyeon Heo, Luca Costabello, Adrian Weller","Post-hoc explanation methods are used with the intent of providing insights
about neural networks and are sometimes said to help engender trust in their
outputs. However, popular explanations methods have been found to be fragile to
minor perturbations of input features or model parameters. Relying on
constraint relaxation techniques from non-convex optimization, we develop a
method that upper-bounds the largest change an adversary can make to a
gradient-based explanation via bounded manipulation of either the input
features or model parameters. By propagating a compact input or parameter set
as symbolic intervals through the forwards and backwards computations of the
neural network we can formally certify the robustness of gradient-based
explanations. Our bounds are differentiable, hence we can incorporate provable
explanation robustness into neural network training. Empirically, our method
surpasses the robustness provided by previous heuristic approaches. We find
that our training method is the only method able to learn neural networks with
certificates of explanation robustness across all six datasets tested.",2212.08507v1,https://arxiv.org/pdf/2212.08507v1
Robust Learning Protocol for Federated Tumor Segmentation Challenge,"Ambrish Rawat, Giulio Zizzo, Swanand Kadhe, Jonathan P. Epperlein, Stefano Braghin","In this work, we devise robust and efficient learning protocols for
orchestrating a Federated Learning (FL) process for the Federated Tumor
Segmentation Challenge (FeTS 2022). Enabling FL for FeTS setup is challenging
mainly due to data heterogeneity among collaborators and communication cost of
training. To tackle these challenges, we propose Robust Learning Protocol
(RoLePRO) which is a combination of server-side adaptive optimisation (e.g.,
server-side Adam) and judicious parameter (weights) aggregation schemes (e.g.,
adaptive weighted aggregation). RoLePRO takes a two-phase approach, where the
first phase consists of vanilla Federated Averaging, while the second phase
consists of a judicious aggregation scheme that uses a sophisticated
reweighting, all in the presence of an adaptive optimisation algorithm at the
server. We draw insights from extensive experimentation to tune learning rates
for the two phases.",2212.08290v1,https://arxiv.org/pdf/2212.08290v1
"On Evaluating Adversarial Robustness of Chest X-ray Classification:
  Pitfalls and Best Practices","Salah Ghamizi, Maxime Cordy, Michail Papadakis, Yves Le Traon","Vulnerability to adversarial attacks is a well-known weakness of Deep Neural
Networks. While most of the studies focus on natural images with standardized
benchmarks like ImageNet and CIFAR, little research has considered real world
applications, in particular in the medical domain. Our research shows that,
contrary to previous claims, robustness of chest x-ray classification is much
harder to evaluate and leads to very different assessments based on the
dataset, the architecture and robustness metric. We argue that previous studies
did not take into account the peculiarity of medical diagnosis, like the
co-occurrence of diseases, the disagreement of labellers (domain experts), the
threat model of the attacks and the risk implications for each successful
attack.
  In this paper, we discuss the methodological foundations, review the pitfalls
and best practices, and suggest new methodological considerations for
evaluating the robustness of chest xray classification models. Our evaluation
on 3 datasets, 7 models, and 18 diseases is the largest evaluation of
robustness of chest x-ray classification models.",2212.08130v1,https://arxiv.org/pdf/2212.08130v1
Variable Clustering via Distributionally Robust Nodewise Regression,"Kaizheng Wang, Xiao Xu, Xun Yu Zhou","We study a multi-factor block model for variable clustering and connect it to
the regularized subspace clustering by formulating a distributionally robust
version of the nodewise regression. To solve the latter problem, we derive a
convex relaxation, provide guidance on selecting the size of the robust region,
and hence the regularization weighting parameter, based on the data, and
propose an ADMM algorithm for implementation. We validate our method in an
extensive simulation study. Finally, we propose and apply a variant of our
method to stock return data, obtain interpretable clusters that facilitate
portfolio selection and compare its out-of-sample performance with other
clustering methods in an empirical study.",2212.07944v2,https://arxiv.org/pdf/2212.07944v2
"Bridging POMDPs and Bayesian decision making for robust maintenance
  planning under model uncertainty: An application to railway systems","Giacomo Arcieri, Cyprien Hoelzl, Oliver Schwery, Daniel Straub, Konstantinos G. Papakonstantinou, Eleni Chatzi","Structural Health Monitoring (SHM) describes a process for inferring
quantifiable metrics of structural condition, which can serve as input to
support decisions on the operation and maintenance of infrastructure assets.
Given the long lifespan of critical structures, this problem can be cast as a
sequential decision making problem over prescribed horizons. Partially
Observable Markov Decision Processes (POMDPs) offer a formal framework to solve
the underlying optimal planning task. However, two issues can undermine the
POMDP solutions. Firstly, the need for a model that can adequately describe the
evolution of the structural condition under deterioration or corrective actions
and, secondly, the non-trivial task of recovery of the observation process
parameters from available monitoring data. Despite these potential challenges,
the adopted POMDP models do not typically account for uncertainty on model
parameters, leading to solutions which can be unrealistically confident. In
this work, we address both key issues. We present a framework to estimate POMDP
transition and observation model parameters directly from available data, via
Markov Chain Monte Carlo (MCMC) sampling of a Hidden Markov Model (HMM)
conditioned on actions. The MCMC inference estimates distributions of the
involved model parameters. We then form and solve the POMDP problem by
exploiting the inferred distributions, to derive solutions that are robust to
model uncertainty. We successfully apply our approach on maintenance planning
for railway track assets on the basis of a ""fractal value"" indicator, which is
computed from actual railway monitoring data.",2212.07933v1,https://arxiv.org/pdf/2212.07933v1
"Statistical Design and Analysis for Robust Machine Learning: A Case
  Study from COVID-19","Davide Pigoli, Kieran Baker, Jobie Budd, Lorraine Butler, Harry Coppock, Sabrina Egglestone, Steven G. Gilmour, Chris Holmes, David Hurley, Radka Jersakova, Ivan Kiskin, Vasiliki Koutra, Jonathon Mellor, George Nicholson, Joe Packham, Selina Patel, Richard Payne, Stephen J. Roberts, Björn W. Schuller, Ana Tendero-Cañadas, Tracey Thornley, Alexander Titcomb","Since early in the coronavirus disease 2019 (COVID-19) pandemic, there has
been interest in using artificial intelligence methods to predict COVID-19
infection status based on vocal audio signals, for example cough recordings.
However, existing studies have limitations in terms of data collection and of
the assessment of the performances of the proposed predictive models. This
paper rigorously assesses state-of-the-art machine learning techniques used to
predict COVID-19 infection status based on vocal audio signals, using a dataset
collected by the UK Health Security Agency. This dataset includes acoustic
recordings and extensive study participant meta-data. We provide guidelines on
testing the performance of methods to classify COVID-19 infection status based
on acoustic features and we discuss how these can be extended more generally to
the development and assessment of predictive methods based on public health
datasets.",2212.08571v2,https://arxiv.org/pdf/2212.08571v2
Robustness Evaluation of Regression Tasks with Skewed Domain Preferences,"Nuno Costa, Nuno Moniz","In natural phenomena, data distributions often deviate from normality. One
can think of cataclysms as a self-explanatory example: events that occur almost
never, and at the same time are many standard deviations away from the common
outcome. In many scientific contexts it is exactly these tail events that
researchers are most interested in anticipating, so that adequate measures can
be taken to prevent or attenuate a major impact on society. Despite such
efforts, we have yet to provide definite answers to crucial issues in
evaluating predictive solutions in domains such as weather, pollution, health.
In this paper, we deal with two encapsulated problems simultaneously. First,
assessing the performance of regression models when non-uniform preferences
apply - not all values are equally relevant concerning the accuracy of their
prediction, and there's a particular interest in the most extreme values.
Second, assessing the robustness of models when dealing with uncertainty
regarding the actual underlying distribution of values relevant for such
problems. We show how different levels of relevance associated with target
values may impact experimental conclusions, and demonstrate the practical
utility of the proposed methods.",2212.07562v1,https://arxiv.org/pdf/2212.07562v1
Robust Policy Optimization in Deep Reinforcement Learning,"Md Masudur Rahman, Yexiang Xue","The policy gradient method enjoys the simplicity of the objective where the
agent optimizes the cumulative reward directly. Moreover, in the continuous
action domain, parameterized distribution of action distribution allows easy
control of exploration, resulting from the variance of the representing
distribution. Entropy can play an essential role in policy optimization by
selecting the stochastic policy, which eventually helps better explore the
environment in reinforcement learning (RL). However, the stochasticity often
reduces as the training progresses; thus, the policy becomes less exploratory.
Additionally, certain parametric distributions might only work for some
environments and require extensive hyperparameter tuning. This paper aims to
mitigate these issues. In particular, we propose an algorithm called Robust
Policy Optimization (RPO), which leverages a perturbed distribution. We
hypothesize that our method encourages high-entropy actions and provides a way
to represent the action space better. We further provide empirical evidence to
verify our hypothesis. We evaluated our methods on various continuous control
tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed
that in many settings, RPO increases the policy entropy early in training and
then maintains a certain level of entropy throughout the training period.
Eventually, our agent RPO shows consistently improved performance compared to
PPO and other techniques: entropy regularization, different distributions, and
data augmentation. Furthermore, in several settings, our method stays robust in
performance, while other baseline mechanisms fail to improve and even worsen
the performance.",2212.07536v1,https://arxiv.org/pdf/2212.07536v1
Generative Robust Classification,Xuwang Yin,"Training adversarially robust discriminative (i.e., softmax) classifier has
been the dominant approach to robust classification. Building on recent work on
adversarial training (AT)-based generative models, we investigate using AT to
learn unnormalized class-conditional density models and then performing
generative robust classification. Our result shows that, under the condition of
similar model capacities, the generative robust classifier achieves comparable
performance to a baseline softmax robust classifier when the test data is clean
or when the test perturbation is of limited size, and much better performance
when the test perturbation size exceeds the training perturbation size. The
generative classifier is also able to generate samples or counterfactuals that
more closely resemble the training data, suggesting that the generative
classifier can better capture the class-conditional distributions. In contrast
to standard discriminative adversarial training where advanced data
augmentation techniques are only effective when combined with weight averaging,
we find it straightforward to apply advanced data augmentation to achieve
better robustness in our approach. Our result suggests that the generative
classifier is a competitive alternative to robust classification, especially
for problems with limited number of classes.",2212.07283v1,https://arxiv.org/pdf/2212.07283v1
"Improving group robustness under noisy labels using predictive
  uncertainty","Dongpin Oh, Dae Lee, Jeunghyun Byun, Bonggun Shin","The standard empirical risk minimization (ERM) can underperform on certain
minority groups (i.e., waterbirds in lands or landbirds in water) due to the
spurious correlation between the input and its label. Several studies have
improved the worst-group accuracy by focusing on the high-loss samples. The
hypothesis behind this is that such high-loss samples are
\textit{spurious-cue-free} (SCF) samples. However, these approaches can be
problematic since the high-loss samples may also be samples with noisy labels
in the real-world scenarios. To resolve this issue, we utilize the predictive
uncertainty of a model to improve the worst-group accuracy under noisy labels.
To motivate this, we theoretically show that the high-uncertainty samples are
the SCF samples in the binary classification problem. This theoretical result
implies that the predictive uncertainty is an adequate indicator to identify
SCF samples in a noisy label setting. Motivated from this, we propose a novel
ENtropy based Debiasing (END) framework that prevents models from learning the
spurious cues while being robust to the noisy labels. In the END framework, we
first train the \textit{identification model} to obtain the SCF samples from a
training set using its predictive uncertainty. Then, another model is trained
on the dataset augmented with an oversampled SCF set. The experimental results
show that our END framework outperforms other strong baselines on several
real-world benchmarks that consider both the noisy labels and the
spurious-cues.",2212.07026v1,https://arxiv.org/pdf/2212.07026v1
"Statistical Safety and Robustness Guarantees for Feedback Motion
  Planning of Unknown Underactuated Stochastic Systems","Craig Knuth, Glen Chou, Jamie Reese, Joe Moore","We present a method for providing statistical guarantees on runtime safety
and goal reachability for integrated planning and control of a class of systems
with unknown nonlinear stochastic underactuated dynamics. Specifically, given a
dynamics dataset, our method jointly learns a mean dynamics model, a
spatially-varying disturbance bound that captures the effect of noise and model
mismatch, and a feedback controller based on contraction theory that stabilizes
the learned dynamics. We propose a sampling-based planner that uses the mean
dynamics model and simultaneously bounds the closed-loop tracking error via a
learned disturbance bound. We employ techniques from Extreme Value Theory (EVT)
to estimate, to a specified level of confidence, several constants which
characterize the learned components and govern the size of the tracking error
bound. This ensures plans are guaranteed to be safely tracked at runtime. We
validate that our guarantees translate to empirical safety in simulation on a
10D quadrotor, and in the real world on a physical CrazyFlie quadrotor and
Clearpath Jackal robot, whereas baselines that ignore the model error and
stochasticity are unsafe.",2212.06874v1,https://arxiv.org/pdf/2212.06874v1
"AdvCat: Domain-Agnostic Robustness Assessment for Cybersecurity-Critical
  Applications with Categorical Inputs","Helene Orsini, Hongyan Bao, Yujun Zhou, Xiangrui Xu, Yufei Han, Longyang Yi, Wei Wang, Xin Gao, Xiangliang Zhang","Machine Learning-as-a-Service systems (MLaaS) have been largely developed for
cybersecurity-critical applications, such as detecting network intrusions and
fake news campaigns. Despite effectiveness, their robustness against
adversarial attacks is one of the key trust concerns for MLaaS deployment. We
are thus motivated to assess the adversarial robustness of the Machine Learning
models residing at the core of these security-critical applications with
categorical inputs. Previous research efforts on accessing model robustness
against manipulation of categorical inputs are specific to use cases and
heavily depend on domain knowledge, or require white-box access to the target
ML model. Such limitations prevent the robustness assessment from being as a
domain-agnostic service provided to various real-world applications. We propose
a provably optimal yet computationally highly efficient adversarial robustness
assessment protocol for a wide band of ML-driven cybersecurity-critical
applications. We demonstrate the use of the domain-agnostic robustness
assessment method with substantial experimental study on fake news detection
and intrusion detection problems.",2212.13989v1,https://arxiv.org/pdf/2212.13989v1
AFLGuard: Byzantine-robust Asynchronous Federated Learning,"Minghong Fang, Jia Liu, Neil Zhenqiang Gong, Elizabeth S. Bentley","Federated learning (FL) is an emerging machine learning paradigm, in which
clients jointly learn a model with the help of a cloud server. A fundamental
challenge of FL is that the clients are often heterogeneous, e.g., they have
different computing powers, and thus the clients may send model updates to the
server with substantially different delays. Asynchronous FL aims to address
this challenge by enabling the server to update the model once any client's
model update reaches it without waiting for other clients' model updates.
However, like synchronous FL, asynchronous FL is also vulnerable to poisoning
attacks, in which malicious clients manipulate the model via poisoning their
local data and/or model updates sent to the server. Byzantine-robust FL aims to
defend against poisoning attacks. In particular, Byzantine-robust FL can learn
an accurate model even if some clients are malicious and have Byzantine
behaviors. However, most existing studies on Byzantine-robust FL focused on
synchronous FL, leaving asynchronous FL largely unexplored. In this work, we
bridge this gap by proposing AFLGuard, a Byzantine-robust asynchronous FL
method. We show that, both theoretically and empirically, AFLGuard is robust
against various existing and adaptive poisoning attacks (both untargeted and
targeted). Moreover, AFLGuard outperforms existing Byzantine-robust
asynchronous FL methods.",2212.06325v1,https://arxiv.org/pdf/2212.06325v1
"Robust and Explainable Identification of Logical Fallacies in Natural
  Language Arguments","Zhivar Sourati, Vishnu Priya Prasanna Venkatesh, Darshan Deshpande, Himanshu Rawlani, Filip Ilievski, Hông-Ân Sandlin, Alain Mermoud","The spread of misinformation, propaganda, and flawed argumentation has been
amplified in the Internet era. Given the volume of data and the subtlety of
identifying violations of argumentation norms, supporting information analytics
tasks, like content moderation, with trustworthy methods that can identify
logical fallacies is essential. In this paper, we formalize prior theoretical
work on logical fallacies into a comprehensive three-stage evaluation framework
of detection, coarse-grained, and fine-grained classification. We adapt
existing evaluation datasets for each stage of the evaluation. We employ three
families of robust and explainable methods based on prototype reasoning,
instance-based reasoning, and knowledge injection. The methods combine language
models with background knowledge and explainable mechanisms. Moreover, we
address data sparsity with strategies for data augmentation and curriculum
learning. Our three-stage framework natively consolidates prior datasets and
methods from existing tasks, like propaganda detection, serving as an
overarching evaluation testbed. We extensively evaluate these methods on our
datasets, focusing on their robustness and explainability. Our results provide
insight into the strengths and weaknesses of the methods on different
components and fallacy classes, indicating that fallacy identification is a
challenging task that may require specialized forms of reasoning to capture
various classes. We share our open-source code and data on GitHub to support
further work on logical fallacy identification.",2212.07425v3,https://arxiv.org/pdf/2212.07425v3
Selective classification using a robust meta-learning approach,"Nishant Jain, Karthikeyan Shanmugam, Pradeep Shenoy","Predictive uncertainty-a model's self awareness regarding its accuracy on an
input-is key for both building robust models via training interventions and for
test-time applications such as selective classification. We propose a novel
instance-conditioned reweighting approach that captures predictive uncertainty
using an auxiliary network and unifies these train- and test-time applications.
The auxiliary network is trained using a meta-objective in a bilevel
optimization framework. A key contribution of our proposal is the
meta-objective of minimizing the dropout variance, an approximation of Bayesian
Predictive uncertainty. We show in controlled experiments that we effectively
capture the diverse specific notions of uncertainty through this
meta-objective, while previous approaches only capture certain aspects. These
results translate to significant gains in real-world settings-selective
classification, label noise, domain adaptation, calibration-and across
datasets-Imagenet, Cifar100, diabetic retinopathy, Camelyon, WILDs,
Imagenet-C,-A,-R, Clothing1M, etc. For Diabetic Retinopathy, we see upto
3.4%/3.3% accuracy and AUC gains over SOTA in selective classification. We also
improve upon large-scale pretrained models such as PLEX.",2212.05987v2,https://arxiv.org/pdf/2212.05987v2
"Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear
  Contextual Bandits and Markov Decision Processes","Chenlu Ye, Wei Xiong, Quanquan Gu, Tong Zhang","Despite the significant interest and progress in reinforcement learning (RL)
problems with adversarial corruption, current works are either confined to the
linear setting or lead to an undesired $\tilde{O}(\sqrt{T}\zeta)$ regret bound,
where $T$ is the number of rounds and $\zeta$ is the total amount of
corruption. In this paper, we consider the contextual bandit with general
function approximation and propose a computationally efficient algorithm to
achieve a regret of $\tilde{O}(\sqrt{T}+\zeta)$. The proposed algorithm relies
on the recently developed uncertainty-weighted least-squares regression from
linear contextual bandit and a new weighted estimator of uncertainty for the
general function class. In contrast to the existing analysis that heavily
relies on the linear structure, we develop a novel technique to control the sum
of weighted uncertainty, thus establishing the final regret bounds. We then
generalize our algorithm to the episodic MDP setting and first achieve an
additive dependence on the corruption level $\zeta$ in the scenario of general
function approximation. Notably, our algorithms achieve regret bounds either
nearly match the performance lower bound or improve the existing methods for
all the corruption levels and in both known and unknown $\zeta$ cases.",2212.05949v4,https://arxiv.org/pdf/2212.05949v4
"Robust Recurrent Neural Network to Identify Ship Motion in Open Water
  with Performance Guarantees -- Technical Report","Daniel Frank, Decky Aspandi Latif, Michael Muehlebach, Benjamin Unger, Steffen Staab","Recurrent neural networks are capable of learning the dynamics of an unknown
nonlinear system purely from input-output measurements. However, the resulting
models do not provide any stability guarantees on the input-output mapping. In
this work, we represent a recurrent neural network as a linear time-invariant
system with nonlinear disturbances. By introducing constraints on the
parameters, we can guarantee finite gain stability and incremental finite gain
stability. We apply this identification method to learn the motion of a
four-degrees-of-freedom ship that is moving in open water and compare it
against other purely learning-based approaches with unconstrained parameters.
Our analysis shows that the constrained recurrent neural network has a lower
prediction accuracy on the test set, but it achieves comparable results on an
out-of-distribution set and respects stability conditions.",2212.05781v2,https://arxiv.org/pdf/2212.05781v2
"On Generalization and Regularization via Wasserstein Distributionally
  Robust Optimization","Qinyu Wu, Jonathan Yu-Meng Li, Tiantian Mao","Wasserstein distributionally robust optimization (DRO) has found success in
operations research and machine learning applications as a powerful means to
obtain solutions with favourable out-of-sample performances. Two compelling
explanations for the success are the generalization bounds derived from
Wasserstein DRO and the equivalency between Wasserstein DRO and the
regularization scheme commonly applied in machine learning. Existing results on
generalization bounds and the equivalency to regularization are largely limited
to the setting where the Wasserstein ball is of a certain type and the decision
criterion takes certain forms of an expected function. In this paper, we show
that by focusing on Wasserstein DRO problems with affine decision rules, it is
possible to obtain generalization bounds and the equivalency to regularization
in a significantly broader setting where the Wasserstein ball can be of a
general type and the decision criterion can be a general measure of risk, i.e.,
nonlinear in distributions. This allows for accommodating many important
classification, regression, and risk minimization applications that have not
been addressed to date using Wasserstein DRO. Our results are strong in that
the generalization bounds do not suffer from the curse of dimensionality and
the equivalency to regularization is exact. As a byproduct, our regularization
results broaden considerably the class of Wasserstein DRO models that can be
solved efficiently via regularization formulations.",2212.05716v1,https://arxiv.org/pdf/2212.05716v1
"Siamese Sleep Transformer For Robust Sleep Stage Scoring With
  Self-knowledge Distillation and Selective Batch Sampling","Heon-Gyu Kwak, Young-Seok Kweon, Gi-Hwan Shin","In this paper, we propose a Siamese sleep transformer (SST) that effectively
extracts features from single-channel raw electroencephalogram signals for
robust sleep stage scoring. Despite the significant advances in sleep stage
scoring in the last few years, most of them mainly focused on the increment of
model performance. However, other problems still exist: the bias of labels in
datasets and the instability of model performance by repetitive training. To
alleviate these problems, we propose the SST, a novel sleep stage scoring model
with a selective batch sampling strategy and self-knowledge distillation. To
evaluate how robust the model was to the bias of labels, we used different
datasets for training and testing: the sleep heart health study and the
Sleep-EDF datasets. In this condition, the SST showed competitive performance
in sleep stage scoring. In addition, we demonstrated the effectiveness of the
selective batch sampling strategy with a reduction of the standard deviation of
performance by repetitive training. These results could show that SST extracted
effective learning features against the bias of labels in datasets, and the
selective batch sampling strategy worked for the model robustness in training.",2212.13919v1,https://arxiv.org/pdf/2212.13919v1
"Understanding and Combating Robust Overfitting via Input Loss Landscape
  Analysis and Regularization","Lin Li, Michael Spratling","Adversarial training is widely used to improve the robustness of deep neural
networks to adversarial attack. However, adversarial training is prone to
overfitting, and the cause is far from clear. This work sheds light on the
mechanisms underlying overfitting through analyzing the loss landscape w.r.t.
the input. We find that robust overfitting results from standard training,
specifically the minimization of the clean loss, and can be mitigated by
regularization of the loss gradients. Moreover, we find that robust overfitting
turns severer during adversarial training partially because the gradient
regularization effect of adversarial training becomes weaker due to the
increase in the loss landscapes curvature. To improve robust generalization, we
propose a new regularizer to smooth the loss landscape by penalizing the
weighted logits variation along the adversarial direction. Our method
significantly mitigates robust overfitting and achieves the highest robustness
and efficiency compared to similar previous methods. Code is available at
https://github.com/TreeLLi/Combating-RO-AdvLC.",2212.04985v1,https://arxiv.org/pdf/2212.04985v1
"Doubly Robust Kernel Statistics for Testing Distributional Treatment
  Effects","Jake Fawkes, Robert Hu, Robin J. Evans, Dino Sejdinovic","With the widespread application of causal inference, it is increasingly
important to have tools which can test for the presence of causal effects in a
diverse array of circumstances. In this vein we focus on the problem of testing
for \emph{distributional} causal effects, where the treatment affects not just
the mean, but also higher order moments of the distribution, as well as
multidimensional or structured outcomes. We build upon a previously introduced
framework, Counterfactual Mean Embeddings, for representing causal
distributions within Reproducing Kernel Hilbert Spaces (RKHS) by proposing new,
improved, estimators for the distributional embeddings. These improved
estimators are inspired by doubly robust estimators of the causal mean, using a
similar form within the kernel space. We analyse these estimators, proving they
retain the doubly robust property and have improved convergence rates compared
to the original estimators. This leads to new permutation based tests for
distributional causal effects, using the estimators we propose as tests
statistics. We experimentally and theoretically demonstrate the validity of our
tests.",2212.04922v2,https://arxiv.org/pdf/2212.04922v2
Robust detection and attribution of climate change under interventions,"Enikő Székely, Sebastian Sippel, Nicolai Meinshausen, Guillaume Obozinski, Reto Knutti","Fingerprints are key tools in climate change detection and attribution (D&A)
that are used to determine whether changes in observations are different from
internal climate variability (detection), and whether observed changes can be
assigned to specific external drivers (attribution). We propose a direct D&A
approach based on supervised learning to extract fingerprints that lead to
robust predictions under relevant interventions on exogenous variables, i.e.,
climate drivers other than the target. We employ anchor regression, a
distributionally-robust statistical learning method inspired by causal
inference that extrapolates well to perturbed data under the interventions
considered. The residuals from the prediction achieve either uncorrelatedness
or mean independence with the exogenous variables, thus guaranteeing
robustness. We define D&A as a unified hypothesis testing framework that relies
on the same statistical model but uses different targets and test statistics.
In the experiments, we first show that the CO2 forcing can be robustly
predicted from temperature spatial patterns under strong interventions on the
solar forcing. Second, we illustrate attribution to the greenhouse gases and
aerosols while protecting against interventions on the aerosols and CO2
forcing, respectively. Our study shows that incorporating robustness
constraints against relevant interventions may significantly benefit detection
and attribution of climate change.",2212.04905v1,https://arxiv.org/pdf/2212.04905v1
Robust Graph Representation Learning via Predictive Coding,"Billy Byiringiro, Tommaso Salvatori, Thomas Lukasiewicz","Predictive coding is a message-passing framework initially developed to model
information processing in the brain, and now also topic of research in machine
learning due to some interesting properties. One of such properties is the
natural ability of generative models to learn robust representations thanks to
their peculiar credit assignment rule, that allows neural activities to
converge to a solution before updating the synaptic weights. Graph neural
networks are also message-passing models, which have recently shown outstanding
results in diverse types of tasks in machine learning, providing
interdisciplinary state-of-the-art performance on structured data. However,
they are vulnerable to imperceptible adversarial attacks, and unfit for
out-of-distribution generalization. In this work, we address this by building
models that have the same structure of popular graph neural network
architectures, but rely on the message-passing rule of predictive coding.
Through an extensive set of experiments, we show that the proposed models are
(i) comparable to standard ones in terms of performance in both inductive and
transductive tasks, (ii) better calibrated, and (iii) robust against multiple
kinds of adversarial attacks.",2212.04656v1,https://arxiv.org/pdf/2212.04656v1
"Contrastive View Design Strategies to Enhance Robustness to Domain
  Shifts in Downstream Object Detection","Kyle Buettner, Adriana Kovashka","Contrastive learning has emerged as a competitive pretraining method for
object detection. Despite this progress, there has been minimal investigation
into the robustness of contrastively pretrained detectors when faced with
domain shifts. To address this gap, we conduct an empirical study of
contrastive learning and out-of-domain object detection, studying how
contrastive view design affects robustness. In particular, we perform a case
study of the detection-focused pretext task Instance Localization (InsLoc) and
propose strategies to augment views and enhance robustness in
appearance-shifted and context-shifted scenarios. Amongst these strategies, we
propose changes to cropping such as altering the percentage used, adding IoU
constraints, and integrating saliency based object priors. We also explore the
addition of shortcut-reducing augmentations such as Poisson blending, texture
flattening, and elastic deformation. We benchmark these strategies on abstract,
weather, and context domain shifts and illustrate robust ways to combine them,
in both pretraining on single-object and multi-object image datasets. Overall,
our results and insights show how to ensure robustness through the choice of
views in contrastive learning.",2212.04613v1,https://arxiv.org/pdf/2212.04613v1
Phone2Proc: Bringing Robust Robots Into Our Chaotic World,"Matt Deitke, Rose Hendrix, Luca Weihs, Ali Farhadi, Kiana Ehsani, Aniruddha Kembhavi","Training embodied agents in simulation has become mainstream for the embodied
AI community. However, these agents often struggle when deployed in the
physical world due to their inability to generalize to real-world environments.
In this paper, we present Phone2Proc, a method that uses a 10-minute phone scan
and conditional procedural generation to create a distribution of training
scenes that are semantically similar to the target environment. The generated
scenes are conditioned on the wall layout and arrangement of large objects from
the scan, while also sampling lighting, clutter, surface textures, and
instances of smaller objects with randomized placement and materials.
Leveraging just a simple RGB camera, training with Phone2Proc shows massive
improvements from 34.7% to 70.7% success rate in sim-to-real ObjectNav
performance across a test suite of over 200 trials in diverse real-world
environments, including homes, offices, and RoboTHOR. Furthermore, Phone2Proc's
diverse distribution of generated scenes makes agents remarkably robust to
changes in the real world, such as human movement, object rearrangement,
lighting changes, or clutter.",2212.04819v1,https://arxiv.org/pdf/2212.04819v1
On the Robustness of Normalizing Flows for Inverse Problems in Imaging,"Seongmin Hong, Inbum Park, Se Young Chun","Conditional normalizing flows can generate diverse image samples for solving
inverse problems. Most normalizing flows for inverse problems in imaging employ
the conditional affine coupling layer that can generate diverse images quickly.
However, unintended severe artifacts are occasionally observed in the output of
them. In this work, we address this critical issue by investigating the origins
of these artifacts and proposing the conditions to avoid them. First of all, we
empirically and theoretically reveal that these problems are caused by
""exploding inverse"" in the conditional affine coupling layer for certain
out-of-distribution (OOD) conditional inputs. Then, we further validated that
the probability of causing erroneous artifacts in pixels is highly correlated
with a Mahalanobis distance-based OOD score for inverse problems in imaging.
Lastly, based on our investigations, we propose a remark to avoid exploding
inverse and then based on it, we suggest a simple remedy that substitutes the
affine coupling layers with the modified rational quadratic spline coupling
layers in normalizing flows, to encourage the robustness of generated image
samples. Our experimental results demonstrated that our suggested methods
effectively suppressed critical artifacts occurring in normalizing flows for
super-resolution space generation and low-light image enhancement.",2212.04319v2,https://arxiv.org/pdf/2212.04319v2
"P2T2: a Physically-primed deep-neural-network approach for robust
  $T_{2}$ distribution estimation from quantitative $T_{2}$-weighted MRI","Hadas Ben-Atya, Moti Freiman","Estimating $T_2$ relaxation time distributions from multi-echo $T_2$-weighted
MRI ($T_2W$) data can provide valuable biomarkers for assessing inflammation,
demyelination, edema, and cartilage composition in various pathologies,
including neurodegenerative disorders, osteoarthritis, and tumors. Deep neural
network (DNN) based methods have been proposed to address the complex inverse
problem of estimating $T_2$ distributions from MRI data, but they are not yet
robust enough for clinical data with low Signal-to-Noise ratio (SNR) and are
highly sensitive to distribution shifts such as variations in echo-times (TE)
used during acquisition. Consequently, their application is hindered in
clinical practice and large-scale multi-institutional trials with heterogeneous
acquisition protocols. We propose a physically-primed DNN approach, called
$P_2T_2$, that incorporates the signal decay forward model in addition to the
MRI signal into the DNN architecture to improve the accuracy and robustness of
$T_2$ distribution estimation. We evaluated our $P_2T_2$ model in comparison to
both DNN-based methods and classical methods for $T_2$ distribution estimation
using 1D and 2D numerical simulations along with clinical data. Our model
improved the baseline model's accuracy for low SNR levels ($SNR<80$) which are
common in the clinical setting. Further, our model achieved a $\sim$35\%
improvement in robustness against distribution shifts in the acquisition
process compared to previously proposed DNN models. Finally, Our $P_2T_2$ model
produces the most detailed Myelin-Water fraction maps compared to baseline
approaches when applied to real human MRI data. Our $P_2T_2$ model offers a
reliable and precise means of estimating $T_2$ distributions from MRI data and
shows promise for use in large-scale multi-institutional trials with
heterogeneous acquisition protocols.",2212.04928v2,https://arxiv.org/pdf/2212.04928v2
"MixBoost: Improving the Robustness of Deep Neural Networks by Boosting
  Data Augmentation","Zhendong Liu, Wenyu Jiang, Min guo, Chongjun Wang","As more and more artificial intelligence (AI) technologies move from the
laboratory to real-world applications, the open-set and robustness challenges
brought by data from the real world have received increasing attention. Data
augmentation is a widely used method to improve model performance, and some
recent works have also confirmed its positive effect on the robustness of AI
models. However, most of the existing data augmentation methods are heuristic,
lacking the exploration of their internal mechanisms. We apply the explainable
artificial intelligence (XAI) method, explore the internal mechanisms of
popular data augmentation methods, analyze the relationship between game
interactions and some widely used robustness metrics, and propose a new proxy
for model robustness in the open-set environment. Based on the analysis of the
internal mechanisms, we develop a mask-based boosting method for data
augmentation that comprehensively improves several robustness measures of AI
models and beats state-of-the-art data augmentation approaches. Experiments
show that our method can be widely applied to many popular data augmentation
methods. Different from the adversarial training, our boosting method not only
significantly improves the robustness of models, but also improves the accuracy
of test sets. Our code is available at
\url{https://github.com/Anonymous_for_submission}.",2212.04059v1,https://arxiv.org/pdf/2212.04059v1
"SeqLink: A Robust Neural-ODE Architecture for Modelling Partially
  Observed Time Series","Futoon M. Abushaqra, Hao Xue, Yongli Ren, Flora D. Salim","Ordinary Differential Equations (ODE) based models have become popular as
foundation models for solving many time series problems. Combining neural ODEs
with traditional RNN models has provided the best representation for irregular
time series. However, ODE-based models typically require the trajectory of
hidden states to be defined based on either the initial observed value or the
most recent observation, raising questions about their effectiveness when
dealing with longer sequences and extended time intervals. In this article, we
explore the behaviour of the ODE models in the context of time series data with
varying degrees of sparsity. We introduce SeqLink, an innovative neural
architecture designed to enhance the robustness of sequence representation.
Unlike traditional approaches that solely rely on the hidden state generated
from the last observed value, SeqLink leverages ODE latent representations
derived from multiple data samples, enabling it to generate robust data
representations regardless of sequence length or data sparsity level. The core
concept behind our model is the definition of hidden states for the unobserved
values based on the relationships between samples (links between sequences).
Through extensive experiments on partially observed synthetic and real-world
datasets, we demonstrate that SeqLink improves the modelling of intermittent
time series, consistently outperforming state-of-the-art approaches.",2212.03560v3,https://arxiv.org/pdf/2212.03560v3
"PyGFI: Analyzing and Enhancing Robustness of Graph Neural Networks
  Against Hardware Errors","Ruixuan Wang, Fred Lin, Daniel Moore, Sriram Sankar, Xun Jiao","Graph neural networks (GNNs) have recently emerged as a promising learning
paradigm in learning graph-structured data and have demonstrated wide success
across various domains such as recommendation systems, social networks, and
electronic design automation (EDA). Like other deep learning (DL) methods, GNNs
are being deployed in sophisticated modern hardware systems, as well as
dedicated accelerators. However, despite the popularity of GNNs and the recent
efforts of bringing GNNs to hardware, the fault tolerance and resilience of
GNNs have generally been overlooked. Inspired by the inherent algorithmic
resilience of DL methods, this paper conducts, for the first time, a
large-scale and empirical study of GNN resilience, aiming to understand the
relationship between hardware faults and GNN accuracy. By developing a
customized fault injection tool on top of PyTorch, we perform extensive fault
injection experiments on various GNN models and application datasets. We
observe that the error resilience of GNN models varies by orders of magnitude
with respect to different models and application datasets. Further, we explore
a low-cost error mitigation mechanism for GNN to enhance its resilience. This
GNN resilience study aims to open up new directions and opportunities for
future GNN accelerator design and architectural optimization.",2212.03475v2,https://arxiv.org/pdf/2212.03475v2
Robust Point Cloud Segmentation with Noisy Annotations,"Shuquan Ye, Dongdong Chen, Songfang Han, Jing Liao","Point cloud segmentation is a fundamental task in 3D. Despite recent progress
on point cloud segmentation with the power of deep networks, current learning
methods based on the clean label assumptions may fail with noisy labels. Yet,
class labels are often mislabeled at both instance-level and boundary-level in
real-world datasets. In this work, we take the lead in solving the
instance-level label noise by proposing a Point Noise-Adaptive Learning (PNAL)
framework. Compared to noise-robust methods on image tasks, our framework is
noise-rate blind, to cope with the spatially variant noise rate specific to
point clouds. Specifically, we propose a point-wise confidence selection to
obtain reliable labels from the historical predictions of each point. A
cluster-wise label correction is proposed with a voting strategy to generate
the best possible label by considering the neighbor correlations. To handle
boundary-level label noise, we also propose a variant ``PNAL-boundary "" with a
progressive boundary label cleaning strategy. Extensive experiments demonstrate
its effectiveness on both synthetic and real-world noisy datasets. Even with
$60\%$ symmetric noise and high-level boundary noise, our framework
significantly outperforms its baselines, and is comparable to the upper bound
trained on completely clean data. Moreover, we cleaned the popular real-world
dataset ScanNetV2 for rigorous experiment. Our code and data is available at
https://github.com/pleaseconnectwifi/PNAL.",2212.03242v1,https://arxiv.org/pdf/2212.03242v1
Robust Speech Recognition via Large-Scale Weak Supervision,"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever","We study the capabilities of speech processing systems trained simply to
predict large amounts of transcripts of audio on the internet. When scaled to
680,000 hours of multilingual and multitask supervision, the resulting models
generalize well to standard benchmarks and are often competitive with prior
fully supervised results but in a zero-shot transfer setting without the need
for any fine-tuning. When compared to humans, the models approach their
accuracy and robustness. We are releasing models and inference code to serve as
a foundation for further work on robust speech processing.",2212.04356v1,https://arxiv.org/pdf/2212.04356v1
Enhancing Quantum Adversarial Robustness by Randomized Encodings,"Weiyuan Gong, Dong Yuan, Weikang Li, Dong-Ling Deng","The interplay between quantum physics and machine learning gives rise to the
emergent frontier of quantum machine learning, where advanced quantum learning
models may outperform their classical counterparts in solving certain
challenging problems. However, quantum learning systems are vulnerable to
adversarial attacks: adding tiny carefully-crafted perturbations on legitimate
input samples can cause misclassifications. To address this issue, we propose a
general scheme to protect quantum learning systems from adversarial attacks by
randomly encoding the legitimate data samples through unitary or quantum error
correction encoders. In particular, we rigorously prove that both global and
local random unitary encoders lead to exponentially vanishing gradients (i.e.
barren plateaus) for any variational quantum circuits that aim to add
adversarial perturbations, independent of the input data and the inner
structures of adversarial circuits and quantum classifiers. In addition, we
prove a rigorous bound on the vulnerability of quantum classifiers under local
unitary adversarial attacks. We show that random black-box quantum error
correction encoders can protect quantum classifiers against local adversarial
noises and their robustness increases as we concatenate error correction codes.
To quantify the robustness enhancement, we adapt quantum differential privacy
as a measure of the prediction stability for quantum classifiers. Our results
establish versatile defense strategies for quantum classifiers against
adversarial perturbations, which provide valuable guidance to enhance the
reliability and security for both near-term and future quantum learning
technologies.",2212.02531v1,https://arxiv.org/pdf/2212.02531v1
"PowRL: A Reinforcement Learning Framework for Robust Management of Power
  Networks","Anandsingh Chauhan, Mayank Baranwal, Ansuma Basumatary","Power grids, across the world, play an important societal and economical role
by providing uninterrupted, reliable and transient-free power to several
industries, businesses and household consumers. With the advent of renewable
power resources and EVs resulting into uncertain generation and highly dynamic
load demands, it has become ever so important to ensure robust operation of
power networks through suitable management of transient stability issues and
localize the events of blackouts. In the light of ever increasing stress on the
modern grid infrastructure and the grid operators, this paper presents a
reinforcement learning (RL) framework, PowRL, to mitigate the effects of
unexpected network events, as well as reliably maintain electricity everywhere
on the network at all times. The PowRL leverages a novel heuristic for overload
management, along with the RL-guided decision making on optimal topology
selection to ensure that the grid is operated safely and reliably (with no
overloads). PowRL is benchmarked on a variety of competition datasets hosted by
the L2RPN (Learning to Run a Power Network). Even with its reduced action
space, PowRL tops the leaderboard in the L2RPN NeurIPS 2020 challenge
(Robustness track) at an aggregate level, while also being the top performing
agent in the L2RPN WCCI 2020 challenge. Moreover, detailed analysis depicts
state-of-the-art performances by the PowRL agent in some of the test scenarios.",2212.02397v2,https://arxiv.org/pdf/2212.02397v2
Lossy Compression for Robust Unsupervised Time-Series Anomaly Detection,"Christopher P. Ley, Jorge F. Silva","A new Lossy Causal Temporal Convolutional Neural Network Autoencoder for
anomaly detection is proposed in this work. Our framework uses a
rate-distortion loss and an entropy bottleneck to learn a compressed latent
representation for the task. The main idea of using a rate-distortion loss is
to introduce representation flexibility that ignores or becomes robust to
unlikely events with distinctive patterns, such as anomalies. These anomalies
manifest as unique distortion features that can be accurately detected in
testing conditions. This new architecture allows us to train a fully
unsupervised model that has high accuracy in detecting anomalies from a
distortion score despite being trained with some portion of unlabelled
anomalous data. This setting is in stark contrast to many of the
state-of-the-art unsupervised methodologies that require the model to be only
trained on ""normal data"". We argue that this partially violates the concept of
unsupervised training for anomaly detection as the model uses an informed
decision that selects what is normal from abnormal for training. Additionally,
there is evidence to suggest it also effects the models ability at
generalisation. We demonstrate that models that succeed in the paradigm where
they are only trained on normal data fail to be robust when anomalous data is
injected into the training. In contrast, our compression-based approach
converges to a robust representation that tolerates some anomalous distortion.
The robust representation achieved by a model using a rate-distortion loss can
be used in a more realistic unsupervised anomaly detection scheme.",2212.02303v1,https://arxiv.org/pdf/2212.02303v1
"Bayesian Learning with Information Gain Provably Bounds Risk for a
  Robust Adversarial Defense","Bao Gia Doan, Ehsan Abbasnejad, Javen Qinfeng Shi, Damith C. Ranasinghe","We present a new algorithm to learn a deep neural network model robust
against adversarial attacks. Previous algorithms demonstrate an adversarially
trained Bayesian Neural Network (BNN) provides improved robustness. We
recognize the adversarial learning approach for approximating the multi-modal
posterior distribution of a Bayesian model can lead to mode collapse;
consequently, the model's achievements in robustness and performance are
sub-optimal. Instead, we first propose preventing mode collapse to better
approximate the multi-modal posterior distribution. Second, based on the
intuition that a robust model should ignore perturbations and only consider the
informative content of the input, we conceptualize and formulate an information
gain objective to measure and force the information learned from both benign
and adversarial training instances to be similar. Importantly. we prove and
demonstrate that minimizing the information gain objective allows the
adversarial risk to approach the conventional empirical risk. We believe our
efforts provide a step toward a basis for a principled method of adversarially
training BNNs. Our model demonstrate significantly improved robustness--up to
20%--compared with adversarial training and Adv-BNN under PGD attacks with
0.035 distortion on both CIFAR-10 and STL-10 datasets.",2212.02003v2,https://arxiv.org/pdf/2212.02003v2
ObjectMatch: Robust Registration using Canonical Object Correspondences,"Can Gümeli, Angela Dai, Matthias Nießner","We present ObjectMatch, a semantic and object-centric camera pose estimator
for RGB-D SLAM pipelines. Modern camera pose estimators rely on direct
correspondences of overlapping regions between frames; however, they cannot
align camera frames with little or no overlap. In this work, we propose to
leverage indirect correspondences obtained via semantic object identification.
For instance, when an object is seen from the front in one frame and from the
back in another frame, we can provide additional pose constraints through
canonical object correspondences. We first propose a neural network to predict
such correspondences on a per-pixel level, which we then combine in our energy
formulation with state-of-the-art keypoint matching solved with a joint
Gauss-Newton optimization. In a pairwise setting, our method improves
registration recall of state-of-the-art feature matching, including from 24% to
45% in pairs with 10% or less inter-frame overlap. In registering RGB-D
sequences, our method outperforms cutting-edge SLAM baselines in challenging,
low-frame-rate scenarios, achieving more than 35% reduction in trajectory error
in multiple scenes.",2212.01985v2,https://arxiv.org/pdf/2212.01985v2
FedCC: Robust Federated Learning against Model Poisoning Attacks,"Hyejun Jeong, Hamin Son, Seohu Lee, Jayun Hyun, Tai-Myoung Chung","Federated Learning, designed to address privacy concerns in learning models,
introduces a new distributed paradigm that safeguards data privacy but
differentiates the attack surface due to the server's inaccessibility to local
datasets and the change in protection objective--parameters' integrity.
Existing approaches, including robust aggregation algorithms, fail to
effectively filter out malicious clients, especially those with
non-Independently and Identically Distributed data. Furthermore, these
approaches often tackle non-IID data and poisoning attacks separately. To
address both challenges simultaneously, we present FedCC, a simple yet novel
algorithm. It leverages the Centered Kernel Alignment similarity of Penultimate
Layer Representations for clustering, allowing it to identify and filter out
malicious clients by selectively averaging chosen parameters, even in non-IID
data settings. Our extensive experiments demonstrate the effectiveness of FedCC
in mitigating untargeted model poisoning and backdoor attacks. FedCC reduces
the attack confidence to a consistent zero compared to existing outlier
detection-based and first-order statistics-based methods. Specifically, it
significantly minimizes the average degradation of global performance by
65.5\%. We believe that this new perspective of assessing learning models makes
it a valuable contribution to the field of FL model security and privacy. The
code will be made available upon paper acceptance.",2212.01976v2,https://arxiv.org/pdf/2212.01976v2
"Recognizing Object by Components with Human Prior Knowledge Enhances
  Adversarial Robustness of Deep Neural Networks","Xiao Li, Ziqi Wang, Bo Zhang, Fuchun Sun, Xiaolin Hu","Adversarial attacks can easily fool object recognition systems based on deep
neural networks (DNNs). Although many defense methods have been proposed in
recent years, most of them can still be adaptively evaded. One reason for the
weak adversarial robustness may be that DNNs are only supervised by category
labels and do not have part-based inductive bias like the recognition process
of humans. Inspired by a well-known theory in cognitive psychology --
recognition-by-components, we propose a novel object recognition model ROCK
(Recognizing Object by Components with human prior Knowledge). It first
segments parts of objects from images, then scores part segmentation results
with predefined human prior knowledge, and finally outputs prediction based on
the scores. The first stage of ROCK corresponds to the process of decomposing
objects into parts in human vision. The second stage corresponds to the
decision process of the human brain. ROCK shows better robustness than
classical recognition models across various attack settings. These results
encourage researchers to rethink the rationality of currently widely-used
DNN-based object recognition models and explore the potential of part-based
models, once important but recently ignored, for improving robustness.",2212.01806v1,https://arxiv.org/pdf/2212.01806v1
"Utilizing Background Knowledge for Robust Reasoning over Traffic
  Situations","Jiarui Zhang, Filip Ilievski, Aravinda Kollaa, Jonathan Francis, Kaixin Ma, Alessandro Oltramari","Understanding novel situations in the traffic domain requires an intricate
combination of domain-specific and causal commonsense knowledge. Prior work has
provided sufficient perception-based modalities for traffic monitoring, in this
paper, we focus on a complementary research aspect of Intelligent
Transportation: traffic understanding. We scope our study to text-based methods
and datasets given the abundant commonsense knowledge that can be extracted
using language models from large corpus and knowledge graphs. We adopt three
knowledge-driven approaches for zero-shot QA over traffic situations, based on
prior natural language inference methods, commonsense models with knowledge
graph self-supervision, and dense retriever-based models. We constructed two
text-based multiple-choice question answering sets: BDD-QA for evaluating
causal reasoning in the traffic domain and HDT-QA for measuring the possession
of domain knowledge akin to human driving license tests. Among the methods,
Unified-QA reaches the best performance on the BDD-QA dataset with the
adaptation of multiple formats of question answers. Language models trained
with inference information and commonsense knowledge are also good at
predicting the cause and effect in the traffic domain but perform badly at
answering human-driving QA sets. For such sets, DPR+Unified-QA performs the
best due to its efficient knowledge extraction.",2212.07798v1,https://arxiv.org/pdf/2212.07798v1
"Understanding the Robustness of Multi-Exit Models under Common
  Corruptions","Akshay Mehra, Skyler Seto, Navdeep Jaitly, Barry-John Theobald","Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy
and efficiency of deep neural networks (DNNs) by allowing samples to exit the
network before the last layer. However, the effectiveness of MEMs in the
presence of distribution shifts remains largely unexplored. Our work examines
how distribution shifts generated by common image corruptions affect the
accuracy/efficiency of MEMs. We find that under common corruptions,
early-exiting at the first correct exit reduces the inference cost and provides
a significant boost in accuracy ( 10%) over exiting at the last layer. However,
with realistic early-exit strategies, which do not assume knowledge about the
correct exits, MEMs still reduce inference cost but provide a marginal
improvement in accuracy (1%) compared to exiting at the last layer. Moreover,
the presence of distribution shift widens the gap between an MEM's maximum
classification accuracy and realistic early-exit strategies by 5% on average
compared with the gap on in-distribution data. Our empirical analysis shows
that the lack of calibration due to a distribution shift increases the
susceptibility of such early-exit strategies to exit early and increases
misclassification rates. Furthermore, the lack of calibration increases the
inconsistency in the predictions of the model across exits, leading to both
inefficient inference and more misclassifications compared with evaluation on
in-distribution data. Finally, we propose two metrics, underthinking and
overthinking, that quantify the different behavior of practical early-exit
strategy under distribution shifts, and provide insights into improving the
practical utility of MEMs.",2212.01562v1,https://arxiv.org/pdf/2212.01562v1
"Hedging Complexity in Generalization via a Parametric Distributionally
  Robust Optimization Framework","Garud Iyengar, Henry Lam, Tianyu Wang","Empirical risk minimization (ERM) and distributionally robust optimization
(DRO) are popular approaches for solving stochastic optimization problems that
appear in operations management and machine learning. Existing generalization
error bounds for these methods depend on either the complexity of the cost
function or dimension of the random perturbations. Consequently, the
performance of these methods can be poor for high-dimensional problems with
complex objective functions. We propose a simple approach in which the
distribution of random perturbations is approximated using a parametric family
of distributions. This mitigates both sources of complexity; however, it
introduces a model misspecification error. We show that this new source of
error can be controlled by suitable DRO formulations. Our proposed parametric
DRO approach has significantly improved generalization bounds over existing ERM
and DRO methods and parametric ERM for a wide variety of settings. Our method
is particularly effective under distribution shifts and works broadly in
contextual optimization. We also illustrate the superior performance of our
approach on both synthetic and real-data portfolio optimization and regression
tasks.",2212.01518v2,https://arxiv.org/pdf/2212.01518v2
Adaptive Robust Model Predictive Control via Uncertainty Cancellation,"Rohan Sinha, James Harrison, Spencer M. Richards, Marco Pavone","We propose a learning-based robust predictive control algorithm that
compensates for significant uncertainty in the dynamics for a class of
discrete-time systems that are nominally linear with an additive nonlinear
component. Such systems commonly model the nonlinear effects of an unknown
environment on a nominal system. We optimize over a class of nonlinear feedback
policies inspired by certainty equivalent ""estimate-and-cancel"" control laws
pioneered in classical adaptive control to achieve significant performance
improvements in the presence of uncertainties of large magnitude, a setting in
which existing learning-based predictive control algorithms often struggle to
guarantee safety. In contrast to previous work in robust adaptive MPC, our
approach allows us to take advantage of structure (i.e., the numerical
predictions) in the a priori unknown dynamics learned online through function
approximation. Our approach also extends typical nonlinear adaptive control
methods to systems with state and input constraints even when we cannot
directly cancel the additive uncertain function from the dynamics. We apply
contemporary statistical estimation techniques to certify the system's safety
through persistent constraint satisfaction with high probability. Moreover, we
propose using Bayesian meta-learning algorithms that learn calibrated model
priors to help satisfy the assumptions of the control design in challenging
settings. Finally, we show in simulation that our method can accommodate more
significant unknown dynamics terms than existing methods and that the use of
Bayesian meta-learning allows us to adapt to the test environments more
rapidly.",2212.01371v1,https://arxiv.org/pdf/2212.01371v1
Robustness in Fatigue Strength Estimation,"Dorina Weichert, Alexander Kister, Sebastian Houben, Gunar Ernis, Stefan Wrobel","Fatigue strength estimation is a costly manual material characterization
process in which state-of-the-art approaches follow a standardized experiment
and analysis procedure. In this paper, we examine a modular, Machine
Learning-based approach for fatigue strength estimation that is likely to
reduce the number of experiments and, thus, the overall experimental costs.
Despite its high potential, deployment of a new approach in a real-life lab
requires more than the theoretical definition and simulation. Therefore, we
study the robustness of the approach against misspecification of the prior and
discretization of the specified loads. We identify its applicability and its
advantageous behavior over the state-of-the-art methods, potentially reducing
the number of costly experiments.",2212.01136v1,https://arxiv.org/pdf/2212.01136v1
"AGRO: Adversarial Discovery of Error-prone groups for Robust
  Optimization","Bhargavi Paranjape, Pradeep Dasigi, Vivek Srikumar, Luke Zettlemoyer, Hannaneh Hajishirzi","Models trained via empirical risk minimization (ERM) are known to rely on
spurious correlations between labels and task-independent input features,
resulting in poor generalization to distributional shifts. Group
distributionally robust optimization (G-DRO) can alleviate this problem by
minimizing the worst-case loss over a set of pre-defined groups over training
data. G-DRO successfully improves performance of the worst-group, where the
correlation does not hold. However, G-DRO assumes that the spurious
correlations and associated worst groups are known in advance, making it
challenging to apply it to new tasks with potentially multiple unknown spurious
correlations. We propose AGRO -- Adversarial Group discovery for
Distributionally Robust Optimization -- an end-to-end approach that jointly
identifies error-prone groups and improves accuracy on them. AGRO equips G-DRO
with an adversarial slicing model to find a group assignment for training
examples which maximizes worst-case loss over the discovered groups. On the
WILDS benchmark, AGRO results in 8% higher model performance on average on
known worst-groups, compared to prior group discovery approaches used with
G-DRO. AGRO also improves out-of-distribution performance on SST2, QQP, and
MS-COCO -- datasets where potential spurious correlations are as yet
uncharacterized. Human evaluation of ARGO groups shows that they contain
well-defined, yet previously unstudied spurious correlations that lead to model
errors.",2212.00921v2,https://arxiv.org/pdf/2212.00921v2
Learning Robust State Observers using Neural ODEs (longer version),"Keyan Miao, Konstantinos Gatsis","Relying on recent research results on Neural ODEs, this paper presents a
methodology for the design of state observers for nonlinear systems based on
Neural ODEs, learning Luenberger-like observers and their nonlinear extension
(Kazantzis-Kravaris-Luenberger (KKL) observers) for systems with
partially-known nonlinear dynamics and fully unknown nonlinear dynamics,
respectively. In particular, for tuneable KKL observers, the relationship
between the design of the observer and its trade-off between convergence speed
and robustness is analysed and used as a basis for improving the robustness of
the learning-based observer in training. We illustrate the advantages of this
approach in numerical simulations.",2212.00866v2,https://arxiv.org/pdf/2212.00866v2
"Denoising after Entropy-based Debiasing A Robust Training Method for
  Dataset Bias with Noisy Labels","Sumyeong Ahn, Se-Young Yun","Improperly constructed datasets can result in inaccurate inferences. For
instance, models trained on biased datasets perform poorly in terms of
generalization (i.e., dataset bias). Recent debiasing techniques have
successfully achieved generalization performance by underestimating
easy-to-learn samples (i.e., bias-aligned samples) and highlighting
difficult-to-learn samples (i.e., bias-conflicting samples). However, these
techniques may fail owing to noisy labels, because the trained model recognizes
noisy labels as difficult-to-learn and thus highlights them. In this study, we
find that earlier approaches that used the provided labels to quantify
difficulty could be affected by the small proportion of noisy labels.
Furthermore, we find that running denoising algorithms before debiasing is
ineffective because denoising algorithms reduce the impact of
difficult-to-learn samples, including valuable bias-conflicting samples.
Therefore, we propose an approach called denoising after entropy-based
debiasing, i.e., DENEB, which has three main stages. (1) The prejudice model is
trained by emphasizing (bias-aligned, clean) samples, which are selected using
a Gaussian Mixture Model. (2) Using the per-sample entropy from the output of
the prejudice model, the sampling probability of each sample that is
proportional to the entropy is computed. (3) The final model is trained using
existing denoising algorithms with the mini-batches constructed by following
the computed sampling probability. Compared to existing debiasing and denoising
algorithms, our method achieves better debiasing performance on multiple
benchmarks.",2212.01189v1,https://arxiv.org/pdf/2212.01189v1
"Toward Robust Diagnosis: A Contour Attention Preserving Adversarial
  Defense for COVID-19 Detection","Kun Xiang, Xing Zhang, Jinwen She, Jinpeng Liu, Haohan Wang, Shiqi Deng, Shancheng Jiang","As the COVID-19 pandemic puts pressure on healthcare systems worldwide, the
computed tomography image based AI diagnostic system has become a sustainable
solution for early diagnosis. However, the model-wise vulnerability under
adversarial perturbation hinders its deployment in practical situation. The
existing adversarial training strategies are difficult to generalized into
medical imaging field challenged by complex medical texture features. To
overcome this challenge, we propose a Contour Attention Preserving (CAP) method
based on lung cavity edge extraction. The contour prior features are injected
to attention layer via a parameter regularization and we optimize the robust
empirical risk with hybrid distance metric. We then introduce a new
cross-nation CT scan dataset to evaluate the generalization capability of the
adversarial robustness under distribution shift. Experimental results indicate
that the proposed method achieves state-of-the-art performance in multiple
adversarial defense and generalization tasks. The code and dataset are
available at https://github.com/Quinn777/CAP.",2211.16806v1,https://arxiv.org/pdf/2211.16806v1
Robust and Fast Measure of Information via Low-rank Representation,"Yuxin Dong, Tieliang Gong, Shujian Yu, Hong Chen, Chen Li","The matrix-based R\'enyi's entropy allows us to directly quantify information
measures from given data, without explicit estimation of the underlying
probability distribution. This intriguing property makes it widely applied in
statistical inference and machine learning tasks. However, this information
theoretical quantity is not robust against noise in the data, and is
computationally prohibitive in large-scale applications. To address these
issues, we propose a novel measure of information, termed low-rank matrix-based
R\'enyi's entropy, based on low-rank representations of infinitely divisible
kernel matrices. The proposed entropy functional inherits the specialty of of
the original definition to directly quantify information from data, but enjoys
additional advantages including robustness and effective calculation.
Specifically, our low-rank variant is more sensitive to informative
perturbations induced by changes in underlying distributions, while being
insensitive to uninformative ones caused by noises. Moreover, low-rank
R\'enyi's entropy can be efficiently approximated by random projection and
Lanczos iteration techniques, reducing the overall complexity from
$\mathcal{O}(n^3)$ to $\mathcal{O}(n^2 s)$ or even $\mathcal{O}(ns^2)$, where
$n$ is the number of data samples and $s \ll n$. We conduct large-scale
experiments to evaluate the effectiveness of this new information measure,
demonstrating superior results compared to matrix-based R\'enyi's entropy in
terms of both performance and computational efficiency.",2211.16784v1,https://arxiv.org/pdf/2211.16784v1
Soft Alignment Objectives for Robust Adaptation of Language Generation,"Michal Štefánik, Marek Kadlčík, Petr Sojka","Domain adaptation allows generative language models to address specific flaws
caused by the domain shift of their application. However, the traditional
adaptation by further training on in-domain data rapidly weakens the model's
ability to generalize to other domains, making the open-ended deployments of
the adapted models prone to errors. This work introduces novel training
objectives built upon a semantic similarity of the predicted tokens to the
reference.
  Our results show that (1) avoiding the common assumption of a single correct
prediction by constructing the training target from tokens' semantic similarity
can mitigate catastrophic forgetting during domain adaptation, while (2)
preserving the quality of the adaptation, (3) with negligible additions to
compute costs.
  In the broader context, the objectives grounded in a continuous token
similarity pioneer the exploration of the middle ground between the efficient
but na\""{\i}ve exact-match token-level objectives and expressive but
computationally- and resource-intensive sequential objectives.",2211.16550v2,https://arxiv.org/pdf/2211.16550v2
Outlier-Robust Sparse Mean Estimation for Heavy-Tailed Distributions,"Ilias Diakonikolas, Daniel M. Kane, Jasper C. H. Lee, Ankit Pensia","We study the fundamental task of outlier-robust mean estimation for
heavy-tailed distributions in the presence of sparsity. Specifically, given a
small number of corrupted samples from a high-dimensional heavy-tailed
distribution whose mean $\mu$ is guaranteed to be sparse, the goal is to
efficiently compute a hypothesis that accurately approximates $\mu$ with high
probability. Prior work had obtained efficient algorithms for robust sparse
mean estimation of light-tailed distributions. In this work, we give the first
sample-efficient and polynomial-time robust sparse mean estimator for
heavy-tailed distributions under mild moment assumptions. Our algorithm
achieves the optimal asymptotic error using a number of samples scaling
logarithmically with the ambient dimension. Importantly, the sample complexity
of our method is optimal as a function of the failure probability $\tau$,
having an additive $\log(1/\tau)$ dependence. Our algorithm leverages the
stability-based approach from the algorithmic robust statistics literature,
with crucial (and necessary) adaptations required in our setting. Our analysis
may be of independent interest, involving the delicate design of a
(non-spectral) decomposition for positive semi-definite matrices satisfying
certain sparsity properties.",2211.16333v1,https://arxiv.org/pdf/2211.16333v1
"Quantization-aware Interval Bound Propagation for Training Certifiably
  Robust Quantized Neural Networks","Mathias Lechner, Đorđe Žikelić, Krishnendu Chatterjee, Thomas A. Henzinger, Daniela Rus","We study the problem of training and certifying adversarially robust
quantized neural networks (QNNs). Quantization is a technique for making neural
networks more efficient by running them using low-bit integer arithmetic and is
therefore commonly adopted in industry. Recent work has shown that
floating-point neural networks that have been verified to be robust can become
vulnerable to adversarial attacks after quantization, and certification of the
quantized representation is necessary to guarantee robustness. In this work, we
present quantization-aware interval bound propagation (QA-IBP), a novel method
for training robust QNNs. Inspired by advances in robust learning of
non-quantized networks, our training algorithm computes the gradient of an
abstract representation of the actual network. Unlike existing approaches, our
method can handle the discrete semantics of QNNs. Based on QA-IBP, we also
develop a complete verification procedure for verifying the adversarial
robustness of QNNs, which is guaranteed to terminate and produce a correct
answer. Compared to existing approaches, the key advantage of our verification
procedure is that it runs entirely on GPU or other accelerator devices. We
demonstrate experimentally that our approach significantly outperforms existing
methods and establish the new state-of-the-art for training and certifying the
robustness of QNNs.",2211.16187v1,https://arxiv.org/pdf/2211.16187v1
Understanding and Enhancing Robustness of Concept-based Models,"Sanchit Sinha, Mengdi Huai, Jianhui Sun, Aidong Zhang","Rising usage of deep neural networks to perform decision making in critical
applications like medical diagnosis and financial analysis have raised concerns
regarding their reliability and trustworthiness. As automated systems become
more mainstream, it is important their decisions be transparent, reliable and
understandable by humans for better trust and confidence. To this effect,
concept-based models such as Concept Bottleneck Models (CBMs) and
Self-Explaining Neural Networks (SENN) have been proposed which constrain the
latent space of a model to represent high level concepts easily understood by
domain experts in the field. Although concept-based models promise a good
approach to both increasing explainability and reliability, it is yet to be
shown if they demonstrate robustness and output consistent concepts under
systematic perturbations to their inputs. To better understand performance of
concept-based models on curated malicious samples, in this paper, we aim to
study their robustness to adversarial perturbations, which are also known as
the imperceptible changes to the input data that are crafted by an attacker to
fool a well-learned concept-based model. Specifically, we first propose and
analyze different malicious attacks to evaluate the security vulnerability of
concept based models. Subsequently, we propose a potential general adversarial
training-based defense mechanism to increase robustness of these systems to the
proposed malicious attacks. Extensive experiments on one synthetic and two
real-world datasets demonstrate the effectiveness of the proposed attacks and
the defense approach.",2211.16080v1,https://arxiv.org/pdf/2211.16080v1
Robustness Disparities in Face Detection,"Samuel Dooley, George Z. Wei, Tom Goldstein, John P. Dickerson","Facial analysis systems have been deployed by large companies and critiqued
by scholars and activists for the past decade. Many existing algorithmic audits
examine the performance of these systems on later stage elements of facial
analysis systems like facial recognition and age, emotion, or perceived gender
prediction; however, a core component to these systems has been vastly
understudied from a fairness perspective: face detection, sometimes called face
localization. Since face detection is a pre-requisite step in facial analysis
systems, the bias we observe in face detection will flow downstream to the
other components like facial recognition and emotion prediction. Additionally,
no prior work has focused on the robustness of these systems under various
perturbations and corruptions, which leaves open the question of how various
people are impacted by these phenomena. We present the first of its kind
detailed benchmark of face detection systems, specifically examining the
robustness to noise of commercial and academic models. We use both standard and
recently released academic facial datasets to quantitatively analyze trends in
face detection robustness. Across all the datasets and systems, we generally
find that photos of individuals who are $\textit{masculine presenting}$,
$\textit{older}$, of $\textit{darker skin type}$, or have $\textit{dim
lighting}$ are more susceptible to errors than their counterparts in other
identities.",2211.15937v1,https://arxiv.org/pdf/2211.15937v1
On Robust Learning from Noisy Labels: A Permutation Layer Approach,"Salman Alsubaihi, Mohammed Alkhrashi, Raied Aljadaany, Fahad Albalawi, Bernard Ghanem","The existence of label noise imposes significant challenges (e.g., poor
generalization) on the training process of deep neural networks (DNN). As a
remedy, this paper introduces a permutation layer learning approach termed
PermLL to dynamically calibrate the training process of the DNN subject to
instance-dependent and instance-independent label noise. The proposed method
augments the architecture of a conventional DNN by an instance-dependent
permutation layer. This layer is essentially a convex combination of
permutation matrices that is dynamically calibrated for each sample. The
primary objective of the permutation layer is to correct the loss of noisy
samples mitigating the effect of label noise. We provide two variants of PermLL
in this paper: one applies the permutation layer to the model's prediction,
while the other applies it directly to the given noisy label. In addition, we
provide a theoretical comparison between the two variants and show that
previous methods can be seen as one of the variants. Finally, we validate
PermLL experimentally and show that it achieves state-of-the-art performance on
both real and synthetic datasets.",2211.15890v1,https://arxiv.org/pdf/2211.15890v1
Understanding the Impact of Adversarial Robustness on Accuracy Disparity,"Yuzheng Hu, Fan Wu, Hongyang Zhang, Han Zhao","While it has long been empirically observed that adversarial robustness may
be at odds with standard accuracy and may have further disparate impacts on
different classes, it remains an open question to what extent such observations
hold and how the class imbalance plays a role within. In this paper, we attempt
to understand this question of accuracy disparity by taking a closer look at
linear classifiers under a Gaussian mixture model. We decompose the impact of
adversarial robustness into two parts: an inherent effect that will degrade the
standard accuracy on all classes due to the robustness constraint, and the
other caused by the class imbalance ratio, which will increase the accuracy
disparity compared to standard training. Furthermore, we also show that such
effects extend beyond the Gaussian mixture model, by generalizing our data
model to the general family of stable distributions. More specifically, we
demonstrate that while the constraint of adversarial robustness consistently
degrades the standard accuracy in the balanced class setting, the class
imbalance ratio plays a fundamentally different role in accuracy disparity
compared to the Gaussian case, due to the heavy tail of the stable
distribution. We additionally perform experiments on both synthetic and
real-world datasets to corroborate our theoretical findings. Our empirical
results also suggest that the implications may extend to nonlinear models over
real-world datasets. Our code is publicly available on GitHub at
https://github.com/Accuracy-Disparity/AT-on-AD.",2211.15762v2,https://arxiv.org/pdf/2211.15762v2
Establishment of Neural Networks Robust to Label Noise,"Pengwei Yang, Chongyangzi Teng, Jack George Mangos","Label noise is a significant obstacle in deep learning model training. It can
have a considerable impact on the performance of image classification models,
particularly deep neural networks, which are especially susceptible because
they have a strong propensity to memorise noisy labels. In this paper, we have
examined the fundamental concept underlying related label noise approaches. A
transition matrix estimator has been created, and its effectiveness against the
actual transition matrix has been demonstrated. In addition, we examined the
label noise robustness of two convolutional neural network classifiers with
LeNet and AlexNet designs. The two FashionMINIST datasets have revealed the
robustness of both models. We are not efficiently able to demonstrate the
influence of the transition matrix noise correction on robustness enhancements
due to our inability to correctly tune the complex convolutional neural network
model due to time and computing resource constraints. There is a need for
additional effort to fine-tune the neural network model and explore the
precision of the estimated transition model in future research.",2211.15279v3,https://arxiv.org/pdf/2211.15279v3
"CorrectNet: Robustness Enhancement of Analog In-Memory Computing for
  Neural Networks by Error Suppression and Compensation","Amro Eldebiky, Grace Li Zhang, Georg Boecherer, Bing Li, Ulf Schlichtmann","The last decade has witnessed the breakthrough of deep neural networks (DNNs)
in many fields. With the increasing depth of DNNs, hundreds of millions of
multiply-and-accumulate (MAC) operations need to be executed. To accelerate
such operations efficiently, analog in-memory computing platforms based on
emerging devices, e.g., resistive RAM (RRAM), have been introduced. These
acceleration platforms rely on analog properties of the devices and thus suffer
from process variations and noise. Consequently, weights in neural networks
configured into these platforms can deviate from the expected values, which may
lead to feature errors and a significant degradation of inference accuracy. To
address this issue, in this paper, we propose a framework to enhance the
robustness of neural networks under variations and noise. First, a modified
Lipschitz constant regularization is proposed during neural network training to
suppress the amplification of errors propagated through network layers.
Afterwards, error compensation is introduced at necessary locations determined
by reinforcement learning to rescue the feature maps with remaining errors.
Experimental results demonstrate that inference accuracy of neural networks can
be recovered from as low as 1.69% under variations and noise back to more than
95% of their original accuracy, while the training and hardware cost are
negligible.",2211.14917v1,https://arxiv.org/pdf/2211.14917v1
"Deep Multi-Emitter Spectrum Occupancy Mapping that is Robust to the
  Number of Sensors, Noise and Threshold","Abbas Termos, Bertrand Hochwald","One of the primary goals in spectrum occupancy mapping is to create a system
that is robust to assumptions about the number of sensors, occupancy threshold
(in dBm), sensor noise, number of emitters and the propagation environment. We
show that such a system may be designed with neural networks using a process of
aggregation to allow a variable number of sensors during training and testing.
This process transforms the variable number of measurements into approximate
log-likelihood ratios (LLRs), which are fed as a fixed-resolution image into a
neural network. The use of LLR's provides robustness to the effects of noise
and occupancy threshold. In other words, a system may be trained for a nominal
number of sensors, threshold and noise levels, and still operate well at
various other levels without retraining. Our system operates without knowledge
of the number of emitters and does not explicitly attempt to estimate their
number or power. Receiver operating curves with realistic propagation
environments using topographic maps with commercial network design tools show
how performance of the neural network varies with the environment. The use of
very low-resolution sensors in this system can still yield good performance.",2212.10444v2,https://arxiv.org/pdf/2212.10444v2
"Domain Generalization for Robust Model-Based Offline Reinforcement
  Learning","Alan Clark, Shoaib Ahmed Siddiqui, Robert Kirk, Usman Anwar, Stephen Chung, David Krueger","Existing offline reinforcement learning (RL) algorithms typically assume that
training data is either: 1) generated by a known policy, or 2) of entirely
unknown origin. We consider multi-demonstrator offline RL, a middle ground
where we know which demonstrators generated each dataset, but make no
assumptions about the underlying policies of the demonstrators. This is the
most natural setting when collecting data from multiple human operators, yet
remains unexplored. Since different demonstrators induce different data
distributions, we show that this can be naturally framed as a domain
generalization problem, with each demonstrator corresponding to a different
domain. Specifically, we propose Domain-Invariant Model-based Offline RL
(DIMORL), where we apply Risk Extrapolation (REx) (Krueger et al., 2020) to the
process of learning dynamics and rewards models. Our results show that models
trained with REx exhibit improved domain generalization performance when
compared with the natural baseline of pooling all demonstrators' data. We
observe that the resulting models frequently enable the learning of superior
policies in the offline model-based RL setting, can improve the stability of
the policy learning process, and potentially enable increased exploration.",2211.14827v1,https://arxiv.org/pdf/2211.14827v1
"Navigation as Attackers Wish? Towards Building Robust Embodied Agents
  under Federated Learning","Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Eric Wang","Federated embodied agent learning protects the data privacy of individual
visual environments by keeping data locally at each client (the individual
environment) during training. However, since the local data is inaccessible to
the server under federated learning, attackers may easily poison the training
data of the local client to build a backdoor in the agent without notice.
Deploying such an agent raises the risk of potential harm to humans, as the
attackers may easily navigate and control the agent as they wish via the
backdoor. Towards Byzantine-robust federated embodied agent learning, in this
paper, we study the attack and defense for the task of vision-and-language
navigation (VLN), where the agent is required to follow natural language
instructions to navigate indoor environments. First, we introduce a simple but
effective attack strategy, Navigation as Wish (NAW), in which the malicious
client manipulates local trajectory data to implant a backdoor into the global
model. Results on two VLN datasets (R2R and RxR) show that NAW can easily
navigate the deployed VLN agent regardless of the language instruction, without
affecting its performance on normal test sets. Then, we propose a new
Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated
VLN, which provides the server with a ''prompt'' of the vision-and-language
alignment variance between the benign and malicious clients so that they can be
distinguished during training. We validate the effectiveness of the PBA method
on protecting the global model from the NAW attack, which outperforms other
state-of-the-art defense methods by a large margin in the defense metrics on
R2R and RxR.",2211.14769v4,https://arxiv.org/pdf/2211.14769v4
"Supervised Contrastive Prototype Learning: Augmentation Free Robust
  Neural Network","Iordanis Fostiropoulos, Laurent Itti","Transformations in the input space of Deep Neural Networks (DNN) lead to
unintended changes in the feature space. Almost perceptually identical inputs,
such as adversarial examples, can have significantly distant feature
representations. On the contrary, Out-of-Distribution (OOD) samples can have
highly similar feature representations to training set samples. Our theoretical
analysis for DNNs trained with a categorical classification head suggests that
the inflexible logit space restricted by the classification problem size is one
of the root causes for the lack of $\textit{robustness}$. Our second
observation is that DNNs over-fit to the training augmentation technique and do
not learn $\textit{nuance invariant}$ representations. Inspired by the recent
success of prototypical and contrastive learning frameworks for both improving
robustness and learning nuance invariant representations, we propose a training
framework, $\textbf{Supervised Contrastive Prototype Learning}$ (SCPL). We use
N-pair contrastive loss with prototypes of the same and opposite classes and
replace a categorical classification head with a $\textbf{Prototype
Classification Head}$ (PCH). Our approach is $\textit{sample efficient}$, does
not require $\textit{sample mining}$, can be implemented on any existing DNN
without modification to their architecture, and combined with other training
augmentation techniques. We empirically evaluate the $\textbf{clean}$
robustness of our method on out-of-distribution and adversarial samples. Our
framework outperforms other state-of-the-art contrastive and prototype learning
approaches in $\textit{robustness}$.",2211.14424v1,https://arxiv.org/pdf/2211.14424v1
Doubly robust nearest neighbors in factor models,"Raaz Dwivedi, Katherine Tian, Sabina Tomkins, Predrag Klasnja, Susan Murphy, Devavrat Shah","We introduce and analyze an improved variant of nearest neighbors (NN) for
estimation with missing data in latent factor models. We consider a matrix
completion problem with missing data, where the $(i, t)$-th entry, when
observed, is given by its mean $f(u_i, v_t)$ plus mean-zero noise for an
unknown function $f$ and latent factors $u_i$ and $v_t$. Prior NN strategies,
like unit-unit NN, for estimating the mean $f(u_i, v_t)$ relies on existence of
other rows $j$ with $u_j \approx u_i$. Similarly, time-time NN strategy relies
on existence of columns $t'$ with $v_{t'} \approx v_t$. These strategies
provide poor performance respectively when similar rows or similar columns are
not available. Our estimate is doubly robust to this deficit in two ways: (1)
As long as there exist either good row or good column neighbors, our estimate
provides a consistent estimate. (2) Furthermore, if both good row and good
column neighbors exist, it provides a (near-)quadratic improvement in the
non-asymptotic error and admits a significantly narrower asymptotic confidence
interval when compared to both unit-unit or time-time NN.",2211.14297v3,https://arxiv.org/pdf/2211.14297v3
Towards Good Practices for Missing Modality Robust Action Recognition,"Sangmin Woo, Sumin Lee, Yeonju Park, Muhammad Adi Nugroho, Changick Kim","Standard multi-modal models assume the use of the same modalities in training
and inference stages. However, in practice, the environment in which
multi-modal models operate may not satisfy such assumption. As such, their
performances degrade drastically if any modality is missing in the inference
stage. We ask: how can we train a model that is robust to missing modalities?
This paper seeks a set of good practices for multi-modal action recognition,
with a particular interest in circumstances where some modalities are not
available at an inference time. First, we study how to effectively regularize
the model during training (e.g., data augmentation). Second, we investigate on
fusion methods for robustness to missing modalities: we find that
transformer-based fusion shows better robustness for missing modality than
summation or concatenation. Third, we propose a simple modular network,
ActionMAE, which learns missing modality predictive coding by randomly dropping
modality features and tries to reconstruct them with the remaining modality
features. Coupling these good practices, we build a model that is not only
effective in multi-modal action recognition but also robust to modality
missing. Our model achieves the state-of-the-arts on multiple benchmarks and
maintains competitive performances even in missing modality scenarios. Codes
are available at https://github.com/sangminwoo/ActionMAE.",2211.13916v2,https://arxiv.org/pdf/2211.13916v2
On Pitfalls of Measuring Occlusion Robustness through Data Distortion,Antonia Marcu,"Over the past years, the crucial role of data has largely been shadowed by
the field's focus on architectures and training procedures. We often cause
changes to the data without being aware of their wider implications. In this
paper we show that distorting images without accounting for the artefacts
introduced leads to biased results when establishing occlusion robustness. To
ensure models behave as expected in real-world scenarios, we need to rule out
the impact added artefacts have on evaluation. We propose a new approach,
iOcclusion, as a fairer alternative for applications where the possible
occluders are unknown.",2211.13734v1,https://arxiv.org/pdf/2211.13734v1
"Prototypical Fine-tuning: Towards Robust Performance Under Varying Data
  Sizes","Yiqiao Jin, Xiting Wang, Yaru Hao, Yizhou Sun, Xing Xie","In this paper, we move towards combining large parametric models with
non-parametric prototypical networks. We propose prototypical fine-tuning, a
novel prototypical framework for fine-tuning pretrained language models (LM),
which automatically learns a bias to improve predictive performance for varying
data sizes, especially low-resource settings. Our prototypical fine-tuning
approach can automatically adjust the model capacity according to the number of
data points and the model's inherent attributes. Moreover, we propose four
principles for effective prototype fine-tuning towards the optimal solution.
Experimental results across various datasets show that our work achieves
significant performance improvements under various low-resource settings, as
well as comparable and usually better performances in high-resource scenarios.",2211.13638v1,https://arxiv.org/pdf/2211.13638v1
Robustness Analysis of Deep Learning Models for Population Synthesis,"Daniel Opoku Mensah, Godwin Badu-Marfo, Bilal Farooq","Deep generative models have become useful for synthetic data generation,
particularly population synthesis. The models implicitly learn the probability
distribution of a dataset and can draw samples from a distribution. Several
models have been proposed, but their performance is only tested on a single
cross-sectional sample. The implementation of population synthesis on single
datasets is seen as a drawback that needs further studies to explore the
robustness of the models on multiple datasets. While comparing with the real
data can increase trust and interpretability of the models, techniques to
evaluate deep generative models' robustness for population synthesis remain
underexplored. In this study, we present bootstrap confidence interval for the
deep generative models, an approach that computes efficient confidence
intervals for mean errors predictions to evaluate the robustness of the models
to multiple datasets. Specifically, we adopt the tabular-based Composite Travel
Generative Adversarial Network (CTGAN) and Variational Autoencoder (VAE), to
estimate the distribution of the population, by generating agents that have
tabular data using several samples over time from the same study area. The
models are implemented on multiple travel diaries of Montreal Origin-
Destination Survey of 2008, 2013, and 2018 and compare the predictive
performance under varying sample sizes from multiple surveys. Results show that
the predictive errors of CTGAN have narrower confidence intervals indicating
its robustness to multiple datasets of the varying sample sizes when compared
to VAE. Again, the evaluation of model robustness against varying sample size
shows a minimal decrease in model performance with decrease in sample size.
This study directly supports agent-based modelling by enabling finer synthetic
generation of populations in a reliable environment.",2211.13339v1,https://arxiv.org/pdf/2211.13339v1
Group SELFIES: A Robust Fragment-Based Molecular String Representation,"Austin Cheng, Andy Cai, Santiago Miret, Gustavo Malkomes, Mariano Phielipp, Alán Aspuru-Guzik","We introduce Group SELFIES, a molecular string representation that leverages
group tokens to represent functional groups or entire substructures while
maintaining chemical robustness guarantees. Molecular string representations,
such as SMILES and SELFIES, serve as the basis for molecular generation and
optimization in chemical language models, deep generative models, and
evolutionary methods. While SMILES and SELFIES leverage atomic representations,
Group SELFIES builds on top of the chemical robustness guarantees of SELFIES by
enabling group tokens, thereby creating additional flexibility to the
representation. Moreover, the group tokens in Group SELFIES can take advantage
of inductive biases of molecular fragments that capture meaningful chemical
motifs. The advantages of capturing chemical motifs and flexibility are
demonstrated in our experiments, which show that Group SELFIES improves
distribution learning of common molecular datasets. Further experiments also
show that random sampling of Group SELFIES strings improves the quality of
generated molecules compared to regular SELFIES strings. Our open-source
implementation of Group SELFIES is available online, which we hope will aid
future research in molecular generation and optimization.",2211.13322v1,https://arxiv.org/pdf/2211.13322v1
CoMadOut -- A Robust Outlier Detection Algorithm based on CoMAD,"Andreas Lohrer, Daniyal Kazempour, Maximilian Hünemörder, Peer Kröger","Unsupervised learning methods are well established in the area of anomaly
detection and achieve state of the art performances on outlier datasets.
Outliers play a significant role, since they bear the potential to distort the
predictions of a machine learning algorithm on a given dataset. Especially
among PCA-based methods, outliers have an additional destructive potential
regarding the result: they may not only distort the orientation and translation
of the principal components, they also make it more complicated to detect
outliers. To address this problem, we propose the robust outlier detection
algorithm CoMadOut, which satisfies two required properties: (1) being robust
towards outliers and (2) detecting them. Our CoMadOut outlier detection
variants using comedian PCA define, dependent on its variant, an inlier region
with a robust noise margin by measures of in-distribution (variant CMO) and
optimized scores by measures of out-of-distribution (variants CMO*), e.g.
kurtosis-weighting by CMO+k. These measures allow distribution based outlier
scoring for each principal component, and thus, an appropriate alignment of the
degree of outlierness between normal and abnormal instances. Experiments
comparing CoMadOut with traditional, deep and other comparable robust outlier
detection methods showed that the performance of the introduced CoMadOut
approach is competitive to well established methods related to average
precision (AP), area under the precision recall curve (AUPRC) and area under
the receiver operating characteristic (AUROC) curve. In summary our approach
can be seen as a robust alternative for outlier detection tasks.",2211.13314v2,https://arxiv.org/pdf/2211.13314v2
"Reliable Robustness Evaluation via Automatically Constructed Attack
  Ensembles","Shengcai Liu, Fu Peng, Ke Tang","Attack Ensemble (AE), which combines multiple attacks together, provides a
reliable way to evaluate adversarial robustness. In practice, AEs are often
constructed and tuned by human experts, which however tends to be sub-optimal
and time-consuming. In this work, we present AutoAE, a conceptually simple
approach for automatically constructing AEs. In brief, AutoAE repeatedly adds
the attack and its iteration steps to the ensemble that maximizes ensemble
improvement per additional iteration consumed. We show theoretically that
AutoAE yields AEs provably within a constant factor of the optimal for a given
defense. We then use AutoAE to construct two AEs for $l_{\infty}$ and $l_2$
attacks, and apply them without any tuning or adaptation to 45 top adversarial
defenses on the RobustBench leaderboard. In all except one cases we achieve
equal or better (often the latter) robustness evaluation than existing AEs, and
notably, in 29 cases we achieve better robustness evaluation than the best
known one. Such performance of AutoAE shows itself as a reliable evaluation
protocol for adversarial robustness, which further indicates the huge potential
of automatic AE construction. Code is available at
\url{https://github.com/LeegerPENG/AutoAE}.",2211.12713v1,https://arxiv.org/pdf/2211.12713v1
Subgroup Robustness Grows On Trees: An Empirical Baseline Investigation,"Josh Gardner, Zoran Popović, Ludwig Schmidt","Researchers have proposed many methods for fair and robust machine learning,
but comprehensive empirical evaluation of their subgroup robustness is lacking.
In this work, we address this gap in the context of tabular data, where
sensitive subgroups are clearly-defined, real-world fairness problems abound,
and prior works often do not compare to state-of-the-art tree-based models as
baselines. We conduct an empirical comparison of several previously-proposed
methods for fair and robust learning alongside state-of-the-art tree-based
methods and other baselines. Via experiments with more than $340{,}000$ model
configurations on eight datasets, we show that tree-based methods have strong
subgroup robustness, even when compared to robustness- and fairness-enhancing
methods. Moreover, the best tree-based models tend to show good performance
over a range of metrics, while robust or group-fair models can show
brittleness, with significant performance differences across different metrics
for a fixed model. We also demonstrate that tree-based models show less
sensitivity to hyperparameter configurations, and are less costly to train. Our
work suggests that tree-based ensemble models make an effective baseline for
tabular data, and are a sensible default when subgroup robustness is desired.
For associated code and detailed results, see
https://github.com/jpgard/subgroup-robustness-grows-on-trees .",2211.12703v2,https://arxiv.org/pdf/2211.12703v2
Benchmarking Adversarially Robust Quantum Machine Learning at Scale,"Maxwell T. West, Sarah M. Erfani, Christopher Leckie, Martin Sevior, Lloyd C. L. Hollenberg, Muhammad Usman","Machine learning (ML) methods such as artificial neural networks are rapidly
becoming ubiquitous in modern science, technology and industry. Despite their
accuracy and sophistication, neural networks can be easily fooled by carefully
designed malicious inputs known as adversarial attacks. While such
vulnerabilities remain a serious challenge for classical neural networks, the
extent of their existence is not fully understood in the quantum ML setting. In
this work, we benchmark the robustness of quantum ML networks, such as quantum
variational classifiers (QVC), at scale by performing rigorous training for
both simple and complex image datasets and through a variety of high-end
adversarial attacks. Our results show that QVCs offer a notably enhanced
robustness against classical adversarial attacks by learning features which are
not detected by the classical neural networks, indicating a possible quantum
advantage for ML tasks. Contrarily, and remarkably, the converse is not true,
with attacks on quantum networks also capable of deceiving classical neural
networks. By combining quantum and classical network outcomes, we propose a
novel adversarial attack detection technology. Traditionally quantum advantage
in ML systems has been sought through increased accuracy or algorithmic
speed-up, but our work has revealed the potential for a new kind of quantum
advantage through superior robustness of ML models, whose practical realisation
will address serious security concerns and reliability issues of ML algorithms
employed in a myriad of applications including autonomous vehicles,
cybersecurity, and surveillance robotic systems.",2211.12681v1,https://arxiv.org/pdf/2211.12681v1
"Improving Robust Generalization by Direct PAC-Bayesian Bound
  Minimization","Zifan Wang, Nan Ding, Tomer Levinboim, Xi Chen, Radu Soricut","Recent research in robust optimization has shown an overfitting-like
phenomenon in which models trained against adversarial attacks exhibit higher
robustness on the training set compared to the test set. Although previous work
provided theoretical explanations for this phenomenon using a robust
PAC-Bayesian bound over the adversarial test error, related algorithmic
derivations are at best only loosely connected to this bound, which implies
that there is still a gap between their empirical success and our understanding
of adversarial robustness theory. To close this gap, in this paper we consider
a different form of the robust PAC-Bayesian bound and directly minimize it with
respect to the model posterior. The derivation of the optimal solution connects
PAC-Bayesian learning to the geometry of the robust loss surface through a
Trace of Hessian (TrH) regularizer that measures the surface flatness. In
practice, we restrict the TrH regularizer to the top layer only, which results
in an analytical solution to the bound whose computational cost does not depend
on the network depth. Finally, we evaluate our TrH regularization approach over
CIFAR-10/100 and ImageNet using Vision Transformers (ViT) and compare against
baseline adversarial robustness algorithms. Experimental results show that TrH
regularization leads to improved ViT robustness that either matches or
surpasses previous state-of-the-art approaches while at the same time requires
less memory and computational cost.",2211.12624v1,https://arxiv.org/pdf/2211.12624v1
Robustness of Physics-Informed Neural Networks to Noise in Sensor Data,"Jian Cheng Wong, Pao-Hsiung Chiu, Chin Chun Ooi, My Ha Da","Physics-Informed Neural Networks (PINNs) have been shown to be an effective
way of incorporating physics-based domain knowledge into neural network models
for many important real-world systems. They have been particularly effective as
a means of inferring system information based on data, even in cases where data
is scarce. Most of the current work however assumes the availability of
high-quality data. In this work, we further conduct a preliminary investigation
of the robustness of physics-informed neural networks to the magnitude of noise
in the data. Interestingly, our experiments reveal that the inclusion of
physics in the neural network is sufficient to negate the impact of noise in
data originating from hypothetical low quality sensors with high
signal-to-noise ratios of up to 1. The resultant predictions for this test case
are seen to still match the predictive value obtained for equivalent data
obtained from high-quality sensors with potentially 10x less noise. This
further implies the utility of physics-informed neural network modeling for
making sense of data from sensor networks in the future, especially with the
advent of Industry 4.0 and the increasing trend towards ubiquitous deployment
of low-cost sensors which are typically noisier.",2211.12042v1,https://arxiv.org/pdf/2211.12042v1
Robust High-dimensional Tuning Free Multiple Testing,"Jianqing Fan, Zhipeng Lou, Mengxin Yu","A stylized feature of high-dimensional data is that many variables have heavy
tails, and robust statistical inference is critical for valid large-scale
statistical inference. Yet, the existing developments such as Winsorization,
Huberization and median of means require the bounded second moments and involve
variable-dependent tuning parameters, which hamper their fidelity in
applications to large-scale problems. To liberate these constraints, this paper
revisits the celebrated Hodges-Lehmann (HL) estimator for estimating location
parameters in both the one- and two-sample problems, from a non-asymptotic
perspective. Our study develops Berry-Esseen inequality and Cram\'{e}r type
moderate deviation for the HL estimator based on newly developed non-asymptotic
Bahadur representation, and builds data-driven confidence intervals via a
weighted bootstrap approach. These results allow us to extend the HL estimator
to large-scale studies and propose \emph{tuning-free} and \emph{moment-free}
high-dimensional inference procedures for testing global null and for
large-scale multiple testing with false discovery proportion control. It is
convincingly shown that the resulting tuning-free and moment-free methods
control false discovery proportion at a prescribed level. The simulation
studies lend further support to our developed theory.",2211.11959v2,https://arxiv.org/pdf/2211.11959v2
Dynamic Loss For Robust Learning,"Shenwang Jiang, Jianan Li, Jizhou Zhang, Ying Wang, Tingfa Xu","Label noise and class imbalance commonly coexist in real-world data. Previous
works for robust learning, however, usually address either one type of the data
biases and underperform when facing them both. To mitigate this gap, this work
presents a novel meta-learning based dynamic loss that automatically adjusts
the objective functions with the training process to robustly learn a
classifier from long-tailed noisy data. Concretely, our dynamic loss comprises
a label corrector and a margin generator, which respectively correct noisy
labels and generate additive per-class classification margins by perceiving the
underlying data distribution as well as the learning state of the classifier.
Equipped with a new hierarchical sampling strategy that enriches a small amount
of unbiased metadata with diverse and hard samples, the two components in the
dynamic loss are optimized jointly through meta-learning and cultivate the
classifier to well adapt to clean and balanced test data. Extensive experiments
show our method achieves state-of-the-art accuracy on multiple real-world and
synthetic datasets with various types of data biases, including CIFAR-10/100,
Animal-10N, ImageNet-LT, and Webvision. Code will soon be publicly available.",2211.12506v2,https://arxiv.org/pdf/2211.12506v2
CLAWSAT: Towards Both Robust and Accurate Code Models,"Jinghan Jia, Shashank Srikant, Tamara Mitrovska, Chuang Gan, Shiyu Chang, Sijia Liu, Una-May O'Reilly","We integrate contrastive learning (CL) with adversarial learning to
co-optimize the robustness and accuracy of code models. Different from existing
works, we show that code obfuscation, a standard code transformation operation,
provides novel means to generate complementary `views' of a code that enable us
to achieve both robust and accurate code models. To the best of our knowledge,
this is the first systematic study to explore and exploit the robustness and
accuracy benefits of (multi-view) code obfuscations in code models.
Specifically, we first adopt adversarial codes as robustness-promoting views in
CL at the self-supervised pre-training phase. This yields improved robustness
and transferability for downstream tasks. Next, at the supervised fine-tuning
stage, we show that adversarial training with a proper temporally-staggered
schedule of adversarial code generation can further improve robustness and
accuracy of the pre-trained code model. Built on the above two modules, we
develop CLAWSAT, a novel self-supervised learning (SSL) framework for code by
integrating $\underline{\textrm{CL}}$ with $\underline{\textrm{a}}$dversarial
vie$\underline{\textrm{w}}$s (CLAW) with $\underline{\textrm{s}}$taggered
$\underline{\textrm{a}}$dversarial $\underline{\textrm{t}}$raining (SAT). On
evaluating three downstream tasks across Python and Java, we show that CLAWSAT
consistently yields the best robustness and accuracy ($\textit{e.g.}$ 11$\%$ in
robustness and 6$\%$ in accuracy on the code summarization task in Python). We
additionally demonstrate the effectiveness of adversarial learning in CLAW by
analyzing the characteristics of the loss landscape and interpretability of the
pre-trained models.",2211.11711v5,https://arxiv.org/pdf/2211.11711v5
"DualApp: Tight Over-Approximation for Neural Network Robustness
  Verification via Under-Approximation","Yiting Wu, Zhaodi Zhang, Zhiyi Xue, Si Liu, Min Zhang","The robustness of neural networks is fundamental to the hosting system's
reliability and security. Formal verification has been proven to be effective
in providing provable robustness guarantees. To improve the verification
scalability, over-approximating the non-linear activation functions in neural
networks by linear constraints is widely adopted, which transforms the
verification problem into an efficiently solvable linear programming problem.
As over-approximations inevitably introduce overestimation, many efforts have
been dedicated to defining the tightest possible approximations. Recent studies
have however showed that the existing so-called tightest approximations are
superior to each other. In this paper we identify and report an crucial factor
in defining tight approximations, namely the approximation domains of
activation functions. We observe that existing approaches only rely on
overestimated domains, while the corresponding tight approximation may not
necessarily be tight on its actual domain. We propose a novel
under-approximation-guided approach, called dual-approximation, to define tight
over-approximations and two complementary under-approximation algorithms based
on sampling and gradient descent. The overestimated domain guarantees the
soundness while the underestimated one guides the tightness. We implement our
approach into a tool called DualApp and extensively evaluate it on a
comprehensive benchmark of 84 collected and trained neural networks with
different architectures. The experimental results show that DualApp outperforms
the state-of-the-art approximation-based approaches, with up to 71.22%
improvement to the verification result.",2211.11186v1,https://arxiv.org/pdf/2211.11186v1
"Simultaneously Learning Robust Audio Embeddings and balanced Hash codes
  for Query-by-Example","Anup Singh, Kris Demuynck, Vipul Arora","Audio fingerprinting systems must efficiently and robustly identify query
snippets in an extensive database. To this end, state-of-the-art systems use
deep learning to generate compact audio fingerprints. These systems deploy
indexing methods, which quantize fingerprints to hash codes in an unsupervised
manner to expedite the search. However, these methods generate imbalanced hash
codes, leading to their suboptimal performance. Therefore, we propose a
self-supervised learning framework to compute fingerprints and balanced hash
codes in an end-to-end manner to achieve both fast and accurate retrieval
performance. We model hash codes as a balanced clustering process, which we
regard as an instance of the optimal transport problem. Experimental results
indicate that the proposed approach improves retrieval efficiency while
preserving high accuracy, particularly at high distortion levels, compared to
the competing methods. Moreover, our system is efficient and scalable in
computational load and memory storage.",2211.11060v2,https://arxiv.org/pdf/2211.11060v2
"Revealing Robust Oil and Gas Company Macro-Strategies using Deep
  Multi-Agent Reinforcement Learning","Dylan Radovic, Lucas Kruitwagen, Christian Schroeder de Witt, Ben Caldecott, Shane Tomlinson, Mark Workman","The energy transition potentially poses an existential risk for major
international oil companies (IOCs) if they fail to adapt to low-carbon business
models. Projections of energy futures, however, are met with diverging
assumptions on its scale and pace, causing disagreement among IOC
decision-makers and their stakeholders over what the business model of an
incumbent fossil fuel company should be. In this work, we used deep multi-agent
reinforcement learning to solve an energy systems wargame wherein players
simulate IOC decision-making, including hydrocarbon and low-carbon investments
decisions, dividend policies, and capital structure measures, through an
uncertain energy transition to explore critical and non-linear governance
questions, from leveraged transitions to reserve replacements. Adversarial play
facilitated by state-of-the-art algorithms revealed decision-making strategies
robust to energy transition uncertainty and against multiple IOCs. In all
games, robust strategies emerged in the form of low-carbon business models as a
result of early transition-oriented movement. IOCs adopting such strategies
outperformed business-as-usual and delayed transition strategies regardless of
hydrocarbon demand projections. In addition to maximizing value, these
strategies benefit greater society by contributing substantial amounts of
capital necessary to accelerate the global low-carbon energy transition. Our
findings point towards the need for lenders and investors to effectively
mobilize transition-oriented finance and engage with IOCs to ensure responsible
reallocation of capital towards low-carbon business models that would enable
the emergence of fossil fuel incumbents as future low-carbon leaders.",2211.11043v1,https://arxiv.org/pdf/2211.11043v1
Spectral Adversarial Training for Robust Graph Neural Network,"Jintang Li, Jiaying Peng, Liang Chen, Zibin Zheng, Tingting Liang, Qing Ling","Recent studies demonstrate that Graph Neural Networks (GNNs) are vulnerable
to slight but adversarially designed perturbations, known as adversarial
examples. To address this issue, robust training methods against adversarial
examples have received considerable attention in the literature.
\emph{Adversarial Training (AT)} is a successful approach to learning a robust
model using adversarially perturbed training samples. Existing AT methods on
GNNs typically construct adversarial perturbations in terms of graph structures
or node features. However, they are less effective and fraught with challenges
on graph data due to the discreteness of graph structure and the relationships
between connected examples. In this work, we seek to address these challenges
and propose Spectral Adversarial Training (SAT), a simple yet effective
adversarial training approach for GNNs. SAT first adopts a low-rank
approximation of the graph structure based on spectral decomposition, and then
constructs adversarial perturbations in the spectral domain rather than
directly manipulating the original graph structure. To investigate its
effectiveness, we employ SAT on three widely used GNNs. Experimental results on
four public graph datasets demonstrate that SAT significantly improves the
robustness of GNNs against adversarial attacks without sacrificing
classification accuracy and training efficiency.",2211.10896v1,https://arxiv.org/pdf/2211.10896v1
On Multi-head Ensemble of Smoothed Classifiers for Certified Robustness,"Kun Fang, Qinghua Tao, Yingwen Wu, Tao Li, Xiaolin Huang, Jie Yang","Randomized Smoothing (RS) is a promising technique for certified robustness,
and recently in RS the ensemble of multiple deep neural networks (DNNs) has
shown state-of-the-art performances. However, such an ensemble brings heavy
computation burdens in both training and certification, and yet under-exploits
individual DNNs and their mutual effects, as the communication between these
classifiers is commonly ignored in optimization. In this work, starting from a
single DNN, we augment the network with multiple heads, each of which pertains
a classifier for the ensemble. A novel training strategy, namely Self-PAced
Circular-TEaching (SPACTE), is proposed accordingly. SPACTE enables a circular
communication flow among those augmented heads, i.e., each head teaches its
neighbor with the self-paced learning using smoothed losses, which are
specifically designed in relation to certified robustness. The deployed
multi-head structure and the circular-teaching scheme of SPACTE jointly
contribute to diversify and enhance the classifiers in augmented heads for
ensemble, leading to even stronger certified robustness than ensembling
multiple DNNs (effectiveness) at the cost of much less computational expenses
(efficiency), verified by extensive experiments and discussions.",2211.10882v1,https://arxiv.org/pdf/2211.10882v1
"Denoising Multi-Similarity Formulation: A Self-paced Curriculum-Driven
  Approach for Robust Metric Learning","Chenkang Zhang, Lei Luo, Bin Gu","Deep Metric Learning (DML) is a group of techniques that aim to measure the
similarity between objects through the neural network. Although the number of
DML methods has rapidly increased in recent years, most previous studies cannot
effectively handle noisy data, which commonly exists in practical applications
and often leads to serious performance deterioration. To overcome this
limitation, in this paper, we build a connection between noisy samples and hard
samples in the framework of self-paced learning, and propose a
\underline{B}alanced \underline{S}elf-\underline{P}aced \underline{M}etric
\underline{L}earning (BSPML) algorithm with a denoising multi-similarity
formulation, where noisy samples are treated as extremely hard samples and
adaptively excluded from the model training by sample weighting. Especially,
due to the pairwise relationship and a new balance regularization term, the
sub-problem \emph{w.r.t.} sample weights is a nonconvex quadratic function. To
efficiently solve this nonconvex quadratic problem, we propose a doubly
stochastic projection coordinate gradient algorithm. Importantly, we
theoretically prove the convergence not only for the doubly stochastic
projection coordinate gradient algorithm, but also for our BSPML algorithm.
Experimental results on several standard data sets demonstrate that our BSPML
algorithm has better generalization ability and robustness than the
state-of-the-art robust DML approaches.",2211.11751v2,https://arxiv.org/pdf/2211.11751v2
Towards Adversarial Robustness of Deep Vision Algorithms,Hanshu Yan,"Deep learning methods have achieved great success in solving computer vision
tasks, and they have been widely utilized in artificially intelligent systems
for image processing, analysis, and understanding. However, deep neural
networks have been shown to be vulnerable to adversarial perturbations in input
data. The security issues of deep neural networks have thus come to the fore.
It is imperative to study the adversarial robustness of deep vision algorithms
comprehensively. This talk focuses on the adversarial robustness of image
classification models and image denoisers. We will discuss the robustness of
deep vision algorithms from three perspectives: 1) robustness evaluation (we
propose the ObsAtk to evaluate the robustness of denoisers), 2) robustness
improvement (HAT, TisODE, and CIFS are developed to robustify vision models),
and 3) the connection between adversarial robustness and generalization
capability to new domains (we find that adversarially robust denoisers can deal
with unseen types of real-world noise).",2211.10670v2,https://arxiv.org/pdf/2211.10670v2
Filterbank Learning for Noise-Robust Small-Footprint Keyword Spotting,"Iván López-Espejo, Ram C. M. C. Shekar, Zheng-Hua Tan, Jesper Jensen, John H. L. Hansen","In the context of keyword spotting (KWS), the replacement of handcrafted
speech features by learnable features has not yielded superior KWS performance.
In this study, we demonstrate that filterbank learning outperforms handcrafted
speech features for KWS whenever the number of filterbank channels is severely
decreased. Reducing the number of channels might yield certain KWS performance
drop, but also a substantial energy consumption reduction, which is key when
deploying common always-on KWS on low-resource devices. Experimental results on
a noisy version of the Google Speech Commands Dataset show that filterbank
learning adapts to noise characteristics to provide a higher degree of
robustness to noise, especially when dropout is integrated. Thus, switching
from typically used 40-channel log-Mel features to 8-channel learned features
leads to a relative KWS accuracy loss of only 3.5% while simultaneously
achieving a 6.3x energy consumption reduction.",2211.10565v2,https://arxiv.org/pdf/2211.10565v2
"Distributionally Robust Survival Analysis: A Novel Fairness Loss Without
  Demographics","Shu Hu, George H. Chen","We propose a general approach for training survival analysis models that
minimizes a worst-case error across all subpopulations that are large enough
(occurring with at least a user-specified minimum probability). This approach
uses a training loss function that does not know any demographic information to
treat as sensitive. Despite this, we demonstrate that our proposed approach
often scores better on recently established fairness metrics (without a
significant drop in prediction accuracy) compared to various baselines,
including ones which directly use sensitive demographic information in their
training loss. Our code is available at: https://github.com/discovershu/DRO_COX",2211.10508v1,https://arxiv.org/pdf/2211.10508v1
"SAMSON: Sharpness-Aware Minimization Scaled by Outlier Normalization for
  Improving DNN Generalization and Robustness","Gonçalo Mordido, Sébastien Henwood, Sarath Chandar, François Leduc-Primeau","Energy-efficient deep neural network (DNN) accelerators are prone to
non-idealities that degrade DNN performance at inference time. To mitigate such
degradation, existing methods typically add perturbations to the DNN weights
during training to simulate inference on noisy hardware. However, this often
requires knowledge about the target hardware and leads to a trade-off between
DNN performance and robustness, decreasing the former to increase the latter.
In this work, we show that applying sharpness-aware training, by optimizing for
both the loss value and loss sharpness, significantly improves robustness to
noisy hardware at inference time without relying on any assumptions about the
target hardware. In particular, we propose a new adaptive sharpness-aware
method that conditions the worst-case perturbation of a given weight not only
on its magnitude but also on the range of the weight distribution. This is
achieved by performing sharpness-aware minimization scaled by outlier
minimization (SAMSON). Our approach outperforms existing sharpness-aware
training methods both in terms of model generalization performance in noiseless
regimes and robustness in noisy settings, as measured on several architectures
and datasets.",2211.11561v2,https://arxiv.org/pdf/2211.11561v2
"Global quantitative robustness of regression feed-forward neural
  networks",Tino Werner,"Neural networks are an indispensable model class for many complex learning
tasks. Despite the popularity and importance of neural networks and many
different established techniques from literature for stabilization and
robustification of the training, the classical concepts from robust statistics
have rarely been considered so far in the context of neural networks.
Therefore, we adapt the notion of the regression breakdown point to regression
neural networks and compute the breakdown point for different feed-forward
network configurations and contamination settings. In an extensive simulation
study, we compare the performance, measured by the out-of-sample loss, by a
proxy of the breakdown rate and by the training steps, of non-robust and robust
regression feed-forward neural networks in a plethora of different
configurations. The results indeed motivate to use robust loss functions for
neural network training.",2211.10124v1,https://arxiv.org/pdf/2211.10124v1
Active Learning by Query by Committee with Robust Divergences,"Hideitsu Hino, Shinto Eguchi","Active learning is a widely used methodology for various problems with high
measurement costs. In active learning, the next object to be measured is
selected by an acquisition function, and measurements are performed
sequentially. The query by committee is a well-known acquisition function. In
conventional methods, committee disagreement is quantified by the
Kullback--Leibler divergence. In this paper, the measure of disagreement is
defined by the Bregman divergence, which includes the Kullback--Leibler
divergence as an instance, and the dual $\gamma$-power divergence. As a
particular class of the Bregman divergence, the $\beta$-divergence is
considered. By deriving the influence function, we show that the proposed
method using $\beta$-divergence and dual $\gamma$-power divergence are more
robust than the conventional method in which the measure of disagreement is
defined by the Kullback--Leibler divergence. Experimental results show that the
proposed method performs as well as or better than the conventional method.",2211.10013v1,https://arxiv.org/pdf/2211.10013v1
"A Tale of Two Cities: Data and Configuration Variances in Robust Deep
  Learning","Guanqin Zhang, Jiankun Sun, Feng Xu, H. M. N. Dilum Bandara, Shiping Chen, Yulei Sui, Tim Menzies","Deep neural networks (DNNs), are widely used in many industries such as image
recognition, supply chain, medical diagnosis, and autonomous driving. However,
prior work has shown the high accuracy of a DNN model does not imply high
robustness (i.e., consistent performances on new and future datasets) because
the input data and external environment (e.g., software and model
configurations) for a deployed model are constantly changing. Hence, ensuring
the robustness of deep learning is not an option but a priority to enhance
business and consumer confidence. Previous studies mostly focus on the data
aspect of model variance. In this article, we systematically summarize DNN
robustness issues and formulate them in a holistic view through two important
aspects, i.e., data and software configuration variances in DNNs. We also
provide a predictive framework to generate representative variances
(counterexamples) by considering both data and configurations for robust
learning through the lens of search-based optimization.",2211.10012v2,https://arxiv.org/pdf/2211.10012v2
Robust Vocal Quality Feature Embeddings for Dysphonic Voice Detection,"Jianwei Zhang, Julie Liss, Suren Jayasuriya, Visar Berisha","Approximately 1.2% of the world's population has impaired voice production.
As a result, automatic dysphonic voice detection has attracted considerable
academic and clinical interest. However, existing methods for automated voice
assessment often fail to generalize outside the training conditions or to other
related applications. In this paper, we propose a deep learning framework for
generating acoustic feature embeddings sensitive to vocal quality and robust
across different corpora. A contrastive loss is combined with a classification
loss to train our deep learning model jointly. Data warping methods are used on
input voice samples to improve the robustness of our method. Empirical results
demonstrate that our method not only achieves high in-corpus and cross-corpus
classification accuracy but also generates good embeddings sensitive to voice
quality and robust across different corpora. We also compare our results
against three baseline methods on clean and three variations of deteriorated
in-corpus and cross-corpus datasets and demonstrate that the proposed model
consistently outperforms the baseline methods.",2211.09858v2,https://arxiv.org/pdf/2211.09858v2
Assessing Neural Network Robustness via Adversarial Pivotal Tuning,"Peter Ebert Christensen, Vésteinn Snæbjarnarson, Andrea Dittadi, Serge Belongie, Sagie Benaim","The robustness of image classifiers is essential to their deployment in the
real world. The ability to assess this resilience to manipulations or
deviations from the training data is thus crucial. These modifications have
traditionally consisted of minimal changes that still manage to fool
classifiers, and modern approaches are increasingly robust to them. Semantic
manipulations that modify elements of an image in meaningful ways have thus
gained traction for this purpose. However, they have primarily been limited to
style, color, or attribute changes. While expressive, these manipulations do
not make use of the full capabilities of a pretrained generative model. In this
work, we aim to bridge this gap. We show how a pretrained image generator can
be used to semantically manipulate images in a detailed, diverse, and
photorealistic way while still preserving the class of the original image.
Inspired by recent GAN-based image inversion methods, we propose a method
called Adversarial Pivotal Tuning (APT). Given an image, APT first finds a
pivot latent space input that reconstructs the image using a pretrained
generator. It then adjusts the generator's weights to create small yet semantic
manipulations in order to fool a pretrained classifier. APT preserves the full
expressive editing capabilities of the generative model. We demonstrate that
APT is capable of a wide range of class-preserving semantic image manipulations
that fool a variety of pretrained classifiers. Finally, we show that
classifiers that are robust to other benchmarks are not robust to APT
manipulations and suggest a method to improve them. Code available at:
https://captaine.github.io/apt/",2211.09782v2,https://arxiv.org/pdf/2211.09782v2
"Molecular Fingerprints for Robust and Efficient ML-Driven Molecular
  Generation","Ruslan N. Tazhigulov, Joshua Schiller, Jacob Oppenheim, Max Winston","We propose a novel molecular fingerprint-based variational autoencoder
applied for molecular generation on real-world drug molecules. We define more
suitable and pharma-relevant baseline metrics and tests, focusing on the
generation of diverse, drug-like, novel small molecules and scaffolds. When we
apply these molecular generation metrics to our novel model, we observe a
substantial improvement in chemical synthetic accessibility
($\Delta\bar{{SAS}}$ = -0.83) and in computational efficiency up to 5.9x in
comparison to an existing state-of-the-art SMILES-based architecture.",2211.09086v1,https://arxiv.org/pdf/2211.09086v1
"Reasons for the Superiority of Stochastic Estimators over Deterministic
  Ones: Robustness, Consistency and Perceptual Quality","Guy Ohayon, Theo Adrai, Michael Elad, Tomer Michaeli","Stochastic restoration algorithms allow to explore the space of solutions
that correspond to the degraded input. In this paper we reveal additional
fundamental advantages of stochastic methods over deterministic ones, which
further motivate their use. First, we prove that any restoration algorithm that
attains perfect perceptual quality and whose outputs are consistent with the
input must be a posterior sampler, and is thus required to be stochastic.
Second, we illustrate that while deterministic restoration algorithms may
attain high perceptual quality, this can be achieved only by filling up the
space of all possible source images using an extremely sensitive mapping, which
makes them highly vulnerable to adversarial attacks. Indeed, we show that
enforcing deterministic models to be robust to such attacks profoundly hinders
their perceptual quality, while robustifying stochastic models hardly
influences their perceptual quality, and improves their output variability.
These findings provide a motivation to foster progress in stochastic
restoration methods, paving the way to better recovery algorithms.",2211.08944v3,https://arxiv.org/pdf/2211.08944v3
Differentially Private Optimizers Can Learn Adversarially Robust Models,"Yuan Zhang, Zhiqi Bu","Machine learning models have shone in a variety of domains and attracted
increasing attention from both the security and the privacy communities. One
important yet worrying question is: Will training models under the differential
privacy (DP) constraint have an unfavorable impact on their adversarial
robustness? While previous works have postulated that privacy comes at the cost
of worse robustness, we give the first theoretical analysis to show that DP
models can indeed be robust and accurate, even sometimes more robust than their
naturally-trained non-private counterparts. We observe three key factors that
influence the privacy-robustness-accuracy tradeoff: (1) hyper-parameters for DP
optimizers are critical; (2) pre-training on public data significantly
mitigates the accuracy and robustness drop; (3) choice of DP optimizers makes a
difference. With these factors set properly, we achieve 90\% natural accuracy,
72\% robust accuracy ($+9\%$ than the non-private model) under $l_2(0.5)$
attack, and 69\% robust accuracy ($+16\%$ than the non-private model) with
pre-trained SimCLRv2 model under $l_\infty(4/255)$ attack on CIFAR10 with
$\epsilon=2$. In fact, we show both theoretically and empirically that DP
models are Pareto optimal on the accuracy-robustness tradeoff. Empirically, the
robustness of DP models is consistently observed across various datasets and
models. We believe our encouraging results are a significant step towards
training models that are private as well as robust.",2211.08942v2,https://arxiv.org/pdf/2211.08942v2
"Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed
  Representations","Linlin Liu, Xingxuan Li, Megh Thakkar, Xin Li, Shafiq Joty, Luo Si, Lidong Bing","Due to the huge amount of parameters, fine-tuning of pretrained language
models (PLMs) is prone to overfitting in the low resource scenarios. In this
work, we present a novel method that operates on the hidden representations of
a PLM to reduce overfitting. During fine-tuning, our method inserts random
autoencoders between the hidden layers of a PLM, which transform activations
from the previous layers into multi-view compressed representations before
feeding them into the upper layers. The autoencoders are plugged out after
fine-tuning, so our method does not add extra parameters or increase
computation cost during inference. Our method demonstrates promising
performance improvement across a wide range of sequence- and token-level
low-resource NLP tasks.",2211.08794v4,https://arxiv.org/pdf/2211.08794v4
"Interpretable Self-Aware Neural Networks for Robust Trajectory
  Prediction","Masha Itkina, Mykel J. Kochenderfer","Although neural networks have seen tremendous success as predictive models in
a variety of domains, they can be overly confident in their predictions on
out-of-distribution (OOD) data. To be viable for safety-critical applications,
like autonomous vehicles, neural networks must accurately estimate their
epistemic or model uncertainty, achieving a level of system self-awareness.
Techniques for epistemic uncertainty quantification often require OOD data
during training or multiple neural network forward passes during inference.
These approaches may not be suitable for real-time performance on
high-dimensional inputs. Furthermore, existing methods lack interpretability of
the estimated uncertainty, which limits their usefulness both to engineers for
further system development and to downstream modules in the autonomy stack. We
propose the use of evidential deep learning to estimate the epistemic
uncertainty over a low-dimensional, interpretable latent space in a trajectory
prediction setting. We introduce an interpretable paradigm for trajectory
prediction that distributes the uncertainty among the semantic concepts: past
agent behavior, road structure, and social context. We validate our approach on
real-world autonomous driving data, demonstrating superior performance over
state-of-the-art baselines. Our code is available at:
https://github.com/sisl/InterpretableSelfAwarePrediction.",2211.08701v1,https://arxiv.org/pdf/2211.08701v1
"Robust Alzheimer's Progression Modeling using Cross-Domain
  Self-Supervised Deep Learning","Saba Dadsetan, Mohsen Hejrati, Shandong Wu, Somaye Hashemifar","Developing successful artificial intelligence systems in practice depends on
both robust deep learning models and large, high-quality data. However,
acquiring and labeling data can be prohibitively expensive and time-consuming
in many real-world applications, such as clinical disease models.
Self-supervised learning has demonstrated great potential in increasing model
accuracy and robustness in small data regimes. In addition, many clinical
imaging and disease modeling applications rely heavily on regression of
continuous quantities. However, the applicability of self-supervised learning
for these medical-imaging regression tasks has not been extensively studied. In
this study, we develop a cross-domain self-supervised learning approach for
disease prognostic modeling as a regression problem using medical images as
input. We demonstrate that self-supervised pretraining can improve the
prediction of Alzheimer's Disease progression from brain MRI. We also show that
pretraining on extended (but not labeled) brain MRI data outperforms
pretraining on natural images. We further observe that the highest performance
is achieved when both natural images and extended brain-MRI data are used for
pretraining.",2211.08559v2,https://arxiv.org/pdf/2211.08559v2
Improved techniques for deterministic l2 robustness,"Sahil Singla, Soheil Feizi","Training convolutional neural networks (CNNs) with a strict 1-Lipschitz
constraint under the $l_{2}$ norm is useful for adversarial robustness,
interpretable gradients and stable training. 1-Lipschitz CNNs are usually
designed by enforcing each layer to have an orthogonal Jacobian matrix (for all
inputs) to prevent the gradients from vanishing during backpropagation.
However, their performance often significantly lags behind that of heuristic
methods to enforce Lipschitz constraints where the resulting CNN is not
\textit{provably} 1-Lipschitz. In this work, we reduce this gap by introducing
(a) a procedure to certify robustness of 1-Lipschitz CNNs by replacing the last
linear layer with a 1-hidden layer MLP that significantly improves their
performance for both standard and provably robust accuracy, (b) a method to
significantly reduce the training time per epoch for Skew Orthogonal
Convolution (SOC) layers (>30\% reduction for deeper networks) and (c) a class
of pooling layers using the mathematical property that the $l_{2}$ distance of
an input to a manifold is 1-Lipschitz. Using these methods, we significantly
advance the state-of-the-art for standard and provable robust accuracies on
CIFAR-10 (gains of +1.79\% and +3.82\%) and similarly on CIFAR-100 (+3.78\% and
+4.75\%) across all networks. Code is available at
\url{https://github.com/singlasahil14/improved_l2_robustness}.",2211.08453v1,https://arxiv.org/pdf/2211.08453v1
"Perona: Robust Infrastructure Fingerprinting for Resource-Efficient Big
  Data Analytics","Dominik Scheinert, Soeren Becker, Jonathan Bader, Lauritz Thamsen, Jonathan Will, Odej Kao","Choosing a good resource configuration for big data analytics applications
can be challenging, especially in cloud environments. Automated approaches are
desirable as poor decisions can reduce performance and raise costs. The
majority of existing automated approaches either build performance models from
previous workload executions or conduct iterative resource configuration
profiling until a near-optimal solution has been found. In doing so, they only
obtain an implicit understanding of the underlying infrastructure, which is
difficult to transfer to alternative infrastructures and, thus, profiling and
modeling insights are not sustained beyond very specific situations.
  We present Perona, a novel approach to robust infrastructure fingerprinting
for usage in the context of big data analytics. Perona employs common sets and
configurations of benchmarking tools for target resources, so that resulting
benchmark metrics are directly comparable and ranking is enabled. Insignificant
benchmark metrics are discarded by learning a low-dimensional representation of
the input metric vector, and previous benchmark executions are taken into
consideration for context-awareness as well, allowing to detect resource
degradation. We evaluate our approach both on data gathered from our own
experiments as well as within related works for resource configuration
optimization, demonstrating that Perona captures the characteristics from
benchmark runs in a compact manner and produces representations that can be
used directly.",2211.08227v2,https://arxiv.org/pdf/2211.08227v2
"MORA: Improving Ensemble Robustness Evaluation with Model-Reweighing
  Attack","Yunrui Yu, Xitong Gao, Cheng-Zhong Xu","Adversarial attacks can deceive neural networks by adding tiny perturbations
to their input data. Ensemble defenses, which are trained to minimize attack
transferability among sub-models, offer a promising research direction to
improve robustness against such attacks while maintaining a high accuracy on
natural inputs. We discover, however, that recent state-of-the-art (SOTA)
adversarial attack strategies cannot reliably evaluate ensemble defenses,
sizeably overestimating their robustness. This paper identifies the two factors
that contribute to this behavior. First, these defenses form ensembles that are
notably difficult for existing gradient-based method to attack, due to gradient
obfuscation. Second, ensemble defenses diversify sub-model gradients,
presenting a challenge to defeat all sub-models simultaneously, simply summing
their contributions may counteract the overall attack objective; yet, we
observe that ensemble may still be fooled despite most sub-models being
correct. We therefore introduce MORA, a model-reweighing attack to steer
adversarial example synthesis by reweighing the importance of sub-model
gradients. MORA finds that recent ensemble defenses all exhibit varying degrees
of overestimated robustness. Comparing it against recent SOTA white-box
attacks, it can converge orders of magnitude faster while achieving higher
attack success rates across all ensemble models examined with three different
ensemble modes (i.e., ensembling by either softmax, voting or logits). In
particular, most ensemble defenses exhibit near or exactly 0% robustness
against MORA with $\ell^\infty$ perturbation within 0.02 on CIFAR-10, and 0.01
on CIFAR-100. We make MORA open source with reproducible results and
pre-trained models; and provide a leaderboard of ensemble defenses under
various attack strategies.",2211.08008v1,https://arxiv.org/pdf/2211.08008v1
Multi-Player Bandits Robust to Adversarial Collisions,"Shivakumar Mahesh, Anshuka Rangi, Haifeng Xu, Long Tran-Thanh","Motivated by cognitive radios, stochastic Multi-Player Multi-Armed Bandits
has been extensively studied in recent years. In this setting, each player
pulls an arm, and receives a reward corresponding to the arm if there is no
collision, namely the arm was selected by one single player. Otherwise, the
player receives no reward if collision occurs. In this paper, we consider the
presence of malicious players (or attackers) who obstruct the cooperative
players (or defenders) from maximizing their rewards, by deliberately colliding
with them. We provide the first decentralized and robust algorithm RESYNC for
defenders whose performance deteriorates gracefully as $\tilde{O}(C)$ as the
number of collisions $C$ from the attackers increases. We show that this
algorithm is order-optimal by proving a lower bound which scales as
$\Omega(C)$. This algorithm is agnostic to the algorithm used by the attackers
and agnostic to the number of collisions $C$ faced from attackers.",2211.07817v1,https://arxiv.org/pdf/2211.07817v1
Robust Deep Learning for Autonomous Driving,Charles Corbière,"The last decade's research in artificial intelligence had a significant
impact on the advance of autonomous driving. Yet, safety remains a major
concern when it comes to deploying such systems in high-risk environments. The
objective of this thesis is to develop methodological tools which provide
reliable uncertainty estimates for deep neural networks. First, we introduce a
new criterion to reliably estimate model confidence: the true class probability
(TCP). We show that TCP offers better properties for failure prediction than
current uncertainty measures. Since the true class is by essence unknown at
test time, we propose to learn TCP criterion from data with an auxiliary model,
introducing a specific learning scheme adapted to this context. The relevance
of the proposed approach is validated on image classification and semantic
segmentation datasets. Then, we extend our learned confidence approach to the
task of domain adaptation where it improves the selection of pseudo-labels in
self-training methods. Finally, we tackle the challenge of jointly detecting
misclassification and out-of-distributions samples by introducing a new
uncertainty measure based on evidential models and defined on the simplex.",2211.07772v1,https://arxiv.org/pdf/2211.07772v1
"Utilizing Synthetic Data in Supervised Learning for Robust 5-DoF
  Magnetic Marker Localization","Mengfan Wu, Thomas Langerak, Otmar Hilliges, Juan Zarate","Tracking passive magnetic markers plays a vital role in advancing healthcare
and robotics, offering the potential to significantly improve the precision and
efficiency of systems. This technology is key to developing smarter, more
responsive tools and devices, such as enhanced surgical instruments, precise
diagnostic tools, and robots with improved environmental interaction
capabilities. However, traditionally, the tracking of magnetic markers is
computationally expensive due to the requirement for iterative optimization
procedures. Moreover, these methods depend on the magnetic dipole model for
their optimization function, which can yield imprecise outcomes due to the
model's significant inaccuracies when dealing with short distances between
non-spherical magnet and sensor.Our paper introduces a novel approach that
leverages neural networks to bypass these limitations, directly inferring the
marker's position and orientation to accurately determine the magnet's 5 DoF in
a single step without initial estimation. Although our method demands an
extensive supervised training phase, we mitigate this by introducing a
computationally more efficient method to generate synthetic, yet realistic data
using Finite Element Methods simulations. The benefits of fast and accurate
inference significantly outweigh the offline training preparation. In our
evaluation, we use different cylindrical magnets, tracked with a square array
of 16 sensors. We perform the sensors' reading and position inference on a
portable, neural networks-oriented single-board computer, ensuring a compact
setup. We benchmark our prototype against vision-based ground truth data,
achieving a mean positional error of 4 mm and an orientation error of 8 degrees
within a 0.2x0.2x0.15 m working volume. These results showcase our prototype's
ability to balance accuracy and compactness effectively in tracking 5 DoF.",2211.07556v2,https://arxiv.org/pdf/2211.07556v2
"Higher degree sum-of-squares relaxations robust against oblivious
  outliers","Tommaso d'Orsi, Rajai Nasser, Gleb Novikov, David Steurer","We consider estimation models of the form $Y=X^*+N$, where $X^*$ is some
$m$-dimensional signal we wish to recover, and $N$ is symmetrically distributed
noise that may be unbounded in all but a small $\alpha$ fraction of the
entries. We introduce a family of algorithms that under mild assumptions
recover the signal $X^*$ in all estimation problems for which there exists a
sum-of-squares algorithm that succeeds in recovering the signal $X^*$ when the
noise $N$ is Gaussian. This essentially shows that it is enough to design a
sum-of-squares algorithm for an estimation problem with Gaussian noise in order
to get the algorithm that works with the symmetric noise model. Our framework
extends far beyond previous results on symmetric noise models and is even
robust to adversarial perturbations.
  As concrete examples, we investigate two problems for which no efficient
algorithms were known to work for heavy-tailed noise: tensor PCA and sparse
PCA. For the former, our algorithm recovers the principal component in
polynomial time when the signal-to-noise ratio is at least
$\tilde{O}(n^{p/4}/\alpha)$, that matches (up to logarithmic factors) current
best known algorithmic guarantees for Gaussian noise. For the latter, our
algorithm runs in quasipolynomial time and matches the state-of-the-art
guarantees for quasipolynomial time algorithms in the case of Gaussian noise.
Using a reduction from the planted clique problem, we provide evidence that the
quasipolynomial time is likely to be necessary for sparse PCA with symmetric
noise.
  In our proofs we use bounds on the covering numbers of sets of
pseudo-expectations, which we obtain by certifying in sum-of-squares upper
bounds on the Gaussian complexities of sets of solutions. This approach for
bounding the covering numbers of sets of pseudo-expectations may be interesting
in its own right and may find other application in future works.",2211.07327v1,https://arxiv.org/pdf/2211.07327v1
Robustifying Deep Vision Models Through Shape Sensitization,"Aditay Tripathi, Rishubh Singh, Anirban Chakraborty, Pradeep Shenoy","Recent work has shown that deep vision models tend to be overly dependent on
low-level or ""texture"" features, leading to poor generalization. Various data
augmentation strategies have been proposed to overcome this so-called texture
bias in DNNs. We propose a simple, lightweight adversarial augmentation
technique that explicitly incentivizes the network to learn holistic shapes for
accurate prediction in an object classification setting. Our augmentations
superpose edgemaps from one image onto another image with shuffled patches,
using a randomly determined mixing proportion, with the image label of the
edgemap image. To classify these augmented images, the model needs to not only
detect and focus on edges but distinguish between relevant and spurious edges.
We show that our augmentations significantly improve classification accuracy
and robustness measures on a range of datasets and neural architectures. As an
example, for ViT-S, We obtain absolute gains on classification accuracy gains
up to 6%. We also obtain gains of up to 28% and 8.5% on natural adversarial and
out-of-distribution datasets like ImageNet-A (for ViT-B) and ImageNet-R (for
ViT-S), respectively. Analysis using a range of probe datasets shows
substantially increased shape sensitivity in our trained models, explaining the
observed improvement in robustness and classification accuracy.",2211.07277v1,https://arxiv.org/pdf/2211.07277v1
"Certifying Robustness of Convolutional Neural Networks with Tight Linear
  Approximation","Yuan Xiao, Tongtong Bai, Mingzheng Gu, Chunrong Fang, Zhenyu Chen","The robustness of neural network classifiers is becoming important in the
safety-critical domain and can be quantified by robustness verification.
However, at present, efficient and scalable verification techniques are always
sound but incomplete. Therefore, the improvement of certified robustness bounds
is the key criterion to evaluate the superiority of robustness verification
approaches. In this paper, we present a Tight Linear approximation approach for
robustness verification of Convolutional Neural Networks(Ti-Lin). For general
CNNs, we first provide a new linear constraints for S-shaped activation
functions, which is better than both existing Neuron-wise Tightest and
Network-wise Tightest tools. We then propose Neuron-wise Tightest linear bounds
for Maxpool function. We implement Ti-Lin, the resulting verification method.
We evaluate it with 48 different CNNs trained on MNIST, CIFAR-10, and Tiny
ImageNet datasets. Experimental results show that Ti-Lin significantly
outperforms other five state-of-the-art methods(CNN-Cert, DeepPoly, DeepCert,
VeriNet, Newise). Concretely, Ti-Lin certifies much more precise robustness
bounds on pure CNNs with Sigmoid/Tanh/Arctan functions and CNNs with Maxpooling
function with at most 63.70% and 253.54% improvement, respectively.",2211.09810v1,https://arxiv.org/pdf/2211.09810v1
"Adversarial and Random Transformations for Robust Domain Adaptation and
  Generalization","Liang Xiao, Jiaolong Xu, Dawei Zhao, Erke Shang, Qi Zhu, Bin Dai","Data augmentation has been widely used to improve generalization in training
deep neural networks. Recent works show that using worst-case transformations
or adversarial augmentation strategies can significantly improve the accuracy
and robustness. However, due to the non-differentiable properties of image
transformations, searching algorithms such as reinforcement learning or
evolution strategy have to be applied, which are not computationally practical
for large scale problems. In this work, we show that by simply applying
consistency training with random data augmentation, state-of-the-art results on
domain adaptation (DA) and generalization (DG) can be obtained. To further
improve the accuracy and robustness with adversarial examples, we propose a
differentiable adversarial data augmentation method based on spatial
transformer networks (STN). The combined adversarial and random transformations
based method outperforms the state-of-the-art on multiple DA and DG benchmark
datasets. Besides, the proposed method shows desirable robustness to
corruption, which is also validated on commonly used datasets.",2211.06788v1,https://arxiv.org/pdf/2211.06788v1
"A Generalized Doubly Robust Learning Framework for Debiasing Post-Click
  Conversion Rate Prediction","Quanyu Dai, Haoxuan Li, Peng Wu, Zhenhua Dong, Xiao-Hua Zhou, Rui Zhang, Rui zhang, Jie Sun","Post-click conversion rate (CVR) prediction is an essential task for
discovering user interests and increasing platform revenues in a range of
industrial applications. One of the most challenging problems of this task is
the existence of severe selection bias caused by the inherent self-selection
behavior of users and the item selection process of systems. Currently, doubly
robust (DR) learning approaches achieve the state-of-the-art performance for
debiasing CVR prediction. However, in this paper, by theoretically analyzing
the bias, variance and generalization bounds of DR methods, we find that
existing DR approaches may have poor generalization caused by inaccurate
estimation of propensity scores and imputation errors, which often occur in
practice. Motivated by such analysis, we propose a generalized learning
framework that not only unifies existing DR methods, but also provides a
valuable opportunity to develop a series of new debiasing techniques to
accommodate different application scenarios. Based on the framework, we propose
two new DR methods, namely DR-BIAS and DR-MSE. DR-BIAS directly controls the
bias of DR loss, while DR-MSE balances the bias and variance flexibly, which
achieves better generalization performance. In addition, we propose a novel
tri-level joint learning optimization method for DR-MSE in CVR prediction, and
an efficient training algorithm correspondingly. We conduct extensive
experiments on both real-world and semi-synthetic datasets, which validate the
effectiveness of our proposed methods.",2211.06684v1,https://arxiv.org/pdf/2211.06684v1
Robust Training of Graph Neural Networks via Noise Governance,"Siyi Qian, Haochao Ying, Renjun Hu, Jingbo Zhou, Jintai Chen, Danny Z. Chen, Jian Wu","Graph Neural Networks (GNNs) have become widely-used models for
semi-supervised learning. However, the robustness of GNNs in the presence of
label noise remains a largely under-explored problem. In this paper, we
consider an important yet challenging scenario where labels on nodes of graphs
are not only noisy but also scarce. In this scenario, the performance of GNNs
is prone to degrade due to label noise propagation and insufficient learning.
To address these issues, we propose a novel RTGNN (Robust Training of Graph
Neural Networks via Noise Governance) framework that achieves better robustness
by learning to explicitly govern label noise. More specifically, we introduce
self-reinforcement and consistency regularization as supplemental supervision.
The self-reinforcement supervision is inspired by the memorization effects of
deep neural networks and aims to correct noisy labels. Further, the consistency
regularization prevents GNNs from overfitting to noisy labels via mimicry loss
in both the inter-view and intra-view perspectives. To leverage such
supervisions, we divide labels into clean and noisy types, rectify inaccurate
labels, and further generate pseudo-labels on unlabeled nodes. Supervision for
nodes with different types of labels is then chosen adaptively. This enables
sufficient learning from clean labels while limiting the impact of noisy ones.
We conduct extensive experiments to evaluate the effectiveness of our RTGNN
framework, and the results validate its consistent superior performance over
state-of-the-art methods with two types of label noises and various noise
rates.",2211.06614v2,https://arxiv.org/pdf/2211.06614v2
RISE: Robust Individualized Decision Learning with Sensitive Variables,"Xiaoqing Tan, Zhengling Qi, Christopher W. Seymour, Lu Tang","This paper introduces RISE, a robust individualized decision learning
framework with sensitive variables, where sensitive variables are collectible
data and important to the intervention decision, but their inclusion in
decision making is prohibited due to reasons such as delayed availability or
fairness concerns. A naive baseline is to ignore these sensitive variables in
learning decision rules, leading to significant uncertainty and bias. To
address this, we propose a decision learning framework to incorporate sensitive
variables during offline training but not include them in the input of the
learned decision rule during model deployment. Specifically, from a causal
perspective, the proposed framework intends to improve the worst-case outcomes
of individuals caused by sensitive variables that are unavailable at the time
of decision. Unlike most existing literature that uses mean-optimal objectives,
we propose a robust learning framework by finding a newly defined quantile- or
infimum-optimal decision rule. The reliable performance of the proposed method
is demonstrated through synthetic experiments and three real-world
applications.",2211.06569v1,https://arxiv.org/pdf/2211.06569v1
"On the robustness of non-intrusive speech quality model by adversarial
  examples","Hsin-Yi Lin, Huan-Hsin Tseng, Yu Tsao","It has been shown recently that deep learning based models are effective on
speech quality prediction and could outperform traditional metrics in various
perspectives. Although network models have potential to be a surrogate for
complex human hearing perception, they may contain instabilities in
predictions. This work shows that deep speech quality predictors can be
vulnerable to adversarial perturbations, where the prediction can be changed
drastically by unnoticeable perturbations as small as $-30$ dB compared with
speech inputs. In addition to exposing the vulnerability of deep speech quality
predictors, we further explore and confirm the viability of adversarial
training for strengthening robustness of models.",2211.06508v1,https://arxiv.org/pdf/2211.06508v1
"Active Task Randomization: Learning Robust Skills via Unsupervised
  Generation of Diverse and Feasible Tasks","Kuan Fang, Toki Migimatsu, Ajay Mandlekar, Li Fei-Fei, Jeannette Bohg","Solving real-world manipulation tasks requires robots to have a repertoire of
skills applicable to a wide range of circumstances. When using learning-based
methods to acquire such skills, the key challenge is to obtain training data
that covers diverse and feasible variations of the task, which often requires
non-trivial manual labor and domain knowledge. In this work, we introduce
Active Task Randomization (ATR), an approach that learns robust skills through
the unsupervised generation of training tasks. ATR selects suitable tasks,
which consist of an initial environment state and manipulation goal, for
learning robust skills by balancing the diversity and feasibility of the tasks.
We propose to predict task diversity and feasibility by jointly learning a
compact task representation. The selected tasks are then procedurally generated
in simulation using graph-based parameterization. The active selection of these
training tasks enables skill policies trained with our framework to robustly
handle a diverse range of objects and arrangements at test time. We demonstrate
that the learned skills can be composed by a task planner to solve unseen
sequential manipulation problems based on visual inputs. Compared to baseline
methods, ATR can achieve superior success rates in single-step and sequential
manipulation tasks.",2211.06134v2,https://arxiv.org/pdf/2211.06134v2
"Robust N-1 secure HV Grid Flexibility Estimation for TSO-DSO coordinated
  Congestion Management with Deep Reinforcement Learning","Zhenqi Wang, Sebastian Wende-von Berg, Martin Braun","Nowadays, the PQ flexibility from the distributed energy resources (DERs) in
the high voltage (HV) grids plays a more critical and significant role in grid
congestion management in TSO grids. This work proposed a multi-stage deep
reinforcement learning approach to estimate the PQ flexibility (PQ area) at the
TSO-DSO interfaces and identifies the DER PQ setpoints for each operating point
in a way, that DERs in the meshed HV grid can be coordinated to offer
flexibility for the transmission grid. In the estimation process, we consider
the steady-state grid limits and the robustness in the resulting voltage
profile against uncertainties and the N-1 security criterion regarding thermal
line loading, essential for real-life grid operational planning applications.
Using deep reinforcement learning (DRL) for PQ flexibility estimation is the
first of its kind. Furthermore, our approach of considering N-1 security
criterion for meshed grids and robustness against uncertainty directly in the
optimization tasks offers a new perspective besides the common relaxation
schema in finding a solution with mathematical optimal power flow (OPF).
Finally, significant improvements in the computational efficiency in estimation
PQ area are the highlights of the proposed method.",2211.05855v2,https://arxiv.org/pdf/2211.05855v2
"Test-time adversarial detection and robustness for localizing humans
  using ultra wide band channel impulse responses","Abhiram Kolli, Muhammad Jehanzeb Mirza, Horst Possegger, Horst Bischof","Keyless entry systems in cars are adopting neural networks for localizing its
operators. Using test-time adversarial defences equip such systems with the
ability to defend against adversarial attacks without prior training on
adversarial samples. We propose a test-time adversarial example detector which
detects the input adversarial example through quantifying the localized
intermediate responses of a pre-trained neural network and confidence scores of
an auxiliary softmax layer. Furthermore, in order to make the network robust,
we extenuate the non-relevant features by non-iterative input sample clipping.
Using our approach, mean performance over 15 levels of adversarial
perturbations is increased by 55.33% for the fast gradient sign method (FGSM)
and 6.3% for both the basic iterative method (BIM) and the projected gradient
method (PGD).",2211.05854v1,https://arxiv.org/pdf/2211.05854v1
"Casual Conversations v2: Designing a large consent-driven dataset to
  measure algorithmic bias and robustness","Caner Hazirbas, Yejin Bang, Tiezheng Yu, Parisa Assar, Bilal Porgali, Vítor Albiero, Stefan Hermanek, Jacqueline Pan, Emily McReynolds, Miranda Bogen, Pascale Fung, Cristian Canton Ferrer","Developing robust and fair AI systems require datasets with comprehensive set
of labels that can help ensure the validity and legitimacy of relevant
measurements. Recent efforts, therefore, focus on collecting person-related
datasets that have carefully selected labels, including sensitive
characteristics, and consent forms in place to use those attributes for model
testing and development. Responsible data collection involves several stages,
including but not limited to determining use-case scenarios, selecting
categories (annotations) such that the data are fit for the purpose of
measuring algorithmic bias for subgroups and most importantly ensure that the
selected categories/subcategories are robust to regional diversities and
inclusive of as many subgroups as possible.
  Meta, in a continuation of our efforts to measure AI algorithmic bias and
robustness
(https://ai.facebook.com/blog/shedding-light-on-fairness-in-ai-with-a-new-data-set),
is working on collecting a large consent-driven dataset with a comprehensive
list of categories. This paper describes our proposed design of such categories
and subcategories for Casual Conversations v2.",2211.05809v1,https://arxiv.org/pdf/2211.05809v1
Robust Model Selection of Gaussian Graphical Models,"Abrar Zahin, Rajasekhar Anguluri, Lalitha Sankar, Oliver Kosut, Gautam Dasarathy","In Gaussian graphical model selection, noise-corrupted samples present
significant challenges. It is known that even minimal amounts of noise can
obscure the underlying structure, leading to fundamental identifiability
issues. A recent line of work addressing this ""robust model selection"" problem
narrows its focus to tree-structured graphical models. Even within this
specific class of models, exact structure recovery is shown to be impossible.
However, several algorithms have been developed that are known to provably
recover the underlying tree-structure up to an (unavoidable) equivalence class.
  In this paper, we extend these results beyond tree-structured graphs. We
first characterize the equivalence class up to which general graphs can be
recovered in the presence of noise. Despite the inherent ambiguity (which we
prove is unavoidable), the structure that can be recovered reveals local
clustering information and global connectivity patterns in the underlying
model. Such information is useful in a range of real-world problems, including
power grids, social networks, protein-protein interactions, and neural
structures. We then propose an algorithm which provably recovers the underlying
graph up to the identified ambiguity. We further provide finite sample
guarantees in the high-dimensional regime for our algorithm and validate our
results through numerical simulations.",2211.05690v2,https://arxiv.org/pdf/2211.05690v2
On Proper Learnability between Average- and Worst-case Robustness,"Vinod Raman, Unique Subedi, Ambuj Tewari","Recently, Montasser et al. [2019] showed that finite VC dimension is not
sufficient for proper adversarially robust PAC learning. In light of this
hardness, there is a growing effort to study what type of relaxations to the
adversarially robust PAC learning setup can enable proper learnability. In this
work, we initiate the study of proper learning under relaxations of the
worst-case robust loss. We give a family of robust loss relaxations under which
VC classes are properly PAC learnable with sample complexity close to what one
would require in the standard PAC learning setup. On the other hand, we show
that for an existing and natural relaxation of the worst-case robust loss,
finite VC dimension is not sufficient for proper learning. Lastly, we give new
generalization guarantees for the adversarially robust empirical risk
minimizer.",2211.05656v5,https://arxiv.org/pdf/2211.05656v5
"Improving the Robustness of Neural Multiplication Units with Reversible
  Stochasticity","Bhumika Mistry, Katayoun Farrahi, Jonathon Hare","Multilayer Perceptrons struggle to learn certain simple arithmetic tasks.
Specialist neural modules for arithmetic can outperform classical architectures
with gains in extrapolation, interpretability and convergence speeds, but are
highly sensitive to the training range. In this paper, we show that Neural
Multiplication Units (NMUs) are unable to reliably learn tasks as simple as
multiplying two inputs when given different training ranges. Causes of failure
are linked to inductive and input biases which encourage convergence to
solutions in undesirable optima. A solution, the stochastic NMU (sNMU), is
proposed to apply reversible stochasticity, encouraging avoidance of such
optima whilst converging to the true solution. Empirically, we show that
stochasticity provides improved robustness with the potential to improve
learned representations of upstream networks for numerical and image tasks.",2211.05624v1,https://arxiv.org/pdf/2211.05624v1
"Robust Federated Learning against both Data Heterogeneity and Poisoning
  Attack via Aggregation Optimization","Yueqi Xie, Weizhong Zhang, Renjie Pi, Fangzhao Wu, Qifeng Chen, Xing Xie, Sunghun Kim","Non-IID data distribution across clients and poisoning attacks are two main
challenges in real-world federated learning (FL) systems. While both of them
have attracted great research interest with specific strategies developed, no
known solution manages to address them in a unified framework. To universally
overcome both challenges, we propose SmartFL, a generic approach that optimizes
the server-side aggregation process with a small amount of proxy data collected
by the service provider itself via a subspace training technique. Specifically,
the aggregation weight of each participating client at each round is optimized
using the server-collected proxy data, which is essentially the optimization of
the global model in the convex hull spanned by client models. Since at each
round, the number of tunable parameters optimized on the server side equals the
number of participating clients (thus independent of the model size), we are
able to train a global model with massive parameters using only a small amount
of proxy data (e.g., around one hundred samples). With optimized aggregation,
SmartFL ensures robustness against both heterogeneous and malicious clients,
which is desirable in real-world FL where either or both problems may occur. We
provide theoretical analyses of the convergence and generalization capacity for
SmartFL. Empirically, SmartFL achieves state-of-the-art performance on both FL
with non-IID data distribution and FL with malicious clients. The source code
will be released.",2211.05554v2,https://arxiv.org/pdf/2211.05554v2
"MGiaD: Multigrid in all dimensions. Efficiency and robustness by
  coarsening in resolution and channel dimensions","Antonia van Betteray, Matthias Rottmann, Karsten Kahl","Current state-of-the-art deep neural networks for image classification are
made up of 10 - 100 million learnable weights and are therefore inherently
prone to overfitting. The complexity of the weight count can be seen as a
function of the number of channels, the spatial extent of the input and the
number of layers of the network. Due to the use of convolutional layers the
scaling of weight complexity is usually linear with regards to the resolution
dimensions, but remains quadratic with respect to the number of channels.
Active research in recent years in terms of using multigrid inspired ideas in
deep neural networks have shown that on one hand a significant number of
weights can be saved by appropriate weight sharing and on the other that a
hierarchical structure in the channel dimension can improve the weight
complexity to linear. In this work, we combine these multigrid ideas to
introduce a joint framework of multigrid inspired architectures, that exploit
multigrid structures in all relevant dimensions to achieve linear weight
complexity scaling and drastically reduced weight counts. Our experiments show
that this structured reduction in weight count is able to reduce overfitting
and thus shows improved performance over state-of-the-art ResNet architectures
on typical image classification benchmarks at lower network complexity.",2211.05525v1,https://arxiv.org/pdf/2211.05525v1
"Impact of Adversarial Training on Robustness and Generalizability of
  Language Models","Enes Altinisik, Hassan Sajjad, Husrev Taha Sencar, Safa Messaoud, Sanjay Chawla","Adversarial training is widely acknowledged as the most effective defense
against adversarial attacks. However, it is also well established that
achieving both robustness and generalization in adversarially trained models
involves a trade-off. The goal of this work is to provide an in depth
comparison of different approaches for adversarial training in language models.
Specifically, we study the effect of pre-training data augmentation as well as
training time input perturbations vs. embedding space perturbations on the
robustness and generalization of transformer-based language models. Our
findings suggest that better robustness can be achieved by pre-training data
augmentation or by training with input space perturbation. However, training
with embedding space perturbation significantly improves generalization. A
linguistic correlation analysis of neurons of the learned models reveals that
the improved generalization is due to 'more specialized' neurons. To the best
of our knowledge, this is the first work to carry out a deep qualitative
analysis of different methods of generating adversarial examples in adversarial
training of language models.",2211.05523v3,https://arxiv.org/pdf/2211.05523v3
Robust Smart Home Face Recognition under Starving Federated Data,"Jaechul Roh, Yajun Fang","Over the past few years, the field of adversarial attack received numerous
attention from various researchers with the help of successful attack success
rate against well-known deep neural networks that were acknowledged to achieve
high classification ability in various tasks. However, majority of the
experiments were completed under a single model, which we believe it may not be
an ideal case in a real-life situation. In this paper, we introduce a novel
federated adversarial training method for smart home face recognition, named
FLATS, where we observed some interesting findings that may not be easily
noticed in a traditional adversarial attack to federated learning experiments.
By applying different variations to the hyperparameters, we have spotted that
our method can make the global model to be robust given a starving federated
environment. Our code can be found on https://github.com/jcroh0508/FLATS.",2211.05410v2,https://arxiv.org/pdf/2211.05410v2
"Robust DNN Surrogate Models with Uncertainty Quantification via
  Adversarial Training","Lixiang Zhang, Jia Li","For computational efficiency, surrogate models have been used to emulate
mathematical simulators for physical or biological processes. High-speed
simulation is crucial for conducting uncertainty quantification (UQ) when the
simulation is repeated over many randomly sampled input points (aka, the Monte
Carlo method). In some cases, UQ is only feasible with a surrogate model.
Recently, Deep Neural Network (DNN) surrogate models have gained popularity for
their hard-to-match emulation accuracy. However, it is well-known that DNN is
prone to errors when input data are perturbed in particular ways, the very
motivation for adversarial training. In the usage scenario of surrogate models,
the concern is less of a deliberate attack but more of the high sensitivity of
the DNN's accuracy to input directions, an issue largely ignored by researchers
using emulation models. In this paper, we show the severity of this issue
through empirical studies and hypothesis testing. Furthermore, we adopt methods
in adversarial training to enhance the robustness of DNN surrogate models.
Experiments demonstrate that our approaches significantly improve the
robustness of the surrogate models without compromising emulation accuracy.",2211.09954v1,https://arxiv.org/pdf/2211.09954v1
"QuanGCN: Noise-Adaptive Training for Robust Quantum Graph Convolutional
  Networks","Kaixiong Zhou, Zhenyu Zhang, Shengyuan Chen, Tianlong Chen, Xiao Huang, Zhangyang Wang, Xia Hu","Quantum neural networks (QNNs), an interdisciplinary field of quantum
computing and machine learning, have attracted tremendous research interests
due to the specific quantum advantages. Despite lots of efforts developed in
computer vision domain, one has not fully explored QNNs for the real-world
graph property classification and evaluated them in the quantum device. To
bridge the gap, we propose quantum graph convolutional networks (QuanGCN),
which learns the local message passing among nodes with the sequence of
crossing-gate quantum operations. To mitigate the inherent noises from modern
quantum devices, we apply sparse constraint to sparsify the nodes' connections
and relieve the error rate of quantum gates, and use skip connection to augment
the quantum outputs with original node features to improve robustness. The
experimental results show that our QuanGCN is functionally comparable or even
superior than the classical algorithms on several benchmark graph datasets. The
comprehensive evaluations in both simulator and real quantum machines
demonstrate the applicability of QuanGCN to the future graph analysis problem.",2211.07379v1,https://arxiv.org/pdf/2211.07379v1
"Bayesian Networks for the robust and unbiased prediction of depression
  and its symptoms utilizing speech and multimodal data","Salvatore Fara, Orlaith Hickey, Alexandra Georgescu, Stefano Goria, Emilia Molimpakis, Nicholas Cummins","Predicting the presence of major depressive disorder (MDD) using behavioural
and cognitive signals is a highly non-trivial task. The heterogeneous clinical
profile of MDD means that any given speech, facial expression and/or observed
cognitive pattern may be associated with a unique combination of depressive
symptoms. Conventional discriminative machine learning models potentially lack
the complexity to robustly model this heterogeneity. Bayesian networks,
however, may instead be well-suited to such a scenario. These networks are
probabilistic graphical models that efficiently describe the joint probability
distribution over a set of random variables by explicitly capturing their
conditional dependencies. This framework provides further advantages over
standard discriminative modelling by offering the possibility to incorporate
expert opinion in the graphical structure of the models, generating explainable
model predictions, informing about the uncertainty of predictions, and
naturally handling missing data. In this study, we apply a Bayesian framework
to capture the relationships between depression, depression symptoms, and
features derived from speech, facial expression and cognitive game data
collected at thymia.",2211.04924v2,https://arxiv.org/pdf/2211.04924v2
"On the Robustness of Explanations of Deep Neural Network Models: A
  Survey","Amlan Jyoti, Karthik Balaji Ganesh, Manoj Gayala, Nandita Lakshmi Tunuguntla, Sandesh Kamath, Vineeth N Balasubramanian","Explainability has been widely stated as a cornerstone of the responsible and
trustworthy use of machine learning models. With the ubiquitous use of Deep
Neural Network (DNN) models expanding to risk-sensitive and safety-critical
domains, many methods have been proposed to explain the decisions of these
models. Recent years have also seen concerted efforts that have shown how such
explanations can be distorted (attacked) by minor input perturbations. While
there have been many surveys that review explainability methods themselves,
there has been no effort hitherto to assimilate the different methods and
metrics proposed to study the robustness of explanations of DNN models. In this
work, we present a comprehensive survey of methods that study, understand,
attack, and defend explanations of DNN models. We also present a detailed
review of different metrics used to evaluate explanation methods, as well as
describe attributional attack and defense methods. We conclude with lessons and
take-aways for the community towards ensuring robust explanations of DNN model
predictions.",2211.04780v1,https://arxiv.org/pdf/2211.04780v1
"Towards Adversarially Robust Recommendation from Adaptive Fraudster
  Detection","Yuni Lai, Yulin Zhu, Wenqi Fan, Xiaoge Zhang, Kai Zhou","The robustness of recommender systems under node injection attacks has
garnered significant attention. Recently, GraphRfi, a GNN-based recommender
system, was proposed and shown to effectively mitigate the impact of injected
fake users. However, we demonstrate that GraphRfi remains vulnerable to attacks
due to the supervised nature of its fraudster detection component, where
obtaining clean labels is challenging in practice. In particular, we propose a
powerful poisoning attack, MetaC, against both GNN-based and MF-based
recommender systems. Furthermore, we analyze why GraphRfi fails under such an
attack. Then, based on our insights obtained from vulnerability analysis, we
design an adaptive fraudster detection module that explicitly considers label
uncertainty. This module can serve as a plug-in for different recommender
systems, resulting in a robust framework named PDR. Comprehensive experiments
show that our defense approach outperforms other benchmark methods under
attacks. Overall, our research presents an effective framework for integrating
fraudster detection into recommendation systems to achieve adversarial
robustness.",2211.11534v3,https://arxiv.org/pdf/2211.11534v3
"Parameter and Data Efficient Continual Pre-training for Robustness to
  Dialectal Variance in Arabic","Soumajyoti Sarkar, Kaixiang Lin, Sailik Sengupta, Leonard Lausen, Sheng Zha, Saab Mansour","The use of multilingual language models for tasks in low and high-resource
languages has been a success story in deep learning. In recent times, Arabic
has been receiving widespread attention on account of its dialectal variance.
While prior research studies have tried to adapt these multilingual models for
dialectal variants of Arabic, it still remains a challenging problem owing to
the lack of sufficient monolingual dialectal data and parallel translation data
of such dialectal variants. It remains an open problem on whether the limited
dialectical data can be used to improve the models trained in Arabic on its
dialectal variants. First, we show that multilingual-BERT (mBERT) incrementally
pretrained on Arabic monolingual data takes less training time and yields
comparable accuracy when compared to our custom monolingual Arabic model and
beat existing models (by an avg metric of +$6.41$). We then explore two
continual pre-training methods -- (1) using small amounts of dialectical data
for continual finetuning and (2) parallel Arabic to English data and a
Translation Language Modeling loss function. We show that both approaches help
improve performance on dialectal classification tasks ($+4.64$ avg. gain) when
used on monolingual models.",2211.03966v1,https://arxiv.org/pdf/2211.03966v1
"Robust Manifold Nonnegative Tucker Factorization for Tensor Data
  Representation","Jianyu Wang, Linruize Tang, Jie Chen, Jingdong Chen","Nonnegative Tucker Factorization (NTF) minimizes the euclidean distance or
Kullback-Leibler divergence between the original data and its low-rank
approximation which often suffers from grossly corruptions or outliers and the
neglect of manifold structures of data. In particular, NTF suffers from
rotational ambiguity, whose solutions with and without rotation transformations
are equally in the sense of yielding the maximum likelihood. In this paper, we
propose three Robust Manifold NTF algorithms to handle outliers by
incorporating structural knowledge about the outliers. They first applies a
half-quadratic optimization algorithm to transform the problem into a general
weighted NTF where the weights are influenced by the outliers. Then, we
introduce the correntropy induced metric, Huber function and Cauchy function
for weights respectively, to handle the outliers. Finally, we introduce a
manifold regularization to overcome the rotational ambiguity of NTF. We have
compared the proposed method with a number of representative references
covering major branches of NTF on a variety of real-world image databases.
Experimental results illustrate the effectiveness of the proposed method under
two evaluation metrics (accuracy and nmi).",2211.03934v1,https://arxiv.org/pdf/2211.03934v1
Are AlphaZero-like Agents Robust to Adversarial Perturbations?,"Li-Cheng Lan, Huan Zhang, Ti-Rong Wu, Meng-Yu Tsai, I-Chen Wu, Cho-Jui Hsieh","The success of AlphaZero (AZ) has demonstrated that neural-network-based Go
AIs can surpass human performance by a large margin. Given that the state space
of Go is extremely large and a human player can play the game from any legal
state, we ask whether adversarial states exist for Go AIs that may lead them to
play surprisingly wrong actions. In this paper, we first extend the concept of
adversarial examples to the game of Go: we generate perturbed states that are
``semantically'' equivalent to the original state by adding meaningless moves
to the game, and an adversarial state is a perturbed state leading to an
undoubtedly inferior action that is obvious even for Go beginners. However,
searching the adversarial state is challenging due to the large, discrete, and
non-differentiable search space. To tackle this challenge, we develop the first
adversarial attack on Go AIs that can efficiently search for adversarial states
by strategically reducing the search space. This method can also be extended to
other board games such as NoGo. Experimentally, we show that the actions taken
by both Policy-Value neural network (PV-NN) and Monte Carlo tree search (MCTS)
can be misled by adding one or two meaningless stones; for example, on 58\% of
the AlphaGo Zero self-play games, our method can make the widely used KataGo
agent with 50 simulations of MCTS plays a losing action by adding two
meaningless stones. We additionally evaluated the adversarial examples found by
our algorithm with amateur human Go players and 90\% of examples indeed lead
the Go agent to play an obviously inferior action. Our code is available at
\url{https://PaperCode.cc/GoAttack}.",2211.03769v1,https://arxiv.org/pdf/2211.03769v1
"Max-Min Off-Policy Actor-Critic Method Focusing on Worst-Case Robustness
  to Model Misspecification","Takumi Tanabe, Rei Sato, Kazuto Fukuchi, Jun Sakuma, Youhei Akimoto","In the field of reinforcement learning, because of the high cost and risk of
policy training in the real world, policies are trained in a simulation
environment and transferred to the corresponding real-world environment.
However, the simulation environment does not perfectly mimic the real-world
environment, lead to model misspecification. Multiple studies report
significant deterioration of policy performance in a real-world environment. In
this study, we focus on scenarios involving a simulation environment with
uncertainty parameters and the set of their possible values, called the
uncertainty parameter set. The aim is to optimize the worst-case performance on
the uncertainty parameter set to guarantee the performance in the corresponding
real-world environment. To obtain a policy for the optimization, we propose an
off-policy actor-critic approach called the Max-Min Twin Delayed Deep
Deterministic Policy Gradient algorithm (M2TD3), which solves a max-min
optimization problem using a simultaneous gradient ascent descent approach.
Experiments in multi-joint dynamics with contact (MuJoCo) environments show
that the proposed method exhibited a worst-case performance superior to several
baseline approaches.",2211.03413v2,https://arxiv.org/pdf/2211.03413v2
"Knowledge is Power: Understanding Causality Makes Legal judgment
  Prediction Models More Generalizable and Robust","Haotian Chen, Lingwei Zhang, Yiran Liu, Fanchao Chen, Yang Yu","Legal Judgment Prediction (LJP), aiming to predict a judgment based on fact
descriptions according to rule of law, serves as legal assistance to mitigate
the great work burden of limited legal practitioners. Most existing methods
apply various large-scale pre-trained language models (PLMs) finetuned in LJP
tasks to obtain consistent improvements. However, we discover the fact that the
state-of-the-art (SOTA) model makes judgment predictions according to
irrelevant (or non-casual) information. The violation of rule of law not only
weakens the robustness and generalization ability of models but also results in
severe social problems like discrimination. In this paper, we use causal
structural models (SCMs) to theoretically analyze how LJP models learn to make
decisions and why they can succeed in passing the traditional testing paradigm
without learning causality. According to our analysis, we provide two solutions
intervening on data and model by causality, respectively. In detail, we first
distinguish non-causal information by applying the open information extraction
(OIE) technique. Then, we propose a method named the Causal Information
Enhanced SAmpling Method (CIESAM) to eliminate the non-causal information from
data. To validate our theoretical analysis, we further propose another method
using our proposed Causality-Aware Self-Attention Mechanism (CASAM) to guide
the model to learn the underlying causality knowledge in legal texts. The
confidence of CASAM in learning causal information is higher than that of
CIESAM. The extensive experimental results show that both our proposed methods
achieve state-of-the-art (SOTA) performance on three commonly used
legal-specific datasets. The stronger performance of CASAM further demonstrates
that causality is the key to the robustness and generalization ability of
models.",2211.03046v2,https://arxiv.org/pdf/2211.03046v2
Robust Lottery Tickets for Pre-trained Language Models,"Rui Zheng, Rong Bao, Yuhao Zhou, Di Liang, Sirui Wang, Wei Wu, Tao Gui, Qi Zhang, Xuanjing Huang","Recent works on Lottery Ticket Hypothesis have shown that pre-trained
language models (PLMs) contain smaller matching subnetworks(winning tickets)
which are capable of reaching accuracy comparable to the original models.
However, these tickets are proved to be notrobust to adversarial examples, and
even worse than their PLM counterparts. To address this problem, we propose a
novel method based on learning binary weight masks to identify robust tickets
hidden in the original PLMs. Since the loss is not differentiable for the
binary mask, we assign the hard concrete distribution to the masks and
encourage their sparsity using a smoothing approximation of L0
regularization.Furthermore, we design an adversarial loss objective to guide
the search for robust tickets and ensure that the tickets perform well bothin
accuracy and robustness. Experimental results show the significant improvement
of the proposed method over previous work on adversarial robustness evaluation.",2211.03013v1,https://arxiv.org/pdf/2211.03013v1
"A Robust and Low Complexity Deep Learning Model for Remote Sensing Image
  Classification","Cam Le, Lam Pham, Nghia NVN, Truong Nguyen, Le Hong Trang","In this paper, we present a robust and low complexity deep learning model for
Remote Sensing Image Classification (RSIC), the task of identifying the scene
of a remote sensing image. In particular, we firstly evaluate different low
complexity and benchmark deep neural networks: MobileNetV1, MobileNetV2,
NASNetMobile, and EfficientNetB0, which present the number of trainable
parameters lower than 5 Million (M). After indicating best network
architecture, we further improve the network performance by applying attention
schemes to multiple feature maps extracted from middle layers of the network.
To deal with the issue of increasing the model footprint as using attention
schemes, we apply the quantization technique to satisfy the maximum of 20 MB
memory occupation. By conducting extensive experiments on the benchmark
datasets NWPU-RESISC45, we achieve a robust and low-complexity model, which is
very competitive to the state-of-the-art systems and potential for real-life
applications on edge devices.",2211.02820v2,https://arxiv.org/pdf/2211.02820v2
Fairness-aware Regression Robust to Adversarial Attacks,"Yulu Jin, Lifeng Lai","In this paper, we take a first step towards answering the question of how to
design fair machine learning algorithms that are robust to adversarial attacks.
Using a minimax framework, we aim to design an adversarially robust fair
regression model that achieves optimal performance in the presence of an
attacker who is able to add a carefully designed adversarial data point to the
dataset or perform a rank-one attack on the dataset. By solving the proposed
nonsmooth nonconvex-nonconcave minimax problem, the optimal adversary as well
as the robust fairness-aware regression model are obtained. For both synthetic
data and real-world datasets, numerical results illustrate that the proposed
adversarially robust fair models have better performance on poisoned datasets
than other fair machine learning models in both prediction accuracy and
group-based fairness measure.",2211.04449v1,https://arxiv.org/pdf/2211.04449v1
An Adversarial Robustness Perspective on the Topology of Neural Networks,"Morgane Goibert, Thomas Ricatte, Elvis Dohmatob","In this paper, we investigate the impact of neural networks (NNs) topology on
adversarial robustness. Specifically, we study the graph produced when an input
traverses all the layers of a NN, and show that such graphs are different for
clean and adversarial inputs. We find that graphs from clean inputs are more
centralized around highway edges, whereas those from adversaries are more
diffuse, leveraging under-optimized edges. Through experiments on a variety of
datasets and architectures, we show that these under-optimized edges are a
source of adversarial vulnerability and that they can be used to detect
adversarial inputs.",2211.02675v1,https://arxiv.org/pdf/2211.02675v1
"Robustness of Fusion-based Multimodal Classifiers to Cross-Modal Content
  Dilutions","Gaurav Verma, Vishwa Vinay, Ryan A. Rossi, Srijan Kumar","As multimodal learning finds applications in a wide variety of high-stakes
societal tasks, investigating their robustness becomes important. Existing work
has focused on understanding the robustness of vision-and-language models to
imperceptible variations on benchmark tasks. In this work, we investigate the
robustness of multimodal classifiers to cross-modal dilutions - a plausible
variation. We develop a model that, given a multimodal (image + text) input,
generates additional dilution text that (a) maintains relevance and topical
coherence with the image and existing text, and (b) when added to the original
text, leads to misclassification of the multimodal input. Via experiments on
Crisis Humanitarianism and Sentiment Detection tasks, we find that the
performance of task-specific fusion-based multimodal classifiers drops by 23.3%
and 22.5%, respectively, in the presence of dilutions generated by our model.
Metric-based comparisons with several baselines and human evaluations indicate
that our dilutions show higher relevance and topical coherence, while
simultaneously being more effective at demonstrating the brittleness of the
multimodal classifiers. Our work aims to highlight and encourage further
research on the robustness of deep multimodal models to realistic variations,
especially in human-facing societal applications. The code and other resources
are available at https://claws-lab.github.io/multimodal-robustness/.",2211.02646v1,https://arxiv.org/pdf/2211.02646v1
"Improving Adversarial Robustness to Sensitivity and Invariance Attacks
  with Deep Metric Learning","Anaelia Ovalle, Evan Czyzycki, Cho-Jui Hsieh","Intentionally crafted adversarial samples have effectively exploited
weaknesses in deep neural networks. A standard method in adversarial robustness
assumes a framework to defend against samples crafted by minimally perturbing a
sample such that its corresponding model output changes. These sensitivity
attacks exploit the model's sensitivity toward task-irrelevant features.
Another form of adversarial sample can be crafted via invariance attacks, which
exploit the model underestimating the importance of relevant features. Previous
literature has indicated a tradeoff in defending against both attack types
within a strictly L_p bounded defense. To promote robustness toward both types
of attacks beyond Euclidean distance metrics, we use metric learning to frame
adversarial regularization as an optimal transport problem. Our preliminary
results indicate that regularizing over invariant perturbations in our
framework improves both invariant and sensitivity defense.",2211.02468v1,https://arxiv.org/pdf/2211.02468v1
Robust Time Series Chain Discovery with Incremental Nearest Neighbors,"Li Zhang, Yan Zhu, Yifeng Gao, Jessica Lin","Time series motif discovery has been a fundamental task to identify
meaningful repeated patterns in time series. Recently, time series chains were
introduced as an expansion of time series motifs to identify the continuous
evolving patterns in time series data. Informally, a time series chain (TSC) is
a temporally ordered set of time series subsequences, in which every
subsequence is similar to the one that precedes it, but the last and the first
can be arbitrarily dissimilar. TSCs are shown to be able to reveal latent
continuous evolving trends in the time series, and identify precursors of
unusual events in complex systems. Despite its promising interpretability,
unfortunately, we have observed that existing TSC definitions lack the ability
to accurately cover the evolving part of a time series: the discovered chains
can be easily cut by noise and can include non-evolving patterns, making them
impractical in real-world applications. Inspired by a recent work that tracks
how the nearest neighbor of a time series subsequence changes over time, we
introduce a new TSC definition which is much more robust to noise in the data,
in the sense that they can better locate the evolving patterns while excluding
the non-evolving ones. We further propose two new quality metrics to rank the
discovered chains. With extensive empirical evaluations, we demonstrate that
the proposed TSC definition is significantly more robust to noise than the
state of the art, and the top ranked chains discovered can reveal meaningful
regularities in a variety of real world datasets.",2211.02146v1,https://arxiv.org/pdf/2211.02146v1
Robust Few-shot Learning Without Using any Adversarial Samples,"Gaurav Kumar Nayak, Ruchit Rawal, Inder Khatri, Anirban Chakraborty","The high cost of acquiring and annotating samples has made the `few-shot'
learning problem of prime importance. Existing works mainly focus on improving
performance on clean data and overlook robustness concerns on the data
perturbed with adversarial noise. Recently, a few efforts have been made to
combine the few-shot problem with the robustness objective using sophisticated
Meta-Learning techniques. These methods rely on the generation of adversarial
samples in every episode of training, which further adds a computational
burden. To avoid such time-consuming and complicated procedures, we propose a
simple but effective alternative that does not require any adversarial samples.
Inspired by the cognitive decision-making process in humans, we enforce
high-level feature matching between the base class data and their corresponding
low-frequency samples in the pretraining stage via self distillation. The model
is then fine-tuned on the samples of novel classes where we additionally
improve the discriminability of low-frequency query set features via cosine
similarity. On a 1-shot setting of the CIFAR-FS dataset, our method yields a
massive improvement of $60.55\%$ & $62.05\%$ in adversarial accuracy on the PGD
and state-of-the-art Auto Attack, respectively, with a minor drop in clean
accuracy compared to the baseline. Moreover, our method only takes $1.69\times$
of the standard training time while being $\approx$ $5\times$ faster than
state-of-the-art adversarial meta-learning methods. The code is available at
https://github.com/vcl-iisc/robust-few-shot-learning.",2211.01598v1,https://arxiv.org/pdf/2211.01598v1
Isometric Representations in Neural Networks Improve Robustness,"Kosio Beshkov, Jonas Verhellen, Mikkel Elle Lepperød","Artificial and biological agents cannon learn given completely random and
unstructured data. The structure of data is encoded in the metric relationships
between data points. In the context of neural networks, neuronal activity
within a layer forms a representation reflecting the transformation that the
layer implements on its inputs. In order to utilize the structure in the data
in a truthful manner, such representations should reflect the input distances
and thus be continuous and isometric. Supporting this statement, recent
findings in neuroscience propose that generalization and robustness are tied to
neural representations being continuously differentiable. In machine learning,
most algorithms lack robustness and are generally thought to rely on aspects of
the data that differ from those that humans use, as is commonly seen in
adversarial attacks. During cross-entropy classification, the metric and
structural properties of network representations are usually broken both
between and within classes. This side effect from training can lead to
instabilities under perturbations near locations where such structure is not
preserved. One of the standard solutions to obtain robustness is to add ad hoc
regularization terms, but to our knowledge, forcing representations to preserve
the metric structure of the input data as a stabilising mechanism has not yet
been studied. In this work, we train neural networks to perform classification
while simultaneously maintaining within-class metric structure, leading to
isometric within-class representations. Such network representations turn out
to be beneficial for accurate and robust inference. By stacking layers with
this property we create a network architecture that facilitates hierarchical
manipulation of internal neural representations. Finally, we verify that
isometric regularization improves the robustness to adversarial attacks on
MNIST.",2211.01236v1,https://arxiv.org/pdf/2211.01236v1
"Investigating the robustness of a learning-based method for quantitative
  phase retrieval from propagation-based x-ray phase contrast measurements
  under laboratory conditions","Rucha Deshpande, Ashish Avachat, Frank J. Brooks, Mark A. Anastasio","Quantitative phase retrieval (QPR) in propagation-based x-ray phase contrast
imaging of heterogeneous and structurally complicated objects is challenging
under laboratory conditions due to partial spatial coherence and
polychromaticity. A learning-based method (LBM) provides a non-linear approach
to this problem while not being constrained by restrictive assumptions about
object properties and beam coherence. In this work, a LBM was assessed for its
applicability under practical scenarios by evaluating its robustness and
generalizability under typical experimental variations. Towards this end, an
end-to-end LBM was employed for QPR under laboratory conditions and its
robustness was investigated across various system and object conditions. The
robustness of the method was tested via varying propagation distances and its
generalizability with respect to object structure and experimental data was
also tested. Although the LBM was stable under the studied variations, its
successful deployment was found to be affected by choices pertaining to data
pre-processing, network training considerations and system modeling. To our
knowledge, we demonstrated for the first time, the potential applicability of
an end-to-end learning-based quantitative phase retrieval method, trained on
simulated data, to experimental propagation-based x-ray phase contrast
measurements acquired under laboratory conditions. We considered conditions of
polychromaticity, partial spatial coherence, and high noise levels, typical to
laboratory conditions. This work further explored the robustness of this method
to practical variations in propagation distances and object structure with the
goal of assessing its potential for experimental use. Such an exploration of
any LBM (irrespective of its network architecture) before practical deployment
provides an understanding of its potential behavior under experimental
settings.",2211.01372v1,https://arxiv.org/pdf/2211.01372v1
"An Easy-to-use and Robust Approach for the Differentially Private
  De-Identification of Clinical Textual Documents","Yakini Tchouka, Jean-François Couchot, David Laiymani","Unstructured textual data is at the heart of healthcare systems. For obvious
privacy reasons, these documents are not accessible to researchers as long as
they contain personally identifiable information. One way to share this data
while respecting the legislative framework (notably GDPR or HIPAA) is, within
the medical structures, to de-identify it, i.e. to detect the personal
information of a person through a Named Entity Recognition (NER) system and
then replacing it to make it very difficult to associate the document with the
person. The challenge is having reliable NER and substitution tools without
compromising confidentiality and consistency in the document. Most of the
conducted research focuses on English medical documents with coarse
substitutions by not benefiting from advances in privacy. This paper shows how
an efficient and differentially private de-identification approach can be
achieved by strengthening the less robust de-identification method and by
adapting state-of-the-art differentially private mechanisms for substitution
purposes. The result is an approach for de-identifying clinical documents in
French language, but also generalizable to other languages and whose robustness
is mathematically proven.",2211.01147v1,https://arxiv.org/pdf/2211.01147v1
"Causal Counterfactuals for Improving the Robustness of Reinforcement
  Learning","Tom He, Jasmina Gajcin, Ivana Dusparic","Reinforcement learning (RL) is used in various robotic applications. RL
enables agents to learn tasks autonomously by interacting with the environment.
The more critical the tasks are, the higher the demand for the robustness of
the RL systems. Causal RL combines RL and causal inference to make RL more
robust. Causal RL agents use a causal representation to capture the invariant
causal mechanisms that can be transferred from one task to another. Currently,
there is limited research in Causal RL, and existing solutions are usually not
complete or feasible for real-world applications. In this work, we propose
CausalCF, the first complete Causal RL solution incorporating ideas from Causal
Curiosity and CoPhy. Causal Curiosity provides an approach for using
interventions, and CoPhy is modified to enable the RL agent to perform
counterfactuals. Causal Curiosity has been applied to robotic grasping and
manipulation tasks in CausalWorld. CausalWorld provides a realistic simulation
environment based on the TriFinger robot. We apply CausalCF to complex robotic
tasks and show that it improves the RL agent's robustness using CausalWorld.",2211.05551v3,https://arxiv.org/pdf/2211.05551v3
"Certified Robustness of Quantum Classifiers against Adversarial Examples
  through Quantum Noise","Jhih-Cing Huang, Yu-Lin Tsai, Chao-Han Huck Yang, Cheng-Fang Su, Chia-Mu Yu, Pin-Yu Chen, Sy-Yen Kuo","Recently, quantum classifiers have been found to be vulnerable to adversarial
attacks, in which quantum classifiers are deceived by imperceptible noises,
leading to misclassification. In this paper, we propose the first theoretical
study demonstrating that adding quantum random rotation noise can improve
robustness in quantum classifiers against adversarial attacks. We link the
definition of differential privacy and show that the quantum classifier trained
with the natural presence of additive noise is differentially private. Finally,
we derive a certified robustness bound to enable quantum classifiers to defend
against adversarial examples, supported by experimental results simulated with
noises from IBM's 7-qubits device.",2211.00887v2,https://arxiv.org/pdf/2211.00887v2
Maximum Likelihood Distillation for Robust Modulation Classification,"Javier Maroto, Gérôme Bovet, Pascal Frossard","Deep Neural Networks are being extensively used in communication systems and
Automatic Modulation Classification (AMC) in particular. However, they are very
susceptible to small adversarial perturbations that are carefully crafted to
change the network decision. In this work, we build on knowledge distillation
ideas and adversarial training in order to build more robust AMC systems. We
first outline the importance of the quality of the training data in terms of
accuracy and robustness of the model. We then propose to use the Maximum
Likelihood function, which could solve the AMC problem in offline settings, to
generate better training labels. Those labels teach the model to be uncertain
in challenging conditions, which permits to increase the accuracy, as well as
the robustness of the model when combined with adversarial training.
Interestingly, we observe that this increase in performance transfers to online
settings, where the Maximum Likelihood function cannot be used in practice.
Overall, this work highlights the potential of learning to be uncertain in
difficult scenarios, compared to directly removing label noise.",2211.00748v1,https://arxiv.org/pdf/2211.00748v1
"Privacy Induces Robustness: Information-Computation Gaps and Sparse Mean
  Estimation","Kristian Georgiev, Samuel B. Hopkins","We establish a simple connection between robust and differentially-private
algorithms: private mechanisms which perform well with very high probability
are automatically robust in the sense that they retain accuracy even if a
constant fraction of the samples they receive are adversarially corrupted.
Since optimal mechanisms typically achieve these high success probabilities,
our results imply that optimal private mechanisms for many basic statistics
problems are robust.
  We investigate the consequences of this observation for both algorithms and
computational complexity across different statistical problems. Assuming the
Brennan-Bresler secret-leakage planted clique conjecture, we demonstrate a
fundamental tradeoff between computational efficiency, privacy leakage, and
success probability for sparse mean estimation. Private algorithms which match
this tradeoff are not yet known -- we achieve that (up to polylogarithmic
factors) in a polynomially-large range of parameters via the Sum-of-Squares
method.
  To establish an information-computation gap for private sparse mean
estimation, we also design new (exponential-time) mechanisms using fewer
samples than efficient algorithms must use. Finally, we give evidence for
privacy-induced information-computation gaps for several other statistics and
learning problems, including PAC learning parity functions and estimation of
the mean of a multivariate Gaussian.",2211.00724v2,https://arxiv.org/pdf/2211.00724v2
DensePure: Understanding Diffusion Models towards Adversarial Robustness,"Chaowei Xiao, Zhongzhu Chen, Kun Jin, Jiongxiao Wang, Weili Nie, Mingyan Liu, Anima Anandkumar, Bo Li, Dawn Song","Diffusion models have been recently employed to improve certified robustness
through the process of denoising. However, the theoretical understanding of why
diffusion models are able to improve the certified robustness is still lacking,
preventing from further improvement. In this study, we close this gap by
analyzing the fundamental properties of diffusion models and establishing the
conditions under which they can enhance certified robustness. This deeper
understanding allows us to propose a new method DensePure, designed to improve
the certified robustness of a pretrained model (i.e. classifier). Given an
(adversarial) input, DensePure consists of multiple runs of denoising via the
reverse process of the diffusion model (with different random seeds) to get
multiple reversed samples, which are then passed through the classifier,
followed by majority voting of inferred labels to make the final prediction.
This design of using multiple runs of denoising is informed by our theoretical
analysis of the conditional distribution of the reversed sample. Specifically,
when the data density of a clean sample is high, its conditional density under
the reverse process in a diffusion model is also high; thus sampling from the
latter conditional distribution can purify the adversarial example and return
the corresponding clean sample with a high probability. By using the highest
density point in the conditional distribution as the reversed sample, we
identify the robust region of a given instance under the diffusion model's
reverse process. We show that this robust region is a union of multiple convex
sets, and is potentially much larger than the robust regions identified in
previous works. In practice, DensePure can approximate the label of the high
density region in the conditional distribution so that it can enhance certified
robustness.",2211.00322v1,https://arxiv.org/pdf/2211.00322v1
Robust Direct Learning for Causal Data Fusion,"Xinyu Li, Yilin Li, Qing Cui, Longfei Li, Jun Zhou","In the era of big data, the explosive growth of multi-source heterogeneous
data offers many exciting challenges and opportunities for improving the
inference of conditional average treatment effects. In this paper, we
investigate homogeneous and heterogeneous causal data fusion problems under a
general setting that allows for the presence of source-specific covariates. We
provide a direct learning framework for integrating multi-source data that
separates the treatment effect from other nuisance functions, and achieves
double robustness against certain misspecification. To improve estimation
precision and stability, we propose a causal information-aware weighting
function motivated by theoretical insights from the semiparametric efficiency
theory; it assigns larger weights to samples containing more causal information
with high interpretability. We introduce a two-step algorithm, the weighted
multi-source direct learner, based on constructing a pseudo-outcome and
regressing it on covariates under a weighted least square criterion; it offers
us a powerful tool for causal data fusion, enjoying the advantages of easy
implementation, double robustness and model flexibility. In simulation studies,
we demonstrate the effectiveness of our proposed methods in both homogeneous
and heterogeneous causal data fusion scenarios.",2211.00249v1,https://arxiv.org/pdf/2211.00249v1
"ARDIR: Improving Robustness using Knowledge Distillation of Internal
  Representation","Tomokatsu Takahashi, Masanori Yamada, Yuuki Yamanaka, Tomoya Yamashita","Adversarial training is the most promising method for learning robust models
against adversarial examples. A recent study has shown that knowledge
distillation between the same architectures is effective in improving the
performance of adversarial training. Exploiting knowledge distillation is a new
approach to improve adversarial training and has attracted much attention.
However, its performance is still insufficient. Therefore, we propose
Adversarial Robust Distillation with Internal Representation~(ARDIR) to utilize
knowledge distillation even more effectively. In addition to the output of the
teacher model, ARDIR uses the internal representation of the teacher model as a
label for adversarial training. This enables the student model to be trained
with richer, more informative labels. As a result, ARDIR can learn more robust
student models. We show that ARDIR outperforms previous methods in our
experiments.",2211.00239v1,https://arxiv.org/pdf/2211.00239v1
"A robust estimator of mutual information for deep learning
  interpretability","Davide Piras, Hiranya V. Peiris, Andrew Pontzen, Luisa Lucie-Smith, Ningyuan Guo, Brian Nord","We develop the use of mutual information (MI), a well-established metric in
information theory, to interpret the inner workings of deep learning models. To
accurately estimate MI from a finite number of samples, we present GMM-MI
(pronounced $``$Jimmie$""$), an algorithm based on Gaussian mixture models that
can be applied to both discrete and continuous settings. GMM-MI is
computationally efficient, robust to the choice of hyperparameters and provides
the uncertainty on the MI estimate due to the finite sample size. We
extensively validate GMM-MI on toy data for which the ground truth MI is known,
comparing its performance against established mutual information estimators. We
then demonstrate the use of our MI estimator in the context of representation
learning, working with synthetic data and physical datasets describing highly
non-linear processes. We train deep learning models to encode high-dimensional
data within a meaningful compressed (latent) representation, and use GMM-MI to
quantify both the level of disentanglement between the latent variables, and
their association with relevant physical quantities, thus unlocking the
interpretability of the latent representation. We make GMM-MI publicly
available.",2211.00024v2,https://arxiv.org/pdf/2211.00024v2
Scoring Black-Box Models for Adversarial Robustness,"Jian Vora, Pranay Reddy Samala","Deep neural networks are susceptible to adversarial inputs and various
methods have been proposed to defend these models against adversarial attacks
under different perturbation models. The robustness of models to adversarial
attacks has been analyzed by first constructing adversarial inputs for the
model, and then testing the model performance on the constructed adversarial
inputs. Most of these attacks require the model to be white-box, need access to
data labels, and finding adversarial inputs can be computationally expensive.
We propose a simple scoring method for black-box models which indicates their
robustness to adversarial input. We show that adversarially more robust models
have a smaller $l_1$-norm of LIME weights and sharper explanations.",2210.17140v1,https://arxiv.org/pdf/2210.17140v1
"ViTASD: Robust Vision Transformer Baselines for Autism Spectrum Disorder
  Facial Diagnosis","Xu Cao, Wenqian Ye, Elena Sizikova, Xue Bai, Megan Coffee, Hongwu Zeng, Jianguo Cao","Autism spectrum disorder (ASD) is a lifelong neurodevelopmental disorder with
very high prevalence around the world. Research progress in the field of ASD
facial analysis in pediatric patients has been hindered due to a lack of
well-established baselines. In this paper, we propose the use of the Vision
Transformer (ViT) for the computational analysis of pediatric ASD. The
presented model, known as ViTASD, distills knowledge from large facial
expression datasets and offers model structure transferability. Specifically,
ViTASD employs a vanilla ViT to extract features from patients' face images and
adopts a lightweight decoder with a Gaussian Process layer to enhance the
robustness for ASD analysis. Extensive experiments conducted on standard ASD
facial analysis benchmarks show that our method outperforms all of the
representative approaches in ASD facial analysis, while the ViTASD-L achieves a
new state-of-the-art. Our code and pretrained models are available at
https://github.com/IrohXu/ViTASD.",2210.16943v2,https://arxiv.org/pdf/2210.16943v2
FI-ODE: Certifiably Robust Forward Invariance in Neural ODEs,"Yujia Huang, Ivan Dario Jimenez Rodriguez, Huan Zhang, Yuanyuan Shi, Yisong Yue","Forward invariance is a long-studied property in control theory that is used
to certify that a dynamical system stays within some pre-specified set of
states for all time, and also admits robustness guarantees (e.g., the
certificate holds under perturbations). We propose a general framework for
training and provably certifying robust forward invariance in Neural ODEs. We
apply this framework to provide certified safety in robust continuous control.
To our knowledge, this is the first instance of training Neural ODE policies
with such non-vacuous certified guarantees. In addition, we explore the
generality of our framework by using it to certify adversarial robustness for
image classification.",2210.16940v4,https://arxiv.org/pdf/2210.16940v4
Distributionally Robust Domain Adaptation,"Akram S. Awad, George K. Atia","Domain Adaptation (DA) has recently received significant attention due to its
potential to adapt a learning model across source and target domains with
mismatched distributions. Since DA methods rely exclusively on the given source
and target domain samples, they generally yield models that are vulnerable to
noise and unable to adapt to unseen samples from the target domain, which calls
for DA methods that guarantee the robustness and generalization of the learned
models. In this paper, we propose DRDA, a distributionally robust domain
adaptation method. DRDA leverages a distributionally robust optimization (DRO)
framework to learn a robust decision function that minimizes the worst-case
target domain risk and generalizes to any sample from the target domain by
transferring knowledge from a given labeled source domain sample. We utilize
the Maximum Mean Discrepancy (MMD) metric to construct an ambiguity set of
distributions that provably contains the source and target domain distributions
with high probability. Hence, the risk is shown to upper bound the
out-of-sample target domain loss. Our experimental results demonstrate that our
formulation outperforms existing robust learning approaches.",2210.16894v1,https://arxiv.org/pdf/2210.16894v1
"Partitioned Gradient Matching-based Data Subset Selection for
  Compute-Efficient Robust ASR Training","Ashish Mittal, Durga Sivasubramanian, Rishabh Iyer, Preethi Jyothi, Ganesh Ramakrishnan","Training state-of-the-art ASR systems such as RNN-T often has a high
associated financial and environmental cost. Training with a subset of training
data could mitigate this problem if the subset selected could achieve on-par
performance with training with the entire dataset. Although there are many data
subset selection(DSS) algorithms, direct application to the RNN-T is difficult,
especially the DSS algorithms that are adaptive and use learning dynamics such
as gradients, as RNN-T tend to have gradients with a significantly larger
memory footprint. In this paper, we propose Partitioned Gradient Matching (PGM)
a novel distributable DSS algorithm, suitable for massive datasets like those
used to train RNN-T. Through extensive experiments on Librispeech 100H and
Librispeech 960H, we show that PGM achieves between 3x to 6x speedup with only
a very small accuracy degradation (under 1% absolute WER difference). In
addition, we demonstrate similar results for PGM even in settings where the
training data is corrupted with noise.",2210.16892v1,https://arxiv.org/pdf/2210.16892v1
"Robust Distributed Learning Against Both Distributional Shifts and
  Byzantine Attacks","Guanqiang Zhou, Ping Xu, Yue Wang, Zhi Tian","In distributed learning systems, robustness issues may arise from two
sources. On one hand, due to distributional shifts between training data and
test data, the trained model could exhibit poor out-of-sample performance. On
the other hand, a portion of working nodes might be subject to byzantine
attacks which could invalidate the learning result. Existing works mostly deal
with these two issues separately. In this paper, we propose a new algorithm
that equips distributed learning with robustness measures against both
distributional shifts and byzantine attacks. Our algorithm is built on recent
advances in distributionally robust optimization as well as norm-based
screening (NBS), a robust aggregation scheme against byzantine attacks. We
provide convergence proofs in three cases of the learning model being
nonconvex, convex, and strongly convex for the proposed algorithm, shedding
light on its convergence behaviors and endurability against byzantine attacks.
In particular, we deduce that any algorithm employing NBS (including ours)
cannot converge when the percentage of byzantine nodes is 1/3 or higher,
instead of 1/2, which is the common belief in current literature. The
experimental results demonstrate the effectiveness of our algorithm against
both robustness issues. To the best of our knowledge, this is the first work to
address distributional shifts and byzantine attacks simultaneously.",2210.16682v1,https://arxiv.org/pdf/2210.16682v1
Robust Boosting Forests with Richer Deep Feature Hierarchy,Jianqiao Wangni,"We propose a robust variant of boosting forest to the various adversarial
defense methods, and apply it to enhance the robustness of the deep neural
network. We retain the deep network architecture, weights, and middle layer
features, then install gradient boosting forest to select the features from
each layer of the deep network, and predict the target. For training each
decision tree, we propose a novel conservative and greedy trade-off, with
consideration for less misprediction instead of pure gain functions, therefore
being suboptimal and conservative. We actively increase tree depth to remedy
the accuracy with splits in more features, being more greedy in growing tree
depth. We propose a new task on 3D face model, whose robustness has not been
carefully studied, despite the great security and privacy concerns related to
face analytics. We tried a simple attack method on a pure convolutional neural
network (CNN) face shape estimator, making it degenerate to only output average
face shape with invisible perturbation. Our conservative-greedy boosting forest
(CGBF) on face landmark datasets showed a great improvement over original pure
deep learning methods under the adversarial attacks.",2210.16451v1,https://arxiv.org/pdf/2210.16451v1
"Elastic Weight Consolidation Improves the Robustness of Self-Supervised
  Learning Methods under Transfer","Andrius Ovsianas, Jason Ramapuram, Dan Busbridge, Eeshan Gunesh Dhekane, Russ Webb","Self-supervised representation learning (SSL) methods provide an effective
label-free initial condition for fine-tuning downstream tasks. However, in
numerous realistic scenarios, the downstream task might be biased with respect
to the target label distribution. This in turn moves the learned fine-tuned
model posterior away from the initial (label) bias-free self-supervised model
posterior. In this work, we re-interpret SSL fine-tuning under the lens of
Bayesian continual learning and consider regularization through the Elastic
Weight Consolidation (EWC) framework. We demonstrate that self-regularization
against an initial SSL backbone improves worst sub-group performance in
Waterbirds by 5% and Celeb-A by 2% when using the ViT-B/16 architecture.
Furthermore, to help simplify the use of EWC with SSL, we pre-compute and
publicly release the Fisher Information Matrix (FIM), evaluated with 10,000
ImageNet-1K variates evaluated on large modern SSL architectures including
ViT-B/16 and ResNet50 trained with DINO.",2210.16365v1,https://arxiv.org/pdf/2210.16365v1
Improving Hyperspectral Adversarial Robustness Under Multiple Attacks,"Nicholas Soucy, Salimeh Yasaei Sekeh","Semantic segmentation models classifying hyperspectral images (HSI) are
vulnerable to adversarial examples. Traditional approaches to adversarial
robustness focus on training or retraining a single network on attacked data,
however, in the presence of multiple attacks these approaches decrease in
performance compared to networks trained individually on each attack. To combat
this issue we propose an Adversarial Discriminator Ensemble Network (ADE-Net)
which focuses on attack type detection and adversarial robustness under a
unified model to preserve per data-type weight optimally while robustifiying
the overall network. In the proposed method, a discriminator network is used to
separate data by attack type into their specific attack-expert ensemble
network.",2210.16346v4,https://arxiv.org/pdf/2210.16346v4
"Investigating Ensemble Methods for Model Robustness Improvement of Text
  Classifiers","Jieyu Zhao, Xuezhi Wang, Yao Qin, Jilin Chen, Kai-Wei Chang","Large pre-trained language models have shown remarkable performance over the
past few years. These models, however, sometimes learn superficial features
from the dataset and cannot generalize to the distributions that are dissimilar
to the training scenario. There have been several approaches proposed to reduce
model's reliance on these bias features which can improve model robustness in
the out-of-distribution setting. However, existing methods usually use a fixed
low-capacity model to deal with various bias features, which ignore the
learnability of those features. In this paper, we analyze a set of existing
bias features and demonstrate there is no single model that works best for all
the cases. We further show that by choosing an appropriate bias model, we can
obtain a better robustness result than baselines with a more sophisticated
model design.",2210.16298v1,https://arxiv.org/pdf/2210.16298v1
Localized Randomized Smoothing for Collective Robustness Certification,"Jan Schuchardt, Tom Wollschläger, Aleksandar Bojchevski, Stephan Günnemann","Models for image segmentation, node classification and many other tasks map a
single input to multiple labels. By perturbing this single shared input (e.g.
the image) an adversary can manipulate several predictions (e.g. misclassify
several pixels). Collective robustness certification is the task of provably
bounding the number of robust predictions under this threat model. The only
dedicated method that goes beyond certifying each output independently is
limited to strictly local models, where each prediction is associated with a
small receptive field. We propose a more general collective robustness
certificate for all types of models. We further show that this approach is
beneficial for the larger class of softly local models, where each output is
dependent on the entire input but assigns different levels of importance to
different input regions (e.g. based on their proximity in the image). The
certificate is based on our novel localized randomized smoothing approach,
where the random perturbation strength for different input regions is
proportional to their importance for the outputs. Localized smoothing
Pareto-dominates existing certificates on both image segmentation and node
classification tasks, simultaneously offering higher accuracy and stronger
certificates.",2210.16140v3,https://arxiv.org/pdf/2210.16140v3
BEBERT: Efficient and Robust Binary Ensemble BERT,"Jiayi Tian, Chao Fang, Haonan Wang, Zhongfeng Wang","Pre-trained BERT models have achieved impressive accuracy on natural language
processing (NLP) tasks. However, their excessive amount of parameters hinders
them from efficient deployment on edge devices. Binarization of the BERT models
can significantly alleviate this issue but comes with a severe accuracy drop
compared with their full-precision counterparts. In this paper, we propose an
efficient and robust binary ensemble BERT (BEBERT) to bridge the accuracy gap.
To the best of our knowledge, this is the first work employing ensemble
techniques on binary BERTs, yielding BEBERT, which achieves superior accuracy
while retaining computational efficiency. Furthermore, we remove the knowledge
distillation procedures during ensemble to speed up the training process
without compromising accuracy. Experimental results on the GLUE benchmark show
that the proposed BEBERT significantly outperforms the existing binary BERT
models in accuracy and robustness with a 2x speedup on training time. Moreover,
our BEBERT has only a negligible accuracy loss of 0.3% compared to the
full-precision baseline while saving 15x and 13x in FLOPs and model size,
respectively. In addition, BEBERT also outperforms other compressed BERTs in
accuracy by up to 6.7%.",2210.15976v2,https://arxiv.org/pdf/2210.15976v2
Noise Injection Node Regularization for Robust Learning,"Noam Levi, Itay M. Bloch, Marat Freytsis, Tomer Volansky","We introduce Noise Injection Node Regularization (NINR), a method of
injecting structured noise into Deep Neural Networks (DNN) during the training
stage, resulting in an emergent regularizing effect. We present theoretical and
empirical evidence for substantial improvement in robustness against various
test data perturbations for feed-forward DNNs when trained under NINR. The
novelty in our approach comes from the interplay of adaptive noise injection
and initialization conditions such that noise is the dominant driver of
dynamics at the start of training. As it simply requires the addition of
external nodes without altering the existing network structure or optimization
algorithms, this method can be easily incorporated into many standard problem
specifications. We find improved stability against a number of data
perturbations, including domain shifts, with the most dramatic improvement
obtained for unstructured noise, where our technique outperforms other existing
methods such as Dropout or $L_2$ regularization, in some cases. We further show
that desirable generalization properties on clean data are generally
maintained.",2210.15764v1,https://arxiv.org/pdf/2210.15764v1
"Robust Monocular Localization of Drones by Adapting Domain Maps to Depth
  Prediction Inaccuracies","Priyesh Shukla, Sureshkumar S., Alex C. Stutts, Sathya Ravi, Theja Tulabandhula, Amit R. Trivedi","We present a novel monocular localization framework by jointly training deep
learning-based depth prediction and Bayesian filtering-based pose reasoning.
The proposed cross-modal framework significantly outperforms deep learning-only
predictions with respect to model scalability and tolerance to environmental
variations. Specifically, we show little-to-no degradation of pose accuracy
even with extremely poor depth estimates from a lightweight depth predictor.
Our framework also maintains high pose accuracy in extreme lighting variations
compared to standard deep learning, even without explicit domain adaptation. By
openly representing the map and intermediate feature maps (such as depth
estimates), our framework also allows for faster updates and reusing
intermediate predictions for other tasks, such as obstacle avoidance, resulting
in much higher resource efficiency.",2210.15559v1,https://arxiv.org/pdf/2210.15559v1
CasNet: Investigating Channel Robustness for Speech Separation,"Fan-Lin Wang, Yao-Fei Cheng, Hung-Shin Lee, Yu Tsao, Hsin-Min Wang","Recording channel mismatch between training and testing conditions has been
shown to be a serious problem for speech separation. This situation greatly
reduces the separation performance, and cannot meet the requirement of daily
use. In this study, inheriting the use of our previously constructed TAT-2mix
corpus, we address the channel mismatch problem by proposing a channel-aware
audio separation network (CasNet), a deep learning framework for end-to-end
time-domain speech separation. CasNet is implemented on top of TasNet. Channel
embedding (characterizing channel information in a mixture of multiple
utterances) generated by Channel Encoder is introduced into the separation
module by the FiLM technique. Through two training strategies, we explore two
roles that channel embedding may play: 1) a real-life noise disturbance, making
the model more robust, or 2) a guide, instructing the separation model to
retain the desired channel information. Experimental results on TAT-2mix show
that CasNet trained with both training strategies outperforms the TasNet
baseline, which does not use channel embeddings.",2210.15370v1,https://arxiv.org/pdf/2210.15370v1
"Exploiting modality-invariant feature for robust multimodal emotion
  recognition with missing modalities","Haolin Zuo, Rui Liu, Jinming Zhao, Guanglai Gao, Haizhou Li","Multimodal emotion recognition leverages complementary information across
modalities to gain performance. However, we cannot guarantee that the data of
all modalities are always present in practice. In the studies to predict the
missing data across modalities, the inherent difference between heterogeneous
modalities, namely the modality gap, presents a challenge. To address this, we
propose to use invariant features for a missing modality imagination network
(IF-MMIN) which includes two novel mechanisms: 1) an invariant feature learning
strategy that is based on the central moment discrepancy (CMD) distance under
the full-modality scenario; 2) an invariant feature based imagination module
(IF-IM) to alleviate the modality gap during the missing modalities prediction,
thus improving the robustness of multimodal joint representation. Comprehensive
experiments on the benchmark dataset IEMOCAP demonstrate that the proposed
model outperforms all baselines and invariantly improves the overall emotion
recognition performance under uncertain missing-modality conditions. We release
the code at: https://github.com/ZhuoYulang/IF-MMIN.",2210.15359v1,https://arxiv.org/pdf/2210.15359v1
"COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with
  Contrastive and Distributionally Robust Learning","Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, Arnold Overwijk","We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to
improve the generalization ability of dense retrieval by combating the
distribution shifts between source training tasks and target scenarios. To
mitigate the impact of document differences, COCO-DR continues pretraining the
language model on the target corpora to adapt the model to target distributions
via COtinuous COtrastive learning. To prepare for unseen target queries,
COCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to
reweight samples from different source query clusters for improving model
robustness over rare queries during fine-tuning. COCO-DR achieves superior
average performance on BEIR, the zero-shot retrieval benchmark. At BERT Base
scale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At
BERT Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model
which has 500x more parameters. Our analysis show the correlation between
COCO-DR's effectiveness in combating distribution shifts and improving
zero-shot accuracy. Our code and model can be found at
\url{https://github.com/OpenMatch/COCO-DR}.",2210.15212v2,https://arxiv.org/pdf/2210.15212v2
Deep Learning is Provably Robust to Symmetric Label Noise,"Carey E. Priebe, Ningyuan Huang, Soledad Villar, Cong Mu, Li Chen","Deep neural networks (DNNs) are capable of perfectly fitting the training
data, including memorizing noisy data. It is commonly believed that
memorization hurts generalization. Therefore, many recent works propose
mitigation strategies to avoid noisy data or correct memorization. In this
work, we step back and ask the question: Can deep learning be robust against
massive label noise without any mitigation? We provide an affirmative answer
for the case of symmetric label noise: We find that certain DNNs, including
under-parameterized and over-parameterized models, can tolerate massive
symmetric label noise up to the information-theoretic threshold. By appealing
to classical statistical theory and universal consistency of DNNs, we prove
that for multiclass classification, $L_1$-consistent DNN classifiers trained
under symmetric label noise can achieve Bayes optimality asymptotically if the
label noise probability is less than $\frac{K-1}{K}$, where $K \ge 2$ is the
number of classes. Our results show that for symmetric label noise, no
mitigation is necessary for $L_1$-consistent estimators. We conjecture that for
general label noise, mitigation strategies that make use of the noisy data will
outperform those that ignore the noisy data.",2210.15083v1,https://arxiv.org/pdf/2210.15083v1
"There is more than one kind of robustness: Fooling Whisper with
  adversarial examples","Raphael Olivier, Bhiksha Raj","Whisper is a recent Automatic Speech Recognition (ASR) model displaying
impressive robustness to both out-of-distribution inputs and random noise. In
this work, we show that this robustness does not carry over to adversarial
noise. We show that we can degrade Whisper performance dramatically, or even
transcribe a target sentence of our choice, by generating very small input
perturbations with Signal Noise Ratio of 35-45dB. We also show that by fooling
the Whisper language detector we can very easily degrade the performance of
multilingual models. These vulnerabilities of a widely popular open-source
model have practical security implications and emphasize the need for
adversarially robust ASR.",2210.17316v2,https://arxiv.org/pdf/2210.17316v2
"Robust Domain Adaptation for Pre-trained Multilingual Neural Machine
  Translation Models","Mathieu Grosso, Pirashanth Ratnamogan, Alexis Mathey, William Vanhuffel, Michael Fotso Fotso","Recent literature has demonstrated the potential of multilingual Neural
Machine Translation (mNMT) models. However, the most efficient models are not
well suited to specialized industries. In these cases, internal data is scarce
and expensive to find in all language pairs. Therefore, fine-tuning a mNMT
model on a specialized domain is hard. In this context, we decided to focus on
a new task: Domain Adaptation of a pre-trained mNMT model on a single pair of
language while trying to maintain model quality on generic domain data for all
language pairs. The risk of loss on generic domain and on other pairs is high.
This task is key for mNMT model adoption in the industry and is at the border
of many others. We propose a fine-tuning procedure for the generic mNMT that
combines embeddings freezing and adversarial loss. Our experiments demonstrated
that the procedure improves performances on specialized data with a minimal
loss in initial performances on generic domain for all languages pairs,
compared to a naive standard approach (+10.0 BLEU score on specialized data,
-0.01 to -0.5 BLEU on WMT and Tatoeba datasets on the other pairs with M2M100).",2210.14979v1,https://arxiv.org/pdf/2210.14979v1
"Disentangled Text Representation Learning with Information-Theoretic
  Perspective for Adversarial Robustness","Jiahao Zhao, Wenji Mao","Adversarial vulnerability remains a major obstacle to constructing reliable
NLP systems. When imperceptible perturbations are added to raw input text, the
performance of a deep learning model may drop dramatically under attacks.
Recent work argues the adversarial vulnerability of the model is caused by the
non-robust features in supervised training. Thus in this paper, we tackle the
adversarial robustness challenge from the view of disentangled representation
learning, which is able to explicitly disentangle robust and non-robust
features in text. Specifically, inspired by the variation of information (VI)
in information theory, we derive a disentangled learning objective composed of
mutual information to represent both the semantic representativeness of latent
embeddings and differentiation of robust and non-robust features. On the basis
of this, we design a disentangled learning network to estimate these mutual
information. Experiments on text classification and entailment tasks show that
our method significantly outperforms the representative methods under
adversarial attacks, indicating that discarding non-robust features is critical
for improving adversarial robustness.",2210.14957v1,https://arxiv.org/pdf/2210.14957v1
Uncertainty-based Meta-Reinforcement Learning for Robust Radar Tracking,"Julius Ott, Lorenzo Servadei, Gianfranco Mauro, Thomas Stadelmayer, Avik Santra, Robert Wille","Nowadays, Deep Learning (DL) methods often overcome the limitations of
traditional signal processing approaches. Nevertheless, DL methods are barely
applied in real-life applications. This is mainly due to limited robustness and
distributional shift between training and test data. To this end, recent work
has proposed uncertainty mechanisms to increase their reliability. Besides,
meta-learning aims at improving the generalization capability of DL models. By
taking advantage of that, this paper proposes an uncertainty-based
Meta-Reinforcement Learning (Meta-RL) approach with Out-of-Distribution (OOD)
detection. The presented method performs a given task in unseen environments
and provides information about its complexity. This is done by determining
first and second-order statistics on the estimated reward. Using information
about its complexity, the proposed algorithm is able to point out when tracking
is reliable. To evaluate the proposed method, we benchmark it on a
radar-tracking dataset. There, we show that our method outperforms related
Meta-RL approaches on unseen tracking scenarios in peak performance by 16% and
the baseline by 35% while detecting OOD data with an F1-Score of 72%. This
shows that our method is robust to environmental changes and reliably detects
OOD scenarios.",2210.14532v1,https://arxiv.org/pdf/2210.14532v1
Robust Contextual Linear Bandits,"Rong Zhu, Branislav Kveton","Model misspecification is a major consideration in applications of
statistical methods and machine learning. However, it is often neglected in
contextual bandits. This paper studies a common form of misspecification, an
inter-arm heterogeneity that is not captured by context. To address this issue,
we assume that the heterogeneity arises due to arm-specific random variables,
which can be learned. We call this setting a robust contextual bandit. The
arm-specific variables explain the unknown inter-arm heterogeneity, and we
incorporate them in the robust contextual estimator of the mean reward and its
uncertainty. We develop two efficient bandit algorithms for our setting: a UCB
algorithm called RoLinUCB and a posterior-sampling algorithm called RoLinTS. We
analyze both algorithms and bound their $n$-round Bayes regret. Our experiments
show that RoLinTS is comparably statistically efficient to the classic methods
when the misspecification is low, more robust when the misspecification is
high, and significantly more computationally efficient than its naive
implementation.",2210.14483v1,https://arxiv.org/pdf/2210.14483v1
"Improving Adversarial Robustness via Joint Classification and Multiple
  Explicit Detection Classes","Sina Baharlouei, Fatemeh Sheikholeslami, Meisam Razaviyayn, Zico Kolter","This work concerns the development of deep networks that are certifiably
robust to adversarial attacks. Joint robust classification-detection was
recently introduced as a certified defense mechanism, where adversarial
examples are either correctly classified or assigned to the ""abstain"" class. In
this work, we show that such a provable framework can benefit by extension to
networks with multiple explicit abstain classes, where the adversarial examples
are adaptively assigned to those. We show that naively adding multiple abstain
classes can lead to ""model degeneracy"", then we propose a regularization
approach and a training method to counter this degeneracy by promoting full use
of the multiple abstain classes. Our experiments demonstrate that the proposed
approach consistently achieves favorable standard vs. robust verified accuracy
tradeoffs, outperforming state-of-the-art algorithms for various choices of
number of abstain classes.",2210.14410v2,https://arxiv.org/pdf/2210.14410v2
"Exploring Robustness of Prefix Tuning in Noisy Data: A Case Study in
  Financial Sentiment Analysis","Sudhandar Balakrishnan, Yihao Fang, Xioadan Zhu","The invention of transformer-based models such as BERT, GPT, and RoBERTa has
enabled researchers and financial companies to finetune these powerful models
and use them in different downstream tasks to achieve state-of-the-art
performance. Recently, a lightweight alternative (approximately 0.1% - 3% of
the original model parameters) to fine-tuning, known as prefix tuning has been
introduced. This method freezes the model parameters and only updates the
prefix to achieve performance comparable to full fine-tuning. Prefix tuning
enables researchers and financial practitioners to achieve similar results with
much fewer parameters. In this paper, we explore the robustness of prefix
tuning when facing noisy data. Our experiments demonstrate that fine-tuning is
more robust to noise than prefix tuning -- the latter method faces a
significant decrease in performance on most corrupted data sets with increasing
noise levels. Furthermore, prefix tuning has high variances in the F1 scores
compared to fine-tuning in many corruption methods. We strongly advocate that
caution should be carefully taken when applying the state-of-the-art prefix
tuning method to noisy data.",2211.05584v1,https://arxiv.org/pdf/2211.05584v1
"Adversarially Robust Medical Classification via Attentive Convolutional
  Neural Networks",Isaac Wasserman,"Convolutional neural network-based medical image classifiers have been shown
to be especially susceptible to adversarial examples. Such instabilities are
likely to be unacceptable in the future of automated diagnoses. Though
statistical adversarial example detection methods have proven to be effective
defense mechanisms, additional research is necessary that investigates the
fundamental vulnerabilities of deep-learning-based systems and how best to
build models that jointly maximize traditional and robust accuracy. This paper
presents the inclusion of attention mechanisms in CNN-based medical image
classifiers as a reliable and effective strategy for increasing robust accuracy
without sacrifice. This method is able to increase robust accuracy by up to 16%
in typical adversarial scenarios and up to 2700% in extreme cases.",2210.14405v1,https://arxiv.org/pdf/2210.14405v1
On Robust Incremental Learning over Many Multilingual Steps,"Karan Praharaj, Irina Matveeva","Recent work in incremental learning has introduced diverse approaches to
tackle catastrophic forgetting from data augmentation to optimized training
regimes. However, most of them focus on very few training steps. We propose a
method for robust incremental learning over dozens of fine-tuning steps using
data from a variety of languages. We show that a combination of
data-augmentation and an optimized training regime allows us to continue
improving the model even for as many as fifty training steps. Crucially, our
augmentation strategy does not require retaining access to previous training
data and is suitable in scenarios with privacy constraints.",2210.14307v1,https://arxiv.org/pdf/2210.14307v1
Accelerating Certified Robustness Training via Knowledge Transfer,"Pratik Vaishnavi, Kevin Eykholt, Amir Rahmati","Training deep neural network classifiers that are certifiably robust against
adversarial attacks is critical to ensuring the security and reliability of
AI-controlled systems. Although numerous state-of-the-art certified training
methods have been developed, they are computationally expensive and scale
poorly with respect to both dataset and network complexity. Widespread usage of
certified training is further hindered by the fact that periodic retraining is
necessary to incorporate new data and network improvements. In this paper, we
propose Certified Robustness Transfer (CRT), a general-purpose framework for
reducing the computational overhead of any certifiably robust training method
through knowledge transfer. Given a robust teacher, our framework uses a novel
training loss to transfer the teacher's robustness to the student. We provide
theoretical and empirical validation of CRT. Our experiments on CIFAR-10 show
that CRT speeds up certified robustness training by $8 \times$ on average
across three different architecture generations while achieving comparable
robustness to state-of-the-art methods. We also show that CRT can scale to
large-scale datasets like ImageNet.",2210.14283v1,https://arxiv.org/pdf/2210.14283v1
"Gradient-based Weight Density Balancing for Robust Dynamic Sparse
  Training","Mathias Parger, Alexander Ertl, Paul Eibensteiner, Joerg H. Mueller, Martin Winter, Markus Steinberger","Training a sparse neural network from scratch requires optimizing connections
at the same time as the weights themselves. Typically, the weights are
redistributed after a predefined number of weight updates, removing a fraction
of the parameters of each layer and inserting them at different locations in
the same layers. The density of each layer is determined using heuristics,
often purely based on the size of the parameter tensor. While the connections
per layer are optimized multiple times during training, the density of each
layer remains constant. This leaves great unrealized potential, especially in
scenarios with a high sparsity of 90% and more. We propose Global
Gradient-based Redistribution, a technique which distributes weights across all
layers - adding more weights to the layers that need them most. Our evaluation
shows that our approach is less prone to unbalanced weight distribution at
initialization than previous work and that it is able to find better performing
sparse subnetworks at very high sparsity levels.",2210.14012v2,https://arxiv.org/pdf/2210.14012v2
"Causal Information Bottleneck Boosts Adversarial Robustness of Deep
  Neural Network","Huan Hua, Jun Yan, Xi Fang, Weiquan Huang, Huilin Yin, Wancheng Ge","The information bottleneck (IB) method is a feasible defense solution against
adversarial attacks in deep learning. However, this method suffers from the
spurious correlation, which leads to the limitation of its further improvement
of adversarial robustness. In this paper, we incorporate the causal inference
into the IB framework to alleviate such a problem. Specifically, we divide the
features obtained by the IB method into robust features (content information)
and non-robust features (style information) via the instrumental variables to
estimate the causal effects. With the utilization of such a framework, the
influence of non-robust features could be mitigated to strengthen the
adversarial robustness. We make an analysis of the effectiveness of our
proposed method. The extensive experiments in MNIST, FashionMNIST, and CIFAR-10
show that our method exhibits the considerable robustness against multiple
adversarial attacks. Our code would be released.",2210.14229v1,https://arxiv.org/pdf/2210.14229v1
"FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node
  Classification","Yulin Zhu, Liang Tong, Gaolei Li, Xiapu Luo, Kai Zhou","Graph Neural Networks (GNNs) are vulnerable to data poisoning attacks, which
will generate a poisoned graph as the input to the GNN models. We present
FocusedCleaner as a poisoned graph sanitizer to effectively identify the poison
injected by attackers. Specifically, FocusedCleaner provides a sanitation
framework consisting of two modules: bi-level structural learning and victim
node detection. In particular, the structural learning module will reverse the
attack process to steadily sanitize the graph while the detection module
provides ``the focus"" -- a narrowed and more accurate search region -- to
structural learning. These two modules will operate in iterations and reinforce
each other to sanitize a poisoned graph step by step. As an important
application, we show that the adversarial robustness of GNNs trained over the
sanitized graph for the node classification task is significantly improved.
Extensive experiments demonstrate that FocusedCleaner outperforms the
state-of-the-art baselines both on poisoned graph sanitation and improving
robustness.",2210.13815v2,https://arxiv.org/pdf/2210.13815v2
Towards Robust Recommender Systems via Triple Cooperative Defense,"Qingyang Wang, Defu Lian, Chenwang Wu, Enhong Chen","Recommender systems are often susceptible to well-crafted fake profiles,
leading to biased recommendations. The wide application of recommender systems
makes studying the defense against attack necessary. Among existing defense
methods, data-processing-based methods inevitably exclude normal samples, while
model-based methods struggle to enjoy both generalization and robustness.
Considering the above limitations, we suggest integrating data processing and
robust model and propose a general framework, Triple Cooperative Defense (TCD),
which cooperates to improve model robustness through the co-training of three
models. Specifically, in each round of training, we sequentially use the
high-confidence prediction ratings (consistent ratings) of any two models as
auxiliary training data for the remaining model, and the three models
cooperatively improve recommendation robustness. Notably, TCD adds pseudo label
data instead of deleting abnormal data, which avoids the cleaning of normal
data, and the cooperative training of the three models is also beneficial to
model generalization. Through extensive experiments with five poisoning attacks
on three real-world datasets, the results show that the robustness improvement
of TCD significantly outperforms baselines. It is worth mentioning that TCD is
also beneficial for model generalizations.",2210.13762v1,https://arxiv.org/pdf/2210.13762v1
On the Robustness of Dataset Inference,"Sebastian Szyller, Rui Zhang, Jian Liu, N. Asokan","Machine learning (ML) models are costly to train as they can require a
significant amount of data, computational resources and technical expertise.
Thus, they constitute valuable intellectual property that needs protection from
adversaries wanting to steal them. Ownership verification techniques allow the
victims of model stealing attacks to demonstrate that a suspect model was in
fact stolen from theirs.
  Although a number of ownership verification techniques based on watermarking
or fingerprinting have been proposed, most of them fall short either in terms
of security guarantees (well-equipped adversaries can evade verification) or
computational cost. A fingerprinting technique, Dataset Inference (DI), has
been shown to offer better robustness and efficiency than prior methods.
  The authors of DI provided a correctness proof for linear (suspect) models.
However, in a subspace of the same setting, we prove that DI suffers from high
false positives (FPs) -- it can incorrectly identify an independent model
trained with non-overlapping data from the same distribution as stolen. We
further prove that DI also triggers FPs in realistic, non-linear suspect
models. We then confirm empirically that DI in the black-box setting leads to
FPs, with high confidence.
  Second, we show that DI also suffers from false negatives (FNs) -- an
adversary can fool DI (at the cost of incurring some accuracy loss) by
regularising a stolen model's decision boundaries using adversarial training,
thereby leading to an FN. To this end, we demonstrate that black-box DI fails
to identify a model adversarially trained from a stolen dataset -- the setting
where DI is the hardest to evade.
  Finally, we discuss the implications of our findings, the viability of
fingerprinting-based ownership verification in general, and suggest directions
for future work.",2210.13631v3,https://arxiv.org/pdf/2210.13631v3
The Robustness Limits of SoTA Vision Models to Natural Variation,"Mark Ibrahim, Quentin Garrido, Ari Morcos, Diane Bouchacourt","Recent state-of-the-art vision models introduced new architectures, learning
paradigms, and larger pretraining data, leading to impressive performance on
tasks such as classification. While previous generations of vision models were
shown to lack robustness to factors such as pose, it's unclear the extent to
which this next generation of models are more robust. To study this question,
we develop a dataset of more than 7 million images with controlled changes in
pose, position, background, lighting, and size. We study not only how robust
recent state-of-the-art models are, but also the extent to which models can
generalize variation in factors when they're present during training. We
consider a catalog of recent vision models, including vision transformers
(ViT), self-supervised models such as masked autoencoders (MAE), and models
trained on larger datasets such as CLIP. We find out-of-the-box, even today's
best models are not robust to common changes in pose, size, and background.
When some samples varied during training, we found models required a
significant portion of diversity to generalize -- though eventually robustness
did improve. When diversity is only seen for some classes however, we found
models did not generalize to other classes, unless the classes were very
similar to those seen varying during training. We hope our work will shed
further light on the blind spots of SoTA models and spur the development of
more robust vision models.",2210.13604v1,https://arxiv.org/pdf/2210.13604v1
Does Self-Rationalization Improve Robustness to Spurious Correlations?,"Alexis Ross, Matthew E. Peters, Ana Marasović","Rationalization is fundamental to human reasoning and learning. NLP models
trained to produce rationales along with predictions, called
self-rationalization models, have been investigated for their interpretability
and utility to end-users. However, the extent to which training with
human-written rationales facilitates learning remains an under-explored
question. We ask whether training models to self-rationalize can aid in their
learning to solve tasks for the right reasons. Specifically, we evaluate how
training self-rationalization models with free-text rationales affects
robustness to spurious correlations in fine-tuned encoder-decoder and
decoder-only models of six different sizes. We evaluate robustness to spurious
correlations by measuring performance on 1) manually annotated challenge
datasets and 2) subsets of original test sets where reliance on spurious
correlations would fail to produce correct answers. We find that while
self-rationalization can improve robustness to spurious correlations in
low-resource settings, it tends to hurt robustness in higher-resource settings.
Furthermore, these effects depend on model family and size, as well as on
rationale content. Together, our results suggest that explainability can come
at the cost of robustness; thus, appropriate care should be taken when training
self-rationalizing models with the goal of creating more trustworthy models.",2210.13575v1,https://arxiv.org/pdf/2210.13575v1
Robust Self-Supervised Learning with Lie Groups,"Mark Ibrahim, Diane Bouchacourt, Ari Morcos","Deep learning has led to remarkable advances in computer vision. Even so,
today's best models are brittle when presented with variations that differ even
slightly from those seen during training. Minor shifts in the pose, color, or
illumination of an object can lead to catastrophic misclassifications.
State-of-the art models struggle to understand how a set of variations can
affect different objects. We propose a framework for instilling a notion of how
objects vary in more realistic settings. Our approach applies the formalism of
Lie groups to capture continuous transformations to improve models' robustness
to distributional shifts. We apply our framework on top of state-of-the-art
self-supervised learning (SSL) models, finding that explicitly modeling
transformations with Lie groups leads to substantial performance gains of
greater than 10% for MAE on both known instances seen in typical poses now
presented in new poses, and on unknown instances in any pose. We also apply our
approach to ImageNet, finding that the Lie operator improves performance by
almost 4%. These results demonstrate the promise of learning transformations to
improve model robustness.",2210.13356v1,https://arxiv.org/pdf/2210.13356v1
"IT-RUDA: Information Theory Assisted Robust Unsupervised Domain
  Adaptation","Shima Rashidi, Ruwan Tennakoon, Aref Miri Rekavandi, Papangkorn Jessadatavornwong, Amanda Freis, Garret Huff, Mark Easton, Adrian Mouritz, Reza Hoseinnezhad, Alireza Bab-Hadiashar","Distribution shift between train (source) and test (target) datasets is a
common problem encountered in machine learning applications. One approach to
resolve this issue is to use the Unsupervised Domain Adaptation (UDA) technique
that carries out knowledge transfer from a label-rich source domain to an
unlabeled target domain. Outliers that exist in either source or target
datasets can introduce additional challenges when using UDA in practice. In
this paper, $\alpha$-divergence is used as a measure to minimize the
discrepancy between the source and target distributions while inheriting
robustness, adjustable with a single parameter $\alpha$, as the prominent
feature of this measure. Here, it is shown that the other well-known
divergence-based UDA techniques can be derived as special cases of the proposed
method. Furthermore, a theoretical upper bound is derived for the loss in the
target domain in terms of the source loss and the initial $\alpha$-divergence
between the two domains. The robustness of the proposed method is validated
through testing on several benchmarked datasets in open-set and partial UDA
setups where extra classes existing in target and source datasets are
considered as outliers.",2210.12947v1,https://arxiv.org/pdf/2210.12947v1
"TIARA: Multi-grained Retrieval for Robust Question Answering over Large
  Knowledge Bases","Yiheng Shu, Zhiwei Yu, Yuhan Li, Börje F. Karlsson, Tingting Ma, Yuzhong Qu, Chin-Yew Lin","Pre-trained language models (PLMs) have shown their effectiveness in multiple
scenarios. However, KBQA remains challenging, especially regarding coverage and
generalization settings. This is due to two main factors: i) understanding the
semantics of both questions and relevant knowledge from the KB; ii) generating
executable logical forms with both semantic and syntactic correctness. In this
paper, we present a new KBQA model, TIARA, which addresses those issues by
applying multi-grained retrieval to help the PLM focus on the most relevant KB
contexts, viz., entities, exemplary logical forms, and schema items. Moreover,
constrained decoding is used to control the output space and reduce generation
errors. Experiments over important benchmarks demonstrate the effectiveness of
our approach. TIARA outperforms previous SOTA, including those using PLMs or
oracle entity annotations, by at least 4.1 and 1.1 F1 points on GrailQA and
WebQuestionsSP, respectively.",2210.12925v1,https://arxiv.org/pdf/2210.12925v1
"Nash Equilibria and Pitfalls of Adversarial Training in Adversarial
  Robustness Games","Maria-Florina Balcan, Rattana Pukdee, Pradeep Ravikumar, Hongyang Zhang","Adversarial training is a standard technique for training adversarially
robust models. In this paper, we study adversarial training as an alternating
best-response strategy in a 2-player zero-sum game. We prove that even in a
simple scenario of a linear classifier and a statistical model that abstracts
robust vs. non-robust features, the alternating best response strategy of such
game may not converge. On the other hand, a unique pure Nash equilibrium of the
game exists and is provably robust. We support our theoretical results with
experiments, showing the non-convergence of adversarial training and the
robustness of Nash equilibrium.",2210.12606v3,https://arxiv.org/pdf/2210.12606v3
"Exploring The Landscape of Distributional Robustness for Question
  Answering Models","Anas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson, Hannaneh Hajishirzi, Ludwig Schmidt","We conduct a large empirical evaluation to investigate the landscape of
distributional robustness in question answering. Our investigation spans over
350 models and 16 question answering datasets, including a diverse set of
architectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter
tuning, in-context learning, etc.). We find that, in many cases, model
variations do not affect robustness and in-distribution performance alone
determines out-of-distribution performance. Moreover, our findings indicate
that i) zero-shot and in-context learning methods are more robust to
distribution shifts than fully fine-tuned models; ii) few-shot prompt
fine-tuned models exhibit better robustness than few-shot fine-tuned span
prediction models; iii) parameter-efficient and robustness enhancing training
methods provide no significant robustness improvements. In addition, we
publicly release all evaluations to encourage researchers to further analyze
robustness trends for question answering models.",2210.12517v1,https://arxiv.org/pdf/2210.12517v1
"Hard Gate Knowledge Distillation -- Leverage Calibration for Robust and
  Reliable Language Model","Dongkyu Lee, Zhiliang Tian, Yingxiu Zhao, Ka Chun Cheung, Nevin L. Zhang","In knowledge distillation, a student model is trained with supervisions from
both knowledge from a teacher and observations drawn from a training data
distribution. Knowledge of a teacher is considered a subject that holds
inter-class relations which send a meaningful supervision to a student; hence,
much effort has been put to find such knowledge to be distilled. In this paper,
we explore a question that has been given little attention: ""when to distill
such knowledge."" The question is answered in our work with the concept of model
calibration; we view a teacher model not only as a source of knowledge but also
as a gauge to detect miscalibration of a student. This simple and yet novel
view leads to a hard gate knowledge distillation scheme that switches between
learning from a teacher model and training data. We verify the gating mechanism
in the context of natural language generation at both the token-level and the
sentence-level. Empirical comparisons with strong baselines show that hard gate
knowledge distillation not only improves model generalization, but also
significantly lowers model calibration error.",2210.12427v1,https://arxiv.org/pdf/2210.12427v1
"Group Distributionally Robust Reinforcement Learning with Hierarchical
  Latent Variables","Mengdi Xu, Peide Huang, Yaru Niu, Visak Kumar, Jielin Qiu, Chao Fang, Kuan-Hui Lee, Xuewei Qi, Henry Lam, Bo Li, Ding Zhao","One key challenge for multi-task Reinforcement learning (RL) in practice is
the absence of task indicators. Robust RL has been applied to deal with task
ambiguity, but may result in over-conservative policies. To balance the
worst-case (robustness) and average performance, we propose Group
Distributionally Robust Markov Decision Process (GDR-MDP), a flexible
hierarchical MDP formulation that encodes task groups via a latent mixture
model. GDR-MDP identifies the optimal policy that maximizes the expected return
under the worst-possible qualified belief over task groups within an ambiguity
set. We rigorously show that GDR-MDP's hierarchical structure improves
distributional robustness by adding regularization to the worst possible
outcomes. We then develop deep RL algorithms for GDR-MDP for both value-based
and policy-based RL methods. Extensive experiments on Box2D control tasks,
MuJoCo benchmarks, and Google football platforms show that our algorithms
outperform classic robust training algorithms across diverse environments in
terms of robustness under belief uncertainties. Demos are available on our
project page (\url{https://sites.google.com/view/gdr-rl/home}).",2210.12262v1,https://arxiv.org/pdf/2210.12262v1
Triplet Losses-based Matrix Factorization for Robust Recommendations,Flavio Giobergia,"Much like other learning-based models, recommender systems can be affected by
biases in the training data. While typical evaluation metrics (e.g. hit rate)
are not concerned with them, some categories of final users are heavily
affected by these biases. In this work, we propose using multiple triplet
losses terms to extract meaningful and robust representations of users and
items. We empirically evaluate the soundness of such representations through
several ""bias-aware"" evaluation metrics, as well as in terms of stability to
changes in the training set and agreement of the predictions variance w.r.t.
that of each user.",2210.12098v1,https://arxiv.org/pdf/2210.12098v1
Robust Singular Values based on L1-norm PCA,"Duc Le, Panos P. Markopoulos","Singular-Value Decomposition (SVD) is a ubiquitous data analysis method in
engineering, science, and statistics. Singular-value estimation, in particular,
is of critical importance in an array of engineering applications, such as
channel estimation in communication systems, electromyography signal analysis,
and image compression, to name just a few. Conventional SVD of a data matrix
coincides with standard Principal-Component Analysis (PCA). The L2-norm (sum of
squared values) formulation of PCA promotes peripheral data points and, thus,
makes PCA sensitive against outliers. Naturally, SVD inherits this outlier
sensitivity. In this work, we present a novel robust non-parametric method for
SVD and singular-value estimation based on a L1-norm (sum of absolute values)
formulation, which we name L1-cSVD. Accordingly, the proposed method
demonstrates sturdy resistance against outliers and can facilitate more
reliable data analysis and processing in a wide range of engineering
applications.",2210.12097v1,https://arxiv.org/pdf/2210.12097v1
"A Causal Framework to Quantify the Robustness of Mathematical Reasoning
  with Language Models","Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schölkopf, Mrinmaya Sachan","We have recently witnessed a number of impressive results on hard
mathematical reasoning problems with language models. At the same time, the
robustness of these models has also been called into question; recent works
have shown that models can rely on shallow patterns in the problem description
when generating a solution. Building on the idea of behavioral testing, we
propose a novel framework, which pins down the causal effect of various factors
in the input, e.g., the surface form of the problem text, the operands, and
math operators on the output solution. By grounding the behavioral analysis in
a causal graph describing an intuitive reasoning process, we study the behavior
of language models in terms of robustness and sensitivity to direct
interventions in the input space. We apply our framework on a test bed of math
word problems. Our analysis shows that robustness does not appear to
continuously improve as a function of size, but the GPT-3 Davinci models (175B)
achieve a dramatic improvement in both robustness and sensitivity compared to
all other GPT variants.",2210.12023v3,https://arxiv.org/pdf/2210.12023v3
Learning Robust Dynamics through Variational Sparse Gating,"Arnav Kumar Jain, Shivakanth Sujit, Shruti Joshi, Vincent Michalski, Danijar Hafner, Samira Ebrahimi-Kahou","Learning world models from their sensory inputs enables agents to plan for
actions by imagining their future outcomes. World models have previously been
shown to improve sample-efficiency in simulated environments with few objects,
but have not yet been applied successfully to environments with many objects.
In environments with many objects, often only a small number of them are moving
or interacting at the same time. In this paper, we investigate integrating this
inductive bias of sparse interactions into the latent dynamics of world models
trained from pixels. First, we introduce Variational Sparse Gating (VSG), a
latent dynamics model that updates its feature dimensions sparsely through
stochastic binary gates. Moreover, we propose a simplified architecture Simple
Variational Sparse Gating (SVSG) that removes the deterministic pathway of
previous models, resulting in a fully stochastic transition function that
leverages the VSG mechanism. We evaluate the two model architectures in the
BringBackShapes (BBS) environment that features a large number of moving
objects and partial observability, demonstrating clear improvements over prior
models.",2210.11698v1,https://arxiv.org/pdf/2210.11698v1
"LOT: Layer-wise Orthogonal Training on Improving $\ell_2$ Certified
  Robustness","Xiaojun Xu, Linyi Li, Bo Li","Recent studies show that training deep neural networks (DNNs) with Lipschitz
constraints are able to enhance adversarial robustness and other model
properties such as stability. In this paper, we propose a layer-wise orthogonal
training method (LOT) to effectively train 1-Lipschitz convolution layers via
parametrizing an orthogonal matrix with an unconstrained matrix. We then
efficiently compute the inverse square root of a convolution kernel by
transforming the input domain to the Fourier frequency domain. On the other
hand, as existing works show that semi-supervised training helps improve
empirical robustness, we aim to bridge the gap and prove that semi-supervised
learning also improves the certified robustness of Lipschitz-bounded models. We
conduct comprehensive evaluations for LOT under different settings. We show
that LOT significantly outperforms baselines regarding deterministic l2
certified robustness, and scales to deeper neural networks. Under the
supervised scenario, we improve the state-of-the-art certified robustness for
all architectures (e.g. from 59.04% to 63.50% on CIFAR-10 and from 32.57% to
34.59% on CIFAR-100 at radius rho = 36/255 for 40-layer networks). With
semi-supervised learning over unlabelled data, we are able to improve
state-of-the-art certified robustness on CIFAR-10 at rho = 108/255 from 36.04%
to 42.39%. In addition, LOT consistently outperforms baselines on different
model architectures with only 1/3 evaluation time.",2210.11620v2,https://arxiv.org/pdf/2210.11620v2
"Multitasking Models are Robust to Structural Failure: A Neural Model for
  Bilingual Cognitive Reserve","Giannis Daras, Negin Raoof, Zoi Gkalitsiou, Alexandros G. Dimakis","We find a surprising connection between multitask learning and robustness to
neuron failures. Our experiments show that bilingual language models retain
higher performance under various neuron perturbations, such as random
deletions, magnitude pruning and weight noise compared to equivalent
monolingual ones. We provide a theoretical justification for this robustness by
mathematically analyzing linear representation learning and showing that
multitasking creates more robust representations. Our analysis connects
robustness to spectral properties of the learned representation and proves that
multitasking leads to higher robustness for diverse task vectors. We
open-source our code and models:
https://github.com/giannisdaras/multilingual_robustness",2210.11618v1,https://arxiv.org/pdf/2210.11618v1
"Global Convergence of Direct Policy Search for State-Feedback
  $\mathcal{H}_\infty$ Robust Control: A Revisit of Nonsmooth Synthesis with
  Goldstein Subdifferential","Xingang Guo, Bin Hu","Direct policy search has been widely applied in modern reinforcement learning
and continuous control. However, the theoretical properties of direct policy
search on nonsmooth robust control synthesis have not been fully understood.
The optimal $\mathcal{H}_\infty$ control framework aims at designing a policy
to minimize the closed-loop $\mathcal{H}_\infty$ norm, and is arguably the most
fundamental robust control paradigm. In this work, we show that direct policy
search is guaranteed to find the global solution of the robust
$\mathcal{H}_\infty$ state-feedback control design problem. Notice that policy
search for optimal $\mathcal{H}_\infty$ control leads to a constrained
nonconvex nonsmooth optimization problem, where the nonconvex feasible set
consists of all the policies stabilizing the closed-loop dynamics. We show that
for this nonsmooth optimization problem, all Clarke stationary points are
global minimum. Next, we identify the coerciveness of the closed-loop
$\mathcal{H}_\infty$ objective function, and prove that all the sublevel sets
of the resultant policy search problem are compact. Based on these properties,
we show that Goldstein's subgradient method and its implementable variants can
be guaranteed to stay in the nonconvex feasible set and eventually find the
global optimal solution of the $\mathcal{H}_\infty$ state-feedback synthesis
problem. Our work builds a new connection between nonconvex nonsmooth
optimization theory and robust control, leading to an interesting global
convergence result for direct policy search on optimal $\mathcal{H}_\infty$
synthesis.",2210.11577v1,https://arxiv.org/pdf/2210.11577v1
Learning Sample Reweighting for Accuracy and Adversarial Robustness,"Chester Holtz, Tsui-Wei Weng, Gal Mishne","There has been great interest in enhancing the robustness of neural network
classifiers to defend against adversarial perturbations through adversarial
training, while balancing the trade-off between robust accuracy and standard
accuracy. We propose a novel adversarial training framework that learns to
reweight the loss associated with individual training samples based on a notion
of class-conditioned margin, with the goal of improving robust generalization.
We formulate weighted adversarial training as a bilevel optimization problem
with the upper-level problem corresponding to learning a robust classifier, and
the lower-level problem corresponding to learning a parametric function that
maps from a sample's \textit{multi-class margin} to an importance weight.
Extensive experiments demonstrate that our approach consistently improves both
clean and robust accuracy compared to related methods and state-of-the-art
baselines.",2210.11513v1,https://arxiv.org/pdf/2210.11513v1
"TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting
  Decomposition","Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, Kui Jia","Creation of 3D content by stylization is a promising yet challenging problem
in computer vision and graphics research. In this work, we focus on stylizing
photorealistic appearance renderings of a given surface mesh of arbitrary
topology. Motivated by the recent surge of cross-modal supervision of the
Contrastive Language-Image Pre-training (CLIP) model, we propose TANGO, which
transfers the appearance style of a given 3D shape according to a text prompt
in a photorealistic manner. Technically, we propose to disentangle the
appearance style as the spatially varying bidirectional reflectance
distribution function, the local geometric variation, and the lighting
condition, which are jointly optimized, via supervision of the CLIP loss, by a
spherical Gaussians based differentiable renderer. As such, TANGO enables
photorealistic 3D style transfer by automatically predicting reflectance
effects even for bare, low-quality meshes, without training on a task-specific
dataset. Extensive experiments show that TANGO outperforms existing methods of
text-driven 3D style transfer in terms of photorealistic quality, consistency
of 3D geometry, and robustness when stylizing low-quality meshes. Our codes and
results are available at our project webpage https://cyw-3d.github.io/tango/.",2210.11277v2,https://arxiv.org/pdf/2210.11277v2
Robust Imitation via Mirror Descent Inverse Reinforcement Learning,"Dong-Sig Han, Hyunseo Kim, Hyundo Lee, Je-Hwan Ryu, Byoung-Tak Zhang","Recently, adversarial imitation learning has shown a scalable reward
acquisition method for inverse reinforcement learning (IRL) problems. However,
estimated reward signals often become uncertain and fail to train a reliable
statistical model since the existing methods tend to solve hard optimization
problems directly. Inspired by a first-order optimization method called mirror
descent, this paper proposes to predict a sequence of reward functions, which
are iterative solutions for a constrained convex problem. IRL solutions derived
by mirror descent are tolerant to the uncertainty incurred by target density
estimation since the amount of reward learning is regulated with respect to
local geometric constraints. We prove that the proposed mirror descent update
rule ensures robust minimization of a Bregman divergence in terms of a rigorous
regret bound of $\mathcal{O}(1/T)$ for step sizes $\{\eta_t\}_{t=1}^{T}$. Our
IRL method was applied on top of an adversarial framework, and it outperformed
existing adversarial methods in an extensive suite of benchmarks.",2210.11201v2,https://arxiv.org/pdf/2210.11201v2
Machine Learning for K-adaptability in Two-stage Robust Optimization,"Esther Julien, Krzysztof Postek, Ş. İlker Birbil","Two-stage robust optimization problems constitute one of the hardest
optimization problem classes. One of the solution approaches to this class of
problems is K-adaptability. This approach simultaneously seeks the best
partitioning of the uncertainty set of scenarios into K subsets, and optimizes
decisions corresponding to each of these subsets. In general case, it is solved
using the K-adaptability branch-and-bound algorithm, which requires exploration
of exponentially-growing solution trees. To accelerate finding high-quality
solutions in such trees, we propose a machine learning-based node selection
strategy. In particular, we construct a feature engineering scheme based on
general two-stage robust optimization insights that allows us to train our
machine learning tool on a database of resolved B&B trees, and to apply it
as-is to problems of different sizes and/or types. We experimentally show that
using our learned node selection strategy outperforms a vanilla, random node
selection strategy when tested on problems of the same type as the training
problems, also in case the K-value or the problem size differs from the
training ones.",2210.11152v2,https://arxiv.org/pdf/2210.11152v2
Robust One-Shot Singing Voice Conversion,"Naoya Takahashi, Mayank Kumar Singh, Yuki Mitsufuji","Recent progress in deep generative models has improved the quality of voice
conversion in the speech domain. However, high-quality singing voice conversion
(SVC) of unseen singers remains challenging due to the wider variety of musical
expressions in pitch, loudness, and pronunciation. Moreover, singing voices are
often recorded with reverb and accompaniment music, which make SVC even more
challenging. In this work, we present a robust one-shot SVC (ROSVC) that
performs any-to-any SVC robustly even on such distorted singing voices. To this
end, we first propose a one-shot SVC model based on generative adversarial
networks that generalizes to unseen singers via partial domain conditioning and
learns to accurately recover the target pitch via pitch distribution matching
and AdaIN-skip conditioning. We then propose a two-stage training method called
Robustify that train the one-shot SVC model in the first stage on clean data to
ensure high-quality conversion, and introduces enhancement modules to the
encoders of the model in the second stage to enhance the feature extraction
from distorted singing voices. To further improve the voice quality and pitch
reconstruction accuracy, we finally propose a hierarchical diffusion model for
singing voice neural vocoders. Experimental results show that the proposed
method outperforms state-of-the-art one-shot SVC baselines for both seen and
unseen singers and significantly improves the robustness against distortions.",2210.11096v2,https://arxiv.org/pdf/2210.11096v2
"Analyzing the Robustness of Decentralized Horizontal and Vertical
  Federated Learning Architectures in a Non-IID Scenario","Pedro Miguel Sánchez Sánchez, Alberto Huertas Celdrán, Enrique Tomás Martínez Beltrán, Daniel Demeter, Gérôme Bovet, Gregorio Martínez Pérez, Burkhard Stiller","Federated learning (FL) allows participants to collaboratively train machine
and deep learning models while protecting data privacy. However, the FL
paradigm still presents drawbacks affecting its trustworthiness since malicious
participants could launch adversarial attacks against the training process.
Related work has studied the robustness of horizontal FL scenarios under
different attacks. However, there is a lack of work evaluating the robustness
of decentralized vertical FL and comparing it with horizontal FL architectures
affected by adversarial attacks. Thus, this work proposes three decentralized
FL architectures, one for horizontal and two for vertical scenarios, namely
HoriChain, VertiChain, and VertiComb. These architectures present different
neural networks and training protocols suitable for horizontal and vertical
scenarios. Then, a decentralized, privacy-preserving, and federated use case
with non-IID data to classify handwritten digits is deployed to evaluate the
performance of the three architectures. Finally, a set of experiments computes
and compares the robustness of the proposed architectures when they are
affected by different data poisoning based on image watermarks and gradient
poisoning adversarial attacks. The experiments show that even though particular
configurations of both attacks can destroy the classification performance of
the architectures, HoriChain is the most robust one.",2210.11061v1,https://arxiv.org/pdf/2210.11061v1
Chaos Theory and Adversarial Robustness,Jonathan S. Kent,"Neural networks, being susceptible to adversarial attacks, should face a
strict level of scrutiny before being deployed in critical or adversarial
applications. This paper uses ideas from Chaos Theory to explain, analyze, and
quantify the degree to which neural networks are susceptible to or robust
against adversarial attacks. To this end, we present a new metric, the
""susceptibility ratio,"" given by $\hat \Psi(h, \theta)$, which captures how
greatly a model's output will be changed by perturbations to a given input.
  Our results show that susceptibility to attack grows significantly with the
depth of the model, which has safety implications for the design of neural
networks for production environments. We provide experimental evidence of the
relationship between $\hat \Psi$ and the post-attack accuracy of classification
models, as well as a discussion of its application to tasks lacking hard
decision boundaries. We also demonstrate how to quickly and easily approximate
the certified robustness radii for extremely large models, which until now has
been computationally infeasible to calculate directly.",2210.13235v2,https://arxiv.org/pdf/2210.13235v2
"Exiting the Simulation: The Road to Robust and Resilient Autonomous
  Vehicles at Scale",Richard Chakra,"In the past two decades, autonomous driving has been catalyzed into reality
by the growing capabilities of machine learning. This paradigm shift possesses
significant potential to transform the future of mobility and reshape our
society as a whole. With the recent advances in perception, planning, and
control capabilities, autonomous driving technologies are being rolled out for
public trials, yet we remain far from being able to rigorously ensure the
resilient operations of these systems across the long-tailed nature of the
driving environment. Given the limitations of real-world testing, autonomous
vehicle simulation stands as the critical component in exploring the edge of
autonomous driving capabilities, developing the robust behaviors required for
successful real-world operation, and enabling the extraction of hidden risks
from these complex systems prior to deployment. This paper presents the current
state-of-the-art simulation frameworks and methodologies used in the
development of autonomous driving systems, with a focus on outlining how
simulation is used to build the resiliency required for real-world operation
and the methods developed to bridge the gap between simulation and reality. A
synthesis of the key challenges surrounding autonomous driving simulation is
presented, specifically highlighting the opportunities to further advance the
ability to continuously learn in simulation and effectively transfer the
learning into the real-world - enabling autonomous vehicles to exit the
guardrails of simulation and deliver robust and resilient operations at scale.",2210.10876v1,https://arxiv.org/pdf/2210.10876v1
Robustness of Demonstration-based Learning Under Limited Data Scenario,"Hongxin Zhang, Yanzhe Zhang, Ruiyi Zhang, Diyi Yang","Demonstration-based learning has shown great potential in stimulating
pretrained language models' ability under limited data scenario. Simply
augmenting the input with some demonstrations can significantly improve
performance on few-shot NER. However, why such demonstrations are beneficial
for the learning process remains unclear since there is no explicit alignment
between the demonstrations and the predictions. In this paper, we design
pathological demonstrations by gradually removing intuitively useful
information from the standard ones to take a deep dive of the robustness of
demonstration-based sequence labeling and show that (1) demonstrations composed
of random tokens still make the model a better few-shot learner; (2) the length
of random demonstrations and the relevance of random tokens are the main
factors affecting the performance; (3) demonstrations increase the confidence
of model predictions on captured superficial patterns. We have publicly
released our code at https://github.com/SALT-NLP/RobustDemo.",2210.10693v1,https://arxiv.org/pdf/2210.10693v1
"Robust Regression with Highly Corrupted Data via Physics Informed Neural
  Networks","Wei Peng, Wen Yao, Weien Zhou, Xiaoya Zhang, Weijie Yao","Physics-informed neural networks (PINNs) have been proposed to solve two main
classes of problems: data-driven solutions and data-driven discovery of partial
differential equations. This task becomes prohibitive when such data is highly
corrupted due to the possible sensor mechanism failing. We propose the Least
Absolute Deviation based PINN (LAD-PINN) to reconstruct the solution and
recover unknown parameters in PDEs - even if spurious data or outliers corrupt
a large percentage of the observations. To further improve the accuracy of
recovering hidden physics, the two-stage Median Absolute Deviation based PINN
(MAD-PINN) is proposed, where LAD-PINN is employed as an outlier detector
followed by MAD screening out the highly corrupted data. Then the vanilla PINN
or its variants can be subsequently applied to exploit the remaining normal
data. Through several examples, including Poisson's equation, wave equation,
and steady or unsteady Navier-Stokes equations, we illustrate the
generalizability, accuracy and efficiency of the proposed algorithms for
recovering governing equations from noisy and highly corrupted measurement
data.",2210.10646v1,https://arxiv.org/pdf/2210.10646v1
A Robust Pedestrian Detection Approach for Autonomous Vehicles,"Bahareh Ghari, Ali Tourani, Asadollah Shahbahrami","Nowadays, utilizing Advanced Driver-Assistance Systems (ADAS) has absorbed a
huge interest as a potential solution for reducing road traffic issues. Despite
recent technological advances in such systems, there are still many inquiries
that need to be overcome. For instance, ADAS requires accurate and real-time
detection of pedestrians in various driving scenarios. To solve the mentioned
problem, this paper aims to fine-tune the YOLOv5s framework for handling
pedestrian detection challenges on the real-world instances of Caltech
pedestrian dataset. We also introduce a developed toolbox for preparing
training and test data and annotations of Caltech pedestrian dataset into the
format recognizable by YOLOv5. Experimental results of utilizing our approach
show that the mean Average Precision (mAP) of our fine-tuned model for
pedestrian detection task is more than 91 percent when performing at the
highest rate of 70 FPS. Moreover, the experiments on the Caltech pedestrian
dataset samples have verified that our proposed approach is an effective and
accurate method for pedestrian detection and can outperform other existing
methodologies.",2210.10489v1,https://arxiv.org/pdf/2210.10489v1
"Learning Transferable Adversarial Robust Representations via Multi-view
  Consistency","Minseon Kim, Hyeonjeong Ha, Dong Bok Lee, Sung Ju Hwang","Despite the success on few-shot learning problems, most meta-learned models
only focus on achieving good performance on clean examples and thus easily
break down when given adversarially perturbed samples. While some recent works
have shown that a combination of adversarial learning and meta-learning could
enhance the robustness of a meta-learner against adversarial attacks, they fail
to achieve generalizable adversarial robustness to unseen domains and tasks,
which is the ultimate goal of meta-learning. To address this challenge, we
propose a novel meta-adversarial multi-view representation learning framework
with dual encoders. Specifically, we introduce the discrepancy across the two
differently augmented samples of the same data instance by first updating the
encoder parameters with them and further imposing a novel label-free
adversarial attack to maximize their discrepancy. Then, we maximize the
consistency across the views to learn transferable robust representations
across domains and tasks. Through experimental validation on multiple
benchmarks, we demonstrate the effectiveness of our framework on few-shot
learning tasks from unseen domains, achieving over 10\% robust accuracy
improvements against previous adversarial meta-learning baselines.",2210.10485v2,https://arxiv.org/pdf/2210.10485v2
"Robust Offline Reinforcement Learning with Gradient Penalty and
  Constraint Relaxation","Chengqian Gao, Ke Xu, Liu Liu, Deheng Ye, Peilin Zhao, Zhiqiang Xu","A promising paradigm for offline reinforcement learning (RL) is to constrain
the learned policy to stay close to the dataset behaviors, known as policy
constraint offline RL. However, existing works heavily rely on the purity of
the data, exhibiting performance degradation or even catastrophic failure when
learning from contaminated datasets containing impure trajectories of diverse
levels. e.g., expert level, medium level, etc., while offline contaminated data
logs exist commonly in the real world. To mitigate this, we first introduce
gradient penalty over the learned value function to tackle the exploding
Q-functions. We then relax the closeness constraints towards non-optimal
actions with critic weighted constraint relaxation. Experimental results show
that the proposed techniques effectively tame the non-optimal trajectories for
policy constraint offline RL methods, evaluated on a set of contaminated D4RL
Mujoco and Adroit datasets.",2210.10469v1,https://arxiv.org/pdf/2210.10469v1
"LightEA: A Scalable, Robust, and Interpretable Entity Alignment
  Framework via Three-view Label Propagation","Xin Mao, Wenting Wang, Yuanbin Wu, Man Lan","Entity Alignment (EA) aims to find equivalent entity pairs between KGs, which
is the core step of bridging and integrating multi-source KGs. In this paper,
we argue that existing GNN-based EA methods inherit the inborn defects from
their neural network lineage: weak scalability and poor interpretability.
Inspired by recent studies, we reinvent the Label Propagation algorithm to
effectively run on KGs and propose a non-neural EA framework -- LightEA,
consisting of three efficient components: (i) Random Orthogonal Label
Generation, (ii) Three-view Label Propagation, and (iii) Sparse Sinkhorn
Iteration. According to the extensive experiments on public datasets, LightEA
has impressive scalability, robustness, and interpretability. With a mere tenth
of time consumption, LightEA achieves comparable results to state-of-the-art
methods across all datasets and even surpasses them on many.",2210.10436v2,https://arxiv.org/pdf/2210.10436v2
On the Adversarial Robustness of Mixture of Experts,"Joan Puigcerver, Rodolphe Jenatton, Carlos Riquelme, Pranjal Awasthi, Srinadh Bhojanapalli","Adversarial robustness is a key desirable property of neural networks. It has
been empirically shown to be affected by their sizes, with larger networks
being typically more robust. Recently, Bubeck and Sellke proved a lower bound
on the Lipschitz constant of functions that fit the training data in terms of
their number of parameters. This raises an interesting open question, do -- and
can -- functions with more parameters, but not necessarily more computational
cost, have better robustness? We study this question for sparse Mixture of
Expert models (MoEs), that make it possible to scale up the model size for a
roughly constant computational cost. We theoretically show that under certain
conditions on the routing and the structure of the data, MoEs can have
significantly smaller Lipschitz constants than their dense counterparts. The
robustness of MoEs can suffer when the highest weighted experts for an input
implement sufficiently different functions. We next empirically evaluate the
robustness of MoEs on ImageNet using adversarial attacks and show they are
indeed more robust than dense models with the same computational cost. We make
key observations showing the robustness of MoEs to the choice of experts,
highlighting the redundancy of experts in models trained in practice.",2210.10253v1,https://arxiv.org/pdf/2210.10253v1
"Output Feedback Tube MPC-Guided Data Augmentation for Robust, Efficient
  Sensorimotor Policy Learning","Andrea Tagliabue, Jonathan P. How","Imitation learning (IL) can generate computationally efficient sensorimotor
policies from demonstrations provided by computationally expensive model-based
sensing and control algorithms. However, commonly employed IL methods are often
data-inefficient, requiring the collection of a large number of demonstrations
and producing policies with limited robustness to uncertainties. In this work,
we combine IL with an output feedback robust tube model predictive controller
(RTMPC) to co-generate demonstrations and a data augmentation strategy to
efficiently learn neural network-based sensorimotor policies. Thanks to the
augmented data, we reduce the computation time and the number of demonstrations
needed by IL, while providing robustness to sensing and process uncertainty. We
tailor our approach to the task of learning a trajectory tracking visuomotor
policy for an aerial robot, leveraging a 3D mesh of the environment as part of
the data augmentation process. We numerically demonstrate that our method can
learn a robust visuomotor policy from a single demonstration--a two-orders of
magnitude improvement in demonstration efficiency compared to existing IL
methods.",2210.10127v1,https://arxiv.org/pdf/2210.10127v1
"Vision Paper: Causal Inference for Interpretable and Robust Machine
  Learning in Mobility Analysis","Yanan Xin, Natasa Tagasovska, Fernando Perez-Cruz, Martin Raubal","Artificial intelligence (AI) is revolutionizing many areas of our lives,
leading a new era of technological advancement. Particularly, the
transportation sector would benefit from the progress in AI and advance the
development of intelligent transportation systems. Building intelligent
transportation systems requires an intricate combination of artificial
intelligence and mobility analysis. The past few years have seen rapid
development in transportation applications using advanced deep neural networks.
However, such deep neural networks are difficult to interpret and lack
robustness, which slows the deployment of these AI-powered algorithms in
practice. To improve their usability, increasing research efforts have been
devoted to developing interpretable and robust machine learning methods, among
which the causal inference approach recently gained traction as it provides
interpretable and actionable information. Moreover, most of these methods are
developed for image or sequential data which do not satisfy specific
requirements of mobility data analysis. This vision paper emphasizes research
challenges in deep learning-based mobility analysis that require
interpretability and robustness, summarizes recent developments in using causal
inference for improving the interpretability and robustness of machine learning
methods, and highlights opportunities in developing causally-enabled machine
learning models tailored for mobility analysis. This research direction will
make AI in the transportation sector more interpretable and reliable, thus
contributing to safer, more efficient, and more sustainable future
transportation systems.",2210.10010v1,https://arxiv.org/pdf/2210.10010v1
"Not All Poisons are Created Equal: Robust Training against Data
  Poisoning","Yu Yang, Tian Yu Liu, Baharan Mirzasoleiman","Data poisoning causes misclassification of test time target examples by
injecting maliciously crafted samples in the training data. Existing defenses
are often effective only against a specific type of targeted attack,
significantly degrade the generalization performance, or are prohibitive for
standard deep learning pipelines.
  In this work, we propose an efficient defense mechanism that significantly
reduces the success rate of various data poisoning attacks, and provides
theoretical guarantees for the performance of the model. Targeted attacks work
by adding bounded perturbations to a randomly selected subset of training data
to match the targets' gradient or representation. We show that: (i) under
bounded perturbations, only a number of poisons can be optimized to have a
gradient that is close enough to that of the target and make the attack
successful; (ii) such effective poisons move away from their original class and
get isolated in the gradient space; (iii) dropping examples in low-density
gradient regions during training can successfully eliminate the effective
poisons, and guarantees similar training dynamics to that of training on full
data. Our extensive experiments show that our method significantly decreases
the success rate of state-of-the-art targeted attacks, including Gradient
Matching and Bullseye Polytope, and easily scales to large datasets.",2210.09671v1,https://arxiv.org/pdf/2210.09671v1
Improving Adversarial Robustness by Contrastive Guided Diffusion Process,"Yidong Ouyang, Liyan Xie, Guang Cheng","Synthetic data generation has become an emerging tool to help improve the
adversarial robustness in classification tasks since robust learning requires a
significantly larger amount of training samples compared with standard
classification tasks. Among various deep generative models, the diffusion model
has been shown to produce high-quality synthetic images and has achieved good
performance in improving the adversarial robustness. However, diffusion-type
methods are typically slow in data generation as compared with other generative
models. Although different acceleration techniques have been proposed recently,
it is also of great importance to study how to improve the sample efficiency of
generated data for the downstream task. In this paper, we first analyze the
optimality condition of synthetic distribution for achieving non-trivial robust
accuracy. We show that enhancing the distinguishability among the generated
data is critical for improving adversarial robustness. Thus, we propose the
Contrastive-Guided Diffusion Process (Contrastive-DP), which adopts the
contrastive loss to guide the diffusion model in data generation. We verify our
theoretical results using simulations and demonstrate the good performance of
Contrastive-DP on image datasets.",2210.09643v2,https://arxiv.org/pdf/2210.09643v2
Split-KalmanNet: A Robust Model-Based Deep Learning Approach for SLAM,"Geon Choi, Jeonghun Park, Nir Shlezinger, Yonina C. Eldar, Namyoon Lee","Simultaneous localization and mapping (SLAM) is a method that constructs a
map of an unknown environment and localizes the position of a moving agent on
the map simultaneously. Extended Kalman filter (EKF) has been widely adopted as
a low complexity solution for online SLAM, which relies on a motion and
measurement model of the moving agent. In practice, however, acquiring precise
information about these models is very challenging, and the model mismatch
effect causes severe performance loss in SLAM. In this paper, inspired by the
recently proposed KalmanNet, we present a robust EKF algorithm using the power
of deep learning for online SLAM, referred to as Split-KalmanNet. The key idea
of Split-KalmanNet is to compute the Kalman gain using the Jacobian matrix of a
measurement function and two recurrent neural networks (RNNs). The two RNNs
independently learn the covariance matrices for a prior state estimate and the
innovation from data. The proposed split structure in the computation of the
Kalman gain allows to compensate for state and measurement model mismatch
effects independently. Numerical simulation results verify that Split-KalmanNet
outperforms the traditional EKF and the state-of-the-art KalmanNet algorithm in
various model mismatch scenarios.",2210.09636v1,https://arxiv.org/pdf/2210.09636v1
Robust Imitation of a Few Demonstrations with a Backwards Model,"Jung Yeon Park, Lawson L. S. Wong","Behavior cloning of expert demonstrations can speed up learning optimal
policies in a more sample-efficient way over reinforcement learning. However,
the policy cannot extrapolate well to unseen states outside of the
demonstration data, creating covariate shift (agent drifting away from
demonstrations) and compounding errors. In this work, we tackle this issue by
extending the region of attraction around the demonstrations so that the agent
can learn how to get back onto the demonstrated trajectories if it veers
off-course. We train a generative backwards dynamics model and generate short
imagined trajectories from states in the demonstrations. By imitating both
demonstrations and these model rollouts, the agent learns the demonstrated
paths and how to get back onto these paths. With optimal or near-optimal
demonstrations, the learned policy will be both optimal and robust to
deviations, with a wider region of attraction. On continuous control domains,
we evaluate the robustness when starting from different initial states unseen
in the demonstration data. While both our method and other imitation learning
baselines can successfully solve the tasks for initial states in the training
distribution, our method exhibits considerably more robustness to different
initial states.",2210.09337v1,https://arxiv.org/pdf/2210.09337v1
"CramNet: Camera-Radar Fusion with Ray-Constrained Cross-Attention for
  Robust 3D Object Detection","Jyh-Jing Hwang, Henrik Kretzschmar, Joshua Manela, Sean Rafferty, Nicholas Armstrong-Crews, Tiffany Chen, Dragomir Anguelov","Robust 3D object detection is critical for safe autonomous driving. Camera
and radar sensors are synergistic as they capture complementary information and
work well under different environmental conditions. Fusing camera and radar
data is challenging, however, as each of the sensors lacks information along a
perpendicular axis, that is, depth is unknown to camera and elevation is
unknown to radar. We propose the camera-radar matching network CramNet, an
efficient approach to fuse the sensor readings from camera and radar in a joint
3D space. To leverage radar range measurements for better camera depth
predictions, we propose a novel ray-constrained cross-attention mechanism that
resolves the ambiguity in the geometric correspondences between camera features
and radar features. Our method supports training with sensor modality dropout,
which leads to robust 3D object detection, even when a camera or radar sensor
suddenly malfunctions on a vehicle. We demonstrate the effectiveness of our
fusion approach through extensive experiments on the RADIATE dataset, one of
the few large-scale datasets that provide radar radio frequency imagery. A
camera-only variant of our method achieves competitive performance in monocular
3D object detection on the Waymo Open Dataset.",2210.09267v2,https://arxiv.org/pdf/2210.09267v2
"Statistical, Robustness, and Computational Guarantees for Sliced
  Wasserstein Distances","Sloan Nietert, Ritwik Sadhu, Ziv Goldfeld, Kengo Kato","Sliced Wasserstein distances preserve properties of classic Wasserstein
distances while being more scalable for computation and estimation in high
dimensions. The goal of this work is to quantify this scalability from three
key aspects: (i) empirical convergence rates; (ii) robustness to data
contamination; and (iii) efficient computational methods. For empirical
convergence, we derive fast rates with explicit dependence of constants on
dimension, subject to log-concavity of the population distributions. For
robustness, we characterize minimax optimal, dimension-free robust estimation
risks, and show an equivalence between robust sliced 1-Wasserstein estimation
and robust mean estimation. This enables lifting statistical and algorithmic
guarantees available for the latter to the sliced 1-Wasserstein setting. Moving
on to computational aspects, we analyze the Monte Carlo estimator for the
average-sliced distance, demonstrating that larger dimension can result in
faster convergence of the numerical integration error. For the max-sliced
distance, we focus on a subgradient-based local optimization algorithm that is
frequently used in practice, albeit without formal guarantees, and establish an
$O(\epsilon^{-4})$ computational complexity bound for it. Our theory is
validated by numerical experiments, which altogether provide a comprehensive
quantitative account of the scalability question.",2210.09160v1,https://arxiv.org/pdf/2210.09160v1
DE-CROP: Data-efficient Certified Robustness for Pretrained Classifiers,"Gaurav Kumar Nayak, Ruchit Rawal, Anirban Chakraborty","Certified defense using randomized smoothing is a popular technique to
provide robustness guarantees for deep neural networks against l2 adversarial
attacks. Existing works use this technique to provably secure a pretrained
non-robust model by training a custom denoiser network on entire training data.
However, access to the training set may be restricted to a handful of data
samples due to constraints such as high transmission cost and the proprietary
nature of the data. Thus, we formulate a novel problem of ""how to certify the
robustness of pretrained models using only a few training samples"". We observe
that training the custom denoiser directly using the existing techniques on
limited samples yields poor certification. To overcome this, our proposed
approach (DE-CROP) generates class-boundary and interpolated samples
corresponding to each training sample, ensuring high diversity in the feature
space of the pretrained classifier. We train the denoiser by maximizing the
similarity between the denoised output of the generated sample and the original
training sample in the classifier's logit space. We also perform distribution
level matching using domain discriminator and maximum mean discrepancy that
yields further benefit. In white box setup, we obtain significant improvements
over the baseline on multiple benchmark datasets and also report similar
performance under the challenging black box setup.",2210.08929v1,https://arxiv.org/pdf/2210.08929v1
"A.I. Robustness: a Human-Centered Perspective on Technological
  Challenges and Opportunities","Andrea Tocchetti, Lorenzo Corti, Agathe Balayn, Mireia Yurrita, Philip Lippmann, Marco Brambilla, Jie Yang","Despite the impressive performance of Artificial Intelligence (AI) systems,
their robustness remains elusive and constitutes a key issue that impedes
large-scale adoption. Robustness has been studied in many domains of AI, yet
with different interpretations across domains and contexts. In this work, we
systematically survey the recent progress to provide a reconciled terminology
of concepts around AI robustness. We introduce three taxonomies to organize and
describe the literature both from a fundamental and applied point of view: 1)
robustness by methods and approaches in different phases of the machine
learning pipeline; 2) robustness for specific model architectures, tasks, and
systems; and in addition, 3) robustness assessment methodologies and insights,
particularly the trade-offs with other trustworthiness properties. Finally, we
identify and discuss research gaps and opportunities and give an outlook on the
field. We highlight the central role of humans in evaluating and enhancing AI
robustness, considering the necessary knowledge humans can provide, and discuss
the need for better understanding practices and developing supportive tools in
the future.",2210.08906v2,https://arxiv.org/pdf/2210.08906v2
"Beyond Model Interpretability: On the Faithfulness and Adversarial
  Robustness of Contrastive Textual Explanations","Julia El Zini, Mariette Awad","Contrastive explanation methods go beyond transparency and address the
contrastive aspect of explanations. Such explanations are emerging as an
attractive option to provide actionable change to scenarios adversely impacted
by classifiers' decisions. However, their extension to textual data is
under-explored and there is little investigation on their vulnerabilities and
limitations.
  This work motivates textual counterfactuals by laying the ground for a novel
evaluation scheme inspired by the faithfulness of explanations. Accordingly, we
extend the computation of three metrics, proximity,connectedness and stability,
to textual data and we benchmark two successful contrastive methods, POLYJUICE
and MiCE, on our suggested metrics. Experiments on sentiment analysis data show
that the connectedness of counterfactuals to their original counterparts is not
obvious in both models. More interestingly, the generated contrastive texts are
more attainable with POLYJUICE which highlights the significance of latent
representations in counterfactual search. Finally, we perform the first
semantic adversarial attack on textual recourse methods. The results
demonstrate the robustness of POLYJUICE and the role that latent input
representations play in robustness and reliability.",2210.08902v1,https://arxiv.org/pdf/2210.08902v1
"Robust Planning for Human-Robot Joint Tasks with Explicit Reasoning on
  Human Mental State","Anthony Favier, Shashank Shekhar, Rachid Alami","We consider the human-aware task planning problem where a human-robot team is
given a shared task with a known objective to achieve. Recent approaches tackle
it by modeling it as a team of independent, rational agents, where the robot
plans for both agents' (shared) tasks. However, the robot knows that humans
cannot be administered like artificial agents, so it emulates and predicts the
human's decisions, actions, and reactions. Based on earlier approaches, we
describe a novel approach to solve such problems, which models and uses
execution-time observability conventions. Abstractly, this modeling is based on
situation assessment, which helps our approach capture the evolution of
individual agents' beliefs and anticipate belief divergences that arise in
practice. It decides if and when belief alignment is needed and achieves it
with communication. These changes improve the solver's performance: (a)
communication is effectively used, and (b) robust for more realistic and
challenging problems.",2210.08879v1,https://arxiv.org/pdf/2210.08879v1
ODG-Q: Robust Quantization via Online Domain Generalization,"Chaofan Tao, Ngai Wong","Quantizing neural networks to low-bitwidth is important for model deployment
on resource-limited edge hardware. Although a quantized network has a smaller
model size and memory footprint, it is fragile to adversarial attacks. However,
few methods study the robustness and training efficiency of quantized networks.
To this end, we propose a new method by recasting robust quantization as an
online domain generalization problem, termed ODG-Q, which generates diverse
adversarial data at a low cost during training. ODG-Q consistently outperforms
existing works against various adversarial attacks. For example, on CIFAR-10
dataset, ODG-Q achieves 49.2% average improvements under five common white-box
attacks and 21.7% average improvements under five common black-box attacks,
with a training cost similar to that of natural training (viz. without
adversaries). To our best knowledge, this work is the first work that trains
both quantized and binary neural networks on ImageNet that consistently improve
robustness under different attacks. We also provide a theoretical insight of
ODG-Q that accounts for the bound of model risk on attacked data.",2210.08701v1,https://arxiv.org/pdf/2210.08701v1
"Robust, General, and Low Complexity Acoustic Scene Classification
  Systems and An Effective Visualization for Presenting a Sound Scene Context","Lam Pham, Dusan Salovic, Anahid Jalali, Alexander Schindler, Khoa Tran, Canh Vu, Phu X. Nguyen","In this paper, we present a comprehensive analysis of Acoustic Scene
Classification (ASC), the task of identifying the scene of an audio recording
from its acoustic signature. In particular, we firstly propose an
inception-based and low footprint ASC model, referred to as the ASC baseline.
The proposed ASC baseline is then compared with benchmark and high-complexity
network architectures of MobileNetV1, MobileNetV2, VGG16, VGG19, ResNet50V2,
ResNet152V2, DenseNet121, DenseNet201, and Xception. Next, we improve the ASC
baseline by proposing a novel deep neural network architecture which leverages
residual-inception architectures and multiple kernels. Given the novel
residual-inception (NRI) model, we further evaluate the trade off between the
model complexity and the model accuracy performance. Finally, we evaluate
whether sound events occurring in a sound scene recording can help to improve
ASC accuracy, then indicate how a sound scene context is well presented by
combining both sound scene and sound event information. We conduct extensive
experiments on various ASC datasets, including Crowded Scenes, IEEE AASP
Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE)
2018 Task 1A and 1B, 2019 Task 1A and 1B, 2020 Task 1A, 2021 Task 1A, 2022 Task
1. The experimental results on several different ASC challenges highlight two
main achievements; the first is to propose robust, general, and low complexity
ASC systems which are suitable for real-life applications on a wide range of
edge devices and mobiles; the second is to propose an effective visualization
method for comprehensively presenting a sound scene context.",2210.08610v1,https://arxiv.org/pdf/2210.08610v1
"Investigating the Robustness of Natural Language Generation from Logical
  Forms via Counterfactual Samples","Chengyuan Liu, Leilei Gan, Kun Kuang, Fei Wu","The aim of Logic2Text is to generate controllable and faithful texts
conditioned on tables and logical forms, which not only requires a deep
understanding of the tables and logical forms, but also warrants symbolic
reasoning over the tables. State-of-the-art methods based on pre-trained models
have achieved remarkable performance on the standard test dataset. However, we
question whether these methods really learn how to perform logical reasoning,
rather than just relying on the spurious correlations between the headers of
the tables and operators of the logical form. To verify this hypothesis, we
manually construct a set of counterfactual samples, which modify the original
logical forms to generate counterfactual logical forms with rarely co-occurred
table headers and logical operators. SOTA methods give much worse results on
these counterfactual samples compared with the results on the original test
dataset, which verifies our hypothesis. To deal with this problem, we firstly
analyze this bias from a causal perspective, based on which we propose two
approaches to reduce the model's reliance on the shortcut. The first one
incorporates the hierarchical structure of the logical forms into the model.
The second one exploits automatically generated counterfactual data for
training. Automatic and manual experimental results on the original test
dataset and the counterfactual dataset show that our method is effective to
alleviate the spurious correlation. Our work points out the weakness of
previous methods and takes a further step toward developing Logic2Text models
with real logical reasoning ability.",2210.08548v1,https://arxiv.org/pdf/2210.08548v1
"RoS-KD: A Robust Stochastic Knowledge Distillation Approach for Noisy
  Medical Imaging","Ajay Jaiswal, Kumar Ashutosh, Justin F Rousseau, Yifan Peng, Zhangyang Wang, Ying Ding","AI-powered Medical Imaging has recently achieved enormous attention due to
its ability to provide fast-paced healthcare diagnoses. However, it usually
suffers from a lack of high-quality datasets due to high annotation cost,
inter-observer variability, human annotator error, and errors in
computer-generated labels. Deep learning models trained on noisy labelled
datasets are sensitive to the noise type and lead to less generalization on the
unseen samples. To address this challenge, we propose a Robust Stochastic
Knowledge Distillation (RoS-KD) framework which mimics the notion of learning a
topic from multiple sources to ensure deterrence in learning noisy information.
More specifically, RoS-KD learns a smooth, well-informed, and robust student
manifold by distilling knowledge from multiple teachers trained on overlapping
subsets of training data. Our extensive experiments on popular medical imaging
classification tasks (cardiopulmonary disease and lesion classification) using
real-world datasets, show the performance benefit of RoS-KD, its ability to
distill knowledge from many popular large networks (ResNet-50, DenseNet-121,
MobileNet-V2) in a comparatively small network, and its robustness to
adversarial attacks (PGD, FSGM). More specifically, RoS-KD achieves >2% and >4%
improvement on F1-score for lesion classification and cardiopulmonary disease
classification tasks, respectively, when the underlying student is ResNet-18
against recent competitive knowledge distillation baseline. Additionally, on
cardiopulmonary disease classification task, RoS-KD outperforms most of the
SOTA baselines by ~1% gain in AUC score.",2210.08388v2,https://arxiv.org/pdf/2210.08388v2
Distributionally Robust Causal Inference with Observational Data,"Dimitris Bertsimas, Kosuke Imai, Michael Lingzhi Li","We consider the estimation of average treatment effects in observational
studies and propose a new framework of robust causal inference with unobserved
confounders. Our approach is based on distributionally robust optimization and
proceeds in two steps. We first specify the maximal degree to which the
distribution of unobserved potential outcomes may deviate from that of observed
outcomes. We then derive sharp bounds on the average treatment effects under
this assumption. Our framework encompasses the popular marginal sensitivity
model as a special case, and we demonstrate how the proposed methodology can
address a primary challenge of the marginal sensitivity model that it produces
uninformative results when unobserved confounders substantially affect
treatment and outcome. Specifically, we develop an alternative sensitivity
model, called the distributional sensitivity model, under the assumption that
heterogeneity of treatment effect due to unobserved variables is relatively
small. Unlike the marginal sensitivity model, the distributional sensitivity
model allows for potential lack of overlap and often produces informative
bounds even when unobserved variables substantially affect both treatment and
outcome. Finally, we show how to extend the distributional sensitivity model to
difference-in-differences designs and settings with instrumental variables.
Through simulation and empirical studies, we demonstrate the applicability of
the proposed methodology.",2210.08326v3,https://arxiv.org/pdf/2210.08326v3
Linear Scalarization for Byzantine-robust learning on non-IID data,"Latifa Errami, El Houcine Bergou","In this work we study the problem of Byzantine-robust learning when data
among clients is heterogeneous. We focus on poisoning attacks targeting the
convergence of SGD. Although this problem has received great attention; the
main Byzantine defenses rely on the IID assumption causing them to fail when
data distribution is non-IID even with no attack. We propose the use of Linear
Scalarization (LS) as an enhancing method to enable current defenses to
circumvent Byzantine attacks in the non-IID setting. The LS method is based on
the incorporation of a trade-off vector that penalizes the suspected malicious
clients. Empirical analysis corroborates that the proposed LS variants are
viable in the IID setting. For mild to strong non-IID data splits, LS is either
comparable or outperforming current approaches under state-of-the-art Byzantine
attack scenarios.",2210.08287v1,https://arxiv.org/pdf/2210.08287v1
"Distributionally Robust Multiclass Classification and Applications in
  Deep Image Classifiers","Ruidi Chen, Boran Hao, Ioannis Ch. Paschalidis","We develop a Distributionally Robust Optimization (DRO) formulation for
Multiclass Logistic Regression (MLR), which could tolerate data contaminated by
outliers. The DRO framework uses a probabilistic ambiguity set defined as a
ball of distributions that are close to the empirical distribution of the
training set in the sense of the Wasserstein metric. We relax the DRO
formulation into a regularized learning problem whose regularizer is a norm of
the coefficient matrix. We establish out-of-sample performance guarantees for
the solutions to our model, offering insights on the role of the regularizer in
controlling the prediction error. We apply the proposed method in rendering
deep Vision Transformer (ViT)-based image classifiers robust to random and
adversarial attacks. Specifically, using the MNIST and CIFAR-10 datasets, we
demonstrate reductions in test error rate by up to 83.5% and loss by up to
91.3% compared with baseline methods, by adopting a novel random training
method.",2210.08198v2,https://arxiv.org/pdf/2210.08198v2
"Distributed Distributionally Robust Optimization with Non-Convex
  Objectives","Yang Jiao, Kai Yang, Dongjin Song","Distributionally Robust Optimization (DRO), which aims to find an optimal
decision that minimizes the worst case cost over the ambiguity set of
probability distribution, has been widely applied in diverse applications,
e.g., network behavior analysis, risk management, etc. However, existing DRO
techniques face three key challenges: 1) how to deal with the asynchronous
updating in a distributed environment; 2) how to leverage the prior
distribution effectively; 3) how to properly adjust the degree of robustness
according to different scenarios. To this end, we propose an asynchronous
distributed algorithm, named Asynchronous Single-looP alternatIve gRadient
projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to
tackle the distributed distributionally robust optimization (DDRO) problem.
Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set,
is developed to effectively leverage the prior distribution and flexibly
control the degree of robustness. Finally, our theoretical analysis elucidates
that the proposed algorithm is guaranteed to converge and the iteration
complexity is also analyzed. Extensive empirical studies on real-world datasets
demonstrate that the proposed method can not only achieve fast convergence, and
remain robust against data heterogeneity as well as malicious attacks, but also
tradeoff robustness with performance.",2210.07588v2,https://arxiv.org/pdf/2210.07588v2
"Robust Candidate Generation for Entity Linking on Short Social Media
  Texts","Liam Hebert, Raheleh Makki, Shubhanshu Mishra, Hamidreza Saghir, Anusha Kamath, Yuval Merhav","Entity Linking (EL) is the gateway into Knowledge Bases. Recent advances in
EL utilize dense retrieval approaches for Candidate Generation, which addresses
some of the shortcomings of the Lookup based approach of matching NER mentions
against pre-computed dictionaries. In this work, we show that in the domain of
Tweets, such methods suffer as users often include informal spelling, limited
context, and lack of specificity, among other issues. We investigate these
challenges on a large and recent Tweets benchmark for EL, empirically evaluate
lookup and dense retrieval approaches, and demonstrate a hybrid solution using
long contextual representation from Wikipedia is necessary to achieve
considerable gains over previous work, achieving 0.93 recall.",2210.07472v1,https://arxiv.org/pdf/2210.07472v1
Learning to Efficiently Plan Robust Frictional Multi-Object Grasps,"Wisdom C. Agboh, Satvik Sharma, Kishore Srinivas, Mallika Parulekar, Gaurav Datta, Tianshuang Qiu, Jeffrey Ichnowski, Eugen Solowjow, Mehmet Dogar, Ken Goldberg","We consider a decluttering problem where multiple rigid convex polygonal
objects rest in randomly placed positions and orientations on a planar surface
and must be efficiently transported to a packing box using both single and
multi-object grasps. Prior work considered frictionless multi-object grasping.
In this paper, we introduce friction to increase the number of potential grasps
for a given group of objects, and thus increase picks per hour. We train a
neural network using real examples to plan robust multi-object grasps. In
physical experiments, we find a 13.7% increase in success rate, a 1.6x increase
in picks per hour, and a 6.3x decrease in grasp planning time compared to prior
work on multi-object grasping. Compared to single-object grasping, we find a
3.1x increase in picks per hour.",2210.07420v3,https://arxiv.org/pdf/2210.07420v3
ScionFL: Efficient and Robust Secure Quantized Aggregation,"Yaniv Ben-Itzhak, Helen Möllering, Benny Pinkas, Thomas Schneider, Ajith Suresh, Oleksandr Tkachenko, Shay Vargaftik, Christian Weinert, Hossein Yalame, Avishay Yanai","Secure aggregation is commonly used in federated learning (FL) to alleviate
privacy concerns related to the central aggregator seeing all parameter updates
in the clear. Unfortunately, most existing secure aggregation schemes ignore
two critical orthogonal research directions that aim to (i) significantly
reduce client-server communication and (ii) mitigate the impact of malicious
clients. However, both of these additional properties are essential to
facilitate cross-device FL with thousands or even millions of (mobile)
participants.
  In this paper, we unite both research directions by introducing ScionFL, the
first secure aggregation framework for FL that operates efficiently on
quantized inputs and simultaneously provides robustness against malicious
clients. Our framework leverages (novel) multi-party computation (MPC)
techniques and supports multiple linear (1-bit) quantization schemes, including
ones that utilize the randomized Hadamard transform and Kashin's
representation.
  Our theoretical results are supported by extensive evaluations. We show that
with no overhead for clients and moderate overhead for the server compared to
transferring and processing quantized updates in plaintext, we obtain
comparable accuracy for standard FL benchmarks. Moreover, we demonstrate the
robustness of our framework against state-of-the-art poisoning attacks.",2210.07376v3,https://arxiv.org/pdf/2210.07376v3
Shot-frugal and Robust quantum kernel classifiers,"Abhay Shastry, Abhijith Jayakumar, Apoorva Patel, Chiranjib Bhattacharyya","Quantum kernel methods are a candidate for quantum speed-ups in supervised
machine learning. The number of quantum measurements N required for a
reasonable kernel estimate is a critical resource, both from complexity
considerations and because of the constraints of near-term quantum hardware. We
emphasize that for classification tasks, the aim is reliable classification and
not precise kernel evaluation, and demonstrate that the former is far more
resource efficient. Furthermore, it is shown that the accuracy of
classification is not a suitable performance metric in the presence of noise
and we motivate a new metric that characterizes the reliability of
classification. We then obtain a bound for N which ensures, with high
probability, that classification errors over a dataset are bounded by the
margin errors of an idealized quantum kernel classifier. Using chance
constraint programming and the subgaussian bounds of quantum kernel
distributions, we derive several Shot-frugal and Robust (ShofaR) programs
starting from the primal formulation of the Support Vector Machine. This
significantly reduces the number of quantum measurements needed and is robust
to noise by construction. Our strategy is applicable to uncertainty in quantum
kernels arising from any source of unbiased noise.",2210.06971v3,https://arxiv.org/pdf/2210.06971v3
Outlier-Robust Group Inference via Gradient Space Clustering,"Yuchen Zeng, Kristjan Greenewald, Kangwook Lee, Justin Solomon, Mikhail Yurochkin","Traditional machine learning models focus on achieving good performance on
the overall training distribution, but they often underperform on minority
groups. Existing methods can improve the worst-group performance, but they can
have several limitations: (i) they require group annotations, which are often
expensive and sometimes infeasible to obtain, and/or (ii) they are sensitive to
outliers. Most related works fail to solve these two issues simultaneously as
they focus on conflicting perspectives of minority groups and outliers. We
address the problem of learning group annotations in the presence of outliers
by clustering the data in the space of gradients of the model parameters. We
show that data in the gradient space has a simpler structure while preserving
information about minority groups and outliers, making it suitable for standard
clustering methods like DBSCAN. Extensive experiments demonstrate that our
method significantly outperforms state-of-the-art both in terms of group
identification and downstream worst-group performance.",2210.06759v1,https://arxiv.org/pdf/2210.06759v1
COLLIDER: A Robust Training Framework for Backdoor Data,"Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie","Deep neural network (DNN) classifiers are vulnerable to backdoor attacks. An
adversary poisons some of the training data in such attacks by installing a
trigger. The goal is to make the trained DNN output the attacker's desired
class whenever the trigger is activated while performing as usual for clean
data. Various approaches have recently been proposed to detect malicious
backdoored DNNs. However, a robust, end-to-end training approach, like
adversarial training, is yet to be discovered for backdoor poisoned data. In
this paper, we take the first step toward such methods by developing a robust
training framework, COLLIDER, that selects the most prominent samples by
exploiting the underlying geometric structures of the data. Specifically, we
effectively filter out candidate poisoned data at each training epoch by
solving a geometrical coreset selection objective. We first argue how clean
data samples exhibit (1) gradients similar to the clean majority of data and
(2) low local intrinsic dimensionality (LID). Based on these criteria, we
define a novel coreset selection objective to find such samples, which are used
for training a DNN. We show the effectiveness of the proposed method for robust
training of DNNs on various poisoned datasets, reducing the backdoor success
rate significantly.",2210.06704v1,https://arxiv.org/pdf/2210.06704v1
Fairness via Adversarial Attribute Neighbourhood Robust Learning,"Qi Qi, Shervin Ardeshir, Yi Xu, Tianbao Yang","Improving fairness between privileged and less-privileged sensitive attribute
groups (e.g, {race, gender}) has attracted lots of attention. To enhance the
model performs uniformly well in different sensitive attributes, we propose a
principled \underline{R}obust \underline{A}dversarial \underline{A}ttribute
\underline{N}eighbourhood (RAAN) loss to debias the classification head and
promote a fairer representation distribution across different sensitive
attribute groups. The key idea of RAAN is to mitigate the differences of biased
representations between different sensitive attribute groups by assigning each
sample an adversarial robust weight, which is defined on the representations of
adversarial attribute neighbors, i.e, the samples from different protected
groups. To provide efficient optimization algorithms, we cast the RAAN into a
sum of coupled compositional functions and propose a stochastic adaptive
(Adam-style) and non-adaptive (SGD-style) algorithm framework SCRAAN with
provable theoretical guarantee. Extensive empirical studies on fairness-related
benchmark datasets verify the effectiveness of the proposed method.",2210.06630v1,https://arxiv.org/pdf/2210.06630v1
"FASTER-CE: Fast, Sparse, Transparent, and Robust Counterfactual
  Explanations","Shubham Sharma, Alan H. Gee, Jette Henderson, Joydeep Ghosh","Counterfactual explanations have substantially increased in popularity in the
past few years as a useful human-centric way of understanding individual
black-box model predictions. While several properties desired of high-quality
counterfactuals have been identified in the literature, three crucial concerns:
the speed of explanation generation, robustness/sensitivity and succinctness of
explanations (sparsity) have been relatively unexplored. In this paper, we
present FASTER-CE: a novel set of algorithms to generate fast, sparse, and
robust counterfactual explanations. The key idea is to efficiently find
promising search directions for counterfactuals in a latent space that is
specified via an autoencoder. These directions are determined based on
gradients with respect to each of the original input features as well as of the
target, as estimated in the latent space. The ability to quickly examine
combinations of the most promising gradient directions as well as to
incorporate additional user-defined constraints allows us to generate multiple
counterfactual explanations that are sparse, realistic, and robust to input
manipulations. Through experiments on three datasets of varied complexities, we
show that FASTER-CE is not only much faster than other state of the art methods
for generating multiple explanations but also is significantly superior when
considering a larger set of desirable (and often conflicting) properties.
Specifically we present results across multiple performance metrics: sparsity,
proximity, validity, speed of generation, and the robustness of explanations,
to highlight the capabilities of the FASTER-CE family.",2210.06578v1,https://arxiv.org/pdf/2210.06578v1
Robust Neural Posterior Estimation and Statistical Model Criticism,"Daniel Ward, Patrick Cannon, Mark Beaumont, Matteo Fasiolo, Sebastian M Schmon","Computer simulations have proven a valuable tool for understanding complex
phenomena across the sciences. However, the utility of simulators for modelling
and forecasting purposes is often restricted by low data quality, as well as
practical limits to model fidelity. In order to circumvent these difficulties,
we argue that modellers must treat simulators as idealistic representations of
the true data generating process, and consequently should thoughtfully consider
the risk of model misspecification. In this work we revisit neural posterior
estimation (NPE), a class of algorithms that enable black-box parameter
inference in simulation models, and consider the implication of a
simulation-to-reality gap. While recent works have demonstrated reliable
performance of these methods, the analyses have been performed using synthetic
data generated by the simulator model itself, and have therefore only addressed
the well-specified case. In this paper, we find that the presence of
misspecification, in contrast, leads to unreliable inference when NPE is used
naively. As a remedy we argue that principled scientific inquiry with
simulators should incorporate a model criticism component, to facilitate
interpretable identification of misspecification and a robust inference
component, to fit 'wrong but useful' models. We propose robust neural posterior
estimation (RNPE), an extension of NPE to simultaneously achieve both these
aims, through explicitly modelling the discrepancies between simulations and
the observed data. We assess the approach on a range of artificially
misspecified examples, and find RNPE performs well across the tasks, whereas
naively using NPE leads to misleading and erratic posteriors.",2210.06564v1,https://arxiv.org/pdf/2210.06564v1
Are Sample-Efficient NLP Models More Robust?,"Nelson F. Liu, Ananya Kumar, Percy Liang, Robin Jia","Recent results in image classification and extractive question answering have
observed that pre-trained models trained on less in-distribution data have
better out-of-distribution performance. However, it is unclear how broadly
these trends hold. We conduct a large empirical study across three tasks, three
broadly-applicable modeling interventions (increasing model size, using a
different adaptation method, and pre-training on more data), and 14 diverse
datasets to investigate the relationship between sample efficiency (amount of
data needed to reach a given ID accuracy) and robustness (how models fare on
OOD evaluation). We find that higher sample efficiency is only correlated with
better average OOD robustness on some modeling interventions and tasks, but not
others. On individual datasets, models with lower sample efficiency can even be
more robust. These results suggest that general-purpose methods for improving
sample efficiency are unlikely to yield universal OOD robustness improvements,
since such improvements are highly dataset- and task-dependent. Even in an era
of large, multi-purpose pretrained models, task-specific decisions may often be
necessary for OOD generalization.",2210.06456v2,https://arxiv.org/pdf/2210.06456v2
Visual Prompting for Adversarial Robustness,"Aochuan Chen, Peter Lorenz, Yuguang Yao, Pin-Yu Chen, Sijia Liu","In this work, we leverage visual prompting (VP) to improve adversarial
robustness of a fixed, pre-trained model at testing time. Compared to
conventional adversarial defenses, VP allows us to design universal (i.e.,
data-agnostic) input prompting templates, which have plug-and-play capabilities
at testing time to achieve desired model performance without introducing much
computation overhead. Although VP has been successfully applied to improving
model generalization, it remains elusive whether and how it can be used to
defend against adversarial attacks. We investigate this problem and show that
the vanilla VP approach is not effective in adversarial defense since a
universal input prompt lacks the capacity for robust learning against
sample-specific adversarial perturbations. To circumvent it, we propose a new
VP method, termed Class-wise Adversarial Visual Prompting (C-AVP), to generate
class-wise visual prompts so as to not only leverage the strengths of ensemble
prompts but also optimize their interrelations to improve model robustness. Our
experiments show that C-AVP outperforms the conventional VP method, with 2.1X
standard accuracy gain and 2X robust accuracy gain. Compared to classical
test-time defenses, C-AVP also yields a 42X inference time speedup.",2210.06284v4,https://arxiv.org/pdf/2210.06284v4
When are Local Queries Useful for Robust Learning?,"Pascale Gourdeau, Varun Kanade, Marta Kwiatkowska, James Worrell","Distributional assumptions have been shown to be necessary for the robust
learnability of concept classes when considering the exact-in-the-ball robust
risk and access to random examples by Gourdeau et al. (2019). In this paper, we
study learning models where the learner is given more power through the use of
local queries, and give the first distribution-free algorithms that perform
robust empirical risk minimization (ERM) for this notion of robustness. The
first learning model we consider uses local membership queries (LMQ), where the
learner can query the label of points near the training sample. We show that,
under the uniform distribution, LMQs do not increase the robustness threshold
of conjunctions and any superclass, e.g., decision lists and halfspaces. Faced
with this negative result, we introduce the local equivalence query
($\mathsf{LEQ}$) oracle, which returns whether the hypothesis and target
concept agree in the perturbation region around a point in the training sample,
as well as a counterexample if it exists. We show a separation result: on the
one hand, if the query radius $\lambda$ is strictly smaller than the
adversary's perturbation budget $\rho$, then distribution-free robust learning
is impossible for a wide variety of concept classes; on the other hand, the
setting $\lambda=\rho$ allows us to develop robust ERM algorithms. We then
bound the query complexity of these algorithms based on online learning
guarantees and further improve these bounds for the special case of
conjunctions. We finish by giving robust learning algorithms for halfspaces on
$\{0,1\}^n$ and then obtaining robustness guarantees for halfspaces in
$\mathbb{R}^n$ against precision-bounded adversaries.",2210.06089v2,https://arxiv.org/pdf/2210.06089v2
"Double Bubble, Toil and Trouble: Enhancing Certified Robustness through
  Transitivity","Andrew C. Cullen, Paul Montague, Shijie Liu, Sarah M. Erfani, Benjamin I. P. Rubinstein","In response to subtle adversarial examples flipping classifications of neural
network models, recent research has promoted certified robustness as a
solution. There, invariance of predictions to all norm-bounded attacks is
achieved through randomised smoothing of network inputs. Today's
state-of-the-art certifications make optimal use of the class output scores at
the input instance under test: no better radius of certification (under the
$L_2$ norm) is possible given only these score. However, it is an open question
as to whether such lower bounds can be improved using local information around
the instance under test. In this work, we demonstrate how today's ""optimal""
certificates can be improved by exploiting both the transitivity of
certifications, and the geometry of the input space, giving rise to what we
term Geometrically-Informed Certified Robustness. By considering the smallest
distance to points on the boundary of a set of certifications this approach
improves certifications for more than $80\%$ of Tiny-Imagenet instances,
yielding an on average $5 \%$ increase in the associated certification. When
incorporating training time processes that enhance the certified radius, our
technique shows even more promising results, with a uniform $4$ percentage
point increase in the achieved certified radius.",2210.06077v1,https://arxiv.org/pdf/2210.06077v1
"Efficient Adversarial Training without Attacking: Worst-Case-Aware
  Robust Reinforcement Learning","Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Furong Huang","Recent studies reveal that a well-trained deep reinforcement learning (RL)
policy can be particularly vulnerable to adversarial perturbations on input
observations. Therefore, it is crucial to train RL agents that are robust
against any attacks with a bounded budget. Existing robust training methods in
deep RL either treat correlated steps separately, ignoring the robustness of
long-term rewards, or train the agents and RL-based attacker together, doubling
the computational burden and sample complexity of the training process. In this
work, we propose a strong and efficient robust training framework for RL, named
Worst-case-aware Robust RL (WocaR-RL) that directly estimates and optimizes the
worst-case reward of a policy under bounded l_p attacks without requiring extra
samples for learning an attacker. Experiments on multiple environments show
that WocaR-RL achieves state-of-the-art performance under various strong
attacks, and obtains significantly higher training efficiency than prior
state-of-the-art robust training methods. The code of this work is available at
https://github.com/umd-huang-lab/WocaR-RL.",2210.05927v1,https://arxiv.org/pdf/2210.05927v1
"Common Corruption Robustness of Point Cloud Detectors: Benchmark and
  Enhancement","Shuangzhi Li, Zhijie Wang, Felix Juefei-Xu, Qing Guo, Xingyu Li, Lei Ma","Object detection through LiDAR-based point cloud has recently been important
in autonomous driving. Although achieving high accuracy on public benchmarks,
the state-of-the-art detectors may still go wrong and cause a heavy loss due to
the widespread corruptions in the real world like rain, snow, sensor noise,
etc. Nevertheless, there is a lack of a large-scale dataset covering diverse
scenes and realistic corruption types with different severities to develop
practical and robust point cloud detectors, which is challenging due to the
heavy collection costs. To alleviate the challenge and start the first step for
robust point cloud detection, we propose the physical-aware simulation methods
to generate degraded point clouds under different real-world common
corruptions. Then, for the first attempt, we construct a benchmark based on the
physical-aware common corruptions for point cloud detectors, which contains a
total of 1,122,150 examples covering 7,481 scenes, 25 common corruption types,
and 6 severities. With such a novel benchmark, we conduct extensive empirical
studies on 8 state-of-the-art detectors that contain 6 different detection
frameworks. Thus we get several insight observations revealing the
vulnerabilities of the detectors and indicating the enhancement directions.
Moreover, we further study the effectiveness of existing robustness enhancement
methods based on data augmentation and data denoising. The benchmark can
potentially be a new platform for evaluating point cloud detectors, opening a
door for developing novel robustness enhancement methods.",2210.05896v1,https://arxiv.org/pdf/2210.05896v1
Designing Robust Transformers using Robust Kernel Density Estimation,"Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, Nhat Ho","Recent advances in Transformer architectures have empowered their empirical
success in a variety of tasks across different domains. However, existing works
mainly focus on predictive accuracy and computational cost, without considering
other practical issues, such as robustness to contaminated samples. Recent work
by Nguyen et al., (2022) has shown that the self-attention mechanism, which is
the center of the Transformer architecture, can be viewed as a non-parametric
estimator based on kernel density estimation (KDE). This motivates us to
leverage a set of robust kernel density estimation methods for alleviating the
issue of data contamination. Specifically, we introduce a series of
self-attention mechanisms that can be incorporated into different Transformer
architectures and discuss the special properties of each method. We then
perform extensive empirical studies on language modeling and image
classification tasks. Our methods demonstrate robust performance in multiple
scenarios while maintaining competitive results on clean datasets.",2210.05794v3,https://arxiv.org/pdf/2210.05794v3
What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness?,"Nikolaos Tsilivis, Julia Kempe","The adversarial vulnerability of neural nets, and subsequent techniques to
create robust models have attracted significant attention; yet we still lack a
full understanding of this phenomenon. Here, we study adversarial examples of
trained neural networks through analytical tools afforded by recent theory
advances connecting neural networks and kernel methods, namely the Neural
Tangent Kernel (NTK), following a growing body of work that leverages the NTK
approximation to successfully analyze important deep learning phenomena and
design algorithms for new applications. We show how NTKs allow to generate
adversarial examples in a ``training-free'' fashion, and demonstrate that they
transfer to fool their finite-width neural net counterparts in the ``lazy''
regime. We leverage this connection to provide an alternative view on robust
and non-robust features, which have been suggested to underlie the adversarial
brittleness of neural nets. Specifically, we define and study features induced
by the eigendecomposition of the kernel to better understand the role of robust
and non-robust features, the reliance on both for standard classification and
the robustness-accuracy trade-off. We find that such features are surprisingly
consistent across architectures, and that robust features tend to correspond to
the largest eigenvalues of the model, and thus are learned early during
training. Our framework allows us to identify and visualize non-robust yet
useful features. Finally, we shed light on the robustness mechanism underlying
adversarial training of neural nets used in practice: quantifying the evolution
of the associated empirical NTK, we demonstrate that its dynamics falls much
earlier into the ``lazy'' regime and manifests a much stronger form of the well
known bias to prioritize learning features within the top eigenspaces of the
kernel, compared to standard training.",2210.05577v2,https://arxiv.org/pdf/2210.05577v2
Schedule-Robust Online Continual Learning,"Ruohan Wang, Marco Ciccone, Giulia Luise, Andrew Yapp, Massimiliano Pontil, Carlo Ciliberto","A continual learning (CL) algorithm learns from a non-stationary data stream.
The non-stationarity is modeled by some schedule that determines how data is
presented over time. Most current methods make strong assumptions on the
schedule and have unpredictable performance when such requirements are not met.
A key challenge in CL is thus to design methods robust against arbitrary
schedules over the same underlying data, since in real-world scenarios
schedules are often unknown and dynamic. In this work, we introduce the notion
of schedule-robustness for CL and a novel approach satisfying this desirable
property in the challenging online class-incremental setting. We also present a
new perspective on CL, as the process of learning a schedule-robust predictor,
followed by adapting the predictor using only replay data. Empirically, we
demonstrate that our approach outperforms existing methods on CL benchmarks for
image classification by a large margin.",2210.05561v2,https://arxiv.org/pdf/2210.05561v2
Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval,"Peitian Zhang, Zheng Liu, Shitao Xiao, Zhicheng Dou, Jing Yao","Inverted file structure is a common technique for accelerating dense
retrieval. It clusters documents based on their embeddings; during searching,
it probes nearby clusters w.r.t. an input query and only evaluates documents
within them by subsequent codecs, thus avoiding the expensive cost of
exhaustive traversal. However, the clustering is always lossy, which results in
the miss of relevant documents in the probed clusters and hence degrades
retrieval quality. In contrast, lexical matching, such as overlaps of salient
terms, tends to be strong feature for identifying relevant documents. In this
work, we present the Hybrid Inverted Index (HI$^2$), where the embedding
clusters and salient terms work collaboratively to accelerate dense retrieval.
To make best of both effectiveness and efficiency, we devise a cluster selector
and a term selector, to construct compact inverted lists and efficiently
searching through them. Moreover, we leverage simple unsupervised algorithms as
well as end-to-end knowledge distillation to learn these two modules, with the
latter further boosting the effectiveness. Based on comprehensive experiments
on popular retrieval benchmarks, we verify that clusters and terms indeed
complement each other, enabling HI$^2$ to achieve lossless retrieval quality
with competitive efficiency across various index settings. Our code and
checkpoint are publicly available at
https://github.com/namespace-Pt/Adon/tree/HI2.",2210.05521v3,https://arxiv.org/pdf/2210.05521v3
"Robust and Controllable Object-Centric Learning through Energy-based
  Models","Ruixiang Zhang, Tong Che, Boris Ivanovic, Renhao Wang, Marco Pavone, Yoshua Bengio, Liam Paull","Humans are remarkably good at understanding and reasoning about complex
visual scenes. The capability to decompose low-level observations into discrete
objects allows us to build a grounded abstract representation and identify the
compositional structure of the world. Accordingly, it is a crucial step for
machine learning models to be capable of inferring objects and their properties
from visual scenes without explicit supervision. However, existing works on
object-centric representation learning either rely on tailor-made neural
network modules or strong probabilistic assumptions in the underlying
generative and inference processes. In this work, we present \ours, a
conceptually simple and general approach to learning object-centric
representations through an energy-based model. By forming a
permutation-invariant energy function using vanilla attention blocks readily
available in Transformers, we can infer object-centric latent variables via
gradient-based MCMC methods where permutation equivariance is automatically
guaranteed. We show that \ours can be easily integrated into existing
architectures and can effectively extract high-quality object-centric
representations, leading to better segmentation accuracy and competitive
downstream task performance. Further, empirical evaluations show that \ours's
learned representations are robust against distribution shift. Finally, we
demonstrate the effectiveness of \ours in systematic compositional
generalization, by re-composing learned energy functions for novel scene
generation and manipulation.",2210.05519v1,https://arxiv.org/pdf/2210.05519v1
Label Noise-Robust Learning using a Confidence-Based Sieving Strategy,"Reihaneh Torkzadehmahani, Reza Nasirigerdeh, Daniel Rueckert, Georgios Kaissis","In learning tasks with label noise, improving model robustness against
overfitting is a pivotal challenge because the model eventually memorizes
labels, including the noisy ones. Identifying the samples with noisy labels and
preventing the model from learning them is a promising approach to address this
challenge. When training with noisy labels, the per-class confidence scores of
the model, represented by the class probabilities, can be reliable criteria for
assessing whether the input label is the true label or the corrupted one. In
this work, we exploit this observation and propose a novel discriminator metric
called confidence error and a sieving strategy called CONFES to differentiate
between the clean and noisy samples effectively. We provide theoretical
guarantees on the probability of error for our proposed metric. Then, we
experimentally illustrate the superior performance of our proposed approach
compared to recent studies on various settings, such as synthetic and
real-world label noise. Moreover, we show CONFES can be combined with other
state-of-the-art approaches, such as Co-teaching and DivideMix to further
improve model performance.",2210.05330v3,https://arxiv.org/pdf/2210.05330v3
"RoHNAS: A Neural Architecture Search Framework with Conjoint
  Optimization for Adversarial Robustness and Hardware Efficiency of
  Convolutional and Capsule Networks","Alberto Marchisio, Vojtech Mrazek, Andrea Massa, Beatrice Bussolino, Maurizio Martina, Muhammad Shafique","Neural Architecture Search (NAS) algorithms aim at finding efficient Deep
Neural Network (DNN) architectures for a given application under given system
constraints. DNNs are computationally-complex as well as vulnerable to
adversarial attacks. In order to address multiple design objectives, we propose
RoHNAS, a novel NAS framework that jointly optimizes for adversarial-robustness
and hardware-efficiency of DNNs executed on specialized hardware accelerators.
Besides the traditional convolutional DNNs, RoHNAS additionally accounts for
complex types of DNNs such as Capsule Networks. For reducing the exploration
time, RoHNAS analyzes and selects appropriate values of adversarial
perturbation for each dataset to employ in the NAS flow. Extensive evaluations
on multi - Graphics Processing Unit (GPU) - High Performance Computing (HPC)
nodes provide a set of Pareto-optimal solutions, leveraging the tradeoff
between the above-discussed design objectives. For example, a Pareto-optimal
DNN for the CIFAR-10 dataset exhibits 86.07% accuracy, while having an energy
of 38.63 mJ, a memory footprint of 11.85 MiB, and a latency of 4.47 ms.",2210.05276v1,https://arxiv.org/pdf/2210.05276v1
"Boosting Adversarial Robustness From The Perspective of Effective Margin
  Regularization","Ziquan Liu, Antoni B. Chan","The adversarial vulnerability of deep neural networks (DNNs) has been
actively investigated in the past several years. This paper investigates the
scale-variant property of cross-entropy loss, which is the most commonly used
loss function in classification tasks, and its impact on the effective margin
and adversarial robustness of deep neural networks. Since the loss function is
not invariant to logit scaling, increasing the effective weight norm will make
the loss approach zero and its gradient vanish while the effective margin is
not adequately maximized. On typical DNNs, we demonstrate that, if not properly
regularized, the standard training does not learn large effective margins and
leads to adversarial vulnerability. To maximize the effective margins and learn
a robust DNN, we propose to regularize the effective weight norm during
training. Our empirical study on feedforward DNNs demonstrates that the
proposed effective margin regularization (EMR) learns large effective margins
and boosts the adversarial robustness in both standard and adversarial
training. On large-scale models, we show that EMR outperforms basic adversarial
training, TRADES and two regularization baselines with substantial improvement.
Moreover, when combined with several strong adversarial defense methods (MART
and MAIL), our EMR further boosts the robustness.",2210.05118v1,https://arxiv.org/pdf/2210.05118v1
"Robustification of Multilingual Language Models to Real-world Noise in
  Crosslingual Zero-shot Settings with Robust Contrastive Pretraining","Asa Cooper Stickland, Sailik Sengupta, Jason Krone, Saab Mansour, He He","Advances in neural modeling have achieved state-of-the-art (SOTA) results on
public natural language processing (NLP) benchmarks, at times surpassing human
performance. However, there is a gap between public benchmarks and real-world
applications where noise, such as typographical or grammatical mistakes, is
abundant and can result in degraded performance. Unfortunately, works which
evaluate the robustness of neural models on noisy data and propose
improvements, are limited to the English language. Upon analyzing noise in
different languages, we observe that noise types vary greatly across languages.
Thus, existing investigations do not generalize trivially to multilingual
settings. To benchmark the performance of pretrained multilingual language
models, we construct noisy datasets covering five languages and four NLP tasks
and observe a clear gap in the performance between clean and noisy data in the
zero-shot cross-lingual setting. After investigating several ways to boost the
robustness of multilingual models in this setting, we propose Robust
Contrastive Pretraining (RCP). RCP combines data augmentation with a
contrastive loss term at the pretraining stage and achieves large improvements
on noisy (and original test data) across two sentence-level (+3.2%) and two
sequence-labeling (+10 F1-score) multilingual classification tasks.",2210.04782v2,https://arxiv.org/pdf/2210.04782v2
Denoising Masked AutoEncoders Help Robust Classification,"Quanlin Wu, Hang Ye, Yuntian Gu, Huishuai Zhang, Liwei Wang, Di He","In this paper, we propose a new self-supervised method, which is called
Denoising Masked AutoEncoders (DMAE), for learning certified robust classifiers
of images. In DMAE, we corrupt each image by adding Gaussian noises to each
pixel value and randomly masking several patches. A Transformer-based
encoder-decoder model is then trained to reconstruct the original image from
the corrupted one. In this learning paradigm, the encoder will learn to capture
relevant semantics for the downstream tasks, which is also robust to Gaussian
additive noises. We show that the pre-trained encoder can naturally be used as
the base classifier in Gaussian smoothed models, where we can analytically
compute the certified radius for any data point. Although the proposed method
is simple, it yields significant performance improvement in downstream
classification tasks. We show that the DMAE ViT-Base model, which just uses
1/10 parameters of the model developed in recent work arXiv:2206.10550,
achieves competitive or better certified accuracy in various settings. The DMAE
ViT-Large model significantly surpasses all previous results, establishing a
new state-of-the-art on ImageNet dataset. We further demonstrate that the
pre-trained model has good transferability to the CIFAR-10 dataset, suggesting
its wide adaptability. Models and code are available at
https://github.com/quanlin-wu/dmae.",2210.06983v4,https://arxiv.org/pdf/2210.06983v4
"Is your noise correction noisy? PLS: Robustness to label noise with two
  stage detection","Paul Albert, Eric Arazo, Tarun Krishna, Noel E. O'Connor, Kevin McGuinness","Designing robust algorithms capable of training accurate neural networks on
uncurated datasets from the web has been the subject of much research as it
reduces the need for time consuming human labor. The focus of many previous
research contributions has been on the detection of different types of label
noise; however, this paper proposes to improve the correction accuracy of noisy
samples once they have been detected. In many state-of-the-art contributions, a
two phase approach is adopted where the noisy samples are detected before
guessing a corrected pseudo-label in a semi-supervised fashion. The guessed
pseudo-labels are then used in the supervised objective without ensuring that
the label guess is likely to be correct. This can lead to confirmation bias,
which reduces the noise robustness. Here we propose the pseudo-loss, a simple
metric that we find to be strongly correlated with pseudo-label correctness on
noisy samples. Using the pseudo-loss, we dynamically down weight
under-confident pseudo-labels throughout training to avoid confirmation bias
and improve the network accuracy. We additionally propose to use a confidence
guided contrastive objective that learns robust representation on an
interpolated objective between class bound (supervised) for confidently
corrected samples and unsupervised representation for under-confident label
corrections. Experiments demonstrate the state-of-the-art performance of our
Pseudo-Loss Selection (PLS) algorithm on a variety of benchmark datasets
including curated data synthetically corrupted with in-distribution and
out-of-distribution noise, and two real world web noise datasets. Our
experiments are fully reproducible github.com/PaulAlbert31/SNCF",2210.04578v2,https://arxiv.org/pdf/2210.04578v2
"Towards Robust Visual Question Answering: Making the Most of Biased
  Samples via Contrastive Learning","Qingyi Si, Yuanxin Liu, Fandong Meng, Zheng Lin, Peng Fu, Yanan Cao, Weiping Wang, Jie Zhou","Models for Visual Question Answering (VQA) often rely on the spurious
correlations, i.e., the language priors, that appear in the biased samples of
training set, which make them brittle against the out-of-distribution (OOD)
test data. Recent methods have achieved promising progress in overcoming this
problem by reducing the impact of biased samples on model training. However,
these models reveal a trade-off that the improvements on OOD data severely
sacrifice the performance on the in-distribution (ID) data (which is dominated
by the biased samples). Therefore, we propose a novel contrastive learning
approach, MMBS, for building robust VQA models by Making the Most of Biased
Samples. Specifically, we construct positive samples for contrastive learning
by eliminating the information related to spurious correlation from the
original training samples and explore several strategies to use the constructed
positive samples for training. Instead of undermining the importance of biased
samples in model training, our approach precisely exploits the biased samples
for unbiased information that contributes to reasoning. The proposed method is
compatible with various VQA backbones. We validate our contributions by
achieving competitive performance on the OOD dataset VQA-CP v2 while preserving
robust performance on the ID dataset VQA v2.",2210.04563v1,https://arxiv.org/pdf/2210.04563v1
"Everything is Varied: The Surprising Impact of Individual Variation on
  ML Robustness in Medicine","Andrea Campagner, Lorenzo Famiglini, Anna Carobene, Federico Cabitza","In medical settings, Individual Variation (IV) refers to variation that is
due not to population differences or errors, but rather to within-subject
variation, that is the intrinsic and characteristic patterns of variation
pertaining to a given instance or the measurement process. While taking into
account IV has been deemed critical for proper analysis of medical data, this
source of uncertainty and its impact on robustness have so far been neglected
in Machine Learning (ML). To fill this gap, we look at how IV affects ML
performance and generalization and how its impact can be mitigated.
Specifically, we provide a methodological contribution to formalize the problem
of IV in the statistical learning framework and, through an experiment based on
one of the largest real-world laboratory medicine datasets for the problem of
COVID-19 diagnosis, we show that: 1) common state-of-the-art ML models are
severely impacted by the presence of IV in data; and 2) advanced learning
strategies, based on data augmentation and data imprecisiation, and proper
study designs can be effective at improving robustness to IV. Our findings
demonstrate the critical relevance of correctly accounting for IV to enable
safe deployment of ML in clinical settings.",2210.04555v2,https://arxiv.org/pdf/2210.04555v2
"Pruning Adversarially Robust Neural Networks without Adversarial
  Examples","Tong Jian, Zifeng Wang, Yanzhi Wang, Jennifer Dy, Stratis Ioannidis","Adversarial pruning compresses models while preserving robustness. Current
methods require access to adversarial examples during pruning. This
significantly hampers training efficiency. Moreover, as new adversarial attacks
and training methods develop at a rapid rate, adversarial pruning methods need
to be modified accordingly to keep up. In this work, we propose a novel
framework to prune a previously trained robust neural network while maintaining
adversarial robustness, without further generating adversarial examples. We
leverage concurrent self-distillation and pruning to preserve knowledge in the
original model as well as regularizing the pruned model via the Hilbert-Schmidt
Information Bottleneck. We comprehensively evaluate our proposed framework and
show its superior performance in terms of both adversarial robustness and
efficiency when pruning architectures trained on the MNIST, CIFAR-10, and
CIFAR-100 datasets against five state-of-the-art attacks. Code is available at
https://github.com/neu-spiral/PwoA/.",2210.04311v1,https://arxiv.org/pdf/2210.04311v1
Coresets for Wasserstein Distributionally Robust Optimization Problems,"Ruomin Huang, Jiawei Huang, Wenjie Liu, Hu Ding","Wasserstein distributionally robust optimization (\textsf{WDRO}) is a popular
model to enhance the robustness of machine learning with ambiguous data.
However, the complexity of \textsf{WDRO} can be prohibitive in practice since
solving its ``minimax'' formulation requires a great amount of computation.
Recently, several fast \textsf{WDRO} training algorithms for some specific
machine learning tasks (e.g., logistic regression) have been developed.
However, the research on designing efficient algorithms for general large-scale
\textsf{WDRO}s is still quite limited, to the best of our knowledge.
\textit{Coreset} is an important tool for compressing large dataset, and thus
it has been widely applied to reduce the computational complexities for many
optimization problems. In this paper, we introduce a unified framework to
construct the $\epsilon$-coreset for the general \textsf{WDRO} problems. Though
it is challenging to obtain a conventional coreset for \textsf{WDRO} due to the
uncertainty issue of ambiguous data, we show that we can compute a ``dual
coreset'' by using the strong duality property of \textsf{WDRO}. Also, the
error introduced by the dual coreset can be theoretically guaranteed for the
original \textsf{WDRO} objective. To construct the dual coreset, we propose a
novel grid sampling approach that is particularly suitable for the dual
formulation of \textsf{WDRO}. Finally, we implement our coreset approach and
illustrate its effectiveness for several \textsf{WDRO} problems in the
experiments.",2210.04260v3,https://arxiv.org/pdf/2210.04260v3
"Unified Probabilistic Neural Architecture and Weight Ensembling Improves
  Model Robustness","Sumegha Premchandar, Sandeep Madireddy, Sanket Jantre, Prasanna Balaprakash","Robust machine learning models with accurately calibrated uncertainties are
crucial for safety-critical applications. Probabilistic machine learning and
especially the Bayesian formalism provide a systematic framework to incorporate
robustness through the distributional estimates and reason about uncertainty.
Recent works have shown that approximate inference approaches that take the
weight space uncertainty of neural networks to generate ensemble prediction are
the state-of-the-art. However, architecture choices have mostly been ad hoc,
which essentially ignores the epistemic uncertainty from the architecture
space. To this end, we propose a Unified probabilistic architecture and weight
ensembling Neural Architecture Search (UraeNAS) that leverages advances in
probabilistic neural architecture search and approximate Bayesian inference to
generate ensembles form the joint distribution of neural network architectures
and weights. The proposed approach showed a significant improvement both with
in-distribution (0.86% in accuracy, 42% in ECE) CIFAR-10 and
out-of-distribution (2.43% in accuracy, 30% in ECE) CIFAR-10-C compared to the
baseline deterministic approach.",2210.04083v1,https://arxiv.org/pdf/2210.04083v1
"Less is More: SlimG for Accurate, Robust, and Interpretable Graph Mining","Jaemin Yoo, Meng-Chieh Lee, Shubhranshu Shekhar, Christos Faloutsos","How can we solve semi-supervised node classification in various graphs
possibly with noisy features and structures? Graph neural networks (GNNs) have
succeeded in many graph mining tasks, but their generalizability to various
graph scenarios is limited due to the difficulty of training, hyperparameter
tuning, and the selection of a model itself. Einstein said that we should ""make
everything as simple as possible, but not simpler."" We rephrase it into the
careful simplicity principle: a carefully-designed simple model can surpass
sophisticated ones in real-world graphs. Based on the principle, we propose
SlimG for semi-supervised node classification, which exhibits four desirable
properties: It is (a) accurate, winning or tying on 10 out of 13 real-world
datasets; (b) robust, being the only one that handles all scenarios of graph
data (homophily, heterophily, random structure, noisy features, etc.); (c) fast
and scalable, showing up to 18 times faster training in million-scale graphs;
and (d) interpretable, thanks to the linearity and sparsity. We explain the
success of SlimG through a systematic study of the designs of existing GNNs,
sanity checks, and comprehensive ablation studies.",2210.04081v4,https://arxiv.org/pdf/2210.04081v4
Robustness of Unsupervised Representation Learning without Labels,"Aleksandar Petrov, Marta Kwiatkowska","Unsupervised representation learning leverages large unlabeled datasets and
is competitive with supervised learning. But non-robust encoders may affect
downstream task robustness. Recently, robust representation encoders have
become of interest. Still, all prior work evaluates robustness using a
downstream classification task. Instead, we propose a family of unsupervised
robustness measures, which are model- and task-agnostic and label-free. We
benchmark state-of-the-art representation encoders and show that none dominates
the rest. We offer unsupervised extensions to the FGSM and PGD attacks. When
used in adversarial training, they improve most unsupervised robustness
measures, including certified robustness. We validate our results against a
linear probe and show that, for MOCOv2, adversarial training results in 3 times
higher certified accuracy, a 2-fold decrease in impersonation attack success
rate and considerable improvements in certified robustness.",2210.04076v1,https://arxiv.org/pdf/2210.04076v1
"Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous
  Driving via Semantic Masked World Model","Zeyu Gao, Yao Mu, Chen Chen, Jingliang Duan, Shengbo Eben Li, Ping Luo, Yanfeng Lu","End-to-end autonomous driving provides a feasible way to automatically
maximize overall driving system performance by directly mapping the raw pixels
from a front-facing camera to control signals. Recent advanced methods
construct a latent world model to map the high dimensional observations into
compact latent space. However, the latent states embedded by the world model
proposed in previous works may contain a large amount of task-irrelevant
information, resulting in low sampling efficiency and poor robustness to input
perturbations. Meanwhile, the training data distribution is usually unbalanced,
and the learned policy is challenging to cope with the corner cases during the
driving process. To solve the above challenges, we present a SEMantic Masked
recurrent world model (SEM2), which introduces a semantic filter to extract key
driving-relevant features and make decisions via the filtered features, and is
trained with a multi-source data sampler, which aggregates common data and
multiple corner case data in a single batch, to balance the data distribution.
Extensive experiments on CARLA show our method outperforms the state-of-the-art
approaches in terms of sample efficiency and robustness to input permutations.",2210.04017v4,https://arxiv.org/pdf/2210.04017v4
"ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial
  Viewpoints","Yinpeng Dong, Shouwei Ruan, Hang Su, Caixin Kang, Xingxing Wei, Jun Zhu","Recent studies have demonstrated that visual recognition models lack
robustness to distribution shift. However, current work mainly considers model
robustness to 2D image transformations, leaving viewpoint changes in the 3D
world less explored. In general, viewpoint changes are prevalent in various
real-world applications (e.g., autonomous driving), making it imperative to
evaluate viewpoint robustness. In this paper, we propose a novel method called
ViewFool to find adversarial viewpoints that mislead visual recognition models.
By encoding real-world objects as neural radiance fields (NeRF), ViewFool
characterizes a distribution of diverse adversarial viewpoints under an
entropic regularizer, which helps to handle the fluctuations of the real camera
pose and mitigate the reality gap between the real objects and their neural
representations. Experiments validate that the common image classifiers are
extremely vulnerable to the generated adversarial viewpoints, which also
exhibit high cross-model transferability. Based on ViewFool, we introduce
ImageNet-V, a new out-of-distribution dataset for benchmarking viewpoint
robustness of image classifiers. Evaluation results on 40 classifiers with
diverse architectures, objective functions, and data augmentations reveal a
significant drop in model performance when tested on ImageNet-V, which provides
a possibility to leverage ViewFool as an effective data augmentation strategy
to improve viewpoint robustness.",2210.03895v1,https://arxiv.org/pdf/2210.03895v1
"1st ICLR International Workshop on Privacy, Accountability,
  Interpretability, Robustness, Reasoning on Structured Data (PAIR^2Struct)","Hao Wang, Wanyu Lin, Hao He, Di Wang, Chengzhi Mao, Muhan Zhang","Recent years have seen advances on principles and guidance relating to
accountable and ethical use of artificial intelligence (AI) spring up around
the globe. Specifically, Data Privacy, Accountability, Interpretability,
Robustness, and Reasoning have been broadly recognized as fundamental
principles of using machine learning (ML) technologies on decision-critical
and/or privacy-sensitive applications. On the other hand, in tremendous
real-world applications, data itself can be well represented as various
structured formalisms, such as graph-structured data (e.g., networks),
grid-structured data (e.g., images), sequential data (e.g., text), etc. By
exploiting the inherently structured knowledge, one can design plausible
approaches to identify and use more relevant variables to make reliable
decisions, thereby facilitating real-world deployments.",2210.03612v1,https://arxiv.org/pdf/2210.03612v1
"Indoor Localization with Robust Global Channel Charting: A
  Time-Distance-Based Approach","Maximilian Stahlke, George Yammine, Tobias Feigl, Bjoern M. Eskofier, Christopher Mutschler","Fingerprinting-based positioning significantly improves the indoor
localization performance in non-line-of-sight-dominated areas. However, its
deployment and maintenance is cost-intensive as it needs ground-truth reference
systems for both the initial training and the adaption to environmental
changes. In contrast, channel charting (CC) works without explicit reference
information and only requires the spatial correlations of channel state
information (CSI). While CC has shown promising results in modelling the
geometry of the radio environment, a deeper insight into CC for localization
using multi-anchor large-bandwidth measurements is still pending. We contribute
a novel distance metric for time-synchronized single-input/single-output CSIs
that approaches a linear correlation to the Euclidean distance. This allows to
learn the environment's global geometry without annotations. To efficiently
optimize the global channel chart we approximate the metric with a Siamese
neural network. This enables full CC-assisted fingerprinting and positioning
only using a linear transformation from the chart to the real-world
coordinates. We compare our approach to the state-of-the-art of CC on two
different real-world data sets recorded with a 5G and UWB radio setup. Our
approach outperforms others with localization accuracies of 0.69m for the UWB
and 1.4m for the 5G setup. We show that CC-assisted fingerprinting enables
highly accurate localization and reduces (or eliminates) the need for annotated
training data.",2210.06294v1,https://arxiv.org/pdf/2210.06294v1
Towards Out-of-Distribution Adversarial Robustness,"Adam Ibrahim, Charles Guille-Escuret, Ioannis Mitliagkas, Irina Rish, David Krueger, Pouya Bashivan","Adversarial robustness continues to be a major challenge for deep learning. A
core issue is that robustness to one type of attack often fails to transfer to
other attacks. While prior work establishes a theoretical trade-off in
robustness against different $L_p$ norms, we show that there is potential for
improvement against many commonly used attacks by adopting a domain
generalisation approach. Concretely, we treat each type of attack as a domain,
and apply the Risk Extrapolation method (REx), which promotes similar levels of
robustness against all training attacks. Compared to existing methods, we
obtain similar or superior worst-case adversarial robustness on attacks seen
during training. Moreover, we achieve superior performance on families or
tunings of attacks only encountered at test time. On ensembles of attacks, our
approach improves the accuracy from 3.4% with the best existing baseline to
25.9% on MNIST, and from 16.9% to 23.5% on CIFAR10.",2210.03150v4,https://arxiv.org/pdf/2210.03150v4
"Ambiguous Images With Human Judgments for Robust Visual Event
  Classification","Kate Sanders, Reno Kriz, Anqi Liu, Benjamin Van Durme","Contemporary vision benchmarks predominantly consider tasks on which humans
can achieve near-perfect performance. However, humans are frequently presented
with visual data that they cannot classify with 100% certainty, and models
trained on standard vision benchmarks achieve low performance when evaluated on
this data. To address this issue, we introduce a procedure for creating
datasets of ambiguous images and use it to produce SQUID-E (""Squidy""), a
collection of noisy images extracted from videos. All images are annotated with
ground truth values and a test set is annotated with human uncertainty
judgments. We use this dataset to characterize human uncertainty in vision
tasks and evaluate existing visual event classification models. Experimental
results suggest that existing vision models are not sufficiently equipped to
provide meaningful outputs for ambiguous images and that datasets of this
nature can be used to assess and improve such models through model training and
direct evaluation of model calibration. These findings motivate large-scale
ambiguous dataset creation and further research focusing on noisy visual data.",2210.03102v2,https://arxiv.org/pdf/2210.03102v2
"Designing a Robust Low-Level Agnostic Controller for a Quadrotor with
  Actor-Critic Reinforcement Learning","Guilherme Siqueira Eduardo, Wouter Caarls","Purpose: Real-life applications using quadrotors introduce a number of
disturbances and time-varying properties that pose a challenge to flight
controllers. We observed that, when a quadrotor is tasked with picking up and
dropping a payload, traditional PID and RL-based controllers found in
literature struggle to maintain flight after the vehicle changes its dynamics
due to interaction with this external object.
  Methods: In this work, we introduce domain randomization during the training
phase of a low-level waypoint guidance controller based on Soft Actor-Critic.
The resulting controller is evaluated on the proposed payload pick up and drop
task with added disturbances that emulate real-life operation of the vehicle.
  Results & Conclusion: We show that, by introducing a certain degree of
uncertainty in quadrotor dynamics during training, we can obtain a controller
that is capable to perform the proposed task using a larger variation of
quadrotor parameters. Additionally, the RL-based controller outperforms a
traditional positional PID controller with optimized gains in this task, while
remaining agnostic to different simulation parameters.",2210.02964v1,https://arxiv.org/pdf/2210.02964v1
"Communication-Efficient and Drift-Robust Federated Learning via Elastic
  Net","Seonhyeong Kim, Jiheon Woo, Daewon Seo, Yongjune Kim","Federated learning (FL) is a distributed method to train a global model over
a set of local clients while keeping data localized. It reduces the risks of
privacy and security but faces important challenges including expensive
communication costs and client drift issues. To address these issues, we
propose FedElasticNet, a communication-efficient and drift-robust FL framework
leveraging the elastic net. It repurposes two types of the elastic net
regularizers (i.e., $\ell_1$ and $\ell_2$ penalties on the local model
updates): (1) the $\ell_1$-norm regularizer sparsifies the local updates to
reduce the communication costs and (2) the $\ell_2$-norm regularizer resolves
the client drift problem by limiting the impact of drifting local updates due
to data heterogeneity. FedElasticNet is a general framework for FL; hence,
without additional costs, it can be integrated into prior FL techniques, e.g.,
FedAvg, FedProx, SCAFFOLD, and FedDyn. We show that our framework effectively
resolves the communication cost and client drift problems simultaneously.",2210.02940v1,https://arxiv.org/pdf/2210.02940v1
"A Closer Look at Robustness to L-infinity and Spatial Perturbations and
  their Composition","Luke Rowe, Benjamin Thérien, Krzysztof Czarnecki, Hongyang Zhang","In adversarial machine learning, the popular $\ell_\infty$ threat model has
been the focus of much previous work. While this mathematical definition of
imperceptibility successfully captures an infinite set of additive image
transformations that a model should be robust to, this is only a subset of all
transformations which leave the semantic label of an image unchanged. Indeed,
previous work also considered robustness to spatial attacks as well as other
semantic transformations; however, designing defense methods against the
composition of spatial and $\ell_{\infty}$ perturbations remains relatively
underexplored. In the following, we improve the understanding of this seldom
investigated compositional setting. We prove theoretically that no linear
classifier can achieve more than trivial accuracy against a composite adversary
in a simple statistical setting, illustrating its difficulty. We then
investigate how state-of-the-art $\ell_{\infty}$ defenses can be adapted to
this novel threat model and study their performance against compositional
attacks. We find that our newly proposed TRADES$_{\text{All}}$ strategy
performs the strongest of all. Analyzing its logit's Lipschitz constant for RT
transformations of different sizes, we find that TRADES$_{\text{All}}$ remains
stable over a wide range of RT transformations with and without $\ell_\infty$
perturbations.",2210.02577v1,https://arxiv.org/pdf/2210.02577v1
"COMPS: Conceptual Minimal Pair Sentences for testing Robust Property
  Knowledge and its Inheritance in Pre-trained Language Models","Kanishka Misra, Julia Taylor Rayz, Allyson Ettinger","A characteristic feature of human semantic cognition is its ability to not
only store and retrieve the properties of concepts observed through experience,
but to also facilitate the inheritance of properties (can breathe) from
superordinate concepts (animal) to their subordinates (dog) -- i.e. demonstrate
property inheritance. In this paper, we present COMPS, a collection of minimal
pair sentences that jointly tests pre-trained language models (PLMs) on their
ability to attribute properties to concepts and their ability to demonstrate
property inheritance behavior. Analyses of 22 different PLMs on COMPS reveal
that they can easily distinguish between concepts on the basis of a property
when they are trivially different, but find it relatively difficult when
concepts are related on the basis of nuanced knowledge representations.
Furthermore, we find that PLMs can demonstrate behavior consistent with
property inheritance to a great extent, but fail in the presence of distracting
information, which decreases the performance of many models, sometimes even
below chance. This lack of robustness in demonstrating simple reasoning raises
important questions about PLMs' capacity to make correct inferences even when
they appear to possess the prerequisite knowledge.",2210.01963v4,https://arxiv.org/pdf/2210.01963v4
Robust Fair Clustering: A Novel Fairness Attack and Defense Framework,"Anshuman Chhabra, Peizhao Li, Prasant Mohapatra, Hongfu Liu","Clustering algorithms are widely used in many societal resource allocation
applications, such as loan approvals and candidate recruitment, among others,
and hence, biased or unfair model outputs can adversely impact individuals that
rely on these applications. To this end, many fair clustering approaches have
been recently proposed to counteract this issue. Due to the potential for
significant harm, it is essential to ensure that fair clustering algorithms
provide consistently fair outputs even under adversarial influence. However,
fair clustering algorithms have not been studied from an adversarial attack
perspective. In contrast to previous research, we seek to bridge this gap and
conduct a robustness analysis against fair clustering by proposing a novel
black-box fairness attack. Through comprehensive experiments, we find that
state-of-the-art models are highly susceptible to our attack as it can reduce
their fairness performance significantly. Finally, we propose Consensus Fair
Clustering (CFC), the first robust fair clustering approach that transforms
consensus clustering into a fair graph partitioning problem, and iteratively
learns to generate fair cluster outputs. Experimentally, we observe that CFC is
highly robust to the proposed attack and is thus a truly robust fair clustering
alternative.",2210.01953v3,https://arxiv.org/pdf/2210.01953v3
"On the Robustness of Deep Clustering Models: Adversarial Attacks and
  Defenses","Anshuman Chhabra, Ashwin Sekhari, Prasant Mohapatra","Clustering models constitute a class of unsupervised machine learning methods
which are used in a number of application pipelines, and play a vital role in
modern data science. With recent advancements in deep learning -- deep
clustering models have emerged as the current state-of-the-art over traditional
clustering approaches, especially for high-dimensional image datasets. While
traditional clustering approaches have been analyzed from a robustness
perspective, no prior work has investigated adversarial attacks and robustness
for deep clustering models in a principled manner. To bridge this gap, we
propose a blackbox attack using Generative Adversarial Networks (GANs) where
the adversary does not know which deep clustering model is being used, but can
query it for outputs. We analyze our attack against multiple state-of-the-art
deep clustering models and real-world datasets, and find that it is highly
successful. We then employ some natural unsupervised defense approaches, but
find that these are unable to mitigate our attack. Finally, we attack Face++, a
production-level face clustering API service, and find that we can
significantly reduce its performance as well. Through this work, we thus aim to
motivate the need for truly robust deep clustering models.",2210.01940v1,https://arxiv.org/pdf/2210.01940v1
"Adaptively Weighted Data Augmentation Consistency Regularization for
  Robust Optimization under Concept Shift","Yijun Dong, Yuege Xie, Rachel Ward","Concept shift is a prevailing problem in natural tasks like medical image
segmentation where samples usually come from different subpopulations with
variant correlations between features and labels. One common type of concept
shift in medical image segmentation is the ""information imbalance"" between
label-sparse samples with few (if any) segmentation labels and label-dense
samples with plentiful labeled pixels. Existing distributionally robust
algorithms have focused on adaptively truncating/down-weighting the ""less
informative"" (i.e., label-sparse in our context) samples. To exploit data
features of label-sparse samples more efficiently, we propose an adaptively
weighted online optimization algorithm -- AdaWAC -- to incorporate data
augmentation consistency regularization in sample reweighting. Our method
introduces a set of trainable weights to balance the supervised loss and
unsupervised consistency regularization of each sample separately. At the
saddle point of the underlying objective, the weights assign label-dense
samples to the supervised loss and label-sparse samples to the unsupervised
consistency regularization. We provide a convergence guarantee by recasting the
optimization as online mirror descent on a saddle point problem. Our empirical
results demonstrate that AdaWAC not only enhances the segmentation performance
and sample efficiency but also improves the robustness to concept shift on
various medical image segmentation tasks with different UNet-style backbones.",2210.01891v2,https://arxiv.org/pdf/2210.01891v2
Robust self-healing prediction model for high dimensional data,"Anirudha Rayasam, Nagamma Patil","Owing to the advantages of increased accuracy and the potential to detect
unseen patterns, provided by data mining techniques they have been widely
incorporated for standard classification problems. They have often been used
for high precision disease prediction in the medical field, and several hybrid
prediction models capable of achieving high accuracies have been proposed.
Though this stands true most of the previous models fail to efficiently address
the recurring issue of bad data quality which plagues most high dimensional
data, and especially proves troublesome in the highly sensitive medical data.
This work proposes a robust self healing (RSH) hybrid prediction model which
functions by using the data in its entirety by removing errors and
inconsistencies from it rather than discarding any data. Initial processing
involves data preparation followed by cleansing or scrubbing through
context-dependent attribute correction, which ensures that there is no
significant loss of relevant information before the feature selection and
prediction phases. An ensemble of heterogeneous classifiers, subjected to local
boosting, is utilized to build the prediction model and genetic algorithm based
wrapper feature selection technique wrapped on the respective classifiers is
employed to select the corresponding optimal set of features, which warrant
higher accuracy. The proposed method is compared with some of the existing high
performing models and the results are analyzed.",2210.01788v1,https://arxiv.org/pdf/2210.01788v1
"Rethinking Lipschitz Neural Networks and Certified Robustness: A Boolean
  Function Perspective","Bohang Zhang, Du Jiang, Di He, Liwei Wang","Designing neural networks with bounded Lipschitz constant is a promising way
to obtain certifiably robust classifiers against adversarial examples. However,
the relevant progress for the important $\ell_\infty$ perturbation setting is
rather limited, and a principled understanding of how to design expressive
$\ell_\infty$ Lipschitz networks is still lacking. In this paper, we bridge the
gap by studying certified $\ell_\infty$ robustness from a novel perspective of
representing Boolean functions. We derive two fundamental impossibility results
that hold for any standard Lipschitz network: one for robust classification on
finite datasets, and the other for Lipschitz function approximation. These
results identify that networks built upon norm-bounded affine layers and
Lipschitz activations intrinsically lose expressive power even in the
two-dimensional case, and shed light on how recently proposed Lipschitz
networks (e.g., GroupSort and $\ell_\infty$-distance nets) bypass these
impossibilities by leveraging order statistic functions. Finally, based on
these insights, we develop a unified Lipschitz network that generalizes prior
works, and design a practical version that can be efficiently trained (making
certified robust training free). Extensive experiments show that our approach
is scalable, efficient, and consistently yields better certified robustness
across multiple datasets and perturbation radii than prior Lipschitz networks.
Our code is available at https://github.com/zbh2047/SortNet.",2210.01787v3,https://arxiv.org/pdf/2210.01787v3
"Robustness Certification of Visual Perception Models via Camera Motion
  Smoothing","Hanjiang Hu, Zuxin Liu, Linyi Li, Jiacheng Zhu, Ding Zhao","A vast literature shows that the learning-based visual perception model is
sensitive to adversarial noises, but few works consider the robustness of
robotic perception models under widely-existing camera motion perturbations. To
this end, we study the robustness of the visual perception model under camera
motion perturbations to investigate the influence of camera motion on robotic
perception. Specifically, we propose a motion smoothing technique for arbitrary
image classification models, whose robustness under camera motion perturbations
could be certified. The proposed robustness certification framework based on
camera motion smoothing provides tight and scalable robustness guarantees for
visual perception modules so that they are applicable to wide robotic
applications. As far as we are aware, this is the first work to provide
robustness certification for the deep perception module against camera motions,
which improves the trustworthiness of robotic perception. A realistic indoor
robotic dataset with a dense point cloud map for the entire room, MetaRoom, is
introduced for the challenging certifiable robust perception task. We conduct
extensive experiments to validate the certification approach via motion
smoothing against camera motion perturbations. Our framework guarantees the
certified accuracy of 81.7% against camera translation perturbation along depth
direction within -0.1m ~ 0.1m. We also validate the effectiveness of our method
on the real-world robot by conducting hardware experiments on the robotic arm
with an eye-in-hand camera. The code is available at
https://github.com/HanjiangHu/camera-motion-smoothing.",2210.04625v2,https://arxiv.org/pdf/2210.04625v2
"Tikhonov Regularization is Optimal Transport Robust under Martingale
  Constraints","Jiajin Li, Sirui Lin, Jose Blanchet, Viet Anh Nguyen","Distributionally robust optimization has been shown to offer a principled way
to regularize learning models. In this paper, we find that Tikhonov
regularization is distributionally robust in an optimal transport sense (i.e.,
if an adversary chooses distributions in a suitable optimal transport
neighborhood of the empirical measure), provided that suitable martingale
constraints are also imposed. Further, we introduce a relaxation of the
martingale constraints which not only provides a unified viewpoint to a class
of existing robust methods but also leads to new regularization tools. To
realize these novel tools, tractable computational algorithms are proposed. As
a byproduct, the strong duality theorem proved in this paper can be potentially
applied to other problems of independent interest.",2210.01413v1,https://arxiv.org/pdf/2210.01413v1
RAP: Risk-Aware Prediction for Robust Planning,"Haruki Nishimura, Jean Mercat, Blake Wulfe, Rowan McAllister, Adrien Gaidon","Robust planning in interactive scenarios requires predicting the uncertain
future to make risk-aware decisions. Unfortunately, due to long-tail
safety-critical events, the risk is often under-estimated by finite-sampling
approximations of probabilistic motion forecasts. This can lead to
overconfident and unsafe robot behavior, even with robust planners. Instead of
assuming full prediction coverage that robust planners require, we propose to
make prediction itself risk-aware. We introduce a new prediction objective to
learn a risk-biased distribution over trajectories, so that risk evaluation
simplifies to an expected cost estimation under this biased distribution. This
reduces the sample complexity of the risk estimation during online planning,
which is needed for safe real-time performance. Evaluation results in a
didactic simulation environment and on a real-world dataset demonstrate the
effectiveness of our approach. The code and a demo are available.",2210.01368v2,https://arxiv.org/pdf/2210.01368v2
Robust Active Distillation,"Cenk Baykal, Khoa Trinh, Fotis Iliopoulos, Gaurav Menghani, Erik Vee","Distilling knowledge from a large teacher model to a lightweight one is a
widely successful approach for generating compact, powerful models in the
semi-supervised learning setting where a limited amount of labeled data is
available. In large-scale applications, however, the teacher tends to provide a
large number of incorrect soft-labels that impairs student performance. The
sheer size of the teacher additionally constrains the number of soft-labels
that can be queried due to prohibitive computational and/or financial costs.
The difficulty in achieving simultaneous \emph{efficiency} (i.e., minimizing
soft-label queries) and \emph{robustness} (i.e., avoiding student inaccuracies
due to incorrect labels) hurts the widespread application of knowledge
distillation to many modern tasks. In this paper, we present a parameter-free
approach with provable guarantees to query the soft-labels of points that are
simultaneously informative and correctly labeled by the teacher. At the core of
our work lies a game-theoretic formulation that explicitly considers the
inherent trade-off between the informativeness and correctness of input
instances. We establish bounds on the expected performance of our approach that
hold even in worst-case distillation instances. We present empirical
evaluations on popular benchmarks that demonstrate the improved distillation
performance enabled by our work relative to that of state-of-the-art active
learning and active distillation methods.",2210.01213v2,https://arxiv.org/pdf/2210.01213v2
"MultiGuard: Provably Robust Multi-label Classification against
  Adversarial Examples","Jinyuan Jia, Wenjie Qu, Neil Zhenqiang Gong","Multi-label classification, which predicts a set of labels for an input, has
many applications. However, multiple recent studies showed that multi-label
classification is vulnerable to adversarial examples. In particular, an
attacker can manipulate the labels predicted by a multi-label classifier for an
input via adding carefully crafted, human-imperceptible perturbation to it.
Existing provable defenses for multi-class classification achieve sub-optimal
provable robustness guarantees when generalized to multi-label classification.
In this work, we propose MultiGuard, the first provably robust defense against
adversarial examples to multi-label classification. Our MultiGuard leverages
randomized smoothing, which is the state-of-the-art technique to build provably
robust classifiers. Specifically, given an arbitrary multi-label classifier,
our MultiGuard builds a smoothed multi-label classifier via adding random noise
to the input. We consider isotropic Gaussian noise in this work. Our major
theoretical contribution is that we show a certain number of ground truth
labels of an input are provably in the set of labels predicted by our
MultiGuard when the $\ell_2$-norm of the adversarial perturbation added to the
input is bounded. Moreover, we design an algorithm to compute our provable
robustness guarantees. Empirically, we evaluate our MultiGuard on VOC 2007,
MS-COCO, and NUS-WIDE benchmark datasets. Our code is available at:
\url{https://github.com/quwenjie/MultiGuard}",2210.01111v1,https://arxiv.org/pdf/2210.01111v1
"Push-Pull: Characterizing the Adversarial Robustness for Audio-Visual
  Active Speaker Detection","Xuanjun Chen, Haibin Wu, Helen Meng, Hung-yi Lee, Jyh-Shing Roger Jang","Audio-visual active speaker detection (AVASD) is well-developed, and now is
an indispensable front-end for several multi-modal applications. However, to
the best of our knowledge, the adversarial robustness of AVASD models hasn't
been investigated, not to mention the effective defense against such attacks.
In this paper, we are the first to reveal the vulnerability of AVASD models
under audio-only, visual-only, and audio-visual adversarial attacks through
extensive experiments. What's more, we also propose a novel audio-visual
interaction loss (AVIL) for making attackers difficult to find feasible
adversarial examples under an allocated attack budget. The loss aims at pushing
the inter-class embeddings to be dispersed, namely non-speech and speech
clusters, sufficiently disentangled, and pulling the intra-class embeddings as
close as possible to keep them compact. Experimental results show the AVIL
outperforms the adversarial training by 33.14 mAP (%) under multi-modal
attacks.",2210.00753v1,https://arxiv.org/pdf/2210.00753v1
"FedDig: Robust Federated Learning Using Data Digest to Represent Absent
  Clients","Chih-Fan Hsu, Ming-Ching Chang, Wei-Chao Chen","Federated Learning (FL) is a collaborative learning performed by a moderator
that protects data privacy. Existing cross-silo FL solutions seldom address the
absence of participating clients during training which can seriously degrade
model performances, particularly for unbalanced and non-IID client data. We
address this issue by generating secure data digests from the raw data and
using them to guide model training at the FL moderator. The proposed FL with
data digest (FedDig) framework can tolerate unexpected client absence while
preserving data privacy. This is achieved by de-identifying digests by mixing
and perturbing the encoded features of the raw data in the feature space. The
feature perturbing is performed following the Laplace mechanism of Differential
Privacy. We evaluate FedDig on EMNIST, CIFAR-10, and CIFAR-100 datasets. The
results consistently outperform three baseline algorithms (FedAvg, FedProx, and
FedNova) by large margins in multiple client absence scenarios.",2210.00737v3,https://arxiv.org/pdf/2210.00737v3
Robust Empirical Risk Minimization with Tolerance,"Robi Bhattacharjee, Max Hopkins, Akash Kumar, Hantao Yu, Kamalika Chaudhuri","Developing simple, sample-efficient learning algorithms for robust
classification is a pressing issue in today's tech-dominated world, and current
theoretical techniques requiring exponential sample complexity and complicated
improper learning rules fall far from answering the need. In this work we study
the fundamental paradigm of (robust) $\textit{empirical risk minimization}$
(RERM), a simple process in which the learner outputs any hypothesis minimizing
its training error. RERM famously fails to robustly learn VC classes (Montasser
et al., 2019a), a bound we show extends even to `nice' settings such as
(bounded) halfspaces. As such, we study a recent relaxation of the robust model
called $\textit{tolerant}$ robust learning (Ashtiani et al., 2022) where the
output classifier is compared to the best achievable error over slightly larger
perturbation sets. We show that under geometric niceness conditions, a natural
tolerant variant of RERM is indeed sufficient for $\gamma$-tolerant robust
learning VC classes over $\mathbb{R}^d$, and requires only $\tilde{O}\left(
\frac{VC(H)d\log \frac{D}{\gamma\delta}}{\epsilon^2}\right)$ samples for
robustness regions of (maximum) diameter $D$.",2210.00635v2,https://arxiv.org/pdf/2210.00635v2
Optimization for Robustness Evaluation beyond $\ell_p$ Metrics,"Hengyue Liang, Buyun Liang, Ying Cui, Tim Mitchell, Ju Sun","Empirical evaluation of deep learning models against adversarial attacks
entails solving nontrivial constrained optimization problems. Popular
algorithms for solving these constrained problems rely on projected gradient
descent (PGD) and require careful tuning of multiple hyperparameters. Moreover,
PGD can only handle $\ell_1$, $\ell_2$, and $\ell_\infty$ attack models due to
the use of analytical projectors. In this paper, we introduce a novel
algorithmic framework that blends a general-purpose constrained-optimization
solver PyGRANSO, With Constraint-Folding (PWCF), to add reliability and
generality to robustness evaluation. PWCF 1) finds good-quality solutions
without the need of delicate hyperparameter tuning, and 2) can handle general
attack models, e.g., general $\ell_p$ ($p \geq 0$) and perceptual attacks,
which are inaccessible to PGD-based algorithms.",2210.00621v2,https://arxiv.org/pdf/2210.00621v2
"Fast and Robust Video-Based Exercise Classification via Body Pose
  Tracking and Scalable Multivariate Time Series Classifiers","Ashish Singh, Antonio Bevilacqua, Thach Le Nguyen, Feiyan Hu, Kevin McGuinness, Martin OReilly, Darragh Whelan, Brian Caulfield, Georgiana Ifrim","Technological advancements have spurred the usage of machine learning based
applications in sports science. Physiotherapists, sports coaches and athletes
actively look to incorporate the latest technologies in order to further
improve performance and avoid injuries. While wearable sensors are very
popular, their use is hindered by constraints on battery power and sensor
calibration, especially for use cases which require multiple sensors to be
placed on the body. Hence, there is renewed interest in video-based data
capture and analysis for sports science. In this paper, we present the
application of classifying S\&C exercises using video. We focus on the popular
Military Press exercise, where the execution is captured with a video-camera
using a mobile device, such as a mobile phone, and the goal is to classify the
execution into different types. Since video recordings need a lot of storage
and computation, this use case requires data reduction, while preserving the
classification accuracy and enabling fast prediction. To this end, we propose
an approach named BodyMTS to turn video into time series by employing body pose
tracking, followed by training and prediction using multivariate time series
classifiers. We analyze the accuracy and robustness of BodyMTS and show that it
is robust to different types of noise caused by either video quality or pose
estimation factors. We compare BodyMTS to state-of-the-art deep learning
methods which classify human activity directly from videos and show that
BodyMTS achieves similar accuracy, but with reduced running time and model
engineering effort. Finally, we discuss some of the practical aspects of
employing BodyMTS in this application in terms of accuracy and robustness under
reduced data quality and size. We show that BodyMTS achieves an average
accuracy of 87\%, which is significantly higher than the accuracy of human
domain experts.",2210.00507v1,https://arxiv.org/pdf/2210.00507v1
"Robust Bayesian optimization with reinforcement learned acquisition
  functions","Zijing Liu, Xiyao Qu, Xuejun Liu, Hongqiang Lyu","In Bayesian optimization (BO) for expensive black-box optimization tasks,
acquisition function (AF) guides sequential sampling and plays a pivotal role
for efficient convergence to better optima. Prevailing AFs usually rely on
artificial experiences in terms of preferences for exploration or exploitation,
which runs a risk of a computational waste or traps in local optima and
resultant re-optimization. To address the crux, the idea of data-driven AF
selection is proposed, and the sequential AF selection task is further
formalized as a Markov decision process (MDP) and resort to powerful
reinforcement learning (RL) technologies. Appropriate selection policy for AFs
is learned from superior BO trajectories to balance between exploration and
exploitation in real time, which is called reinforcement-learning-assisted
Bayesian optimization (RLABO). Competitive and robust BO evaluations on five
benchmark problems demonstrate RL's recognition of the implicit AF selection
pattern and imply the proposal's potential practicality for intelligent AF
selection as well as efficient optimization in expensive black-box problems.",2210.00476v1,https://arxiv.org/pdf/2210.00476v1
"Understanding Adversarial Robustness Against On-manifold Adversarial
  Examples","Jiancong Xiao, Liusha Yang, Yanbo Fan, Jue Wang, Zhi-Quan Luo","Deep neural networks (DNNs) are shown to be vulnerable to adversarial
examples. A well-trained model can be easily attacked by adding small
perturbations to the original data. One of the hypotheses of the existence of
the adversarial examples is the off-manifold assumption: adversarial examples
lie off the data manifold. However, recent research showed that on-manifold
adversarial examples also exist. In this paper, we revisit the off-manifold
assumption and want to study a question: at what level is the poor performance
of neural networks against adversarial attacks due to on-manifold adversarial
examples? Since the true data manifold is unknown in practice, we consider two
approximated on-manifold adversarial examples on both real and synthesis
datasets. On real datasets, we show that on-manifold adversarial examples have
greater attack rates than off-manifold adversarial examples on both
standard-trained and adversarially-trained models. On synthetic datasets,
theoretically, We prove that on-manifold adversarial examples are powerful, yet
adversarial training focuses on off-manifold directions and ignores the
on-manifold adversarial examples. Furthermore, we provide analysis to show that
the properties derived theoretically can also be observed in practice. Our
analysis suggests that on-manifold adversarial examples are important, and we
should pay more attention to on-manifold adversarial examples for training
robust models.",2210.00430v1,https://arxiv.org/pdf/2210.00430v1
"On the tightness of linear relaxation based robustness certification
  methods",Cheng Tang,"There has been a rapid development and interest in adversarial training and
defenses in the machine learning community in the recent years. One line of
research focuses on improving the performance and efficiency of adversarial
robustness certificates for neural networks \cite{gowal:19, wong_zico:18,
raghunathan:18, WengTowardsFC:18, wong:scalable:18, singh:convex_barrier:19,
Huang_etal:19, single-neuron-relax:20, Zhang2020TowardsSA}. While each
providing a certification to lower (or upper) bound the true distortion under
adversarial attacks via relaxation, less studied was the tightness of
relaxation. In this paper, we analyze a family of linear outer approximation
based certificate methods via a meta algorithm, IBP-Lin. The aforementioned
works often lack quantitative analysis to answer questions such as how does the
performance of the certificate method depend on the network configuration and
the choice of approximation parameters. Under our framework, we make a first
attempt at answering these questions, which reveals that the tightness of
linear approximation based certification can depend heavily on the
configuration of the trained networks.",2210.00178v2,https://arxiv.org/pdf/2210.00178v2
Adversarial Robustness of Representation Learning for Knowledge Graphs,Peru Bhardwaj,"Knowledge graphs represent factual knowledge about the world as relationships
between concepts and are critical for intelligent decision making in enterprise
applications. New knowledge is inferred from the existing facts in the
knowledge graphs by encoding the concepts and relations into low-dimensional
feature vector representations. The most effective representations for this
task, called Knowledge Graph Embeddings (KGE), are learned through neural
network architectures. Due to their impressive predictive performance, they are
increasingly used in high-impact domains like healthcare, finance and
education. However, are the black-box KGE models adversarially robust for use
in domains with high stakes? This thesis argues that state-of-the-art KGE
models are vulnerable to data poisoning attacks, that is, their predictive
performance can be degraded by systematically crafted perturbations to the
training knowledge graph. To support this argument, two novel data poisoning
attacks are proposed that craft input deletions or additions at training time
to subvert the learned model's performance at inference time. These adversarial
attacks target the task of predicting the missing facts in knowledge graphs
using KGE models, and the evaluation shows that the simpler attacks are
competitive with or outperform the computationally expensive ones. The thesis
contributions not only highlight and provide an opportunity to fix the security
vulnerabilities of KGE models, but also help to understand the black-box
predictive behaviour of KGE models.",2210.00122v1,https://arxiv.org/pdf/2210.00122v1
Improving Robustness with Adaptive Weight Decay,"Amin Ghiasi, Ali Shafahi, Reza Ardekani","We propose adaptive weight decay, which automatically tunes the
hyper-parameter for weight decay during each training iteration. For
classification problems, we propose changing the value of the weight decay
hyper-parameter on the fly based on the strength of updates from the
classification loss (i.e., gradient of cross-entropy), and the regularization
loss (i.e., $\ell_2$-norm of the weights). We show that this simple
modification can result in large improvements in adversarial robustness -- an
area which suffers from robust overfitting -- without requiring extra data
across various datasets and architecture choices. For example, our
reformulation results in $20\%$ relative robustness improvement for CIFAR-100,
and $10\%$ relative robustness improvement on CIFAR-10 comparing to the best
tuned hyper-parameters of traditional weight decay resulting in models that
have comparable performance to SOTA robustness methods. In addition, this
method has other desirable properties, such as less sensitivity to learning
rate, and smaller weight norms, which the latter contributes to robustness to
overfitting to label noise, and pruning.",2210.00094v2,https://arxiv.org/pdf/2210.00094v2
Learning Robust Kernel Ensembles with Kernel Average Pooling,"Pouya Bashivan, Adam Ibrahim, Amirozhan Dehghani, Yifei Ren","Model ensembles have long been used in machine learning to reduce the
variance in individual model predictions, making them more robust to input
perturbations. Pseudo-ensemble methods like dropout have also been commonly
used in deep learning models to improve generalization. However, the
application of these techniques to improve neural networks' robustness against
input perturbations remains underexplored. We introduce Kernel Average Pooling
(KAP), a neural network building block that applies the mean filter along the
kernel dimension of the layer activation tensor. We show that ensembles of
kernels with similar functionality naturally emerge in convolutional neural
networks equipped with KAP and trained with backpropagation. Moreover, we show
that when trained on inputs perturbed with additive Gaussian noise, KAP models
are remarkably robust against various forms of adversarial attacks. Empirical
evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show
substantial improvements in robustness against strong adversarial attacks such
as AutoAttack without training on any adversarial examples.",2210.00062v2,https://arxiv.org/pdf/2210.00062v2
"Robust $Q$-learning Algorithm for Markov Decision Processes under
  Wasserstein Uncertainty","Ariel Neufeld, Julian Sester","We present a novel $Q$-learning algorithm tailored to solve distributionally
robust Markov decision problems where the corresponding ambiguity set of
transition probabilities for the underlying Markov decision process is a
Wasserstein ball around a (possibly estimated) reference measure. We prove
convergence of the presented algorithm and provide several examples also using
real data to illustrate both the tractability of our algorithm as well as the
benefits of considering distributional robustness when solving stochastic
optimal control problems, in particular when the estimated distributions turn
out to be misspecified in practice.",2210.00898v3,https://arxiv.org/pdf/2210.00898v3
"Bounded Robustness in Reinforcement Learning via Lexicographic
  Objectives","Daniel Jarne Ornia, Licio Romao, Lewis Hammond, Manuel Mazo Jr., Alessandro Abate","Policy robustness in Reinforcement Learning may not be desirable at any cost:
the alterations caused by robustness requirements from otherwise optimal
policies should be explainable, quantifiable and formally verifiable. In this
work we study how policies can be maximally robust to arbitrary observational
noise by analysing how they are altered by this noise through a stochastic
linear operator interpretation of the disturbances, and establish connections
between robustness and properties of the noise kernel and of the underlying
MDPs. Then, we construct sufficient conditions for policy robustness, and
propose a robustness-inducing scheme, applicable to any policy gradient
algorithm, that formally trades off expected policy utility for robustness
through lexicographic optimisation, while preserving convergence and
sub-optimality in the policy synthesis.",2209.15320v2,https://arxiv.org/pdf/2209.15320v2
Online Multi-Agent Decentralized Byzantine-robust Gradient Estimation,"Alexandre Reiffers-Masson, Isabel Amigo","In this paper, we propose an iterative scheme for distributed
Byzantineresilient estimation of a gradient associated with a black-box model.
Our algorithm is based on simultaneous perturbation, secure state estimation
and two-timescale stochastic approximations. We also show the performance of
our algorithm through numerical experiments.",2209.15274v1,https://arxiv.org/pdf/2209.15274v1
Your Out-of-Distribution Detection Method is Not Robust!,"Mohammad Azizmalayeri, Arshia Soltani Moakhar, Arman Zarei, Reihaneh Zohrabi, Mohammad Taghi Manzuri, Mohammad Hossein Rohban","Out-of-distribution (OOD) detection has recently gained substantial attention
due to the importance of identifying out-of-domain samples in reliability and
safety. Although OOD detection methods have advanced by a great deal, they are
still susceptible to adversarial examples, which is a violation of their
purpose. To mitigate this issue, several defenses have recently been proposed.
Nevertheless, these efforts remained ineffective, as their evaluations are
based on either small perturbation sizes, or weak attacks. In this work, we
re-examine these defenses against an end-to-end PGD attack on in/out data with
larger perturbation sizes, e.g. up to commonly used $\epsilon=8/255$ for the
CIFAR-10 dataset. Surprisingly, almost all of these defenses perform worse than
a random detection under the adversarial setting. Next, we aim to provide a
robust OOD detection method. In an ideal defense, the training should expose
the model to almost all possible adversarial perturbations, which can be
achieved through adversarial training. That is, such training perturbations
should based on both in- and out-of-distribution samples. Therefore, unlike OOD
detection in the standard setting, access to OOD, as well as in-distribution,
samples sounds necessary in the adversarial training setup. These tips lead us
to adopt generative OOD detection methods, such as OpenGAN, as a baseline. We
subsequently propose the Adversarially Trained Discriminator (ATD), which
utilizes a pre-trained robust model to extract robust features, and a generator
model to create OOD samples. Using ATD with CIFAR-10 and CIFAR-100 as the
in-distribution data, we could significantly outperform all previous methods in
the robust AUROC while maintaining high standard AUROC and classification
accuracy. The code repository is available at https://github.com/rohban-lab/ATD .",2209.15246v1,https://arxiv.org/pdf/2209.15246v1
"Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture
  Models","Ye Tian, Haolei Weng, Lucy Xia, Yang Feng","Unsupervised learning has been widely used in many real-world applications.
One of the simplest and most important unsupervised learning models is the
Gaussian mixture model (GMM). In this work, we study the multi-task learning
problem on GMMs, which aims to leverage potentially similar GMM parameter
structures among tasks to obtain improved learning performance compared to
single-task learning. We propose a multi-task GMM learning procedure based on
the EM algorithm that effectively utilizes unknown similarities between related
tasks and is robust against a fraction of outlier tasks from arbitrary
distributions. The proposed procedure is shown to achieve the minimax optimal
rate of convergence for both parameter estimation error and the excess
mis-clustering error, in a wide range of regimes. Moreover, we generalize our
approach to tackle the problem of transfer learning for GMMs, where similar
theoretical results are derived. Additionally, iterative unsupervised
multi-task and transfer learning methods may suffer from an initialization
alignment problem, and two alignment algorithms are proposed to resolve the
issue. Finally, we demonstrate the effectiveness of our methods through
simulations and real data examples. To the best of our knowledge, this is the
first work studying multi-task and transfer learning on GMMs with theoretical
guarantees.",2209.15224v4,https://arxiv.org/pdf/2209.15224v4
Generalizability of Adversarial Robustness Under Distribution Shifts,"Kumail Alhamoud, Hasan Abed Al Kader Hammoud, Motasem Alfarra, Bernard Ghanem","Recent progress in empirical and certified robustness promises to deliver
reliable and deployable Deep Neural Networks (DNNs). Despite that success, most
existing evaluations of DNN robustness have been done on images sampled from
the same distribution on which the model was trained. However, in the real
world, DNNs may be deployed in dynamic environments that exhibit significant
distribution shifts. In this work, we take a first step towards thoroughly
investigating the interplay between empirical and certified adversarial
robustness on one hand and domain generalization on another. To do so, we train
robust models on multiple domains and evaluate their accuracy and robustness on
an unseen domain. We observe that: (1) both empirical and certified robustness
generalize to unseen domains, and (2) the level of generalizability does not
correlate well with input visual similarity, measured by the FID between source
and target domains. We also extend our study to cover a real-world medical
application, in which adversarial augmentation significantly boosts the
generalization of robustness with minimal effect on clean data accuracy.",2209.15042v3,https://arxiv.org/pdf/2209.15042v3
"NAG-GS: Semi-Implicit, Accelerated and Robust Stochastic Optimizer","Valentin Leplat, Daniil Merkulov, Aleksandr Katrutsa, Daniel Bershatsky, Olga Tsymboi, Ivan Oseledets","Classical machine learning models such as deep neural networks are usually
trained by using Stochastic Gradient Descent-based (SGD) algorithms. The
classical SGD can be interpreted as a discretization of the stochastic gradient
flow. In this paper we propose a novel, robust and accelerated stochastic
optimizer that relies on two key elements: (1) an accelerated Nesterov-like
Stochastic Differential Equation (SDE) and (2) its semi-implicit Gauss-Seidel
type discretization. The convergence and stability of the obtained method,
referred to as NAG-GS, are first studied extensively in the case of the
minimization of a quadratic function. This analysis allows us to come up with
an optimal learning rate in terms of the convergence rate while ensuring the
stability of NAG-GS. This is achieved by the careful analysis of the spectral
radius of the iteration matrix and the covariance matrix at stationarity with
respect to all hyperparameters of our method. Further, we show that NAG- GS is
competitive with state-of-the-art methods such as momentum SGD with weight
decay and AdamW for the training of machine learning models such as the
logistic regression model, the residual networks models on standard computer
vision datasets, Transformers in the frame of the GLUE benchmark and the recent
Vision Transformers.",2209.14937v2,https://arxiv.org/pdf/2209.14937v2
"Learning Low-Frequency Motion Control for Robust and Dynamic Robot
  Locomotion","Siddhant Gangapurwala, Luigi Campanaro, Ioannis Havoutis","Robotic locomotion is often approached with the goal of maximizing robustness
and reactivity by increasing motion control frequency. We challenge this
intuitive notion by demonstrating robust and dynamic locomotion with a learned
motion controller executing at as low as 8 Hz on a real ANYmal C quadruped. The
robot is able to robustly and repeatably achieve a high heading velocity of 1.5
m/s, traverse uneven terrain, and resist unexpected external perturbations. We
further present a comparative analysis of deep reinforcement learning (RL)
based motion control policies trained and executed at frequencies ranging from
5 Hz to 200 Hz. We show that low-frequency policies are less sensitive to
actuation latencies and variations in system dynamics. This is to the extent
that a successful sim-to-real transfer can be performed even without any
dynamics randomization or actuation modeling. We support this claim through a
set of rigorous empirical evaluations. Moreover, to assist reproducibility, we
provide the training and deployment code along with an extended analysis at
https://ori-drs.github.io/lfmc/.",2209.14887v2,https://arxiv.org/pdf/2209.14887v2
Local and Regional Counterfactual Rules: Summarized and Robust Recourses,"Salim I. Amoukou, Nicolas J. B Brunel","Counterfactual Explanations (CE) face several unresolved challenges, such as
ensuring stability, synthesizing multiple CEs, and providing plausibility and
sparsity guarantees. From a more practical point of view, recent studies
[Pawelczyk et al., 2022] show that the prescribed counterfactual recourses are
often not implemented exactly by individuals and demonstrate that most
state-of-the-art CE algorithms are very likely to fail in this noisy
environment. To address these issues, we propose a probabilistic framework that
gives a sparse local counterfactual rule for each observation, providing rules
that give a range of values capable of changing decisions with high
probability. These rules serve as a summary of diverse counterfactual
explanations and yield robust recourses. We further aggregate these local rules
into a regional counterfactual rule, identifying shared recourses for subgroups
of the data. Our local and regional rules are derived from the Random Forest
algorithm, which offers statistical guarantees and fidelity to data
distribution by selecting recourses in high-density regions. Moreover, our
rules are sparse as we first select the smallest set of variables having a high
probability of changing the decision. We have conducted experiments to validate
the effectiveness of our counterfactual rules in comparison to standard CE and
recent similar attempts. Our methods are available as a Python package.",2209.14568v3,https://arxiv.org/pdf/2209.14568v3
"Semantics-Guided Object Removal for Facial Images: with Broad
  Applicability and Robust Style Preservation","Jookyung Song, Yeonjin Chang, Seonguk Park, Nojun Kwak","Object removal and image inpainting in facial images is a task in which
objects that occlude a facial image are specifically targeted, removed, and
replaced by a properly reconstructed facial image. Two different approaches
utilizing U-net and modulated generator respectively have been widely endorsed
for this task for their unique advantages but notwithstanding each method's
innate disadvantages. U-net, a conventional approach for conditional GANs,
retains fine details of unmasked regions but the style of the reconstructed
image is inconsistent with the rest of the original image and only works
robustly when the size of the occluding object is small enough. In contrast,
the modulated generative approach can deal with a larger occluded area in an
image and provides {a} more consistent style, yet it usually misses out on most
of the detailed features. This trade-off between these two models necessitates
an invention of a model that can be applied to any size of mask while
maintaining a consistent style and preserving minute details of facial
features. Here, we propose Semantics-Guided Inpainting Network (SGIN) which
itself is a modification of the modulated generator, aiming to take advantage
of its advanced generative capability and preserve the high-fidelity details of
the original image. By using the guidance of a semantic map, our model is
capable of manipulating facial features which grants direction to the
one-to-many problem for further practicability.",2209.14479v1,https://arxiv.org/pdf/2209.14479v1
Variational Bayes for robust radar single object tracking,"Alp Sarı, Tak Kaneko, Lense H. M. Swaenen, Wouter M. Kouw","We address object tracking by radar and the robustness of the current
state-of-the-art methods to process outliers. The standard tracking algorithms
extract detections from radar image space to use it in the filtering stage.
Filtering is performed by a Kalman filter, which assumes Gaussian distributed
noise. However, this assumption does not account for large modeling errors and
results in poor tracking performance during abrupt motions. We take the
Gaussian Sum Filter (single-object variant of the Multi Hypothesis Tracker) as
our baseline and propose a modification by modelling process noise with a
distribution that has heavier tails than a Gaussian. Variational Bayes provides
a fast, computationally cheap inference algorithm. Our simulations show that -
in the presence of process outliers - the robust tracker outperforms the
Gaussian Sum filter when tracking single objects.",2209.14397v1,https://arxiv.org/pdf/2209.14397v1
Conformal Prediction is Robust to Dispersive Label Noise,"Shai Feldman, Bat-Sheva Einbinder, Stephen Bates, Anastasios N. Angelopoulos, Asaf Gendler, Yaniv Romano","We study the robustness of conformal prediction, a powerful tool for
uncertainty quantification, to label noise. Our analysis tackles both
regression and classification problems, characterizing when and how it is
possible to construct uncertainty sets that correctly cover the unobserved
noiseless ground truth labels. We further extend our theory and formulate the
requirements for correctly controlling a general loss function, such as the
false negative proportion, with noisy labels. Our theory and experiments
suggest that conformal prediction and risk-controlling techniques with noisy
labels attain conservative risk over the clean ground truth labels except in
adversarial cases. In such cases, we can also correct for noise of bounded size
in the conformal prediction algorithm in order to ensure achieving the correct
risk of the ground truth labels without score or data regularity.",2209.14295v2,https://arxiv.org/pdf/2209.14295v2
"Exploring the Relationship between Architecture and Adversarially Robust
  Generalization","Aishan Liu, Shiyu Tang, Siyuan Liang, Ruihao Gong, Boxi Wu, Xianglong Liu, Dacheng Tao","Adversarial training has been demonstrated to be one of the most effective
remedies for defending adversarial examples, yet it often suffers from the huge
robustness generalization gap on unseen testing adversaries, deemed as the
adversarially robust generalization problem. Despite the preliminary
understandings devoted to adversarially robust generalization, little is known
from the architectural perspective. To bridge the gap, this paper for the first
time systematically investigated the relationship between adversarially robust
generalization and architectural design. Inparticular, we comprehensively
evaluated 20 most representative adversarially trained architectures on
ImageNette and CIFAR-10 datasets towards multiple `p-norm adversarial attacks.
Based on the extensive experiments, we found that, under aligned settings,
Vision Transformers (e.g., PVT, CoAtNet) often yield better adversarially
robust generalization while CNNs tend to overfit on specific attacks and fail
to generalize on multiple adversaries. To better understand the nature behind
it, we conduct theoretical analysis via the lens of Rademacher complexity. We
revealed the fact that the higher weight sparsity contributes significantly
towards the better adversarially robust generalization of Transformers, which
can be often achieved by the specially-designed attention blocks. We hope our
paper could help to better understand the mechanism for designing robust DNNs.
Our model weights can be found at http://robust.art.",2209.14105v2,https://arxiv.org/pdf/2209.14105v2
"Global Weighted Tensor Nuclear Norm for Tensor Robust Principal
  Component Analysis","Libin Wang, Yulong Wang, Shiyuan Wang, Youheng Liu, Yutao Hu, Longlong Chen, Hong Chen","Tensor Robust Principal Component Analysis (TRPCA), which aims to recover a
low-rank tensor corrupted by sparse noise, has attracted much attention in many
real applications. This paper develops a new Global Weighted TRPCA method
(GWTRPCA), which is the first approach simultaneously considers the
significance of intra-frontal slice and inter-frontal slice singular values in
the Fourier domain. Exploiting this global information, GWTRPCA penalizes the
larger singular values less and assigns smaller weights to them. Hence, our
method can recover the low-tubal-rank components more exactly. Moreover, we
propose an effective adaptive weight learning strategy by a Modified Cauchy
Estimator (MCE) since the weight setting plays a crucial role in the success of
GWTRPCA. To implement the GWTRPCA method, we devise an optimization algorithm
using an Alternating Direction Method of Multipliers (ADMM) method. Experiments
on real-world datasets validate the effectiveness of our proposed method.",2209.14084v2,https://arxiv.org/pdf/2209.14084v2
"Discussion about Attacks and Defenses for Fair and Robust Recommendation
  System Design","Mirae Kim, Simon Woo","Information has exploded on the Internet and mobile with the advent of the
big data era. In particular, recommendation systems are widely used to help
consumers who struggle to select the best products among such a large amount of
information. However, recommendation systems are vulnerable to malicious user
biases, such as fake reviews to promote or demote specific products, as well as
attacks that steal personal information. Such biases and attacks compromise the
fairness of the recommendation model and infringe the privacy of users and
systems by distorting data.Recently, deep-learning collaborative filtering
recommendation systems have shown to be more vulnerable to this bias. In this
position paper, we examine the effects of bias that cause various ethical and
social issues, and discuss the need for designing the robust recommendation
system for fairness and stability.",2210.07817v1,https://arxiv.org/pdf/2210.07817v1
"On the Robustness of Random Forest Against Untargeted Data Poisoning: An
  Ensemble-Based Approach","Marco Anisetti, Claudio A. Ardagna, Alessandro Balestrucci, Nicola Bena, Ernesto Damiani, Chan Yeob Yeun","Machine learning is becoming ubiquitous. From finance to medicine, machine
learning models are boosting decision-making processes and even outperforming
humans in some tasks. This huge progress in terms of prediction quality does
not however find a counterpart in the security of such models and corresponding
predictions, where perturbations of fractions of the training set (poisoning)
can seriously undermine the model accuracy. Research on poisoning attacks and
defenses received increasing attention in the last decade, leading to several
promising solutions aiming to increase the robustness of machine learning.
Among them, ensemble-based defenses, where different models are trained on
portions of the training set and their predictions are then aggregated, provide
strong theoretical guarantees at the price of a linear overhead. Surprisingly,
ensemble-based defenses, which do not pose any restrictions on the base model,
have not been applied to increase the robustness of random forest models. The
work in this paper aims to fill in this gap by designing and implementing a
novel hash-based ensemble approach that protects random forest against
untargeted, random poisoning attacks. An extensive experimental evaluation
measures the performance of our approach against a variety of attacks, as well
as its sustainability in terms of resource consumption and performance, and
compares it with a traditional monolithic model based on random forest. A final
discussion presents our main findings and compares our approach with existing
poisoning defenses targeting random forests.",2209.14013v3,https://arxiv.org/pdf/2209.14013v3
Online Policy Optimization for Robust MDP,"Jing Dong, Jingwei Li, Baoxiang Wang, Jingzhao Zhang","Reinforcement learning (RL) has exceeded human performance in many synthetic
settings such as video games and Go. However, real-world deployment of
end-to-end RL models is less common, as RL models can be very sensitive to
slight perturbation of the environment. The robust Markov decision process
(MDP) framework -- in which the transition probabilities belong to an
uncertainty set around a nominal model -- provides one way to develop robust
models. While previous analysis shows RL algorithms are effective assuming
access to a generative model, it remains unclear whether RL can be efficient
under a more realistic online setting, which requires a careful balance between
exploration and exploitation. In this work, we consider online robust MDP by
interacting with an unknown nominal system. We propose a robust optimistic
policy optimization algorithm that is provably efficient. To address the
additional uncertainty caused by an adversarial environment, our model features
a new optimistic update rule derived via Fenchel conjugates. Our analysis
establishes the first regret bound for online robust MDPs.",2209.13841v1,https://arxiv.org/pdf/2209.13841v1
mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark,"Vitor Jeronymo, Mauricio Nascimento, Roberto Lotufo, Rodrigo Nogueira","Robust 2004 is an information retrieval benchmark whose large number of
judgments per query make it a reliable evaluation dataset. In this paper, we
present mRobust04, a multilingual version of Robust04 that was translated to 8
languages using Google Translate. We also provide results of three different
multilingual retrievers on this dataset. The dataset is available at
https://huggingface.co/datasets/unicamp-dl/mrobust",2209.13738v1,https://arxiv.org/pdf/2209.13738v1
"Reconstruction-guided attention improves the robustness and shape
  processing of neural networks","Seoyoung Ahn, Hossein Adeli, Gregory J. Zelinsky","Many visual phenomena suggest that humans use top-down generative or
reconstructive processes to create visual percepts (e.g., imagery, object
completion, pareidolia), but little is known about the role reconstruction
plays in robust object recognition. We built an iterative encoder-decoder
network that generates an object reconstruction and used it as top-down
attentional feedback to route the most relevant spatial and feature information
to feed-forward object recognition processes. We tested this model using the
challenging out-of-distribution digit recognition dataset, MNIST-C, where 15
different types of transformation and corruption are applied to handwritten
digit images. Our model showed strong generalization performance against
various image perturbations, on average outperforming all other models
including feedforward CNNs and adversarially trained networks. Our model is
particularly robust to blur, noise, and occlusion corruptions, where shape
perception plays an important role. Ablation studies further reveal two
complementary roles of spatial and feature-based attention in robust object
recognition, with the former largely consistent with spatial masking benefits
in the attention literature (the reconstruction serves as a mask) and the
latter mainly contributing to the model's inference speed (i.e., number of time
steps to reach a certain confidence threshold) by reducing the space of
possible object hypotheses. We also observed that the model sometimes
hallucinates a non-existing pattern out of noise, leading to highly
interpretable human-like errors. Our study shows that modeling
reconstruction-based feedback endows AI systems with a powerful attention
mechanism, which can help us understand the role of generating perception in
human visual processing.",2209.13620v2,https://arxiv.org/pdf/2209.13620v2
"DAMO-NLP at NLPCC-2022 Task 2: Knowledge Enhanced Robust NER for Speech
  Entity Linking","Shen Huang, Yuchen Zhai, Xinwei Long, Yong Jiang, Xiaobin Wang, Yin Zhang, Pengjun Xie","Speech Entity Linking aims to recognize and disambiguate named entities in
spoken languages. Conventional methods suffer gravely from the unfettered
speech styles and the noisy transcripts generated by ASR systems. In this
paper, we propose a novel approach called Knowledge Enhanced Named Entity
Recognition (KENER), which focuses on improving robustness through painlessly
incorporating proper knowledge in the entity recognition stage and thus
improving the overall performance of entity linking. KENER first retrieves
candidate entities for a sentence without mentions, and then utilizes the
entity descriptions as extra information to help recognize mentions. The
candidate entities retrieved by a dense retrieval module are especially useful
when the input is short or noisy. Moreover, we investigate various data
sampling strategies and design effective loss functions, in order to improve
the quality of retrieved entities in both recognition and disambiguation
stages. Lastly, a linking with filtering module is applied as the final
safeguard, making it possible to filter out wrongly-recognized mentions. Our
system achieves 1st place in Track 1 and 2nd place in Track 2 of NLPCC-2022
Shared Task 2.",2209.13187v2,https://arxiv.org/pdf/2209.13187v2
"Learning and Deploying Robust Locomotion Policies with Minimal Dynamics
  Randomization","Luigi Campanaro, Siddhant Gangapurwala, Wolfgang Merkt, Ioannis Havoutis","Training deep reinforcement learning (DRL) locomotion policies often require
massive amounts of data to converge to the desired behaviour. In this regard,
simulators provide a cheap and abundant source. For successful sim-to-real
transfer, exhaustively engineered approaches such as system identification,
dynamics randomization, and domain adaptation are generally employed. As an
alternative, we investigate a simple strategy of random force injection (RFI)
to perturb system dynamics during training. We show that the application of
random forces enables us to emulate dynamics randomization. This allows us to
obtain locomotion policies that are robust to variations in system dynamics. We
further extend RFI, referred to as extended random force injection (ERFI), by
introducing an episodic actuation offset. We demonstrate that ERFI provides
additional robustness for variations in system mass offering on average a 53%
improved performance over RFI. We also show that ERFI is sufficient to perform
a successful sim-to-real transfer on two different quadrupedal platforms,
ANYmal C and Unitree A1, even for perceptive locomotion over uneven terrain in
outdoor environments.",2209.12878v2,https://arxiv.org/pdf/2209.12878v2
"DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras
  and Radars","Florian Drews, Di Feng, Florian Faion, Lars Rosenbaum, Michael Ulrich, Claudius Gläser","We propose DeepFusion, a modular multi-modal architecture to fuse lidars,
cameras and radars in different combinations for 3D object detection.
Specialized feature extractors take advantage of each modality and can be
exchanged easily, making the approach simple and flexible. Extracted features
are transformed into bird's-eye-view as a common representation for fusion.
Spatial and semantic alignment is performed prior to fusing modalities in the
feature space. Finally, a detection head exploits rich multi-modal features for
improved 3D detection performance. Experimental results for lidar-camera,
lidar-camera-radar and camera-radar fusion show the flexibility and
effectiveness of our fusion approach. In the process, we study the largely
unexplored task of faraway car detection up to 225 meters, showing the benefits
of our lidar-camera fusion. Furthermore, we investigate the required density of
lidar points for 3D object detection and illustrate implications at the example
of robustness against adverse weather conditions. Moreover, ablation studies on
our camera-radar fusion highlight the importance of accurate depth estimation.",2209.12729v2,https://arxiv.org/pdf/2209.12729v2
"The ""Beatrix'' Resurrections: Robust Backdoor Detection via Gram
  Matrices","Wanlun Ma, Derui Wang, Ruoxi Sun, Minhui Xue, Sheng Wen, Yang Xiang","Deep Neural Networks (DNNs) are susceptible to backdoor attacks during
training. The model corrupted in this way functions normally, but when
triggered by certain patterns in the input, produces a predefined target label.
Existing defenses usually rely on the assumption of the universal backdoor
setting in which poisoned samples share the same uniform trigger. However,
recent advanced backdoor attacks show that this assumption is no longer valid
in dynamic backdoors where the triggers vary from input to input, thereby
defeating the existing defenses.
  In this work, we propose a novel technique, Beatrix (backdoor detection via
Gram matrix). Beatrix utilizes Gram matrix to capture not only the feature
correlations but also the appropriately high-order information of the
representations. By learning class-conditional statistics from activation
patterns of normal samples, Beatrix can identify poisoned samples by capturing
the anomalies in activation patterns. To further improve the performance in
identifying target labels, Beatrix leverages kernel-based testing without
making any prior assumptions on representation distribution. We demonstrate the
effectiveness of our method through extensive evaluation and comparison with
state-of-the-art defensive techniques. The experimental results show that our
approach achieves an F1 score of 91.1% in detecting dynamic backdoors, while
the state of the art can only reach 36.9%.",2209.11715v3,https://arxiv.org/pdf/2209.11715v3
Robust Domain Adaptation for Machine Reading Comprehension,"Liang Jiang, Zhenyu Huang, Jia Liu, Zujie Wen, Xi Peng","Most domain adaptation methods for machine reading comprehension (MRC) use a
pre-trained question-answer (QA) construction model to generate pseudo QA pairs
for MRC transfer. Such a process will inevitably introduce mismatched pairs
(i.e., noisy correspondence) due to i) the unavailable QA pairs in target
documents, and ii) the domain shift during applying the QA construction model
to the target domain. Undoubtedly, the noisy correspondence will degenerate the
performance of MRC, which however is neglected by existing works. To solve such
an untouched problem, we propose to construct QA pairs by additionally using
the dialogue related to the documents, as well as a new domain adaptation
method for MRC. Specifically, we propose Robust Domain Adaptation for Machine
Reading Comprehension (RMRC) method which consists of an answer extractor (AE),
a question selector (QS), and an MRC model. Specifically, RMRC filters out the
irrelevant answers by estimating the correlation to the document via the AE,
and extracts the questions by fusing the candidate questions in multiple rounds
of dialogue chats via the QS. With the extracted QA pairs, MRC is fine-tuned
and provides the feedback to optimize the QS through a novel reinforced
self-training method. Thanks to the optimization of the QS, our method will
greatly alleviate the noisy correspondence problem caused by the domain shift.
To the best of our knowledge, this could be the first study to reveal the
influence of noisy correspondence in domain adaptation MRC models and show a
feasible way to achieve robustness to mismatched pairs. Extensive experiments
on three datasets demonstrate the effectiveness of our method.",2209.11615v1,https://arxiv.org/pdf/2209.11615v1
"Quantification before Selection: Active Dynamics Preference for Robust
  Reinforcement Learning","Kang Xu, Yan Ma, Wei Li","Training a robust policy is critical for policy deployment in real-world
systems or dealing with unknown dynamics mismatch in different dynamic systems.
Domain Randomization~(DR) is a simple and elegant approach that trains a
conservative policy to counter different dynamic systems without expert
knowledge about the target system parameters. However, existing works reveal
that the policy trained through DR tends to be over-conservative and performs
poorly in target domains. Our key insight is that dynamic systems with
different parameters provide different levels of difficulty for the policy, and
the difficulty of behaving well in a system is constantly changing due to the
evolution of the policy. If we can actively sample the systems with proper
difficulty for the policy on the fly, it will stabilize the training process
and prevent the policy from becoming over-conservative or over-optimistic. To
operationalize this idea, we introduce Active Dynamics Preference~(ADP), which
quantifies the informativeness and density of sampled system parameters. ADP
actively selects system parameters with high informativeness and low density.
We validate our approach in four robotic locomotion tasks with various
discrepancies between the training and testing environments. Extensive results
demonstrate that our approach has superior robustness for system inconsistency
compared to several baselines.",2209.11596v3,https://arxiv.org/pdf/2209.11596v3
"MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust
  Classifier","Mozhdeh Rouhsedaghat, Masoud Monajatipoor, C. -C. Jay Kuo, Iacopo Masi","We offer a method for one-shot mask-guided image synthesis that allows
controlling manipulations of a single image by inverting a quasi-robust
classifier equipped with strong regularizers. Our proposed method, entitled
MAGIC, leverages structured gradients from a pre-trained quasi-robust
classifier to better preserve the input semantics while preserving its
classification accuracy, thereby guaranteeing credibility in the synthesis.
Unlike current methods that use complex primitives to supervise the process or
use attention maps as a weak supervisory signal, MAGIC aggregates gradients
over the input, driven by a guide binary mask that enforces a strong, spatial
prior. MAGIC implements a series of manipulations with a single framework
achieving shape and location control, intense non-rigid shape deformations, and
copy/move operations in the presence of repeating objects and gives users firm
control over the synthesis by requiring to simply specify binary guide masks.
Our study and findings are supported by various qualitative comparisons with
the state-of-the-art on the same images sampled from ImageNet and quantitative
analysis using machine perception along with a user survey of 100+ participants
that endorse our synthesis quality. Project page at
https://mozhdehrouhsedaghat.github.io/magic.html. Code is available at
https://github.com/mozhdehrouhsedaghat/magic",2209.11549v3,https://arxiv.org/pdf/2209.11549v3
"A Robust and Explainable Data-Driven Anomaly Detection Approach For
  Power Electronics","Alexander Beattie, Pavol Mulinka, Subham Sahoo, Ioannis T. Christou, Charalampos Kalalas, Daniel Gutierrez-Rojas, Pedro H. J. Nardelli","Timely and accurate detection of anomalies in power electronics is becoming
increasingly critical for maintaining complex production systems. Robust and
explainable strategies help decrease system downtime and preempt or mitigate
infrastructure cyberattacks. This work begins by explaining the types of
uncertainty present in current datasets and machine learning algorithm outputs.
Three techniques for combating these uncertainties are then introduced and
analyzed. We further present two anomaly detection and classification
approaches, namely the Matrix Profile algorithm and anomaly transformer, which
are applied in the context of a power electronic converter dataset.
Specifically, the Matrix Profile algorithm is shown to be well suited as a
generalizable approach for detecting real-time anomalies in streaming
time-series data. The STUMPY python library implementation of the iterative
Matrix Profile is used for the creation of the detector. A series of custom
filters is created and added to the detector to tune its sensitivity, recall,
and detection accuracy. Our numerical results show that, with simple parameter
tuning, the detector provides high accuracy and performance in a variety of
fault scenarios.",2209.11427v1,https://arxiv.org/pdf/2209.11427v1
"Evaluating Latent Space Robustness and Uncertainty of EEG-ML Models
  under Realistic Distribution Shifts","Neeraj Wagh, Jionghao Wei, Samarth Rawal, Brent M. Berry, Yogatheesan Varatharajah","The recent availability of large datasets in bio-medicine has inspired the
development of representation learning methods for multiple healthcare
applications. Despite advances in predictive performance, the clinical utility
of such methods is limited when exposed to real-world data. This study develops
model diagnostic measures to detect potential pitfalls before deployment
without assuming access to external data. Specifically, we focus on modeling
realistic data shifts in electrophysiological signals (EEGs) via data
transforms and extend the conventional task-based evaluations with analyses of
a) the model's latent space and b) predictive uncertainty under these
transforms. We conduct experiments on multiple EEG feature encoders and two
clinically relevant downstream tasks using publicly available large-scale
clinical EEGs. Within this experimental setting, our results suggest that
measures of latent space integrity and model uncertainty under the proposed
data shifts may help anticipate performance degradation during deployment.",2209.11233v2,https://arxiv.org/pdf/2209.11233v2
"A Closer Look at Learned Optimization: Stability, Robustness, and
  Inductive Biases","James Harrison, Luke Metz, Jascha Sohl-Dickstein","Learned optimizers -- neural networks that are trained to act as optimizers
-- have the potential to dramatically accelerate training of machine learning
models. However, even when meta-trained across thousands of tasks at huge
computational expense, blackbox learned optimizers often struggle with
stability and generalization when applied to tasks unlike those in their
meta-training set. In this paper, we use tools from dynamical systems to
investigate the inductive biases and stability properties of optimization
algorithms, and apply the resulting insights to designing inductive biases for
blackbox optimizers. Our investigation begins with a noisy quadratic model,
where we characterize conditions in which optimization is stable, in terms of
eigenvalues of the training dynamics. We then introduce simple modifications to
a learned optimizer's architecture and meta-training procedure which lead to
improved stability, and improve the optimizer's inductive bias. We apply the
resulting learned optimizer to a variety of neural network training tasks,
where it outperforms the current state of the art learned optimizer -- at
matched optimizer computational overhead -- with regard to optimization
performance and meta-training speed, and is capable of generalization to tasks
far different from those it was meta-trained on.",2209.11208v1,https://arxiv.org/pdf/2209.11208v1
Robust Collaborative Learning with Linear Gradient Overhead,"Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Lê Nguyên Hoang, Rafael Pinot, John Stephan","Collaborative learning algorithms, such as distributed SGD (or D-SGD), are
prone to faulty machines that may deviate from their prescribed algorithm
because of software or hardware bugs, poisoned data or malicious behaviors.
While many solutions have been proposed to enhance the robustness of D-SGD to
such machines, previous works either resort to strong assumptions (trusted
server, homogeneous data, specific noise model) or impose a gradient
computational cost that is several orders of magnitude higher than that of
D-SGD. We present MoNNA, a new algorithm that (a) is provably robust under
standard assumptions and (b) has a gradient computation overhead that is linear
in the fraction of faulty machines, which is conjectured to be tight.
Essentially, MoNNA uses Polyak's momentum of local gradients for local updates
and nearest-neighbor averaging (NNA) for global mixing, respectively. While
MoNNA is rather simple to implement, its analysis has been more challenging and
relies on two key elements that may be of independent interest. Specifically,
we introduce the mixing criterion of $(\alpha, \lambda)$-reduction to analyze
the non-linear mixing of non-faulty machines, and present a way to control the
tension between the momentum and the model drifts. We validate our theory by
experiments on image classification and make our code available at
https://github.com/LPD-EPFL/robust-collaborative-learning.",2209.10931v2,https://arxiv.org/pdf/2209.10931v2
IntereStyle: Encoding an Interest Region for Robust StyleGAN Inversion,"Seungjun Moon, Gyeong-Moon Park","Recently, manipulation of real-world images has been highly elaborated along
with the development of Generative Adversarial Networks (GANs) and
corresponding encoders, which embed real-world images into the latent space.
However, designing encoders of GAN still remains a challenging task due to the
trade-off between distortion and perception. In this paper, we point out that
the existing encoders try to lower the distortion not only on the interest
region, e.g., human facial region but also on the uninterest region, e.g.,
background patterns and obstacles. However, most uninterest regions in
real-world images are located at out-of-distribution (OOD), which are
infeasible to be ideally reconstructed by generative models. Moreover, we
empirically find that the uninterest region overlapped with the interest region
can mangle the original feature of the interest region, e.g., a microphone
overlapped with a facial region is inverted into the white beard. As a result,
lowering the distortion of the whole image while maintaining the perceptual
quality is very challenging. To overcome this trade-off, we propose a simple
yet effective encoder training scheme, coined IntereStyle, which facilitates
encoding by focusing on the interest region. IntereStyle steers the encoder to
disentangle the encodings of the interest and uninterest regions. To this end,
we filter the information of the uninterest region iteratively to regulate the
negative impact of the uninterest region. We demonstrate that IntereStyle
achieves both lower distortion and higher perceptual quality compared to the
existing state-of-the-art encoders. Especially, our model robustly conserves
features of the original images, which shows the robust image editing and style
mixing results. We will release our code with the pre-trained model after the
review.",2209.10811v2,https://arxiv.org/pdf/2209.10811v2
Robust Forecasting for Robotic Control: A Game-Theoretic Approach,"Shubhankar Agarwal, David Fridovich-Keil, Sandeep P. Chinchali","Modern robots require accurate forecasts to make optimal decisions in the
real world. For example, self-driving cars need an accurate forecast of other
agents' future actions to plan safe trajectories. Current methods rely heavily
on historical time series to accurately predict the future. However, relying
entirely on the observed history is problematic since it could be corrupted by
noise, have outliers, or not completely represent all possible outcomes. To
solve this problem, we propose a novel framework for generating robust
forecasts for robotic control. In order to model real-world factors affecting
future forecasts, we introduce the notion of an adversary, which perturbs
observed historical time series to increase a robot's ultimate control cost.
Specifically, we model this interaction as a zero-sum two-player game between a
robot's forecaster and this hypothetical adversary. We show that our proposed
game may be solved to a local Nash equilibrium using gradient-based
optimization techniques. Furthermore, we show that a forecaster trained with
our method performs 30.14% better on out-of-distribution real-world lane change
data than baselines.",2209.10802v3,https://arxiv.org/pdf/2209.10802v3
Fair Robust Active Learning by Joint Inconsistency,"Tsung-Han Wu, Hung-Ting Su, Shang-Tse Chen, Winston H. Hsu","Fairness and robustness play vital roles in trustworthy machine learning.
Observing safety-critical needs in various annotation-expensive vision
applications, we introduce a novel learning framework, Fair Robust Active
Learning (FRAL), generalizing conventional active learning to fair and
adversarial robust scenarios. This framework allows us to achieve standard and
robust minimax fairness with limited acquired labels. In FRAL, we then observe
existing fairness-aware data selection strategies suffer from either
ineffectiveness under severe data imbalance or inefficiency due to huge
computations of adversarial training. To address these two problems, we develop
a novel Joint INconsistency (JIN) method exploiting prediction inconsistencies
between benign and adversarial inputs as well as between standard and robust
models. These two inconsistencies can be used to identify potential fairness
gains and data imbalance mitigations. Thus, by performing label acquisition
with our inconsistency-based ranking metrics, we can alleviate the class
imbalance issue and enhance minimax fairness with limited computation.
Extensive experiments on diverse datasets and sensitive groups demonstrate that
our method obtains the best results in standard and robust fairness under
white-box PGD attacks compared with existing active data selection baselines.",2209.10729v2,https://arxiv.org/pdf/2209.10729v2
First-order Policy Optimization for Robust Markov Decision Process,"Yan Li, Guanghui Lan, Tuo Zhao","We consider the problem of solving robust Markov decision process (MDP),
which involves a set of discounted, finite state, finite action space MDPs with
uncertain transition kernels. The goal of planning is to find a robust policy
that optimizes the worst-case values against the transition uncertainties, and
thus encompasses the standard MDP planning as a special case. For
$(\mathbf{s},\mathbf{a})$-rectangular uncertainty sets, we establish several
structural observations on the robust objective, which facilitates the
development of a policy-based first-order method, namely the robust policy
mirror descent (RPMD). An $\mathcal{O}(\log(1/\epsilon))$ iteration complexity
for finding an $\epsilon$-optimal policy is established with linearly
increasing stepsizes. We further develop a stochastic variant of the robust
policy mirror descent method, named SRPMD, when the first-order information is
only available through online interactions with the nominal environment. We
show that the optimality gap converges linearly up to the noise level, and
consequently establish an $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity
by developing a temporal difference learning method for policy evaluation. Both
iteration and sample complexities are also discussed for RPMD with a constant
stepsize. To the best of our knowledge, all the aforementioned results appear
to be new for policy-based first-order methods applied to the robust MDP
problem.",2209.10579v2,https://arxiv.org/pdf/2209.10579v2
"Robust Information Bottleneck for Task-Oriented Communication with
  Digital Modulation","Songjie Xie, Shuai Ma, Ming Ding, Yuanming Shi, Mingjian Tang, Youlong Wu","Task-oriented communications, mostly using learning-based joint
source-channel coding (JSCC), aim to design a communication-efficient edge
inference system by transmitting task-relevant information to the receiver.
However, only transmitting task-relevant information without introducing any
redundancy may cause robustness issues in learning due to the channel
variations, and the JSCC which directly maps the source data into continuous
channel input symbols poses compatibility issues on existing digital
communication systems. In this paper, we address these two issues by first
investigating the inherent tradeoff between the informativeness of the encoded
representations and the robustness to information distortion in the received
representations, and then propose a task-oriented communication scheme with
digital modulation, named discrete task-oriented JSCC (DT-JSCC), where the
transmitter encodes the features into a discrete representation and transmits
it to the receiver with the digital modulation scheme. In the DT-JSCC scheme,
we develop a robust encoding framework, named robust information bottleneck
(RIB), to improve the communication robustness to the channel variations, and
derive a tractable variational upper bound of the RIB objective function using
the variational approximation to overcome the computational intractability of
mutual information. The experimental results demonstrate that the proposed
DT-JSCC achieves better inference performance than the baseline methods with
low communication latency, and exhibits robustness to channel variations due to
the applied RIB framework.",2209.10382v2,https://arxiv.org/pdf/2209.10382v2
"DARTSRepair: Core-failure-set Guided DARTS for Network Robustness to
  Common Corruptions","Xuhong Ren, Jianlang Chen, Felix Juefei-Xu, Wanli Xue, Qing Guo, Lei Ma, Jianjun Zhao, Shengyong Chen","Network architecture search (NAS), in particular the differentiable
architecture search (DARTS) method, has shown a great power to learn excellent
model architectures on the specific dataset of interest. In contrast to using a
fixed dataset, in this work, we focus on a different but important scenario for
NAS: how to refine a deployed network's model architecture to enhance its
robustness with the guidance of a few collected and misclassified examples that
are degraded by some real-world unknown corruptions having a specific pattern
(e.g., noise, blur, etc.). To this end, we first conduct an empirical study to
validate that the model architectures can be definitely related to the
corruption patterns. Surprisingly, by just adding a few corrupted and
misclassified examples (e.g., $10^3$ examples) to the clean training dataset
(e.g., $5.0 \times 10^4$ examples), we can refine the model architecture and
enhance the robustness significantly. To make it more practical, the key
problem, i.e., how to select the proper failure examples for the effective NAS
guidance, should be carefully investigated. Then, we propose a novel
core-failure-set guided DARTS that embeds a K-center-greedy algorithm for DARTS
to select suitable corrupted failure examples to refine the model architecture.
We use our method for DARTS-refined DNNs on the clean as well as 15 corruptions
with the guidance of four specific real-world corruptions. Compared with the
state-of-the-art NAS as well as data-augmentation-based enhancement methods,
our final method can achieve higher accuracy on both corrupted datasets and the
original clean dataset. On some of the corruption patterns, we can achieve as
high as over 45% absolute accuracy improvements.",2209.10381v1,https://arxiv.org/pdf/2209.10381v1
Analyzing Robustness of Angluin's L* Algorithm in Presence of Noise,"Igor Khmelnitsky, Serge Haddad, Lina Ye, Benoît Barbot, Benedikt Bollig, Martin Leucker, Daniel Neider, Rajarshi Roy","Angluin's L* algorithm learns the minimal (complete) deterministic finite
automaton (DFA) of a regular language using membership and equivalence queries.
Its probabilistic approximatively correct (PAC) version substitutes an
equivalence query by a large enough set of random membership queries to get a
high level confidence to the answer. Thus it can be applied to any kind of
(also non-regular) device and may be viewed as an algorithm for synthesizing an
automaton abstracting the behavior of the device based on observations. Here we
are interested on how Angluin's PAC learning algorithm behaves for devices
which are obtained from a DFA by introducing some noise. More precisely we
study whether Angluin's algorithm reduces the noise and produces a DFA closer
to the original one than the noisy device. We propose several ways to introduce
the noise: (1) the noisy device inverts the classification of words w.r.t. the
DFA with a small probability, (2) the noisy device modifies with a small
probability the letters of the word before asking its classification w.r.t. the
DFA, and (3) the noisy device combines the classification of a word w.r.t. the
DFA and its classification w.r.t. a counter automaton. Our experiments were
performed on several hundred DFAs.
  Our main contributions, bluntly stated, consist in showing that: (1)
Angluin's algorithm behaves well whenever the noisy device is produced by a
random process, (2) but poorly with a structured noise, and, that (3) almost
surely randomness yields systems with non-recursively enumerable languages.",2209.10315v1,https://arxiv.org/pdf/2209.10315v1
On the convex formulations of robust Markov decision processes,"Julien Grand-Clément, Marek Petrik","Robust Markov decision processes (MDPs) are used for applications of dynamic
optimization in uncertain environments and have been studied extensively. Many
of the main properties and algorithms of MDPs, such as value iteration and
policy iteration, extend directly to RMDPs. Surprisingly, there is no known
analog of the MDP convex optimization formulation for solving RMDPs. This work
describes the first convex optimization formulation of RMDPs under the
classical sa-rectangularity and s-rectangularity assumptions. By using entropic
regularization and exponential change of variables, we derive a convex
formulation with a number of variables and constraints polynomial in the number
of states and actions, but with large coefficients in the constraints. We
further simplify the formulation for RMDPs with polyhedral, ellipsoidal, or
entropy-based uncertainty sets, showing that, in these cases, RMDPs can be
reformulated as conic programs based on exponential cones, quadratic cones, and
non-negative orthants. Our work opens a new research direction for RMDPs and
can serve as a first step toward obtaining a tractable convex formulation of
RMDPs.",2209.10187v2,https://arxiv.org/pdf/2209.10187v2
"Robust, High-Rate Trajectory Tracking on Insect-Scale Soft-Actuated
  Aerial Robots with Deep-Learned Tube MPC","Andrea Tagliabue, Yi-Hsuan Hsiao, Urban Fasel, J. Nathan Kutz, Steven L. Brunton, YuFeng Chen, Jonathan P. How","Accurate and agile trajectory tracking in sub-gram Micro Aerial Vehicles
(MAVs) is challenging, as the small scale of the robot induces large model
uncertainties, demanding robust feedback controllers, while the fast dynamics
and computational constraints prevent the deployment of computationally
expensive strategies. In this work, we present an approach for agile and
computationally efficient trajectory tracking on the MIT SoftFly, a sub-gram
MAV (0.7 grams). Our strategy employs a cascaded control scheme, where an
adaptive attitude controller is combined with a neural network policy trained
to imitate a trajectory tracking robust tube model predictive controller
(RTMPC). The neural network policy is obtained using our recent work, which
enables the policy to preserve the robustness of RTMPC, but at a fraction of
its computational cost. We experimentally evaluate our approach, achieving
position Root Mean Square Errors lower than 1.8 cm even in the more challenging
maneuvers, obtaining a 60% reduction in maximum position error compared to our
previous work, and demonstrating robustness to large external disturbances",2209.10007v2,https://arxiv.org/pdf/2209.10007v2
"Audit and Improve Robustness of Private Neural Networks on Encrypted
  Data","Jiaqi Xue, Lei Xu, Lin Chen, Weidong Shi, Kaidi Xu, Qian Lou","Performing neural network inference on encrypted data without decryption is
one popular method to enable privacy-preserving neural networks (PNet) as a
service. Compared with regular neural networks deployed for
machine-learning-as-a-service, PNet requires additional encoding, e.g.,
quantized-precision numbers, and polynomial activation. Encrypted input also
introduces novel challenges such as adversarial robustness and security. To the
best of our knowledge, we are the first to study questions including (i)
Whether PNet is more robust against adversarial inputs than regular neural
networks? (ii) How to design a robust PNet given the encrypted input without
decryption? We propose PNet-Attack to generate black-box adversarial examples
that can successfully attack PNet in both target and untarget manners. The
attack results show that PNet robustness against adversarial inputs needs to be
improved. This is not a trivial task because the PNet model owner does not have
access to the plaintext of the input values, which prevents the application of
existing detection and defense methods such as input tuning, model
normalization, and adversarial training. To tackle this challenge, we propose a
new fast and accurate noise insertion method, called RPNet, to design Robust
and Private Neural Networks. Our comprehensive experiments show that
PNet-Attack reduces at least $2.5\times$ queries than prior works. We
theoretically analyze our RPNet methods and demonstrate that RPNet can decrease
$\sim 91.88\%$ attack success rate.",2209.09996v1,https://arxiv.org/pdf/2209.09996v1
Soft Action Priors: Towards Robust Policy Transfer,"Matheus Centa, Philippe Preux","Despite success in many challenging problems, reinforcement learning (RL) is
still confronted with sample inefficiency, which can be mitigated by
introducing prior knowledge to agents. However, many transfer techniques in
reinforcement learning make the limiting assumption that the teacher is an
expert. In this paper, we use the action prior from the Reinforcement Learning
as Inference framework - that is, a distribution over actions at each state
which resembles a teacher policy, rather than a Bayesian prior - to recover
state-of-the-art policy distillation techniques. Then, we propose a class of
adaptive methods that can robustly exploit action priors by combining reward
shaping and auxiliary regularization losses. In contrast to prior work, we
develop algorithms for leveraging suboptimal action priors that may
nevertheless impart valuable knowledge - which we call soft action priors. The
proposed algorithms adapt by adjusting the strength of teacher feedback
according to an estimate of the teacher's usefulness in each state. We perform
tabular experiments, which show that the proposed methods achieve
state-of-the-art performance, surpassing it when learning from suboptimal
priors. Finally, we demonstrate the robustness of the adaptive algorithms in
continuous action deep RL problems, in which adaptive algorithms considerably
improved stability when compared to existing policy distillation methods.",2209.09882v1,https://arxiv.org/pdf/2209.09882v1
Fairness and robustness in anti-causal prediction,"Maggie Makar, Alexander D'Amour","Robustness to distribution shift and fairness have independently emerged as
two important desiderata required of modern machine learning models. While
these two desiderata seem related, the connection between them is often unclear
in practice. Here, we discuss these connections through a causal lens, focusing
on anti-causal prediction tasks, where the input to a classifier (e.g., an
image) is assumed to be generated as a function of the target label and the
protected attribute. By taking this perspective, we draw explicit connections
between a common fairness criterion - separation - and a common notion of
robustness - risk invariance. These connections provide new motivation for
applying the separation criterion in anticausal settings, and inform old
discussions regarding fairness-performance tradeoffs. In addition, our findings
suggest that robustness-motivated approaches can be used to enforce separation,
and that they often work better in practice than methods designed to directly
enforce separation. Using a medical dataset, we empirically validate our
findings on the task of detecting pneumonia from X-rays, in a setting where
differences in prevalence across sex groups motivates a fairness mitigation.
Our findings highlight the importance of considering causal structure when
choosing and enforcing fairness criteria.",2209.09423v2,https://arxiv.org/pdf/2209.09423v2
"State-driven Implicit Modeling for Sparsity and Robustness in Neural
  Networks","Alicia Y. Tsai, Juliette Decugis, Laurent El Ghaoui, Alper Atamtürk","Implicit models are a general class of learning models that forgo the
hierarchical layer structure typical in neural networks and instead define the
internal states based on an ``equilibrium'' equation, offering competitive
performance and reduced memory consumption. However, training such models
usually relies on expensive implicit differentiation for backward propagation.
In this work, we present a new approach to training implicit models, called
State-driven Implicit Modeling (SIM), where we constrain the internal states
and outputs to match that of a baseline model, circumventing costly backward
computations. The training problem becomes convex by construction and can be
solved in a parallel fashion, thanks to its decomposable structure. We
demonstrate how the SIM approach can be applied to significantly improve
sparsity (parameter reduction) and robustness of baseline models trained on
FashionMNIST and CIFAR-100 datasets.",2209.09389v1,https://arxiv.org/pdf/2209.09389v1
"RAMP-Net: A Robust Adaptive MPC for Quadrotors via Physics-informed
  Neural Network","Sourav Sanyal, Kaushik Roy","Model Predictive Control (MPC) is a state-of-the-art (SOTA) control technique
which requires solving hard constrained optimization problems iteratively. For
uncertain dynamics, analytical model based robust MPC imposes additional
constraints, increasing the hardness of the problem. The problem exacerbates in
performance-critical applications, when more compute is required in lesser
time. Data-driven regression methods such as Neural Networks have been proposed
in the past to approximate system dynamics. However, such models rely on high
volumes of labeled data, in the absence of symbolic analytical priors. This
incurs non-trivial training overheads. Physics-informed Neural Networks (PINNs)
have gained traction for approximating non-linear system of ordinary
differential equations (ODEs), with reasonable accuracy. In this work, we
propose a Robust Adaptive MPC framework via PINNs (RAMP-Net), which uses a
neural network trained partly from simple ODEs and partly from data. A physics
loss is used to learn simple ODEs representing ideal dynamics. Having access to
analytical functions inside the loss function acts as a regularizer, enforcing
robust behavior for parametric uncertainties. On the other hand, a regular data
loss is used for adapting to residual disturbances (non-parametric
uncertainties), unaccounted during mathematical modelling. Experiments are
performed in a simulated environment for trajectory tracking of a quadrotor. We
report 7.8% to 43.2% and 8.04% to 61.5% reduction in tracking errors for speeds
ranging from 0.5 to 1.75 m/s compared to two SOTA regression based MPC methods.",2209.09025v3,https://arxiv.org/pdf/2209.09025v3
Measuring Interventional Robustness in Reinforcement Learning,"Katherine Avery, Jack Kenney, Pracheta Amaranath, Erica Cai, David Jensen","Recent work in reinforcement learning has focused on several characteristics
of learned policies that go beyond maximizing reward. These properties include
fairness, explainability, generalization, and robustness. In this paper, we
define interventional robustness (IR), a measure of how much variability is
introduced into learned policies by incidental aspects of the training
procedure, such as the order of training data or the particular exploratory
actions taken by agents. A training procedure has high IR when the agents it
produces take very similar actions under intervention, despite variation in
these incidental aspects of the training procedure. We develop an intuitive,
quantitative measure of IR and calculate it for eight algorithms in three Atari
environments across dozens of interventions and states. From these experiments,
we find that IR varies with the amount of training and type of algorithm and
that high performance does not imply high IR, as one might expect.",2209.09058v1,https://arxiv.org/pdf/2209.09058v1
"Adversarial Catoptric Light: An Effective, Stealthy and Robust
  Physical-World Attack to DNNs","Chengyin Hu, Weiwen Shi","Deep neural networks (DNNs) have demonstrated exceptional success across
various tasks, underscoring the need to evaluate the robustness of advanced
DNNs. However, traditional methods using stickers as physical perturbations to
deceive classifiers present challenges in achieving stealthiness and suffer
from printing loss. Recent advancements in physical attacks have utilized light
beams such as lasers and projectors to perform attacks, where the optical
patterns generated are artificial rather than natural. In this study, we
introduce a novel physical attack, adversarial catoptric light (AdvCL), where
adversarial perturbations are generated using a common natural phenomenon,
catoptric light, to achieve stealthy and naturalistic adversarial attacks
against advanced DNNs in a black-box setting. We evaluate the proposed method
in three aspects: effectiveness, stealthiness, and robustness. Quantitative
results obtained in simulated environments demonstrate the effectiveness of the
proposed method, and in physical scenarios, we achieve an attack success rate
of 83.5%, surpassing the baseline. We use common catoptric light as a
perturbation to enhance the stealthiness of the method and make physical
samples appear more natural. Robustness is validated by successfully attacking
advanced and robust DNNs with a success rate over 80% in all cases.
Additionally, we discuss defense strategy against AdvCL and put forward some
light-based physical attacks.",2209.11739v2,https://arxiv.org/pdf/2209.11739v2
Importance Tempering: Group Robustness for Overparameterized Models,"Yiping Lu, Wenlong Ji, Zachary Izzo, Lexing Ying","Although overparameterized models have shown their success on many machine
learning tasks, the accuracy could drop on the testing distribution that is
different from the training one. This accuracy drop still limits applying
machine learning in the wild. At the same time, importance weighting, a
traditional technique to handle distribution shifts, has been demonstrated to
have less or even no effect on overparameterized models both empirically and
theoretically. In this paper, we propose importance tempering to improve the
decision boundary and achieve consistently better results for overparameterized
models. Theoretically, we justify that the selection of group temperature can
be different under label shift and spurious correlation setting. At the same
time, we also prove that properly selected temperatures can extricate the
minority collapse for imbalanced classification. Empirically, we achieve
state-of-the-art results on worst group classification tasks using importance
tempering.",2209.08745v2,https://arxiv.org/pdf/2209.08745v2
Towards Robust Off-Policy Evaluation via Human Inputs,"Harvineet Singh, Shalmali Joshi, Finale Doshi-Velez, Himabindu Lakkaraju","Off-policy Evaluation (OPE) methods are crucial tools for evaluating policies
in high-stakes domains such as healthcare, where direct deployment is often
infeasible, unethical, or expensive. When deployment environments are expected
to undergo changes (that is, dataset shifts), it is important for OPE methods
to perform robust evaluation of the policies amidst such changes. Existing
approaches consider robustness against a large class of shifts that can
arbitrarily change any observable property of the environment. This often
results in highly pessimistic estimates of the utilities, thereby invalidating
policies that might have been useful in deployment. In this work, we address
the aforementioned problem by investigating how domain knowledge can help
provide more realistic estimates of the utilities of policies. We leverage
human inputs on which aspects of the environments may plausibly change, and
adapt the OPE methods to only consider shifts on these aspects. Specifically,
we propose a novel framework, Robust OPE (ROPE), which considers shifts on a
subset of covariates in the data based on user inputs, and estimates worst-case
utility under these shifts. We then develop computationally efficient
algorithms for OPE that are robust to the aforementioned shifts for contextual
bandits and Markov decision processes. We also theoretically analyze the sample
complexity of these algorithms. Extensive experimentation with synthetic and
real world datasets from the healthcare domain demonstrates that our approach
not only captures realistic dataset shifts accurately, but also results in less
pessimistic policy evaluations.",2209.08682v1,https://arxiv.org/pdf/2209.08682v1
"RVSL: Robust Vehicle Similarity Learning in Real Hazy Scenes Based on
  Semi-supervised Learning","Wei-Ting Chen, I-Hsiang Chen, Chih-Yuan Yeh, Hao-Hsiang Yang, Hua-En Chang, Jian-Jiun Ding, Sy-Yen Kuo","Recently, vehicle similarity learning, also called re-identification (ReID),
has attracted significant attention in computer vision. Several algorithms have
been developed and obtained considerable success. However, most existing
methods have unpleasant performance in the hazy scenario due to poor
visibility. Though some strategies are possible to resolve this problem, they
still have room to be improved due to the limited performance in real-world
scenarios and the lack of real-world clear ground truth. Thus, to resolve this
problem, inspired by CycleGAN, we construct a training paradigm called
\textbf{RVSL} which integrates ReID and domain transformation techniques. The
network is trained on semi-supervised fashion and does not require to employ
the ID labels and the corresponding clear ground truths to learn hazy vehicle
ReID mission in the real-world haze scenes. To further constrain the
unsupervised learning process effectively, several losses are developed.
Experimental results on synthetic and real-world datasets indicate that the
proposed method can achieve state-of-the-art performance on hazy vehicle ReID
problems. It is worth mentioning that although the proposed method is trained
without real-world label information, it can achieve competitive performance
compared to existing supervised methods trained on complete label information.",2209.08630v1,https://arxiv.org/pdf/2209.08630v1
"Robust Online and Distributed Mean Estimation Under Adversarial Data
  Corruption","Tong Yao, Shreyas Sundaram","We study robust mean estimation in an online and distributed scenario in the
presence of adversarial data attacks. At each time step, each agent in a
network receives a potentially corrupted data point, where the data points were
originally independent and identically distributed samples of a random
variable. We propose online and distributed algorithms for all agents to
asymptotically estimate the mean. We provide the error-bound and the
convergence properties of the estimates to the true mean under our algorithms.
Based on the network topology, we further evaluate each agent's trade-off in
convergence rate between incorporating data from neighbors and learning with
only local observations.",2209.09624v1,https://arxiv.org/pdf/2209.09624v1
"A Robust and Constrained Multi-Agent Reinforcement Learning Electric
  Vehicle Rebalancing Method in AMoD Systems","Sihong He, Yue Wang, Shuo Han, Shaofeng Zou, Fei Miao","Electric vehicles (EVs) play critical roles in autonomous mobility-on-demand
(AMoD) systems, but their unique charging patterns increase the model
uncertainties in AMoD systems (e.g. state transition probability). Since there
usually exists a mismatch between the training and test/true environments,
incorporating model uncertainty into system design is of critical importance in
real-world applications. However, model uncertainties have not been considered
explicitly in EV AMoD system rebalancing by existing literature yet, and the
coexistence of model uncertainties and constraints that the decision should
satisfy makes the problem even more challenging. In this work, we design a
robust and constrained multi-agent reinforcement learning (MARL) framework with
state transition kernel uncertainty for EV AMoD systems. We then propose a
robust and constrained MARL algorithm (ROCOMA) with robust natural policy
gradients (RNPG) that trains a robust EV rebalancing policy to balance the
supply-demand ratio and the charging utilization rate across the city under
model uncertainty. Experiments show that the ROCOMA can learn an effective and
robust rebalancing policy. It outperforms non-robust MARL methods in the
presence of model uncertainties. It increases the system fairness by 19.6% and
decreases the rebalancing costs by 75.8%.",2209.08230v2,https://arxiv.org/pdf/2209.08230v2
A Systematic Evaluation of Node Embedding Robustness,"Alexandru Mara, Jefrey Lijffijt, Stephan Günnemann, Tijl De Bie","Node embedding methods map network nodes to low dimensional vectors that can
be subsequently used in a variety of downstream prediction tasks. The
popularity of these methods has grown significantly in recent years, yet, their
robustness to perturbations of the input data is still poorly understood. In
this paper, we assess the empirical robustness of node embedding models to
random and adversarial poisoning attacks. Our systematic evaluation covers
representative embedding methods based on Skip-Gram, matrix factorization, and
deep neural networks. We compare edge addition, deletion and rewiring attacks
computed using network properties as well as node labels. We also investigate
the performance of popular node classification attack baselines that assume
full knowledge of the node labels. We report qualitative results via embedding
visualization and quantitative results in terms of downstream node
classification and network reconstruction performances. We find that node
classification results are impacted more than network reconstruction ones, that
degree-based and label-based attacks are on average the most damaging and that
label heterophily can strongly influence attack performance.",2209.08064v3,https://arxiv.org/pdf/2209.08064v3
"Trustworthy Reinforcement Learning Against Intrinsic Vulnerabilities:
  Robustness, Safety, and Generalizability","Mengdi Xu, Zuxin Liu, Peide Huang, Wenhao Ding, Zhepeng Cen, Bo Li, Ding Zhao","A trustworthy reinforcement learning algorithm should be competent in solving
challenging real-world problems, including {robustly} handling uncertainties,
satisfying {safety} constraints to avoid catastrophic failures, and
{generalizing} to unseen scenarios during deployments. This study aims to
overview these main perspectives of trustworthy reinforcement learning
considering its intrinsic vulnerabilities on robustness, safety, and
generalizability. In particular, we give rigorous formulations, categorize
corresponding methodologies, and discuss benchmarks for each perspective.
Moreover, we provide an outlook section to spur promising future directions
with a brief discussion on extrinsic vulnerabilities considering human
feedback. We hope this survey could bring together separate threads of studies
together in a unified framework and promote the trustworthiness of
reinforcement learning.",2209.08025v1,https://arxiv.org/pdf/2209.08025v1
"Robust Inference of Manifold Density and Geometry by Doubly Stochastic
  Scaling","Boris Landa, Xiuyuan Cheng","The Gaussian kernel and its traditional normalizations (e.g., row-stochastic)
are popular approaches for assessing similarities between data points. Yet,
they can be inaccurate under high-dimensional noise, especially if the noise
magnitude varies considerably across the data, e.g., under heteroskedasticity
or outliers. In this work, we investigate a more robust alternative -- the
doubly stochastic normalization of the Gaussian kernel. We consider a setting
where points are sampled from an unknown density on a low-dimensional manifold
embedded in high-dimensional space and corrupted by possibly strong,
non-identically distributed, sub-Gaussian noise. We establish that the doubly
stochastic affinity matrix and its scaling factors concentrate around certain
population forms, and provide corresponding finite-sample probabilistic error
bounds. We then utilize these results to develop several tools for robust
inference under general high-dimensional noise. First, we derive a robust
density estimator that reliably infers the underlying sampling density and can
substantially outperform the standard kernel density estimator under
heteroskedasticity and outliers. Second, we obtain estimators for the pointwise
noise magnitudes, the pointwise signal magnitudes, and the pairwise Euclidean
distances between clean data points. Lastly, we derive robust graph Laplacian
normalizations that accurately approximate various manifold Laplacians,
including the Laplace Beltrami operator, improving over traditional
normalizations in noisy settings. We exemplify our results in simulations and
on real single-cell RNA-sequencing data. For the latter, we show that in
contrast to traditional methods, our approach is robust to variability in
technical noise levels across cell types.",2209.08004v2,https://arxiv.org/pdf/2209.08004v2
Model Predictive Robustness of Signal Temporal Logic Predicates,"Yuanfei Lin, Haoxuan Li, Matthias Althoff","The robustness of signal temporal logic not only assesses whether a signal
adheres to a specification but also provides a measure of how much a formula is
fulfilled or violated. The calculation of robustness is based on evaluating the
robustness of underlying predicates. However, the robustness of predicates is
usually defined in a model-free way, i.e., without including the system
dynamics. Moreover, it is often nontrivial to define the robustness of
complicated predicates precisely. To address these issues, we propose a notion
of model predictive robustness, which provides a more systematic way of
evaluating robustness compared to previous approaches by considering
model-based predictions. In particular, we use Gaussian process regression to
learn the robustness based on precomputed predictions so that robustness values
can be efficiently computed online. We evaluate our approach for the use case
of autonomous driving with predicates used in formalized traffic rules on a
recorded dataset, which highlights the advantage of our approach compared to
traditional approaches in terms of precision. By incorporating our robustness
definitions into a trajectory planner, autonomous vehicles obey traffic rules
more robustly than human drivers in the dataset.",2209.07881v3,https://arxiv.org/pdf/2209.07881v3
"Less is Better: Recovering Intended-Feature Subspace to Robustify NLU
  Models","Ting Wu, Tao Gui","Datasets with significant proportions of bias present threats for training a
trustworthy model on NLU tasks. Despite yielding great progress, current
debiasing methods impose excessive reliance on the knowledge of bias
attributes. Definition of the attributes, however, is elusive and varies across
different datasets. Furthermore, leveraging these attributes at input level to
bias mitigation may leave a gap between intrinsic properties and the underlying
decision rule. To narrow down this gap and liberate the supervision on bias, we
suggest extending bias mitigation into feature space. Therefore, a novel model,
Recovering Intended-Feature Subspace with Knowledge-Free (RISK) is developed.
Assuming that shortcut features caused by various biases are unintended for
prediction, RISK views them as redundant features. When delving into a lower
manifold to remove redundancies, RISK reveals that an extremely low-dimensional
subspace with intended features can robustly represent the highly biased
dataset. Empirical results demonstrate our model can consistently improve model
generalization to out-of-distribution set, and achieves a new state-of-the-art
performance.",2209.07879v1,https://arxiv.org/pdf/2209.07879v1
M$^2$DQN: A Robust Method for Accelerating Deep Q-learning Network,"Zhe Zhang, Yukun Zou, Junjie Lai, Qing Xu","Deep Q-learning Network (DQN) is a successful way which combines
reinforcement learning with deep neural networks and leads to a widespread
application of reinforcement learning. One challenging problem when applying
DQN or other reinforcement learning algorithms to real world problem is data
collection. Therefore, how to improve data efficiency is one of the most
important problems in the research of reinforcement learning. In this paper, we
propose a framework which uses the Max-Mean loss in Deep Q-Network (M$^2$DQN).
Instead of sampling one batch of experiences in the training step, we sample
several batches from the experience replay and update the parameters such that
the maximum TD-error of these batches is minimized. The proposed method can be
combined with most of existing techniques of DQN algorithm by replacing the
loss function. We verify the effectiveness of this framework with one of the
most widely used techniques, Double DQN (DDQN), in several gym games. The
results show that our method leads to a substantial improvement in both the
learning speed and performance.",2209.07809v1,https://arxiv.org/pdf/2209.07809v1
On the Robustness of Graph Neural Diffusion to Topology Perturbations,"Yang Song, Qiyu Kang, Sijie Wang, Zhao Kai, Wee Peng Tay","Neural diffusion on graphs is a novel class of graph neural networks that has
attracted increasing attention recently. The capability of graph neural partial
differential equations (PDEs) in addressing common hurdles of graph neural
networks (GNNs), such as the problems of over-smoothing and bottlenecks, has
been investigated but not their robustness to adversarial attacks. In this
work, we explore the robustness properties of graph neural PDEs. We empirically
demonstrate that graph neural PDEs are intrinsically more robust against
topology perturbation as compared to other GNNs. We provide insights into this
phenomenon by exploiting the stability of the heat semigroup under graph
topology perturbations. We discuss various graph diffusion operators and relate
them to existing graph neural PDEs. Furthermore, we propose a general graph
neural PDE framework based on which a new class of robust GNNs can be defined.
We verify that the new model achieves comparable state-of-the-art performance
on several benchmark datasets.",2209.07754v2,https://arxiv.org/pdf/2209.07754v2
"Renyi Differential Privacy of Propose-Test-Release and Applications to
  Private and Robust Machine Learning","Jiachen T. Wang, Saeed Mahloujifar, Shouda Wang, Ruoxi Jia, Prateek Mittal","Propose-Test-Release (PTR) is a differential privacy framework that works
with local sensitivity of functions, instead of their global sensitivity. This
framework is typically used for releasing robust statistics such as median or
trimmed mean in a differentially private manner. While PTR is a common
framework introduced over a decade ago, using it in applications such as robust
SGD where we need many adaptive robust queries is challenging. This is mainly
due to the lack of Renyi Differential Privacy (RDP) analysis, an essential
ingredient underlying the moments accountant approach for differentially
private deep learning. In this work, we generalize the standard PTR and derive
the first RDP bound for it when the target function has bounded global
sensitivity. We show that our RDP bound for PTR yields tighter DP guarantees
than the directly analyzed $(\eps, \delta)$-DP. We also derive the
algorithm-specific privacy amplification bound of PTR under subsampling. We
show that our bound is much tighter than the general upper bound and close to
the lower bound. Our RDP bounds enable tighter privacy loss calculation for the
composition of many adaptive runs of PTR. As an application of our analysis, we
show that PTR and our theoretical results can be used to design differentially
private variants for byzantine robust training algorithms that use robust
statistics for gradients aggregation. We conduct experiments on the settings of
label, feature, and gradient corruption across different datasets and
architectures. We show that PTR-based private and robust training algorithm
significantly improves the utility compared with the baseline.",2209.07716v1,https://arxiv.org/pdf/2209.07716v1
Computing the optimal distributionally-robust strategy to commit to,"Sai Mali Ananthanarayanan, Christian Kroer","The Stackelberg game model, where a leader commits to a strategy and the
follower best responds, has found widespread application, particularly to
security problems. In the security setting, the goal is for the leader to
compute an optimal strategy to commit to, in order to protect some asset. In
many of these applications, the parameters of the follower utility model are
not known with certainty. Distributionally-robust optimization addresses this
issue by allowing a distribution over possible model parameters, where this
distribution comes from a set of possible distributions. The goal is to
maximize the expected utility with respect to the worst-case distribution. We
initiate the study of distributionally-robust models for computing the optimal
strategy to commit to. We consider the case of normal-form games with
uncertainty about the follower utility model. Our main theoretical result is to
show that a distributionally-robust Stackelberg equilibrium always exists
across a wide array of uncertainty models. For the case of a finite set of
possible follower utility functions we present two algorithms to compute a
distributionally-robust strong Stackelberg equilibrium (DRSSE) using
mathematical programs. Next, in the general case where there is an infinite
number of possible follower utility functions and the uncertainty is
represented by a Wasserstein ball around a finitely-supported nominal
distribution, we give an incremental mixed-integer-programming-based algorithm
for computing the optimal distributionally-robust strategy. Experiments
substantiate the tractability of our algorithm on a classical Stackelberg game,
showing that our approach scales to medium-sized games.",2209.07647v1,https://arxiv.org/pdf/2209.07647v1
"Explicit Tradeoffs between Adversarial and Natural Distributional
  Robustness","Mazda Moayeri, Kiarash Banihashem, Soheil Feizi","Several existing works study either adversarial or natural distributional
robustness of deep neural networks separately. In practice, however, models
need to enjoy both types of robustness to ensure reliability. In this work, we
bridge this gap and show that in fact, explicit tradeoffs exist between
adversarial and natural distributional robustness. We first consider a simple
linear regression setting on Gaussian data with disjoint sets of core and
spurious features. In this setting, through theoretical and empirical analysis,
we show that (i) adversarial training with $\ell_1$ and $\ell_2$ norms
increases the model reliance on spurious features; (ii) For $\ell_\infty$
adversarial training, spurious reliance only occurs when the scale of the
spurious features is larger than that of the core features; (iii) adversarial
training can have an unintended consequence in reducing distributional
robustness, specifically when spurious correlations are changed in the new test
domain. Next, we present extensive empirical evidence, using a test suite of
twenty adversarially trained models evaluated on five benchmark datasets
(ObjectNet, RIVAL10, Salient ImageNet-1M, ImageNet-9, Waterbirds), that
adversarially trained classifiers rely on backgrounds more than their
standardly trained counterparts, validating our theoretical results. We also
show that spurious correlations in training data (when preserved in the test
domain) can improve adversarial robustness, revealing that previous claims that
adversarial vulnerability is rooted in spurious correlations are incomplete.",2209.07592v1,https://arxiv.org/pdf/2209.07592v1
A Light Recipe to Train Robust Vision Transformers,"Edoardo Debenedetti, Vikash Sehwag, Prateek Mittal","In this paper, we ask whether Vision Transformers (ViTs) can serve as an
underlying architecture for improving the adversarial robustness of machine
learning models against evasion attacks. While earlier works have focused on
improving Convolutional Neural Networks, we show that also ViTs are highly
suitable for adversarial training to achieve competitive performance. We
achieve this objective using a custom adversarial training recipe, discovered
using rigorous ablation studies on a subset of the ImageNet dataset. The
canonical training recipe for ViTs recommends strong data augmentation, in part
to compensate for the lack of vision inductive bias of attention modules, when
compared to convolutions. We show that this recipe achieves suboptimal
performance when used for adversarial training. In contrast, we find that
omitting all heavy data augmentation, and adding some additional bag-of-tricks
($\varepsilon$-warmup and larger weight decay), significantly boosts the
performance of robust ViTs. We show that our recipe generalizes to different
classes of ViT architectures and large-scale models on full ImageNet-1k.
Additionally, investigating the reasons for the robustness of our models, we
show that it is easier to generate strong attacks during training when using
our recipe and that this leads to better robustness at test time. Finally, we
further study one consequence of adversarial training by proposing a way to
quantify the semantic nature of adversarial perturbations and highlight its
correlation with the robustness of the model. Overall, we recommend that the
community should avoid translating the canonical training recipes in ViTs to
robust training and rethink common training choices in the context of
adversarial training.",2209.07399v2,https://arxiv.org/pdf/2209.07399v2
Part-Based Models Improve Adversarial Robustness,"Chawin Sitawarin, Kornrapat Pongmala, Yizheng Chen, Nicholas Carlini, David Wagner","We show that combining human prior knowledge with end-to-end learning can
improve the robustness of deep neural networks by introducing a part-based
model for object classification. We believe that the richer form of annotation
helps guide neural networks to learn more robust features without requiring
more samples or larger models. Our model combines a part segmentation model
with a tiny classifier and is trained end-to-end to simultaneously segment
objects into parts and then classify the segmented object. Empirically, our
part-based models achieve both higher accuracy and higher adversarial
robustness than a ResNet-50 baseline on all three datasets. For instance, the
clean accuracy of our part models is up to 15 percentage points higher than the
baseline's, given the same level of robustness. Our experiments indicate that
these models also reduce texture bias and yield better robustness against
common corruptions and spurious correlations. The code is publicly available at
https://github.com/chawins/adv-part-model.",2209.09117v2,https://arxiv.org/pdf/2209.09117v2
"Adversarially Robust Learning: A Generic Minimax Optimal Learner and
  Characterization","Omar Montasser, Steve Hanneke, Nathan Srebro","We present a minimax optimal learner for the problem of learning predictors
robust to adversarial examples at test-time. Interestingly, we find that this
requires new algorithmic ideas and approaches to adversarially robust learning.
In particular, we show, in a strong negative sense, the suboptimality of the
robust learner proposed by Montasser, Hanneke, and Srebro (2019) and a broader
family of learners we identify as local learners. Our results are enabled by
adopting a global perspective, specifically, through a key technical
contribution: the global one-inclusion graph, which may be of independent
interest, that generalizes the classical one-inclusion graph due to Haussler,
Littlestone, and Warmuth (1994). Finally, as a byproduct, we identify a
dimension characterizing qualitatively and quantitatively what classes of
predictors $\mathcal{H}$ are robustly learnable. This resolves an open problem
due to Montasser et al. (2019), and closes a (potentially) infinite gap between
the established upper and lower bounds on the sample complexity of
adversarially robust learning.",2209.07369v1,https://arxiv.org/pdf/2209.07369v1
Improving Robust Fairness via Balance Adversarial Training,"Chunyu Sun, Chenye Xu, Chengyuan Yao, Siyuan Liang, Yichao Wu, Ding Liang, XiangLong Liu, Aishan Liu","Adversarial training (AT) methods are effective against adversarial attacks,
yet they introduce severe disparity of accuracy and robustness between
different classes, known as the robust fairness problem. Previously proposed
Fair Robust Learning (FRL) adaptively reweights different classes to improve
fairness. However, the performance of the better-performed classes decreases,
leading to a strong performance drop. In this paper, we observed two unfair
phenomena during adversarial training: different difficulties in generating
adversarial examples from each class (source-class fairness) and disparate
target class tendencies when generating adversarial examples (target-class
fairness). From the observations, we propose Balance Adversarial Training (BAT)
to address the robust fairness problem. Regarding source-class fairness, we
adjust the attack strength and difficulties of each class to generate samples
near the decision boundary for easier and fairer model learning; considering
target-class fairness, by introducing a uniform distribution constraint, we
encourage the adversarial example generation process for each class with a fair
tendency. Extensive experiments conducted on multiple datasets (CIFAR-10,
CIFAR-100, and ImageNette) demonstrate that our method can significantly
outperform other baselines in mitigating the robust fairness problem (+5-10\%
on the worst class accuracy)",2209.07534v1,https://arxiv.org/pdf/2209.07534v1
"Robustness in deep learning: The good (width), the bad (depth), and the
  ugly (initialization)","Zhenyu Zhu, Fanghui Liu, Grigorios G Chrysos, Volkan Cevher","We study the average robustness notion in deep neural networks in (selected)
wide and narrow, deep and shallow, as well as lazy and non-lazy training
settings. We prove that in the under-parameterized setting, width has a
negative effect while it improves robustness in the over-parameterized setting.
The effect of depth closely depends on the initialization and the training
mode. In particular, when initialized with LeCun initialization, depth helps
robustness with the lazy training regime. In contrast, when initialized with
Neural Tangent Kernel (NTK) and He-initialization, depth hurts the robustness.
Moreover, under the non-lazy training regime, we demonstrate how the width of a
two-layer ReLU network benefits robustness. Our theoretical developments
improve the results by [Huang et al. NeurIPS21; Wu et al. NeurIPS21] and are
consistent with [Bubeck and Sellke NeurIPS21; Bubeck et al. COLT21].",2209.07263v4,https://arxiv.org/pdf/2209.07263v4
"Double Doubly Robust Thompson Sampling for Generalized Linear Contextual
  Bandits","Wonyoung Kim, Kyungbok Lee, Myunghee Cho Paik","We propose a novel contextual bandit algorithm for generalized linear rewards
with an $\tilde{O}(\sqrt{\kappa^{-1} \phi T})$ regret over $T$ rounds where
$\phi$ is the minimum eigenvalue of the covariance of contexts and $\kappa$ is
a lower bound of the variance of rewards. In several practical cases where
$\phi=O(d)$, our result is the first regret bound for generalized linear model
(GLM) bandits with the order $\sqrt{d}$ without relying on the approach of Auer
[2002]. We achieve this bound using a novel estimator called double
doubly-robust (DDR) estimator, a subclass of doubly-robust (DR) estimator but
with a tighter error bound. The approach of Auer [2002] achieves independence
by discarding the observed rewards, whereas our algorithm achieves independence
considering all contexts using our DDR estimator. We also provide an
$O(\kappa^{-1} \phi \log (NT) \log T)$ regret bound for $N$ arms under a
probabilistic margin condition. Regret bounds under the margin condition are
given by Bastani and Bayati [2020] and Bastani et al. [2021] under the setting
that contexts are common to all arms but coefficients are arm-specific. When
contexts are different for all arms but coefficients are common, ours is the
first regret bound under the margin condition for linear models or GLMs. We
conduct empirical studies using synthetic data and real examples, demonstrating
the effectiveness of our algorithm.",2209.06983v2,https://arxiv.org/pdf/2209.06983v2
"PointACL:Adversarial Contrastive Learning for Robust Point Clouds
  Representation under Adversarial Attack","Junxuan Huang, Yatong An, Lu cheng, Bai Chen, Junsong Yuan, Chunming Qiao","Despite recent success of self-supervised based contrastive learning model
for 3D point clouds representation, the adversarial robustness of such
pre-trained models raised concerns. Adversarial contrastive learning (ACL) is
considered an effective way to improve the robustness of pre-trained models. In
contrastive learning, the projector is considered an effective component for
removing unnecessary feature information during contrastive pretraining and
most ACL works also use contrastive loss with projected feature representations
to generate adversarial examples in pretraining, while ""unprojected "" feature
representations are used in generating adversarial inputs during
inference.Because of the distribution gap between projected and ""unprojected""
features, their models are constrained of obtaining robust feature
representations for downstream tasks. We introduce a new method to generate
high-quality 3D adversarial examples for adversarial training by utilizing
virtual adversarial loss with ""unprojected"" feature representations in
contrastive learning framework. We present our robust aware loss function to
train self-supervised contrastive learning framework adversarially.
Furthermore, we find selecting high difference points with the Difference of
Normal (DoN) operator as additional input for adversarial self-supervised
contrastive learning can significantly improve the adversarial robustness of
the pre-trained model. We validate our method, PointACL on downstream tasks,
including 3D classification and 3D segmentation with multiple datasets. It
obtains comparable robust accuracy over state-of-the-art contrastive
adversarial learning methods.",2209.06971v1,https://arxiv.org/pdf/2209.06971v1
"On the interplay of adversarial robustness and architecture components:
  patches, convolution and attention","Francesco Croce, Matthias Hein","In recent years novel architecture components for image classification have
been developed, starting with attention and patches used in transformers. While
prior works have analyzed the influence of some aspects of architecture
components on the robustness to adversarial attacks, in particular for vision
transformers, the understanding of the main factors is still limited. We
compare several (non)-robust classifiers with different architectures and study
their properties, including the effect of adversarial training on the
interpretability of the learnt features and robustness to unseen threat models.
An ablation from ResNet to ConvNeXt reveals key architectural changes leading
to almost $10\%$ higher $\ell_\infty$-robustness.",2209.06953v1,https://arxiv.org/pdf/2209.06953v1
"Robust Transferable Feature Extractors: Learning to Defend Pre-Trained
  Networks Against White Box Adversaries","Alexander Cann, Ian Colbert, Ihab Amer","The widespread adoption of deep neural networks in computer vision
applications has brought forth a significant interest in adversarial
robustness. Existing research has shown that maliciously perturbed inputs
specifically tailored for a given model (i.e., adversarial examples) can be
successfully transferred to another independently trained model to induce
prediction errors. Moreover, this property of adversarial examples has been
attributed to features derived from predictive patterns in the data
distribution. Thus, we are motivated to investigate the following question: Can
adversarial defenses, like adversarial examples, be successfully transferred to
other independently trained models? To this end, we propose a deep
learning-based pre-processing mechanism, which we refer to as a robust
transferable feature extractor (RTFE). After examining theoretical motivation
and implications, we experimentally show that our method can provide
adversarial robustness to multiple independently pre-trained classifiers that
are otherwise ineffective against an adaptive white box adversary. Furthermore,
we show that RTFEs can even provide one-shot adversarial robustness to models
independently trained on different datasets.",2209.06931v1,https://arxiv.org/pdf/2209.06931v1
Robust Constrained Reinforcement Learning,"Yue Wang, Fei Miao, Shaofeng Zou","Constrained reinforcement learning is to maximize the expected reward subject
to constraints on utilities/costs. However, the training environment may not be
the same as the test one, due to, e.g., modeling error, adversarial attack,
non-stationarity, resulting in severe performance degradation and more
importantly constraint violation. We propose a framework of robust constrained
reinforcement learning under model uncertainty, where the MDP is not fixed but
lies in some uncertainty set, the goal is to guarantee that constraints on
utilities/costs are satisfied for all MDPs in the uncertainty set, and to
maximize the worst-case reward performance over the uncertainty set. We design
a robust primal-dual approach, and further theoretically develop guarantee on
its convergence, complexity and robust feasibility. We then investigate a
concrete example of $\delta$-contamination uncertainty set, design an online
and model-free algorithm and theoretically characterize its sample complexity.",2209.06866v1,https://arxiv.org/pdf/2209.06866v1
Robust field-level inference with dark matter halos,"Helen Shao, Francisco Villaescusa-Navarro, Pablo Villanueva-Domingo, Romain Teyssier, Lehman H. Garrison, Marco Gatti, Derek Inman, Yueying Ni, Ulrich P. Steinwandel, Mihir Kulkarni, Eli Visbal, Greg L. Bryan, Daniel Angles-Alcazar, Tiago Castro, Elena Hernandez-Martinez, Klaus Dolag","We train graph neural networks on halo catalogues from Gadget N-body
simulations to perform field-level likelihood-free inference of cosmological
parameters. The catalogues contain $\lesssim$5,000 halos with masses $\gtrsim
10^{10}~h^{-1}M_\odot$ in a periodic volume of $(25~h^{-1}{\rm Mpc})^3$; every
halo in the catalogue is characterized by several properties such as position,
mass, velocity, concentration, and maximum circular velocity. Our models, built
to be permutationally, translationally, and rotationally invariant, do not
impose a minimum scale on which to extract information and are able to infer
the values of $\Omega_{\rm m}$ and $\sigma_8$ with a mean relative error of
$\sim6\%$, when using positions plus velocities and positions plus masses,
respectively. More importantly, we find that our models are very robust: they
can infer the value of $\Omega_{\rm m}$ and $\sigma_8$ when tested using halo
catalogues from thousands of N-body simulations run with five different N-body
codes: Abacus, CUBEP$^3$M, Enzo, PKDGrav3, and Ramses. Surprisingly, the model
trained to infer $\Omega_{\rm m}$ also works when tested on thousands of
state-of-the-art CAMELS hydrodynamic simulations run with four different codes
and subgrid physics implementations. Using halo properties such as
concentration and maximum circular velocity allow our models to extract more
information, at the expense of breaking the robustness of the models. This may
happen because the different N-body codes are not converged on the relevant
scales corresponding to these parameters.",2209.06843v1,https://arxiv.org/pdf/2209.06843v1
"Distributionally Robust Offline Reinforcement Learning with Linear
  Function Approximation","Xiaoteng Ma, Zhipeng Liang, Jose Blanchet, Mingwen Liu, Li Xia, Jiheng Zhang, Qianchuan Zhao, Zhengyuan Zhou","Among the reasons hindering reinforcement learning (RL) applications to
real-world problems, two factors are critical: limited data and the mismatch
between the testing environment (real environment in which the policy is
deployed) and the training environment (e.g., a simulator). This paper attempts
to address these issues simultaneously with distributionally robust offline RL,
where we learn a distributionally robust policy using historical data obtained
from the source environment by optimizing against a worst-case perturbation
thereof. In particular, we move beyond tabular settings and consider linear
function approximation. More specifically, we consider two settings, one where
the dataset is well-explored and the other where the dataset has sufficient
coverage of the optimal policy. We propose two algorithms~-- one for each of
the two settings~-- that achieve error bounds $\tilde{O}(d^{1/2}/N^{1/2})$ and
$\tilde{O}(d^{3/2}/N^{1/2})$ respectively, where $d$ is the dimension in the
linear function approximation and $N$ is the number of trajectories in the
dataset. To the best of our knowledge, they provide the first non-asymptotic
results of the sample complexity in this setting. Diverse experiments are
conducted to demonstrate our theoretical findings, showing the superiority of
our algorithm against the non-robust one.",2209.06620v3,https://arxiv.org/pdf/2209.06620v3
"Point Cloud Registration-Driven Robust Feature Matching for 3D Siamese
  Object Tracking","Haobo Jiang, Kaihao Lan, Le Hui, Guangyu Li, Jin Xie, Jian Yang","Learning robust feature matching between the template and search area is
crucial for 3D Siamese tracking. The core of Siamese feature matching is how to
assign high feature similarity on the corresponding points between the template
and search area for precise object localization. In this paper, we propose a
novel point cloud registration-driven Siamese tracking framework, with the
intuition that spatially aligned corresponding points (via 3D registration)
tend to achieve consistent feature representations. Specifically, our method
consists of two modules, including a tracking-specific nonlocal registration
module and a registration-aided Sinkhorn template-feature aggregation module.
The registration module targets at the precise spatial alignment between the
template and search area. The tracking-specific spatial distance constraint is
proposed to refine the cross-attention weights in the nonlocal module for
discriminative feature learning. Then, we use the weighted SVD to compute the
rigid transformation between the template and search area, and align them to
achieve the desired spatially aligned corresponding points. For the feature
aggregation model, we formulate the feature matching between the transformed
template and search area as an optimal transport problem and utilize the
Sinkhorn optimization to search for the outlier-robust matching solution. Also,
a registration-aided spatial distance map is built to improve the matching
robustness in indistinguishable regions (e.g., smooth surface). Finally, guided
by the obtained feature matching map, we aggregate the target information from
the template into the search area to construct the target-specific feature,
which is then fed into a CenterPoint-like detection head for object
localization. Extensive experiments on KITTI, NuScenes and Waymo datasets
verify the effectiveness of our proposed method.",2209.06395v2,https://arxiv.org/pdf/2209.06395v2
"A Robust Scientific Machine Learning for Optimization: A Novel
  Robustness Theorem","Luana P. Queiroz, Carine M. Rebello, Erber A. Costa, Vinicius V. Santana, Alirio E. Rodrigues, Ana M. Ribeiro, Idelfonso B. R. Nogueira","Scientific machine learning (SciML) is a field of increasing interest in
several different application fields. In an optimization context, SciML-based
tools have enabled the development of more efficient optimization methods.
However, implementing SciML tools for optimization must be rigorously evaluated
and performed with caution. This work proposes the deductions of a robustness
test that guarantees the robustness of multiobjective SciML-based optimization
by showing that its results respect the universal approximator theorem. The
test is applied in the framework of a novel methodology which is evaluated in a
series of benchmarks illustrating its consistency. Moreover, the proposed
methodology results are compared with feasible regions of rigorous
optimization, which requires a significantly higher computational effort.
Hence, this work provides a robustness test for guaranteed robustness in
applying SciML tools in multiobjective optimization with lower computational
effort than the existent alternative.",2209.06642v1,https://arxiv.org/pdf/2209.06642v1
Adversarial Coreset Selection for Efficient Robust Training,"Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie","Neural networks are vulnerable to adversarial attacks: adding well-crafted,
imperceptible perturbations to their input can modify their output. Adversarial
training is one of the most effective approaches to training robust models
against such attacks. Unfortunately, this method is much slower than vanilla
training of neural networks since it needs to construct adversarial examples
for the entire training data at every iteration. By leveraging the theory of
coreset selection, we show how selecting a small subset of training data
provides a principled approach to reducing the time complexity of robust
training. To this end, we first provide convergence guarantees for adversarial
coreset selection. In particular, we show that the convergence bound is
directly related to how well our coresets can approximate the gradient computed
over the entire training data. Motivated by our theoretical analysis, we
propose using this gradient approximation error as our adversarial coreset
selection objective to reduce the training set size effectively. Once built, we
run adversarial training over this subset of the training data. Unlike existing
methods, our approach can be adapted to a wide variety of training objectives,
including TRADES, $\ell_p$-PGD, and Perceptual Adversarial Training. We conduct
extensive experiments to demonstrate that our approach speeds up adversarial
training by 2-3 times while experiencing a slight degradation in the clean and
robust accuracy.",2209.05785v2,https://arxiv.org/pdf/2209.05785v2
Boosting Robustness Verification of Semantic Feature Neighborhoods,"Anan Kabaha, Dana Drachsler-Cohen","Deep neural networks have been shown to be vulnerable to adversarial attacks
that perturb inputs based on semantic features. Existing robustness analyzers
can reason about semantic feature neighborhoods to increase the networks'
reliability. However, despite the significant progress in these techniques,
they still struggle to scale to deep networks and large neighborhoods. In this
work, we introduce VeeP, an active learning approach that splits the
verification process into a series of smaller verification steps, each is
submitted to an existing robustness analyzer. The key idea is to build on prior
steps to predict the next optimal step. The optimal step is predicted by
estimating the certification velocity and sensitivity via parametric
regression. We evaluate VeeP on MNIST, Fashion-MNIST, CIFAR-10 and ImageNet and
show that it can analyze neighborhoods of various features: brightness,
contrast, hue, saturation, and lightness. We show that, on average, given a 90
minute timeout, VeeP verifies 96% of the maximally certifiable neighborhoods
within 29 minutes, while existing splitting approaches verify, on average, 73%
of the maximally certifiable neighborhoods within 58 minutes.",2209.05446v1,https://arxiv.org/pdf/2209.05446v1
"Self-supervised Sequential Information Bottleneck for Robust Exploration
  in Deep Reinforcement Learning","Bang You, Jingming Xie, Youping Chen, Jan Peters, Oleg Arenz","Effective exploration is critical for reinforcement learning agents in
environments with sparse rewards or high-dimensional state-action spaces.
Recent works based on state-visitation counts, curiosity and
entropy-maximization generate intrinsic reward signals to motivate the agent to
visit novel states for exploration. However, the agent can get distracted by
perturbations to sensor inputs that contain novel but task-irrelevant
information, e.g. due to sensor noise or changing background. In this work, we
introduce the sequential information bottleneck objective for learning
compressed and temporally coherent representations by modelling and compressing
sequential predictive information in time-series observations. For efficient
exploration in noisy environments, we further construct intrinsic rewards that
capture task-relevant state novelty based on the learned representations. We
derive a variational upper bound of our sequential information bottleneck
objective for practical optimization and provide an information-theoretic
interpretation of the derived upper bound. Our experiments on a set of
challenging image-based simulated control tasks show that our method achieves
better sample efficiency, and robustness to both white noise and natural video
backgrounds compared to state-of-art methods based on curiosity, entropy
maximization and information-gain.",2209.05333v1,https://arxiv.org/pdf/2209.05333v1
"CARE: Certifiably Robust Learning with Reasoning via Variational
  Inference","Jiawei Zhang, Linyi Li, Ce Zhang, Bo Li","Despite great recent advances achieved by deep neural networks (DNNs), they
are often vulnerable to adversarial attacks. Intensive research efforts have
been made to improve the robustness of DNNs; however, most empirical defenses
can be adaptively attacked again, and the theoretically certified robustness is
limited, especially on large-scale datasets. One potential root cause of such
vulnerabilities for DNNs is that although they have demonstrated powerful
expressiveness, they lack the reasoning ability to make robust and reliable
predictions. In this paper, we aim to integrate domain knowledge to enable
robust learning with the reasoning paradigm. In particular, we propose a
certifiably robust learning with reasoning pipeline (CARE), which consists of a
learning component and a reasoning component. Concretely, we use a set of
standard DNNs to serve as the learning component to make semantic predictions,
and we leverage the probabilistic graphical models, such as Markov logic
networks (MLN), to serve as the reasoning component to enable knowledge/logic
reasoning. However, it is known that the exact inference of MLN (reasoning) is
#P-complete, which limits the scalability of the pipeline. To this end, we
propose to approximate the MLN inference via variational inference based on an
efficient expectation maximization algorithm. In particular, we leverage graph
convolutional networks (GCNs) to encode the posterior distribution during
variational inference and update the parameters of GCNs (E-step) and the
weights of knowledge rules in MLN (M-step) iteratively. We conduct extensive
experiments on different datasets and show that CARE achieves significantly
higher certified robustness compared with the state-of-the-art baselines. We
additionally conducted different ablation studies to demonstrate the empirical
robustness of CARE and the effectiveness of different knowledge integration.",2209.05055v3,https://arxiv.org/pdf/2209.05055v3
"A novel learning-based robust model predictive control energy management
  strategy for fuel cell electric vehicles","Shibo Li, Zhuoran Hou, Liang Chu, Jingjing Jiang, Yuanjian Zhang","The multi-source electromechanical coupling makes the energy management of
fuel cell electric vehicles (FCEVs) relatively nonlinear and complex especially
in the types of 4-wheel-drive (4WD) FCEVs. Accurate state observing for
complicated nonlinear system is the basis for fantastic energy managing in
FCEVs. Aiming at releasing the energy-saving potential of FCEVs, a novel
learning-based robust model predictive control (LRMPC) strategy is proposed for
a 4WD FCEV, contributing to suitable power distribution among multiple energy
sources. The well-designed strategy based on machine learning (ML) translates
the knowledge of the nonlinear system to the explicit controlling scheme with
superior robust performance. To start with, ML methods with high regression
accuracy and superior generalization ability are trained offline to establish
the precise state observer for SOC. Then, explicit data tables for SOC
generated by state observer are used for grabbing accurate state changing,
whose input features include the vehicle status and the states of vehicle
components. To be specific, the vehicle velocity estimation for providing
future speed reference is constructed by deep forest. Next, the components
including explicit data tables and vehicle velocity estimation are combined
with model predictive control (MPC) to release the state-of-the-art
energy-saving ability for the multi-freedom system in FCEVs, whose name is
LRMPC. At last, the detailed assessment is performed in simulation test to
validate the advancing performance of LRMPC. The corresponding results
highlight the optimal control effect in energy-saving potential and strong
real-time application ability of LRMPC.",2209.04995v1,https://arxiv.org/pdf/2209.04995v1
Robust-by-Design Classification via Unitary-Gradient Neural Networks,"Fabio Brau, Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo","The use of neural networks in safety-critical systems requires safe and
robust models, due to the existence of adversarial attacks. Knowing the minimal
adversarial perturbation of any input x, or, equivalently, knowing the distance
of x from the classification boundary, allows evaluating the classification
robustness, providing certifiable predictions. Unfortunately, state-of-the-art
techniques for computing such a distance are computationally expensive and
hence not suited for online applications. This work proposes a novel family of
classifiers, namely Signed Distance Classifiers (SDCs), that, from a
theoretical perspective, directly output the exact distance of x from the
classification boundary, rather than a probability score (e.g., SoftMax). SDCs
represent a family of robust-by-design classifiers. To practically address the
theoretical requirements of a SDC, a novel network architecture named
Unitary-Gradient Neural Network is presented. Experimental results show that
the proposed architecture approximates a signed distance classifier, hence
allowing an online certifiable classification of x at the cost of a single
inference.",2209.04293v1,https://arxiv.org/pdf/2209.04293v1
"Shapley value-based approaches to explain the robustness of classifiers
  in machine learning","Guilherme Dean Pelegrina, Sajid Siraj","The use of algorithm-agnostic approaches is an emerging area of research for
explaining the contribution of individual features towards the predicted
outcome. Whilst there is a focus on explaining the prediction itself, a little
has been done on explaining the robustness of these models, that is, how each
feature contributes towards achieving that robustness. In this paper, we
propose the use of Shapley values to explain the contribution of each feature
towards the model's robustness, measured in terms of Receiver-operating
Characteristics (ROC) curve and the Area under the ROC curve (AUC). With the
help of an illustrative example, we demonstrate the proposed idea of explaining
the ROC curve, and visualising the uncertainties in these curves. For
imbalanced datasets, the use of Precision-Recall Curve (PRC) is considered more
appropriate, therefore we also demonstrate how to explain the PRCs with the
help of Shapley values. The explanation of robustness can help analysts in a
number of ways, for example, it can help in feature selection by identifying
the irrelevant features that can be removed to reduce the computational
complexity. It can also help in identifying the features having critical
contributions or negative contributions towards robustness.",2209.04254v2,https://arxiv.org/pdf/2209.04254v2
"Robust and Lossless Fingerprinting of Deep Neural Networks via Pooled
  Membership Inference",Hanzhou Wu,"Deep neural networks (DNNs) have already achieved great success in a lot of
application areas and brought profound changes to our society. However, it also
raises new security problems, among which how to protect the intellectual
property (IP) of DNNs against infringement is one of the most important yet
very challenging topics. To deal with this problem, recent studies focus on the
IP protection of DNNs by applying digital watermarking, which embeds source
information and/or authentication data into DNN models by tuning network
parameters directly or indirectly. However, tuning network parameters
inevitably distorts the DNN and therefore surely impairs the performance of the
DNN model on its original task regardless of the degree of the performance
degradation. It has motivated the authors in this paper to propose a novel
technique called pooled membership inference (PMI) so as to protect the IP of
the DNN models. The proposed PMI neither alters the network parameters of the
given DNN model nor fine-tunes the DNN model with a sequence of carefully
crafted trigger samples. Instead, it leaves the original DNN model unchanged,
but can determine the ownership of the DNN model by inferring which
mini-dataset among multiple mini-datasets was once used to train the target DNN
model, which differs from previous arts and has remarkable potential in
practice. Experiments also have demonstrated the superiority and applicability
of this work.",2209.04113v2,https://arxiv.org/pdf/2209.04113v2
RASR: Risk-Averse Soft-Robust MDPs with EVaR and Entropic Risk,"Jia Lin Hau, Marek Petrik, Mohammad Ghavamzadeh, Reazul Russel","Prior work on safe Reinforcement Learning (RL) has studied risk-aversion to
randomness in dynamics (aleatory) and to model uncertainty (epistemic) in
isolation. We propose and analyze a new framework to jointly model the risk
associated with epistemic and aleatory uncertainties in finite-horizon and
discounted infinite-horizon MDPs. We call this framework that combines
Risk-Averse and Soft-Robust methods RASR. We show that when the risk-aversion
is defined using either EVaR or the entropic risk, the optimal policy in RASR
can be computed efficiently using a new dynamic program formulation with a
time-dependent risk level. As a result, the optimal risk-averse policies are
deterministic but time-dependent, even in the infinite-horizon discounted
setting. We also show that particular RASR objectives reduce to risk-averse RL
with mean posterior transition probabilities. Our empirical results show that
our new algorithms consistently mitigate uncertainty as measured by EVaR and
other standard risk measures.",2209.04067v2,https://arxiv.org/pdf/2209.04067v2
"Unraveling the Connections between Privacy and Certified Robustness in
  Federated Learning Against Poisoning Attacks","Chulin Xie, Yunhui Long, Pin-Yu Chen, Qinbin Li, Arash Nourian, Sanmi Koyejo, Bo Li","Federated learning (FL) provides an efficient paradigm to jointly train a
global model leveraging data from distributed users. As local training data
comes from different users who may not be trustworthy, several studies have
shown that FL is vulnerable to poisoning attacks. Meanwhile, to protect the
privacy of local users, FL is usually trained in a differentially private way
(DPFL). Thus, in this paper, we ask: What are the underlying connections
between differential privacy and certified robustness in FL against poisoning
attacks? Can we leverage the innate privacy property of DPFL to provide
certified robustness for FL? Can we further improve the privacy of FL to
improve such robustness certification? We first investigate both user-level and
instance-level privacy of FL and provide formal privacy analysis to achieve
improved instance-level privacy. We then provide two robustness certification
criteria: certified prediction and certified attack inefficacy for DPFL on both
user and instance levels. Theoretically, we provide the certified robustness of
DPFL based on both criteria given a bounded number of adversarial users or
instances. Empirically, we conduct extensive experiments to verify our theories
under a range of poisoning attacks on different datasets. We find that
increasing the level of privacy protection in DPFL results in stronger
certified attack inefficacy; however, it does not necessarily lead to a
stronger certified prediction. Thus, achieving the optimal certified prediction
requires a proper balance between privacy and utility loss.",2209.04030v3,https://arxiv.org/pdf/2209.04030v3
"Improved Robust Algorithms for Learning with Discriminative Feature
  Feedback",Sivan Sabato,"Discriminative Feature Feedback is a setting proposed by Dastupta et al.
(2018), which provides a protocol for interactive learning based on feature
explanations that are provided by a human teacher. The features distinguish
between the labels of pairs of possibly similar instances. That work has shown
that learning in this model can have considerable statistical and computational
advantages over learning in standard label-based interactive learning models.
  In this work, we provide new robust interactive learning algorithms for the
Discriminative Feature Feedback model, with mistake bounds that are
significantly lower than those of previous robust algorithms for this setting.
In the adversarial setting, we reduce the dependence on the number of protocol
exceptions from quadratic to linear. In addition, we provide an algorithm for a
slightly more restricted model, which obtains an even smaller mistake bound for
large models with many exceptions.
  In the stochastic setting, we provide the first algorithm that converges to
the exception rate with a polynomial sample complexity. Our algorithm and
analysis for the stochastic setting involve a new construction that we call
Feature Influence, which may be of wider applicability.",2209.03753v3,https://arxiv.org/pdf/2209.03753v3
"A Data-dependent Approach for High Dimensional (Robust) Wasserstein
  Alignment","Hu Ding, Wenjie Liu, Mingquan Ye","Many real-world problems can be formulated as the alignment between two
geometric patterns. Previously, a great amount of research focus on the
alignment of 2D or 3D patterns in the field of computer vision. Recently, the
alignment problem in high dimensions finds several novel applications in
practice. However, the research is still rather limited in the algorithmic
aspect. To the best of our knowledge, most existing approaches are just simple
extensions of their counterparts for 2D and 3D cases, and often suffer from the
issues such as high computational complexities. In this paper, we propose an
effective framework to compress the high dimensional geometric patterns. Any
existing alignment method can be applied to the compressed geometric patterns
and the time complexity can be significantly reduced. Our idea is inspired by
the observation that high dimensional data often has a low intrinsic dimension.
Our framework is a ``data-dependent'' approach that has the complexity
depending on the intrinsic dimension of the input data. Our experimental
results reveal that running the alignment algorithm on compressed patterns can
achieve similar qualities, comparing with the results on the original patterns,
but the runtimes (including the times cost for compression) are substantially
lower.",2209.02905v2,https://arxiv.org/pdf/2209.02905v2
"Improving the Accuracy and Robustness of CNNs Using a Deep CCA Neural
  Data Regularizer","Cassidy Pirlot, Richard C. Gerum, Cory Efird, Joel Zylberberg, Alona Fyshe","As convolutional neural networks (CNNs) become more accurate at object
recognition, their representations become more similar to the primate visual
system. This finding has inspired us and other researchers to ask if the
implication also runs the other way: If CNN representations become more
brain-like, does the network become more accurate? Previous attempts to address
this question showed very modest gains in accuracy, owing in part to
limitations of the regularization method. To overcome these limitations, we
developed a new neural data regularizer for CNNs that uses Deep Canonical
Correlation Analysis (DCCA) to optimize the resemblance of the CNN's image
representations to that of the monkey visual cortex. Using this new neural data
regularizer, we see much larger performance gains in both classification
accuracy and within-super-class accuracy, as compared to the previous
state-of-the-art neural data regularizers. These networks are also more robust
to adversarial attacks than their unregularized counterparts. Together, these
results confirm that neural data regularization can push CNN performance
higher, and introduces a new method that obtains a larger performance boost.",2209.02582v1,https://arxiv.org/pdf/2209.02582v1
"Robust and Efficient Imbalanced Positive-Unlabeled Learning with
  Self-supervision","Emilio Dorigatti, Jonas Schweisthal, Bernd Bischl, Mina Rezaei","Learning from positive and unlabeled (PU) data is a setting where the learner
only has access to positive and unlabeled samples while having no information
on negative examples. Such PU setting is of great importance in various tasks
such as medical diagnosis, social network analysis, financial markets analysis,
and knowledge base completion, which also tend to be intrinsically imbalanced,
i.e., where most examples are actually negatives. Most existing approaches for
PU learning, however, only consider artificially balanced datasets and it is
unclear how well they perform in the realistic scenario of imbalanced and
long-tail data distribution. This paper proposes to tackle this challenge via
robust and efficient self-supervised pretraining. However, training
conventional self-supervised learning methods when applied with highly
imbalanced PU distribution needs better reformulation. In this paper, we
present \textit{ImPULSeS}, a unified representation learning framework for
\underline{Im}balanced \underline{P}ositive \underline{U}nlabeled
\underline{L}earning leveraging \underline{Se}lf-\underline{S}upervised debiase
pre-training. ImPULSeS uses a generic combination of large-scale unsupervised
learning with debiased contrastive loss and additional reweighted PU loss. We
performed different experiments across multiple datasets to show that ImPULSeS
is able to halve the error rate of the previous state-of-the-art, even compared
with previous methods that are given the true prior. Moreover, our method
showed increased robustness to prior misspecification and superior performance
even when pretraining was performed on an unrelated dataset. We anticipate such
robustness and efficiency will make it much easier for practitioners to obtain
excellent results on other PU datasets of interest. The source code is
available at \url{https://github.com/JSchweisthal/ImPULSeS}",2209.02459v1,https://arxiv.org/pdf/2209.02459v1
"A Robust Learning Methodology for Uncertainty-aware Scientific Machine
  Learning models","Erbet Costa Almeida, Carine de Menezes Rebello, Marcio Fontana, Leizer Schnitman, Idelfonso Bessa dos Reis Nogueira","Robust learning is an important issue in Scientific Machine Learning (SciML).
There are several works in the literature addressing this topic. However, there
is an increasing demand for methods that can simultaneously consider all the
different uncertainty components involved in SciML model identification. Hence,
this work proposes a comprehensive methodology for uncertainty evaluation of
the SciML that also considers several possible sources of uncertainties
involved in the identification process. The uncertainties considered in the
proposed method are the absence of theory and causal models, the sensitiveness
to data corruption or imperfection, and the computational effort. Therefore, it
was possible to provide an overall strategy for the uncertainty-aware models in
the SciML field. The methodology is validated through a case study, developing
a Soft Sensor for a polymerization reactor. The results demonstrated that the
identified Soft Sensor are robust for uncertainties, corroborating with the
consistency of the proposed approach.",2209.01900v1,https://arxiv.org/pdf/2209.01900v1
"""Is your explanation stable?"": A Robustness Evaluation Framework for
  Feature Attribution","Yuyou Gan, Yuhao Mao, Xuhong Zhang, Shouling Ji, Yuwen Pu, Meng Han, Jianwei Yin, Ting Wang","Understanding the decision process of neural networks is hard. One vital
method for explanation is to attribute its decision to pivotal features.
Although many algorithms are proposed, most of them solely improve the
faithfulness to the model. However, the real environment contains many random
noises, which may leads to great fluctuations in the explanations. More
seriously, recent works show that explanation algorithms are vulnerable to
adversarial attacks. All of these make the explanation hard to trust in real
scenarios.
  To bridge this gap, we propose a model-agnostic method \emph{Median Test for
Feature Attribution} (MeTFA) to quantify the uncertainty and increase the
stability of explanation algorithms with theoretical guarantees. MeTFA has the
following two functions: (1) examine whether one feature is significantly
important or unimportant and generate a MeTFA-significant map to visualize the
results; (2) compute the confidence interval of a feature attribution score and
generate a MeTFA-smoothed map to increase the stability of the explanation.
Experiments show that MeTFA improves the visual quality of explanations and
significantly reduces the instability while maintaining the faithfulness. To
quantitatively evaluate the faithfulness of an explanation under different
noise settings, we further propose several robust faithfulness metrics.
Experiment results show that the MeTFA-smoothed explanation can significantly
increase the robust faithfulness. In addition, we use two scenarios to show
MeTFA's potential in the applications. First, when applied to the SOTA
explanation method to locate context bias for semantic segmentation models,
MeTFA-significant explanations use far smaller regions to maintain 99\%+
faithfulness. Second, when tested with different explanation-oriented attacks,
MeTFA can help defend vanilla, as well as adaptive, adversarial attacks against
explanations.",2209.01782v1,https://arxiv.org/pdf/2209.01782v1
Phishing URL Detection: A Network-based Approach Robust to Evasion,"Taeri Kim, Noseong Park, Jiwon Hong, Sang-Wook Kim","Many cyberattacks start with disseminating phishing URLs. When clicking these
phishing URLs, the victim's private information is leaked to the attacker.
There have been proposed several machine learning methods to detect phishing
URLs. However, it still remains under-explored to detect phishing URLs with
evasion, i.e., phishing URLs that pretend to be benign by manipulating
patterns. In many cases, the attacker i) reuses prepared phishing web pages
because making a completely brand-new set costs non-trivial expenses, ii)
prefers hosting companies that do not require private information and are
cheaper than others, iii) prefers shared hosting for cost efficiency, and iv)
sometimes uses benign domains, IP addresses, and URL string patterns to evade
existing detection methods. Inspired by those behavioral characteristics, we
present a network-based inference method to accurately detect phishing URLs
camouflaged with legitimate patterns, i.e., robust to evasion. In the network
approach, a phishing URL will be still identified as phishy even after evasion
unless a majority of its neighbors in the network are evaded at the same time.
Our method consistently shows better detection performance throughout various
experimental tests than state-of-the-art methods, e.g., F-1 of 0.89 for our
method vs. 0.84 for the best feature-based method.",2209.01454v1,https://arxiv.org/pdf/2209.01454v1
Noise-Robust Bidirectional Learning with Dynamic Sample Reweighting,"Chen-Chen Zong, Zheng-Tao Cao, Hong-Tao Guo, Yun Du, Ming-Kun Xie, Shao-Yuan Li, Sheng-Jun Huang","Deep neural networks trained with standard cross-entropy loss are more prone
to memorize noisy labels, which degrades their performance. Negative learning
using complementary labels is more robust when noisy labels intervene but with
an extremely slow model convergence speed. In this paper, we first introduce a
bidirectional learning scheme, where positive learning ensures convergence
speed while negative learning robustly copes with label noise. Further, a
dynamic sample reweighting strategy is proposed to globally weaken the effect
of noise-labeled samples by exploiting the excellent discriminatory ability of
negative learning on the sample probability distribution. In addition, we
combine self-distillation to further improve the model performance. The code is
available at \url{https://github.com/chenchenzong/BLDR}.",2209.01334v1,https://arxiv.org/pdf/2209.01334v1
Impact of Colour Variation on Robustness of Deep Neural Networks,"Chengyin Hu, Weiwen Shi","Deep neural networks (DNNs) have have shown state-of-the-art performance for
computer vision applications like image classification, segmentation and object
detection. Whereas recent advances have shown their vulnerability to manual
digital perturbations in the input data, namely adversarial attacks. The
accuracy of the networks is significantly affected by the data distribution of
their training dataset. Distortions or perturbations on color space of input
images generates out-of-distribution data, which make networks more likely to
misclassify them. In this work, we propose a color-variation dataset by
distorting their RGB color on a subset of the ImageNet with 27 different
combinations. The aim of our work is to study the impact of color variation on
the performance of DNNs. We perform experiments on several state-of-the-art DNN
architectures on the proposed dataset, and the result shows a significant
correlation between color variation and loss of accuracy. Furthermore, based on
the ResNet50 architecture, we demonstrate some experiments of the performance
of recently proposed robust training techniques and strategies, such as Augmix,
revisit, and free normalizer, on our proposed dataset. Experimental results
indicate that these robust training techniques can improve the robustness of
deep networks to color variation.",2209.02832v2,https://arxiv.org/pdf/2209.02832v2
"MA-RECON: Mask-aware deep-neural-network for robust fast MRI k-space
  interpolation","Nitzan Avidan, Moti Freiman","High-quality reconstruction of MRI images from under-sampled `k-space' data,
which is in the Fourier domain, is crucial for shortening MRI acquisition times
and ensuring superior temporal resolution. Over recent years, a wealth of deep
neural network (DNN) methods have emerged, aiming to tackle the complex,
ill-posed inverse problem linked to this process. However, their instability
against variations in the acquisition process and anatomical distribution
exposes a deficiency in the generalization of relevant physical models within
these DNN architectures. The goal of our work is to enhance the generalization
capabilities of DNN methods for k-space interpolation by introducing
`MA-RECON', an innovative mask-aware DNN architecture and associated training
method. Unlike preceding approaches, our `MA-RECON' architecture encodes not
only the observed data but also the under-sampling mask within the model
structure. It implements a tailored training approach that leverages data
generated with a variety of under-sampling masks to stimulate the model's
generalization of the under-sampled MRI reconstruction problem. Therefore,
effectively represents the associated inverse problem, akin to the classical
compressed sensing approach. The benefits of our MA-RECON approach were
affirmed through rigorous testing with the widely accessible fastMRI dataset.
Compared to standard DNN methods and DNNs trained with under-sampling mask
augmentation, our approach demonstrated superior generalization capabilities.
This resulted in a considerable improvement in robustness against variations in
both the acquisition process and anatomical distribution, especially in regions
with pathology. In conclusion, our mask-aware strategy holds promise for
enhancing the generalization capacity and robustness of DNN-based methodologies
for MRI reconstruction from undersampled k-space data.",2209.00462v2,https://arxiv.org/pdf/2209.00462v2
"Formalising the Robustness of Counterfactual Explanations for Neural
  Networks","Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni","The use of counterfactual explanations (CFXs) is an increasingly popular
explanation strategy for machine learning models. However, recent studies have
shown that these explanations may not be robust to changes in the underlying
model (e.g., following retraining), which raises questions about their
reliability in real-world applications. Existing attempts towards solving this
problem are heuristic, and the robustness to model changes of the resulting
CFXs is evaluated with only a small number of retrained models, failing to
provide exhaustive guarantees. To remedy this, we propose {\Delta}-robustness,
the first notion to formally and deterministically assess the robustness (to
model changes) of CFXs for neural networks. We introduce an abstraction
framework based on interval neural networks to verify the {\Delta}-robustness
of CFXs against a possibly infinite set of changes to the model parameters,
i.e., weights and biases. We then demonstrate the utility of this approach in
two distinct ways. First, we analyse the {\Delta}-robustness of a number of CFX
generation methods from the literature and show that they unanimously host
significant deficiencies in this regard. Second, we demonstrate how embedding
{\Delta}-robustness within existing methods can provide CFXs which are provably
robust.",2208.14878v3,https://arxiv.org/pdf/2208.14878v3
"Robustness of an Artificial Intelligence Solution for Diagnosis of
  Normal Chest X-Rays","Tom Dyer, Jordan Smith, Gaetan Dissez, Nicole Tay, Qaiser Malik, Tom Naunton Morgan, Paul Williams, Liliana Garcia-Mondragon, George Pearse, Simon Rasalingham","Purpose: Artificial intelligence (AI) solutions for medical diagnosis require
thorough evaluation to demonstrate that performance is maintained for all
patient sub-groups and to ensure that proposed improvements in care will be
delivered equitably. This study evaluates the robustness of an AI solution for
the diagnosis of normal chest X-rays (CXRs) by comparing performance across
multiple patient and environmental subgroups, as well as comparing AI errors
with those made by human experts.
  Methods: A total of 4,060 CXRs were sampled to represent a diverse dataset of
NHS patients and care settings. Ground-truth labels were assigned by a
3-radiologist panel. AI performance was evaluated against assigned labels and
sub-groups analysis was conducted against patient age and sex, as well as CXR
view, modality, device manufacturer and hospital site.
  Results: The AI solution was able to remove 18.5% of the dataset by
classification as High Confidence Normal (HCN). This was associated with a
negative predictive value (NPV) of 96.0%, compared to 89.1% for diagnosis of
normal scans by radiologists. In all AI false negative (FN) cases, a
radiologist was found to have also made the same error when compared to final
ground-truth labels. Subgroup analysis showed no statistically significant
variations in AI performance, whilst reduced normal classification was observed
in data from some hospital sites.
  Conclusion: We show the AI solution could provide meaningful workload savings
by diagnosis of 18.5% of scans as HCN with a superior NPV to human readers. The
AI solution is shown to perform well across patient subgroups and error cases
were shown to be subjective or subtle in nature.",2209.09204v1,https://arxiv.org/pdf/2209.09204v1
"GaitFi: Robust Device-Free Human Identification via WiFi and Vision
  Multimodal Learning","Lang Deng, Jianfei Yang, Shenghai Yuan, Han Zou, Chris Xiaoxuan Lu, Lihua Xie","As an important biomarker for human identification, human gait can be
collected at a distance by passive sensors without subject cooperation, which
plays an essential role in crime prevention, security detection and other human
identification applications. At present, most research works are based on
cameras and computer vision techniques to perform gait recognition. However,
vision-based methods are not reliable when confronting poor illuminations,
leading to degrading performances. In this paper, we propose a novel multimodal
gait recognition method, namely GaitFi, which leverages WiFi signals and videos
for human identification. In GaitFi, Channel State Information (CSI) that
reflects the multi-path propagation of WiFi is collected to capture human
gaits, while videos are captured by cameras. To learn robust gait information,
we propose a Lightweight Residual Convolution Network (LRCN) as the backbone
network, and further propose the two-stream GaitFi by integrating WiFi and
vision features for the gait retrieval task. The GaitFi is trained by the
triplet loss and classification loss on different levels of features. Extensive
experiments are conducted in the real world, which demonstrates that the GaitFi
outperforms state-of-the-art gait recognition methods based on single WiFi or
camera, achieving 94.2% for human identification tasks of 12 subjects.",2208.14326v1,https://arxiv.org/pdf/2208.14326v1
Robustness and invariance properties of image classifiers,Apostolos Modas,"Deep neural networks have achieved impressive results in many image
classification tasks. However, since their performance is usually measured in
controlled settings, it is important to ensure that their decisions remain
correct when deployed in noisy environments. In fact, deep networks are not
robust to a large variety of semantic-preserving image modifications, even to
imperceptible image changes known as adversarial perturbations. The poor
robustness of image classifiers to small data distribution shifts raises
serious concerns regarding their trustworthiness. To build reliable machine
learning models, we must design principled methods to analyze and understand
the mechanisms that shape robustness and invariance. This is exactly the focus
of this thesis.
  First, we study the problem of computing sparse adversarial perturbations. We
exploit the geometry of the decision boundaries of image classifiers for
computing sparse perturbations very fast, and reveal a qualitative connection
between adversarial examples and the data features that image classifiers
learn. Then, to better understand this connection, we propose a geometric
framework that connects the distance of data samples to the decision boundary,
with the features existing in the data. We show that deep classifiers have a
strong inductive bias towards invariance to non-discriminative features, and
that adversarial training exploits this property to confer robustness. Finally,
we focus on the challenging problem of generalization to unforeseen corruptions
of the data, and we propose a novel data augmentation scheme for achieving
state-of-the-art robustness to common corruptions of the images.
  Overall, our results contribute to the understanding of the fundamental
mechanisms of deep image classifiers, and pave the way for building more
reliable machine learning systems that can be deployed in real-world
environments.",2209.02408v1,https://arxiv.org/pdf/2209.02408v1
"DR-DSGD: A Distributionally Robust Decentralized Learning Algorithm over
  Graphs","Chaouki Ben Issaid, Anis Elgabli, Mehdi Bennis","In this paper, we propose to solve a regularized distributionally robust
learning problem in the decentralized setting, taking into account the data
distribution shift. By adding a Kullback-Liebler regularization function to the
robust min-max optimization problem, the learning problem can be reduced to a
modified robust minimization problem and solved efficiently. Leveraging the
newly formulated optimization problem, we propose a robust version of
Decentralized Stochastic Gradient Descent (DSGD), coined Distributionally
Robust Decentralized Stochastic Gradient Descent (DR-DSGD). Under some mild
assumptions and provided that the regularization parameter is larger than one,
we theoretically prove that DR-DSGD achieves a convergence rate of
$\mathcal{O}\left(1/\sqrt{KT} + K/T\right)$, where $K$ is the number of devices
and $T$ is the number of iterations. Simulation results show that our proposed
algorithm can improve the worst distribution test accuracy by up to $10\%$.
Moreover, DR-DSGD is more communication-efficient than DSGD since it requires
fewer communication rounds (up to $20$ times less) to achieve the same worst
distribution test accuracy target. Furthermore, the conducted experiments
reveal that DR-DSGD results in a fairer performance across devices in terms of
test accuracy.",2208.13810v2,https://arxiv.org/pdf/2208.13810v2
"Shaken, and Stirred: Long-Range Dependencies Enable Robust Outlier
  Detection with PixelCNN++","Barath Mohan Umapathi, Kushal Chauhan, Pradeep Shenoy, Devarajan Sridharan","Reliable outlier detection is critical for real-world deployment of deep
learning models. Although extensively studied, likelihoods produced by deep
generative models have been largely dismissed as being impractical for outlier
detection. First, deep generative model likelihoods are readily biased by
low-level input statistics. Second, many recent solutions for correcting these
biases are computationally expensive, or do not generalize well to complex,
natural datasets. Here, we explore outlier detection with a state-of-the-art
deep autoregressive model: PixelCNN++. We show that biases in PixelCNN++
likelihoods arise primarily from predictions based on local dependencies. We
propose two families of bijective transformations -- ``stirring'' and
``shaking'' -- which ameliorate low-level biases and isolate the contribution
of long-range dependencies to PixelCNN++ likelihoods. These transformations are
inexpensive and readily computed at evaluation time. We test our approaches
extensively with five grayscale and six natural image datasets and show that
they achieve or exceed state-of-the-art outlier detection, particularly on
datasets with complex, natural images. We also show that our solutions work
well with other types of generative models (generative flows and variational
autoencoders) and that their efficacy is governed by each model's reliance on
local dependencies. In sum, lightweight remedies suffice to achieve robust
outlier detection on image data with deep generative models.",2208.13579v2,https://arxiv.org/pdf/2208.13579v2
"AutoQML: Automatic Generation and Training of Robust Quantum-Inspired
  Classifiers by Using Genetic Algorithms on Grayscale Images","Sergio Altares-López, Juan José García-Ripoll, Angela Ribeiro","We propose a new hybrid system for automatically generating and training
quantum-inspired classifiers on grayscale images by using multiobjective
genetic algorithms. We define a dynamic fitness function to obtain the smallest
possible circuit and highest accuracy on unseen data, ensuring that the
proposed technique is generalizable and robust. We minimize the complexity of
the generated circuits in terms of the number of entanglement gates by
penalizing their appearance. We reduce the size of the images with two
dimensionality reduction approaches: principal component analysis (PCA), which
is encoded in the individual for optimization purpose, and a small
convolutional autoencoder (CAE). These two methods are compared with one
another and with a classical nonlinear approach to understand their behaviors
and to ensure that the classification ability is due to the quantum circuit and
not the preprocessing technique used for dimensionality reduction.",2208.13246v1,https://arxiv.org/pdf/2208.13246v1
"Adversarial Robustness for Tabular Data through Cost and Utility
  Awareness","Klim Kireev, Bogdan Kulynych, Carmela Troncoso","Many safety-critical applications of machine learning, such as fraud or abuse
detection, use data in tabular domains. Adversarial examples can be
particularly damaging for these applications. Yet, existing works on
adversarial robustness primarily focus on machine-learning models in image and
text domains. We argue that, due to the differences between tabular data and
images or text, existing threat models are not suitable for tabular domains.
These models do not capture that the costs of an attack could be more
significant than imperceptibility, or that the adversary could assign different
values to the utility obtained from deploying different adversarial examples.
We demonstrate that, due to these differences, the attack and defense methods
used for images and text cannot be directly applied to tabular settings. We
address these issues by proposing new cost and utility-aware threat models that
are tailored to the adversarial capabilities and constraints of attackers
targeting tabular domains. We introduce a framework that enables us to design
attack and defense mechanisms that result in models protected against cost and
utility-aware adversaries, for example, adversaries constrained by a certain
financial budget. We show that our approach is effective on three datasets
corresponding to applications for which adversarial examples can have economic
and social implications.",2208.13058v2,https://arxiv.org/pdf/2208.13058v2
BOBA: Byzantine-Robust Federated Learning with Label Skewness,"Wenxuan Bao, Jun Wu, Jingrui He","In federated learning, most existing robust aggregation rules (AGRs) combat
Byzantine attacks in the IID setting, where client data is assumed to be
independent and identically distributed. In this paper, we address label
skewness, a more realistic and challenging non-IID setting, where each client
only has access to a few classes of data. In this setting, state-of-the-art
AGRs suffer from selection bias, leading to significant performance drop for
particular classes; they are also more vulnerable to Byzantine attacks due to
the increased variation among gradients of honest clients. To address these
limitations, we propose an efficient two-stage method named BOBA.
Theoretically, we prove the convergence of BOBA with an error of the optimal
order. Our empirical evaluations demonstrate BOBA's superior unbiasedness and
robustness across diverse models and datasets when compared to various
baselines. Our code is available at https://github.com/baowenxuan/BOBA .",2208.12932v2,https://arxiv.org/pdf/2208.12932v2
Remote Work Optimization with Robust Multi-channel Graph Neural Networks,"Qinyi Zhu, Liang Wu, Qi Guo, Liangjie Hong","The spread of COVID-19 leads to the global shutdown of many corporate
offices, and encourages companies to open more opportunities that allow
employees to work from a remote location. As the workplace type expands from
onsite offices to remote areas, an emerging challenge for an online hiring
marketplace is how these remote opportunities and user intentions to work
remotely can be modeled and matched without prior information. Despite the
unprecedented amount of remote jobs posted amid COVID-19, there is no existing
approach that can be directly applied.
  Introducing a brand new workplace type naturally leads to the cold-start
problem, which is particularly more common for less active job seekers. It is
challenging, if not impossible, to onboard a new workplace type for any
predictive model if existing information sources can provide little information
related to a new category of jobs, including data from resumes and job
descriptions. Hence, in this work, we aim to propose a principled approach that
jointly models the remoteness of job seekers and job opportunities with limited
information, which also suffices the needs of web-scale applications. Existing
research on the emerging type of remote workplace mainly focuses on qualitative
studies, and classic predictive modeling approaches are inapplicable
considering the problem of cold-start and information scarcity. We precisely
try to close this gap with a novel graph neural architecture. Extensive
experiments on large-scale data from real-world applications have been
conducted to validate the superiority of the proposed approach over competitive
baselines. The improvement may translate to more rapid onboarding of the new
workplace type that can benefit job seekers who are interested in working
remotely.",2209.03150v1,https://arxiv.org/pdf/2209.03150v1
"Take One Gram of Neural Features, Get Enhanced Group Robustness","Simon Roburin, Charles Corbière, Gilles Puy, Nicolas Thome, Matthieu Aubry, Renaud Marlet, Patrick Pérez","Predictive performance of machine learning models trained with empirical risk
minimization (ERM) can degrade considerably under distribution shifts. The
presence of spurious correlations in training datasets leads ERM-trained models
to display high loss when evaluated on minority groups not presenting such
correlations. Extensive attempts have been made to develop methods improving
worst-group robustness. However, they require group information for each
training input or at least, a validation set with group labels to tune their
hyperparameters, which may be expensive to get or unknown a priori. In this
paper, we address the challenge of improving group robustness without group
annotation during training or validation. To this end, we propose to partition
the training dataset into groups based on Gram matrices of features extracted
by an ``identification'' model and to apply robust optimization based on these
pseudo-groups. In the realistic context where no group labels are available,
our experiments show that our approach not only improves group robustness over
ERM but also outperforms all recent baselines",2208.12625v2,https://arxiv.org/pdf/2208.12625v2
"Lower Difficulty and Better Robustness: A Bregman Divergence Perspective
  for Adversarial Training","Zihui Wu, Haichang Gao, Bingqian Zhou, Xiaoyan Guo, Shudong Zhang","In this paper, we investigate on improving the adversarial robustness
obtained in adversarial training (AT) via reducing the difficulty of
optimization. To better study this problem, we build a novel Bregman divergence
perspective for AT, in which AT can be viewed as the sliding process of the
training data points on the negative entropy curve. Based on this perspective,
we analyze the learning objectives of two typical AT methods, i.e., PGD-AT and
TRADES, and we find that the optimization process of TRADES is easier than
PGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function
of entropy in TRADES, and we find that models with high entropy can be better
robustness learners. Inspired by the above findings, we propose two methods,
i.e., FAIT and MER, which can both not only reduce the difficulty of
optimization under the 10-step PGD adversaries, but also provide better
robustness. Our work suggests that reducing the difficulty of optimization
under the 10-step PGD adversaries is a promising approach for enhancing the
adversarial robustness in AT.",2208.12511v3,https://arxiv.org/pdf/2208.12511v3
"Robust Prototypical Few-Shot Organ Segmentation with Regularized
  Neural-ODEs","Prashant Pandey, Mustafa Chasmai, Tanuj Sur, Brejesh Lall","Despite the tremendous progress made by deep learning models in image
semantic segmentation, they typically require large annotated examples, and
increasing attention is being diverted to problem settings like Few-Shot
Learning (FSL) where only a small amount of annotation is needed for
generalisation to novel classes. This is especially seen in medical domains
where dense pixel-level annotations are expensive to obtain. In this paper, we
propose Regularized Prototypical Neural Ordinary Differential Equation
(R-PNODE), a method that leverages intrinsic properties of Neural-ODEs,
assisted and enhanced by additional cluster and consistency losses to perform
Few-Shot Segmentation (FSS) of organs. R-PNODE constrains support and query
features from the same classes to lie closer in the representation space
thereby improving the performance over the existing Convolutional Neural
Network (CNN) based FSS methods. We further demonstrate that while many
existing Deep CNN based methods tend to be extremely vulnerable to adversarial
attacks, R-PNODE exhibits increased adversarial robustness for a wide array of
these attacks. We experiment with three publicly available multi-organ
segmentation datasets in both in-domain and cross-domain FSS settings to
demonstrate the efficacy of our method. In addition, we perform experiments
with seven commonly used adversarial attacks in various settings to demonstrate
R-PNODE's robustness. R-PNODE outperforms the baselines for FSS by significant
margins and also shows superior performance for a wide array of attacks varying
in intensity and design.",2208.12428v3,https://arxiv.org/pdf/2208.12428v3
"Toward Robust Graph Semi-Supervised Learning against Extreme Data
  Scarcity","Kaize Ding, Elnaz Nouri, Guoqing Zheng, Huan Liu, Ryen White","The success of graph neural networks on graph-based web mining highly relies
on abundant human-annotated data, which is laborious to obtain in practice.
When only few labeled nodes are available, how to improve their robustness is a
key to achieve replicable and sustainable graph semi-supervised learning.
Though self-training has been shown to be powerful for semi-supervised
learning, its application on graph-structured data may fail because (1) larger
receptive fields are not leveraged to capture long-range node interactions,
which exacerbates the difficulty of propagating feature-label patterns from
labeled nodes to unlabeled nodes; and (2) limited labeled data makes it
challenging to learn well-separated decision boundaries for different node
classes without explicitly capturing the underlying semantic structure. To
address the challenges of capturing informative structural and semantic
knowledge, we propose a new graph data augmentation framework, AGST (Augmented
Graph Self-Training), which is built with two new (i.e., structural and
semantic) augmentation modules on top of a decoupled GST backbone. In this
work, we investigate whether this novel framework can learn a robust graph
predictive model under the low-data context. We conduct comprehensive
evaluations on semi-supervised node classification under different scenarios of
limited labeled-node data. The experimental results demonstrate the unique
contributions of the novel data augmentation framework for node classification
with few labeled data.",2208.12422v2,https://arxiv.org/pdf/2208.12422v2
ARRID: ANN-based Rotordynamics for Robust and Integrated Design,"Soheyl Massoudi, Jürg Schiffmann","The purpose of this study is to introduce ANN-based software for the fast
evaluation of rotordynamics in the context of robust and integrated design. It
is based on a surrogate model made of ensembles of artificial neural networks
running in a Bokeh web application. The use of a surrogate model has sped up
the computation by three orders of magnitude compared to the current models.
ARRID offers fast performance information, including the effect of
manufacturing deviations. As such, it helps the designer to make optimal design
choices early in the design process. The designer can manipulate the parameters
of the design and the operating conditions to obtain performance information in
a matter of seconds.",2208.12640v1,https://arxiv.org/pdf/2208.12640v1
CNN-based Prediction of Network Robustness With Missing Edges,"Chengpei Wu, Yang Lou, Ruizi Wu, Wenwen Liu, Junli Li","Connectivity and controllability of a complex network are two important
issues that guarantee a networked system to function. Robustness of
connectivity and controllability guarantees the system to function properly and
stably under various malicious attacks. Evaluating network robustness using
attack simulations is time consuming, while the convolutional neural network
(CNN)-based prediction approach provides a cost-efficient method to approximate
the network robustness. In this paper, we investigate the performance of
CNN-based approaches for connectivity and controllability robustness
prediction, when partial network information is missing, namely the adjacency
matrix is incomplete. Extensive experimental studies are carried out. A
threshold is explored that if a total amount of more than 7.29\% information is
lost, the performance of CNN-based prediction will be significantly degenerated
for all cases in the experiments. Two scenarios of missing edge representations
are compared, 1) a missing edge is marked `no edge' in the input for
prediction, and 2) a missing edge is denoted using a special marker of
`unknown'. Experimental results reveal that the first representation is
misleading to the CNN-based predictors.",2208.11847v1,https://arxiv.org/pdf/2208.11847v1
"Too Fine or Too Coarse? The Goldilocks Composition of Data Complexity
  for Robust Left-Right Eye-Tracking Classifiers","Brian Xiang, Abdelrahman Abdelmonsef","The differences in distributional patterns between benchmark data and
real-world data have been one of the main challenges of using
electroencephalogram (EEG) signals for eye-tracking (ET) classification.
Therefore, increasing the robustness of machine learning models in predicting
eye-tracking positions from EEG data is integral for both research and consumer
use. Previously, we compared the performance of classifiers trained solely on
finer-grain data to those trained solely on coarse-grain. Results indicated
that despite the overall improvement in robustness, the performance of the
fine-grain trained models decreased, compared to coarse-grain trained models,
when the testing and training set contained the same distributional patterns
\cite{vectorbased}. This paper aims to address this case by training models
using datasets of mixed data complexity to determine the ideal distribution of
fine- and coarse-grain data. We train machine learning models utilizing a mixed
dataset composed of both fine- and coarse-grain data and then compare the
accuracies to models trained using solely fine- or coarse-grain data. For our
purposes, finer-grain data refers to data collected using more complex methods
whereas coarser-grain data refers to data collected using more simple methods.
We apply covariate distributional shifts to test for the susceptibility of each
training set. Our results indicated that the optimal training dataset for
EEG-ET classification is not composed of solely fine- or coarse-grain data, but
rather a mix of the two, leaning towards finer-grain.",2209.03761v1,https://arxiv.org/pdf/2209.03761v1
Robustness to Unbounded Smoothness of Generalized SignSGD,"Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, Zhenxun Zhuang","Traditional analyses in non-convex optimization typically rely on the
smoothness assumption, namely requiring the gradients to be Lipschitz. However,
recent evidence shows that this smoothness condition does not capture the
properties of some deep learning objective functions, including the ones
involving Recurrent Neural Networks and LSTMs. Instead, they satisfy a much
more relaxed condition, with potentially unbounded smoothness. Under this
relaxed assumption, it has been theoretically and empirically shown that the
gradient-clipped SGD has an advantage over the vanilla one. In this paper, we
show that clipping is not indispensable for Adam-type algorithms in tackling
such scenarios: we theoretically prove that a generalized SignSGD algorithm can
obtain similar convergence rates as SGD with clipping but does not need
explicit clipping at all. This family of algorithms on one end recovers SignSGD
and on the other end closely resembles the popular Adam algorithm. Our analysis
underlines the critical role that momentum plays in analyzing SignSGD-type and
Adam-type algorithms: it not only reduces the effects of noise, thus removing
the need for large mini-batch in previous analyses of SignSGD-type algorithms,
but it also substantially reduces the effects of unbounded smoothness and
gradient norms. We also compare these algorithms with popular optimizers on a
set of deep learning tasks, observing that we can match the performance of Adam
while beating the others.",2208.11195v1,https://arxiv.org/pdf/2208.11195v1
"Building Robust Machine Learning Models for Small Chemical Science Data:
  The Case of Shear Viscosity","Nikhil V. S. Avula, Shivanand K. Veesam, Sudarshan Behera, Sundaram Balasubramanian","Shear viscosity, though being a fundamental property of all liquids, is
computationally expensive to estimate from equilibrium molecular dynamics
simulations. Recently, Machine Learning (ML) methods have been used to augment
molecular simulations in many contexts, thus showing promise to estimate
viscosity too in a relatively inexpensive manner. However, ML methods face
significant challenges like overfitting when the size of the data set is small,
as is the case with viscosity. In this work, we train several ML models to
predict the shear viscosity of a Lennard-Jones (LJ) fluid, with particular
emphasis on addressing issues arising from a small data set. Specifically, the
issues related to model selection, performance estimation and uncertainty
quantification were investigated. First, we show that the widely used
performance estimation procedure of using a single unseen data set shows a wide
variability on small data sets. In this context, the common practice of using
Cross validation (CV) to select the hyperparameters (model selection) can be
adapted to estimate the generalization error (performance estimation) as well.
We compare two simple CV procedures for their ability to do both model
selection and performance estimation, and find that k-fold CV based procedure
shows a lower variance of error estimates. We discuss the role of performance
metrics in training and evaluation. Finally, Gaussian Process Regression (GPR)
and ensemble methods were used to estimate the uncertainty on individual
predictions. The uncertainty estimates from GPR were also used to construct an
applicability domain using which the ML models provided more reliable
predictions on another small data set generated in this work. Overall, the
procedures prescribed in this work, together, lead to robust ML models for
small data sets.",2208.10784v1,https://arxiv.org/pdf/2208.10784v1
"Predicting Query-Item Relationship using Adversarial Training and Robust
  Modeling Techniques",Min Seok Kim,"We present an effective way to predict search query-item relationship. We
combine pre-trained transformer and LSTM models, and increase model robustness
using adversarial training, exponential moving average, multi-sampled dropout,
and diversity based ensemble, to tackle an extremely difficult problem of
predicting against queries not seen before. All of our strategies focus on
increasing robustness of deep learning models and are applicable in any task
where deep learning models are used. Applying our strategies, we achieved 10th
place in KDD Cup 2022 Product Substitution Classification task.",2208.10751v1,https://arxiv.org/pdf/2208.10751v1
Estimation Contracts for Outlier-Robust Geometric Perception,Luca Carlone,"Outlier-robust estimation is a fundamental problem and has been extensively
investigated by statisticians and practitioners. The last few years have seen a
convergence across research fields towards ""algorithmic robust statistics"",
which focuses on developing tractable outlier-robust techniques for
high-dimensional estimation problems. Despite this convergence, research
efforts across fields have been mostly disconnected from one another. This
monograph bridges recent work on certifiable outlier-robust estimation for
geometric perception in robotics and computer vision with parallel work in
robust statistics. In particular, we adapt and extend recent results on robust
linear regression (applicable to the low-outlier regime with << 50% outliers)
and list-decodable regression (applicable to the high-outlier regime with >>
50% outliers) to the setup commonly found in robotics and vision, where (i)
variables (e.g., rotations, poses) belong to a non-convex domain, (ii)
measurements are vector-valued, and (iii) the number of outliers is not known a
priori. The emphasis here is on performance guarantees: rather than proposing
radically new algorithms, we provide conditions on the input measurements under
which modern estimation algorithms (possibly after small modifications) are
guaranteed to recover an estimate close to the ground truth in the presence of
outliers. These conditions are what we call an ""estimation contract"". Besides
the proposed extensions of existing results, we believe the main contributions
of this monograph are (i) to unify parallel research lines by pointing out
commonalities and differences, (ii) to introduce advanced material (e.g.,
sum-of-squares proofs) in an accessible and self-contained presentation for the
practitioner, and (iii) to point out a few immediate opportunities and open
questions in outlier-robust geometric perception.",2208.10521v2,https://arxiv.org/pdf/2208.10521v2
"BARReL: Bottleneck Attention for Adversarial Robustness in Vision-Based
  Reinforcement Learning","Eugene Bykovets, Yannick Metz, Mennatallah El-Assady, Daniel A. Keim, Joachim M. Buhmann","Robustness to adversarial perturbations has been explored in many areas of
computer vision. This robustness is particularly relevant in vision-based
reinforcement learning, as the actions of autonomous agents might be
safety-critic or impactful in the real world. We investigate the susceptibility
of vision-based reinforcement learning agents to gradient-based adversarial
attacks and evaluate a potential defense. We observe that Bottleneck Attention
Modules (BAM) included in CNN architectures can act as potential tools to
increase robustness against adversarial attacks. We show how learned attention
maps can be used to recover activations of a convolutional layer by restricting
the spatial activations to salient regions. Across a number of RL environments,
BAM-enhanced architectures show increased robustness during inference. Finally,
we discuss potential future research directions.",2208.10481v1,https://arxiv.org/pdf/2208.10481v1
"Quantifying probabilistic robustness of tree-based classifiers against
  natural distortions","Christoph Schweimer, Sebastian Scher","The concept of trustworthy AI has gained widespread attention lately. One of
the aspects relevant to trustworthy AI is robustness of ML models. In this
study, we show how to probabilistically quantify robustness against naturally
occurring distortions of input data for tree-based classifiers under the
assumption that the natural distortions can be described by multivariate
probability distributions that can be transformed to multivariate normal
distributions. The idea is to extract the decision rules of a trained
tree-based classifier, separate the feature space into non-overlapping regions
and determine the probability that a data sample with distortion returns its
predicted label. The approach is based on the recently introduced measure of
real-world-robustness, which works for all black box classifiers, but is only
an approximation and only works if the input dimension is not too high, whereas
our proposed method gives an exact measure.",2208.10354v3,https://arxiv.org/pdf/2208.10354v3
"Dynamic Adaptive Threshold based Learning for Noisy Annotations Robust
  Facial Expression Recognition","Darshan Gera, Naveen Siva Kumar Badveeti, Bobbili Veerendra Raj Kumar, S Balasubramanian","The real-world facial expression recognition (FER) datasets suffer from noisy
annotations due to crowd-sourcing, ambiguity in expressions, the subjectivity
of annotators and inter-class similarity. However, the recent deep networks
have strong capacity to memorize the noisy annotations leading to corrupted
feature embedding and poor generalization. To handle noisy annotations, we
propose a dynamic FER learning framework (DNFER) in which clean samples are
selected based on dynamic class specific threshold during training.
Specifically, DNFER is based on supervised training using selected clean
samples and unsupervised consistent training using all the samples. During
training, the mean posterior class probabilities of each mini-batch is used as
dynamic class-specific threshold to select the clean samples for supervised
training. This threshold is independent of noise rate and does not need any
clean data unlike other methods. In addition, to learn from all samples, the
posterior distributions between weakly-augmented image and strongly-augmented
image are aligned using an unsupervised consistency loss. We demonstrate the
robustness of DNFER on both synthetic as well as on real noisy annotated FER
datasets like RAFDB, FERPlus, SFEW and AffectNet.",2208.10221v1,https://arxiv.org/pdf/2208.10221v1
"MUDGUARD: Taming Malicious Majorities in Federated Learning using
  Privacy-Preserving Byzantine-Robust Clustering","Rui Wang, Xingkai Wang, Huanhuan Chen, Jérémie Decouchant, Stjepan Picek, Nikolaos Laoutaris, Kaitai Liang","Byzantine-robust Federated Learning (FL) aims to counter malicious clients
and train an accurate global model while maintaining an extremely low attack
success rate. Most existing systems, however, are only robust when most of the
clients are honest. FLTrust (NDSS '21) and Zeno++ (ICML '20) do not make such
an honest majority assumption but can only be applied to scenarios where the
server is provided with an auxiliary dataset used to filter malicious updates.
FLAME (USENIX '22) and EIFFeL (CCS '22) maintain the semi-honest majority
assumption to guarantee robustness and the confidentiality of updates. It is
therefore currently impossible to ensure Byzantine robustness and
confidentiality of updates without assuming a semi-honest majority. To tackle
this problem, we propose a novel Byzantine-robust and privacy-preserving FL
system, called MUDGUARD, that can operate under malicious minority \emph{or
majority} in both the server and client sides. Based on DBSCAN, we design a new
method for extracting features from model updates via pairwise adjusted cosine
similarity to boost the accuracy of the resulting clustering. To thwart attacks
from a malicious majority, we develop a method called \textit{Model
Segmentation}, that aggregates together only the updates from within a cluster,
sending the corresponding model only to the clients of the corresponding
cluster. The fundamental idea is that even if malicious clients are in their
majority, their poisoned updates cannot harm benign clients if they are
confined only within the malicious cluster. We also leverage multiple
cryptographic tools to conduct clustering without sacrificing training
correctness and updates confidentiality. We present a detailed security proof
and empirical evaluation along with a convergence analysis for MUDGUARD.",2208.10161v2,https://arxiv.org/pdf/2208.10161v2
"Robust Bayesian Nonnegative Matrix Factorization with Implicit
  Regularizers","Jun Lu, Christine P. Chai","We introduce a probabilistic model with implicit norm regularization for
learning nonnegative matrix factorization (NMF) that is commonly used for
predicting missing values and finding hidden patterns in the data, in which the
matrix factors are latent variables associated with each data dimension. The
nonnegativity constraint for the latent factors is handled by choosing priors
with support on the nonnegative subspace, e.g., exponential density or
distribution based on exponential function. Bayesian inference procedure based
on Gibbs sampling is employed. We evaluate the model on several real-world
datasets including Genomics of Drug Sensitivity in Cancer (GDSC $IC_{50}$) and
Gene body methylation with different sizes and dimensions, and show that the
proposed Bayesian NMF GL$_2^2$ and GL$_\infty$ models lead to robust
predictions for different data values and avoid overfitting compared with
competitive Bayesian NMF approaches.",2208.10053v1,https://arxiv.org/pdf/2208.10053v1
NOSMOG: Learning Noise-robust and Structure-aware MLPs on Graphs,"Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, Nitesh V. Chawla","While Graph Neural Networks (GNNs) have demonstrated their efficacy in
dealing with non-Euclidean structural data, they are difficult to be deployed
in real applications due to the scalability constraint imposed by multi-hop
data dependency. Existing methods attempt to address this scalability issue by
training multi-layer perceptrons (MLPs) exclusively on node content features
using labels derived from trained GNNs. Even though the performance of MLPs can
be significantly improved, two issues prevent MLPs from outperforming GNNs and
being used in practice: the ignorance of graph structural information and the
sensitivity to node feature noises. In this paper, we propose to learn
NOise-robust Structure-aware MLPs On Graphs (NOSMOG) to overcome the
challenges. Specifically, we first complement node content with position
features to help MLPs capture graph structural information. We then design a
novel representational similarity distillation strategy to inject structural
node similarities into MLPs. Finally, we introduce the adversarial feature
augmentation to ensure stable learning against feature noises and further
improve performance. Extensive experiments demonstrate that NOSMOG outperforms
GNNs and the state-of-the-art method in both transductive and inductive
settings across seven datasets, while maintaining a competitive inference
efficiency. Codes are available at https://github.com/meettyj/NOSMOG.",2208.10010v2,https://arxiv.org/pdf/2208.10010v2
"Provably Tightest Linear Approximation for Robustness Verification of
  Sigmoid-like Neural Networks","Zhaodi Zhang, Yiting Wu, Si Liu, Jing Liu, Min Zhang","The robustness of deep neural networks is crucial to modern AI-enabled
systems and should be formally verified. Sigmoid-like neural networks have been
adopted in a wide range of applications. Due to their non-linearity,
Sigmoid-like activation functions are usually over-approximated for efficient
verification, which inevitably introduces imprecision. Considerable efforts
have been devoted to finding the so-called tighter approximations to obtain
more precise verification results. However, existing tightness definitions are
heuristic and lack theoretical foundations. We conduct a thorough empirical
analysis of existing neuron-wise characterizations of tightness and reveal that
they are superior only on specific neural networks. We then introduce the
notion of network-wise tightness as a unified tightness definition and show
that computing network-wise tightness is a complex non-convex optimization
problem. We bypass the complexity from different perspectives via two
efficient, provably tightest approximations. The results demonstrate the
promising performance achievement of our approaches over state of the art: (i)
achieving up to 251.28% improvement to certified lower robustness bounds; and
(ii) exhibiting notably more precise verification results on convolutional
networks.",2208.09872v1,https://arxiv.org/pdf/2208.09872v1
Robust Tests in Online Decision-Making,"Gi-Soo Kim, Hyun-Joon Yang, Jane P. Kim","Bandit algorithms are widely used in sequential decision problems to maximize
the cumulative reward. One potential application is mobile health, where the
goal is to promote the user's health through personalized interventions based
on user specific information acquired through wearable devices. Important
considerations include the type of, and frequency with which data is collected
(e.g. GPS, or continuous monitoring), as such factors can severely impact app
performance and users' adherence. In order to balance the need to collect data
that is useful with the constraint of impacting app performance, one needs to
be able to assess the usefulness of variables. Bandit feedback data are
sequentially correlated, so traditional testing procedures developed for
independent data cannot apply. Recently, a statistical testing procedure was
developed for the actor-critic bandit algorithm. An actor-critic algorithm
maintains two separate models, one for the actor, the action selection policy,
and the other for the critic, the reward model. The performance of the
algorithm as well as the validity of the test are guaranteed only when the
critic model is correctly specified. However, misspecification is frequent in
practice due to incorrect functional form or missing covariates. In this work,
we propose a modified actor-critic algorithm which is robust to critic
misspecification and derive a novel testing procedure for the actor parameters
in this case.",2208.09819v1,https://arxiv.org/pdf/2208.09819v1
"Robust Node Classification on Graphs: Jointly from Bayesian Label
  Transition and Topology-based Label Propagation","Jun Zhuang, Mohammad Al Hasan","Node classification using Graph Neural Networks (GNNs) has been widely
applied in various real-world scenarios. However, in recent years, compelling
evidence emerges that the performance of GNN-based node classification may
deteriorate substantially by topological perturbation, such as random
connections or adversarial attacks. Various solutions, such as topological
denoising methods and mechanism design methods, have been proposed to develop
robust GNN-based node classifiers but none of these works can fully address the
problems related to topological perturbations. Recently, the Bayesian label
transition model is proposed to tackle this issue but its slow convergence may
lead to inferior performance. In this work, we propose a new label inference
model, namely LInDT, which integrates both Bayesian label transition and
topology-based label propagation for improving the robustness of GNNs against
topological perturbations. LInDT is superior to existing label transition
methods as it improves the label prediction of uncertain nodes by utilizing
neighborhood-based label propagation leading to better convergence of label
inference. Besides, LIndT adopts asymmetric Dirichlet distribution as a prior,
which also helps it to improve label inference. Extensive experiments on five
graph datasets demonstrate the superiority of LInDT for GNN-based node
classification under three scenarios of topological perturbations.",2208.09779v1,https://arxiv.org/pdf/2208.09779v1
"An End-to-End OCR Framework for Robust Arabic-Handwriting Recognition
  using a Novel Transformers-based Model and an Innovative 270 Million-Words
  Multi-Font Corpus of Classical Arabic with Diacritics","Aly Mostafa, Omar Mohamed, Ali Ashraf, Ahmed Elbehery, Salma Jamal, Anas Salah, Amr S. Ghoneim","This research is the second phase in a series of investigations on developing
an Optical Character Recognition (OCR) of Arabic historical documents and
examining how different modeling procedures interact with the problem. The
first research studied the effect of Transformers on our custom-built Arabic
dataset. One of the downsides of the first research was the size of the
training data, a mere 15000 images from our 30 million images, due to lack of
resources. Also, we add an image enhancement layer, time and space
optimization, and Post-Correction layer to aid the model in predicting the
correct word for the correct context. Notably, we propose an end-to-end text
recognition approach using Vision Transformers as an encoder, namely BEIT, and
vanilla Transformer as a decoder, eliminating CNNs for feature extraction and
reducing the model's complexity. The experiments show that our end-to-end model
outperforms Convolutions Backbones. The model attained a CER of 4.46%.",2208.11484v2,https://arxiv.org/pdf/2208.11484v2
"On Robustness in Nonconvex Optimization with Application to Defense
  Planning",Johannes O. Royset,"In the context of structured nonconvex optimization, we estimate the increase
in minimum value for a decision that is robust to parameter perturbations as
compared to the value of a nominal problem. The estimates rely on detailed
expressions for subgradients and local Lipschitz moduli of min-value functions
in nonconvex robust optimization and require only the solution of the nominal
problem. The theoretical results are illustrated by examples from military
operations research involving mixed-integer optimization models. Across 54
cases examined, the median error in estimating the increase in minimum value is
12%. Therefore, the derived expressions for subgradients and local Lipschitz
moduli may accurately inform analysts about the possibility of obtaining
cost-effective, parameter-robust decisions in nonconvex optimization.",2208.09725v2,https://arxiv.org/pdf/2208.09725v2
A Novel Plug-and-Play Approach for Adversarially Robust Generalization,"Deepak Maurya, Adarsh Barik, Jean Honorio","In this work, we propose a robust framework that employs adversarially robust
training to safeguard the machine learning models against perturbed testing
data. We achieve this by incorporating the worst-case additive adversarial
error within a fixed budget for each sample during model estimation. Our main
focus is to provide a plug-and-play solution that can be incorporated in the
existing machine learning algorithms with minimal changes. To that end, we
derive the ready-to-use solution for several widely used loss functions with a
variety of norm constraints on adversarial perturbation for various supervised
and unsupervised ML problems, including regression, classification, two-layer
neural networks, graphical models, and matrix completion. The solutions are
either in closed-form, 1-D optimization, semidefinite programming, difference
of convex programming or a sorting-based algorithm. Finally, we validate our
approach by showing significant performance improvement on real-world datasets
for supervised problems such as regression and classification, as well as for
unsupervised problems such as matrix completion and learning graphical models,
with very little computational overhead.",2208.09449v2,https://arxiv.org/pdf/2208.09449v2
"SAFARI: Versatile and Efficient Evaluations for Robustness of
  Interpretability","Wei Huang, Xingyu Zhao, Gaojie Jin, Xiaowei Huang","Interpretability of Deep Learning (DL) is a barrier to trustworthy AI.
Despite great efforts made by the Explainable AI (XAI) community, explanations
lack robustness -- indistinguishable input perturbations may lead to different
XAI results. Thus, it is vital to assess how robust DL interpretability is,
given an XAI method. In this paper, we identify several challenges that the
state-of-the-art is unable to cope with collectively: i) existing metrics are
not comprehensive; ii) XAI techniques are highly heterogeneous; iii)
misinterpretations are normally rare events. To tackle these challenges, we
introduce two black-box evaluation methods, concerning the worst-case
interpretation discrepancy and a probabilistic notion of how robust in general,
respectively. Genetic Algorithm (GA) with bespoke fitness function is used to
solve constrained optimisation for efficient worst-case evaluation. Subset
Simulation (SS), dedicated to estimate rare event probabilities, is used for
evaluating overall robustness. Experiments show that the accuracy, sensitivity,
and efficiency of our methods outperform the state-of-the-arts. Finally, we
demonstrate two applications of our methods: ranking robust XAI methods and
selecting training schemes to improve both classification and interpretation
robustness.",2208.09418v4,https://arxiv.org/pdf/2208.09418v4
"Real-Time Robust Video Object Detection System Against Physical-World
  Adversarial Attacks","Husheng Han, Xing Hu, Kaidi Xu, Pucheng Dang, Ying Wang, Yongwei Zhao, Zidong Du, Qi Guo, Yanzhi Yang, Tianshi Chen","DNN-based video object detection (VOD) powers autonomous driving and video
surveillance industries with rising importance and promising opportunities.
However, adversarial patch attack yields huge concern in live vision tasks
because of its practicality, feasibility, and powerful attack effectiveness.
This work proposes Themis, a software/hardware system to defend against
adversarial patches for real-time robust video object detection. We observe
that adversarial patches exhibit extremely localized superficial feature
importance in a small region with non-robust predictions, and thus propose the
adversarial region detection algorithm for adversarial effect elimination.
Themis also proposes a systematic design to efficiently support the algorithm
by eliminating redundant computations and memory traffics. Experimental results
show that the proposed methodology can effectively recover the system from the
adversarial attack with negligible hardware overhead.",2208.09195v1,https://arxiv.org/pdf/2208.09195v1
"RRWaveNet: A Compact End-to-End Multi-Scale Residual CNN for Robust PPG
  Respiratory Rate Estimation","Pongpanut Osathitporn, Guntitat Sawadwuthikul, Punnawish Thuwajit, Kawisara Ueafuea, Thee Mateepithaktham, Narin Kunaseth, Tanut Choksatchawathi, Proadpran Punyabukkana, Emmanuel Mignot, Theerawit Wilaiprasitporn","Respiratory rate (RR) is an important biomarker as RR changes can reflect
severe medical events such as heart disease, lung disease, and sleep disorders.
Unfortunately, standard manual RR counting is prone to human error and cannot
be performed continuously. This study proposes a method for continuously
estimating RR, RRWaveNet. The method is a compact end-to-end deep learning
model which does not require feature engineering and can use low-cost raw
photoplethysmography (PPG) as input signal. RRWaveNet was tested
subject-independently and compared to baseline in four datasets (BIDMC,
CapnoBase, WESAD, and SensAI) and using three window sizes (16, 32, and 64
seconds). RRWaveNet outperformed current state-of-the-art methods with mean
absolute errors at optimal window size of 1.66 \pm 1.01, 1.59 \pm 1.08, 1.92
\pm 0.96 and 1.23 \pm 0.61 breaths per minute for each dataset. In remote
monitoring settings, such as in the WESAD and SensAI datasets, we apply
transfer learning to improve the performance using two other ICU datasets as
pretraining datasets, reducing the MAE by up to 21$\%$. This shows that this
model allows accurate and practical estimation of RR on affordable and wearable
devices. Our study also shows feasibility of remote RR monitoring in the
context of telemedicine and at home.",2208.08672v2,https://arxiv.org/pdf/2208.08672v2
"Enhancing Diffusion-Based Image Synthesis with Robust Classifier
  Guidance","Bahjat Kawar, Roy Ganz, Michael Elad","Denoising diffusion probabilistic models (DDPMs) are a recent family of
generative models that achieve state-of-the-art results. In order to obtain
class-conditional generation, it was suggested to guide the diffusion process
by gradients from a time-dependent classifier. While the idea is theoretically
sound, deep learning-based classifiers are infamously susceptible to
gradient-based adversarial attacks. Therefore, while traditional classifiers
may achieve good accuracy scores, their gradients are possibly unreliable and
might hinder the improvement of the generation results. Recent work discovered
that adversarially robust classifiers exhibit gradients that are aligned with
human perception, and these could better guide a generative process towards
semantically meaningful images. We utilize this observation by defining and
training a time-dependent adversarially robust classifier and use it as
guidance for a generative diffusion model. In experiments on the highly
challenging and diverse ImageNet dataset, our scheme introduces significantly
more intelligible intermediate gradients, better alignment with theoretical
findings, as well as improved generation results under several evaluation
metrics. Furthermore, we conduct an opinion survey whose findings indicate that
human raters prefer our method's results.",2208.08664v2,https://arxiv.org/pdf/2208.08664v2
Robust Causal Graph Representation Learning against Confounding Effects,"Hang Gao, Jiangmeng Li, Wenwen Qiang, Lingyu Si, Bing Xu, Changwen Zheng, Fuchun Sun","The prevailing graph neural network models have achieved significant progress
in graph representation learning. However, in this paper, we uncover an
ever-overlooked phenomenon: the pre-trained graph representation learning model
tested with full graphs underperforms the model tested with well-pruned graphs.
This observation reveals that there exist confounders in graphs, which may
interfere with the model learning semantic information, and current graph
representation learning methods have not eliminated their influence. To tackle
this issue, we propose Robust Causal Graph Representation Learning (RCGRL) to
learn robust graph representations against confounding effects. RCGRL
introduces an active approach to generate instrumental variables under
unconditional moment restrictions, which empowers the graph representation
learning model to eliminate confounders, thereby capturing discriminative
information that is causally related to downstream predictions. We offer
theorems and proofs to guarantee the theoretical effectiveness of the proposed
approach. Empirically, we conduct extensive experiments on a synthetic dataset
and multiple benchmark datasets. The results demonstrate that compared with
state-of-the-art methods, RCGRL achieves better prediction performance and
generalization ability.",2208.08584v2,https://arxiv.org/pdf/2208.08584v2
"Analyzing Robustness of End-to-End Neural Models for Automatic Speech
  Recognition","Goutham Rajendran, Wei Zou","We investigate robustness properties of pre-trained neural models for
automatic speech recognition. Real life data in machine learning is usually
very noisy and almost never clean, which can be attributed to various factors
depending on the domain, e.g. outliers, random noise and adversarial noise.
Therefore, the models we develop for various tasks should be robust to such
kinds of noisy data, which led to the thriving field of robust machine
learning. We consider this important issue in the setting of automatic speech
recognition. With the increasing popularity of pre-trained models, it's an
important question to analyze and understand the robustness of such models to
noise. In this work, we perform a robustness analysis of the pre-trained neural
models wav2vec2, HuBERT and DistilHuBERT on the LibriSpeech and TIMIT datasets.
We use different kinds of noising mechanisms and measure the model performances
as quantified by the inference time and the standard Word Error Rate metric. We
also do an in-depth layer-wise analysis of the wav2vec2 model when injecting
noise in between layers, enabling us to predict at a high level what each layer
learns. Finally for this model, we visualize the propagation of errors across
the layers and compare how it behaves on clean versus noisy data. Our
experiments conform the predictions of Pasad et al. [2021] and also raise
interesting directions for future work.",2208.08509v1,https://arxiv.org/pdf/2208.08509v1
"Two-Stage Robust and Sparse Distributed Statistical Inference for
  Large-Scale Data","Emadaldin Mozafari-Majd, Visa Koivunen","In this paper, we address the problem of conducting statistical inference in
settings involving large-scale data that may be high-dimensional and
contaminated by outliers. The high volume and dimensionality of the data
require distributed processing and storage solutions. We propose a two-stage
distributed and robust statistical inference procedures coping with
high-dimensional models by promoting sparsity. In the first stage, known as
model selection, relevant predictors are locally selected by applying robust
Lasso estimators to the distinct subsets of data. The variable selections from
each computation node are then fused by a voting scheme to find the sparse
basis for the complete data set. It identifies the relevant variables in a
robust manner. In the second stage, the developed statistically robust and
computationally efficient bootstrap methods are employed. The actual inference
constructs confidence intervals, finds parameter estimates and quantifies
standard deviation. Similar to stage 1, the results of local inference are
communicated to the fusion center and combined there. By using analytical
methods, we establish the favorable statistical properties of the robust and
computationally efficient bootstrap methods including consistency for a fixed
number of predictors, and robustness. The proposed two-stage robust and
distributed inference procedures demonstrate reliable performance and
robustness in variable selection, finding confidence intervals and bootstrap
approximations of standard deviations even when data is high-dimensional and
contaminated by outliers.",2208.08230v1,https://arxiv.org/pdf/2208.08230v1
On Establishing Robust Consistency in Answer Set Programs,"Andre Thevapalan, Gabriele Kern-Isberner","Answer set programs used in real-world applications often require that the
program is usable with different input data. This, however, can often lead to
contradictory statements and consequently to an inconsistent program. Causes
for potential contradictions in a program are conflicting rules. In this paper,
we show how to ensure that a program $\mathcal{P}$ remains non-contradictory
given any allowed set of such input data. For that, we introduce the notion of
conflict-resolving $\lambda$- extensions. A conflict-resolving
$\lambda$-extension for a conflicting rule $r$ is a set $\lambda$ of (default)
literals such that extending the body of $r$ by $\lambda$ resolves all
conflicts of $r$ at once. We investigate the properties that suitable
$\lambda$-extensions should possess and building on that, we develop a strategy
to compute all such conflict-resolving $\lambda$-extensions for each
conflicting rule in $\mathcal{P}$. We show that by implementing a conflict
resolution process that successively resolves conflicts using
$\lambda$-extensions eventually yields a program that remains non-contradictory
given any allowed set of input data.",2208.08157v1,https://arxiv.org/pdf/2208.08157v1
"Towards Generating Robust, Fair, and Emotion-Aware Explanations for
  Recommender Systems","Bingbing Wen, Yunhe Feng, Yongfeng Zhang, Chirag Shah","As recommender systems become increasingly sophisticated and complex, they
often suffer from lack of fairness and transparency. Providing robust and
unbiased explanations for recommendations has been drawing more and more
attention as it can help address these issues and improve trustworthiness and
informativeness of recommender systems. However, despite the fact that such
explanations are generated for humans who respond more strongly to messages
with appropriate emotions, there is a lack of consideration for emotions when
generating explanations for recommendations. Current explanation generation
models are found to exaggerate certain emotions without accurately capturing
the underlying tone or the meaning. In this paper, we propose a novel method
based on a multi-head transformer, called Emotion-aware Transformer for
Explainable Recommendation (EmoTER), to generate more robust, fair, and
emotion-enhanced explanations. To measure the linguistic quality and emotion
fairness of the generated explanations, we adopt both automatic text metrics
and human perceptions for evaluation. Experiments on three widely-used
benchmark datasets with multiple evaluation metrics demonstrate that EmoTER
consistently outperforms the existing state-of-the-art explanation generation
models in terms of text quality, explainability, and consideration for fairness
to emotion distribution. Implementation of EmoTER will be released as an
open-source toolkit to support further research.",2208.08017v1,https://arxiv.org/pdf/2208.08017v1
FedPerm: Private and Robust Federated Learning by Parameter Permutation,"Hamid Mozaffari, Virendra J. Marathe, Dave Dice","Federated Learning (FL) is a distributed learning paradigm that enables
mutually untrusting clients to collaboratively train a common machine learning
model. Client data privacy is paramount in FL. At the same time, the model must
be protected from poisoning attacks from adversarial clients. Existing
solutions address these two problems in isolation. We present FedPerm, a new FL
algorithm that addresses both these problems by combining a novel intra-model
parameter shuffling technique that amplifies data privacy, with Private
Information Retrieval (PIR) based techniques that permit cryptographic
aggregation of clients' model updates. The combination of these techniques
further helps the federation server constrain parameter updates from clients so
as to curtail effects of model poisoning attacks by adversarial clients. We
further present FedPerm's unique hyperparameters that can be used effectively
to trade off computation overheads with model utility. Our empirical evaluation
on the MNIST dataset demonstrates FedPerm's effectiveness over existing
Differential Privacy (DP) enforcement solutions in FL.",2208.07922v1,https://arxiv.org/pdf/2208.07922v1
"Generating a Terrain-Robustness Benchmark for Legged Locomotion: A
  Prototype via Terrain Authoring and Active Learning","Chong Zhang, Lizhi Yang","Terrain-aware locomotion has become an emerging topic in legged robotics.
However, it is hard to generate diverse, challenging, and realistic
unstructured terrains in simulation, which limits the way researchers evaluate
their locomotion policies. In this paper, we prototype the generation of a
terrain dataset via terrain authoring and active learning, and the learned
samplers can stably generate diverse high-quality terrains. We expect the
generated dataset to make a terrain-robustness benchmark for legged locomotion.
The dataset, the code implementation, and some policy evaluations are released
at https://bit.ly/3bn4j7f.",2208.07681v3,https://arxiv.org/pdf/2208.07681v3
"Efficient Multimodal Transformer with Dual-Level Feature Restoration for
  Robust Multimodal Sentiment Analysis","Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao","With the proliferation of user-generated online videos, Multimodal Sentiment
Analysis (MSA) has attracted increasing attention recently. Despite significant
progress, there are still two major challenges on the way towards robust MSA:
1) inefficiency when modeling cross-modal interactions in unaligned multimodal
data; and 2) vulnerability to random modality feature missing which typically
occurs in realistic settings. In this paper, we propose a generic and unified
framework to address them, named Efficient Multimodal Transformer with
Dual-Level Feature Restoration (EMT-DLFR). Concretely, EMT employs
utterance-level representations from each modality as the global multimodal
context to interact with local unimodal features and mutually promote each
other. It not only avoids the quadratic scaling cost of previous local-local
cross-modal interaction methods but also leads to better performance. To
improve model robustness in the incomplete modality setting, on the one hand,
DLFR performs low-level feature reconstruction to implicitly encourage the
model to learn semantic information from incomplete data. On the other hand, it
innovatively regards complete and incomplete data as two different views of one
sample and utilizes siamese representation learning to explicitly attract their
high-level representations. Comprehensive experiments on three popular datasets
demonstrate that our method achieves superior performance in both complete and
incomplete modality settings.",2208.07589v2,https://arxiv.org/pdf/2208.07589v2
Deletion Robust Non-Monotone Submodular Maximization over Matroids,"Paul Dütting, Federico Fusco, Silvio Lattanzi, Ashkan Norouzi-Fard, Morteza Zadimoghaddam","Maximizing a submodular function is a fundamental task in machine learning
and in this paper we study the deletion robust version of the problem under the
classic matroids constraint. Here the goal is to extract a small size summary
of the dataset that contains a high value independent set even after an
adversary deleted some elements. We present constant-factor approximation
algorithms, whose space complexity depends on the rank $k$ of the matroid and
the number $d$ of deleted elements. In the centralized setting we present a
$(4.597+O(\varepsilon))$-approximation algorithm with summary size $O(
\frac{k+d}{\varepsilon^2}\log \frac{k}{\varepsilon})$ that is improved to a
$(3.582+O(\varepsilon))$-approximation with $O(k + \frac{d}{\varepsilon^2}\log
\frac{k}{\varepsilon})$ summary size when the objective is monotone. In the
streaming setting we provide a $(9.435 + O(\varepsilon))$-approximation
algorithm with summary size and memory $O(k + \frac{d}{\varepsilon^2}\log
\frac{k}{\varepsilon})$; the approximation factor is then improved to
$(5.582+O(\varepsilon))$ in the monotone case.",2208.07582v1,https://arxiv.org/pdf/2208.07582v1
"An Overview and Prospective Outlook on Robust Training and Certification
  of Machine Learning Models","Brendon G. Anderson, Tanmay Gautam, Somayeh Sojoudi","In this discussion paper, we survey recent research surrounding robustness of
machine learning models. As learning algorithms become increasingly more
popular in data-driven control systems, their robustness to data uncertainty
must be ensured in order to maintain reliable safety-critical operations. We
begin by reviewing common formalisms for such robustness, and then move on to
discuss popular and state-of-the-art techniques for training robust machine
learning models as well as methods for provably certifying such robustness.
From this unification of robust machine learning, we identify and discuss
pressing directions for future research in the area.",2208.07464v2,https://arxiv.org/pdf/2208.07464v2
MENLI: Robust Evaluation Metrics from Natural Language Inference,"Yanran Chen, Steffen Eger","Recently proposed BERT-based evaluation metrics for text generation perform
well on standard benchmarks but are vulnerable to adversarial attacks, e.g.,
relating to information correctness. We argue that this stems (in part) from
the fact that they are models of semantic similarity. In contrast, we develop
evaluation metrics based on Natural Language Inference (NLI), which we deem a
more appropriate modeling. We design a preference-based adversarial attack
framework and show that our NLI based metrics are much more robust to the
attacks than the recent BERT-based metrics. On standard benchmarks, our NLI
based metrics outperform existing summarization metrics, but perform below SOTA
MT metrics. However, when combining existing metrics with our NLI metrics, we
obtain both higher adversarial robustness (15%-30%) and higher quality metrics
as measured on standard benchmarks (+5% to 30%).",2208.07316v5,https://arxiv.org/pdf/2208.07316v5
A Tool for Neural Network Global Robustness Certification and Training,"Zhilu Wang, Yixuan Wang, Feisi Fu, Ruochen Jiao, Chao Huang, Wenchao Li, Qi Zhu","With the increment of interest in leveraging machine learning technology in
safety-critical systems, the robustness of neural networks under external
disturbance receives more and more concerns. Global robustness is a robustness
property defined on the entire input domain. And a certified globally robust
network can ensure its robustness on any possible network input. However, the
state-of-the-art global robustness certification algorithm can only certify
networks with at most several thousand neurons. In this paper, we propose the
GPU-supported global robustness certification framework GROCET, which is more
efficient than the previous optimization-based certification approach.
Moreover, GROCET provides differentiable global robustness, which is leveraged
in the training of globally robust neural networks.",2208.07289v1,https://arxiv.org/pdf/2208.07289v1
"Occlusion-Robust Multi-Sensory Posture Estimation in Physical
  Human-Robot Interaction","Amir Yazdani, Roya Sabbagh Novin, Andrew Merryweather, Tucker Hermans","3D posture estimation is important in analyzing and improving ergonomics in
physical human-robot interaction and reducing the risk of musculoskeletal
disorders. Vision-based posture estimation approaches are prone to sensor and
model errors, as well as occlusion, while posture estimation solely from the
interacting robot's trajectory suffers from ambiguous solutions. To benefit
from the advantages of both approaches and improve upon their drawbacks, we
introduce a low-cost, non-intrusive, and occlusion-robust multi-sensory 3D
postural estimation algorithm in physical human-robot interaction. We use 2D
postures from OpenPose over a single camera, and the trajectory of the
interacting robot while the human performs a task. We model the problem as a
partially-observable dynamical system and we infer the 3D posture via a
particle filter. We present our work in teleoperation, but it can be
generalized to other applications of physical human-robot interaction. We show
that our multi-sensory system resolves human kinematic redundancy better than
posture estimation solely using OpenPose or posture estimation solely using the
robot's trajectory. This will increase the accuracy of estimated postures
compared to the gold-standard motion capture postures. Moreover, our approach
also performs better than other single sensory methods when postural assessment
using RULA assessment tool.",2208.06494v1,https://arxiv.org/pdf/2208.06494v1
Unifying Gradients to Improve Real-world Robustness for Deep Networks,"Yingwen Wu, Sizhe Chen, Kun Fang, Xiaolin Huang","The wide application of deep neural networks (DNNs) demands an increasing
amount of attention to their real-world robustness, i.e., whether a DNN resists
black-box adversarial attacks, among which score-based query attacks (SQAs) are
most threatening since they can effectively hurt a victim network with the only
access to model outputs. Defending against SQAs requires a slight but artful
variation of outputs due to the service purpose for users, who share the same
output information with SQAs. In this paper, we propose a real-world defense by
Unifying Gradients (UniG) of different data so that SQAs could only probe a
much weaker attack direction that is similar for different samples. Since such
universal attack perturbations have been validated as less aggressive than the
input-specific perturbations, UniG protects real-world DNNs by indicating
attackers a twisted and less informative attack direction. We implement UniG
efficiently by a Hadamard product module which is plug-and-play. According to
extensive experiments on 5 SQAs, 2 adaptive attacks and 7 defense baselines,
UniG significantly improves real-world robustness without hurting clean
accuracy on CIFAR10 and ImageNet. For instance, UniG maintains a model of
77.80% accuracy under 2500-query Square attack while the state-of-the-art
adversarially-trained model only has 67.34% on CIFAR10. Simultaneously, UniG
outperforms all compared baselines in terms of clean accuracy and achieves the
smallest modification of the model output. The code is released at
https://github.com/snowien/UniG-pytorch.",2208.06228v2,https://arxiv.org/pdf/2208.06228v2
"Region-Based Evidential Deep Learning to Quantify Uncertainty and
  Improve Robustness of Brain Tumor Segmentation","Hao Li, Yang Nan, Javier Del Ser, Guang Yang","Despite recent advances in the accuracy of brain tumor segmentation, the
results still suffer from low reliability and robustness. Uncertainty
estimation is an efficient solution to this problem, as it provides a measure
of confidence in the segmentation results. The current uncertainty estimation
methods based on quantile regression, Bayesian neural network, ensemble, and
Monte Carlo dropout are limited by their high computational cost and
inconsistency. In order to overcome these challenges, Evidential Deep Learning
(EDL) was developed in recent work but primarily for natural image
classification. In this paper, we proposed a region-based EDL segmentation
framework that can generate reliable uncertainty maps and robust segmentation
results. We used the Theory of Evidence to interpret the output of a neural
network as evidence values gathered from input features. Following Subjective
Logic, evidence was parameterized as a Dirichlet distribution, and predicted
probabilities were treated as subjective opinions. To evaluate the performance
of our model on segmentation and uncertainty estimation, we conducted
quantitative and qualitative experiments on the BraTS 2020 dataset. The results
demonstrated the top performance of the proposed method in quantifying
segmentation uncertainty and robustly segmenting tumors. Furthermore, our
proposed new framework maintained the advantages of low computational cost and
easy implementation and showed the potential for clinical application.",2208.06038v1,https://arxiv.org/pdf/2208.06038v1
"PointTree: Transformation-Robust Point Cloud Encoder with Relaxed K-D
  Trees","Jun-Kun Chen, Yu-Xiong Wang","Being able to learn an effective semantic representation directly on raw
point clouds has become a central topic in 3D understanding. Despite rapid
progress, state-of-the-art encoders are restrictive to canonicalized point
clouds, and have weaker than necessary performance when encountering geometric
transformation distortions. To overcome this challenge, we propose PointTree, a
general-purpose point cloud encoder that is robust to transformations based on
relaxed K-D trees. Key to our approach is the design of the division rule in
K-D trees by using principal component analysis (PCA). We use the structure of
the relaxed K-D tree as our computational graph, and model the features as
border descriptors which are merged with pointwise-maximum operation. In
addition to this novel architecture design, we further improve the robustness
by introducing pre-alignment -- a simple yet effective PCA-based normalization
scheme. Our PointTree encoder combined with pre-alignment consistently
outperforms state-of-the-art methods by large margins, for applications from
object classification to semantic segmentation on various transformed versions
of the widely-benchmarked datasets. Code and pre-trained models are available
at https://github.com/immortalCO/PointTree.",2208.05962v1,https://arxiv.org/pdf/2208.05962v1
"Distributionally Robust Model-Based Offline Reinforcement Learning with
  Near-Optimal Sample Complexity","Laixi Shi, Yuejie Chi","This paper concerns the central issues of model robustness and sample
efficiency in offline reinforcement learning (RL), which aims to learn to
perform decision making from history data without active exploration. Due to
uncertainties and variabilities of the environment, it is critical to learn a
robust policy -- with as few samples as possible -- that performs well even
when the deployed environment deviates from the nominal one used to collect the
history dataset. We consider a distributionally robust formulation of offline
RL, focusing on tabular robust Markov decision processes with an uncertainty
set specified by the Kullback-Leibler divergence in both finite-horizon and
infinite-horizon settings. To combat with sample scarcity, a model-based
algorithm that combines distributionally robust value iteration with the
principle of pessimism in the face of uncertainty is proposed, by penalizing
the robust value estimates with a carefully designed data-driven penalty term.
Under a mild and tailored assumption of the history dataset that measures
distribution shift without requiring full coverage of the state-action space,
we establish the finite-sample complexity of the proposed algorithms. We
further develop an information-theoretic lower bound, which suggests that
learning RMDPs is at least as hard as the standard MDPs when the uncertainty
level is sufficient small, and corroborates the tightness of our upper bound up
to polynomial factors of the (effective) horizon length for a range of
uncertainty levels. To the best our knowledge, this provides the first provably
near-optimal robust offline RL algorithm that learns under model uncertainty
and partial coverage.",2208.05767v4,https://arxiv.org/pdf/2208.05767v4
"Quality Not Quantity: On the Interaction between Dataset Design and
  Robustness of CLIP","Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, Ludwig Schmidt","Web-crawled datasets have enabled remarkable generalization capabilities in
recent image-text models such as CLIP (Contrastive Language-Image pre-training)
or Flamingo, but little is known about the dataset creation processes. In this
work, we introduce a testbed of six publicly available data sources - YFCC,
LAION, Conceptual Captions, WIT, RedCaps, Shutterstock - to investigate how
pre-training distributions induce robustness in CLIP. We find that the
performance of the pre-training data varies substantially across distribution
shifts, with no single data source dominating. Moreover, we systematically
study the interactions between these data sources and find that combining
multiple sources does not necessarily yield better models, but rather dilutes
the robustness of the best individual data source. We complement our empirical
findings with theoretical insights from a simple setting, where combining the
training data also results in diluted robustness. In addition, our theoretical
model provides a candidate explanation for the success of the CLIP-based data
filtering technique recently employed in the LAION dataset. Overall our results
demonstrate that simply gathering a large amount of data from the web is not
the most effective way to build a pre-training dataset for robust
generalization, necessitating further study into dataset design. Code is
available at https://github.com/mlfoundations/clip_quality_not_quantity.",2208.05516v4,https://arxiv.org/pdf/2208.05516v4
Robust Methods for High-Dimensional Linear Learning,"Ibrahim Merad, Stéphane Gaïffas","We propose statistically robust and computationally efficient linear learning
methods in the high-dimensional batch setting, where the number of features $d$
may exceed the sample size $n$. We employ, in a generic learning setting, two
algorithms depending on whether the considered loss function is
gradient-Lipschitz or not. Then, we instantiate our framework on several
applications including vanilla sparse, group-sparse and low-rank matrix
recovery. This leads, for each application, to efficient and robust learning
algorithms, that reach near-optimal estimation rates under heavy-tailed
distributions and the presence of outliers. For vanilla $s$-sparsity, we are
able to reach the $s\log (d)/n$ rate under heavy-tails and $\eta$-corruption,
at a computational cost comparable to that of non-robust analogs. We provide an
efficient implementation of our algorithms in an open-source $\mathtt{Python}$
library called $\mathtt{linlearn}$, by means of which we carry out numerical
experiments which confirm our theoretical findings together with a comparison
to other recent approaches proposed in the literature.",2208.05447v2,https://arxiv.org/pdf/2208.05447v2
Robust Reinforcement Learning using Offline Data,"Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, Mohammad Ghavamzadeh","The goal of robust reinforcement learning (RL) is to learn a policy that is
robust against the uncertainty in model parameters. Parameter uncertainty
commonly occurs in many real-world RL applications due to simulator modeling
errors, changes in the real-world system dynamics over time, and adversarial
disturbances. Robust RL is typically formulated as a max-min problem, where the
objective is to learn the policy that maximizes the value against the worst
possible models that lie in an uncertainty set. In this work, we propose a
robust RL algorithm called Robust Fitted Q-Iteration (RFQI), which uses only an
offline dataset to learn the optimal robust policy. Robust RL with offline data
is significantly more challenging than its non-robust counterpart because of
the minimization over all models present in the robust Bellman operator. This
poses challenges in offline data collection, optimization over the models, and
unbiased estimation. In this work, we propose a systematic approach to overcome
these challenges, resulting in our RFQI algorithm. We prove that RFQI learns a
near-optimal robust policy under standard assumptions and demonstrate its
superior performance on standard benchmark problems.",2208.05129v2,https://arxiv.org/pdf/2208.05129v2
NOTE: Robust Continual Test-time Adaptation Against Temporal Correlation,"Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, Sung-Ju Lee","Test-time adaptation (TTA) is an emerging paradigm that addresses
distributional shifts between training and testing phases without additional
data acquisition or labeling cost; only unlabeled test data streams are used
for continual model adaptation. Previous TTA schemes assume that the test
samples are independent and identically distributed (i.i.d.), even though they
are often temporally correlated (non-i.i.d.) in application scenarios, e.g.,
autonomous driving. We discover that most existing TTA methods fail
dramatically under such scenarios. Motivated by this, we present a new
test-time adaptation scheme that is robust against non-i.i.d. test data
streams. Our novelty is mainly two-fold: (a) Instance-Aware Batch Normalization
(IABN) that corrects normalization for out-of-distribution samples, and (b)
Prediction-balanced Reservoir Sampling (PBRS) that simulates i.i.d. data stream
from non-i.i.d. stream in a class-balanced manner. Our evaluation with various
datasets, including real-world non-i.i.d. streams, demonstrates that the
proposed robust TTA not only outperforms state-of-the-art TTA algorithms in the
non-i.i.d. setting, but also achieves comparable performance to those
algorithms under the i.i.d. assumption. Code is available at
https://github.com/TaesikGong/NOTE.",2208.05117v3,https://arxiv.org/pdf/2208.05117v3
"Neural-Rendezvous: Provably Robust Guidance and Control to Encounter
  Interstellar Objects","Hiroyasu Tsukamoto, Soon-Jo Chung, Benjamin Donitz, Michel Ingham, Declan Mages, Yashwanth Kumar Nakka","Interstellar objects (ISOs) are likely representatives of primitive materials
invaluable in understanding exoplanetary star systems. Due to their poorly
constrained orbits with generally high inclinations and relative velocities,
however, exploring ISOs with conventional human-in-the-loop approaches is
significantly challenging. This paper presents Neural-Rendezvous, a deep
learning-based guidance and control framework for encountering fast-moving
objects, including ISOs, robustly, accurately, and autonomously in real time.
It uses pointwise minimum norm tracking control on top of a guidance policy
modeled by a spectrally-normalized deep neural network, where its
hyperparameters are tuned with a loss function directly penalizing the MPC
state trajectory tracking error. We show that Neural-Rendezvous provides a high
probability exponential bound on the expected spacecraft delivery error, the
proof of which leverages stochastic incremental stability analysis. In
particular, it is used to construct a non-negative function with a
supermartingale property, explicitly accounting for the ISO state uncertainty
and the local nature of nonlinear state estimation guarantees. In numerical
simulations, Neural-Rendezvous is demonstrated to satisfy the expected error
bound for 100 ISO candidates. This performance is also empirically validated
using our spacecraft simulator and in high-conflict and distributed UAV swarm
reconfiguration with up to 20 UAVs.",2208.04883v2,https://arxiv.org/pdf/2208.04883v2
"RDA: Reciprocal Distribution Alignment for Robust Semi-supervised
  Learning","Yue Duan, Lei Qi, Lei Wang, Luping Zhou, Yinghuan Shi","In this work, we propose Reciprocal Distribution Alignment (RDA) to address
semi-supervised learning (SSL), which is a hyperparameter-free framework that
is independent of confidence threshold and works with both the matched
(conventionally) and the mismatched class distributions. Distribution mismatch
is an often overlooked but more general SSL scenario where the labeled and the
unlabeled data do not fall into the identical class distribution. This may lead
to the model not exploiting the labeled data reliably and drastically degrade
the performance of SSL methods, which could not be rescued by the traditional
distribution alignment. In RDA, we enforce a reciprocal alignment on the
distributions of the predictions from two classifiers predicting pseudo-labels
and complementary labels on the unlabeled data. These two distributions,
carrying complementary information, could be utilized to regularize each other
without any prior of class distribution. Moreover, we theoretically show that
RDA maximizes the input-output mutual information. Our approach achieves
promising performance in SSL under a variety of scenarios of mismatched
distributions, as well as the conventional matched SSL setting. Our code is
available at: https://github.com/NJUyued/RDA4RobustSSL.",2208.04619v2,https://arxiv.org/pdf/2208.04619v2
"Bayesian Pseudo Labels: Expectation Maximization for Robust and
  Efficient Semi-Supervised Segmentation","Mou-Cheng Xu, Yukun Zhou, Chen Jin, Marius de Groot, Daniel C. Alexander, Neil P. Oxtoby, Yipeng Hu, Joseph Jacob","This paper concerns pseudo labelling in segmentation. Our contribution is
fourfold. Firstly, we present a new formulation of pseudo-labelling as an
Expectation-Maximization (EM) algorithm for clear statistical interpretation.
Secondly, we propose a semi-supervised medical image segmentation method purely
based on the original pseudo labelling, namely SegPL. We demonstrate SegPL is a
competitive approach against state-of-the-art consistency regularisation based
methods on semi-supervised segmentation on a 2D multi-class MRI brain tumour
segmentation task and a 3D binary CT lung vessel segmentation task. The
simplicity of SegPL allows less computational cost comparing to prior methods.
Thirdly, we demonstrate that the effectiveness of SegPL may originate from its
robustness against out-of-distribution noises and adversarial attacks. Lastly,
under the EM framework, we introduce a probabilistic generalisation of SegPL
via variational inference, which learns a dynamic threshold for pseudo
labelling during the training. We show that SegPL with variational inference
can perform uncertainty estimation on par with the gold-standard method Deep
Ensemble.",2208.04435v3,https://arxiv.org/pdf/2208.04435v3
"Learning from imperfect training data using a robust loss function:
  application to brain image segmentation","Haleh Akrami, Wenhui Cui, Anand A Joshi, Richard M. Leahy","Segmentation is one of the most important tasks in MRI medical image analysis
and is often the first and the most critical step in many clinical
applications. In brain MRI analysis, head segmentation is commonly used for
measuring and visualizing the brain's anatomical structures and is also a
necessary step for other applications such as current-source reconstruction in
electroencephalography and magnetoencephalography (EEG/MEG). Here we propose a
deep learning framework that can segment brain, skull, and extra-cranial tissue
using only T1-weighted MRI as input. In addition, we describe a robust method
for training the model in the presence of noisy labels.",2208.04941v1,https://arxiv.org/pdf/2208.04941v1
Adversarial robustness of VAEs through the lens of local geometry,"Asif Khan, Amos Storkey","In an unsupervised attack on variational autoencoders (VAEs), an adversary
finds a small perturbation in an input sample that significantly changes its
latent space encoding, thereby compromising the reconstruction for a fixed
decoder. A known reason for such vulnerability is the distortions in the latent
space resulting from a mismatch between approximated latent posterior and a
prior distribution. Consequently, a slight change in an input sample can move
its encoding to a low/zero density region in the latent space resulting in an
unconstrained generation. This paper demonstrates that an optimal way for an
adversary to attack VAEs is to exploit a directional bias of a stochastic
pullback metric tensor induced by the encoder and decoder networks. The
pullback metric tensor of an encoder measures the change in infinitesimal
latent volume from an input to a latent space. Thus, it can be viewed as a lens
to analyse the effect of input perturbations leading to latent space
distortions. We propose robustness evaluation scores using the eigenspectrum of
a pullback metric tensor. Moreover, we empirically show that the scores
correlate with the robustness parameter $\beta$ of the $\beta-$VAE. Since
increasing $\beta$ also degrades reconstruction quality, we demonstrate a
simple alternative using \textit{mixup} training to fill the empty regions in
the latent space, thus improving robustness with improved reconstruction.",2208.03923v2,https://arxiv.org/pdf/2208.03923v2
"Robust Training and Verification of Implicit Neural Networks: A
  Non-Euclidean Contractive Approach","Saber Jafarpour, Alexander Davydov, Matthew Abate, Francesco Bullo, Samuel Coogan","This paper proposes a theoretical and computational framework for training
and robustness verification of implicit neural networks based upon
non-Euclidean contraction theory. The basic idea is to cast the robustness
analysis of a neural network as a reachability problem and use (i) the
$\ell_{\infty}$-norm input-output Lipschitz constant and (ii) the tight
inclusion function of the network to over-approximate its reachable sets.
First, for a given implicit neural network, we use $\ell_{\infty}$-matrix
measures to propose sufficient conditions for its well-posedness, design an
iterative algorithm to compute its fixed points, and provide upper bounds for
its $\ell_\infty$-norm input-output Lipschitz constant. Second, we introduce a
related embedded network and show that the embedded network can be used to
provide an $\ell_\infty$-norm box over-approximation of the reachable sets of
the original network. Moreover, we use the embedded network to design an
iterative algorithm for computing the upper bounds of the original system's
tight inclusion function. Third, we use the upper bounds of the Lipschitz
constants and the upper bounds of the tight inclusion functions to design two
algorithms for the training and robustness verification of implicit neural
networks. Finally, we apply our algorithms to train implicit neural networks on
the MNIST dataset and compare the robustness of our models with the models
trained via existing approaches in the literature.",2208.03889v1,https://arxiv.org/pdf/2208.03889v1
"On Transfer of Adversarial Robustness from Pretraining to Downstream
  Tasks","Laura Fee Nern, Harsh Raj, Maurice Georgi, Yash Sharma","As large-scale training regimes have gained popularity, the use of pretrained
models for downstream tasks has become common practice in machine learning.
While pretraining has been shown to enhance the performance of models in
practice, the transfer of robustness properties from pretraining to downstream
tasks remains poorly understood. In this study, we demonstrate that the
robustness of a linear predictor on downstream tasks can be constrained by the
robustness of its underlying representation, regardless of the protocol used
for pretraining. We prove (i) a bound on the loss that holds independent of any
downstream task, as well as (ii) a criterion for robust classification in
particular. We validate our theoretical results in practical applications, show
how our results can be used for calibrating expectations of downstream
robustness, and when our results are useful for optimal transfer learning.
Taken together, our results offer an initial step towards characterizing the
requirements of the representation function for reliable post-adaptation
performance.",2208.03835v2,https://arxiv.org/pdf/2208.03835v2
Towards Robust Deep Learning using Entropic Losses,David Macêdo,"Current deep learning solutions are well known for not informing whether they
can reliably classify an example during inference. One of the most effective
ways to build more reliable deep learning solutions is to improve their
performance in the so-called out-of-distribution detection task, which
essentially consists of ""know that you do not know"" or ""know the unknown"". In
other words, out-of-distribution detection capable systems may reject
performing a nonsense classification when submitted to instances of classes on
which the neural network was not trained. This thesis tackles the defiant
out-of-distribution detection task by proposing novel loss functions and
detection scores. Uncertainty estimation is also a crucial auxiliary task in
building more robust deep learning systems. Therefore, we also deal with this
robustness-related task, which evaluates how realistic the probabilities
presented by the deep neural network are. To demonstrate the effectiveness of
our approach, in addition to a substantial set of experiments, which includes
state-of-the-art results, we use arguments based on the principle of maximum
entropy to establish the theoretical foundation of the proposed approaches.
Unlike most current methods, our losses and scores are seamless and principled
solutions that produce accurate predictions in addition to fast and efficient
inference. Moreover, our approaches can be incorporated into current and future
projects simply by replacing the loss used to train the deep neural network and
computing a rapid score for detection.",2208.03566v1,https://arxiv.org/pdf/2208.03566v1
"Enhancing the Robustness via Adversarial Learning and Joint
  Spatial-Temporal Embeddings in Traffic Forecasting","Juyong Jiang, Binqing Wu, Ling Chen, Kai Zhang, Sunghun Kim","Traffic forecasting is an essential problem in urban planning and computing.
The complex dynamic spatial-temporal dependencies among traffic objects (e.g.,
sensors and road segments) have been calling for highly flexible models;
unfortunately, sophisticated models may suffer from poor robustness especially
in capturing the trend of the time series (1st-order derivatives with time),
leading to unrealistic forecasts. To address the challenge of balancing
dynamics and robustness, we propose TrendGCN, a new scheme that extends the
flexibility of GCNs and the distribution-preserving capacity of generative and
adversarial loss for handling sequential data with inherent statistical
correlations. On the one hand, our model simultaneously incorporates spatial
(node-wise) embeddings and temporal (time-wise) embeddings to account for
heterogeneous space-and-time convolutions; on the other hand, it uses GAN
structure to systematically evaluate statistical consistencies between the real
and the predicted time series in terms of both the temporal trending and the
complex spatial-temporal dependencies. Compared with traditional approaches
that handle step-wise predictive errors independently, our approach can produce
more realistic and robust forecasts. Experiments on six benchmark traffic
forecasting datasets and theoretical analysis both demonstrate the superiority
and the state-of-the-art performance of TrendGCN. Source code is available at
https://github.com/juyongjiang/TrendGCN.",2208.03063v2,https://arxiv.org/pdf/2208.03063v2
"A Noise-Robust Loss for Unlabeled Entity Problem in Named Entity
  Recognition","Wentao Kang, Guijun Zhang, Xiao Fu","Named Entity Recognition (NER) is an important task in natural language
processing. However, traditional supervised NER requires large-scale annotated
datasets. Distantly supervision is proposed to alleviate the massive demand for
datasets, but datasets constructed in this way are extremely noisy and have a
serious unlabeled entity problem. The cross entropy (CE) loss function is
highly sensitive to unlabeled data, leading to severe performance degradation.
As an alternative, we propose a new loss function called NRCES to cope with
this problem. A sigmoid term is used to mitigate the negative impact of noise.
In addition, we balance the convergence and noise tolerance of the model
according to samples and the training process. Experiments on synthetic and
real-world datasets demonstrate that our approach shows strong robustness in
the case of severe unlabeled entity problem, achieving new state-of-the-art on
real-world datasets.",2208.02934v1,https://arxiv.org/pdf/2208.02934v1
"The Role of Morphological Variation in Evolutionary Robotics: Maximizing
  Performance and Robustness","Jonata Tyska Carvalho, Stefano Nolfi","Exposing an Evolutionary Algorithm that is used to evolve robot controllers
to variable conditions is necessary to obtain solutions which are robust and
can cross the reality gap. However, we do not yet have methods for analyzing
and understanding the impact of the varying morphological conditions which
impact the evolutionary process, and therefore for choosing suitable variation
ranges. By morphological conditions, we refer to the starting state of the
robot, and to variations in its sensor readings during operation due to noise.
In this article, we introduce a method that permits us to measure the impact of
these morphological variations and we analyze the relation between the
amplitude of variations, the modality with which they are introduced, and the
performance and robustness of evolving agents. Our results demonstrate that (i)
the evolutionary algorithm can tolerate morphological variations which have a
very high impact, (ii) variations affecting the actions of the agent are
tolerated much better than variations affecting the initial state of the agent
or of the environment, and (iii) improving the accuracy of the fitness measure
through multiple evaluations is not always useful. Moreover, our results show
that morphological variations permit generating solutions which perform better
both in varying and non-varying conditions.",2208.02809v2,https://arxiv.org/pdf/2208.02809v2
Study of General Robust Subband Adaptive Filtering,"Yi Yu, Hongsen He, Rodrigo C. de Lamare, Badong Chen","In this paper, we propose a general robust subband adaptive filtering
(GR-SAF) scheme against impulsive noise by minimizing the mean square deviation
under the random-walk model with individual weight uncertainty. Specifically,
by choosing different scaling factors such as from the M-estimate and maximum
correntropy robust criteria in the GR-SAF scheme, we can easily obtain
different GR-SAF algorithms. Importantly, the proposed GR-SAF algorithm can be
reduced to a variable regularization robust normalized SAF algorithm, thus
having fast convergence rate and low steady-state error. Simulations in the
contexts of system identification with impulsive noise and echo cancellation
with double-talk have verified that the proposed GR-SAF algorithms outperforms
its counterparts.",2208.08856v2,https://arxiv.org/pdf/2208.08856v2
Design of secure and robust cognitive system for malware detection,Sanket Shukla,"Machine learning based malware detection techniques rely on grayscale images
of malware and tends to classify malware based on the distribution of textures
in graycale images. Albeit the advancement and promising results shown by
machine learning techniques, attackers can exploit the vulnerabilities by
generating adversarial samples. Adversarial samples are generated by
intelligently crafting and adding perturbations to the input samples. There
exists majority of the software based adversarial attacks and defenses. To
defend against the adversaries, the existing malware detection based on machine
learning and grayscale images needs a preprocessing for the adversarial data.
This can cause an additional overhead and can prolong the real-time malware
detection. So, as an alternative to this, we explore RRAM (Resistive Random
Access Memory) based defense against adversaries. Therefore, the aim of this
thesis is to address the above mentioned critical system security issues. The
above mentioned challenges are addressed by demonstrating proposed techniques
to design a secure and robust cognitive system. First, a novel technique to
detect stealthy malware is proposed. The technique uses malware binary images
and then extract different features from the same and then employ different
ML-classifiers on the dataset thus obtained. Results demonstrate that this
technique is successful in differentiating classes of malware based on the
features extracted. Secondly, I demonstrate the effects of adversarial attacks
on a reconfigurable RRAM-neuromorphic architecture with different learning
algorithms and device characteristics. I also propose an integrated solution
for mitigating the effects of the adversarial attack using the reconfigurable
RRAM architecture.",2208.02310v1,https://arxiv.org/pdf/2208.02310v1
"Robust PCA for Anomaly Detection and Data Imputation in Seasonal Time
  Series","Hong-Lan Botterman, Julien Roussel, Thomas Morzadec, Ali Jabbari, Nicolas Brunel","We propose a robust principal component analysis (RPCA) framework to recover
low-rank and sparse matrices from temporal observations. We develop an online
version of the batch temporal algorithm in order to process larger datasets or
streaming data. We empirically compare the proposed approaches with different
RPCA frameworks and show their effectiveness in practical situations.",2208.01998v1,https://arxiv.org/pdf/2208.01998v1
Robust Graph Neural Networks using Weighted Graph Laplacian,"Bharat Runwal, Vivek, Sandeep Kumar","Graph neural network (GNN) is achieving remarkable performances in a variety
of application domains. However, GNN is vulnerable to noise and adversarial
attacks in input data. Making GNN robust against noises and adversarial attacks
is an important problem. The existing defense methods for GNNs are
computationally demanding and are not scalable. In this paper, we propose a
generic framework for robustifying GNN known as Weighted Laplacian GNN
(RWL-GNN). The method combines Weighted Graph Laplacian learning with the GNN
implementation. The proposed method benefits from the positive
semi-definiteness property of Laplacian matrix, feature smoothness, and latent
features via formulating a unified optimization framework, which ensures the
adversarial/noisy edges are discarded and connections in the graph are
appropriately weighted. For demonstration, the experiments are conducted with
Graph convolutional neural network(GCNN) architecture, however, the proposed
framework is easily amenable to any existing GNN architecture. The simulation
results with benchmark dataset establish the efficacy of the proposed method,
both in accuracy and computational efficiency. Code can be accessed at
https://github.com/Bharat-Runwal/RWL-GNN.",2208.01853v1,https://arxiv.org/pdf/2208.01853v1
"Robust Learning of Deep Time Series Anomaly Detection Models with
  Contaminated Training Data","Wenkai Li, Cheng Feng, Ting Chen, Jun Zhu","Time series anomaly detection (TSAD) is an important data mining task with
numerous applications in the IoT era. In recent years, a large number of deep
neural network-based methods have been proposed, demonstrating significantly
better performance than conventional methods on addressing challenging TSAD
problems in a variety of areas. Nevertheless, these deep TSAD methods typically
rely on a clean training dataset that is not polluted by anomalies to learn the
""normal profile"" of the underlying dynamics. This requirement is nontrivial
since a clean dataset can hardly be provided in practice. Moreover, without the
awareness of their robustness, blindly applying deep TSAD methods with
potentially contaminated training data can possibly incur significant
performance degradation in the detection phase. In this work, to tackle this
important challenge, we firstly investigate the robustness of commonly used
deep TSAD methods with contaminated training data which provides a guideline
for applying these methods when the provided training data are not guaranteed
to be anomaly-free. Furthermore, we propose a model-agnostic method which can
effectively improve the robustness of learning mainstream deep TSAD models with
potentially contaminated data. Experiment results show that our method can
consistently prevent or mitigate performance degradation of mainstream deep
TSAD models on widely used benchmark datasets.",2208.01841v1,https://arxiv.org/pdf/2208.01841v1
"GeoECG: Data Augmentation via Wasserstein Geodesic Perturbation for
  Robust Electrocardiogram Prediction","Jiacheng Zhu, Jielin Qiu, Zhuolin Yang, Douglas Weber, Michael A. Rosenberg, Emerson Liu, Bo Li, Ding Zhao","There has been an increased interest in applying deep neural networks to
automatically interpret and analyze the 12-lead electrocardiogram (ECG). The
current paradigms with machine learning methods are often limited by the amount
of labeled data. This phenomenon is particularly problematic for
clinically-relevant data, where labeling at scale can be time-consuming and
costly in terms of the specialized expertise and human effort required.
Moreover, deep learning classifiers may be vulnerable to adversarial examples
and perturbations, which could have catastrophic consequences, for example,
when applied in the context of medical treatment, clinical trials, or insurance
claims. In this paper, we propose a physiologically-inspired data augmentation
method to improve performance and increase the robustness of heart disease
detection based on ECG signals. We obtain augmented samples by perturbing the
data distribution towards other classes along the geodesic in Wasserstein
space. To better utilize domain-specific knowledge, we design a ground metric
that recognizes the difference between ECG signals based on physiologically
determined features. Learning from 12-lead ECG signals, our model is able to
distinguish five categories of cardiac conditions. Our results demonstrate
improvements in accuracy and robustness, reflecting the effectiveness of our
data augmentation method.",2208.01220v2,https://arxiv.org/pdf/2208.01220v2
"Making a Spiking Net Work: Robust brain-like unsupervised machine
  learning","Peter G. Stratton, Andrew Wabnitz, Chip Essam, Allen Cheung, Tara J. Hamilton","The surge in interest in Artificial Intelligence (AI) over the past decade
has been driven almost exclusively by advances in Artificial Neural Networks
(ANNs). While ANNs set state-of-the-art performance for many previously
intractable problems, the use of global gradient descent necessitates large
datasets and computational resources for training, potentially limiting their
scalability for real-world domains. Spiking Neural Networks (SNNs) are an
alternative to ANNs that use more brain-like artificial neurons and can use
local unsupervised learning to rapidly discover sparse recognizable features in
the input data. SNNs, however, struggle with dynamical stability and have
failed to match the accuracy of ANNs. Here we show how an SNN can overcome many
of the shortcomings that have been identified in the literature, including
offering a principled solution to the dynamical ""vanishing spike problem"", to
outperform all existing shallow SNNs and equal the performance of an ANN. It
accomplishes this while using unsupervised learning with unlabeled data and
only 1/50th of the training epochs (labeled data is used only for a simple
linear readout layer). This result makes SNNs a viable new method for fast,
accurate, efficient, explainable, and re-deployable machine learning with
unlabeled data.",2208.01204v2,https://arxiv.org/pdf/2208.01204v2
Robust Change Detection Based on Neural Descriptor Fields,"Jiahui Fu, Yilun Du, Kurran Singh, Joshua B. Tenenbaum, John J. Leonard","The ability to reason about changes in the environment is crucial for robots
operating over extended periods of time. Agents are expected to capture changes
during operation so that actions can be followed to ensure a smooth progression
of the working session. However, varying viewing angles and accumulated
localization errors make it easy for robots to falsely detect changes in the
surrounding world due to low observation overlap and drifted object
associations. In this paper, based on the recently proposed category-level
Neural Descriptor Fields (NDFs), we develop an object-level online change
detection approach that is robust to partially overlapping observations and
noisy localization results. Utilizing the shape completion capability and
SE(3)-equivariance of NDFs, we represent objects with compact shape codes
encoding full object shapes from partial observations. The objects are then
organized in a spatial tree structure based on object centers recovered from
NDFs for fast queries of object neighborhoods. By associating objects via shape
code similarity and comparing local object-neighbor spatial layout, our
proposed approach demonstrates robustness to low observation overlap and
localization noises. We conduct experiments on both synthetic and real-world
sequences and achieve improved change detection results compared to multiple
baseline methods. Project webpage: https://yilundu.github.io/ndf_change",2208.01014v1,https://arxiv.org/pdf/2208.01014v1
"Understanding Adversarial Robustness of Vision Transformers via Cauchy
  Problem","Zheng Wang, Wenjie Ruan","Recent research on the robustness of deep learning has shown that Vision
Transformers (ViTs) surpass the Convolutional Neural Networks (CNNs) under some
perturbations, e.g., natural corruption, adversarial attacks, etc. Some papers
argue that the superior robustness of ViT comes from the segmentation of its
input images; others say that the Multi-head Self-Attention (MSA) is the key to
preserving the robustness. In this paper, we aim to introduce a principled and
unified theoretical framework to investigate such an argument on ViT's
robustness. We first theoretically prove that, unlike Transformers in Natural
Language Processing, ViTs are Lipschitz continuous. Then we theoretically
analyze the adversarial robustness of ViTs from the perspective of the Cauchy
Problem, via which we can quantify how the robustness propagates through
layers. We demonstrate that the first and last layers are the critical factors
to affect the robustness of ViTs. Furthermore, based on our theory, we
empirically show that unlike the claims from existing research, MSA only
contributes to the adversarial robustness of ViTs under weak adversarial
attacks, e.g., FGSM, and surprisingly, MSA actually comprises the model's
adversarial robustness under stronger attacks, e.g., PGD attacks.",2208.00906v1,https://arxiv.org/pdf/2208.00906v1
Generative Bias for Robust Visual Question Answering,"Jae Won Cho, Dong-jin Kim, Hyeonggon Ryu, In So Kweon","The task of Visual Question Answering (VQA) is known to be plagued by the
issue of VQA models exploiting biases within the dataset to make its final
prediction. Various previous ensemble based debiasing methods have been
proposed where an additional model is purposefully trained to be biased in
order to train a robust target model. However, these methods compute the bias
for a model simply from the label statistics of the training data or from
single modal branches. In this work, in order to better learn the bias a target
VQA model suffers from, we propose a generative method to train the bias model
directly from the target model, called GenB. In particular, GenB employs a
generative network to learn the bias in the target model through a combination
of the adversarial objective and knowledge distillation. We then debias our
target model with GenB as a bias model, and show through extensive experiments
the effects of our method on various VQA bias datasets including VQA-CP2,
VQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT
architecture on VQA-CP2.",2208.00690v3,https://arxiv.org/pdf/2208.00690v3
"Is current research on adversarial robustness addressing the right
  problem?",Ali Borji,"Short answer: Yes, Long answer: No! Indeed, research on adversarial
robustness has led to invaluable insights helping us understand and explore
different aspects of the problem. Many attacks and defenses have been proposed
over the last couple of years. The problem, however, remains largely unsolved
and poorly understood. Here, I argue that the current formulation of the
problem serves short term goals, and needs to be revised for us to achieve
bigger gains. Specifically, the bound on perturbation has created a somewhat
contrived setting and needs to be relaxed. This has misled us to focus on model
classes that are not expressive enough to begin with. Instead, inspired by
human vision and the fact that we rely more on robust features such as shape,
vertices, and foreground objects than non-robust features such as texture,
efforts should be steered towards looking for significantly different classes
of models. Maybe instead of narrowing down on imperceptible adversarial
perturbations, we should attack a more general problem which is finding
architectures that are simultaneously robust to perceptible perturbations,
geometric transformations (e.g. rotation, scaling), image distortions
(lighting, blur), and more (e.g. occlusion, shadow). Only then we may be able
to solve the problem of adversarial vulnerability.",2208.00539v2,https://arxiv.org/pdf/2208.00539v2
"Adaptive Temperature Scaling for Robust Calibration of Deep Neural
  Networks","Sergio A. Balanya, Juan Maroñas, Daniel Ramos","In this paper, we study the post-hoc calibration of modern neural networks, a
problem that has drawn a lot of attention in recent years. Many calibration
methods of varying complexity have been proposed for the task, but there is no
consensus about how expressive these should be. We focus on the task of
confidence scaling, specifically on post-hoc methods that generalize
Temperature Scaling, we call these the Adaptive Temperature Scaling family. We
analyse expressive functions that improve calibration and propose interpretable
methods. We show that when there is plenty of data complex models like neural
networks yield better performance, but are prone to fail when the amount of
data is limited, a common situation in certain post-hoc calibration
applications like medical diagnosis. We study the functions that expressive
methods learn under ideal conditions and design simpler methods but with a
strong inductive bias towards these well-performing functions. Concretely, we
propose Entropy-based Temperature Scaling, a simple method that scales the
confidence of a prediction according to its entropy. Results show that our
method obtains state-of-the-art performance when compared to others and, unlike
complex models, it is robust against data scarcity. Moreover, our proposed
model enables a deeper interpretation of the calibration process.",2208.00461v1,https://arxiv.org/pdf/2208.00461v1
"Symmetry Regularization and Saturating Nonlinearity for Robust
  Quantization","Sein Park, Yeongsang Jang, Eunhyeok Park","Robust quantization improves the tolerance of networks for various
implementations, allowing reliable output in different bit-widths or fragmented
low-precision arithmetic. In this work, we perform extensive analyses to
identify the sources of quantization error and present three insights to
robustify a network against quantization: reduction of error propagation, range
clamping for error minimization, and inherited robustness against quantization.
Based on these insights, we propose two novel methods called symmetry
regularization (SymReg) and saturating nonlinearity (SatNL). Applying the
proposed methods during training can enhance the robustness of arbitrary neural
networks against quantization on existing post-training quantization (PTQ) and
quantization-aware training (QAT) algorithms and enables us to obtain a single
weight flexible enough to maintain the output quality under various conditions.
We conduct extensive studies on CIFAR and ImageNet datasets and validate the
effectiveness of the proposed methods.",2208.00338v1,https://arxiv.org/pdf/2208.00338v1
Robust Contact State Estimation in Humanoid Walking Gaits,"Stylianos Piperakis, Michael Maravgakis, Dimitrios Kanoulas, Panos Trahanias","In this article, we propose a deep learning framework that provides a unified
approach to the problem of leg contact detection in humanoid robot walking
gaits. Our formulation accomplishes to accurately and robustly estimate the
contact state probability for each leg (i.e., stable or slip/no contact). The
proposed framework employs solely proprioceptive sensing and although it relies
on simulated ground-truth contact data for the classification process, we
demonstrate that it generalizes across varying friction surfaces and different
legged robotic platforms and, at the same time, is readily transferred from
simulation to practice. The framework is quantitatively and qualitatively
assessed in simulation via the use of ground-truth contact data and is
contrasted against state of-the-art methods with an ATLAS, a NAO, and a TALOS
humanoid robot. Furthermore, its efficacy is demonstrated in base estimation
with a real TALOS humanoid. To reinforce further research endeavors, our
implementation is offered as an open-source ROS/Python package, coined Legged
Contact Detection (LCD).",2208.00278v1,https://arxiv.org/pdf/2208.00278v1
"Robust Rayleigh Regression Method for SAR Image Processing in Presence
  of Outliers","B. G. Palm, F. M. Bayer, R. Machado, M. I. Pettersson, V. T. Vu, R. J. Cintra","The presence of outliers (anomalous values) in synthetic aperture radar (SAR)
data and the misspecification in statistical image models may result in
inaccurate inferences. To avoid such issues, the Rayleigh regression model
based on a robust estimation process is proposed as a more realistic approach
to model this type of data. This paper aims at obtaining Rayleigh regression
model parameter estimators robust to the presence of outliers. The proposed
approach considered the weighted maximum likelihood method and was submitted to
numerical experiments using simulated and measured SAR images. Monte Carlo
simulations were employed for the numerical assessment of the proposed robust
estimator performance in finite signal lengths, their sensitivity to outliers,
and the breakdown point. For instance, the non-robust estimators show a
relative bias value $65$-fold larger than the results provided by the robust
approach in corrupted signals. In terms of sensitivity analysis and break down
point, the robust scheme resulted in a reduction of about $96\%$ and $10\%$,
respectively, in the mean absolute value of both measures, in compassion to the
non-robust estimators. Moreover, two SAR data sets were used to compare the
ground type and anomaly detection results of the proposed robust scheme with
competing methods in the literature.",2208.00097v1,https://arxiv.org/pdf/2208.00097v1
Robust Trajectory Prediction against Adversarial Attacks,"Yulong Cao, Danfei Xu, Xinshuo Weng, Zhuoqing Mao, Anima Anandkumar, Chaowei Xiao, Marco Pavone","Trajectory prediction using deep neural networks (DNNs) is an essential
component of autonomous driving (AD) systems. However, these methods are
vulnerable to adversarial attacks, leading to serious consequences such as
collisions. In this work, we identify two key ingredients to defend trajectory
prediction models against adversarial attacks including (1) designing effective
adversarial training methods and (2) adding domain-specific data augmentation
to mitigate the performance degradation on clean data. We demonstrate that our
method is able to improve the performance by 46% on adversarial data and at the
cost of only 3% performance degradation on clean data, compared to the model
trained with clean data. Additionally, compared to existing robust methods, our
method can improve performance by 21% on adversarial examples and 9% on clean
data. Our robust model is evaluated with a planner to study its downstream
impacts. We demonstrate that our model can significantly reduce the severe
accident rates (e.g., collisions and off-road driving).",2208.00094v1,https://arxiv.org/pdf/2208.00094v1
"Improving the Performance of Robust Control through Event-Triggered
  Learning","Alexander von Rohr, Friedrich Solowjow, Sebastian Trimpe","Robust controllers ensure stability in feedback loops designed under
uncertainty but at the cost of performance. Model uncertainty in time-invariant
systems can be reduced by recently proposed learning-based methods, which
improve the performance of robust controllers using data. However, in practice,
many systems also exhibit uncertainty in the form of changes over time, e.g.,
due to weight shifts or wear and tear, leading to decreased performance or
instability of the learning-based controller. We propose an event-triggered
learning algorithm that decides when to learn in the face of uncertainty in the
LQR problem with rare or slow changes. Our key idea is to switch between robust
and learned controllers. For learning, we first approximate the optimal length
of the learning phase via Monte-Carlo estimations using a probabilistic model.
We then design a statistical test for uncertain systems based on the
moment-generating function of the LQR cost. The test detects changes in the
system under control and triggers re-learning when control performance
deteriorates due to system changes. We demonstrate improved performance over a
robust controller baseline in a numerical example.",2207.14252v2,https://arxiv.org/pdf/2207.14252v2
"Generating Teammates for Training Robust Ad Hoc Teamwork Agents via
  Best-Response Diversity","Arrasy Rahman, Elliot Fosong, Ignacio Carlucho, Stefano V. Albrecht","Ad hoc teamwork (AHT) is the challenge of designing a robust learner agent
that effectively collaborates with unknown teammates without prior coordination
mechanisms. Early approaches address the AHT challenge by training the learner
with a diverse set of handcrafted teammate policies, usually designed based on
an expert's domain knowledge about the policies the learner may encounter.
However, implementing teammate policies for training based on domain knowledge
is not always feasible. In such cases, recent approaches attempted to improve
the robustness of the learner by training it with teammate policies generated
by optimising information-theoretic diversity metrics. The problem with
optimising existing information-theoretic diversity metrics for teammate policy
generation is the emergence of superficially different teammates. When used for
AHT training, superficially different teammate behaviours may not improve a
learner's robustness during collaboration with unknown teammates. In this
paper, we present an automated teammate policy generation method optimising the
Best-Response Diversity (BRDiv) metric, which measures diversity based on the
compatibility of teammate policies in terms of returns. We evaluate our
approach in environments with multiple valid coordination strategies, comparing
against methods optimising information-theoretic diversity metrics and an
ablation not optimising any diversity metric. Our experiments indicate that
optimising BRDiv yields a diverse set of training teammate policies that
improve the learner's performance relative to previous teammate generation
approaches when collaborating with near-optimal previously unseen teammate
policies.",2207.14138v3,https://arxiv.org/pdf/2207.14138v3
Multi-layer Representation Learning for Robust OOD Image Classification,"Aristotelis Ballas, Christos Diou","Convolutional Neural Networks have become the norm in image classification.
Nevertheless, their difficulty to maintain high accuracy across datasets has
become apparent in the past few years. In order to utilize such models in
real-world scenarios and applications, they must be able to provide trustworthy
predictions on unseen data. In this paper, we argue that extracting features
from a CNN's intermediate layers can assist in the model's final prediction.
Specifically, we adapt the Hypercolumns method to a ResNet-18 and find a
significant increase in the model's accuracy, when evaluating on the NICO
dataset.",2207.13678v1,https://arxiv.org/pdf/2207.13678v1
"On the robustness of self-supervised representations for multi-view
  object classification","David Torpey, Richard Klein","It is known that representations from self-supervised pre-training can
perform on par, and often better, on various downstream tasks than
representations from fully-supervised pre-training. This has been shown in a
host of settings such as generic object classification and detection, semantic
segmentation, and image retrieval. However, some issues have recently come to
the fore that demonstrate some of the failure modes of self-supervised
representations, such as performance on non-ImageNet-like data, or complex
scenes. In this paper, we show that self-supervised representations based on
the instance discrimination objective lead to better representations of objects
that are more robust to changes in the viewpoint and perspective of the object.
We perform experiments of modern self-supervised methods against multiple
supervised baselines to demonstrate this, including approximating object
viewpoint variation through homographies, and real-world tests based on several
multi-view datasets. We find that self-supervised representations are more
robust to object viewpoint and appear to encode more pertinent information
about objects that facilitate the recognition of objects from novel views.",2208.00787v1,https://arxiv.org/pdf/2208.00787v1
"Safe and Robust Experience Sharing for Deterministic Policy Gradient
  Algorithms","Baturay Saglam, Dogan C. Cicek, Furkan B. Mutlu, Suleyman S. Kozat","Learning in high dimensional continuous tasks is challenging, mainly when the
experience replay memory is very limited. We introduce a simple yet effective
experience sharing mechanism for deterministic policies in continuous action
domains for the future off-policy deep reinforcement learning applications in
which the allocated memory for the experience replay buffer is limited. To
overcome the extrapolation error induced by learning from other agents'
experiences, we facilitate our algorithm with a novel off-policy correction
technique without any action probability estimates. We test the effectiveness
of our method in challenging OpenAI Gym continuous control tasks and conclude
that it can achieve a safe experience sharing across multiple agents and
exhibits a robust performance when the replay memory is strictly limited.",2207.13453v1,https://arxiv.org/pdf/2207.13453v1
"PointFix: Learning to Fix Domain Bias for Robust Online Stereo
  Adaptation","Kwonyoung Kim, Jungin Park, Jiyoung Lee, Dongbo Min, Kwanghoon Sohn","Online stereo adaptation tackles the domain shift problem, caused by
different environments between synthetic (training) and real (test) datasets,
to promptly adapt stereo models in dynamic real-world applications such as
autonomous driving. However, previous methods often fail to counteract
particular regions related to dynamic objects with more severe environmental
changes. To mitigate this issue, we propose to incorporate an auxiliary
point-selective network into a meta-learning framework, called PointFix, to
provide a robust initialization of stereo models for online stereo adaptation.
In a nutshell, our auxiliary network learns to fix local variants intensively
by effectively back-propagating local information through the meta-gradient for
the robust initialization of the baseline model. This network is
model-agnostic, so can be used in any kind of architectures in a plug-and-play
manner. We conduct extensive experiments to verify the effectiveness of our
method under three adaptation settings such as short-, mid-, and long-term
sequences. Experimental results show that the proper initialization of the base
stereo model by the auxiliary network enables our learning paradigm to achieve
state-of-the-art performance at inference.",2207.13340v1,https://arxiv.org/pdf/2207.13340v1
"Visual correspondence-based explanations improve AI robustness and
  human-AI team accuracy","Giang Nguyen, Mohammad Reza Taesiri, Anh Nguyen","Explaining artificial intelligence (AI) predictions is increasingly important
and even imperative in many high-stakes applications where humans are the
ultimate decision-makers. In this work, we propose two novel architectures of
self-interpretable image classifiers that first explain, and then predict (as
opposed to post-hoc explanations) by harnessing the visual correspondences
between a query image and exemplars. Our models consistently improve (by 1 to 4
points) on out-of-distribution (OOD) datasets while performing marginally worse
(by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest
neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB,
our correspondence-based explanations are found to be more useful to users than
kNN explanations. Our explanations help users more accurately reject AI's wrong
decisions than all other tested methods. Interestingly, for the first time, we
show that it is possible to achieve complementary human-AI team accuracy (i.e.,
that is higher than either AI-alone or human-alone), in ImageNet and CUB image
classification tasks.",2208.00780v5,https://arxiv.org/pdf/2208.00780v5
Improving Adversarial Robustness via Mutual Information Estimation,"Dawei Zhou, Nannan Wang, Xinbo Gao, Bo Han, Xiaoyu Wang, Yibing Zhan, Tongliang Liu","Deep neural networks (DNNs) are found to be vulnerable to adversarial noise.
They are typically misled by adversarial samples to make wrong predictions. To
alleviate this negative effect, in this paper, we investigate the dependence
between outputs of the target model and input adversarial samples from the
perspective of information theory, and propose an adversarial defense method.
Specifically, we first measure the dependence by estimating the mutual
information (MI) between outputs and the natural patterns of inputs (called
natural MI) and MI between outputs and the adversarial patterns of inputs
(called adversarial MI), respectively. We find that adversarial samples usually
have larger adversarial MI and smaller natural MI compared with those w.r.t.
natural samples. Motivated by this observation, we propose to enhance the
adversarial robustness by maximizing the natural MI and minimizing the
adversarial MI during the training process. In this way, the target model is
expected to pay more attention to the natural pattern that contains objective
semantics. Empirical evaluations demonstrate that our method could effectively
improve the adversarial accuracy against multiple attacks.",2207.12203v1,https://arxiv.org/pdf/2207.12203v1
Minimax Rates for Robust Community Detection,"Allen Liu, Ankur Moitra","In this work, we study the problem of community detection in the stochastic
block model with adversarial node corruptions. Our main result is an efficient
algorithm that can tolerate an $\epsilon$-fraction of corruptions and achieves
error $O(\epsilon) + e^{-\frac{C}{2} (1 \pm o(1))}$ where $C = (\sqrt{a} -
\sqrt{b})^2$ is the signal-to-noise ratio and $a/n$ and $b/n$ are the
inter-community and intra-community connection probabilities respectively.
These bounds essentially match the minimax rates for the SBM without
corruptions. We also give robust algorithms for $\mathbb{Z}_2$-synchronization.
At the heart of our algorithm is a new semidefinite program that uses global
information to robustly boost the accuracy of a rough clustering. Moreover, we
show that our algorithms are doubly-robust in the sense that they work in an
even more challenging noise model that mixes adversarial corruptions with
unbounded monotone changes, from the semi-random model.",2207.11903v1,https://arxiv.org/pdf/2207.11903v1
Can we achieve robustness from data alone?,"Nikolaos Tsilivis, Jingtong Su, Julia Kempe","We introduce a meta-learning algorithm for adversarially robust
classification. The proposed method tries to be as model agnostic as possible
and optimizes a dataset prior to its deployment in a machine learning system,
aiming to effectively erase its non-robust features. Once the dataset has been
created, in principle no specialized algorithm (besides standard gradient
descent) is needed to train a robust model. We formulate the data optimization
procedure as a bi-level optimization problem on kernel regression, with a class
of kernels that describe infinitely wide neural nets (Neural Tangent Kernels).
We present extensive experiments on standard computer vision benchmarks using a
variety of different models, demonstrating the effectiveness of our method,
while also pointing out its current shortcomings. In parallel, we revisit prior
work that also focused on the problem of data optimization for robust
classification \citep{Ily+19}, and show that being robust to adversarial
attacks after standard (gradient descent) training on a suitable dataset is
more challenging than previously thought.",2207.11727v2,https://arxiv.org/pdf/2207.11727v2
Distributed Robust Principal Component Analysis,Wenda Chu,"We study the robust principal component analysis (RPCA) problem in a
distributed setting. The goal of RPCA is to find an underlying low-rank
estimation for a raw data matrix when the data matrix is subject to the
corruption of gross sparse errors. Previous studies have developed RPCA
algorithms that provide stable solutions with fast convergence. However, these
algorithms are typically hard to scale and cannot be implemented distributedly,
due to the use of either SVD or large matrix multiplication. In this paper, we
propose the first distributed robust principal analysis algorithm based on
consensus factorization, dubbed DCF-PCA. We prove the convergence of DCF-PCA
and evaluate DCF-PCA on various problem setting",2207.11669v2,https://arxiv.org/pdf/2207.11669v2
"Training Robust Spiking Neural Networks on Neuromorphic Data with
  Spatiotemporal Fragments","Haibo Shen, Yihao Luo, Xiang Cao, Liangqi Zhang, Juyu Xiao, Tianjiang Wang","Neuromorphic vision sensors (event cameras) are inherently suitable for
spiking neural networks (SNNs) and provide novel neuromorphic vision data for
this biomimetic model. Due to the spatiotemporal characteristics, novel data
augmentations are required to process the unconventional visual signals of
these cameras. In this paper, we propose a novel Event SpatioTemporal Fragments
(ESTF) augmentation method. It preserves the continuity of neuromorphic data by
drifting or inverting fragments of the spatiotemporal event stream to simulate
the disturbance of brightness variations, leading to more robust spiking neural
networks. Extensive experiments are performed on prevailing neuromorphic
datasets. It turns out that ESTF provides substantial improvements over pure
geometric transformations and outperforms other event data augmentation
methods. It is worth noting that the SNNs with ESTF achieve the
state-of-the-art accuracy of 83.9\% on the CIFAR10-DVS dataset.",2207.11659v3,https://arxiv.org/pdf/2207.11659v3
Robust Scene Inference under Noise-Blur Dual Corruptions,"Bhavya Goyal, Jean-François Lalonde, Yin Li, Mohit Gupta","Scene inference under low-light is a challenging problem due to severe noise
in the captured images. One way to reduce noise is to use longer exposure
during the capture. However, in the presence of motion (scene or camera
motion), longer exposures lead to motion blur, resulting in loss of image
information. This creates a trade-off between these two kinds of image
degradations: motion blur (due to long exposure) vs. noise (due to short
exposure), also referred as a dual image corruption pair in this paper. With
the rise of cameras capable of capturing multiple exposures of the same scene
simultaneously, it is possible to overcome this trade-off. Our key observation
is that although the amount and nature of degradation varies for these
different image captures, the semantic content remains the same across all
images. To this end, we propose a method to leverage these multi exposure
captures for robust inference under low-light and motion. Our method builds on
a feature consistency loss to encourage similar results from these individual
captures, and uses the ensemble of their final predictions for robust visual
recognition. We demonstrate the effectiveness of our approach on simulated
images as well as real captures with multiple exposures, and across the tasks
of object detection and image classification.",2207.11643v1,https://arxiv.org/pdf/2207.11643v1
Testing the Robustness of Learned Index Structures,"Matthias Bachfischer, Renata Borovica-Gajic, Benjamin I. P. Rubinstein","While early empirical evidence has supported the case for learned index
structures as having favourable average-case performance, little is known about
their worst-case performance. By contrast, classical structures are known to
achieve optimal worst-case behaviour. This work evaluates the robustness of
learned index structures in the presence of adversarial workloads. To simulate
adversarial workloads, we carry out a data poisoning attack on linear
regression models that manipulates the cumulative distribution function (CDF)
on which the learned index model is trained. The attack deteriorates the fit of
the underlying ML model by injecting a set of poisoning keys into the training
dataset, which leads to an increase in the prediction error of the model and
thus deteriorates the overall performance of the learned index structure. We
assess the performance of various regression methods and the learned index
implementations ALEX and PGM-Index. We show that learned index structures can
suffer from a significant performance deterioration of up to 20% when evaluated
on poisoned vs. non-poisoned datasets.",2207.11575v1,https://arxiv.org/pdf/2207.11575v1
An Impartial Take to the CNN vs Transformer Robustness Contest,"Francesco Pinto, Philip H. S. Torr, Puneet K. Dokania","Following the surge of popularity of Transformers in Computer Vision, several
studies have attempted to determine whether they could be more robust to
distribution shifts and provide better uncertainty estimates than Convolutional
Neural Networks (CNNs). The almost unanimous conclusion is that they are, and
it is often conjectured more or less explicitly that the reason of this
supposed superiority is to be attributed to the self-attention mechanism. In
this paper we perform extensive empirical analyses showing that recent
state-of-the-art CNNs (particularly, ConvNeXt) can be as robust and reliable or
even sometimes more than the current state-of-the-art Transformers. However,
there is no clear winner. Therefore, although it is tempting to state the
definitive superiority of one family of architectures over another, they seem
to enjoy similar extraordinary performances on a variety of tasks while also
suffering from similar vulnerabilities such as texture, background, and
simplicity biases.",2207.11347v1,https://arxiv.org/pdf/2207.11347v1
"Decoupled Adversarial Contrastive Learning for Self-supervised
  Adversarial Robustness","Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Axi Niu, Jiu Feng, Chang D. Yoo, In So Kweon","Adversarial training (AT) for robust representation learning and
self-supervised learning (SSL) for unsupervised representation learning are two
active research fields. Integrating AT into SSL, multiple prior works have
accomplished a highly significant yet challenging task: learning robust
representation without labels. A widely used framework is adversarial
contrastive learning which couples AT and SSL, and thus constitute a very
complex optimization problem. Inspired by the divide-and-conquer philosophy, we
conjecture that it might be simplified as well as improved by solving two
sub-problems: non-robust SSL and pseudo-supervised AT. This motivation shifts
the focus of the task from seeking an optimal integrating strategy for a
coupled problem to finding sub-solutions for sub-problems. With this said, this
work discards prior practices of directly introducing AT to SSL frameworks and
proposed a two-stage framework termed Decoupled Adversarial Contrastive
Learning (DeACL). Extensive experimental results demonstrate that our DeACL
achieves SOTA self-supervised adversarial robustness while significantly
reducing the training time, which validates its effectiveness and efficiency.
Moreover, our DeACL constitutes a more explainable solution, and its success
also bridges the gap with semi-supervised AT for exploiting unlabeled samples
for robust representation learning. The code is publicly accessible at
https://github.com/pantheon5100/DeACL.",2207.10899v1,https://arxiv.org/pdf/2207.10899v1
Robust Knowledge Adaptation for Dynamic Graph Neural Networks,"Hanjie Li, Changsheng Li, Kaituo Feng, Ye Yuan, Guoren Wang, Hongyuan Zha","Graph structured data often possess dynamic characters in nature. Recent
years have witnessed the increasing attentions paid to dynamic graph neural
networks for modelling graph data. However, almost all existing approaches
operate under the assumption that, upon the establishment of a new link, the
embeddings of the neighboring nodes should undergo updates to learn temporal
dynamics. Nevertheless, these approaches face the following limitation: If the
node introduced by a new connection contains noisy information, propagating its
knowledge to other nodes becomes unreliable and may even lead to the collapse
of the model. In this paper, we propose Ada-DyGNN: a robust knowledge
Adaptation framework via reinforcement learning for Dynamic Graph Neural
Networks. In contrast to previous approaches, which update the embeddings of
the neighbor nodes immediately after adding a new link, Ada-DyGNN adaptively
determines which nodes should be updated. Considering that the decision to
update the embedding of one neighbor node can significantly impact other
neighbor nodes, we conceptualize the node update selection as a sequence
decision problem and employ reinforcement learning to address it effectively.
By this means, we can adaptively propagate knowledge to other nodes for
learning robust node embedding representations. To the best of our knowledge,
our approach constitutes the first attempt to explore robust knowledge
adaptation via reinforcement learning specifically tailored for dynamic graph
neural networks. Extensive experiments on three benchmark datasets demonstrate
that Ada-DyGNN achieves the state-of-the-art performance. In addition, we
conduct experiments by introducing different degrees of noise into the dataset,
quantitatively and qualitatively illustrating the robustness of Ada-DyGNN.",2207.10839v2,https://arxiv.org/pdf/2207.10839v2
"Towards Robust On-Ramp Merging via Augmented Multimodal Reinforcement
  Learning","Gaurav Bagwe, Jian Li, Xiaoyong Yuan, Lan Zhang","Despite the success of AI-enabled onboard perception, on-ramp merging has
been one of the main challenges for autonomous driving. Due to limited sensing
range of onboard sensors, a merging vehicle can hardly observe main road
conditions and merge properly. By leveraging the wireless communications
between connected and automated vehicles (CAVs), a merging CAV has potential to
proactively obtain the intentions of nearby vehicles. However, CAVs can be
prone to inaccurate observations, such as the noisy basic safety messages (BSM)
and poor quality surveillance images. In this paper, we present a novel
approach for Robust on-ramp merge of CAVs via Augmented and Multi-modal
Reinforcement Learning, named by RAMRL. Specifically, we formulate the on-ramp
merging problem as a Markov decision process (MDP) by taking driving safety,
comfort driving behavior, and traffic efficiency into account. To provide
reliable merging maneuvers, we simultaneously leverage BSM and surveillance
images for multi-modal observation, which is used to learn a policy model
through proximal policy optimization (PPO). Moreover, to improve data
efficiency and provide better generalization performance, we train the policy
model with augmented data (e.g., noisy BSM and noisy surveillance images).
Extensive experiments are conducted with Simulation of Urban MObility (SUMO)
platform under two typical merging scenarios. Experimental results demonstrate
the effectiveness and efficiency of our robust on-ramp merging design.",2208.07307v1,https://arxiv.org/pdf/2208.07307v1
On the Robustness of 3D Object Detectors,"Fatima Albreiki, Sultan Abughazal, Jean Lahoud, Rao Anwer, Hisham Cholakkal, Fahad Khan","In recent years, significant progress has been achieved for 3D object
detection on point clouds thanks to the advances in 3D data collection and deep
learning techniques. Nevertheless, 3D scenes exhibit a lot of variations and
are prone to sensor inaccuracies as well as information loss during
pre-processing. Thus, it is crucial to design techniques that are robust
against these variations. This requires a detailed analysis and understanding
of the effect of such variations. This work aims to analyze and benchmark
popular point-based 3D object detectors against several data corruptions. To
the best of our knowledge, we are the first to investigate the robustness of
point-based 3D object detectors. To this end, we design and evaluate
corruptions that involve data addition, reduction, and alteration. We further
study the robustness of different modules against local and global variations.
Our experimental results reveal several intriguing findings. For instance, we
show that methods that integrate Transformers at a patch or object level lead
to increased robustness, compared to using Transformers at the point level.",2207.10205v1,https://arxiv.org/pdf/2207.10205v1
Robust Landmark-based Stent Tracking in X-ray Fluoroscopy,"Luojie Huang, Yikang Liu, Li Chen, Eric Z. Chen, Xiao Chen, Shanhui Sun","In clinical procedures of angioplasty (i.e., open clogged coronary arteries),
devices such as balloons and stents need to be placed and expanded in arteries
under the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,
the resulting images are often noisy. To check the correct placement of these
devices, typically multiple motion-compensated frames are averaged to enhance
the view. Therefore, device tracking is a necessary procedure for this purpose.
Even though angioplasty devices are designed to have radiopaque markers for the
ease of tracking, current methods struggle to deliver satisfactory results due
to the small marker size and complex scenes in angioplasty. In this paper, we
propose an end-to-end deep learning framework for single stent tracking, which
consists of three hierarchical modules: U-Net based landmark detection, ResNet
based stent proposal and feature extraction, and graph convolutional neural
network (GCN) based stent tracking that temporally aggregates both spatial
information and appearance features. The experiments show that our method
performs significantly better in detection compared with the state-of-the-art
point-based tracking models. In addition, its fast inference speed satisfies
clinical requirements.",2207.09933v3,https://arxiv.org/pdf/2207.09933v3
"Correntropy-Based Logistic Regression with Automatic Relevance
  Determination for Robust Sparse Brain Activity Decoding","Yuanhao Li, Badong Chen, Yuxi Shi, Natsue Yoshimura, Yasuharu Koike","Recent studies have utilized sparse classifications to predict categorical
variables from high-dimensional brain activity signals to expose human's
intentions and mental states, selecting the relevant features automatically in
the model training process. However, existing sparse classification models will
likely be prone to the performance degradation which is caused by noise
inherent in the brain recordings. To address this issue, we aim to propose a
new robust and sparse classification algorithm in this study. To this end, we
introduce the correntropy learning framework into the automatic relevance
determination based sparse classification model, proposing a new
correntropy-based robust sparse logistic regression algorithm. To demonstrate
the superior brain activity decoding performance of the proposed algorithm, we
evaluate it on a synthetic dataset, an electroencephalogram (EEG) dataset, and
a functional magnetic resonance imaging (fMRI) dataset. The extensive
experimental results confirm that not only the proposed method can achieve
higher classification accuracy in a noisy and high-dimensional classification
task, but also it would select those more informative features for the decoding
scenarios. Integrating the correntropy learning approach with the automatic
relevance determination technique will significantly improve the robustness
with respect to the noise, leading to more adequate robust sparse brain
decoding algorithm. It provides a more powerful approach in the real-world
brain activity decoding and the brain-computer interfaces.",2207.09693v1,https://arxiv.org/pdf/2207.09693v1
"Generalizable and Robust Deep Learning Algorithm for Atrial Fibrillation
  Diagnosis Across Ethnicities, Ages and Sexes","Shany Biton, Mohsin Aldhafeeri, Erez Marcusohn, Kenta Tsutsui, Tom Szwagier, Adi Elias, Julien Oster, Jean Marc Sellal, Mahmoud Suleiman, Joachim A. Behar","To drive health innovation that meets the needs of all and democratize
healthcare, there is a need to assess the generalization performance of deep
learning (DL) algorithms across various distribution shifts to ensure that
these algorithms are robust. This retrospective study is, to the best of our
knowledge, the first to develop and assess the generalization performance of a
deep learning (DL) model for AF events detection from long term beat-to-beat
intervals across ethnicities, ages and sexes. The new recurrent DL model,
denoted ArNet2, was developed on a large retrospective dataset of 2,147
patients totaling 51,386 hours of continuous electrocardiogram (ECG). The
models generalization was evaluated on manually annotated test sets from four
centers (USA, Israel, Japan and China) totaling 402 patients. The model was
further validated on a retrospective dataset of 1,730 consecutives Holter
recordings from the Rambam Hospital Holter clinic, Haifa, Israel. The model
outperformed benchmark state-of-the-art models and generalized well across
ethnicities, ages and sexes. Performance was higher for female than male and
young adults (less than 60 years old) and showed some differences across
ethnicities. The main finding explaining these variations was an impairment in
performance in groups with a higher prevalence of atrial flutter (AFL). Our
findings on the relative performance of ArNet2 across groups may have clinical
implications on the choice of the preferred AF examination method to use
relative to the group of interest.",2207.09667v1,https://arxiv.org/pdf/2207.09667v1
"Towards Accurate and Robust Classification in Continuously Transitioning
  Industrial Sprays with Mixup","Hongjiang Li, Huanyi Shui, Alemayehu Admasu, Praveen Narayanan, Devesh Upadhyay","Image classification with deep neural networks has seen a surge of
technological breakthroughs with promising applications in areas such as face
recognition, medical imaging, and autonomous driving. In engineering problems,
however, such as high-speed imaging of engine fuel injector sprays or body
paint sprays, deep neural networks face a fundamental challenge related to the
availability of adequate and diverse data. Typically, only thousands or
sometimes even hundreds of samples are available for training. In addition, the
transition between different spray classes is a continuum and requires a high
level of domain expertise to label the images accurately. In this work, we used
Mixup as an approach to systematically deal with the data scarcity and
ambiguous class boundaries found in industrial spray applications. We show that
data augmentation can mitigate the over-fitting problem of large neural
networks on small data sets, to a certain level, but cannot fundamentally
resolve the issue. We discuss how a convex linear interpolation of different
classes naturally aligns with the continuous transition between different
classes in our application. Our experiments demonstrate Mixup as a simple yet
effective method to train an accurate and robust deep neural network classifier
with only a few hundred samples.",2207.09609v1,https://arxiv.org/pdf/2207.09609v1
"Feasible Adversarial Robust Reinforcement Learning for Underspecified
  Environments","JB Lanier, Stephen McAleer, Pierre Baldi, Roy Fox","Robust reinforcement learning (RL) considers the problem of learning policies
that perform well in the worst case among a set of possible environment
parameter values. In real-world environments, choosing the set of possible
values for robust RL can be a difficult task. When that set is specified too
narrowly, the agent will be left vulnerable to reasonable parameter values
unaccounted for. When specified too broadly, the agent will be too cautious. In
this paper, we propose Feasible Adversarial Robust RL (FARR), a novel problem
formulation and objective for automatically determining the set of environment
parameter values over which to be robust. FARR implicitly defines the set of
feasible parameter values as those on which an agent could achieve a benchmark
reward given enough training resources. By formulating this problem as a
two-player zero-sum game, optimizing the FARR objective jointly produces an
adversarial distribution over parameter values with feasible support and a
policy robust over this feasible parameter set. We demonstrate that approximate
Nash equilibria for this objective can be found using a variation of the PSRO
algorithm. Furthermore, we show that an optimal agent trained with FARR is more
robust to feasible adversarial parameter selection than with existing minimax,
domain-randomization, and regret objectives in a parameterized gridworld and
three MuJoCo control environments.",2207.09597v2,https://arxiv.org/pdf/2207.09597v2
"Robust Multivariate Time-Series Forecasting: Adversarial Attacks and
  Defense Mechanisms","Linbo Liu, Youngsuk Park, Trong Nghia Hoang, Hilaf Hasson, Jun Huan","This work studies the threats of adversarial attack on multivariate
probabilistic forecasting models and viable defense mechanisms. Our studies
discover a new attack pattern that negatively impact the forecasting of a
target time series via making strategic, sparse (imperceptible) modifications
to the past observations of a small number of other time series. To mitigate
the impact of such attack, we have developed two defense strategies. First, we
extend a previously developed randomized smoothing technique in classification
to multivariate forecasting scenarios. Second, we develop an adversarial
training algorithm that learns to create adversarial examples and at the same
time optimizes the forecasting model to improve its robustness against such
adversarial simulation. Extensive experiments on real-world datasets confirm
that our attack schemes are powerful and our defense algorithms are more
effective compared with baseline defense mechanisms.",2207.09572v3,https://arxiv.org/pdf/2207.09572v3
Holistic Robust Data-Driven Decisions,"Amine Bennouna, Bart Van Parys","The design of data-driven formulations for machine learning and
decision-making with good out-of-sample performance is a key challenge. The
observation that good in-sample performance does not guarantee good
out-of-sample performance is generally known as overfitting. Practical
overfitting can typically not be attributed to a single cause but instead is
caused by several factors all at once. We consider here three overfitting
sources: (i) statistical error as a result of working with finite sample data,
(ii) data noise which occurs when the data points are measured only with finite
precision, and finally (iii) data misspecification in which a small fraction of
all data may be wholly corrupted. We argue that although existing data-driven
formulations may be robust against one of these three sources in isolation they
do not provide holistic protection against all overfitting sources
simultaneously. We design a novel data-driven formulation which does guarantee
such holistic protection and is furthermore computationally viable. Our
distributionally robust optimization formulation can be interpreted as a novel
combination of a Kullback-Leibler and Levy-Prokhorov robust optimization
formulation which is novel in its own right. However, we show how in the
context of classification and regression problems that several popular
regularized and robust formulations reduce to a particular case of our proposed
novel formulation. Finally, we apply the proposed HR formulation on a portfolio
selection problem with real stock data, and analyze its risk/return tradeoff
against several benchmarks formulations. Our experiments show that our novel
ambiguity set provides a significantly better risk/return trade-off.",2207.09560v3,https://arxiv.org/pdf/2207.09560v3
"ESPnet-SE++: Speech Enhancement for Robust Speech Recognition,
  Translation, and Understanding","Yen-Ju Lu, Xuankai Chang, Chenda Li, Wangyou Zhang, Samuele Cornell, Zhaoheng Ni, Yoshiki Masuyama, Brian Yan, Robin Scheibler, Zhong-Qiu Wang, Yu Tsao, Yanmin Qian, Shinji Watanabe","This paper presents recent progress on integrating speech separation and
enhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE
work, numerous features have been added, including recent state-of-the-art
speech enhancement models with their respective training and evaluation
recipes. Importantly, a new interface has been designed to flexibly combine
speech enhancement front-ends with other tasks, including automatic speech
recognition (ASR), speech translation (ST), and spoken language understanding
(SLU). To showcase such integration, we performed experiments on carefully
designed synthetic datasets for noisy-reverberant multi-channel ST and SLU
tasks, which can be used as benchmark corpora for future research. In addition
to these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi- and
single-channel SE approaches. Results show that the integration of SE
front-ends with back-end tasks is a promising research direction even for tasks
besides ASR, especially in the multi-channel scenario. The code is available
online at https://github.com/ESPnet/ESPnet. The multi-channel ST and SLU
datasets, which are another contribution of this work, are released on
HuggingFace.",2207.09514v1,https://arxiv.org/pdf/2207.09514v1
"Decorrelative Network Architecture for Robust Electrocardiogram
  Classification","Christopher Wiedeman, Ge Wang","Artificial intelligence has made great progress in medical data analysis, but
the lack of robustness and trustworthiness has kept these methods from being
widely deployed. As it is not possible to train networks that are accurate in
all scenarios, models must recognize situations where they cannot operate
confidently. Bayesian deep learning methods sample the model parameter space to
estimate uncertainty, but these parameters are often subject to the same
vulnerabilities, which can be exploited by adversarial attacks. We propose a
novel ensemble approach based on feature decorrelation and Fourier partitioning
for teaching networks diverse complementary features, reducing the chance of
perturbation-based fooling. We test our approach on single and multi-channel
electrocardiogram classification, and adapt adversarial training and DVERGE
into the Bayesian ensemble framework for comparison. Our results indicate that
the combination of decorrelation and Fourier partitioning generally maintains
performance on unperturbed data while demonstrating superior robustness and
uncertainty estimation on projected gradient descent and smooth adversarial
attacks of various magnitudes. Furthermore, our approach does not require
expensive optimization with adversarial samples, adding much less compute to
the training process than adversarial training or DVERGE. These methods can be
applied to other tasks for more robust and trustworthy models.",2207.09031v4,https://arxiv.org/pdf/2207.09031v4
"Robustar: Interactive Toolbox Supporting Precise Data Annotation for
  Robust Vision Learning","Chonghan Chen, Haohan Wang, Leyang Hu, Yuhao Zhang, Shuguang Lyu, Jingcheng Wu, Xinnuo Li, Linjing Sun, Eric P. Xing","We introduce the initial release of our software Robustar, which aims to
improve the robustness of vision classification machine learning models through
a data-driven perspective. Building upon the recent understanding that the lack
of machine learning model's robustness is the tendency of the model's learning
of spurious features, we aim to solve this problem from its root at the data
perspective by removing the spurious features from the data before training. In
particular, we introduce a software that helps the users to better prepare the
data for training image classification models by allowing the users to annotate
the spurious features at the pixel level of images. To facilitate this process,
our software also leverages recent advances to help identify potential images
and pixels worthy of attention and to continue the training with newly
annotated data. Our software is hosted at the GitHub Repository
https://github.com/HaohanWang/Robustar.",2207.08944v1,https://arxiv.org/pdf/2207.08944v1
"Benchmarking Machine Learning Robustness in Covid-19 Genome Sequence
  Classification","Sarwan Ali, Bikram Sahoo, Alexander Zelikovskiy, Pin-Yu Chen, Murray Patterson","The rapid spread of the COVID-19 pandemic has resulted in an unprecedented
amount of sequence data of the SARS-CoV-2 genome -- millions of sequences and
counting. This amount of data, while being orders of magnitude beyond the
capacity of traditional approaches to understanding the diversity, dynamics,
and evolution of viruses is nonetheless a rich resource for machine learning
(ML) approaches as alternatives for extracting such important information from
these data. It is of hence utmost importance to design a framework for testing
and benchmarking the robustness of these ML models.
  This paper makes the first effort (to our knowledge) to benchmark the
robustness of ML models by simulating biological sequences with errors. In this
paper, we introduce several ways to perturb SARS-CoV-2 genome sequences to
mimic the error profiles of common sequencing platforms such as Illumina and
PacBio. We show from experiments on a wide array of ML models that some
simulation-based approaches are more robust (and accurate) than others for
specific embedding methods to certain adversarial attacks to the input
sequences. Our benchmarking framework may assist researchers in properly
assessing different ML models and help them understand the behavior of the
SARS-CoV-2 virus or avoid possible future pandemics.",2207.08898v1,https://arxiv.org/pdf/2207.08898v1
"Romanus: Robust Task Offloading in Modular Multi-Sensor Autonomous
  Driving Systems","Luke Chen, Mohanad Odema, Mohammad Abdullah Al Faruque","Due to the high performance and safety requirements of self-driving
applications, the complexity of modern autonomous driving systems (ADS) has
been growing, instigating the need for more sophisticated hardware which could
add to the energy footprint of the ADS platform. Addressing this, edge
computing is poised to encompass self-driving applications, enabling the
compute-intensive autonomy-related tasks to be offloaded for processing at
compute-capable edge servers. Nonetheless, the intricate hardware architecture
of ADS platforms, in addition to the stringent robustness demands, set forth
complications for task offloading which are unique to autonomous driving.
Hence, we present $ROMANUS$, a methodology for robust and efficient task
offloading for modular ADS platforms with multi-sensor processing pipelines.
Our methodology entails two phases: (i) the introduction of efficient
offloading points along the execution path of the involved deep learning
models, and (ii) the implementation of a runtime solution based on Deep
Reinforcement Learning to adapt the operating mode according to variations in
the perceived road scene complexity, network connectivity, and server load.
Experiments on the object detection use case demonstrated that our approach is
14.99% more energy-efficient than pure local execution while achieving a 77.06%
reduction in risky behavior from a robust-agnostic offloading baseline.",2207.08865v1,https://arxiv.org/pdf/2207.08865v1
Rethinking Data Augmentation for Robust Visual Question Answering,"Long Chen, Yuhang Zheng, Jun Xiao","Data Augmentation (DA) -- generating extra training samples beyond original
training set -- has been widely-used in today's unbiased VQA models to mitigate
the language biases. Current mainstream DA strategies are synthetic-based
methods, which synthesize new samples by either editing some visual
regions/words, or re-generating them from scratch. However, these synthetic
samples are always unnatural and error-prone. To avoid this issue, a recent DA
work composes new augmented samples by randomly pairing pristine images and
other human-written questions. Unfortunately, to guarantee augmented samples
have reasonable ground-truth answers, they manually design a set of heuristic
rules for several question types, which extremely limits its generalization
abilities. To this end, we propose a new Knowledge Distillation based Data
Augmentation for VQA, dubbed KDDAug. Specifically, we first relax the
requirements of reasonable image-question pairs, which can be easily applied to
any question types. Then, we design a knowledge distillation (KD) based answer
assignment to generate pseudo answers for all composed image-question pairs,
which are robust to both in-domain and out-of-distribution settings. Since
KDDAug is a model-agnostic DA strategy, it can be seamlessly incorporated into
any VQA architectures. Extensive ablation studies on multiple backbones and
benchmarks have demonstrated the effectiveness and generalization abilities of
KDDAug.",2207.08739v2,https://arxiv.org/pdf/2207.08739v2
"Symmetrized Robust Procrustes: Constant-Factor Approximation and Exact
  Recovery","Tal Amir, Shahar Kovalsky, Nadav Dym","The classical $\textit{Procrustes}$ problem is to find a rigid motion
(orthogonal transformation and translation) that best aligns two given
point-sets in the least-squares sense. The $\textit{Robust Procrustes}$ problem
is an important variant, in which a power-1 objective is used instead of least
squares to improve robustness to outliers. While the optimal solution of the
least-squares problem can be easily computed in closed form, dating back to
Sch\""onemann (1966), no such solution is known for the power-1 problem. In this
paper we propose a novel convex relaxation for the Robust Procrustes problem.
Our relaxation enjoys several theoretical and practical advantages:
Theoretically, we prove that our method provides a $\sqrt{2}$-factor
approximation to the Robust Procrustes problem, and that, under appropriate
assumptions, it exactly recovers the true rigid motion from point
correspondences contaminated by outliers. In practice, we find in numerical
experiments on both synthetic and real robust Procrustes problems, that our
method performs similarly to the standard Iteratively Reweighted Least Squares
(IRLS). However the convexity of our algorithm allows incorporating additional
convex penalties, which are not readily amenable to IRLS. This turns out to be
a substantial advantage, leading to improved results in high-dimensional
problems, including non-rigid shape alignment and semi-supervised interlingual
word translation.",2207.08592v1,https://arxiv.org/pdf/2207.08592v1
"Robust Simulation-Based Inference in Cosmology with Bayesian Neural
  Networks","Pablo Lemos, Miles Cranmer, Muntazir Abidi, ChangHoon Hahn, Michael Eickenberg, Elena Massara, David Yallup, Shirley Ho","Simulation-based inference (SBI) is rapidly establishing itself as a standard
machine learning technique for analyzing data in cosmological surveys. Despite
continual improvements to the quality of density estimation by learned models,
applications of such techniques to real data are entirely reliant on the
generalization power of neural networks far outside the training distribution,
which is mostly unconstrained. Due to the imperfections in scientist-created
simulations, and the large computational expense of generating all possible
parameter combinations, SBI methods in cosmology are vulnerable to such
generalization issues. Here, we discuss the effects of both issues, and show
how using a Bayesian neural network framework for training SBI can mitigate
biases, and result in more reliable inference outside the training set. We
introduce cosmoSWAG, the first application of Stochastic Weight Averaging to
cosmology, and apply it to SBI trained for inference on the cosmic microwave
background.",2207.08435v3,https://arxiv.org/pdf/2207.08435v3
"Robust Action Governor for Uncertain Piecewise Affine Systems with
  Non-convex Constraints and Safe Reinforcement Learning","Yutong Li, Nan Li, H. Eric Tseng, Anouck Girard, Dimitar Filev, Ilya Kolmanovsky","The action governor is an add-on scheme to a nominal control loop that
monitors and adjusts the control actions to enforce safety specifications
expressed as pointwise-in-time state and control constraints. In this paper, we
introduce the Robust Action Governor (RAG) for systems the dynamics of which
can be represented using discrete-time Piecewise Affine (PWA) models with both
parametric and additive uncertainties and subject to non-convex constraints. We
develop the theoretical properties and computational approaches for the RAG.
After that, we introduce the use of the RAG for realizing safe Reinforcement
Learning (RL), i.e., ensuring all-time constraint satisfaction during online RL
exploration-and-exploitation process. This development enables safe real-time
evolution of the control policy and adaptation to changes in the operating
environment and system parameters (due to aging, damage, etc.). We illustrate
the effectiveness of the RAG in constraint enforcement and safe RL using the
RAG by considering their applications to a soft-landing problem of a
mass-spring-damper system.",2207.08240v1,https://arxiv.org/pdf/2207.08240v1
"MixTailor: Mixed Gradient Aggregation for Robust Learning Against
  Tailored Attacks","Ali Ramezani-Kebrya, Iman Tabrizian, Fartash Faghri, Petar Popovski","Implementations of SGD on distributed systems create new vulnerabilities,
which can be identified and misused by one or more adversarial agents.
Recently, it has been shown that well-known Byzantine-resilient gradient
aggregation schemes are indeed vulnerable to informed attackers that can tailor
the attacks (Fang et al., 2020; Xie et al., 2020b). We introduce MixTailor, a
scheme based on randomization of the aggregation strategies that makes it
impossible for the attacker to be fully informed. Deterministic schemes can be
integrated into MixTailor on the fly without introducing any additional
hyperparameters. Randomization decreases the capability of a powerful adversary
to tailor its attacks, while the resulting randomized aggregation scheme is
still competitive in terms of performance. For both iid and non-iid settings,
we establish almost sure convergence guarantees that are both stronger and more
general than those available in the literature. Our empirical studies across
various datasets, attacks, and settings, validate our hypothesis and show that
MixTailor successfully defends when well-known Byzantine-tolerant schemes fail.",2207.07941v2,https://arxiv.org/pdf/2207.07941v2
Adaptive Sketches for Robust Regression with Importance Sampling,"Sepideh Mahabadi, David P. Woodruff, Samson Zhou","We introduce data structures for solving robust regression through stochastic
gradient descent (SGD) by sampling gradients with probability proportional to
their norm, i.e., importance sampling. Although SGD is widely used for large
scale machine learning, it is well-known for possibly experiencing slow
convergence rates due to the high variance from uniform sampling. On the other
hand, importance sampling can significantly decrease the variance but is
usually difficult to implement because computing the sampling probabilities
requires additional passes over the data, in which case standard gradient
descent (GD) could be used instead. In this paper, we introduce an algorithm
that approximately samples $T$ gradients of dimension $d$ from nearly the
optimal importance sampling distribution for a robust regression problem over
$n$ rows. Thus our algorithm effectively runs $T$ steps of SGD with importance
sampling while using sublinear space and just making a single pass over the
data. Our techniques also extend to performing importance sampling for
second-order optimization.",2207.07822v1,https://arxiv.org/pdf/2207.07822v1
CARBEN: Composite Adversarial Robustness Benchmark,"Lei Hsiung, Yun-Yun Tsai, Pin-Yu Chen, Tsung-Yi Ho","Prior literature on adversarial attack methods has mainly focused on
attacking with and defending against a single threat model, e.g., perturbations
bounded in Lp ball. However, multiple threat models can be combined into
composite perturbations. One such approach, composite adversarial attack (CAA),
not only expands the perturbable space of the image, but also may be overlooked
by current modes of robustness evaluation. This paper demonstrates how CAA's
attack order affects the resulting image, and provides real-time inferences of
different models, which will facilitate users' configuration of the parameters
of the attack level and their rapid evaluation of model prediction. A
leaderboard to benchmark adversarial robustness against CAA is also introduced.",2207.07797v1,https://arxiv.org/pdf/2207.07797v1
3DVerifier: Efficient Robustness Verification for 3D Point Cloud Models,"Ronghui Mu, Wenjie Ruan, Leandro S. Marcolino, Qiang Ni","3D point cloud models are widely applied in safety-critical scenes, which
delivers an urgent need to obtain more solid proofs to verify the robustness of
models. Existing verification method for point cloud model is time-expensive
and computationally unattainable on large networks. Additionally, they cannot
handle the complete PointNet model with joint alignment network (JANet) that
contains multiplication layers, which effectively boosts the performance of 3D
models. This motivates us to design a more efficient and general framework to
verify various architectures of point cloud models. The key challenges in
verifying the large-scale complete PointNet models are addressed as dealing
with the cross-non-linearity operations in the multiplication layers and the
high computational complexity of high-dimensional point cloud inputs and added
layers. Thus, we propose an efficient verification framework, 3DVerifier, to
tackle both challenges by adopting a linear relaxation function to bound the
multiplication layer and combining forward and backward propagation to compute
the certified bounds of the outputs of the point cloud models. Our
comprehensive experiments demonstrate that 3DVerifier outperforms existing
verification algorithms for 3D models in terms of both efficiency and accuracy.
Notably, our approach achieves an orders-of-magnitude improvement in
verification efficiency for the large network, and the obtained certified
bounds are also significantly tighter than the state-of-the-art verifiers. We
release our tool 3DVerifier via https://github.com/TrustAI/3DVerifier for use
by the community.",2207.07539v1,https://arxiv.org/pdf/2207.07539v1
"Robust Deep Compressive Sensing with Recurrent-Residual Structural
  Constraints",Jun Niu,"Existing deep compressive sensing (CS) methods either ignore adaptive online
optimization or depend on costly iterative optimizer during reconstruction.
This work explores a novel image CS framework with recurrent-residual
structural constraint, termed as R$^2$CS-NET. The R$^2$CS-NET first
progressively optimizes the acquired samplings through a novel recurrent neural
network. The cascaded residual convolutional network then fully reconstructs
the image from optimized latent representation. As the first deep CS framework
efficiently bridging adaptive online optimization, the R$^2$CS-NET integrates
the robustness of online optimization with the efficiency and nonlinear
capacity of deep learning methods. Signal correlation has been addressed
through the network architecture. The adaptive sensing nature further makes it
an ideal candidate for color image CS via leveraging channel correlation.
Numerical experiments verify the proposed recurrent latent optimization design
not only fulfills the adaptation motivation, but also outperforms classic long
short-term memory (LSTM) architecture in the same scenario. The overall
framework demonstrates hardware implementation feasibility, with leading
robustness and generalization capability among existing deep CS benchmarks.",2207.07301v1,https://arxiv.org/pdf/2207.07301v1
"Improving Task-free Continual Learning by Distributionally Robust Memory
  Evolution","Zhenyi Wang, Li Shen, Le Fang, Qiuling Suo, Tiehang Duan, Mingchen Gao","Task-free continual learning (CL) aims to learn a non-stationary data stream
without explicit task definitions and not forget previous knowledge. The widely
adopted memory replay approach could gradually become less effective for long
data streams, as the model may memorize the stored examples and overfit the
memory buffer. Second, existing methods overlook the high uncertainty in the
memory data distribution since there is a big gap between the memory data
distribution and the distribution of all the previous data examples. To address
these problems, for the first time, we propose a principled memory evolution
framework to dynamically evolve the memory data distribution by making the
memory buffer gradually harder to be memorized with distributionally robust
optimization (DRO). We then derive a family of methods to evolve the memory
buffer data in the continuous probability measure space with Wasserstein
gradient flow (WGF). The proposed DRO is w.r.t the worst-case evolved memory
data distribution, thus guarantees the model performance and learns
significantly more robust features than existing memory-replay-based methods.
Extensive experiments on existing benchmarks demonstrate the effectiveness of
the proposed methods for alleviating forgetting. As a by-product of the
proposed framework, our method is more robust to adversarial examples than
existing task-free CL methods. Code is available on GitHub
\url{https://github.com/joey-wang123/DRO-Task-free}",2207.07256v2,https://arxiv.org/pdf/2207.07256v2
Provably Adversarially Robust Nearest Prototype Classifiers,"Václav Voráček, Matthias Hein","Nearest prototype classifiers (NPCs) assign to each input point the label of
the nearest prototype with respect to a chosen distance metric. A direct
advantage of NPCs is that the decisions are interpretable. Previous work could
provide lower bounds on the minimal adversarial perturbation in the
$\ell_p$-threat model when using the same $\ell_p$-distance for the NPCs. In
this paper we provide a complete discussion on the complexity when using
$\ell_p$-distances for decision and $\ell_q$-threat models for certification
for $p,q \in \{1,2,\infty\}$. In particular we provide scalable algorithms for
the \emph{exact} computation of the minimal adversarial perturbation when using
$\ell_2$-distance and improved lower bounds in other cases. Using efficient
improved lower bounds we train our Provably adversarially robust NPC (PNPC),
for MNIST which have better $\ell_2$-robustness guarantees than neural
networks. Additionally, we show up to our knowledge the first certification
results w.r.t. to the LPIPS perceptual metric which has been argued to be a
more realistic threat model for image classification than $\ell_p$-balls. Our
PNPC has on CIFAR10 higher certified robust accuracy than the empirical robust
accuracy reported in (Laidlaw et al., 2021). The code is available in our
repository.",2207.07208v1,https://arxiv.org/pdf/2207.07208v1
Contrastive Adapters for Foundation Model Group Robustness,"Michael Zhang, Christopher Ré","While large pretrained foundation models (FMs) have shown remarkable
zero-shot classification robustness to dataset-level distribution shifts, their
robustness to subpopulation or group shifts is relatively underexplored. We
study this problem, and find that FMs such as CLIP may not be robust to various
group shifts. Across 9 robustness benchmarks, zero-shot classification with
their embeddings results in gaps of up to 80.7 percentage points (pp) between
average and worst-group accuracy. Unfortunately, existing methods to improve
robustness require retraining, which can be prohibitively expensive on large
foundation models. We also find that efficient ways to improve model inference
(e.g., via adapters, lightweight networks with FM embeddings as inputs) do not
consistently improve and can sometimes hurt group robustness compared to
zero-shot (e.g., increasing the accuracy gap by 50.1 pp on CelebA). We thus
develop an adapter training strategy to effectively and efficiently improve FM
group robustness. Our motivating observation is that while poor robustness
results from groups in the same class being embedded far apart in the
foundation model ""embedding space,"" standard adapter training may not bring
these points closer together. We thus propose contrastive adapting, which
trains adapters with contrastive learning to bring sample embeddings close to
both their ground-truth class embeddings and other sample embeddings in the
same class. Across the 9 benchmarks, our approach consistently improves group
robustness, raising worst-group accuracy by 8.5 to 56.0 pp over zero-shot. Our
approach is also efficient, doing so without any FM finetuning and only a fixed
set of frozen FM embeddings. On benchmarks such as Waterbirds and CelebA, this
leads to worst-group accuracy comparable to state-of-the-art methods that
retrain entire models, while only training $\leq$1% of the model parameters.",2207.07180v1,https://arxiv.org/pdf/2207.07180v1
"Work In Progress: Safety and Robustness Verification of
  Autoencoder-Based Regression Models using the NNV Tool","Neelanjana Pal, Taylor T Johnson","This work in progress paper introduces robustness verification for
autoencoder-based regression neural network (NN) models, following
state-of-the-art approaches for robustness verification of image classification
NNs. Despite the ongoing progress in developing verification methods for safety
and robustness in various deep neural networks (DNNs), robustness checking of
autoencoder models has not yet been considered. We explore this open space of
research and check ways to bridge the gap between existing DNN verification
methods by extending existing robustness analysis methods for such autoencoder
networks. While classification models using autoencoders work more or less
similar to image classification NNs, the functionality of regression models is
distinctly different. We introduce two definitions of robustness evaluation
metrics for autoencoder-based regression models, specifically the percentage
robustness and un-robustness grade. We also modified the existing Imagestar
approach, adjusting the variables to take care of the specific input types for
regression networks. The approach is implemented as an extension of NNV, then
applied and evaluated on a dataset, with a case study experiment shown using
the same dataset. As per the authors' understanding, this work in progress
paper is the first to show possible reachability analysis of autoencoder-based
NNs.",2207.06759v1,https://arxiv.org/pdf/2207.06759v1
Learning robust marking policies for adaptive mesh refinement,"Andrew Gillette, Brendan Keith, Socratis Petrides","In this work, we revisit the marking decisions made in the standard adaptive
finite element method (AFEM). Experience shows that a na\""{i}ve marking policy
leads to inefficient use of computational resources for adaptive mesh
refinement (AMR). Consequently, using AFEM in practice often involves ad-hoc or
time-consuming offline parameter tuning to set appropriate parameters for the
marking subroutine. To address these practical concerns, we recast AMR as a
Markov decision process in which refinement parameters can be selected
on-the-fly at run time, without the need for pre-tuning by expert users. In
this new paradigm, the refinement parameters are also chosen adaptively via a
marking policy that can be optimized using methods from reinforcement learning.
We use the Poisson equation to demonstrate our techniques on $h$- and
$hp$-refinement benchmark problems, and our experiments suggest that superior
marking policies remain undiscovered for many classical AFEM applications.
Furthermore, an unexpected observation from this work is that marking policies
trained on one family of PDEs are sometimes robust enough to perform well on
problems far outside the training family. For illustration, we show that a
simple $hp$-refinement policy trained on 2D domains with only a single
re-entrant corner can be deployed on far more complicated 2D domains, and even
3D domains, without significant performance loss. For reproduction and broader
adoption, we accompany this work with an open-source implementation of our
methods.",2207.06339v2,https://arxiv.org/pdf/2207.06339v2
On the Robustness of Bayesian Neural Networks to Adversarial Attacks,"Luca Bortolussi, Ginevra Carbone, Luca Laurenti, Andrea Patane, Guido Sanguinetti, Matthew Wicker","Vulnerability to adversarial attacks is one of the principal hurdles to the
adoption of deep learning in safety-critical applications. Despite significant
efforts, both practical and theoretical, training deep learning models robust
to adversarial attacks is still an open problem. In this paper, we analyse the
geometry of adversarial attacks in the large-data, overparameterized limit for
Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to
gradient-based attacks arises as a result of degeneracy in the data
distribution, i.e., when the data lies on a lower-dimensional submanifold of
the ambient space. As a direct consequence, we demonstrate that in this limit
BNN posteriors are robust to gradient-based adversarial attacks. Crucially, we
prove that the expected gradient of the loss with respect to the BNN posterior
distribution is vanishing, even when each neural network sampled from the
posterior is vulnerable to gradient-based attacks. Experimental results on the
MNIST, Fashion MNIST, and half moons datasets, representing the finite data
regime, with BNNs trained with Hamiltonian Monte Carlo and Variational
Inference, support this line of arguments, showing that BNNs can display both
high accuracy on clean data and robustness to both gradient-based and
gradient-free based adversarial attacks.",2207.06154v3,https://arxiv.org/pdf/2207.06154v3
"Probing the Robustness of Independent Mechanism Analysis for
  Representation Learning","Joanna Sliwa, Shubhangi Ghosh, Vincent Stimper, Luigi Gresele, Bernhard Schölkopf","One aim of representation learning is to recover the original latent code
that generated the data, a task which requires additional information or
inductive biases. A recently proposed approach termed Independent Mechanism
Analysis (IMA) postulates that each latent source should influence the observed
mixtures independently, complementing standard nonlinear independent component
analysis, and taking inspiration from the principle of independent causal
mechanisms. While it was shown in theory and experiments that IMA helps
recovering the true latents, the method's performance was so far only
characterized when the modeling assumptions are exactly satisfied. Here, we
test the method's robustness to violations of the underlying assumptions. We
find that the benefits of IMA-based regularization for recovering the true
sources extend to mixing functions with various degrees of violation of the IMA
principle, while standard regularizers do not provide the same merits.
Moreover, we show that unregularized maximum likelihood recovers mixing
functions which systematically deviate from the IMA principle, and provide an
argument elucidating the benefits of IMA-based regularization.",2207.06137v1,https://arxiv.org/pdf/2207.06137v1
"Visual Context-driven Audio Feature Enhancement for Robust End-to-End
  Audio-Visual Speech Recognition","Joanna Hong, Minsu Kim, Daehun Yoo, Yong Man Ro","This paper focuses on designing a noise-robust end-to-end Audio-Visual Speech
Recognition (AVSR) system. To this end, we propose Visual Context-driven Audio
Feature Enhancement module (V-CAFE) to enhance the input noisy audio speech
with a help of audio-visual correspondence. The proposed V-CAFE is designed to
capture the transition of lip movements, namely visual context and to generate
a noise reduction mask by considering the obtained visual context. Through
context-dependent modeling, the ambiguity in viseme-to-phoneme mapping can be
refined for mask generation. The noisy representations are masked out with the
noise reduction mask resulting in enhanced audio features. The enhanced audio
features are fused with the visual features and taken to an encoder-decoder
model composed of Conformer and Transformer for speech recognition. We show the
proposed end-to-end AVSR with the V-CAFE can further improve the
noise-robustness of AVSR. The effectiveness of the proposed method is evaluated
in noisy speech recognition and overlapped speech recognition experiments using
the two largest audio-visual datasets, LRS2 and LRS3.",2207.06020v1,https://arxiv.org/pdf/2207.06020v1
"RobustAnalog: Fast Variation-Aware Analog Circuit Design Via Multi-task
  RL","Wei Shi, Hanrui Wang, Jiaqi Gu, Mingjie Liu, David Pan, Song Han, Nan Sun","Analog/mixed-signal circuit design is one of the most complex and
time-consuming stages in the whole chip design process. Due to various process,
voltage, and temperature (PVT) variations from chip manufacturing, analog
circuits inevitably suffer from performance degradation. Although there has
been plenty of work on automating analog circuit design under the typical
condition, limited research has been done on exploring robust designs under
real and unpredictable silicon variations. Automatic analog design against
variations requires prohibitive computation and time costs. To address the
challenge, we present RobustAnalog, a robust circuit design framework that
involves the variation information in the optimization process. Specifically,
circuit optimizations under different variations are considered as a set of
tasks. Similarities among tasks are leveraged and competitions are alleviated
to realize a sample-efficient multi-task training. Moreover, RobustAnalog
prunes the task space according to the current performance in each iteration,
leading to a further simulation cost reduction. In this way, RobustAnalog can
rapidly produce a set of circuit parameters that satisfies diverse constraints
(e.g. gain, bandwidth, noise...) across variations. We compare RobustAnalog
with Bayesian optimization, Evolutionary algorithm, and Deep Deterministic
Policy Gradient (DDPG) and demonstrate that RobustAnalog can significantly
reduce required optimization time by 14-30 times. Therefore, our study provides
a feasible method to handle various real silicon conditions.",2207.06412v1,https://arxiv.org/pdf/2207.06412v1
"Long Term Fairness for Minority Groups via Performative Distributionally
  Robust Optimization","Liam Peet-Pare, Nidhi Hegde, Alona Fyshe","Fairness researchers in machine learning (ML) have coalesced around several
fairness criteria which provide formal definitions of what it means for an ML
model to be fair. However, these criteria have some serious limitations. We
identify four key shortcomings of these formal fairness criteria, and aim to
help to address them by extending performative prediction to include a
distributionally robust objective.",2207.05777v1,https://arxiv.org/pdf/2207.05777v1
"Robust and efficient computation of retinal fractal dimension through
  deep approximation","Justin Engelmann, Ana Villaplana-Velasco, Amos Storkey, Miguel O. Bernabeu","A retinal trait, or phenotype, summarises a specific aspect of a retinal
image in a single number. This can then be used for further analyses, e.g. with
statistical methods. However, reducing an aspect of a complex image to a
single, meaningful number is challenging. Thus, methods for calculating retinal
traits tend to be complex, multi-step pipelines that can only be applied to
high quality images. This means that researchers often have to discard
substantial portions of the available data. We hypothesise that such pipelines
can be approximated with a single, simpler step that can be made robust to
common quality issues. We propose Deep Approximation of Retinal Traits (DART)
where a deep neural network is used predict the output of an existing pipeline
on high quality images from synthetically degraded versions of these images. We
demonstrate DART on retinal Fractal Dimension (FD) calculated by VAMPIRE, using
retinal images from UK Biobank that previous work identified as high quality.
Our method shows very high agreement with FD VAMPIRE on unseen test images
(Pearson r=0.9572). Even when those images are severely degraded, DART can
still recover an FD estimate that shows good agreement with FD VAMPIRE obtained
from the original images (Pearson r=0.8817). This suggests that our method
could enable researchers to discard fewer images in the future. Our method can
compute FD for over 1,000img/s using a single GPU. We consider these to be very
encouraging initial results and hope to develop this approach into a useful
tool for retinal analysis.",2207.05757v1,https://arxiv.org/pdf/2207.05757v1
"Exploring Adversarial Examples and Adversarial Robustness of
  Convolutional Neural Networks by Mutual Information","Jiebao Zhang, Wenhua Qian, Rencan Nie, Jinde Cao, Dan Xu","A counter-intuitive property of convolutional neural networks (CNNs) is their
inherent susceptibility to adversarial examples, which severely hinders the
application of CNNs in security-critical fields. Adversarial examples are
similar to original examples but contain malicious perturbations. Adversarial
training is a simple and effective defense method to improve the robustness of
CNNs to adversarial examples. The mechanisms behind adversarial examples and
adversarial training are worth exploring. Therefore, this work investigates
similarities and differences between normally trained CNNs (NT-CNNs) and
adversarially trained CNNs (AT-CNNs) in information extraction from the mutual
information perspective. We show that 1) whether NT-CNNs or AT-CNNs, for
original and adversarial examples, the trends towards mutual information are
almost similar throughout training; 2) compared with normal training,
adversarial training is more difficult and the amount of information that
AT-CNNs extract from the input is less; 3) the CNNs trained with different
methods have different preferences for certain types of information; NT-CNNs
tend to extract texture-based information from the input, while AT-CNNs prefer
to shape-based information. The reason why adversarial examples mislead CNNs
may be that they contain more texture-based information about other classes.
Furthermore, we also analyze the mutual information estimators used in this
work and find that they outline the geometric properties of the middle layer's
output.",2207.05756v2,https://arxiv.org/pdf/2207.05756v2
Adversarial Robustness Assessment of NeuroEvolution Approaches,"Inês Valentim, Nuno Lourenço, Nuno Antunes","NeuroEvolution automates the generation of Artificial Neural Networks through
the application of techniques from Evolutionary Computation. The main goal of
these approaches is to build models that maximize predictive performance,
sometimes with an additional objective of minimizing computational complexity.
Although the evolved models achieve competitive results performance-wise, their
robustness to adversarial examples, which becomes a concern in
security-critical scenarios, has received limited attention. In this paper, we
evaluate the adversarial robustness of models found by two prominent
NeuroEvolution approaches on the CIFAR-10 image classification task: DENSER and
NSGA-Net. Since the models are publicly available, we consider white-box
untargeted attacks, where the perturbations are bounded by either the L2 or the
Linfinity-norm. Similarly to manually-designed networks, our results show that
when the evolved models are attacked with iterative methods, their accuracy
usually drops to, or close to, zero under both distance metrics. The DENSER
model is an exception to this trend, showing some resistance under the L2
threat model, where its accuracy only drops from 93.70% to 18.10% even with
iterative attacks. Additionally, we analyzed the impact of pre-processing
applied to the data before the first layer of the network. Our observations
suggest that some of these techniques can exacerbate the perturbations added to
the original inputs, potentially harming robustness. Thus, this choice should
not be neglected when automatically designing networks for applications where
adversarial attacks are prone to occur.",2207.05451v1,https://arxiv.org/pdf/2207.05451v1
"Bi-fidelity Evolutionary Multiobjective Search for Adversarially Robust
  Deep Neural Architectures","Jia Liu, Ran Cheng, Yaochu Jin","Deep neural networks have been found vulnerable to adversarial attacks, thus
raising potentially concerns in security-sensitive contexts. To address this
problem, recent research has investigated the adversarial robustness of deep
neural networks from the architectural point of view. However, searching for
architectures of deep neural networks is computationally expensive,
particularly when coupled with adversarial training process. To meet the above
challenge, this paper proposes a bi-fidelity multiobjective neural architecture
search approach. First, we formulate the NAS problem for enhancing adversarial
robustness of deep neural networks into a multiobjective optimization problem.
Specifically, in addition to a low-fidelity performance predictor as the first
objective, we leverage an auxiliary-objective -- the value of which is the
output of a surrogate model trained with high-fidelity evaluations. Secondly,
we reduce the computational cost by combining three performance estimation
methods, i.e., parameter sharing, low-fidelity evaluation, and surrogate-based
predictor. The effectiveness of the proposed approach is confirmed by extensive
experiments conducted on CIFAR-10, CIFAR-100 and SVHN datasets.",2207.05321v1,https://arxiv.org/pdf/2207.05321v1
"TabSynDex: A Universal Metric for Robust Evaluation of Synthetic Tabular
  Data","Vikram S Chundawat, Ayush K Tarun, Murari Mandal, Mukund Lahoti, Pratik Narang","Synthetic tabular data generation becomes crucial when real data is limited,
expensive to collect, or simply cannot be used due to privacy concerns.
However, producing good quality synthetic data is challenging. Several
probabilistic, statistical, generative adversarial networks (GANs), and
variational auto-encoder (VAEs) based approaches have been presented for
synthetic tabular data generation. Once generated, evaluating the quality of
the synthetic data is quite challenging. Some of the traditional metrics have
been used in the literature but there is lack of a common, robust, and single
metric. This makes it difficult to properly compare the effectiveness of
different synthetic tabular data generation methods. In this paper we propose a
new universal metric, TabSynDex, for robust evaluation of synthetic data. The
proposed metric assesses the similarity of synthetic data with real data
through different component scores which evaluate the characteristics that are
desirable for ``high quality'' synthetic data. Being a single score metric and
having an implicit bound, TabSynDex can also be used to observe and evaluate
the training of neural network based approaches. This would help in obtaining
insights that was not possible earlier. We present several baseline models for
comparative analysis of the proposed evaluation metric with existing generative
models. We also give a comparative analysis between TabSynDex and existing
synthetic tabular data evaluation metrics. This shows the effectiveness and
universality of our metric over the existing metrics. Source Code:
\url{https://github.com/vikram2000b/tabsyndex}",2207.05295v2,https://arxiv.org/pdf/2207.05295v2
RUSH: Robust Contrastive Learning via Randomized Smoothing,"Yijiang Pang, Boyang Liu, Jiayu Zhou","Recently, adversarial training has been incorporated in self-supervised
contrastive pre-training to augment label efficiency with exciting adversarial
robustness. However, the robustness came at a cost of expensive adversarial
training. In this paper, we show a surprising fact that contrastive
pre-training has an interesting yet implicit connection with robustness, and
such natural robustness in the pre trained representation enables us to design
a powerful robust algorithm against adversarial attacks, RUSH, that combines
the standard contrastive pre-training and randomized smoothing. It boosts both
standard accuracy and robust accuracy, and significantly reduces training costs
as compared with adversarial training. We use extensive empirical studies to
show that the proposed RUSH outperforms robust classifiers from adversarial
training, by a significant margin on common benchmarks (CIFAR-10, CIFAR-100,
and STL-10) under first-order attacks. In particular, under
$\ell_{\infty}$-norm perturbations of size 8/255 PGD attack on CIFAR-10, our
model using ResNet-18 as backbone reached 77.8% robust accuracy and 87.9%
standard accuracy. Our work has an improvement of over 15% in robust accuracy
and a slight improvement in standard accuracy, compared to the
state-of-the-arts.",2207.05127v2,https://arxiv.org/pdf/2207.05127v2
"Generalizing to Unseen Domains with Wasserstein Distributional
  Robustness under Limited Source Knowledge","Jingge Wang, Liyan Xie, Yao Xie, Shao-Lun Huang, Yang Li","Domain generalization aims at learning a universal model that performs well
on unseen target domains, incorporating knowledge from multiple source domains.
In this research, we consider the scenario where different domain shifts occur
among conditional distributions of different classes across domains. When
labeled samples in the source domains are limited, existing approaches are not
sufficiently robust. To address this problem, we propose a novel domain
generalization framework called {Wasserstein Distributionally Robust Domain
Generalization} (WDRDG), inspired by the concept of distributionally robust
optimization. We encourage robustness over conditional distributions within
class-specific Wasserstein uncertainty sets and optimize the worst-case
performance of a classifier over these uncertainty sets. We further develop a
test-time adaptation module leveraging optimal transport to quantify the
relationship between the unseen target domain and source domains to make
adaptive inference for target data. Experiments on the Rotated MNIST, PACS and
the VLCS datasets demonstrate that our method could effectively balance the
robustness and discriminability in challenging generalization scenarios.",2207.04913v2,https://arxiv.org/pdf/2207.04913v2
"How Robust is your Fair Model? Exploring the Robustness of Diverse
  Fairness Strategies","Edward Small, Wei Shao, Zeliang Zhang, Peihan Liu, Jeffrey Chan, Kacper Sokol, Flora Salim","With the introduction of machine learning in high-stakes decision making,
ensuring algorithmic fairness has become an increasingly important problem to
solve. In response to this, many mathematical definitions of fairness have been
proposed, and a variety of optimisation techniques have been developed, all
designed to maximise a defined notion of fairness. However, fair solutions are
reliant on the quality of the training data, and can be highly sensitive to
noise. Recent studies have shown that robustness (the ability for a model to
perform well on unseen data) plays a significant role in the type of strategy
that should be used when approaching a new problem and, hence, measuring the
robustness of these strategies has become a fundamental problem. In this work,
we therefore propose a new criterion to measure the robustness of various
fairness optimisation strategies - the robustness ratio. We conduct multiple
extensive experiments on five bench mark fairness data sets using three of the
most popular fairness strategies with respect to four of the most popular
definitions of fairness. Our experiments empirically show that fairness methods
that rely on threshold optimisation are very sensitive to noise in all the
evaluated data sets, despite mostly outperforming other methods. This is in
contrast to the other two methods, which are less fair for low noise scenarios
but fairer for high noise ones. To the best of our knowledge, we are the first
to quantitatively evaluate the robustness of fairness optimisation strategies.
This can potentially can serve as a guideline in choosing the most suitable
fairness strategy for various data sets.",2207.04581v4,https://arxiv.org/pdf/2207.04581v4
"Adversarial Framework with Certified Robustness for Time-Series Domain
  via Statistical Features","Taha Belkhouja, Janardhan Rao Doppa","Time-series data arises in many real-world applications (e.g., mobile health)
and deep neural networks (DNNs) have shown great success in solving them.
Despite their success, little is known about their robustness to adversarial
attacks. In this paper, we propose a novel adversarial framework referred to as
Time-Series Attacks via STATistical Features (TSA-STAT)}. To address the unique
challenges of time-series domain, TSA-STAT employs constraints on statistical
features of the time-series data to construct adversarial examples. Optimized
polynomial transformations are used to create attacks that are more effective
(in terms of successfully fooling DNNs) than those based on additive
perturbations. We also provide certified bounds on the norm of the statistical
features for constructing adversarial examples. Our experiments on diverse
real-world benchmark datasets show the effectiveness of TSA-STAT in fooling
DNNs for time-series domain and in improving their robustness. The source code
of TSA-STAT algorithms is available at
https://github.com/tahabelkhouja/Time-Series-Attacks-via-STATistical-Features",2207.04307v1,https://arxiv.org/pdf/2207.04307v1
"Training Robust Deep Models for Time-Series Domain: Novel Algorithms and
  Theoretical Analysis","Taha Belkhouja, Yan Yan, Janardhan Rao Doppa","Despite the success of deep neural networks (DNNs) for real-world
applications over time-series data such as mobile health, little is known about
how to train robust DNNs for time-series domain due to its unique
characteristics compared to images and text data. In this paper, we propose a
novel algorithmic framework referred as RObust Training for Time-Series (RO-TS)
to create robust DNNs for time-series classification tasks. Specifically, we
formulate a min-max optimization problem over the model parameters by
explicitly reasoning about the robustness criteria in terms of additive
perturbations to time-series inputs measured by the global alignment kernel
(GAK) based distance. We also show the generality and advantages of our
formulation using the summation structure over time-series alignments by
relating both GAK and dynamic time warping (DTW). This problem is an instance
of a family of compositional min-max optimization problems, which are
challenging and open with unclear theoretical guarantee. We propose a
principled stochastic compositional alternating gradient descent ascent
(SCAGDA) algorithm for this family of optimization problems. Unlike traditional
methods for time-series that require approximate computation of distance
measures, SCAGDA approximates the GAK based distance on-the-fly using a moving
average approach. We theoretically analyze the convergence rate of SCAGDA and
provide strong theoretical support for the estimation of GAK based distance.
Our experiments on real-world benchmarks demonstrate that RO-TS creates more
robust DNNs when compared to adversarial training using prior methods that rely
on data augmentation or new definitions of loss functions. We also demonstrate
the importance of GAK for time-series data over the Euclidean distance. The
source code of RO-TS algorithms is available at
https://github.com/tahabelkhouja/Robust-Training-for-Time-Series",2207.04305v2,https://arxiv.org/pdf/2207.04305v2
Multiple Robust Learning for Recommendation,"Haoxuan Li, Quanyu Dai, Yuru Li, Yan Lyu, Zhenhua Dong, Xiao-Hua Zhou, Peng Wu","In recommender systems, a common problem is the presence of various biases in
the collected data, which deteriorates the generalization ability of the
recommendation models and leads to inaccurate predictions. Doubly robust (DR)
learning has been studied in many tasks in RS, with the advantage that unbiased
learning can be achieved when either a single imputation or a single propensity
model is accurate. In this paper, we propose a multiple robust (MR) estimator
that can take the advantage of multiple candidate imputation and propensity
models to achieve unbiasedness. Specifically, the MR estimator is unbiased when
any of the imputation or propensity models, or a linear combination of these
models is accurate. Theoretical analysis shows that the proposed MR is an
enhanced version of DR when only having a single imputation and propensity
model, and has a smaller bias. Inspired by the generalization error bound of
MR, we further propose a novel multiple robust learning approach with
stabilization. We conduct extensive experiments on real-world and
semi-synthetic datasets, which demonstrates the superiority of the proposed
approach over state-of-the-art methods.",2207.10796v4,https://arxiv.org/pdf/2207.10796v4
On the Robustness and Anomaly Detection of Sparse Neural Networks,"Morgane Ayle, Bertrand Charpentier, John Rachwan, Daniel Zügner, Simon Geisler, Stephan Günnemann","The robustness and anomaly detection capability of neural networks are
crucial topics for their safe adoption in the real-world. Moreover, the
over-parameterization of recent networks comes with high computational costs
and raises questions about its influence on robustness and anomaly detection.
In this work, we show that sparsity can make networks more robust and better
anomaly detectors. To motivate this even further, we show that a pre-trained
neural network contains, within its parameter space, sparse subnetworks that
are better at these tasks without any further training. We also show that
structured sparsity greatly helps in reducing the complexity of expensive
robustness and detection methods, while maintaining or even improving their
results on these tasks. Finally, we introduce a new method, SensNorm, which
uses the sensitivity of weights derived from an appropriate pruning method to
detect anomalous samples in the input.",2207.04227v1,https://arxiv.org/pdf/2207.04227v1
"BOSS: Bottom-up Cross-modal Semantic Composition with Hybrid
  Counterfactual Training for Robust Content-based Image Retrieval","Wenqiao Zhang, Jiannan Guo, Mengze Li, Haochen Shi, Shengyu Zhang, Juncheng Li, Siliang Tang, Yueting Zhuang","Content-Based Image Retrieval (CIR) aims to search for a target image by
concurrently comprehending the composition of an example image and a
complementary text, which potentially impacts a wide variety of real-world
applications, such as internet search and fashion retrieval. In this scenario,
the input image serves as an intuitive context and background for the search,
while the corresponding language expressly requests new traits on how specific
characteristics of the query image should be modified in order to get the
intended target image. This task is challenging since it necessitates learning
and understanding the composite image-text representation by incorporating
cross-granular semantic updates. In this paper, we tackle this task by a novel
\underline{\textbf{B}}ottom-up cr\underline{\textbf{O}}ss-modal
\underline{\textbf{S}}emantic compo\underline{\textbf{S}}ition (\textbf{BOSS})
with Hybrid Counterfactual Training framework, which sheds new light on the CIR
task by studying it from two previously overlooked perspectives:
\emph{implicitly bottom-up composition of visiolinguistic representation} and
\emph{explicitly fine-grained correspondence of query-target construction}. On
the one hand, we leverage the implicit interaction and composition of
cross-modal embeddings from the bottom local characteristics to the top global
semantics, preserving and transforming the visual representation conditioned on
language semantics in several continuous steps for effective target image
search. On the other hand, we devise a hybrid counterfactual training strategy
that can reduce the model's ambiguity for similar queries.",2207.04211v1,https://arxiv.org/pdf/2207.04211v1
"How many perturbations break this model? Evaluating robustness beyond
  adversarial accuracy","Raphael Olivier, Bhiksha Raj","Robustness to adversarial attacks is typically evaluated with adversarial
accuracy. While essential, this metric does not capture all aspects of
robustness and in particular leaves out the question of how many perturbations
can be found for each point. In this work, we introduce an alternative
approach, adversarial sparsity, which quantifies how difficult it is to find a
successful perturbation given both an input point and a constraint on the
direction of the perturbation. We show that sparsity provides valuable insight
into neural networks in multiple ways: for instance, it illustrates important
differences between current state-of-the-art robust models them that accuracy
analysis does not, and suggests approaches for improving their robustness. When
applying broken defenses effective against weak attacks but not strong ones,
sparsity can discriminate between the totally ineffective and the partially
effective defenses. Finally, with sparsity we can measure increases in
robustness that do not affect accuracy: we show for example that data
augmentation can by itself increase adversarial robustness, without using
adversarial training.",2207.04129v3,https://arxiv.org/pdf/2207.04129v3
Models Out of Line: A Fourier Lens on Distribution Shift Robustness,"Sara Fridovich-Keil, Brian R. Bartoldson, James Diffenderfer, Bhavya Kailkhura, Peer-Timo Bremer","Improving the accuracy of deep neural networks (DNNs) on out-of-distribution
(OOD) data is critical to an acceptance of deep learning (DL) in real world
applications. It has been observed that accuracies on in-distribution (ID)
versus OOD data follow a linear trend and models that outperform this baseline
are exceptionally rare (and referred to as ""effectively robust""). Recently,
some promising approaches have been developed to improve OOD robustness: model
pruning, data augmentation, and ensembling or zero-shot evaluating large
pretrained models. However, there still is no clear understanding of the
conditions on OOD data and model properties that are required to observe
effective robustness. We approach this issue by conducting a comprehensive
empirical study of diverse approaches that are known to impact OOD robustness
on a broad range of natural and synthetic distribution shifts of CIFAR-10 and
ImageNet. In particular, we view the ""effective robustness puzzle"" through a
Fourier lens and ask how spectral properties of both models and OOD data
influence the corresponding effective robustness. We find this Fourier lens
offers some insight into why certain robust models, particularly those from the
CLIP family, achieve OOD robustness. However, our analysis also makes clear
that no known metric is consistently the best explanation (or even a strong
explanation) of OOD robustness. Thus, to aid future research into the OOD
puzzle, we address the gap in publicly-available models with effective
robustness by introducing a set of pretrained models--RobustNets--with varying
levels of OOD robustness.",2207.04075v1,https://arxiv.org/pdf/2207.04075v1
"Learning with Muscles: Benefits for Data-Efficiency and Robustness in
  Anthropomorphic Tasks","Isabell Wochner, Pierre Schumacher, Georg Martius, Dieter Büchler, Syn Schmitt, Daniel F. B. Haeufle","Humans are able to outperform robots in terms of robustness, versatility, and
learning of new tasks in a wide variety of movements. We hypothesize that
highly nonlinear muscle dynamics play a large role in providing inherent
stability, which is favorable to learning. While recent advances have been made
in applying modern learning techniques to muscle-actuated systems both in
simulation as well as in robotics, so far, no detailed analysis has been
performed to show the benefits of muscles when learning from scratch. Our study
closes this gap and showcases the potential of muscle actuators for core
robotics challenges in terms of data-efficiency, hyperparameter sensitivity,
and robustness.",2207.03952v2,https://arxiv.org/pdf/2207.03952v2
"Robust Newsvendor Problem in Global Market: Stable Operation Strategy
  for a Two-Market Stochastic System",Xiaoli Yan,"The global markets provide enterprises with selling opportunities and
challenges in stabilizing operational strategies. From the perspective of
production management, it is important to improve the profitability of an
enterprise by exploiting the different timing of the selling season in
different markets to develop an operational strategy that is optimized and
configured on a global scale. This paper examines the above issue with an
insightful model of selling the product to two markets (a primary and a
secondary market) with multiple risks of changes in the market environment and
nonoverlapping selling seasons. We refer to this problem as the ""global robust
newsvendor"" problem. We provide closed-form solutions of the optimal operation
strategy for demand-independent and demand-related scenarios for the above two
market stochastic systems. The closed-form solutions fully reflect the
influence of the relationship between supply and demand on strategy selection.
We find that the demand correlation and the lack of demand information will not
substantially affect the operation strategy, and the enterprise's industrial
chain and supply chain remain stable. However, the reduction of inter-market
tariffs or logistics costs will cause changes, and the existence of the
secondary market will lead to more capacity planning in the primary market. In
addition, our model explicitly considers the impact of exchange rate
uncertainty on operating strategies.",2207.03801v2,https://arxiv.org/pdf/2207.03801v2
"CausalAgents: A Robustness Benchmark for Motion Forecasting using Causal
  Relationships","Rebecca Roelofs, Liting Sun, Ben Caine, Khaled S. Refaat, Ben Sapp, Scott Ettinger, Wei Chai","As machine learning models become increasingly prevalent in motion
forecasting for autonomous vehicles (AVs), it is critical to ensure that model
predictions are safe and reliable. However, exhaustively collecting and
labeling the data necessary to fully test the long tail of rare and challenging
scenarios is difficult and expensive. In this work, we construct a new
benchmark for evaluating and improving model robustness by applying
perturbations to existing data. Specifically, we conduct an extensive labeling
effort to identify causal agents, or agents whose presence influences human
drivers' behavior in any format, in the Waymo Open Motion Dataset (WOMD), and
we use these labels to perturb the data by deleting non-causal agents from the
scene. We evaluate a diverse set of state-of-the-art deep-learning model
architectures on our proposed benchmark and find that all models exhibit large
shifts under even non-causal perturbation: we observe a 25-38% relative change
in minADE as compared to the original. We also investigate techniques to
improve model robustness, including increasing the training dataset size and
using targeted data augmentations that randomly drop non-causal agents
throughout training. Finally, we release the causal agent labels (at
https://github.com/google-research/causal-agents) as an additional attribute to
WOMD and the robustness benchmarks to aid the community in building more
reliable and safe deep-learning models for motion forecasting.",2207.03586v2,https://arxiv.org/pdf/2207.03586v2
"On the Relationship Between Adversarial Robustness and Decision Region
  in Deep Neural Network","Seongjin Park, Haedong Jeong, Giyoung Jeon, Jaesik Choi","In general, Deep Neural Networks (DNNs) are evaluated by the generalization
performance measured on unseen data excluded from the training phase. Along
with the development of DNNs, the generalization performance converges to the
state-of-the-art and it becomes difficult to evaluate DNNs solely based on this
metric. The robustness against adversarial attack has been used as an
additional metric to evaluate DNNs by measuring their vulnerability. However,
few studies have been performed to analyze the adversarial robustness in terms
of the geometry in DNNs. In this work, we perform an empirical study to analyze
the internal properties of DNNs that affect model robustness under adversarial
attacks. In particular, we propose the novel concept of the Populated Region
Set (PRS), where training samples are populated more frequently, to represent
the internal properties of DNNs in a practical setting. From systematic
experiments with the proposed concept, we provide empirical evidence to
validate that a low PRS ratio has a strong relationship with the adversarial
robustness of DNNs. We also devise PRS regularizer leveraging the
characteristics of PRS to improve the adversarial robustness without
adversarial training.",2207.03400v1,https://arxiv.org/pdf/2207.03400v1
NESC: Robust Neural End-2-End Speech Coding with GANs,"Nicola Pia, Kishan Gupta, Srikanth Korse, Markus Multrus, Guillaume Fuchs","Neural networks have proven to be a formidable tool to tackle the problem of
speech coding at very low bit rates. However, the design of a neural coder that
can be operated robustly under real-world conditions remains a major challenge.
Therefore, we present Neural End-2-End Speech Codec (NESC) a robust, scalable
end-to-end neural speech codec for high-quality wideband speech coding at 3
kbps. The encoder uses a new architecture configuration, which relies on our
proposed Dual-PathConvRNN (DPCRNN) layer, while the decoder architecture is
based on our previous work Streamwise-StyleMelGAN. Our subjective listening
tests on clean and noisy speech show that NESC is particularly robust to unseen
conditions and signal perturbations.",2207.03282v1,https://arxiv.org/pdf/2207.03282v1
"Robust optimal well control using an adaptive multi-grid reinforcement
  learning framework","Atish Dixit, Ahmed H. ElSheikh","Reinforcement learning (RL) is a promising tool to solve robust optimal well
control problems where the model parameters are highly uncertain, and the
system is partially observable in practice. However, RL of robust control
policies often relies on performing a large number of simulations. This could
easily become computationally intractable for cases with computationally
intensive simulations. To address this bottleneck, an adaptive multi-grid RL
framework is introduced which is inspired by principles of geometric multi-grid
methods used in iterative numerical algorithms. RL control policies are
initially learned using computationally efficient low fidelity simulations
using coarse grid discretization of the underlying partial differential
equations (PDEs). Subsequently, the simulation fidelity is increased in an
adaptive manner towards the highest fidelity simulation that correspond to
finest discretization of the model domain. The proposed framework is
demonstrated using a state-of-the-art, model-free policy-based RL algorithm,
namely the Proximal Policy Optimisation (PPO) algorithm. Results are shown for
two case studies of robust optimal well control problems which are inspired
from SPE-10 model 2 benchmark case studies. Prominent gains in the
computational efficiency is observed using the proposed framework saving around
60-70% of computational cost of its single fine-grid counterpart.",2207.03253v2,https://arxiv.org/pdf/2207.03253v2
Robust Counterfactual Explanations for Tree-Based Ensembles,"Sanghamitra Dutta, Jason Long, Saumitra Mishra, Cecilia Tilli, Daniele Magazzeni","Counterfactual explanations inform ways to achieve a desired outcome from a
machine learning model. However, such explanations are not robust to certain
real-world changes in the underlying model (e.g., retraining the model,
changing hyperparameters, etc.), questioning their reliability in several
applications, e.g., credit lending. In this work, we propose a novel strategy
-- that we call RobX -- to generate robust counterfactuals for tree-based
ensembles, e.g., XGBoost. Tree-based ensembles pose additional challenges in
robust counterfactual generation, e.g., they have a non-smooth and
non-differentiable objective function, and they can change a lot in the
parameter space under retraining on very similar data. We first introduce a
novel metric -- that we call Counterfactual Stability -- that attempts to
quantify how robust a counterfactual is going to be to model changes under
retraining, and comes with desirable theoretical properties. Our proposed
strategy RobX works with any counterfactual generation method (base method) and
searches for robust counterfactuals by iteratively refining the counterfactual
generated by the base method using our metric Counterfactual Stability. We
compare the performance of RobX with popular counterfactual generation methods
(for tree-based ensembles) across benchmark datasets. The results demonstrate
that our strategy generates counterfactuals that are significantly more robust
(nearly 100% validity after actual model changes) and also realistic (in terms
of local outlier factor) over existing state-of-the-art methods.",2207.02739v2,https://arxiv.org/pdf/2207.02739v2
"UniCR: Universally Approximated Certified Robustness via Randomized
  Smoothing","Hanbin Hong, Binghui Wang, Yuan Hong","We study certified robustness of machine learning classifiers against
adversarial perturbations. In particular, we propose the first universally
approximated certified robustness (UniCR) framework, which can approximate the
robustness certification of any input on any classifier against any $\ell_p$
perturbations with noise generated by any continuous probability distribution.
Compared with the state-of-the-art certified defenses, UniCR provides many
significant benefits: (1) the first universal robustness certification
framework for the above 4 'any's; (2) automatic robustness certification that
avoids case-by-case analysis, (3) tightness validation of certified robustness,
and (4) optimality validation of noise distributions used by randomized
smoothing. We conduct extensive experiments to validate the above benefits of
UniCR and the advantages of UniCR over state-of-the-art certified defenses
against $\ell_p$ perturbations.",2207.02152v2,https://arxiv.org/pdf/2207.02152v2
"PRoA: A Probabilistic Robustness Assessment against Functional
  Perturbations","Tianle Zhang, Wenjie Ruan, Jonathan E. Fieldsend","In safety-critical deep learning applications robustness measurement is a
vital pre-deployment phase. However, existing robustness verification methods
are not sufficiently practical for deploying machine learning systems in the
real world. On the one hand, these methods attempt to claim that no
perturbations can ``fool'' deep neural networks (DNNs), which may be too
stringent in practice. On the other hand, existing works rigorously consider
$L_p$ bounded additive perturbations on the pixel space, although
perturbations, such as colour shifting and geometric transformations, are more
practically and frequently occurring in the real world. Thus, from the
practical standpoint, we present a novel and general {\it probabilistic
robustness assessment method} (PRoA) based on the adaptive concentration, and
it can measure the robustness of deep learning models against functional
perturbations. PRoA can provide statistical guarantees on the probabilistic
robustness of a model, \textit{i.e.}, the probability of failure encountered by
the trained model after deployment. Our experiments demonstrate the
effectiveness and flexibility of PRoA in terms of evaluating the probabilistic
robustness against a broad range of functional perturbations, and PRoA can
scale well to various large-scale deep neural networks compared to existing
state-of-the-art baselines. For the purpose of reproducibility, we release our
tool on GitHub: \url{ https://github.com/TrustAI/PRoA}.",2207.02036v1,https://arxiv.org/pdf/2207.02036v1
"Robust Reinforcement Learning in Continuous Control Tasks with
  Uncertainty Set Regularization","Yuan Zhang, Jianhong Wang, Joschka Boedecker","Reinforcement learning (RL) is recognized as lacking generalization and
robustness under environmental perturbations, which excessively restricts its
application for real-world robotics. Prior work claimed that adding
regularization to the value function is equivalent to learning a robust policy
with uncertain transitions. Although the regularization-robustness
transformation is appealing for its simplicity and efficiency, it is still
lacking in continuous control tasks. In this paper, we propose a new
regularizer named $\textbf{U}$ncertainty $\textbf{S}$et $\textbf{R}$egularizer
(USR), by formulating the uncertainty set on the parameter space of the
transition function. In particular, USR is flexible enough to be plugged into
any existing RL framework. To deal with unknown uncertainty sets, we further
propose a novel adversarial approach to generate them based on the value
function. We evaluate USR on the Real-world Reinforcement Learning (RWRL)
benchmark, demonstrating improvements in the robust performance for perturbed
testing environments.",2207.02016v4,https://arxiv.org/pdf/2207.02016v4
Vector Quantisation for Robust Segmentation,"Ainkaran Santhirasekaram, Avinash Kori, Mathias Winkler, Andrea Rockall, Ben Glocker","The reliability of segmentation models in the medical domain depends on the
model's robustness to perturbations in the input space. Robustness is a
particular challenge in medical imaging exhibiting various sources of image
noise, corruptions, and domain shifts. Obtaining robustness is often attempted
via simulating heterogeneous environments, either heuristically in the form of
data augmentation or by learning to generate specific perturbations in an
adversarial manner. We propose and justify that learning a discrete
representation in a low dimensional embedding space improves robustness of a
segmentation model. This is achieved with a dictionary learning method called
vector quantisation. We use a set of experiments designed to analyse robustness
in both the latent and output space under domain shift and noise perturbations
in the input space. We adapt the popular UNet architecture, inserting a
quantisation block in the bottleneck. We demonstrate improved segmentation
accuracy and better robustness on three segmentation tasks. Code is available
at
\url{https://github.com/AinkaranSanthi/Vector-Quantisation-for-Robust-Segmentation}",2207.01919v1,https://arxiv.org/pdf/2207.01919v1
"Counterbalancing Teacher: Regularizing Batch Normalized Models for
  Robustness","Saeid Asgari Taghanaki, Ali Gholami, Fereshte Khani, Kristy Choi, Linh Tran, Ran Zhang, Aliasghar Khani","Batch normalization (BN) is a ubiquitous technique for training deep neural
networks that accelerates their convergence to reach higher accuracy. However,
we demonstrate that BN comes with a fundamental drawback: it incentivizes the
model to rely on low-variance features that are highly specific to the training
(in-domain) data, hurting generalization performance on out-of-domain examples.
In this work, we investigate this phenomenon by first showing that removing BN
layers across a wide range of architectures leads to lower out-of-domain and
corruption errors at the cost of higher in-domain errors. We then propose
Counterbalancing Teacher (CT), a method which leverages a frozen copy of the
same model without BN as a teacher to enforce the student network's learning of
robust representations by substantially adapting its weights through a
consistency loss function. This regularization signal helps CT perform well in
unforeseen data shifts, even without information from the target domain as in
prior works. We theoretically show in an overparameterized linear regression
setting why normalization leads to a model's reliance on such in-domain
features, and empirically demonstrate the efficacy of CT by outperforming
several baselines on robustness benchmarks such as CIFAR-10-C, CIFAR-100-C, and
VLCS.",2207.01548v1,https://arxiv.org/pdf/2207.01548v1
"How Robust is Your Fairness? Evaluating and Sustaining Fairness under
  Unseen Distribution Shifts","Haotao Wang, Junyuan Hong, Jiayu Zhou, Zhangyang Wang","Increasing concerns have been raised on deep learning fairness in recent
years. Existing fairness-aware machine learning methods mainly focus on the
fairness of in-distribution data. However, in real-world applications, it is
common to have distribution shift between the training and test data. In this
paper, we first show that the fairness achieved by existing methods can be
easily broken by slight distribution shifts. To solve this problem, we propose
a novel fairness learning method termed CUrvature MAtching (CUMA), which can
achieve robust fairness generalizable to unseen domains with unknown
distributional shifts. Specifically, CUMA enforces the model to have similar
generalization ability on the majority and minority groups, by matching the
loss curvature distributions of the two groups. We evaluate our method on three
popular fairness datasets. Compared with existing methods, CUMA achieves
superior fairness under unseen distribution shifts, without sacrificing either
the overall accuracy or the in-distribution fairness.",2207.01168v1,https://arxiv.org/pdf/2207.01168v1
"Tricking the Hashing Trick: A Tight Lower Bound on the Robustness of
  CountSketch to Adaptive Inputs","Edith Cohen, Jelani Nelson, Tamás Sarlós, Uri Stemmer","CountSketch and Feature Hashing (the ""hashing trick"") are popular randomized
dimensionality reduction methods that support recovery of $\ell_2$-heavy
hitters (keys $i$ where $v_i^2 > \epsilon \|\boldsymbol{v}\|_2^2$) and
approximate inner products. When the inputs are {\em not adaptive} (do not
depend on prior outputs), classic estimators applied to a sketch of size
$O(\ell/\epsilon)$ are accurate for a number of queries that is exponential in
$\ell$. When inputs are adaptive, however, an adversarial input can be
constructed after $O(\ell)$ queries with the classic estimator and the best
known robust estimator only supports $\tilde{O}(\ell^2)$ queries. In this work
we show that this quadratic dependence is in a sense inherent: We design an
attack that after $O(\ell^2)$ queries produces an adversarial input vector
whose sketch is highly biased. Our attack uses ""natural"" non-adaptive inputs
(only the final adversarial input is chosen adaptively) and universally applies
with any correct estimator, including one that is unknown to the attacker. In
that, we expose inherent vulnerability of this fundamental method.",2207.00956v2,https://arxiv.org/pdf/2207.00956v2
Online Reflective Learning for Robust Medical Image Segmentation,"Yuhao Huang, Xin Yang, Xiaoqiong Huang, Jiamin Liang, Xinrui Zhou, Cheng Chen, Haoran Dou, Xindi Hu, Yan Cao, Dong Ni","Deep segmentation models often face the failure risks when the testing image
presents unseen distributions. Improving model robustness against these risks
is crucial for the large-scale clinical application of deep models. In this
study, inspired by human learning cycle, we propose a novel online reflective
learning framework (RefSeg) to improve segmentation robustness. Based on the
reflection-on-action conception, our RefSeg firstly drives the deep model to
take action to obtain semantic segmentation. Then, RefSeg triggers the model to
reflect itself. Because making deep models realize their segmentation failures
during testing is challenging, RefSeg synthesizes a realistic proxy image from
the semantic mask to help deep models build intuitive and effective
reflections. This proxy translates and emphasizes the segmentation flaws. By
maximizing the structural similarity between the raw input and the proxy, the
reflection-on-action loop is closed with segmentation robustness improved.
RefSeg runs in the testing phase and is general for segmentation models.
Extensive validation on three medical image segmentation tasks with a public
cardiac MR dataset and two in-house large ultrasound datasets show that our
RefSeg remarkably improves model robustness and reports state-of-the-art
performance over strong competitors.",2207.00476v1,https://arxiv.org/pdf/2207.00476v1
"Robust Bayesian Learning for Reliable Wireless AI: Framework and
  Applications","Matteo Zecchin, Sangwoo Park, Osvaldo Simeone, Marios Kountouris, David Gesbert","This work takes a critical look at the application of conventional machine
learning methods to wireless communication problems through the lens of
reliability and robustness. Deep learning techniques adopt a frequentist
framework, and are known to provide poorly calibrated decisions that do not
reproduce the true uncertainty caused by limitations in the size of the
training data. Bayesian learning, while in principle capable of addressing this
shortcoming, is in practice impaired by model misspecification and by the
presence of outliers. Both problems are pervasive in wireless communication
settings, in which the capacity of machine learning models is subject to
resource constraints and training data is affected by noise and interference.
In this context, we explore the application of the framework of robust Bayesian
learning. After a tutorial-style introduction to robust Bayesian learning, we
showcase the merits of robust Bayesian learning on several important wireless
communication problems in terms of accuracy, calibration, and robustness to
outliers and misspecification.",2207.00300v1,https://arxiv.org/pdf/2207.00300v1
An Understanding-Oriented Robust Machine Reading Comprehension Model,"Feiliang Ren, Yongkang Liu, Bochao Li, Shilei Liu, Bingchao Wang, Jiaqi Wang, Chunchao Liu, Qi Ma","Although existing machine reading comprehension models are making rapid
progress on many datasets, they are far from robust. In this paper, we propose
an understanding-oriented machine reading comprehension model to address three
kinds of robustness issues, which are over sensitivity, over stability and
generalization. Specifically, we first use a natural language inference module
to help the model understand the accurate semantic meanings of input questions
so as to address the issues of over sensitivity and over stability. Then in the
machine reading comprehension module, we propose a memory-guided multi-head
attention method that can further well understand the semantic meanings of
input questions and passages. Third, we propose a multilanguage learning
mechanism to address the issue of generalization. Finally, these modules are
integrated with a multi-task learning based method. We evaluate our model on
three benchmark datasets that are designed to measure models robustness,
including DuReader (robust) and two SQuAD-related datasets. Extensive
experiments show that our model can well address the mentioned three kinds of
robustness issues. And it achieves much better results than the compared
state-of-the-art models on all these datasets under different evaluation
metrics, even under some extreme and unfair evaluations. The source code of our
work is available at: https://github.com/neukg/RobustMRC.",2207.00187v1,https://arxiv.org/pdf/2207.00187v1
Robustness of Epinets against Distributional Shifts,"Xiuyuan Lu, Ian Osband, Seyed Mohammad Asghari, Sven Gowal, Vikranth Dwaracherla, Zheng Wen, Benjamin Van Roy","Recent work introduced the epinet as a new approach to uncertainty modeling
in deep learning. An epinet is a small neural network added to traditional
neural networks, which, together, can produce predictive distributions. In
particular, using an epinet can greatly improve the quality of joint
predictions across multiple inputs, a measure of how well a neural network
knows what it does not know. In this paper, we examine whether epinets can
offer similar advantages under distributional shifts. We find that, across
ImageNet-A/O/C, epinets generally improve robustness metrics. Moreover, these
improvements are more significant than those afforded by even very large
ensembles at orders of magnitude lower computational costs. However, these
improvements are relatively small compared to the outstanding issues in
distributionally-robust deep learning. Epinets may be a useful tool in the
toolbox, but they are far from the complete solution.",2207.00137v1,https://arxiv.org/pdf/2207.00137v1
"Reliable Representations Make A Stronger Defender: Unsupervised
  Structure Refinement for Robust GNN","Kuan Li, Yang Liu, Xiang Ao, Jianfeng Chi, Jinghua Feng, Hao Yang, Qing He","Benefiting from the message passing mechanism, Graph Neural Networks (GNNs)
have been successful on flourish tasks over graph data. However, recent studies
have shown that attackers can catastrophically degrade the performance of GNNs
by maliciously modifying the graph structure. A straightforward solution to
remedy this issue is to model the edge weights by learning a metric function
between pairwise representations of two end nodes, which attempts to assign low
weights to adversarial edges. The existing methods use either raw features or
representations learned by supervised GNNs to model the edge weights. However,
both strategies are faced with some immediate problems: raw features cannot
represent various properties of nodes (e.g., structure information), and
representations learned by supervised GNN may suffer from the poor performance
of the classifier on the poisoned graph. We need representations that carry
both feature information and as mush correct structure information as possible
and are insensitive to structural perturbations. To this end, we propose an
unsupervised pipeline, named STABLE, to optimize the graph structure. Finally,
we input the well-refined graph into a downstream classifier. For this part, we
design an advanced GCN that significantly enhances the robustness of vanilla
GCN without increasing the time complexity. Extensive experiments on four
real-world graph benchmarks demonstrate that STABLE outperforms the
state-of-the-art methods and successfully defends against various attacks.",2207.00012v4,https://arxiv.org/pdf/2207.00012v4
"FeaRLESS: Feature Refinement Loss for Ensembling Self-Supervised
  Learning Features in Robust End-to-end Speech Recognition","Szu-Jui Chen, Jiamin Xie, John H. L. Hansen","Self-supervised learning representations (SSLR) have resulted in robust
features for downstream tasks in many fields. Recently, several SSLRs have
shown promising results on automatic speech recognition (ASR) benchmark
corpora. However, previous studies have only shown performance for solitary
SSLRs as an input feature for ASR models. In this study, we propose to
investigate the effectiveness of diverse SSLR combinations using various fusion
methods within end-to-end (E2E) ASR models. In addition, we will show there are
correlations between these extracted SSLRs. As such, we further propose a
feature refinement loss for decorrelation to efficiently combine the set of
input features. For evaluation, we show that the proposed 'FeaRLESS learning
features' perform better than systems without the proposed feature refinement
loss for both the WSJ and Fearless Steps Challenge (FSC) corpora.",2206.15056v1,https://arxiv.org/pdf/2206.15056v1
"Benchmarking the Robustness of Deep Neural Networks to Common
  Corruptions in Digital Pathology","Yunlong Zhang, Yuxuan Sun, Honglin Li, Sunyi Zheng, Chenglu Zhu, Lin Yang","When designing a diagnostic model for a clinical application, it is crucial
to guarantee the robustness of the model with respect to a wide range of image
corruptions. Herein, an easy-to-use benchmark is established to evaluate how
deep neural networks perform on corrupted pathology images. Specifically,
corrupted images are generated by injecting nine types of common corruptions
into validation images. Besides, two classification and one ranking metrics are
designed to evaluate the prediction and confidence performance under
corruption. Evaluated on two resulting benchmark datasets, we find that (1) a
variety of deep neural network models suffer from a significant accuracy
decrease (double the error on clean images) and the unreliable confidence
estimation on corrupted images; (2) A low correlation between the validation
and test errors while replacing the validation set with our benchmark can
increase the correlation. Our codes are available on
https://github.com/superjamessyx/robustness_benchmark.",2206.14973v1,https://arxiv.org/pdf/2206.14973v1
"On the Robustness of Dialogue History Representation in Conversational
  Question Answering: A Comprehensive Study and a New Prompt-based Method","Zorik Gekhman, Nadav Oved, Orgad Keller, Idan Szpektor, Roi Reichart","Most works on modeling the conversation history in Conversational Question
Answering (CQA) report a single main result on a common CQA benchmark. While
existing models show impressive results on CQA leaderboards, it remains unclear
whether they are robust to shifts in setting (sometimes to more realistic
ones), training data size (e.g. from large to small sets) and domain. In this
work, we design and conduct the first large-scale robustness study of history
modeling approaches for CQA. We find that high benchmark scores do not
necessarily translate to strong robustness, and that various methods can
perform extremely differently under different settings. Equipped with the
insights from our study, we design a novel prompt-based history modeling
approach, and demonstrate its strong robustness across various settings. Our
approach is inspired by existing methods that highlight historic answers in the
passage. However, instead of highlighting by modifying the passage token
embeddings, we add textual prompts directly in the passage text. Our approach
is simple, easy-to-plug into practically any model, and highly effective, thus
we recommend it as a starting point for future model developers. We also hope
that our study and insights will raise awareness to the importance of
robustness-focused evaluation, in addition to obtaining high leaderboard
scores, leading to better CQA systems.",2206.14796v2,https://arxiv.org/pdf/2206.14796v2
"IBP Regularization for Verified Adversarial Robustness via
  Branch-and-Bound","Alessandro De Palma, Rudy Bunel, Krishnamurthy Dvijotham, M. Pawan Kumar, Robert Stanforth","Recent works have tried to increase the verifiability of adversarially
trained networks by running the attacks over domains larger than the original
perturbations and adding various regularization terms to the objective.
However, these algorithms either underperform or require complex and expensive
stage-wise training procedures, hindering their practical applicability. We
present IBP-R, a novel verified training algorithm that is both simple and
effective. IBP-R induces network verifiability by coupling adversarial attacks
on enlarged domains with a regularization term, based on inexpensive interval
bound propagation, that minimizes the gap between the non-convex verification
problem and its approximations. By leveraging recent branch-and-bound
frameworks, we show that IBP-R obtains state-of-the-art verified
robustness-accuracy trade-offs for small perturbations on CIFAR-10 while
training significantly faster than relevant previous work. Additionally, we
present UPB, a novel branching strategy that, relying on a simple heuristic
based on $\beta$-CROWN, reduces the cost of state-of-the-art branching
algorithms while yielding splits of comparable quality.",2206.14772v2,https://arxiv.org/pdf/2206.14772v2
"A Robustly Optimized Long Text to Math Models for Numerical Reasoning On
  FinQA","Renhui Zhang, Youwei Zhang, Yao Yu","Numerical reasoning is required when solving most problems in our life, but
it has been neglected in previous artificial intelligence researches. FinQA
challenge has been organized to strengthen the study on numerical reasoning
where the participants are asked to predict the numerical reasoning program to
solve financial question. The result of FinQA will be evaluated by both
execution accuracy and program accuracy. In this paper, we present our approach
to tackle the task objective by developing models with different specialized
capabilities and fusing their strength. Overall, our approach achieves the 1st
place in FinQA challenge, with 71.93% execution accuracy and 67.03% program
accuracy.",2207.06490v1,https://arxiv.org/pdf/2207.06490v1
"RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy and
  Out Distribution Robustness","Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H. S. Torr, Puneet K. Dokania","We show that the effectiveness of the well celebrated Mixup [Zhang et al.,
2018] can be further improved if instead of using it as the sole learning
objective, it is utilized as an additional regularizer to the standard
cross-entropy loss. This simple change not only provides much improved accuracy
but also significantly improves the quality of the predictive uncertainty
estimation of Mixup in most cases under various forms of covariate shifts and
out-of-distribution detection experiments. In fact, we observe that Mixup
yields much degraded performance on detecting out-of-distribution samples
possibly, as we show empirically, because of its tendency to learn models that
exhibit high-entropy throughout; making it difficult to differentiate
in-distribution samples from out-distribution ones. To show the efficacy of our
approach (RegMixup), we provide thorough analyses and experiments on vision
datasets (ImageNet & CIFAR-10/100) and compare it with a suite of recent
approaches for reliable uncertainty estimation.",2206.14502v2,https://arxiv.org/pdf/2206.14502v2
Hardness and Algorithms for Robust and Sparse Optimization,"Eric Price, Sandeep Silwal, Samson Zhou","We explore algorithms and limitations for sparse optimization problems such
as sparse linear regression and robust linear regression. The goal of the
sparse linear regression problem is to identify a small number of key features,
while the goal of the robust linear regression problem is to identify a small
number of erroneous measurements. Specifically, the sparse linear regression
problem seeks a $k$-sparse vector $x\in\mathbb{R}^d$ to minimize $\|Ax-b\|_2$,
given an input matrix $A\in\mathbb{R}^{n\times d}$ and a target vector
$b\in\mathbb{R}^n$, while the robust linear regression problem seeks a set $S$
that ignores at most $k$ rows and a vector $x$ to minimize $\|(Ax-b)_S\|_2$.
  We first show bicriteria, NP-hardness of approximation for robust regression
building on the work of [OWZ15] which implies a similar result for sparse
regression. We further show fine-grained hardness of robust regression through
a reduction from the minimum-weight $k$-clique conjecture. On the positive
side, we give an algorithm for robust regression that achieves arbitrarily
accurate additive error and uses runtime that closely matches the lower bound
from the fine-grained hardness result, as well as an algorithm for sparse
regression with similar runtime. Both our upper and lower bounds rely on a
general reduction from robust linear regression to sparse regression that we
introduce. Our algorithms, inspired by the 3SUM problem, use approximate
nearest neighbor data structures and may be of independent interest for solving
sparse optimization problems. For instance, we demonstrate that our techniques
can also be used for the well-studied sparse PCA problem.",2206.14354v1,https://arxiv.org/pdf/2206.14354v1
Increasing Confidence in Adversarial Robustness Evaluations,"Roland S. Zimmermann, Wieland Brendel, Florian Tramer, Nicholas Carlini","Hundreds of defenses have been proposed to make deep neural networks robust
against minimal (adversarial) input perturbations. However, only a handful of
these defenses held up their claims because correctly evaluating robustness is
extremely challenging: Weak attacks often fail to find adversarial examples
even if they unknowingly exist, thereby making a vulnerable network look
robust. In this paper, we propose a test to identify weak attacks, and thus
weak defense evaluations. Our test slightly modifies a neural network to
guarantee the existence of an adversarial example for every sample.
Consequentially, any correct attack must succeed in breaking this modified
network. For eleven out of thirteen previously-published defenses, the original
evaluation of the defense fails our test, while stronger attacks that break
these defenses pass it. We hope that attack unit tests - such as ours - will be
a major component in future robustness evaluations and increase confidence in
an empirical field that is currently riddled with skepticism.",2206.13991v1,https://arxiv.org/pdf/2206.13991v1
"Robustifying Vision Transformer without Retraining from Scratch by
  Test-Time Class-Conditional Feature Alignment","Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa","Vision Transformer (ViT) is becoming more popular in image processing.
Specifically, we investigate the effectiveness of test-time adaptation (TTA) on
ViT, a technique that has emerged to correct its prediction during test-time by
itself. First, we benchmark various test-time adaptation approaches on ViT-B16
and ViT-L16. It is shown that the TTA is effective on ViT and the
prior-convention (sensibly selecting modulation parameters) is not necessary
when using proper loss function. Based on the observation, we propose a new
test-time adaptation method called class-conditional feature alignment (CFA),
which minimizes both the class-conditional distribution differences and the
whole distribution differences of the hidden representation between the source
and target in an online manner. Experiments of image classification tasks on
common corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domain
adaptation (digits datasets and ImageNet-Sketch) show that CFA stably
outperforms the existing baselines on various datasets. We also verify that CFA
is model agnostic by experimenting on ResNet, MLP-Mixer, and several ViT
variants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8%
top-1 error rate on ImageNet-C, outperforming the existing test-time adaptation
baseline 44.0%. This is a state-of-the-art result among TTA methods that do not
need to alter training phase.",2206.13951v1,https://arxiv.org/pdf/2206.13951v1
"FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical
  Image Classification","Nannan Wu, Li Yu, Xin Yang, Kwang-Ting Cheng, Zengqiang Yan","Federated learning (FL), training deep models from decentralized data without
privacy leakage, has shown great potential in medical image computing recently.
However, considering the ubiquitous class imbalance in medical data, FL can
exhibit performance degradation, especially for minority classes (e.g. rare
diseases). Existing methods towards this problem mainly focus on training a
balanced classifier to eliminate class prior bias among classes, but neglect to
explore better representation to facilitate classification performance. In this
paper, we present a privacy-preserving FL method named FedIIC to combat class
imbalance from two perspectives: feature learning and classifier learning. In
feature learning, two levels of contrastive learning are designed to extract
better class-specific features with imbalanced data in FL. In classifier
learning, per-class margins are dynamically set according to real-time
difficulty and class priors, which helps the model learn classes equally.
Experimental results on publicly-available datasets demonstrate the superior
performance of FedIIC in dealing with both real-world and simulated
multi-source medical imaging data under class imbalance. Code is available at
https://github.com/wnn2000/FedIIC.",2206.13803v3,https://arxiv.org/pdf/2206.13803v3
"Attention-based conditioning methods using variable frame rate for
  style-robust speaker verification","Amber Afshan, Abeer Alwan","We propose an approach to extract speaker embeddings that are robust to
speaking style variations in text-independent speaker verification. Typically,
speaker embedding extraction includes training a DNN for speaker classification
and using the bottleneck features as speaker representations. Such a network
has a pooling layer to transform frame-level to utterance-level features by
calculating statistics over all utterance frames, with equal weighting.
However, self-attentive embeddings perform weighted pooling such that the
weights correspond to the importance of the frames in a speaker classification
task. Entropy can capture acoustic variability due to speaking style
variations. Hence, an entropy-based variable frame rate vector is proposed as
an external conditioning vector for the self-attention layer to provide the
network with information that can address style effects. This work explores
five different approaches to conditioning. The best conditioning approach,
concatenation with gating, provided statistically significant improvements over
the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23
tasks when using the UCLA speaker variability database. It also significantly
outperformed self-attention without conditioning in 9/23 tasks and was worse in
1/23. The method also showed significant improvements in multi-speaker
scenarios of SITW.",2206.13680v1,https://arxiv.org/pdf/2206.13680v1
"Robustness Implies Generalization via Data-Dependent Generalization
  Bounds","Kenji Kawaguchi, Zhun Deng, Kyle Luh, Jiaoyang Huang","This paper proves that robustness implies generalization via data-dependent
generalization bounds. As a result, robustness and generalization are shown to
be connected closely in a data-dependent manner. Our bounds improve previous
bounds in two directions, to solve an open problem that has seen little
development since 2010. The first is to reduce the dependence on the covering
number. The second is to remove the dependence on the hypothesis space. We
present several examples, including ones for lasso and deep learning, in which
our bounds are provably preferable. The experiments on real-world data and
theoretical models demonstrate near-exponential improvements in various
situations. To achieve these improvements, we do not require additional
assumptions on the unknown distribution; instead, we only incorporate an
observable and computable property of the training samples. A key technical
innovation is an improved concentration bound for multinomial random variables
that is of independent interest beyond robustness and generalization.",2206.13497v4,https://arxiv.org/pdf/2206.13497v4
"Utilizing Class Separation Distance for the Evaluation of Corruption
  Robustness of Machine Learning Classifiers","Georg Siedel, Silvia Vock, Andrey Morozov, Stefan Voß","Robustness is a fundamental pillar of Machine Learning (ML) classifiers,
substantially determining their reliability. Methods for assessing classifier
robustness are therefore essential. In this work, we address the challenge of
evaluating corruption robustness in a way that allows comparability and
interpretability on a given dataset. We propose a test data augmentation method
that uses a robustness distance $\epsilon$ derived from the datasets minimal
class separation distance. The resulting MSCR (mean statistical corruption
robustness) metric allows a dataset-specific comparison of different
classifiers with respect to their corruption robustness. The MSCR value is
interpretable, as it represents the classifiers avoidable loss of accuracy due
to statistical corruptions. On 2D and image data, we show that the metric
reflects different levels of classifier robustness. Furthermore, we observe
unexpected optima in classifiers robust accuracy through training and testing
classifiers with different levels of noise. While researchers have frequently
reported on a significant tradeoff on accuracy when training robust models, we
strengthen the view that a tradeoff between accuracy and corruption robustness
is not inherent. Our results indicate that robustness training through simple
data augmentation can already slightly improve accuracy.",2206.13405v1,https://arxiv.org/pdf/2206.13405v1
"Wasserstein Distributionally Robust Estimation in High Dimensions:
  Performance Analysis and Optimal Hyperparameter Tuning","Liviu Aolaritei, Soroosh Shafiee, Florian Dörfler","Wasserstein distributionally robust optimization has recently emerged as a
powerful framework for robust estimation, enjoying good out-of-sample
performance guarantees, well-understood regularization effects, and
computationally tractable reformulations. In such framework, the estimator is
obtained by minimizing the worst-case expected loss over all probability
distributions which are close, in a Wasserstein sense, to the empirical
distribution. In this paper, we propose a Wasserstein distributionally robust
estimation framework to estimate an unknown parameter from noisy linear
measurements, and we focus on the task of analyzing the squared error
performance of such estimators. Our study is carried out in the modern
high-dimensional proportional regime, where both the ambient dimension and the
number of samples go to infinity at a proportional rate which encodes the
under/over-parametrization of the problem. Under an isotropic Gaussian features
assumption, we show that the squared error can be recovered as the solution of
a convex-concave optimization problem which, surprinsingly, involves at most
four scalar variables. Importantly, the precise quantification of the squared
error allows to accurately and efficiently compare different ambiguity radii
and to understand the effect of the under/over-parametrization on the
estimation error. We conclude the paper with a list of exciting research
directions enabled by our results.",2206.13269v2,https://arxiv.org/pdf/2206.13269v2
"Towards Harnessing Feature Embedding for Robust Learning with Noisy
  Labels","Chuang Zhang, Li Shen, Jian Yang, Chen Gong","The memorization effect of deep neural networks (DNNs) plays a pivotal role
in recent label noise learning methods. To exploit this effect, the model
prediction-based methods have been widely adopted, which aim to exploit the
outputs of DNNs in the early stage of learning to correct noisy labels.
However, we observe that the model will make mistakes during label prediction,
resulting in unsatisfactory performance. By contrast, the produced features in
the early stage of learning show better robustness. Inspired by this
observation, in this paper, we propose a novel feature embedding-based method
for deep learning with label noise, termed LabEl NoiseDilution (LEND). To be
specific, we first compute a similarity matrix based on current embedded
features to capture the local structure of training data. Then, the noisy
supervision signals carried by mislabeled data are overwhelmed by nearby
correctly labeled ones (\textit{i.e.}, label noise dilution), of which the
effectiveness is guaranteed by the inherent robustness of feature embedding.
Finally, the training data with diluted labels are further used to train a
robust classifier. Empirically, we conduct extensive experiments on both
synthetic and real-world noisy datasets by comparing our LEND with several
representative robust learning approaches. The results verify the effectiveness
of our LEND.",2206.13025v1,https://arxiv.org/pdf/2206.13025v1
Adversarially Robust PAC Learnability of Real-Valued Functions,"Idan Attias, Steve Hanneke","We study robustness to test-time adversarial attacks in the regression
setting with $\ell_p$ losses and arbitrary perturbation sets. We address the
question of which function classes are PAC learnable in this setting. We show
that classes of finite fat-shattering dimension are learnable in both
realizable and agnostic settings. Moreover, for convex function classes, they
are even properly learnable. In contrast, some non-convex function classes
provably require improper learning algorithms. Our main technique is based on a
construction of an adversarially robust sample compression scheme of a size
determined by the fat-shattering dimension. Along the way, we introduce a novel
agnostic sample compression scheme for real-valued functions, which may be of
independent interest.",2206.12977v3,https://arxiv.org/pdf/2206.12977v3
Self-Healing Robust Neural Networks via Closed-Loop Control,"Zhuotong Chen, Qianxiao Li, Zheng Zhang","Despite the wide applications of neural networks, there have been increasing
concerns about their vulnerability issue. While numerous attack and defense
techniques have been developed, this work investigates the robustness issue
from a new angle: can we design a self-healing neural network that can
automatically detect and fix the vulnerability issue by itself? A typical
self-healing mechanism is the immune system of a human body. This
biology-inspired idea has been used in many engineering designs but is rarely
investigated in deep learning. This paper considers the post-training
self-healing of a neural network, and proposes a closed-loop control
formulation to automatically detect and fix the errors caused by various
attacks or perturbations. We provide a margin-based analysis to explain how
this formulation can improve the robustness of a classifier. To speed up the
inference of the proposed self-healing network, we solve the control problem
via improving the Pontryagin Maximum Principle-based solver. Lastly, we present
an error estimation of the proposed framework for neural networks with
nonlinear activation functions. We validate the performance on several network
architectures against various perturbations. Since the self-healing method does
not need a-priori information about data perturbations/attacks, it can handle a
broad class of unforeseen perturbations.",2206.12963v1,https://arxiv.org/pdf/2206.12963v1
Noise-aware Physics-informed Machine Learning for Robust PDE Discovery,"Pongpisit Thanasutives, Takashi Morita, Masayuki Numao, Ken-ichi Fukui","This work is concerned with discovering the governing partial differential
equation (PDE) of a physical system. Existing methods have demonstrated the PDE
identification from finite observations but failed to maintain satisfying
results against noisy data, partly owing to suboptimal estimated derivatives
and found PDE coefficients. We address the issues by introducing a noise-aware
physics-informed machine learning (nPIML) framework to discover the governing
PDE from data following arbitrary distributions. We propose training a couple
of neural networks, namely solver and preselector, in a multi-task learning
paradigm, which yields important scores of basis candidates that constitute the
hidden physical constraint. After they are jointly trained, the solver network
estimates potential candidates, e.g., partial derivatives, for the sparse
regression algorithm to initially unveil the most likely parsimonious PDE,
decided according to the information criterion. We also propose the denoising
physics-informed neural networks (dPINNs), based on Discrete Fourier Transform
(DFT), to deliver a set of the optimal finetuned PDE coefficients respecting
the noise-reduced variables. The denoising PINNs are structured into forefront
projection networks and a PINN, by which the formerly learned solver
initializes. Our extensive experiments on five canonical PDEs affirm that the
proposed framework presents a robust and interpretable approach for PDE
discovery, applicable to a wide range of systems, possibly complicated by
noise.",2206.12901v5,https://arxiv.org/pdf/2206.12901v5
"Design and Analysis of Robust Resilient Diffusion over Multi-Task
  Networks Against Byzantine Attacks","Tao Yu, Rodrigo C. de Lamare, Yi Yu","This paper studies distributed diffusion adaptation over clustered multi-task
networks in the presence of impulsive interferences and Byzantine attacks. We
develop a robust resilient diffusion least mean Geman-McClure-estimation
(RDLMG) algorithm based on the cost function used by the Geman-McClure
estimator, which can reduce the sensitivity to large outliers and make the
algorithm robust under impulsive interferences. Moreover, the mean sub-sequence
reduced method, in which each node discards the extreme value information of
cost contributions received from its neighbors, can make the network resilient
against Byzantine attacks. In this regard, the proposed RDLMG algorithm ensures
that all normal nodes converge to their ideal states with cooperation among
nodes. A statistical analysis of the RDLMG algorithm is also carried out in
terms of mean and mean-square performances. Numerical results evaluate the
proposed RDLMG algorithm in applications to multi-target localization and
multi-task spectrum sensing.",2206.12749v1,https://arxiv.org/pdf/2206.12749v1
"Robustness Evaluation of Deep Unsupervised Learning Algorithms for
  Intrusion Detection Systems","D'Jeff Kanda Nkashama, Arian Soltani, Jean-Charles Verdier, Marc Frappier, Pierre-Martin Tardif, Froduald Kabanza","Recently, advances in deep learning have been observed in various fields,
including computer vision, natural language processing, and cybersecurity.
Machine learning (ML) has demonstrated its ability as a potential tool for
anomaly detection-based intrusion detection systems to build secure computer
networks. Increasingly, ML approaches are widely adopted than heuristic
approaches for cybersecurity because they learn directly from data. Data is
critical for the development of ML systems, and becomes potential targets for
attackers. Basically, data poisoning or contamination is one of the most common
techniques used to fool ML models through data. This paper evaluates the
robustness of six recent deep learning algorithms for intrusion detection on
contaminated data. Our experiments suggest that the state-of-the-art algorithms
used in this study are sensitive to data contamination and reveal the
importance of self-defense against data perturbation when developing novel
models, especially for intrusion detection systems.",2207.03576v2,https://arxiv.org/pdf/2207.03576v2
"Analyzing Explainer Robustness via Probabilistic Lipschitzness of
  Prediction Functions","Zulqarnain Khan, Davin Hill, Aria Masoomi, Joshua Bone, Jennifer Dy","Machine learning methods have significantly improved in their predictive
capabilities, but at the same time they are becoming more complex and less
transparent. As a result, explainers are often relied on to provide
interpretability to these black-box prediction models. As crucial diagnostics
tools, it is important that these explainers themselves are robust. In this
paper we focus on one particular aspect of robustness, namely that an explainer
should give similar explanations for similar data inputs. We formalize this
notion by introducing and defining explainer astuteness, analogous to
astuteness of prediction functions. Our formalism allows us to connect
explainer robustness to the predictor's probabilistic Lipschitzness, which
captures the probability of local smoothness of a function. We provide lower
bound guarantees on the astuteness of a variety of explainers (e.g., SHAP,
RISE, CXPlain) given the Lipschitzness of the prediction function. These
theoretical results imply that locally smooth prediction functions lend
themselves to locally robust explanations. We evaluate these results
empirically on simulated as well as real datasets.",2206.12481v3,https://arxiv.org/pdf/2206.12481v3
Robustness to corruption in pre-trained Bayesian neural networks,"Xi Wang, Laurence Aitchison","We develop ShiftMatch, a new training-data-dependent likelihood for
robustness to corruption in Bayesian neural networks (BNNs). ShiftMatch is
inspired by the training-data-dependent ""EmpCov"" priors from Izmailov et al.
(2021a), and efficiently matches test-time spatial correlations to those at
training time. Critically, ShiftMatch is designed to leave the neural network's
training time likelihood unchanged, allowing it to use publicly available
samples from pre-trained BNNs. Using pre-trained HMC samples, ShiftMatch gives
strong performance improvements on CIFAR-10-C, outperforms EmpCov priors
(though ShiftMatch uses extra information from a minibatch of corrupted test
points), and is perhaps the first Bayesian method capable of convincingly
outperforming plain deep ensembles.",2206.12361v3,https://arxiv.org/pdf/2206.12361v3
Robustness of Explanation Methods for NLP Models,"Shriya Atmakuri, Tejas Chheda, Dinesh Kandula, Nishant Yadav, Taesung Lee, Hessel Tuinhof","Explanation methods have emerged as an important tool to highlight the
features responsible for the predictions of neural networks. There is mounting
evidence that many explanation methods are rather unreliable and susceptible to
malicious manipulations. In this paper, we particularly aim to understand the
robustness of explanation methods in the context of text modality. We provide
initial insights and results towards devising a successful adversarial attack
against text explanations. To our knowledge, this is the first attempt to
evaluate the adversarial robustness of an explanation method. Our experiments
show the explanation method can be largely disturbed for up to 86% of the
tested samples with small changes in the input sentence and its semantics.",2206.12284v1,https://arxiv.org/pdf/2206.12284v1
"Adversarial Robustness of Deep Neural Networks: A Survey from a Formal
  Verification Perspective","Mark Huasong Meng, Guangdong Bai, Sin Gee Teo, Zhe Hou, Yan Xiao, Yun Lin, Jin Song Dong","Neural networks have been widely applied in security applications such as
spam and phishing detection, intrusion prevention, and malware detection. This
black-box method, however, often has uncertainty and poor explainability in
applications. Furthermore, neural networks themselves are often vulnerable to
adversarial attacks. For those reasons, there is a high demand for trustworthy
and rigorous methods to verify the robustness of neural network models.
Adversarial robustness, which concerns the reliability of a neural network when
dealing with maliciously manipulated inputs, is one of the hottest topics in
security and machine learning. In this work, we survey existing literature in
adversarial robustness verification for neural networks and collect 39
diversified research works across machine learning, security, and software
engineering domains. We systematically analyze their approaches, including how
robustness is formulated, what verification techniques are used, and the
strengths and limitations of each technique. We provide a taxonomy from a
formal verification perspective for a comprehensive understanding of this
topic. We classify the existing techniques based on property specification,
problem reduction, and reasoning strategies. We also demonstrate representative
techniques that have been applied in existing studies with a sample model.
Finally, we discuss open questions for future research.",2206.12227v2,https://arxiv.org/pdf/2206.12227v2
FLVoogd: Robust And Privacy Preserving Federated Learning,"Yuhang Tian, Rui Wang, Yanqi Qiao, Emmanouil Panaousis, Kaitai Liang","In this work, we propose FLVoogd, an updated federated learning method in
which servers and clients collaboratively eliminate Byzantine attacks while
preserving privacy. In particular, servers use automatic Density-based Spatial
Clustering of Applications with Noise (DBSCAN) combined with S2PC to cluster
the benign majority without acquiring sensitive personal information.
Meanwhile, clients build dual models and perform test-based distance
controlling to adjust their local models toward the global one to achieve
personalizing. Our framework is automatic and adaptive that servers/clients
don't need to tune the parameters during the training. In addition, our
framework leverages Secure Multi-party Computation (SMPC) operations, including
multiplications, additions, and comparison, where costly operations, like
division and square root, are not required. Evaluations are carried out on some
conventional datasets from the image classification field. The result shows
that FLVoogd can effectively reject malicious uploads in most scenarios;
meanwhile, it avoids data leakage from the server-side.",2207.00428v1,https://arxiv.org/pdf/2207.00428v1
TreeDRNet:A Robust Deep Model for Long Term Time Series Forecasting,"Tian Zhou, Jianqing Zhu, Xue Wang, Ziqing Ma, Qingsong Wen, Liang Sun, Rong Jin","Various deep learning models, especially some latest Transformer-based
approaches, have greatly improved the state-of-art performance for long-term
time series forecasting.However, those transformer-based models suffer a severe
deterioration performance with prolonged input length, which prohibits them
from using extended historical info.Moreover, these methods tend to handle
complex examples in long-term forecasting with increased model complexity,
which often leads to a significant increase in computation and less robustness
in performance(e.g., overfitting). We propose a novel neural network
architecture, called TreeDRNet, for more effective long-term forecasting.
Inspired by robust regression, we introduce doubly residual link structure to
make prediction more robust.Built upon Kolmogorov-Arnold representation
theorem, we explicitly introduce feature selection, model ensemble, and a tree
structure to further utilize the extended input sequence, which improves the
robustness and representation power of TreeDRNet. Unlike previous deep models
for sequential forecasting work, TreeDRNet is built entirely on multilayer
perceptron and thus enjoys high computational efficiency. Our extensive
empirical studies show that TreeDRNet is significantly more effective than
state-of-the-art methods, reducing prediction errors by 20% to 40% for
multivariate time series. In particular, TreeDRNet is over 10 times more
efficient than transformer-based methods. The code will be released soon.",2206.12106v1,https://arxiv.org/pdf/2206.12106v1
zPROBE: Zero Peek Robustness Checks for Federated Learning,"Zahra Ghodsi, Mojan Javaheripi, Nojan Sheybani, Xinqiao Zhang, Ke Huang, Farinaz Koushanfar","Privacy-preserving federated learning allows multiple users to jointly train
a model with coordination of a central server. The server only learns the final
aggregation result, thus the users' (private) training data is not leaked from
the individual model updates. However, keeping the individual updates private
allows malicious users to perform Byzantine attacks and degrade the accuracy
without being detected. Best existing defenses against Byzantine workers rely
on robust rank-based statistics, e.g., median, to find malicious updates.
However, implementing privacy-preserving rank-based statistics is nontrivial
and not scalable in the secure domain, as it requires sorting all individual
updates. We establish the first private robustness check that uses high break
point rank-based statistics on aggregated model updates. By exploiting
randomized clustering, we significantly improve the scalability of our defense
without compromising privacy. We leverage our statistical bounds in
zero-knowledge proofs to detect and remove malicious updates without revealing
the private user updates. Our novel framework, zPROBE, enables Byzantine
resilient and secure federated learning. Empirical evaluations demonstrate that
zPROBE provides a low overhead solution to defend against state-of-the-art
Byzantine attacks while preserving privacy.",2206.12100v3,https://arxiv.org/pdf/2206.12100v3
On making optimal transport robust to all outliers,Kilian Fatras,"Optimal transport (OT) is known to be sensitive against outliers because of
its marginal constraints. Outlier robust OT variants have been proposed based
on the definition that outliers are samples which are expensive to move. In
this paper, we show that this definition is restricted by considering the case
where outliers are closer to the target measure than clean samples. We show
that outlier robust OT fully transports these outliers leading to poor
performances in practice. To tackle these outliers, we propose to detect them
by relying on a classifier trained with adversarial training to classify source
and target samples. A sample is then considered as an outlier if the prediction
from the classifier is different from its assigned label. To decrease the
influence of these outliers in the transport problem, we propose to either
remove them from the problem or to increase the cost of moving them by using
the classifier prediction. We show that we successfully detect these outliers
and that they do not influence the transport problem on several experiments
such as gradient flows, generative models and label propagation.",2206.11988v1,https://arxiv.org/pdf/2206.11988v1
"Measuring Representational Robustness of Neural Networks Through Shared
  Invariances","Vedant Nanda, Till Speicher, Camila Kolling, John P. Dickerson, Krishna P. Gummadi, Adrian Weller","A major challenge in studying robustness in deep learning is defining the set
of ``meaningless'' perturbations to which a given Neural Network (NN) should be
invariant. Most work on robustness implicitly uses a human as the reference
model to define such perturbations. Our work offers a new view on robustness by
using another reference NN to define the set of perturbations a given NN should
be invariant to, thus generalizing the reliance on a reference ``human NN'' to
any NN. This makes measuring robustness equivalent to measuring the extent to
which two NNs share invariances, for which we propose a measure called STIR.
STIR re-purposes existing representation similarity measures to make them
suitable for measuring shared invariances. Using our measure, we are able to
gain insights into how shared invariances vary with changes in weight
initialization, architecture, loss functions, and training dataset. Our
implementation is available at: \url{https://github.com/nvedant07/STIR}.",2206.11939v1,https://arxiv.org/pdf/2206.11939v1
"Deep Reinforcement Learning-Assisted Federated Learning for Robust
  Short-term Utility Demand Forecasting in Electricity Wholesale Markets","Chenghao Huang, Weilong Chen, Shengrong Bu, Yanru Zhang","Short-term load forecasting (STLF) plays a significant role in the operation
of electricity trading markets. Considering the growing concern of data
privacy, federated learning (FL) is increasingly adopted to train STLF models
for utility companies (UCs) in recent research. Inspiringly, in wholesale
markets, as it is not realistic for power plants (PPs) to access UCs' data
directly, FL is definitely a feasible solution of obtaining an accurate STLF
model for PPs. However, due to FL's distributed nature and intense competition
among UCs, defects increasingly occur and lead to poor performance of the STLF
model, indicating that simply adopting FL is not enough. In this paper, we
propose a DRL-assisted FL approach, DEfect-AwaRe federated soft actor-critic
(DearFSAC), to robustly train an accurate STLF model for PPs to forecast
precise short-term utility electricity demand. Firstly. we design a STLF model
based on long short-term memory (LSTM) using just historical load data and time
data. Furthermore, considering the uncertainty of defects occurrence, a deep
reinforcement learning (DRL) algorithm is adopted to assist FL by alleviating
model degradation caused by defects. In addition, for faster convergence of FL
training, an auto-encoder is designed for both dimension reduction and quality
evaluation of uploaded models. In the simulations, we validate our approach on
real data of Helsinki's UCs in 2019. The results show that DearFSAC outperforms
all the other approaches no matter if defects occur or not.",2206.11715v2,https://arxiv.org/pdf/2206.11715v2
"Bi-stochastically normalized graph Laplacian: convergence to manifold
  Laplacian and robustness to outlier noise","Xiuyuan Cheng, Boris Landa","Bi-stochastic normalization provides an alternative normalization of graph
Laplacians in graph-based data analysis and can be computed efficiently by
Sinkhorn-Knopp (SK) iterations. This paper proves the convergence of
bi-stochastically normalized graph Laplacian to manifold (weighted-)Laplacian
with rates, when $n$ data points are i.i.d. sampled from a general
$d$-dimensional manifold embedded in a possibly high-dimensional space. Under
certain joint limit of $n \to \infty$ and kernel bandwidth $\epsilon \to 0$,
the point-wise convergence rate of the graph Laplacian operator (under 2-norm)
is proved to be $ O( n^{-1/(d/2+3)})$ at finite large $n$ up to log factors,
achieved at the scaling of $\epsilon \sim n^{-1/(d/2+3)} $. When the manifold
data are corrupted by outlier noise, we theoretically prove the graph Laplacian
point-wise consistency which matches the rate for clean manifold data plus an
additional term proportional to the boundedness of the inner-products of the
noise vectors among themselves and with data vectors. Motivated by our
analysis, which suggests that not exact bi-stochastic normalization but an
approximate one will achieve the same consistency rate, we propose an
approximate and constrained matrix scaling problem that can be solved by SK
iterations with early termination. Numerical experiments support our
theoretical results and show the robustness of bi-stochastically normalized
graph Laplacian to high-dimensional outlier noise.",2206.11386v3,https://arxiv.org/pdf/2206.11386v3
"Optimal transport meets noisy label robust loss and MixUp regularization
  for domain adaptation","Kilian Fatras, Hiroki Naganuma, Ioannis Mitliagkas","It is common in computer vision to be confronted with domain shift: images
which have the same class but different acquisition conditions. In domain
adaptation (DA), one wants to classify unlabeled target images using source
labeled images. Unfortunately, deep neural networks trained on a source
training set perform poorly on target images which do not belong to the
training domain. One strategy to improve these performances is to align the
source and target image distributions in an embedded space using optimal
transport (OT). However OT can cause negative transfer, i.e. aligning samples
with different labels, which leads to overfitting especially in the presence of
label shift between domains. In this work, we mitigate negative alignment by
explaining it as a noisy label assignment to target images. We then mitigate
its effect by appropriate regularization. We propose to couple the MixUp
regularization \citep{zhang2018mixup} with a loss that is robust to noisy
labels in order to improve domain adaptation performance. We show in an
extensive ablation study that a combination of the two techniques is critical
to achieve improved performance. Finally, we evaluate our method, called
\textsc{mixunbot}, on several benchmarks and real-world DA problems.",2206.11180v1,https://arxiv.org/pdf/2206.11180v1
ROSE: A RObust and SEcure DNN Watermarking,"Kassem Kallas, Teddy Furon","Protecting the Intellectual Property rights of DNN models is of primary
importance prior to their deployment. So far, the proposed methods either
necessitate changes to internal model parameters or the machine learning
pipeline, or they fail to meet both the security and robustness requirements.
This paper proposes a lightweight, robust, and secure black-box DNN
watermarking protocol that takes advantage of cryptographic one-way functions
as well as the injection of in-task key image-label pairs during the training
process. These pairs are later used to prove DNN model ownership during
testing. The main feature is that the value of the proof and its security are
measurable. The extensive experiments watermarking image classification models
for various datasets as well as exposing them to a variety of attacks, show
that it provides protection while maintaining an adequate level of security and
robustness.",2206.11024v1,https://arxiv.org/pdf/2206.11024v1
Robust Universal Adversarial Perturbations,"Changming Xu, Gagandeep Singh","Universal Adversarial Perturbations (UAPs) are imperceptible, image-agnostic
vectors that cause deep neural networks (DNNs) to misclassify inputs with high
probability. In practical attack scenarios, adversarial perturbations may
undergo transformations such as changes in pixel intensity, scaling, etc.
before being added to DNN inputs. Existing methods do not create UAPs robust to
these real-world transformations, thereby limiting their applicability in
practical attack scenarios. In this work, we introduce and formulate UAPs
robust against real-world transformations. We build an iterative algorithm
using probabilistic robustness bounds and construct such UAPs robust to
transformations generated by composing arbitrary sub-differentiable
transformation functions. We perform an extensive evaluation on the popular
CIFAR-10 and ILSVRC 2012 datasets measuring our UAPs' robustness under a wide
range common, real-world transformations such as rotation, contrast changes,
etc. We further show that by using a set of primitive transformations our
method can generalize well to unseen transformations such as fog, JPEG
compression, etc. Our results show that our method can generate UAPs up to 23%
more robust than state-of-the-art baselines.",2206.10858v2,https://arxiv.org/pdf/2206.10858v2
"Quantization Robust Federated Learning for Efficient Inference on
  Heterogeneous Devices","Kartik Gupta, Marios Fournarakis, Matthias Reisser, Christos Louizos, Markus Nagel","Federated Learning (FL) is a machine learning paradigm to distributively
learn machine learning models from decentralized data that remains on-device.
Despite the success of standard Federated optimization methods, such as
Federated Averaging (FedAvg) in FL, the energy demands and hardware induced
constraints for on-device learning have not been considered sufficiently in the
literature. Specifically, an essential demand for on-device learning is to
enable trained models to be quantized to various bit-widths based on the energy
needs and heterogeneous hardware designs across the federation. In this work,
we introduce multiple variants of federated averaging algorithm that train
neural networks robust to quantization. Such networks can be quantized to
various bit-widths with only limited reduction in full precision model
accuracy. We perform extensive experiments on standard FL benchmarks to
evaluate our proposed FedAvg variants for quantization robustness and provide a
convergence analysis for our Quantization-Aware variants in FL. Our results
demonstrate that integrating quantization robustness results in FL models that
are significantly more robust to different bit-widths during quantized
on-device inference.",2206.10844v1,https://arxiv.org/pdf/2206.10844v1
Robust Bayesian Recourse,"Tuan-Duy H. Nguyen, Ngoc Bui, Duy Nguyen, Man-Chung Yue, Viet Anh Nguyen","Algorithmic recourse aims to recommend an informative feedback to overturn an
unfavorable machine learning decision. We introduce in this paper the Bayesian
recourse, a model-agnostic recourse that minimizes the posterior probability
odds ratio. Further, we present its min-max robust counterpart with the goal of
hedging against future changes in the machine learning model parameters. The
robust counterpart explicitly takes into account possible perturbations of the
data in a Gaussian mixture ambiguity set prescribed using the optimal transport
(Wasserstein) distance. We show that the resulting worst-case objective
function can be decomposed into solving a series of two-dimensional
optimization subproblems, and the min-max recourse finding problem is thus
amenable to a gradient descent algorithm. Contrary to existing methods for
generating robust recourses, the robust Bayesian recourse does not require a
linear approximation step. The numerical experiment demonstrates the
effectiveness of our proposed robust Bayesian recourse facing model shifts. Our
code is available at https://github.com/VinAIResearch/robust-bayesian-recourse.",2206.10833v1,https://arxiv.org/pdf/2206.10833v1
"Robust SDE-Based Variational Formulations for Solving Linear PDEs via
  Deep Learning","Lorenz Richter, Julius Berner","The combination of Monte Carlo methods and deep learning has recently led to
efficient algorithms for solving partial differential equations (PDEs) in high
dimensions. Related learning problems are often stated as variational
formulations based on associated stochastic differential equations (SDEs),
which allow the minimization of corresponding losses using gradient-based
optimization methods. In respective numerical implementations it is therefore
crucial to rely on adequate gradient estimators that exhibit low variance in
order to reach convergence accurately and swiftly. In this article, we
rigorously investigate corresponding numerical aspects that appear in the
context of linear Kolmogorov PDEs. In particular, we systematically compare
existing deep learning approaches and provide theoretical explanations for
their performances. Subsequently, we suggest novel methods that can be shown to
be more robust both theoretically and numerically, leading to substantial
performance improvements.",2206.10588v2,https://arxiv.org/pdf/2206.10588v2
(Certified!!) Adversarial Robustness for Free!,"Nicholas Carlini, Florian Tramer, Krishnamurthy Dj Dvijotham, Leslie Rice, Mingjie Sun, J. Zico Kolter","In this paper we show how to achieve state-of-the-art certified adversarial
robustness to 2-norm bounded perturbations by relying exclusively on
off-the-shelf pretrained models. To do so, we instantiate the denoised
smoothing approach of Salman et al. 2020 by combining a pretrained denoising
diffusion probabilistic model and a standard high-accuracy classifier. This
allows us to certify 71% accuracy on ImageNet under adversarial perturbations
constrained to be within an 2-norm of 0.5, an improvement of 14 percentage
points over the prior certified SoTA using any approach, or an improvement of
30 percentage points over denoised smoothing. We obtain these results using
only pretrained diffusion models and image classifiers, without requiring any
fine tuning or retraining of model parameters.",2206.10550v2,https://arxiv.org/pdf/2206.10550v2
"Robust Task Representations for Offline Meta-Reinforcement Learning via
  Contrastive Learning","Haoqi Yuan, Zongqing Lu","We study offline meta-reinforcement learning, a practical reinforcement
learning paradigm that learns from offline data to adapt to new tasks. The
distribution of offline data is determined jointly by the behavior policy and
the task. Existing offline meta-reinforcement learning algorithms cannot
distinguish these factors, making task representations unstable to the change
of behavior policies. To address this problem, we propose a contrastive
learning framework for task representations that are robust to the distribution
mismatch of behavior policies in training and test. We design a bi-level
encoder structure, use mutual information maximization to formalize task
representation learning, derive a contrastive learning objective, and introduce
several approaches to approximate the true distribution of negative pairs.
Experiments on a variety of offline meta-reinforcement learning benchmarks
demonstrate the advantages of our method over prior methods, especially on the
generalization to out-of-distribution behavior policies. The code is available
at https://github.com/PKU-AI-Edge/CORRO.",2206.10442v1,https://arxiv.org/pdf/2206.10442v1
Plug and Play Counterfactual Text Generation for Model Robustness,"Nishtha Madaan, Srikanta Bedathur, Diptikalyan Saha","Generating counterfactual test-cases is an important backbone for testing NLP
models and making them as robust and reliable as traditional software. In
generating the test-cases, a desired property is the ability to control the
test-case generation in a flexible manner to test for a large variety of
failure cases and to explain and repair them in a targeted manner. In this
direction, significant progress has been made in the prior works by manually
writing rules for generating controlled counterfactuals. However, this approach
requires heavy manual supervision and lacks the flexibility to easily introduce
new controls. Motivated by the impressive flexibility of the plug-and-play
approach of PPLM, we propose bringing the framework of plug-and-play to
counterfactual test case generation task. We introduce CASPer, a plug-and-play
counterfactual generation framework to generate test cases that satisfy goal
attributes on demand. Our plug-and-play model can steer the test case
generation process given any attribute model without requiring
attribute-specific training of the model. In experiments, we show that CASPer
effectively generates counterfactual text that follow the steering provided by
an attribute model while also being fluent, diverse and preserving the original
content. We also show that the generated counterfactuals from CASPer can be
used for augmenting the training data and thereby fixing and making the test
model more robust.",2206.10429v1,https://arxiv.org/pdf/2206.10429v1
Neural Moving Horizon Estimation for Robust Flight Control,"Bingheng Wang, Zhengtian Ma, Shupeng Lai, Lin Zhao","Estimating and reacting to disturbances is crucial for robust flight control
of quadrotors. Existing estimators typically require significant tuning for a
specific flight scenario or training with extensive ground-truth disturbance
data to achieve satisfactory performance. In this paper, we propose a neural
moving horizon estimator (NeuroMHE) that can automatically tune its key
parameters modeled by a neural network and adapt to different flight scenarios.
We achieve this by deriving the analytical gradients of the MHE estimates with
respect to the MHE weighting matrices, which enables a seamless embedding of
the MHE as a learnable layer into the neural network for highly effective
learning. Interestingly, we show that the gradients can be computed efficiently
using a Kalman filter in a recursive form. Moreover, we develop a model-based
policy gradient algorithm to train NeuroMHE directly from the quadrotor
trajectory tracking error without needing the ground-truth disturbance data.
The effectiveness of NeuroMHE is verified extensively via both simulations and
physical experiments on quadrotors in various challenging flights. Notably,
NeuroMHE outperforms a state-of-the-art neural network-based estimator,
reducing force estimation errors by up to 76.7%, while using a portable neural
network that has only 7.7% of the learnable parameters of the latter. The
proposed method is general and can be applied to robust adaptive control of
other robotic systems.",2206.10397v13,https://arxiv.org/pdf/2206.10397v13
Why Robust Natural Language Understanding is a Challenge,"Marco Casadio, Ekaterina Komendantskaya, Verena Rieser, Matthew L. Daggitt, Daniel Kienitz, Luca Arnaboldi, Wen Kokke","With the proliferation of Deep Machine Learning into real-life applications,
a particular property of this technology has been brought to attention:
robustness Neural Networks notoriously present low robustness and can be highly
sensitive to small input perturbations. Recently, many methods for verifying
networks' general properties of robustness have been proposed, but they are
mostly applied in Computer Vision. In this paper we propose a Verification
specification for Natural Language Understanding classification based on larger
regions of interest, and we discuss the challenges of such task. We observe
that, although the data is almost linearly separable, the verifier struggles to
output positive results and we explain the problems and implications.",2206.14575v2,https://arxiv.org/pdf/2206.14575v2
"Certifiably Robust Policy Learning against Adversarial Communication in
  Multi-agent Systems","Yanchao Sun, Ruijie Zheng, Parisa Hassanzadeh, Yongyuan Liang, Soheil Feizi, Sumitra Ganesh, Furong Huang","Communication is important in many multi-agent reinforcement learning (MARL)
problems for agents to share information and make good decisions. However, when
deploying trained communicative agents in a real-world application where noise
and potential attackers exist, the safety of communication-based policies
becomes a severe issue that is underexplored. Specifically, if communication
messages are manipulated by malicious attackers, agents relying on
untrustworthy communication may take unsafe actions that lead to catastrophic
consequences. Therefore, it is crucial to ensure that agents will not be misled
by corrupted communication, while still benefiting from benign communication.
In this work, we consider an environment with $N$ agents, where the attacker
may arbitrarily change the communication from any $C<\frac{N-1}{2}$ agents to a
victim agent. For this strong threat model, we propose a certifiable defense by
constructing a message-ensemble policy that aggregates multiple randomly
ablated message sets. Theoretical analysis shows that this message-ensemble
policy can utilize benign communication while being certifiably robust to
adversarial communication, regardless of the attacking algorithm. Experiments
in multiple environments verify that our defense significantly improves the
robustness of trained policies against various types of attacks.",2206.10158v2,https://arxiv.org/pdf/2206.10158v2
"Robust Deep Reinforcement Learning through Bootstrapped Opportunistic
  Curriculum","Junlin Wu, Yevgeniy Vorobeychik","Despite considerable advances in deep reinforcement learning, it has been
shown to be highly vulnerable to adversarial perturbations to state
observations. Recent efforts that have attempted to improve adversarial
robustness of reinforcement learning can nevertheless tolerate only very small
perturbations, and remain fragile as perturbation size increases. We propose
Bootstrapped Opportunistic Adversarial Curriculum Learning (BCL), a novel
flexible adversarial curriculum learning framework for robust reinforcement
learning. Our framework combines two ideas: conservatively bootstrapping each
curriculum phase with highest quality solutions obtained from multiple runs of
the previous phase, and opportunistically skipping forward in the curriculum.
In our experiments we show that the proposed BCL framework enables dramatic
improvements in robustness of learned policies to adversarial perturbations.
The greatest improvement is for Pong, where our framework yields robustness to
perturbations of up to 25/255; in contrast, the best existing approach can only
tolerate adversarial noise up to 5/255. Our code is available at:
https://github.com/jlwu002/BCL.",2206.10057v2,https://arxiv.org/pdf/2206.10057v2
"Only Tails Matter: Average-Case Universality and Robustness in the
  Convex Regime","Leonardo Cunha, Gauthier Gidel, Fabian Pedregosa, Damien Scieur, Courtney Paquette","The recently developed average-case analysis of optimization methods allows a
more fine-grained and representative convergence analysis than usual worst-case
results. In exchange, this analysis requires a more precise hypothesis over the
data generating process, namely assuming knowledge of the expected spectral
distribution (ESD) of the random matrix associated with the problem. This work
shows that the concentration of eigenvalues near the edges of the ESD
determines a problem's asymptotic average complexity. This a priori information
on this concentration is a more grounded assumption than complete knowledge of
the ESD. This approximate concentration is effectively a middle ground between
the coarseness of the worst-case scenario convergence and the restrictive
previous average-case analysis. We also introduce the Generalized Chebyshev
method, asymptotically optimal under a hypothesis on this concentration and
globally optimal when the ESD follows a Beta distribution. We compare its
performance to classical optimization algorithms, such as gradient descent or
Nesterov's scheme, and we show that, in the average-case context, Nesterov's
method is universally nearly optimal asymptotically.",2206.09901v2,https://arxiv.org/pdf/2206.09901v2
"Understanding Robust Learning through the Lens of Representation
  Similarities","Christian Cianfarani, Arjun Nitin Bhagoji, Vikash Sehwag, Ben Y. Zhao, Prateek Mittal, Haitao Zheng","Representation learning, i.e. the generation of representations useful for
downstream applications, is a task of fundamental importance that underlies
much of the success of deep neural networks (DNNs). Recently, robustness to
adversarial examples has emerged as a desirable property for DNNs, spurring the
development of robust training methods that account for adversarial examples.
In this paper, we aim to understand how the properties of representations
learned by robust training differ from those obtained from standard, non-robust
training. This is critical to diagnosing numerous salient pitfalls in robust
networks, such as, degradation of performance on benign inputs, poor
generalization of robustness, and increase in over-fitting. We utilize a
powerful set of tools known as representation similarity metrics, across three
vision datasets, to obtain layer-wise comparisons between robust and non-robust
DNNs with different training procedures, architectural parameters and
adversarial constraints. Our experiments highlight hitherto unseen properties
of robust representations that we posit underlie the behavioral differences of
robust networks. We discover a lack of specialization in robust networks'
representations along with a disappearance of `block structure'. We also find
overfitting during robust training largely impacts deeper layers. These, along
with other findings, suggest ways forward for the design and training of better
robust networks.",2206.09868v2,https://arxiv.org/pdf/2206.09868v2
"Robust One Round Federated Learning with Predictive Space Bayesian
  Inference","Mohsin Hasan, Zehao Zhang, Kaiyang Guo, Mahdi Karami, Guojun Zhang, Xi Chen, Pascal Poupart","Making predictions robust is an important challenge. A separate challenge in
federated learning (FL) is to reduce the number of communication rounds,
particularly since doing so reduces performance in heterogeneous data settings.
To tackle both issues, we take a Bayesian perspective on the problem of
learning a global model. We show how the global predictive posterior can be
approximated using client predictive posteriors. This is unlike other works
which aggregate the local model space posteriors into the global model space
posterior, and are susceptible to high approximation errors due to the
posterior's high dimensional multimodal nature. In contrast, our method
performs the aggregation on the predictive posteriors, which are typically
easier to approximate owing to the low-dimensionality of the output space. We
present an algorithm based on this idea, which performs MCMC sampling at each
client to obtain an estimate of the local posterior, and then aggregates these
in one round to obtain a global ensemble model. Through empirical evaluation on
several classification and regression tasks, we show that despite using one
round of communication, the method is competitive with other FL techniques, and
outperforms them on heterogeneous settings. The code is publicly available at
https://github.com/hasanmohsin/FedPredSpace_1Round.",2206.09526v1,https://arxiv.org/pdf/2206.09526v1
"Adversarially trained neural representations may already be as robust as
  corresponding biological neural representations","Chong Guo, Michael J. Lee, Guillaume Leclerc, Joel Dapello, Yug Rao, Aleksander Madry, James J. DiCarlo","Visual systems of primates are the gold standard of robust perception. There
is thus a general belief that mimicking the neural representations that
underlie those systems will yield artificial visual systems that are
adversarially robust. In this work, we develop a method for performing
adversarial visual attacks directly on primate brain activity. We then leverage
this method to demonstrate that the above-mentioned belief might not be well
founded. Specifically, we report that the biological neurons that make up
visual systems of primates exhibit susceptibility to adversarial perturbations
that is comparable in magnitude to existing (robustly trained) artificial
neural networks.",2206.11228v1,https://arxiv.org/pdf/2206.11228v1
Robust Imitation Learning against Variations in Environment Dynamics,"Jongseong Chae, Seungyul Han, Whiyoung Jung, Myungsik Cho, Sungho Choi, Youngchul Sung","In this paper, we propose a robust imitation learning (IL) framework that
improves the robustness of IL when environment dynamics are perturbed. The
existing IL framework trained in a single environment can catastrophically fail
with perturbations in environment dynamics because it does not capture the
situation that underlying environment dynamics can be changed. Our framework
effectively deals with environments with varying dynamics by imitating multiple
experts in sampled environment dynamics to enhance the robustness in general
variations in environment dynamics. In order to robustly imitate the multiple
sample experts, we minimize the risk with respect to the Jensen-Shannon
divergence between the agent's policy and each of the sample experts. Numerical
results show that our algorithm significantly improves robustness against
dynamics perturbations compared to conventional IL baselines.",2206.09314v1,https://arxiv.org/pdf/2206.09314v1
"Reduced Robust Random Cut Forest for Out-Of-Distribution detection in
  machine learning models","Harsh Vardhan, Janos Sztipanovits","Most machine learning-based regressors extract information from data
collected via past observations of limited length to make predictions in the
future. Consequently, when input to these trained models is data with
significantly different statistical properties from data used for training,
there is no guarantee of accurate prediction. Consequently, using these models
on out-of-distribution input data may result in a completely different
predicted outcome from the desired one, which is not only erroneous but can
also be hazardous in some cases. Successful deployment of these machine
learning models in any system requires a detection system, which should be able
to distinguish between out-of-distribution and in-distribution data (i.e.
similar to training data). In this paper, we introduce a novel approach for
this detection process using a Reduced Robust Random Cut Forest (RRRCF) data
structure, which can be used on both small and large data sets. Similar to the
Robust Random Cut Forest (RRCF), RRRCF is a structured, but a reduced
representation of the training data sub-space in form of cut trees. Empirical
results of this method on both low and high-dimensional data showed that
inference about data being in/out of training distribution can be made
efficiently and the model is easy to train with no difficult hyper-parameter
tuning. The paper discusses two different use-cases for testing and validating
results.",2206.09247v1,https://arxiv.org/pdf/2206.09247v1
"Bioinspired random projections for robust, sparse classification","Nina Dekoninck Bruhin, Bryn Davies","Inspired by the use of random projections in biological sensing systems, we
present a new algorithm for processing data in classification problems. This is
based on observations of the human brain and the fruit fly's olfactory system
and involves randomly projecting data into a space of greatly increased
dimension before applying a cap operation to truncate the smaller entries. This
leads to a simple algorithm that is very computationally efficient and can be
used to either give a sparse representation with minimal loss in classification
accuracy or give improved robustness, in the sense that classification accuracy
is improved when noise is added to the data. This is demonstrated with
numerical experiments, which supplement theoretical results demonstrating that
the resulting signal transform is continuous and invertible, in an appropriate
sense.",2206.09222v2,https://arxiv.org/pdf/2206.09222v2
"Demystifying the Adversarial Robustness of Random Transformation
  Defenses","Chawin Sitawarin, Zachary Golan-Strieb, David Wagner","Neural networks' lack of robustness against attacks raises concerns in
security-sensitive settings such as autonomous vehicles. While many
countermeasures may look promising, only a few withstand rigorous evaluation.
Defenses using random transformations (RT) have shown impressive results,
particularly BaRT (Raff et al., 2019) on ImageNet. However, this type of
defense has not been rigorously evaluated, leaving its robustness properties
poorly understood. Their stochastic properties make evaluation more challenging
and render many proposed attacks on deterministic models inapplicable. First,
we show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluation
is ineffective and likely overestimates its robustness. We then attempt to
construct the strongest possible RT defense through the informed selection of
transformations and Bayesian optimization for tuning their parameters.
Furthermore, we create the strongest possible attack to evaluate our RT
defense. Our new attack vastly outperforms the baseline, reducing the accuracy
by 83% compared to the 19% reduction by the commonly used EoT attack
($4.3\times$ improvement). Our result indicates that the RT defense on the
Imagenette dataset (a ten-class subset of ImageNet) is not robust against
adversarial examples. Extending the study further, we use our new attack to
adversarially train RT defense (called AdvRT), resulting in a large robustness
gain. Code is available at
https://github.com/wagner-group/demystify-random-transform.",2207.03574v2,https://arxiv.org/pdf/2207.03574v2
"Fast and Provable Tensor Robust Principal Component Analysis via Scaled
  Gradient Descent","Harry Dong, Tian Tong, Cong Ma, Yuejie Chi","An increasing number of data science and machine learning problems rely on
computation with tensors, which better capture the multi-way relationships and
interactions of data than matrices. When tapping into this critical advantage,
a key challenge is to develop computationally efficient and provably correct
algorithms for extracting useful information from tensor data that are
simultaneously robust to corruptions and ill-conditioning. This paper tackles
tensor robust principal component analysis (RPCA), which aims to recover a
low-rank tensor from its observations contaminated by sparse corruptions, under
the Tucker decomposition. To minimize the computation and memory footprints, we
propose to directly recover the low-dimensional tensor factors -- starting from
a tailored spectral initialization -- via scaled gradient descent (ScaledGD),
coupled with an iteration-varying thresholding operation to adaptively remove
the impact of corruptions. Theoretically, we establish that the proposed
algorithm converges linearly to the true low-rank tensor at a constant rate
that is independent with its condition number, as long as the level of
corruptions is not too large. Empirically, we demonstrate that the proposed
algorithm achieves better and more scalable performance than state-of-the-art
matrix and tensor RPCA algorithms through synthetic experiments and real-world
applications.",2206.09109v2,https://arxiv.org/pdf/2206.09109v2
Adversarial Robustness is at Odds with Lazy Training,"Yunjuan Wang, Enayat Ullah, Poorya Mianjy, Raman Arora","Recent works show that adversarial examples exist for random neural networks
[Daniely and Schacham, 2020] and that these examples can be found using a
single step of gradient ascent [Bubeck et al., 2021]. In this work, we extend
this line of work to ""lazy training"" of neural networks -- a dominant model in
deep learning theory in which neural networks are provably efficiently
learnable. We show that over-parametrized neural networks that are guaranteed
to generalize well and enjoy strong computational guarantees remain vulnerable
to attacks generated using a single step of gradient ascent.",2207.00411v2,https://arxiv.org/pdf/2207.00411v2
Riemannian CUR Decompositions for Robust Principal Component Analysis,"Keaton Hamm, Mohamed Meskini, HanQin Cai","Robust Principal Component Analysis (PCA) has received massive attention in
recent years. It aims to recover a low-rank matrix and a sparse matrix from
their sum. This paper proposes a novel nonconvex Robust PCA algorithm, coined
Riemannian CUR (RieCUR), which utilizes the ideas of Riemannian optimization
and robust CUR decompositions. This algorithm has the same computational
complexity as Iterated Robust CUR, which is currently state-of-the-art, but is
more robust to outliers. RieCUR is also able to tolerate a significant amount
of outliers, and is comparable to Accelerated Alternating Projections, which
has high outlier tolerance but worse computational complexity than the proposed
method. Thus, the proposed algorithm achieves state-of-the-art performance on
Robust PCA both in terms of computational complexity and outlier tolerance.",2206.09042v1,https://arxiv.org/pdf/2206.09042v1
Robust Group Synchronization via Quadratic Programming,"Yunpeng Shi, Cole Wyeth, Gilad Lerman","We propose a novel quadratic programming formulation for estimating the
corruption levels in group synchronization, and use these estimates to solve
this problem. Our objective function exploits the cycle consistency of the
group and we thus refer to our method as detection and estimation of structural
consistency (DESC). This general framework can be extended to other algebraic
and geometric structures. Our formulation has the following advantages: it can
tolerate corruption as high as the information-theoretic bound, it does not
require a good initialization for the estimates of group elements, it has a
simple interpretation, and under some mild conditions the global minimum of our
objective function exactly recovers the corruption levels. We demonstrate the
competitive accuracy of our approach on both synthetic and real data
experiments of rotation averaging.",2206.08994v1,https://arxiv.org/pdf/2206.08994v1
"StaDRe and StaDRo: Reliability and Robustness Estimation of ML-based
  Forecasting using Statistical Distance Measures","Mohammed Naveed Akram, Akshatha Ambekar, Ioannis Sorokos, Koorosh Aslansefat, Daniel Schneider","Reliability estimation of Machine Learning (ML) models is becoming a crucial
subject. This is particularly the case when such \mbox{models} are deployed in
safety-critical applications, as the decisions based on model predictions can
result in hazardous situations. In this regard, recent research has proposed
methods to achieve safe, \mbox{dependable}, and reliable ML systems. One such
method consists of detecting and analyzing distributional shift, and then
measuring how such systems respond to these shifts. This was proposed in
earlier work in SafeML. This work focuses on the use of SafeML for time series
data, and on reliability and robustness estimation of ML-forecasting methods
using statistical distance measures. To this end, distance measures based on
the Empirical Cumulative Distribution Function (ECDF) proposed in SafeML are
explored to measure Statistical-Distance Dissimilarity (SDD) across time
series. We then propose SDD-based Reliability Estimate (StaDRe) and SDD-based
Robustness (StaDRo) measures. With the help of a clustering technique, the
similarity between the statistical properties of data seen during training and
the forecasts is identified. The proposed method is capable of providing a link
between dataset SDD and Key Performance Indicators (KPIs) of the ML models.",2206.11116v1,https://arxiv.org/pdf/2206.11116v1
RetrievalGuard: Provably Robust 1-Nearest Neighbor Image Retrieval,"Yihan Wu, Hongyang Zhang, Heng Huang","Recent research works have shown that image retrieval models are vulnerable
to adversarial attacks, where slightly modified test inputs could lead to
problematic retrieval results. In this paper, we aim to design a provably
robust image retrieval model which keeps the most important evaluation metric
Recall@1 invariant to adversarial perturbation. We propose the first 1-nearest
neighbor (NN) image retrieval algorithm, RetrievalGuard, which is provably
robust against adversarial perturbations within an $\ell_2$ ball of calculable
radius. The challenge is to design a provably robust algorithm that takes into
consideration the 1-NN search and the high-dimensional nature of the embedding
space. Algorithmically, given a base retrieval model and a query sample, we
build a smoothed retrieval model by carefully analyzing the 1-NN search
procedure in the high-dimensional embedding space. We show that the smoothed
retrieval model has bounded Lipschitz constant and thus the retrieval score is
invariant to $\ell_2$ adversarial perturbations. Experiments on image retrieval
tasks validate the robustness of our RetrievalGuard method.",2206.11225v1,https://arxiv.org/pdf/2206.11225v1
"How Robust is Unsupervised Representation Learning to Distribution
  Shift?","Yuge Shi, Imant Daunhawer, Julia E. Vogt, Philip H. S. Torr, Amartya Sanyal","The robustness of machine learning algorithms to distributions shift is
primarily discussed in the context of supervised learning (SL). As such, there
is a lack of insight on the robustness of the representations learned from
unsupervised methods, such as self-supervised learning (SSL) and auto-encoder
based algorithms (AE), to distribution shift. We posit that the input-driven
objectives of unsupervised algorithms lead to representations that are more
robust to distribution shift than the target-driven objective of SL. We verify
this by extensively evaluating the performance of SSL and AE on both synthetic
and realistic distribution shift datasets. Following observations that the
linear layer used for classification itself can be susceptible to spurious
correlations, we evaluate the representations using a linear head trained on a
small amount of out-of-distribution (OOD) data, to isolate the robustness of
the learned representations from that of the linear head. We also develop
""controllable"" versions of existing realistic domain generalisation datasets
with adjustable degrees of distribution shifts. This allows us to study the
robustness of different learning algorithms under versatile yet realistic
distribution shift conditions. Our experiments show that representations
learned from unsupervised learning algorithms generalise better than SL under a
wide variety of extreme as well as realistic distribution shifts.",2206.08871v2,https://arxiv.org/pdf/2206.08871v2
"Is Multi-Modal Necessarily Better? Robustness Evaluation of Multi-modal
  Fake News Detection","Jinyin Chen, Chengyu Jia, Haibin Zheng, Ruoxi Chen, Chenbo Fu","The proliferation of fake news and its serious negative social influence push
fake news detection methods to become necessary tools for web managers.
Meanwhile, the multi-media nature of social media makes multi-modal fake news
detection popular for its ability to capture more modal features than uni-modal
detection methods. However, current literature on multi-modal detection is more
likely to pursue the detection accuracy but ignore the robustness of the
detector. To address this problem, we propose a comprehensive robustness
evaluation of multi-modal fake news detectors. In this work, we simulate the
attack methods of malicious users and developers, i.e., posting fake news and
injecting backdoors. Specifically, we evaluate multi-modal detectors with five
adversarial and two backdoor attack methods. Experiment results imply that: (1)
The detection performance of the state-of-the-art detectors degrades
significantly under adversarial attacks, even worse than general detectors; (2)
Most multi-modal detectors are more vulnerable when subjected to attacks on
visual modality than textual modality; (3) Popular events' images will cause
significant degradation to the detectors when they are subjected to backdoor
attacks; (4) The performance of these detectors under multi-modal attacks is
worse than under uni-modal attacks; (5) Defensive methods will improve the
robustness of the multi-modal detectors.",2206.08788v1,https://arxiv.org/pdf/2206.08788v1
Understanding Robust Overfitting of Adversarial Training and Beyond,"Chaojian Yu, Bo Han, Li Shen, Jun Yu, Chen Gong, Mingming Gong, Tongliang Liu","Robust overfitting widely exists in adversarial training of deep networks.
The exact underlying reasons for this are still not completely understood.
Here, we explore the causes of robust overfitting by comparing the data
distribution of \emph{non-overfit} (weak adversary) and \emph{overfitted}
(strong adversary) adversarial training, and observe that the distribution of
the adversarial data generated by weak adversary mainly contain small-loss
data. However, the adversarial data generated by strong adversary is more
diversely distributed on the large-loss data and the small-loss data. Given
these observations, we further designed data ablation adversarial training and
identify that some small-loss data which are not worthy of the adversary
strength cause robust overfitting in the strong adversary mode. To relieve this
issue, we propose \emph{minimum loss constrained adversarial training} (MLCAT):
in a minibatch, we learn large-loss data as usual, and adopt additional
measures to increase the loss of the small-loss data. Technically, MLCAT
hinders data fitting when they become easy to learn to prevent robust
overfitting; philosophically, MLCAT reflects the spirit of turning waste into
treasure and making the best use of each adversarial data; algorithmically, we
designed two realizations of MLCAT, and extensive experiments demonstrate that
MLCAT can eliminate robust overfitting and further boost adversarial
robustness.",2206.08675v2,https://arxiv.org/pdf/2206.08675v2
Thompson Sampling for Robust Transfer in Multi-Task Bandits,"Zhi Wang, Chicheng Zhang, Kamalika Chaudhuri","We study the problem of online multi-task learning where the tasks are
performed within similar but not necessarily identical multi-armed bandit
environments. In particular, we study how a learner can improve its overall
performance across multiple related tasks through robust transfer of knowledge.
While an upper confidence bound (UCB)-based algorithm has recently been shown
to achieve nearly-optimal performance guarantees in a setting where all tasks
are solved concurrently, it remains unclear whether Thompson sampling (TS)
algorithms, which have superior empirical performance in general, share similar
theoretical properties. In this work, we present a TS-type algorithm for a more
general online multi-task learning protocol, which extends the concurrent
setting. We provide its frequentist analysis and prove that it is also
nearly-optimal using a novel concentration inequality for multi-task data
aggregation at random stopping times. Finally, we evaluate the algorithm on
synthetic data and show that the TS-type algorithm enjoys superior empirical
performance in comparison with the UCB-based algorithm and a baseline algorithm
that performs TS for each individual task without transfer.",2206.08556v1,https://arxiv.org/pdf/2206.08556v1
"A Robust Stacking Framework for Training Deep Graph Models with
  Multifaceted Node Features","Jiuhai Chen, Jonas Mueller, Vassilis N. Ioannidis, Tom Goldstein, David Wipf","Graph Neural Networks (GNNs) with numerical node features and graph structure
as inputs have demonstrated superior performance on various supervised learning
tasks with graph data. However the numerical node features utilized by GNNs are
commonly extracted from raw data which is of text or tabular
(numeric/categorical) type in most real-world applications. The best models for
such data types in most standard supervised learning settings with IID
(non-graph) data are not simple neural network layers and thus are not easily
incorporated into a GNN. Here we propose a robust stacking framework that fuses
graph-aware propagation with arbitrary models intended for IID data, which are
ensembled and stacked in multiple layers. Our layer-wise framework leverages
bagging and stacking strategies to enjoy strong generalization, in a manner
which effectively mitigates label leakage and overfitting. Across a variety of
graph datasets with tabular/text node features, our method achieves comparable
or superior performance relative to both tabular/text and graph neural network
models, as well as existing state-of-the-art hybrid strategies that combine the
two.",2206.08473v1,https://arxiv.org/pdf/2206.08473v1
"Empirical Bayesian Approaches for Robust Constraint-based Causal
  Discovery under Insufficient Data","Zijun Cui, Naiyu Yin, Yuru Wang, Qiang Ji","Causal discovery is to learn cause-effect relationships among variables given
observational data and is important for many applications. Existing causal
discovery methods assume data sufficiency, which may not be the case in many
real world datasets. As a result, many existing causal discovery methods can
fail under limited data. In this work, we propose Bayesian-augmented
frequentist independence tests to improve the performance of constraint-based
causal discovery methods under insufficient data: 1) We firstly introduce a
Bayesian method to estimate mutual information (MI), based on which we propose
a robust MI based independence test; 2) Secondly, we consider the Bayesian
estimation of hypothesis likelihood and incorporate it into a well-defined
statistical test, resulting in a robust statistical testing based independence
test. We apply proposed independence tests to constraint-based causal discovery
methods and evaluate the performance on benchmark datasets with insufficient
samples. Experiments show significant performance improvement in terms of both
accuracy and efficiency over SOTA methods.",2206.08448v1,https://arxiv.org/pdf/2206.08448v1
Methods for Estimating and Improving Robustness of Language Models,Michal Štefánik,"Despite their outstanding performance, large language models (LLMs) suffer
notorious flaws related to their preference for simple, surface-level textual
relations over full semantic complexity of the problem. This proposal
investigates a common denominator of this problem in their weak ability to
generalise outside of the training domain. We survey diverse research
directions providing estimations of model generalisation ability and find that
incorporating some of these measures in the training objectives leads to
enhanced distributional robustness of neural models. Based on these findings,
we present future research directions towards enhancing the robustness of LLMs.",2206.08446v1,https://arxiv.org/pdf/2206.08446v1
"Catastrophic overfitting can be induced with discriminative non-robust
  features","Guillermo Ortiz-Jiménez, Pau de Jorge, Amartya Sanyal, Adel Bibi, Puneet K. Dokania, Pascal Frossard, Gregory Rogéz, Philip H. S. Torr","Adversarial training (AT) is the de facto method for building robust neural
networks, but it can be computationally expensive. To mitigate this, fast
single-step attacks can be used, but this may lead to catastrophic overfitting
(CO). This phenomenon appears when networks gain non-trivial robustness during
the first stages of AT, but then reach a breaking point where they become
vulnerable in just a few iterations. The mechanisms that lead to this failure
mode are still poorly understood. In this work, we study the onset of CO in
single-step AT methods through controlled modifications of typical datasets of
natural images. In particular, we show that CO can be induced at much smaller
$\epsilon$ values than it was observed before just by injecting images with
seemingly innocuous features. These features aid non-robust classification but
are not enough to achieve robustness on their own. Through extensive
experiments we analyze this novel phenomenon and discover that the presence of
these easy features induces a learning shortcut that leads to CO. Our findings
provide new insights into the mechanisms of CO and improve our understanding of
the dynamics of AT. The code to reproduce our experiments can be found at
https://github.com/gortizji/co_features.",2206.08242v2,https://arxiv.org/pdf/2206.08242v2
Noisy Learning for Neural ODEs Acts as a Robustness Locus Widening,"Martin Gonzalez, Hatem Hajri, Loic Cantat, Mihaly Petreczky","We investigate the problems and challenges of evaluating the robustness of
Differential Equation-based (DE) networks against synthetic distribution
shifts. We propose a novel and simple accuracy metric which can be used to
evaluate intrinsic robustness and to validate dataset corruption simulators. We
also propose methodology recommendations, destined for evaluating the many
faces of neural DEs' robustness and for comparing them with their discrete
counterparts rigorously. We then use this criteria to evaluate a cheap data
augmentation technique as a reliable way for demonstrating the natural
robustness of neural ODEs against simulated image corruptions across multiple
datasets.",2206.08237v1,https://arxiv.org/pdf/2206.08237v1
"""Understanding Robustness Lottery"": A Geometric Visual Comparative
  Analysis of Neural Network Pruning Approaches","Zhimin Li, Shusen Liu, Xin Yu, Kailkhura Bhavya, Jie Cao, Diffenderfer James Daniel, Peer-Timo Bremer, Valerio Pascucci","Deep learning approaches have provided state-of-the-art performance in many
applications by relying on large and overparameterized neural networks.
However, such networks have been shown to be very brittle and are difficult to
deploy on resource-limited platforms. Model pruning, i.e., reducing the size of
the network, is a widely adopted strategy that can lead to a more robust and
compact model. Many heuristics exist for model pruning, but empirical studies
show that some heuristics improve performance whereas others can make models
more brittle or have other side effects. This work aims to shed light on how
different pruning methods alter the network's internal feature representation
and the corresponding impact on model performance. To facilitate a
comprehensive comparison and characterization of the high-dimensional model
feature space, we introduce a visual geometric analysis of feature
representations. We decomposed and evaluated a set of critical geometric
concepts from the common adopted classification loss, and used them to design a
visualization system to compare and highlight the impact of pruning on model
performance and feature representation. The proposed tool provides an
environment for in-depth comparison of pruning methods and a comprehensive
understanding of how model response to common data corruption. By leveraging
the proposed visualization, machine learning researchers can reveal the
similarities between pruning methods and redundant in robustness evaluation
benchmarks, obtain geometric insights about the differences between pruned
models that achieve superior robustness performance, and identify samples that
are robust or fragile to model pruning and common data corruption to model
pruning and data corruption but also obtain insights and explanations on how
some pruned models achieve superior robustness performance.",2206.07918v2,https://arxiv.org/pdf/2206.07918v2
"Queried Unlabeled Data Improves and Robustifies Class-Incremental
  Learning","Tianlong Chen, Sijia Liu, Shiyu Chang, Lisa Amini, Zhangyang Wang","Class-incremental learning (CIL) suffers from the notorious dilemma between
learning newly added classes and preserving previously learned class knowledge.
That catastrophic forgetting issue could be mitigated by storing historical
data for replay, which yet would cause memory overheads as well as imbalanced
prediction updates. To address this dilemma, we propose to leverage ""free""
external unlabeled data querying in continual learning. We first present a CIL
with Queried Unlabeled Data (CIL-QUD) scheme, where we only store a handful of
past training samples as anchors and use them to query relevant unlabeled
examples each time. Along with new and past stored data, the queried unlabeled
are effectively utilized, through learning-without-forgetting (LwF)
regularizers and class-balance training. Besides preserving model
generalization over past and current tasks, we next study the problem of
adversarial robustness for CIL-QUD. Inspired by the recent success of learning
robust models with unlabeled data, we explore a new robustness-aware CIL
setting, where the learned adversarial robustness has to resist forgetting and
be transferred as new tasks come in continually. While existing options easily
fail, we show queried unlabeled data can continue to benefit, and seamlessly
extend CIL-QUD into its robustified versions, RCIL-QUD. Extensive experiments
demonstrate that CIL-QUD achieves substantial accuracy gains on CIFAR-10 and
CIFAR-100, compared to previous state-of-the-art CIL approaches. Moreover,
RCIL-QUD establishes the first strong milestone for robustness-aware CIL. Codes
are available in https://github.com/VITA-Group/CIL-QUD.",2206.07842v2,https://arxiv.org/pdf/2206.07842v2
Linearity Grafting: Relaxed Neuron Pruning Helps Certifiable Robustness,"Tianlong Chen, Huan Zhang, Zhenyu Zhang, Shiyu Chang, Sijia Liu, Pin-Yu Chen, Zhangyang Wang","Certifiable robustness is a highly desirable property for adopting deep
neural networks (DNNs) in safety-critical scenarios, but often demands tedious
computations to establish. The main hurdle lies in the massive amount of
non-linearity in large DNNs. To trade off the DNN expressiveness (which calls
for more non-linearity) and robustness certification scalability (which prefers
more linearity), we propose a novel solution to strategically manipulate
neurons, by ""grafting"" appropriate levels of linearity. The core of our
proposal is to first linearize insignificant ReLU neurons, to eliminate the
non-linear components that are both redundant for DNN performance and harmful
to its certification. We then optimize the associated slopes and intercepts of
the replaced linear activations for restoring model performance while
maintaining certifiability. Hence, typical neuron pruning could be viewed as a
special case of grafting a linear function of the fixed zero slopes and
intercept, that might overly restrict the network flexibility and sacrifice its
performance. Extensive experiments on multiple datasets and network backbones
show that our linearity grafting can (1) effectively tighten certified bounds;
(2) achieve competitive certifiable robustness without certified robust
training (i.e., over 30% improvements on CIFAR-10 models); and (3) scale up
complete verification to large adversarially trained models with 17M
parameters. Codes are available at
https://github.com/VITA-Group/Linearity-Grafting.",2206.07839v1,https://arxiv.org/pdf/2206.07839v1
Robust Attack Graph Generation,"Dennis Mouwen, Sicco Verwer, Azqa Nadeem","We present a method to learn automaton models that are more robust to input
modifications. It iteratively aligns sequences to a learned model, modifies the
sequences to their aligned versions, and re-learns the model. Automaton
learning algorithms are typically very good at modeling the frequent behavior
of a software system. Our solution can be used to also learn the behavior
present in infrequent sequences, as these will be aligned to the frequent ones
represented by the model. We apply our method to the SAGE tool for modeling
attacker behavior from intrusion alerts. In experiments, we demonstrate that
our algorithm learns models that can handle noise such as added and removed
symbols from sequences. Furthermore, it learns more concise models that fit
better to the training data.",2206.07776v1,https://arxiv.org/pdf/2206.07776v1
"Physics-Infused Fuzzy Generative Adversarial Network for Robust Failure
  Prognosis","Ryan Nguyen, Shubhendu Kumar Singh, Rahul Rai","Prognostics aid in the longevity of fielded systems or products. Quantifying
the system's current health enable prognosis to enhance the operator's
decision-making to preserve the system's health. Creating a prognosis for a
system can be difficult due to (a) unknown physical relationships and/or (b)
irregularities in data appearing well beyond the initiation of a problem.
Traditionally, three different modeling paradigms have been used to develop a
prognostics model: physics-based (PbM), data-driven (DDM), and hybrid modeling.
Recently, the hybrid modeling approach that combines the strength of both PbM
and DDM based approaches and alleviates their limitations is gaining traction
in the prognostics domain. In this paper, a novel hybrid modeling approach for
prognostics applications based on combining concepts from fuzzy logic and
generative adversarial networks (GANs) is outlined. The FuzzyGAN based method
embeds a physics-based model in the aggregation of the fuzzy implications. This
technique constrains the output of the learning method to a realistic solution.
Results on a bearing problem showcases the efficacy of adding a physics-based
aggregation in a fuzzy logic model to improve GAN's ability to model health and
give a more accurate system prognosis.",2206.07762v1,https://arxiv.org/pdf/2206.07762v1
"BIO-CXRNET: A Robust Multimodal Stacking Machine Learning Technique for
  Mortality Risk Prediction of COVID-19 Patients using Chest X-Ray Images and
  Clinical Data","Tawsifur Rahman, Muhammad E. H. Chowdhury, Amith Khandakar, Zaid Bin Mahbub, Md Sakib Abrar Hossain, Abraham Alhatou, Eynas Abdalla, Sreekumar Muthiyal, Khandaker Farzana Islam, Saad Bin Abul Kashem, Muhammad Salman Khan, Susu M. Zughaier, Maqsud Hossain","Fast and accurate detection of the disease can significantly help in reducing
the strain on the healthcare facility of any country to reduce the mortality
during any pandemic. The goal of this work is to create a multimodal system
using a novel machine learning framework that uses both Chest X-ray (CXR)
images and clinical data to predict severity in COVID-19 patients. In addition,
the study presents a nomogram-based scoring technique for predicting the
likelihood of death in high-risk patients. This study uses 25 biomarkers and
CXR images in predicting the risk in 930 COVID-19 patients admitted during the
first wave of COVID-19 (March-June 2020) in Italy. The proposed multimodal
stacking technique produced the precision, sensitivity, and F1-score, of
89.03%, 90.44%, and 89.03%, respectively to identify low or high-risk patients.
This multimodal approach improved the accuracy by 6% in comparison to the CXR
image or clinical data alone. Finally, nomogram scoring system using
multivariate logistic regression -- was used to stratify the mortality risk
among the high-risk patients identified in the first stage. Lactate
Dehydrogenase (LDH), O2 percentage, White Blood Cells (WBC) Count, Age, and
C-reactive protein (CRP) were identified as useful predictor using random
forest feature selection model. Five predictors parameters and a CXR image
based nomogram score was developed for quantifying the probability of death and
categorizing them into two risk groups: survived (<50%), and death (>=50%),
respectively. The multi-modal technique was able to predict the death
probability of high-risk patients with an F1 score of 92.88 %. The area under
the curves for the development and validation cohorts are 0.981 and 0.939,
respectively.",2206.07595v1,https://arxiv.org/pdf/2206.07595v1
"Robust and Sparse Estimation of Linear Regression Coefficients with
  Heavy-tailed Noises and Covariates",Takeyuki Sasai,"Robust and sparse estimation of linear regression coefficients is
investigated. The situation addressed by the present paper is that covariates
and noises are sampled from heavy-tailed distributions, and the covariates and
noises are contaminated by malicious outliers. Our estimator can be computed
efficiently. Further, the error bound of the estimator is nearly optimal.",2206.07594v3,https://arxiv.org/pdf/2206.07594v3
A Meta-Analysis of Distributionally-Robust Models,"Benjamin Feuer, Ameya Joshi, Chinmay Hegde","State-of-the-art image classifiers trained on massive datasets (such as
ImageNet) have been shown to be vulnerable to a range of both intentional and
incidental distribution shifts. On the other hand, several recent classifiers
with favorable out-of-distribution (OOD) robustness properties have emerged,
achieving high accuracy on their target tasks while maintaining their
in-distribution accuracy on challenging benchmarks. We present a meta-analysis
on a wide range of publicly released models, most of which have been published
over the last twelve months. Through this meta-analysis, we empirically
identify four main commonalities for all the best-performing OOD-robust models,
all of which illuminate the considerable promise of vision-language
pre-training.",2206.07565v1,https://arxiv.org/pdf/2206.07565v1
Corruption-Robust Contextual Search through Density Updates,"Renato Paes Leme, Chara Podimata, Jon Schneider","We study the problem of contextual search in the adversarial noise model. Let
$d$ be the dimension of the problem, $T$ be the time horizon and $C$ be the
total amount of noise in the system. For the $\eps$-ball loss, we give a tight
regret bound of $O(C + d \log(1/\eps))$ improving over the $O(d^3 \log(1/\eps))
\log^2(T) + C \log(T) \log(1/\eps))$ bound of Krishnamurthy et al (STOC21). For
the symmetric loss, we give an efficient algorithm with regret $O(C+d \log T)$.
  Our techniques are a significant departure from prior approaches.
Specifically, we keep track of density functions over the candidate vectors
instead of a knowledge set consisting of the candidate vectors consistent with
the feedback obtained.",2206.07528v1,https://arxiv.org/pdf/2206.07528v1
"Robust SAR ATR on MSTAR with Deep Learning Models trained on Full
  Synthetic MOCEM data","Benjamin Camus, Corentin Le Barbu, Eric Monteux","The promising potential of Deep Learning for Automatic Target Recognition
(ATR) on Synthetic Aperture Radar (SAR) images vanishes when considering the
complexity of collecting training datasets measurements. Simulation can
overcome this issue by producing synthetic training datasets. However, because
of the limited representativeness of simulation, models trained in a classical
way with synthetic images have limited generalization abilities when dealing
with real measurement at test time. Previous works identified a set of equally
promising deep-learning algorithms to tackle this issue. However, these
approaches have been evaluated in a very favorable scenario with a synthetic
training dataset that overfits the ground truth of the measured test data. In
this work, we study the ATR problem outside of this ideal condition, which is
unlikely to occur in real operational contexts. Our contribution is threefold.
(1) Using the MOCEM simulator (developed by SCALIAN DS for the French MoD/DGA),
we produce a synthetic MSTAR training dataset that differs significantly from
the real measurements. (2) We experimentally demonstrate the limits of the
state-of-the-art. (3) We show that domain randomization techniques and
adversarial training can be combined to overcome this issue. We demonstrate
that this approach is more robust than the state-of-the-art, with an accuracy
of 75 %, while having a limited impact on computing performance during
training.",2206.07352v1,https://arxiv.org/pdf/2206.07352v1
"Fast and Reliable Evaluation of Adversarial Robustness with
  Minimum-Margin Attack","Ruize Gao, Jiongxiao Wang, Kaiwen Zhou, Feng Liu, Binghui Xie, Gang Niu, Bo Han, James Cheng","The AutoAttack (AA) has been the most reliable method to evaluate adversarial
robustness when considerable computational resources are available. However,
the high computational cost (e.g., 100 times more than that of the project
gradient descent attack) makes AA infeasible for practitioners with limited
computational resources, and also hinders applications of AA in the adversarial
training (AT). In this paper, we propose a novel method, minimum-margin (MM)
attack, to fast and reliably evaluate adversarial robustness. Compared with AA,
our method achieves comparable performance but only costs 3% of the
computational time in extensive experiments. The reliability of our method lies
in that we evaluate the quality of adversarial examples using the margin
between two targets that can precisely identify the most adversarial example.
The computational efficiency of our method lies in an effective Sequential
TArget Ranking Selection (STARS) method, ensuring that the cost of the MM
attack is independent of the number of classes. The MM attack opens a new way
for evaluating adversarial robustness and provides a feasible and reliable way
to generate high-quality adversarial examples in AT.",2206.07314v1,https://arxiv.org/pdf/2206.07314v1
Can pruning improve certified robustness of neural networks?,"Zhangheng Li, Tianlong Chen, Linyi Li, Bo Li, Zhangyang Wang","With the rapid development of deep learning, the sizes of neural networks
become larger and larger so that the training and inference often overwhelm the
hardware resources. Given the fact that neural networks are often
over-parameterized, one effective way to reduce such computational overhead is
neural network pruning, by removing redundant parameters from trained neural
networks. It has been recently observed that pruning can not only reduce
computational overhead but also can improve empirical robustness of deep neural
networks (NNs), potentially owing to removing spurious correlations while
preserving the predictive accuracies. This paper for the first time
demonstrates that pruning can generally improve certified robustness for
ReLU-based NNs under the complete verification setting. Using the popular
Branch-and-Bound (BaB) framework, we find that pruning can enhance the
estimated bound tightness of certified robustness verification, by alleviating
linear relaxation and sub-domain split problems. We empirically verify our
findings with off-the-shelf pruning methods and further present a new
stability-based pruning method tailored for reducing neuron instability, that
outperforms existing pruning methods in enhancing certified robustness. Our
experiments show that by appropriately pruning an NN, its certified accuracy
can be boosted up to 8.2% under standard training, and up to 24.5% under
adversarial training on the CIFAR10 dataset. We additionally observe the
existence of certified lottery tickets that can match both standard and
certified robust accuracies of the original dense models across different
datasets. Our findings offer a new angle to study the intriguing interaction
between sparsity and robustness, i.e. interpreting the interaction of sparsity
and certified robustness via neuron stability. Codes are available at:
https://github.com/VITA-Group/CertifiedPruning.",2206.07311v2,https://arxiv.org/pdf/2206.07311v2
"A Gift from Label Smoothing: Robust Training with Adaptive Label
  Smoothing via Auxiliary Classifier under Label Noise","Jongwoo Ko, Bongsoo Yi, Se-Young Yun","As deep neural networks can easily overfit noisy labels, robust training in
the presence of noisy labels is becoming an important challenge in modern deep
learning. While existing methods address this problem in various directions,
they still produce unpredictable sub-optimal results since they rely on the
posterior information estimated by the feature extractor corrupted by noisy
labels. Lipschitz regularization successfully alleviates this problem by
training a robust feature extractor, but it requires longer training time and
expensive computations. Motivated by this, we propose a simple yet effective
method, called ALASCA, which efficiently provides a robust feature extractor
under label noise. ALASCA integrates two key ingredients: (1) adaptive label
smoothing based on our theoretical analysis that label smoothing implicitly
induces Lipschitz regularization, and (2) auxiliary classifiers that enable
practical application of intermediate Lipschitz regularization with negligible
computations. We conduct wide-ranging experiments for ALASCA and combine our
proposed method with previous noise-robust methods on several synthetic and
real-world datasets. Experimental results show that our framework consistently
improves the robustness of feature extractors and the performance of existing
baselines with efficiency. Our code is available at
https://github.com/jongwooko/ALASCA.",2206.07277v2,https://arxiv.org/pdf/2206.07277v2
MBGDT:Robust Mini-Batch Gradient Descent,"Hanming Wang, Haozheng Luo, Yue Wang","In high dimensions, most machine learning method perform fragile even there
are a little outliers. To address this, we hope to introduce a new method with
the base learner, such as Bayesian regression or stochastic gradient descent to
solve the problem of the vulnerability in the model. Because the mini-batch
gradient descent allows for a more robust convergence than the batch gradient
descent, we work a method with the mini-batch gradient descent, called
Mini-Batch Gradient Descent with Trimming (MBGDT). Our method show state-of-art
performance and have greater robustness than several baselines when we apply
our method in designed dataset.",2206.07139v1,https://arxiv.org/pdf/2206.07139v1
On Provably Robust Meta-Bayesian Optimization,"Zhongxiang Dai, Yizhou Chen, Haibin Yu, Bryan Kian Hsiang Low, Patrick Jaillet","Bayesian optimization (BO) has become popular for sequential optimization of
black-box functions. When BO is used to optimize a target function, we often
have access to previous evaluations of potentially related functions. This begs
the question as to whether we can leverage these previous experiences to
accelerate the current BO task through meta-learning (meta-BO), while ensuring
robustness against potentially harmful dissimilar tasks that could sabotage the
convergence of BO. This paper introduces two scalable and provably robust
meta-BO algorithms: robust meta-Gaussian process-upper confidence bound
(RM-GP-UCB) and RM-GP-Thompson sampling (RM-GP-TS). We prove that both
algorithms are asymptotically no-regret even when some or all previous tasks
are dissimilar to the current task, and show that RM-GP-UCB enjoys a better
theoretical robustness than RM-GP-TS. We also exploit the theoretical
guarantees to optimize the weights assigned to individual previous tasks
through regret minimization via online learning, which diminishes the impact of
dissimilar tasks and hence further enhances the robustness. Empirical
evaluations show that (a) RM-GP-UCB performs effectively and consistently
across various applications, and (b) RM-GP-TS, despite being less robust than
RM-GP-UCB both in theory and in practice, performs competitively in some
scenarios with less dissimilar tasks and is more computationally efficient.",2206.06872v2,https://arxiv.org/pdf/2206.06872v2
"Robust Reinforcement Learning with Distributional Risk-averse
  formulation","Pierre Clavier, Stéphanie Allassonière, Erwan Le Pennec","Robust Reinforcement Learning tries to make predictions more robust to
changes in the dynamics or rewards of the system. This problem is particularly
important when the dynamics and rewards of the environment are estimated from
the data. In this paper, we approximate the Robust Reinforcement Learning
constrained with a $\Phi$-divergence using an approximate Risk-Averse
formulation. We show that the classical Reinforcement Learning formulation can
be robustified using standard deviation penalization of the objective. Two
algorithms based on Distributional Reinforcement Learning, one for discrete and
one for continuous action spaces are proposed and tested in a classical Gym
environment to demonstrate the robustness of the algorithms.",2206.06841v1,https://arxiv.org/pdf/2206.06841v1
"Adversarially Robust Multi-Armed Bandit Algorithm with
  Variance-Dependent Regret Bounds","Shinji Ito, Taira Tsuchiya, Junya Honda","This paper considers the multi-armed bandit (MAB) problem and provides a new
best-of-both-worlds (BOBW) algorithm that works nearly optimally in both
stochastic and adversarial settings. In stochastic settings, some existing BOBW
algorithms achieve tight gap-dependent regret bounds of $O(\sum_{i: \Delta_i>0}
\frac{\log T}{\Delta_i})$ for suboptimality gap $\Delta_i$ of arm $i$ and time
horizon $T$. As Audibert et al. [2007] have shown, however, that the
performance can be improved in stochastic environments with low-variance arms.
In fact, they have provided a stochastic MAB algorithm with
gap-variance-dependent regret bounds of $O(\sum_{i: \Delta_i>0}
(\frac{\sigma_i^2}{\Delta_i} + 1) \log T )$ for loss variance $\sigma_i^2$ of
arm $i$. In this paper, we propose the first BOBW algorithm with
gap-variance-dependent bounds, showing that the variance information can be
used even in the possibly adversarial environment. Further, the leading
constant factor in our gap-variance dependent bound is only (almost) twice the
value for the lower bound. Additionally, the proposed algorithm enjoys multiple
data-dependent regret bounds in adversarial settings and works well in
stochastic settings with adversarial corruptions. The proposed algorithm is
based on the follow-the-regularized-leader method and employs adaptive learning
rates that depend on the empirical prediction error of the loss, which leads to
gap-variance-dependent regret bounds reflecting the variance of the arms.",2206.06810v1,https://arxiv.org/pdf/2206.06810v1
"Towards Alternative Techniques for Improving Adversarial Robustness:
  Analysis of Adversarial Training at a Spectrum of Perturbations","Kaustubh Sridhar, Souradeep Dutta, Ramneet Kaur, James Weimer, Oleg Sokolsky, Insup Lee","Adversarial training (AT) and its variants have spearheaded progress in
improving neural network robustness to adversarial perturbations and common
corruptions in the last few years. Algorithm design of AT and its variants are
focused on training models at a specified perturbation strength $\epsilon$ and
only using the feedback from the performance of that $\epsilon$-robust model to
improve the algorithm. In this work, we focus on models, trained on a spectrum
of $\epsilon$ values. We analyze three perspectives: model performance,
intermediate feature precision and convolution filter sensitivity. In each, we
identify alternative improvements to AT that otherwise wouldn't have been
apparent at a single $\epsilon$. Specifically, we find that for a PGD attack at
some strength $\delta$, there is an AT model at some slightly larger strength
$\epsilon$, but no greater, that generalizes best to it. Hence, we propose
overdesigning for robustness where we suggest training models at an $\epsilon$
just above $\delta$. Second, we observe (across various $\epsilon$ values) that
robustness is highly sensitive to the precision of intermediate features and
particularly those after the first and second layer. Thus, we propose adding a
simple quantization to defenses that improves accuracy on seen and unseen
adaptive attacks. Third, we analyze convolution filters of each layer of models
at increasing $\epsilon$ and notice that those of the first and second layer
may be solely responsible for amplifying input perturbations. We present our
findings and demonstrate our techniques through experiments with ResNet and
WideResNet models on the CIFAR-10 and CIFAR-10-C datasets.",2206.06496v1,https://arxiv.org/pdf/2206.06496v1
Robust Distillation for Worst-class Performance,"Serena Wang, Harikrishna Narasimhan, Yichen Zhou, Sara Hooker, Michal Lukasik, Aditya Krishna Menon","Knowledge distillation has proven to be an effective technique in improving
the performance a student model using predictions from a teacher model.
However, recent work has shown that gains in average efficiency are not uniform
across subgroups in the data, and in particular can often come at the cost of
accuracy on rare subgroups and classes. To preserve strong performance across
classes that may follow a long-tailed distribution, we develop distillation
techniques that are tailored to improve the student's worst-class performance.
Specifically, we introduce robust optimization objectives in different
combinations for the teacher and student, and further allow for training with
any tradeoff between the overall accuracy and the robust worst-class objective.
We show empirically that our robust distillation techniques not only achieve
better worst-class performance, but also lead to Pareto improvement in the
tradeoff between overall performance and worst-class performance compared to
other baseline methods. Theoretically, we provide insights into what makes a
good teacher when the goal is to train a robust student.",2206.06479v1,https://arxiv.org/pdf/2206.06479v1
"Distributed Adversarial Training to Robustify Deep Neural Networks at
  Scale","Gaoyuan Zhang, Songtao Lu, Yihua Zhang, Xiangyi Chen, Pin-Yu Chen, Quanfu Fan, Lee Martie, Lior Horesh, Mingyi Hong, Sijia Liu","Current deep neural networks (DNNs) are vulnerable to adversarial attacks,
where adversarial perturbations to the inputs can change or manipulate
classification. To defend against such attacks, an effective and popular
approach, known as adversarial training (AT), has been shown to mitigate the
negative impact of adversarial attacks by virtue of a min-max robust training
method. While effective, it remains unclear whether it can successfully be
adapted to the distributed learning context. The power of distributed
optimization over multiple machines enables us to scale up robust training over
large models and datasets. Spurred by that, we propose distributed adversarial
training (DAT), a large-batch adversarial training framework implemented over
multiple machines. We show that DAT is general, which supports training over
labeled and unlabeled data, multiple types of attack generation methods, and
gradient compression operations favored for distributed optimization.
Theoretically, we provide, under standard conditions in the optimization
theory, the convergence rate of DAT to the first-order stationary points in
general non-convex settings. Empirically, we demonstrate that DAT either
matches or outperforms state-of-the-art robust accuracies and achieves a
graceful training speedup (e.g., on ResNet-50 under ImageNet). Codes are
available at https://github.com/dat-2022/dat.",2206.06257v2,https://arxiv.org/pdf/2206.06257v2
Robust Time Series Denoising with Learnable Wavelet Packet Transform,"Gaetan Frusque, Olga Fink","Signal denoising is a key preprocessing step for many applications, as the
performance of a learning task is closely related to the quality of the input
data. In this paper, we apply a signal processing based deep neural network
architecture, a learnable extension of the wavelet packet transform. As main
advantages, this model has few parameters, an intuitive initialization and
strong learning capabilities. Moreover, we show that it is possible to easily
modify the parameters of the model after the training step to tailor to
different noise intensities. Two case studies are conducted to compare this
model with the state of the art and commonly used denoising procedures. The
first experiment uses standard signals to study denoising properties of the
algorithms. The second experiment is a real application with the objective to
remove audio background noises. We show that the learnable wavelet packet
transform has the learning capabilities of deep learning methods while
maintaining the robustness of standard signal processing approaches. More
specifically, we demonstrate that our approach maintains excellent denoising
performances on signal classes separate from those used during the training
step. Moreover, the learnable wavelet packet transform was found to be robust
when different noise intensities, noise varieties and artifacts are considered.",2206.06126v4,https://arxiv.org/pdf/2206.06126v4
"PRO-TIP: Phantom for RObust automatic ultrasound calibration by TIP
  detection","Matteo Ronchetti, Julia Rackerseder, Maria Tirindelli, Mehrdad Salehi, Nassir Navab, Wolfgang Wein, Oliver Zettinig","We propose a novel method to automatically calibrate tracked ultrasound
probes. To this end we design a custom phantom consisting of nine cones with
different heights. The tips are used as key points to be matched between
multiple sweeps. We extract them using a convolutional neural network to
segment the cones in every ultrasound frame and then track them across the
sweep. The calibration is robustly estimated using RANSAC and later refined
employing image based techniques. Our phantom can be 3D-printed and offers many
advantages over state-of-the-art methods. The phantom design and algorithm code
are freely available online. Since our phantom does not require a tracking
target on itself, ease of use is improved over currently used techniques. The
fully automatic method generalizes to new probes and different vendors, as
shown in our experiments. Our approach produces results comparable to
calibrations obtained by a domain expert.",2206.05962v1,https://arxiv.org/pdf/2206.05962v1
Pixel to Binary Embedding Towards Robustness for CNNs,"Ikki Kishida, Hideki Nakayama","There are several problems with the robustness of Convolutional Neural
Networks (CNNs). For example, the prediction of CNNs can be changed by adding a
small magnitude of noise to an input, and the performances of CNNs are degraded
when the distribution of input is shifted by a transformation never seen during
training (e.g., the blur effect). There are approaches to replace pixel values
with binary embeddings to tackle the problem of adversarial perturbations,
which successfully improve robustness. In this work, we propose Pixel to Binary
Embedding (P2BE) to improve the robustness of CNNs. P2BE is a learnable binary
embedding method as opposed to previous hand-coded binary embedding methods.
P2BE outperforms other binary embedding methods in robustness against
adversarial perturbations and visual corruptions that are not shown during
training.",2206.05898v1,https://arxiv.org/pdf/2206.05898v1
"InBiaseD: Inductive Bias Distillation to Improve Generalization and
  Robustness through Shape-awareness","Shruthi Gowda, Bahram Zonooz, Elahe Arani","Humans rely less on spurious correlations and trivial cues, such as texture,
compared to deep neural networks which lead to better generalization and
robustness. It can be attributed to the prior knowledge or the high-level
cognitive inductive bias present in the brain. Therefore, introducing
meaningful inductive bias to neural networks can help learn more generic and
high-level representations and alleviate some of the shortcomings. We propose
InBiaseD to distill inductive bias and bring shape-awareness to the neural
networks. Our method includes a bias alignment objective that enforces the
networks to learn more generic representations that are less vulnerable to
unintended cues in the data which results in improved generalization
performance. InBiaseD is less susceptible to shortcut learning and also
exhibits lower texture bias. The better representations also aid in improving
robustness to adversarial attacks and we hence plugin InBiaseD seamlessly into
the existing adversarial training schemes to show a better trade-off between
generalization and robustness.",2206.05846v1,https://arxiv.org/pdf/2206.05846v1
Communication-Efficient Robust Federated Learning with Noisy Labels,"Junyi Li, Jian Pei, Heng Huang","Federated learning (FL) is a promising privacy-preserving machine learning
paradigm over distributed located data. In FL, the data is kept locally by each
user. This protects the user privacy, but also makes the server difficult to
verify data quality, especially if the data are correctly labeled. Training
with corrupted labels is harmful to the federated learning task; however,
little attention has been paid to FL in the case of label noise. In this paper,
we focus on this problem and propose a learning-based reweighting approach to
mitigate the effect of noisy labels in FL. More precisely, we tuned a weight
for each training sample such that the learned model has optimal generalization
performance over a validation set. More formally, the process can be formulated
as a Federated Bilevel Optimization problem. Bilevel optimization problem is a
type of optimization problem with two levels of entangled problems. The
non-distributed bilevel problems have witnessed notable progress recently with
new efficient algorithms. However, solving bilevel optimization problems under
the Federated Learning setting is under-investigated. We identify that the high
communication cost in hypergradient evaluation is the major bottleneck. So we
propose \textit{Comm-FedBiO} to solve the general Federated Bilevel
Optimization problems; more specifically, we propose two
communication-efficient subroutines to estimate the hypergradient. Convergence
analysis of the proposed algorithms is also provided. Finally, we apply the
proposed algorithms to solve the noisy label problem. Our approach has shown
superior performance on several real-world datasets compared to various
baselines.",2206.05558v1,https://arxiv.org/pdf/2206.05558v1
"Memory Classifiers: Two-stage Classification for Robustness in Machine
  Learning","Souradeep Dutta, Yahan Yang, Elena Bernardis, Edgar Dobriban, Insup Lee","The performance of machine learning models can significantly degrade under
distribution shifts of the data. We propose a new method for classification
which can improve robustness to distribution shifts, by combining expert
knowledge about the ``high-level"" structure of the data with standard
classifiers. Specifically, we introduce two-stage classifiers called memory
classifiers. First, these identify prototypical data points -- memories -- to
cluster the training data. This step is based on features designed with expert
guidance; for instance, for image data they can be extracted using digital
image processing algorithms. Then, within each cluster, we learn local
classifiers based on finer discriminating features, via standard models like
deep neural networks. We establish generalization bounds for memory
classifiers. We illustrate in experiments that they can improve generalization
and robustness to distribution shifts on image datasets. We show improvements
which push beyond standard data augmentation techniques.",2206.05323v2,https://arxiv.org/pdf/2206.05323v2
Is Self-Supervised Learning More Robust Than Supervised Learning?,"Yuanyi Zhong, Haoran Tang, Junkun Chen, Jian Peng, Yu-Xiong Wang","Self-supervised contrastive learning is a powerful tool to learn visual
representation without labels. Prior work has primarily focused on evaluating
the recognition accuracy of various pre-training algorithms, but has overlooked
other behavioral aspects. In addition to accuracy, distributional robustness
plays a critical role in the reliability of machine learning models. We design
and conduct a series of robustness tests to quantify the behavioral differences
between contrastive learning and supervised learning to downstream or
pre-training data distribution changes. These tests leverage data corruptions
at multiple levels, ranging from pixel-level gamma distortion to patch-level
shuffling and to dataset-level distribution shift. Our tests unveil intriguing
robustness behaviors of contrastive and supervised learning. On the one hand,
under downstream corruptions, we generally observe that contrastive learning is
surprisingly more robust than supervised learning. On the other hand, under
pre-training corruptions, we find contrastive learning vulnerable to patch
shuffling and pixel intensity change, yet less sensitive to dataset-level
distribution change. We attempt to explain these results through the role of
data augmentation and feature space properties. Our insight has implications in
improving the downstream robustness of supervised learning.",2206.05259v1,https://arxiv.org/pdf/2206.05259v1
Distributionally Robust End-to-End Portfolio Construction,"Giorgio Costa, Garud N. Iyengar","We propose an end-to-end distributionally robust system for portfolio
construction that integrates the asset return prediction model with a
distributionally robust portfolio optimization model. We also show how to learn
the risk-tolerance parameter and the degree of robustness directly from data.
End-to-end systems have an advantage in that information can be communicated
between the prediction and decision layers during training, allowing the
parameters to be trained for the final task rather than solely for predictive
performance. However, existing end-to-end systems are not able to quantify and
correct for the impact of model risk on the decision layer. Our proposed
distributionally robust end-to-end portfolio selection system explicitly
accounts for the impact of model risk. The decision layer chooses portfolios by
solving a minimax problem where the distribution of the asset returns is
assumed to belong to an ambiguity set centered around a nominal distribution.
Using convex duality, we recast the minimax problem in a form that allows for
efficient training of the end-to-end system.",2206.05134v1,https://arxiv.org/pdf/2206.05134v1
"Trimmed Maximum Likelihood Estimation for Robust Learning in Generalized
  Linear Models","Pranjal Awasthi, Abhimanyu Das, Weihao Kong, Rajat Sen","We study the problem of learning generalized linear models under adversarial
corruptions. We analyze a classical heuristic called the iterative trimmed
maximum likelihood estimator which is known to be effective against label
corruptions in practice. Under label corruptions, we prove that this simple
estimator achieves minimax near-optimal risk on a wide range of generalized
linear models, including Gaussian regression, Poisson regression and Binomial
regression. Finally, we extend the estimator to the more challenging setting of
label and covariate corruptions and demonstrate its robustness and optimality
in that setting as well.",2206.04777v3,https://arxiv.org/pdf/2206.04777v3
Data-Efficient Double-Win Lottery Tickets from Robust Pre-training,"Tianlong Chen, Zhenyu Zhang, Sijia Liu, Yang Zhang, Shiyu Chang, Zhangyang Wang","Pre-training serves as a broadly adopted starting point for transfer learning
on various downstream tasks. Recent investigations of lottery tickets
hypothesis (LTH) demonstrate such enormous pre-trained models can be replaced
by extremely sparse subnetworks (a.k.a. matching subnetworks) without
sacrificing transferability. However, practical security-crucial applications
usually pose more challenging requirements beyond standard transfer, which also
demand these subnetworks to overcome adversarial vulnerability. In this paper,
we formulate a more rigorous concept, Double-Win Lottery Tickets, in which a
located subnetwork from a pre-trained model can be independently transferred on
diverse downstream tasks, to reach BOTH the same standard and robust
generalization, under BOTH standard and adversarial training regimes, as the
full pre-trained model can do. We comprehensively examine various pre-training
mechanisms and find that robust pre-training tends to craft sparser double-win
lottery tickets with superior performance over the standard counterparts. For
example, on downstream CIFAR-10/100 datasets, we identify double-win matching
subnetworks with the standard, fast adversarial, and adversarial pre-training
from ImageNet, at 89.26%/73.79%, 89.26%/79.03%, and 91.41%/83.22% sparsity,
respectively. Furthermore, we observe the obtained double-win lottery tickets
can be more data-efficient to transfer, under practical data-limited (e.g., 1%
and 10%) downstream schemes. Our results show that the benefits from robust
pre-training are amplified by the lottery ticket scheme, as well as the
data-limited transfer setting. Codes are available at
https://github.com/VITA-Group/Double-Win-LTH.",2206.04762v1,https://arxiv.org/pdf/2206.04762v1
"Optimal SQ Lower Bounds for Robustly Learning Discrete Product
  Distributions and Ising Models","Ilias Diakonikolas, Daniel M. Kane, Yuxin Sun","We establish optimal Statistical Query (SQ) lower bounds for robustly
learning certain families of discrete high-dimensional distributions. In
particular, we show that no efficient SQ algorithm with access to an
$\epsilon$-corrupted binary product distribution can learn its mean within
$\ell_2$-error $o(\epsilon \sqrt{\log(1/\epsilon)})$. Similarly, we show that
no efficient SQ algorithm with access to an $\epsilon$-corrupted ferromagnetic
high-temperature Ising model can learn the model to total variation distance
$o(\epsilon \log(1/\epsilon))$. Our SQ lower bounds match the error guarantees
of known algorithms for these problems, providing evidence that current upper
bounds for these tasks are best possible. At the technical level, we develop a
generic SQ lower bound for discrete high-dimensional distributions starting
from low dimensional moment matching constructions that we believe will find
other applications. Additionally, we introduce new ideas to analyze these
moment-matching constructions for discrete univariate distributions.",2206.04589v1,https://arxiv.org/pdf/2206.04589v1
"AI-based Clinical Assessment of Optic Nerve Head Robustness Superseding
  Biomechanical Testing","Fabian A. Braeu, Thanadet Chuangsuwanich, Tin A. Tun, Alexandre H. Thiery, Tin Aung, George Barbastathis, Michaël J. A. Girard","$\mathbf{Purpose}$: To use artificial intelligence (AI) to: (1) exploit
biomechanical knowledge of the optic nerve head (ONH) from a relatively large
population; (2) assess ONH robustness from a single optical coherence
tomography (OCT) scan of the ONH; (3) identify what critical three-dimensional
(3D) structural features make a given ONH robust.
  $\mathbf{Design}$: Retrospective cross-sectional study.
  $\mathbf{Methods}$: 316 subjects had their ONHs imaged with OCT before and
after acute intraocular pressure (IOP) elevation through ophthalmo-dynamometry.
IOP-induced lamina-cribrosa deformations were then mapped in 3D and used to
classify ONHs. Those with LC deformations superior to 4% were considered
fragile, while those with deformations inferior to 4% robust. Learning from
these data, we compared three AI algorithms to predict ONH robustness strictly
from a baseline (undeformed) OCT volume: (1) a random forest classifier; (2) an
autoencoder; and (3) a dynamic graph CNN (DGCNN). The latter algorithm also
allowed us to identify what critical 3D structural features make a given ONH
robust.
  $\mathbf{Results}$: All 3 methods were able to predict ONH robustness from 3D
structural information alone and without the need to perform biomechanical
testing. The DGCNN (area under the receiver operating curve [AUC]: 0.76 $\pm$
0.08) outperformed the autoencoder (AUC: 0.70 $\pm$ 0.07) and the random forest
classifier (AUC: 0.69 $\pm$ 0.05). Interestingly, to assess ONH robustness, the
DGCNN mainly used information from the scleral canal and the LC insertion
sites.
  $\mathbf{Conclusions}$: We propose an AI-driven approach that can assess the
robustness of a given ONH solely from a single OCT scan of the ONH, and without
the need to perform biomechanical testing. Longitudinal studies should
establish whether ONH robustness could help us identify fast visual field loss
progressors.",2206.04689v1,https://arxiv.org/pdf/2206.04689v1
"GSmooth: Certified Robustness against Semantic Transformations via
  Generalized Randomized Smoothing","Zhongkai Hao, Chengyang Ying, Yinpeng Dong, Hang Su, Jun Zhu, Jian Song","Certified defenses such as randomized smoothing have shown promise towards
building reliable machine learning systems against $\ell_p$-norm bounded
attacks. However, existing methods are insufficient or unable to provably
defend against semantic transformations, especially those without closed-form
expressions (such as defocus blur and pixelate), which are more common in
practice and often unrestricted. To fill up this gap, we propose generalized
randomized smoothing (GSmooth), a unified theoretical framework for certifying
robustness against general semantic transformations via a novel dimension
augmentation strategy. Under the GSmooth framework, we present a scalable
algorithm that uses a surrogate image-to-image network to approximate the
complex transformation. The surrogate model provides a powerful tool for
studying the properties of semantic transformations and certifying robustness.
Experimental results on several datasets demonstrate the effectiveness of our
approach for robustness certification against multiple kinds of semantic
transformations and corruptions, which is not achievable by the alternative
baselines.",2206.04310v2,https://arxiv.org/pdf/2206.04310v2
Robust Matrix Completion with Heavy-tailed Noise,"Bingyan Wang, Jianqing Fan","This paper studies low-rank matrix completion in the presence of heavy-tailed
and possibly asymmetric noise, where we aim to estimate an underlying low-rank
matrix given a set of highly incomplete noisy entries. Though the matrix
completion problem has attracted much attention in the past decade, there is
still lack of theoretical understanding when the observations are contaminated
by heavy-tailed noises. Prior theory falls short of explaining the empirical
results and is unable to capture the optimal dependence of the estimation error
on the noise level. In this paper, we adopt an adaptive Huber loss to
accommodate heavy-tailed noise, which is robust against large and possibly
asymmetric errors when the parameter in the loss function is carefully designed
to balance the Huberization biases and robustness to outliers. Then, we propose
an efficient nonconvex algorithm via a balanced low-rank Burer-Monteiro matrix
factorization and gradient decent with robust spectral initialization. We prove
that under merely bounded second moment condition on the error distributions,
rather than the sub-Gaussian assumption, the Euclidean error of the iterates
generated by the proposed algorithm decrease geometrically fast until achieving
a minimax-optimal statistical estimation error, which has the same order as
that in the sub-Gaussian case. The key technique behind this significant
advancement is a powerful leave-one-out analysis framework. The theoretical
results are corroborated by our simulation studies.",2206.04276v1,https://arxiv.org/pdf/2206.04276v1
Robust Semantic Communications with Masked VQ-VAE Enabled Codebook,"Qiyu Hu, Guangyi Zhang, Zhijin Qin, Yunlong Cai, Guanding Yu, Geoffrey Ye Li","Although semantic communications have exhibited satisfactory performance for
a large number of tasks, the impact of semantic noise and the robustness of the
systems have not been well investigated. Semantic noise refers to the
misleading between the intended semantic symbols and received ones, thus cause
the failure of tasks. In this paper, we first propose a framework for the
robust end-to-end semantic communication systems to combat the semantic noise.
In particular, we analyze sample-dependent and sample-independent semantic
noise. To combat the semantic noise, the adversarial training with weight
perturbation is developed to incorporate the samples with semantic noise in the
training dataset. Then, we propose to mask a portion of the input, where the
semantic noise appears frequently, and design the masked vector
quantized-variational autoencoder (VQ-VAE) with the noise-related masking
strategy. We use a discrete codebook shared by the transmitter and the receiver
for encoded feature representation. To further improve the system robustness,
we develop a feature importance module (FIM) to suppress the noise-related and
task-unrelated features. Thus, the transmitter simply needs to transmit the
indices of these important task-related features in the codebook. Simulation
results show that the proposed method can be applied in many downstream tasks
and significantly improve the robustness against semantic noise with remarkable
reduction on the transmission overhead.",2206.04011v2,https://arxiv.org/pdf/2206.04011v2
Toward Certified Robustness Against Real-World Distribution Shifts,"Haoze Wu, Teruhiro Tagomori, Alexander Robey, Fengjun Yang, Nikolai Matni, George Pappas, Hamed Hassani, Corina Pasareanu, Clark Barrett","We consider the problem of certifying the robustness of deep neural networks
against real-world distribution shifts. To do so, we bridge the gap between
hand-crafted specifications and realistic deployment settings by proposing a
novel neural-symbolic verification framework, in which we train a generative
model to learn perturbations from data and define specifications with respect
to the output of the learned model. A unique challenge arising from this
setting is that existing verifiers cannot tightly approximate sigmoid
activations, which are fundamental to many state-of-the-art generative models.
To address this challenge, we propose a general meta-algorithm for handling
sigmoid activations which leverages classical notions of counter-example-guided
abstraction refinement. The key idea is to ""lazily"" refine the abstraction of
sigmoid functions to exclude spurious counter-examples found in the previous
abstraction, thus guaranteeing progress in the verification process while
keeping the state-space small. Experiments on the MNIST and CIFAR-10 datasets
show that our framework significantly outperforms existing methods on a range
of challenging distribution shifts.",2206.03669v3,https://arxiv.org/pdf/2206.03669v3
Certifying Data-Bias Robustness in Linear Regression,"Anna P. Meyer, Aws Albarghouthi, Loris D'Antoni","Datasets typically contain inaccuracies due to human error and societal
biases, and these inaccuracies can affect the outcomes of models trained on
such datasets. We present a technique for certifying whether linear regression
models are pointwise-robust to label bias in the training dataset, i.e.,
whether bounded perturbations to the labels of a training dataset result in
models that change the prediction of test points. We show how to solve this
problem exactly for individual test points, and provide an approximate but more
scalable method that does not require advance knowledge of the test point. We
extensively evaluate both techniques and find that linear models -- both
regression- and classification-based -- often display high levels of
bias-robustness. However, we also unearth gaps in bias-robustness, such as high
levels of non-robustness for certain bias assumptions on some datasets.
Overall, our approach can serve as a guide for when to trust, or question, a
model's output.",2206.03575v1,https://arxiv.org/pdf/2206.03575v1
Robust Sparse Mean Estimation via Sum of Squares,"Ilias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Ankit Pensia, Thanasis Pittas","We study the problem of high-dimensional sparse mean estimation in the
presence of an $\epsilon$-fraction of adversarial outliers. Prior work obtained
sample and computationally efficient algorithms for this task for
identity-covariance subgaussian distributions. In this work, we develop the
first efficient algorithms for robust sparse mean estimation without a priori
knowledge of the covariance. For distributions on $\mathbb R^d$ with
""certifiably bounded"" $t$-th moments and sufficiently light tails, our
algorithm achieves error of $O(\epsilon^{1-1/t})$ with sample complexity $m =
(k\log(d))^{O(t)}/\epsilon^{2-2/t}$. For the special case of the Gaussian
distribution, our algorithm achieves near-optimal error of $\tilde O(\epsilon)$
with sample complexity $m = O(k^4 \mathrm{polylog}(d))/\epsilon^2$. Our
algorithms follow the Sum-of-Squares based, proofs to algorithms approach. We
complement our upper bounds with Statistical Query and low-degree polynomial
testing lower bounds, providing evidence that the sample-time-error tradeoffs
achieved by our algorithms are qualitatively the best possible.",2206.03441v2,https://arxiv.org/pdf/2206.03441v2
Building Robust Ensembles via Margin Boosting,"Dinghuai Zhang, Hongyang Zhang, Aaron Courville, Yoshua Bengio, Pradeep Ravikumar, Arun Sai Suggala","In the context of adversarial robustness, a single model does not usually
have enough power to defend against all possible adversarial attacks, and as a
result, has sub-optimal robustness. Consequently, an emerging line of work has
focused on learning an ensemble of neural networks to defend against
adversarial attacks. In this work, we take a principled approach towards
building robust ensembles. We view this problem from the perspective of
margin-boosting and develop an algorithm for learning an ensemble with maximum
margin. Through extensive empirical evaluation on benchmark datasets, we show
that our algorithm not only outperforms existing ensembling techniques, but
also large models trained in an end-to-end fashion. An important byproduct of
our work is a margin-maximizing cross-entropy (MCE) loss, which is a better
alternative to the standard cross-entropy (CE) loss. Empirically, we show that
replacing the CE loss in state-of-the-art adversarial training techniques with
our MCE loss leads to significant performance improvement.",2206.03362v1,https://arxiv.org/pdf/2206.03362v1
"Improving Adversarial Robustness by Putting More Regularizations on Less
  Robust Samples","Dongyoon Yang, Insung Kong, Yongdai Kim","Adversarial training, which is to enhance robustness against adversarial
attacks, has received much attention because it is easy to generate
human-imperceptible perturbations of data to deceive a given deep neural
network. In this paper, we propose a new adversarial training algorithm that is
theoretically well motivated and empirically superior to other existing
algorithms. A novel feature of the proposed algorithm is to apply more
regularization to data vulnerable to adversarial attacks than other existing
regularization algorithms do. Theoretically, we show that our algorithm can be
understood as an algorithm of minimizing the regularized empirical risk
motivated from a newly derived upper bound of the robust risk. Numerical
experiments illustrate that our proposed algorithm improves the generalization
(accuracy on examples) and robustness (accuracy on adversarial attacks)
simultaneously to achieve the state-of-the-art performance.",2206.03353v4,https://arxiv.org/pdf/2206.03353v4
"CAISAR: A platform for Characterizing Artificial Intelligence Safety and
  Robustness","Julien Girard-Satabin, Michele Alberti, François Bobot, Zakaria Chihani, Augustin Lemesle","We present CAISAR, an open-source platform under active development for the
characterization of AI systems' robustness and safety. CAISAR provides a
unified entry point for defining verification problems by using WhyML, the
mature and expressive language of the Why3 verification platform. Moreover,
CAISAR orchestrates and composes state-of-the-art machine learning verification
tools which, individually, are not able to efficiently handle all problems but,
collectively, can cover a growing number of properties. Our aim is to assist,
on the one hand, the V\&V process by reducing the burden of choosing the
methodology tailored to a given verification problem, and on the other hand the
tools developers by factorizing useful features-visualization, report
generation, property description-in one platform. CAISAR will soon be available
at https://git.frama-c.com/pub/caisar.",2206.03044v2,https://arxiv.org/pdf/2206.03044v2
"Adaptive Weighted Nonnegative Matrix Factorization for Robust Feature
  Representation","Tingting Shen, Junhang Li, Can Tong, Qiang He, Chen Li, Yudong Yao, Yueyang Teng","Nonnegative matrix factorization (NMF) has been widely used to dimensionality
reduction in machine learning. However, the traditional NMF does not properly
handle outliers, so that it is sensitive to noise. In order to improve the
robustness of NMF, this paper proposes an adaptive weighted NMF, which
introduces weights to emphasize the different importance of each data point,
thus the algorithmic sensitivity to noisy data is decreased. It is very
different from the existing robust NMFs that use a slow growth similarity
measure. Specifically, two strategies are proposed to achieve this: fuzzier
weighted technique and entropy weighted regularized technique, and both of them
lead to an iterative solution with a simple form. Experimental results showed
that new methods have more robust feature representation on several real
datasets with noise than exsiting methods.",2206.03020v1,https://arxiv.org/pdf/2206.03020v1
"Robust Time Series Dissimilarity Measure for Outlier Detection and
  Periodicity Detection","Xiaomin Song, Qingsong Wen, Yan Li, Liang Sun","Dynamic time warping (DTW) is an effective dissimilarity measure in many time
series applications. Despite its popularity, it is prone to noises and
outliers, which leads to singularity problem and bias in the measurement. The
time complexity of DTW is quadratic to the length of time series, making it
inapplicable in real-time applications. In this paper, we propose a novel time
series dissimilarity measure named RobustDTW to reduce the effects of noises
and outliers. Specifically, the RobustDTW estimates the trend and optimizes the
time warp in an alternating manner by utilizing our designed temporal graph
trend filtering. To improve efficiency, we propose a multi-level framework that
estimates the trend and the warp function at a lower resolution, and then
repeatedly refines them at a higher resolution. Based on the proposed
RobustDTW, we further extend it to periodicity detection and outlier time
series detection. Experiments on real-world datasets demonstrate the superior
performance of RobustDTW compared to DTW variants in both outlier time series
detection and periodicity detection.",2206.02956v1,https://arxiv.org/pdf/2206.02956v1
"Memory-efficient model-based deep learning with convergence and
  robustness guarantees","Aniket Pramanik, M. Bridget Zimmerman, Mathews Jacob","Computational imaging has been revolutionized by compressed sensing
algorithms, which offer guaranteed uniqueness, convergence, and stability
properties. Model-based deep learning methods that combine imaging physics with
learned regularization priors have emerged as more powerful alternatives for
image recovery. The main focus of this paper is to introduce a memory efficient
model-based algorithm with similar theoretical guarantees as CS methods. The
proposed iterative algorithm alternates between a gradient descent involving
the score function and a conjugate gradient algorithm to encourage data
consistency. The score function is modeled as a monotone convolutional neural
network. Our analysis shows that the monotone constraint is necessary and
sufficient to enforce the uniqueness of the fixed point in arbitrary inverse
problems. In addition, it also guarantees the convergence to a fixed point,
which is robust to input perturbations. We introduce two implementations of the
proposed MOL framework, which differ in the way the monotone property is
imposed. The first approach enforces a strict monotone constraint, while the
second one relies on an approximation. The guarantees are not valid for the
second approach in the strict sense. However, our empirical studies show that
the convergence and robustness of both approaches are comparable, while the
less constrained approximate implementation offers better performance. The
proposed deep equilibrium formulation is significantly more memory efficient
than unrolled methods, which allows us to apply it to 3D or 2D+time problems
that current unrolled algorithms cannot handle.",2206.04797v4,https://arxiv.org/pdf/2206.04797v4
RORL: Robust Offline Reinforcement Learning via Conservative Smoothing,"Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, Lei Han","Offline reinforcement learning (RL) provides a promising direction to exploit
massive amount of offline data for complex decision-making tasks. Due to the
distribution shift issue, current offline RL algorithms are generally designed
to be conservative in value estimation and action selection. However, such
conservatism can impair the robustness of learned policies when encountering
observation deviation under realistic conditions, such as sensor errors and
adversarial attacks. To trade off robustness and conservatism, we propose
Robust Offline Reinforcement Learning (RORL) with a novel conservative
smoothing technique. In RORL, we explicitly introduce regularization on the
policy and the value function for states near the dataset, as well as
additional conservative value estimation on these states. Theoretically, we
show RORL enjoys a tighter suboptimality bound than recent theoretical results
in linear MDPs. We demonstrate that RORL can achieve state-of-the-art
performance on the general offline RL benchmark and is considerably robust to
adversarial observation perturbations.",2206.02829v3,https://arxiv.org/pdf/2206.02829v3
"Communication-constrained hypothesis testing: Optimality, robustness,
  and reverse data processing inequalities","Ankit Pensia, Varun Jog, Po-Ling Loh","We study hypothesis testing under communication constraints, where each
sample is quantized before being revealed to a statistician. Without
communication constraints, it is well known that the sample complexity of
simple binary hypothesis testing is characterized by the Hellinger distance
between the distributions. We show that the sample complexity of simple binary
hypothesis testing under communication constraints is at most a logarithmic
factor larger than in the unconstrained setting and this bound is tight. We
develop a polynomial-time algorithm that achieves the aforementioned sample
complexity. Our framework extends to robust hypothesis testing, where the
distributions are corrupted in the total variation distance. Our proofs rely on
a new reverse data processing inequality and a reverse Markov inequality, which
may be of independent interest. For simple $M$-ary hypothesis testing, the
sample complexity in the absence of communication constraints has a logarithmic
dependence on $M$. We show that communication constraints can cause an
exponential blow-up leading to $\Omega(M)$ sample complexity even for adaptive
algorithms.",2206.02765v2,https://arxiv.org/pdf/2206.02765v2
Robust Calibration with Multi-domain Temperature Scaling,"Yaodong Yu, Stephen Bates, Yi Ma, Michael I. Jordan","Uncertainty quantification is essential for the reliable deployment of
machine learning models to high-stakes application domains. Uncertainty
quantification is all the more challenging when training distribution and test
distribution are different, even the distribution shifts are mild. Despite the
ubiquity of distribution shifts in real-world applications, existing
uncertainty quantification approaches mainly study the in-distribution setting
where the train and test distributions are the same. In this paper, we develop
a systematic calibration model to handle distribution shifts by leveraging data
from multiple domains. Our proposed method -- multi-domain temperature scaling
-- uses the heterogeneity in the domains to improve calibration robustness
under distribution shift. Through experiments on three benchmark data sets, we
find our proposed method outperforms existing methods as measured on both
in-distribution and out-of-distribution test sets.",2206.02757v1,https://arxiv.org/pdf/2206.02757v1
"Robust and Fast Data-Driven Power System State Estimator Using Graph
  Neural Networks","Ognjen Kundacina, Mirsad Cosovic, Dejan Vukobratovic","The power system state estimation (SE) algorithm estimates the complex bus
voltages based on the available set of measurements. Because phasor measurement
units (PMUs) are becoming more widely employed in transmission power systems, a
fast SE solver capable of exploiting PMUs' high sample rates is required. To
accomplish this, we present a method for training a model based on graph neural
networks (GNNs) to learn estimates from PMU voltage and current measurements,
which, once it is trained, has a linear computational complexity with respect
to the number of nodes in the power system. We propose an original GNN
implementation over the power system's factor graph to simplify the
incorporation of various types and numbers of measurements both on power system
buses and branches. Furthermore, we augment the factor graph to improve the
robustness of GNN predictions. Training and test examples were generated by
randomly sampling sets of power system measurements and annotated with the
exact solutions of linear SE with PMUs. The numerical results demonstrate that
the GNN model provides an accurate approximation of the SE solutions.
Additionally, errors caused by PMU malfunctions or the communication failures
that make the SE problem unobservable have a local effect and do not
deteriorate the results in the rest of the power system.",2206.02731v1,https://arxiv.org/pdf/2206.02731v1
"Robust Adversarial Attacks Detection based on Explainable Deep
  Reinforcement Learning For UAV Guidance and Planning","Thomas Hickling, Nabil Aouf, Phillippa Spencer","The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agents
operating in public are increasing. Adopting AI-based techniques and, more
specifically, Deep Learning (DL) approaches to control and guide these UAVs can
be beneficial in terms of performance but can add concerns regarding the safety
of those techniques and their vulnerability against adversarial attacks.
Confusion in the agent's decision-making process caused by these attacks can
seriously affect the safety of the UAV. This paper proposes an innovative
approach based on the explainability of DL methods to build an efficient
detector that will protect these DL schemes and the UAVs adopting them from
attacks. The agent adopts a Deep Reinforcement Learning (DRL) scheme for
guidance and planning. The agent is trained with a Deep Deterministic Policy
Gradient (DDPG) with Prioritised Experience Replay (PER) DRL scheme that
utilises Artificial Potential Field (APF) to improve training times and
obstacle avoidance performance. A simulated environment for UAV explainable
DRL-based planning and guidance, including obstacles and adversarial attacks,
is built. The adversarial attacks are generated by the Basic Iterative Method
(BIM) algorithm and reduced obstacle course completion rates from 97\% to 35\%.
Two adversarial attack detectors are proposed to counter this reduction. The
first one is a Convolutional Neural Network Adversarial Detector (CNN-AD),
which achieves accuracy in the detection of 80\%. The second detector utilises
a Long Short Term Memory (LSTM) network. It achieves an accuracy of 91\% with
faster computing times compared to the CNN-AD, allowing for real-time
adversarial detection.",2206.02670v4,https://arxiv.org/pdf/2206.02670v4
Robust Pareto Set Identification with Contaminated Bandit Feedback,"Kerem Bozgan, Cem Tekin","We consider the Pareto set identification (PSI) problem in multi-objective
multi-armed bandits (MO-MAB) with contaminated reward observations. At each arm
pull, with some probability, the true reward samples are replaced with the
samples from an arbitrary contamination distribution chosen by the adversary.
We propose a median-based MO-MAB algorithm for robust PSI that abides by the
accuracy requirements set by the user via an accuracy parameter. We prove that
the sample complexity of this algorithm depends on the accuracy parameter
inverse squarely. We compare the proposed algorithm with a mean-based method
from MO-MAB literature on Gaussian reward distributions. Our numerical results
verify our theoretical expectations and show the necessity for robust algorithm
design in the adversarial setting.",2206.02666v1,https://arxiv.org/pdf/2206.02666v1
"Robust Fine-Tuning of Deep Neural Networks with Hessian-based
  Generalization Guarantees","Haotian Ju, Dongyue Li, Hongyang R. Zhang","We consider fine-tuning a pretrained deep neural network on a target task. We
study the generalization properties of fine-tuning to understand the problem of
overfitting, which has often been observed (e.g., when the target dataset is
small or when the training labels are noisy). Existing generalization measures
for deep networks depend on notions such as distance from the initialization
(i.e., the pretrained network) of the fine-tuned model and noise stability
properties of deep networks. This paper identifies a Hessian-based distance
measure through PAC-Bayesian analysis, which is shown to correlate well with
observed generalization gaps of fine-tuned models. Theoretically, we prove
Hessian distance-based generalization bounds for fine-tuned models. We also
describe an extended study of fine-tuning against label noise, where
overfitting remains a critical problem. We present an algorithm and a
generalization error guarantee for this algorithm under a class conditional
independent noise model. Empirically, we observe that the Hessian-based
distance measure can match the scale of the observed generalization gap of
fine-tuned models in practice. We also test our algorithm on several image
classification tasks with noisy training labels, showing gains over prior
methods and decreases in the Hessian distance measure of the fine-tuned model.",2206.02659v6,https://arxiv.org/pdf/2206.02659v6
Certified Robustness in Federated Learning,"Motasem Alfarra, Juan C. Pérez, Egor Shulgin, Peter Richtárik, Bernard Ghanem","Federated learning has recently gained significant attention and popularity
due to its effectiveness in training machine learning models on distributed
data privately. However, as in the single-node supervised learning setup,
models trained in federated learning suffer from vulnerability to imperceptible
input transformations known as adversarial attacks, questioning their
deployment in security-related applications. In this work, we study the
interplay between federated training, personalization, and certified
robustness. In particular, we deploy randomized smoothing, a widely-used and
scalable certification method, to certify deep networks trained on a federated
setup against input perturbations and transformations. We find that the simple
federated averaging technique is effective in building not only more accurate,
but also more certifiably-robust models, compared to training solely on local
data. We further analyze personalization, a popular technique in federated
training that increases the model's bias towards local data, on robustness. We
show several advantages of personalization over both~(that is, only training on
local data and federated training) in building more robust models with faster
training. Finally, we explore the robustness of mixtures of global and
local~(i.e. personalized) models, and find that the robustness of local models
degrades as they diverge from the global model",2206.02535v2,https://arxiv.org/pdf/2206.02535v2
AugLoss: A Robust Augmentation-based Fine Tuning Methodology,"Kyle Otstot, Andrew Yang, John Kevin Cava, Lalitha Sankar","Deep Learning (DL) models achieve great successes in many domains. However,
DL models increasingly face safety and robustness concerns, including noisy
labeling in the training stage and feature distribution shifts in the testing
stage. Previous works made significant progress in addressing these problems,
but the focus has largely been on developing solutions for only one problem at
a time. For example, recent work has argued for the use of tunable robust loss
functions to mitigate label noise, and data augmentation (e.g., AugMix) to
combat distribution shifts. As a step towards addressing both problems
simultaneously, we introduce AugLoss, a simple but effective methodology that
achieves robustness against both train-time noisy labeling and test-time
feature distribution shifts by unifying data augmentation and robust loss
functions. We conduct comprehensive experiments in varied settings of
real-world dataset corruption to showcase the gains achieved by AugLoss
compared to previous state-of-the-art methods. Lastly, we hope this work will
open new directions for designing more robust and reliable DL models under
real-world corruptions.",2206.02286v2,https://arxiv.org/pdf/2206.02286v2
"Vanilla Feature Distillation for Improving the Accuracy-Robustness
  Trade-Off in Adversarial Training","Guodong Cao, Zhibo Wang, Xiaowei Dong, Zhifei Zhang, Hengchang Guo, Zhan Qin, Kui Ren","Adversarial training has been widely explored for mitigating attacks against
deep models. However, most existing works are still trapped in the dilemma
between higher accuracy and stronger robustness since they tend to fit a model
towards robust features (not easily tampered with by adversaries) while
ignoring those non-robust but highly predictive features. To achieve a better
robustness-accuracy trade-off, we propose the Vanilla Feature Distillation
Adversarial Training (VFD-Adv), which conducts knowledge distillation from a
pre-trained model (optimized towards high accuracy) to guide adversarial
training towards higher accuracy, i.e., preserving those non-robust but
predictive features. More specifically, both adversarial examples and their
clean counterparts are forced to be aligned in the feature space by distilling
predictive representations from the pre-trained/clean model, while previous
works barely utilize predictive features from clean models. Therefore, the
adversarial training model is updated towards maximally preserving the accuracy
as gaining robustness. A key advantage of our method is that it can be
universally adapted to and boost existing works. Exhaustive experiments on
various datasets, classification models, and adversarial training algorithms
demonstrate the effectiveness of our proposed method.",2206.02158v1,https://arxiv.org/pdf/2206.02158v1
"Learning Robust Representations Of Generative Models Using Set-Based
  Artificial Fingerprints","Hae Jin Song, Wael AbdAlmageed","With recent progress in deep generative models, the problem of identifying
synthetic data and comparing their underlying generative processes has become
an imperative task for various reasons, including fighting visual
misinformation and source attribution. Existing methods often approximate the
distance between the models via their sample distributions. In this paper, we
approach the problem of fingerprinting generative models by learning
representations that encode the residual artifacts left by the generative
models as unique signals that identify the source models. We consider these
unique traces (a.k.a. ""artificial fingerprints"") as representations of
generative models, and demonstrate their usefulness in both the discriminative
task of source attribution and the unsupervised task of defining a similarity
between the underlying models. We first extend the existing studies on
fingerprints of GANs to four representative classes of generative models (VAEs,
Flows, GANs and score-based models), and demonstrate their existence and
attributability. We then improve the stability and attributability of the
fingerprints by proposing a new learning method based on set-encoding and
contrastive training. Our set-encoder, unlike existing methods that operate on
individual images, learns fingerprints from a \textit{set} of images. We
demonstrate improvements in the stability and attributability through
comparisons to state-of-the-art fingerprint methods and ablation studies.
Further, our method employs contrastive training to learn an implicit
similarity between models. We discover latent families of generative models
using this metric in a standard hierarchical clustering algorithm.",2206.02067v1,https://arxiv.org/pdf/2206.02067v1
MSR: Making Self-supervised learning Robust to Aggressive Augmentations,"Yingbin Bai, Erkun Yang, Zhaoqing Wang, Yuxuan Du, Bo Han, Cheng Deng, Dadong Wang, Tongliang Liu","Most recent self-supervised learning methods learn visual representation by
contrasting different augmented views of images. Compared with supervised
learning, more aggressive augmentations have been introduced to further improve
the diversity of training pairs. However, aggressive augmentations may distort
images' structures leading to a severe semantic shift problem that augmented
views of the same image may not share the same semantics, thus degrading the
transfer performance. To address this problem, we propose a new SSL paradigm,
which counteracts the impact of semantic shift by balancing the role of weak
and aggressively augmented pairs. Specifically, semantically inconsistent pairs
are of minority and we treat them as noisy pairs. Note that deep neural
networks (DNNs) have a crucial memorization effect that DNNs tend to first
memorize clean (majority) examples before overfitting to noisy (minority)
examples. Therefore, we set a relatively large weight for aggressively
augmented data pairs at the early learning stage. With the training going on,
the model begins to overfit noisy pairs. Accordingly, we gradually reduce the
weights of aggressively augmented pairs. In doing so, our method can better
embrace the aggressive augmentations and neutralize the semantic shift problem.
Experiments show that our model achieves 73.1% top-1 accuracy on ImageNet-1K
with ResNet-50 for 200 epochs, which is a 2.5% improvement over BYOL. Moreover,
experiments also demonstrate that the learned representations can transfer well
for various downstream tasks.",2206.01999v1,https://arxiv.org/pdf/2206.01999v1
"Robust Meta-learning with Sampling Noise and Label Noise via
  Eigen-Reptile","Dong Chen, Lingfei Wu, Siliang Tang, Xiao Yun, Bo Long, Yueting Zhuang","Recent years have seen a surge of interest in meta-learning techniques for
tackling the few-shot learning (FSL) problem. However, the meta-learner is
prone to overfitting since there are only a few available samples, which can be
identified as sampling noise on a clean dataset. Moreover, when handling the
data with noisy labels, the meta-learner could be extremely sensitive to label
noise on a corrupted dataset. To address these two challenges, we present
Eigen-Reptile (ER) that updates the meta-parameters with the main direction of
historical task-specific parameters to alleviate sampling and label noise.
Specifically, the main direction is computed in a fast way, where the scale of
the calculated matrix is related to the number of gradient steps instead of the
number of parameters. Furthermore, to obtain a more accurate main direction for
Eigen-Reptile in the presence of many noisy labels, we further propose
Introspective Self-paced Learning (ISPL). We have theoretically and
experimentally demonstrated the soundness and effectiveness of the proposed
Eigen-Reptile and ISPL. Particularly, our experiments on different tasks show
that the proposed method is able to outperform or achieve highly competitive
performance compared with other gradient-based methods with or without noisy
labels. The code and data for the proposed method are provided for research
purposes https://github.com/Anfeather/Eigen-Reptile.",2206.01944v1,https://arxiv.org/pdf/2206.01944v1
"Toward Learning Robust and Invariant Representations with Alignment
  Regularization and Data Augmentation","Haohan Wang, Zeyi Huang, Xindi Wu, Eric P. Xing","Data augmentation has been proven to be an effective technique for developing
machine learning models that are robust to known classes of distributional
shifts (e.g., rotations of images), and alignment regularization is a technique
often used together with data augmentation to further help the model learn
representations invariant to the shifts used to augment the data. In this
paper, motivated by a proliferation of options of alignment regularizations, we
seek to evaluate the performances of several popular design choices along the
dimensions of robustness and invariance, for which we introduce a new test
procedure. Our synthetic experiment results speak to the benefits of squared l2
norm regularization. Further, we also formally analyze the behavior of
alignment regularization to complement our empirical study under assumptions we
consider realistic. Finally, we test this simple technique we identify
(worst-case data augmentation with squared l2 norm alignment regularization)
and show that the benefits of this method outrun those of the specially
designed methods. We also release a software package in both TensorFlow and
PyTorch for users to use the method with a couple of lines at
https://github.com/jyanln/AlignReg.",2206.01909v1,https://arxiv.org/pdf/2206.01909v1
A Robust Backpropagation-Free Framework for Images,"Timothy Zee, Alexander G. Ororbia, Ankur Mali, Ifeoma Nwogu","While current deep learning algorithms have been successful for a wide
variety of artificial intelligence (AI) tasks, including those involving
structured image data, they present deep neurophysiological conceptual issues
due to their reliance on the gradients that are computed by backpropagation of
errors (backprop). Gradients are required to obtain synaptic weight adjustments
but require knowledge of feed-forward activities in order to conduct backward
propagation, a biologically implausible process. This is known as the ""weight
transport problem"". Therefore, in this work, we present a more biologically
plausible approach towards solving the weight transport problem for image data.
This approach, which we name the error kernel driven activation alignment
(EKDAA) algorithm, accomplishes through the introduction of locally derived
error transmission kernels and error maps. Like standard deep learning
networks, EKDAA performs the standard forward process via weights and
activation functions; however, its backward error computation involves adaptive
error kernels that propagate local error signals through the network. The
efficacy of EKDAA is demonstrated by performing visual-recognition tasks on the
Fashion MNIST, CIFAR-10 and SVHN benchmarks, along with demonstrating its
ability to extract visual features from natural color images. Furthermore, in
order to demonstrate its non-reliance on gradient computations, results are
presented for an EKDAA trained CNN that employs a non-differentiable activation
function.",2206.01820v2,https://arxiv.org/pdf/2206.01820v2
Robust Topological Inference in the Presence of Outliers,"Siddharth Vishwanath, Bharath K. Sriperumbudur, Kenji Fukumizu, Satoshi Kuriki","The distance function to a compact set plays a crucial role in the paradigm
of topological data analysis. In particular, the sublevel sets of the distance
function are used in the computation of persistent homology -- a backbone of
the topological data analysis pipeline. Despite its stability to perturbations
in the Hausdorff distance, persistent homology is highly sensitive to outliers.
In this work, we develop a framework of statistical inference for persistent
homology in the presence of outliers. Drawing inspiration from recent
developments in robust statistics, we propose a $\textit{median-of-means}$
variant of the distance function ($\textsf{MoM Dist}$), and establish its
statistical properties. In particular, we show that, even in the presence of
outliers, the sublevel filtrations and weighted filtrations induced by
$\textsf{MoM Dist}$ are both consistent estimators of the true underlying
population counterpart, and their rates of convergence in the bottleneck metric
are controlled by the fraction of outliers in the data. Finally, we demonstrate
the advantages of the proposed methodology through simulations and
applications.",2206.01795v1,https://arxiv.org/pdf/2206.01795v1
On the Generalization of Wasserstein Robust Federated Learning,"Tung-Anh Nguyen, Tuan Dung Nguyen, Long Tan Le, Canh T. Dinh, Nguyen H. Tran","In federated learning, participating clients typically possess non-i.i.d.
data, posing a significant challenge to generalization to unseen distributions.
To address this, we propose a Wasserstein distributionally robust optimization
scheme called WAFL. Leveraging its duality, we frame WAFL as an empirical
surrogate risk minimization problem, and solve it using a local SGD-based
algorithm with convergence guarantees. We show that the robustness of WAFL is
more general than related approaches, and the generalization bound is robust to
all adversarial distributions inside the Wasserstein ball (ambiguity set).
Since the center location and radius of the Wasserstein ball can be suitably
modified, WAFL shows its applicability not only in robustness but also in
domain adaptation. Through empirical evaluation, we demonstrate that WAFL
generalizes better than the vanilla FedAvg in non-i.i.d. settings, and is more
robust than other related methods in distribution shift settings. Further,
using benchmark datasets we show that WAFL is capable of generalizing to unseen
target domains.",2206.01432v1,https://arxiv.org/pdf/2206.01432v1
"Adaptive Adversarial Training to Improve Adversarial Robustness of DNNs
  for Medical Image Segmentation and Detection","Linhai Ma, Liang Liang","It is known that Deep Neural networks (DNNs) are vulnerable to adversarial
attacks, and the adversarial robustness of DNNs could be improved by adding
adversarial noises to training data (e.g., the standard adversarial training
(SAT)). However, inappropriate noises added to training data may reduce a
model's performance, which is termed the trade-off between accuracy and
robustness. This problem has been sufficiently studied for the classification
of whole images but has rarely been explored for image analysis tasks in the
medical application domain, including image segmentation, landmark detection,
and object detection tasks. In this study, we show that, for those medical
image analysis tasks, the SAT method has a severe issue that limits its
practical use: it generates a fixed and unified level of noise for all training
samples for robust DNN training. A high noise level may lead to a large
reduction in model performance and a low noise level may not be effective in
improving robustness. To resolve this issue, we design an adaptive-margin
adversarial training (AMAT) method that generates sample-wise adaptive
adversarial noises for robust DNN training. In contrast to the existing,
classification-oriented adversarial training methods, our AMAT method uses a
loss-defined-margin strategy so that it can be applied to different tasks as
long as the loss functions are well-defined. We successfully apply our AMAT
method to state-of-the-art DNNs, using five publicly available datasets. The
experimental results demonstrate that: (1) our AMAT method can be applied to
the three seemingly different tasks in the medical image application domain;
(2) AMAT outperforms the SAT method in adversarial robustness; (3) AMAT has a
minimal reduction in prediction accuracy on clean data, compared with the SAT
method; and (4) AMAT has almost the same training time cost as SAT.",2206.01736v2,https://arxiv.org/pdf/2206.01736v2
"Robustness to Label Noise Depends on the Shape of the Noise Distribution
  in Feature Space","Diane Oyen, Michal Kucer, Nick Hengartner, Har Simrat Singh","Machine learning classifiers have been demonstrated, both empirically and
theoretically, to be robust to label noise under certain conditions -- notably
the typical assumption is that label noise is independent of the features given
the class label. We provide a theoretical framework that generalizes beyond
this typical assumption by modeling label noise as a distribution over feature
space. We show that both the scale and the shape of the noise distribution
influence the posterior likelihood; and the shape of the noise distribution has
a stronger impact on classification performance if the noise is concentrated in
feature space where the decision boundary can be moved. For the special case of
uniform label noise (independent of features and the class label), we show that
the Bayes optimal classifier for $c$ classes is robust to label noise until the
ratio of noisy samples goes above $\frac{c-1}{c}$ (e.g. 90% for 10 classes),
which we call the tipping point. However, for the special case of
class-dependent label noise (independent of features given the class label),
the tipping point can be as low as 50%. Most importantly, we show that when the
noise distribution targets decision boundaries (label noise is directly
dependent on feature space), classification robustness can drop off even at a
small scale of noise. Even when evaluating recent label-noise mitigation
methods we see reduced accuracy when label noise is dependent on features.
These findings explain why machine learning often handles label noise well if
the noise distribution is uniform in feature-space; yet it also points to the
difficulty of overcoming label noise when it is concentrated in a region of
feature space where a decision boundary can move.",2206.01106v1,https://arxiv.org/pdf/2206.01106v1
Adversarial Laser Spot: Robust and Covert Physical-World Attack to DNNs,"Chengyin Hu, Yilong Wang, Kalibinuer Tiliwalidi, Wen Li","Most existing deep neural networks (DNNs) are easily disturbed by slight
noise. However, there are few researches on physical attacks by deploying
lighting equipment. The light-based physical attacks has excellent covertness,
which brings great security risks to many vision-based applications (such as
self-driving). Therefore, we propose a light-based physical attack, called
adversarial laser spot (AdvLS), which optimizes the physical parameters of
laser spots through genetic algorithm to perform physical attacks. It realizes
robust and covert physical attack by using low-cost laser equipment. As far as
we know, AdvLS is the first light-based physical attack that perform physical
attacks in the daytime. A large number of experiments in the digital and
physical environments show that AdvLS has excellent robustness and covertness.
In addition, through in-depth analysis of the experimental data, we find that
the adversarial perturbations generated by AdvLS have superior adversarial
attack migration. The experimental results show that AdvLS impose serious
interference to advanced DNNs, we call for the attention of the proposed AdvLS.
The code of AdvLS is available at: https://github.com/ChengYinHu/AdvLS",2206.01034v2,https://arxiv.org/pdf/2206.01034v2
"Improving the Robustness and Generalization of Deep Neural Network with
  Confidence Threshold Reduction","Xiangyuan Yang, Jie Lin, Hanlin Zhang, Xinyu Yang, Peng Zhao","Deep neural networks are easily attacked by imperceptible perturbation.
Presently, adversarial training (AT) is the most effective method to enhance
the robustness of the model against adversarial examples. However, because
adversarial training solved a min-max value problem, in comparison with natural
training, the robustness and generalization are contradictory, i.e., the
robustness improvement of the model will decrease the generalization of the
model. To address this issue, in this paper, a new concept, namely confidence
threshold (CT), is introduced and the reducing of the confidence threshold,
known as confidence threshold reduction (CTR), is proven to improve both the
generalization and robustness of the model. Specifically, to reduce the CT for
natural training (i.e., for natural training with CTR), we propose a
mask-guided divergence loss function (MDL) consisting of a cross-entropy loss
term and an orthogonal term. The empirical and theoretical analysis
demonstrates that the MDL loss improves the robustness and generalization of
the model simultaneously for natural training. However, the model robustness
improvement of natural training with CTR is not comparable to that of
adversarial training. Therefore, for adversarial training, we propose a
standard deviation loss function (STD), which minimizes the difference in the
probabilities of the wrong categories, to reduce the CT by being integrated
into the loss function of adversarial training. The empirical and theoretical
analysis demonstrates that the STD based loss function can further improve the
robustness of the adversarially trained model on basis of guaranteeing the
changeless or slight improvement of the natural accuracy.",2206.00913v2,https://arxiv.org/pdf/2206.00913v2
"Robustness Evaluation and Adversarial Training of an Instance
  Segmentation Model","Jacob Bond, Andrew Lingg","To evaluate the robustness of non-classifier models, we propose probabilistic
local equivalence, based on the notion of randomized smoothing, as a way to
quantitatively evaluate the robustness of an arbitrary function. In addition,
to understand the effect of adversarial training on non-classifiers and to
investigate the level of robustness that can be obtained without degrading
performance on the training distribution, we apply Fast is Better than Free
adversarial training together with the TRADES robust loss to the training of an
instance segmentation network. In this direction, we were able to achieve a
symmetric best dice score of 0.85 on the TuSimple lane detection challenge,
outperforming the standardly-trained network's score of 0.82. Additionally, we
were able to obtain an F-measure of 0.49 on manipulated inputs, in contrast to
the standardly-trained network's score of 0. We show that probabilisitic local
equivalence is able to successfully distinguish between standardly-trained and
adversarially-trained models, providing another view of the improved robustness
of the adversarially-trained models.",2206.02539v1,https://arxiv.org/pdf/2206.02539v1
"Applied Federated Learning: Architectural Design for Robust and
  Efficient Learning in Privacy Aware Settings","Branislav Stojkovic, Jonathan Woodbridge, Zhihan Fang, Jerry Cai, Andrey Petrov, Sathya Iyer, Daoyu Huang, Patrick Yau, Arvind Sastha Kumar, Hitesh Jawa, Anamita Guha","The classical machine learning paradigm requires the aggregation of user data
in a central location where machine learning practitioners can preprocess data,
calculate features, tune models and evaluate performance. The advantage of this
approach includes leveraging high performance hardware (such as GPUs) and the
ability of machine learning practitioners to do in depth data analysis to
improve model performance. However, these advantages may come at a cost to data
privacy. User data is collected, aggregated, and stored on centralized servers
for model development. Centralization of data poses risks, including a
heightened risk of internal and external security incidents as well as
accidental data misuse. Federated learning with differential privacy is
designed to avoid the server-side centralization pitfall by bringing the ML
learning step to users' devices. Learning is done in a federated manner where
each mobile device runs a training loop on a local copy of a model. Updates
from on-device models are sent to the server via encrypted communication and
through differential privacy to improve the global model. In this paradigm,
users' personal data remains on their devices. Surprisingly, model training in
this manner comes at a fairly minimal degradation in model performance.
However, federated learning comes with many other challenges due to its
distributed nature, heterogeneous compute environments and lack of data
visibility. This paper explores those challenges and outlines an architectural
design solution we are exploring and testing to productionize federated
learning at Meta scale.",2206.00807v2,https://arxiv.org/pdf/2206.00807v2
"RoCourseNet: Distributionally Robust Training of a Prediction Aware
  Recourse Model","Hangzhi Guo, Feiran Jia, Jinghui Chen, Anna Squicciarini, Amulya Yadav","Counterfactual (CF) explanations for machine learning (ML) models are
preferred by end-users, as they explain the predictions of ML models by
providing a recourse (or contrastive) case to individuals who are adversely
impacted by predicted outcomes. Existing CF explanation methods generate
recourses under the assumption that the underlying target ML model remains
stationary over time. However, due to commonly occurring distributional shifts
in training data, ML models constantly get updated in practice, which might
render previously generated recourses invalid and diminish end-users trust in
our algorithmic framework. To address this problem, we propose RoCourseNet, a
training framework that jointly optimizes predictions and recourses that are
robust to future data shifts. This work contains four key contributions: (1) We
formulate the robust recourse generation problem as a tri-level optimization
problem which consists of two sub-problems: (i) a bi-level problem that finds
the worst-case adversarial shift in the training data, and (ii) an outer
minimization problem to generate robust recourses against this worst-case
shift. (2) We leverage adversarial training to solve this tri-level
optimization problem by: (i) proposing a novel virtual data shift (VDS)
algorithm to find worst-case shifted ML models via explicitly considering the
worst-case data shift in the training dataset, and (ii) a block-wise coordinate
descent procedure to optimize for prediction and corresponding robust
recourses. (3) We evaluate RoCourseNet's performance on three real-world
datasets, and show that RoCourseNet consistently achieves more than 96% robust
validity and outperforms state-of-the-art baselines by at least 10% in
generating robust CF explanations. (4) Finally, we generalize the RoCourseNet
framework to accommodate any parametric post-hoc methods for improving robust
validity.",2206.00700v2,https://arxiv.org/pdf/2206.00700v2
The robust way to stack and bag: the local Lipschitz way,"Thulasi Tholeti, Sheetal Kalyani","Recent research has established that the local Lipschitz constant of a neural
network directly influences its adversarial robustness. We exploit this
relationship to construct an ensemble of neural networks which not only
improves the accuracy, but also provides increased adversarial robustness. The
local Lipschitz constants for two different ensemble methods - bagging and
stacking - are derived and the architectures best suited for ensuring
adversarial robustness are deduced. The proposed ensemble architectures are
tested on MNIST and CIFAR-10 datasets in the presence of white-box attacks,
FGSM and PGD. The proposed architecture is found to be more robust than a) a
single network and b) traditional ensemble methods.",2206.00513v1,https://arxiv.org/pdf/2206.00513v1
In the Eye of the Beholder: Robust Prediction with Causal User Modeling,"Amir Feder, Guy Horowitz, Yoav Wald, Roi Reichart, Nir Rosenfeld","Accurately predicting the relevance of items to users is crucial to the
success of many social platforms. Conventional approaches train models on
logged historical data; but recommendation systems, media services, and online
marketplaces all exhibit a constant influx of new content -- making relevancy a
moving target, to which standard predictive models are not robust. In this
paper, we propose a learning framework for relevance prediction that is robust
to changes in the data distribution. Our key observation is that robustness can
be obtained by accounting for how users causally perceive the environment. We
model users as boundedly-rational decision makers whose causal beliefs are
encoded by a causal graph, and show how minimal information regarding the graph
can be used to contend with distributional changes. Experiments in multiple
settings demonstrate the effectiveness of our approach.",2206.00416v2,https://arxiv.org/pdf/2206.00416v2
On the Perils of Cascading Robust Classifiers,"Ravi Mangal, Zifan Wang, Chi Zhang, Klas Leino, Corina Pasareanu, Matt Fredrikson","Ensembling certifiably robust neural networks is a promising approach for
improving the \emph{certified robust accuracy} of neural models. Black-box
ensembles that assume only query-access to the constituent models (and their
robustness certifiers) during prediction are particularly attractive due to
their modular structure. Cascading ensembles are a popular instance of
black-box ensembles that appear to improve certified robust accuracies in
practice. However, we show that the robustness certifier used by a cascading
ensemble is unsound. That is, when a cascading ensemble is certified as locally
robust at an input $x$ (with respect to $\epsilon$), there can be inputs $x'$
in the $\epsilon$-ball centered at $x$, such that the cascade's prediction at
$x'$ is different from $x$ and thus the ensemble is not locally robust. Our
theoretical findings are accompanied by empirical results that further
demonstrate this unsoundness. We present \emph{cascade attack} (CasA), an
adversarial attack against cascading ensembles, and show that: (1) there exists
an adversarial input for up to 88\% of the samples where the ensemble claims to
be certifiably robust and accurate; and (2) the accuracy of a cascading
ensemble under our attack is as low as 11\% when it claims to be certifiably
robust and accurate on 97\% of the test set. Our work reveals a critical
pitfall of cascading certifiably robust models by showing that the seemingly
beneficial strategy of cascading can actually hurt the robustness of the
resulting ensemble. Our code is available at
\url{https://github.com/TristaChi/ensembleKW}.",2206.00278v2,https://arxiv.org/pdf/2206.00278v2
Byzantine-Robust Online and Offline Distributed Reinforcement Learning,"Yiding Chen, Xuezhou Zhang, Kaiqing Zhang, Mengdi Wang, Xiaojin Zhu","We consider a distributed reinforcement learning setting where multiple
agents separately explore the environment and communicate their experiences
through a central server. However, $\alpha$-fraction of agents are adversarial
and can report arbitrary fake information. Critically, these adversarial agents
can collude and their fake data can be of any sizes. We desire to robustly
identify a near-optimal policy for the underlying Markov decision process in
the presence of these adversarial agents. Our main technical contribution is
Weighted-Clique, a novel algorithm for the robust mean estimation from batches
problem, that can handle arbitrary batch sizes. Building upon this new
estimator, in the offline setting, we design a Byzantine-robust distributed
pessimistic value iteration algorithm; in the online setting, we design a
Byzantine-robust distributed optimistic value iteration algorithm. Both
algorithms obtain near-optimal sample complexities and achieve superior
robustness guarantee than prior works.",2206.00165v1,https://arxiv.org/pdf/2206.00165v1
PAGER: Progressive Attribute-Guided Extendable Robust Image Generation,"Zohreh Azizi, C. -C. Jay Kuo","This work presents a generative modeling approach based on successive
subspace learning (SSL). Unlike most generative models in the literature, our
method does not utilize neural networks to analyze the underlying source
distribution and synthesize images. The resulting method, called the
progressive attribute-guided extendable robust image generative (PAGER) model,
has advantages in mathematical transparency, progressive content generation,
lower training time, robust performance with fewer training samples, and
extendibility to conditional image generation. PAGER consists of three modules:
core generator, resolution enhancer, and quality booster. The core generator
learns the distribution of low-resolution images and performs unconditional
image generation. The resolution enhancer increases image resolution via
conditional generation. Finally, the quality booster adds finer details to
generated images. Extensive experiments on MNIST, Fashion-MNIST, and CelebA
datasets are conducted to demonstrate generative performance of PAGER.",2206.00162v2,https://arxiv.org/pdf/2206.00162v2
"Robust Longitudinal Control for Vehicular Autonomous Platoons Using Deep
  Reinforcement Learning","Armando Alves Neto, Leonardo Amaral Mozelli","In the last few years, researchers have applied machine learning strategies
in the context of vehicular platoons to increase the safety and efficiency of
cooperative transportation. Reinforcement Learning methods have been employed
in the longitudinal spacing control of Cooperative Adaptive Cruise Control
systems, but to date, none of those studies have addressed problems of
disturbance rejection in such scenarios. Characteristics such as uncertain
parameters in the model and external interferences may prevent agents from
reaching null-spacing errors when traveling at cruising speed. On the other
hand, complex communication topologies lead to specific training processes that
can not be generalized to other contexts, demanding re-training every time the
configuration changes. Therefore, in this paper, we propose an approach to
generalize the training process of a vehicular platoon, such that the
acceleration command of each agent becomes independent of the network topology.
Also, we have modeled the acceleration input as a term with integral action,
such that the Artificial Neural Network is capable of learning corrective
actions when the states are disturbed by unknown effects. We illustrate the
effectiveness of our proposal with experiments using different network
topologies, uncertain parameters, and external forces. Comparative analyses, in
terms of the steady-state error and overshoot response, were conducted against
the state-of-the-art literature. The findings offer new insights concerning
generalization and robustness of using Reinforcement Learning in the control of
autonomous platoons.",2206.01175v2,https://arxiv.org/pdf/2206.01175v2
Evaluating Robustness to Dataset Shift via Parametric Robustness Sets,"Nikolaj Thams, Michael Oberst, David Sontag","We give a method for proactively identifying small, plausible shifts in
distribution which lead to large differences in model performance. These shifts
are defined via parametric changes in the causal mechanisms of observed
variables, where constraints on parameters yield a ""robustness set"" of
plausible distributions and a corresponding worst-case loss over the set. While
the loss under an individual parametric shift can be estimated via reweighting
techniques such as importance sampling, the resulting worst-case optimization
problem is non-convex, and the estimate may suffer from large variance. For
small shifts, however, we can construct a local second-order approximation to
the loss under shift and cast the problem of finding a worst-case shift as a
particular non-convex quadratic optimization problem, for which efficient
algorithms are available. We demonstrate that this second-order approximation
can be estimated directly for shifts in conditional exponential family models,
and we bound the approximation error. We apply our approach to a computer
vision task (classifying gender from images), revealing sensitivity to shifts
in non-causal attributes.",2205.15947v4,https://arxiv.org/pdf/2205.15947v4
"A robust and lightweight deep attention multiple instance learning
  algorithm for predicting genetic alterations","Bangwei Guo, Xingyu Li, Miaomiao Yang, Hong Zhang, Xu Steven Xu","Deep-learning models based on whole-slide digital pathology images (WSIs)
become increasingly popular for predicting molecular biomarkers. Instance-based
models has been the mainstream strategy for predicting genetic alterations
using WSIs although bag-based models along with self-attention mechanism-based
algorithms have been proposed for other digital pathology applications. In this
paper, we proposed a novel Attention-based Multiple Instance Mutation Learning
(AMIML) model for predicting gene mutations. AMIML was comprised of successive
1-D convolutional layers, a decoder, and a residual weight connection to
facilitate further integration of a lightweight attention mechanism to detect
the most predictive image patches. Using data for 24 clinically relevant genes
from four cancer cohorts in The Cancer Genome Atlas (TCGA) studies (UCEC, BRCA,
GBM and KIRC), we compared AMIML with one popular instance-based model and four
recently published bag-based models (e.g., CHOWDER, HE2RNA, etc.). AMIML
demonstrated excellent robustness, not only outperforming all the five baseline
algorithms in the vast majority of the tested genes (17 out of 24), but also
providing near-best-performance for the other seven genes. Conversely, the
performance of the baseline published algorithms varied across different
cancers/genes. In addition, compared to the published models for genetic
alterations, AMIML provided a significant improvement for predicting a wide
range of genes (e.g., KMT2C, TP53, and SETD2 for KIRC; ERBB2, BRCA1, and BRCA2
for BRCA; JAK1, POLE, and MTOR for UCEC) as well as produced outstanding
predictive models for other clinically relevant gene mutations, which have not
been reported in the current literature. Furthermore, with the flexible and
interpretable attention-based MIL pooling mechanism, AMIML could further
zero-in and detect predictive image patches.",2206.00455v1,https://arxiv.org/pdf/2206.00455v1
Attribution-based Explanations that Provide Recourse Cannot be Robust,"Hidde Fokkema, Rianne de Heide, Tim van Erven","Different users of machine learning methods require different explanations,
depending on their goals. To make machine learning accountable to society, one
important goal is to get actionable options for recourse, which allow an
affected user to change the decision $f(x)$ of a machine learning system by
making limited changes to its input $x$. We formalize this by providing a
general definition of recourse sensitivity, which needs to be instantiated with
a utility function that describes which changes to the decisions are relevant
to the user. This definition applies to local attribution methods, which
attribute an importance weight to each input feature. It is often argued that
such local attributions should be robust, in the sense that a small change in
the input $x$ that is being explained, should not cause a large change in the
feature weights. However, we prove formally that it is in general impossible
for any single attribution method to be both recourse sensitive and robust at
the same time. It follows that there must always exist counterexamples to at
least one of these properties. We provide such counterexamples for several
popular attribution methods, including LIME, SHAP, Integrated Gradients and
SmoothGrad. Our results also cover counterfactual explanations, which may be
viewed as attributions that describe a perturbation of $x$. We further discuss
possible ways to work around our impossibility result, for instance by allowing
the output to consist of sets with multiple attributions, and we provide
sufficient conditions for specific classes of continuous functions to be
recourse sensitive. Finally, we strengthen our impossibility result for the
restricted case where users are only able to change a single attribute of $x$,
by providing an exact characterization of the functions $f$ to which
impossibility applies.",2205.15834v3,https://arxiv.org/pdf/2205.15834v3
Robust Anytime Learning of Markov Decision Processes,"Marnix Suilen, Thiago D. Simão, David Parker, Nils Jansen","Markov decision processes (MDPs) are formal models commonly used in
sequential decision-making. MDPs capture the stochasticity that may arise, for
instance, from imprecise actuators via probabilities in the transition
function. However, in data-driven applications, deriving precise probabilities
from (limited) data introduces statistical errors that may lead to unexpected
or undesirable outcomes. Uncertain MDPs (uMDPs) do not require precise
probabilities but instead use so-called uncertainty sets in the transitions,
accounting for such limited data. Tools from the formal verification community
efficiently compute robust policies that provably adhere to formal
specifications, like safety constraints, under the worst-case instance in the
uncertainty set. We continuously learn the transition probabilities of an MDP
in a robust anytime-learning approach that combines a dedicated Bayesian
inference scheme with the computation of robust policies. In particular, our
method (1) approximates probabilities as intervals, (2) adapts to new data that
may be inconsistent with an intermediate model, and (3) may be stopped at any
time to compute a robust policy on the uMDP that faithfully captures the data
so far. Furthermore, our method is capable of adapting to changes in the
environment. We show the effectiveness of our approach and compare it to robust
policies computed on uMDPs learned by the UCRL2 reinforcement learning
algorithm in an experimental evaluation on several benchmarks.",2205.15827v4,https://arxiv.org/pdf/2205.15827v4
"Scalable Distributional Robustness in a Class of Non Convex Optimization
  with Guarantees","Avinandan Bose, Arunesh Sinha, Tien Mai","Distributionally robust optimization (DRO) has shown lot of promise in
providing robustness in learning as well as sample based optimization problems.
We endeavor to provide DRO solutions for a class of sum of fractionals,
non-convex optimization which is used for decision making in prominent areas
such as facility location and security games. In contrast to previous work, we
find it more tractable to optimize the equivalent variance regularized form of
DRO rather than the minimax form. We transform the variance regularized form to
a mixed-integer second order cone program (MISOCP), which, while guaranteeing
near global optimality, does not scale enough to solve problems with real world
data-sets. We further propose two abstraction approaches based on clustering
and stratified sampling to increase scalability, which we then use for real
world data-sets. Importantly, we provide near global optimality guarantees for
our approach and show experimentally that our solution quality is better than
the locally optimal ones achieved by state-of-the-art gradient-based methods.
We experimentally compare our different approaches and baselines, and reveal
nuanced properties of a DRO solution.",2205.15624v1,https://arxiv.org/pdf/2205.15624v1
Communication-Efficient Distributionally Robust Decentralized Learning,"Matteo Zecchin, Marios Kountouris, David Gesbert","Decentralized learning algorithms empower interconnected devices to share
data and computational resources to collaboratively train a machine learning
model without the aid of a central coordinator. In the case of heterogeneous
data distributions at the network nodes, collaboration can yield predictors
with unsatisfactory performance for a subset of the devices. For this reason,
in this work, we consider the formulation of a distributionally robust
decentralized learning task and we propose a decentralized single loop gradient
descent/ascent algorithm (AD-GDA) to directly solve the underlying minimax
optimization problem. We render our algorithm communication-efficient by
employing a compressed consensus scheme and we provide convergence guarantees
for smooth convex and non-convex loss functions. Finally, we corroborate the
theoretical findings with empirical results that highlight AD-GDA's ability to
provide unbiased predictors and to greatly improve communication efficiency
compared to existing distributionally robust algorithms.",2205.15614v3,https://arxiv.org/pdf/2205.15614v3
"HW-Aware Initialization of DNN Auto-Tuning to Improve Exploration Time
  and Robustness","Dennis Rieber, Moritz Reiber, Oliver Bringmann, Holger Fröning","The process of optimizing the latency of DNN operators with ML models and
hardware-in-the-loop, called auto-tuning, has established itself as a pervasive
method for the deployment of neural networks. From a search space of
loop-optimizations, the candidate providing the best performance has to be
selected. Performance of individual configurations is evaluated through
hardware measurements. The combinatorial explosion of possible configurations,
together with the cost of hardware evaluation makes exhaustive explorations of
the search space infeasible in practice. Machine Learning methods, like random
forests or reinforcement learning are used to aid in the selection of
candidates for hardware evaluation. For general purpose hardware like x86 and
GPGPU architectures impressive performance gains can be achieved, compared to
hand-optimized libraries like cuDNN. The method is also useful in the space of
hardware accelerators with less wide-spread adoption, where a high-performance
library is not always available. However, hardware accelerators are often less
flexible with respect to their programming which leads to operator
configurations not executable on the hardware target. This work evaluates how
these invalid configurations affect the auto-tuning process and its underlying
performance prediction model for the VTA hardware. From these results, a
validity-driven initialization method for AutoTVM is developed, only requiring
41.6% of the necessary hardware measurements to find the best solution, while
improving search robustness.",2205.15568v1,https://arxiv.org/pdf/2205.15568v1
"Robust Projection based Anomaly Extraction (RPE) in Univariate
  Time-Series","Mostafa Rahmani, Anoop Deoras, Laurent Callot","This paper presents a novel, closed-form, and data/computation efficient
online anomaly detection algorithm for time-series data. The proposed method,
dubbed RPE, is a window-based method and in sharp contrast to the existing
window-based methods, it is robust to the presence of anomalies in its window
and it can distinguish the anomalies in time-stamp level. RPE leverages the
linear structure of the trajectory matrix of the time-series and employs a
robust projection step which makes the algorithm able to handle the presence of
multiple arbitrarily large anomalies in its window. A closed-form/non-iterative
algorithm for the robust projection step is provided and it is proved that it
can identify the corrupted time-stamps. RPE is a great candidate for the
applications where a large training data is not available which is the common
scenario in the area of time-series. An extensive set of numerical experiments
show that RPE can outperform the existing approaches with a notable margin.",2205.15548v1,https://arxiv.org/pdf/2205.15548v1
Data Banzhaf: A Robust Data Valuation Framework for Machine Learning,"Jiachen T. Wang, Ruoxi Jia","Data valuation has wide use cases in machine learning, including improving
data quality and creating economic incentives for data sharing. This paper
studies the robustness of data valuation to noisy model performance scores.
Particularly, we find that the inherent randomness of the widely used
stochastic gradient descent can cause existing data value notions (e.g., the
Shapley value and the Leave-one-out error) to produce inconsistent data value
rankings across different runs. To address this challenge, we introduce the
concept of safety margin, which measures the robustness of a data value notion.
We show that the Banzhaf value, a famous value notion that originated from
cooperative game theory literature, achieves the largest safety margin among
all semivalues (a class of value notions that satisfy crucial properties
entailed by ML applications and include the famous Shapley value and
Leave-one-out error). We propose an algorithm to efficiently estimate the
Banzhaf value based on the Maximum Sample Reuse (MSR) principle. Our evaluation
demonstrates that the Banzhaf value outperforms the existing semivalue-based
data value notions on several ML tasks such as learning with weighted samples
and noisy label detection. Overall, our study suggests that when the underlying
ML algorithm is stochastic, the Banzhaf value is a promising alternative to the
other semivalue-based data value schemes given its computational advantage and
ability to robustly differentiate data quality.",2205.15466v7,https://arxiv.org/pdf/2205.15466v7
"Level Up with RealAEs: Leveraging Domain Constraints in Feature Space to
  Strengthen Robustness of Android Malware Detection","Hamid Bostani, Zhengyu Zhao, Zhuoran Liu, Veelasha Moonsamy","The vulnerability to adversarial examples remains one major obstacle for
Machine Learning (ML)-based Android malware detection. Realistic attacks in the
Android malware domain create Realizable Adversarial Examples (RealAEs), i.e.,
AEs that satisfy the domain constraints of Android malware. Recent studies have
shown that using such RealAEs in Adversarial Training (AT) is more effective in
defending against realistic attacks than using unrealizable AEs (unRealAEs).
This is because RealAEs allow defenders to explore certain pockets in the
feature space that are vulnerable to realistic attacks. However, existing
defenses commonly generate RealAEs in the problem space, which is known to be
time-consuming and impractical for AT. In this paper, we propose to generate
RealAEs in the feature space, leading to a simpler and more efficient solution.
Our approach is driven by a novel interpretation of Android domain constraints
in the feature space. More concretely, our defense first learns feature-space
domain constraints by extracting meaningful feature dependencies from data and
then applies them to generating feature-space RealAEs during AT. Extensive
experiments on DREBIN, a well-known Android malware detector, demonstrate that
our new defense outperforms not only unRealAE-based AT but also the
state-of-the-art defense that relies on non-uniform perturbations. We further
validate the ability of our learned feature-space domain constraints in
representing Android malware properties by showing that our feature-space
domain constraints can help distinguish RealAEs from unRealAEs.",2205.15128v3,https://arxiv.org/pdf/2205.15128v3
Robust Weight Perturbation for Adversarial Training,"Chaojian Yu, Bo Han, Mingming Gong, Li Shen, Shiming Ge, Bo Du, Tongliang Liu","Overfitting widely exists in adversarial robust training of deep networks. An
effective remedy is adversarial weight perturbation, which injects the
worst-case weight perturbation during network training by maximizing the
classification loss on adversarial examples. Adversarial weight perturbation
helps reduce the robust generalization gap; however, it also undermines the
robustness improvement. A criterion that regulates the weight perturbation is
therefore crucial for adversarial training. In this paper, we propose such a
criterion, namely Loss Stationary Condition (LSC) for constrained perturbation.
With LSC, we find that it is essential to conduct weight perturbation on
adversarial data with small classification loss to eliminate robust
overfitting. Weight perturbation on adversarial data with large classification
loss is not necessary and may even lead to poor robustness. Based on these
observations, we propose a robust perturbation strategy to constrain the extent
of weight perturbation. The perturbation strategy prevents deep networks from
overfitting while avoiding the side effect of excessive weight perturbation,
significantly improving the robustness of adversarial training. Extensive
experiments demonstrate the superiority of the proposed method over the
state-of-the-art adversarial training methods.",2205.14826v1,https://arxiv.org/pdf/2205.14826v1
"Evaluating Automated Driving Planner Robustness against Adversarial
  Influence","Andres Molina-Markham, Silvia G. Ionescu, Erin Lanus, Derek Ng, Sam Sommerer, Joseph J. Rushanan","Evaluating the robustness of automated driving planners is a critical and
challenging task. Although methodologies to evaluate vehicles are well
established, they do not yet account for a reality in which vehicles with
autonomous components share the road with adversarial agents. Our approach,
based on probabilistic trust models, aims to help researchers assess the
robustness of protections for machine learning-enabled planners against
adversarial influence. In contrast with established practices that evaluate
safety using the same evaluation dataset for all vehicles, we argue that
adversarial evaluation fundamentally requires a process that seeks to defeat a
specific protection. Hence, we propose that evaluations be based on estimating
the difficulty for an adversary to determine conditions that effectively induce
unsafe behavior. This type of inference requires precise statements about
threats, protections, and aspects of planning decisions to be guarded. We
demonstrate our approach by evaluating protections for planners relying on
camera-based object detectors.",2205.14697v1,https://arxiv.org/pdf/2205.14697v1
"On the Robustness of Safe Reinforcement Learning under Observational
  Perturbations","Zuxin Liu, Zijian Guo, Zhepeng Cen, Huan Zhang, Jie Tan, Bo Li, Ding Zhao","Safe reinforcement learning (RL) trains a policy to maximize the task reward
while satisfying safety constraints. While prior works focus on the performance
optimality, we find that the optimal solutions of many safe RL problems are not
robust and safe against carefully designed observational perturbations. We
formally analyze the unique properties of designing effective observational
adversarial attackers in the safe RL setting. We show that baseline adversarial
attack techniques for standard RL tasks are not always effective for safe RL
and propose two new approaches - one maximizes the cost and the other maximizes
the reward. One interesting and counter-intuitive finding is that the maximum
reward attack is strong, as it can both induce unsafe behaviors and make the
attack stealthy by maintaining the reward. We further propose a robust training
framework for safe RL and evaluate it via comprehensive experiments. This paper
provides a pioneer work to investigate the safety and robustness of RL under
observational attacks for future safe RL studies. Code is available at:
\url{https://github.com/liuzuxin/safe-rl-robustness}",2205.14691v3,https://arxiv.org/pdf/2205.14691v3
"Efficient Policy Iteration for Robust Markov Decision Processes via
  Regularization","Navdeep Kumar, Kfir Levy, Kaixin Wang, Shie Mannor","Robust Markov decision processes (MDPs) provide a general framework to model
decision problems where the system dynamics are changing or only partially
known. Efficient methods for some \texttt{sa}-rectangular robust MDPs exist,
using its equivalence with reward regularized MDPs, generalizable to online
settings. In comparison to \texttt{sa}-rectangular robust MDPs,
\texttt{s}-rectangular robust MDPs are less restrictive but much more difficult
to deal with. Interestingly, recent works have established the equivalence
between \texttt{s}-rectangular robust MDPs and policy regularized MDPs. But we
don't have a clear understanding to exploit this equivalence, to do policy
improvement steps to get the optimal value function or policy. We don't have a
clear understanding of greedy/optimal policy except it can be stochastic. There
exist no methods that can naturally be generalized to model-free settings. We
show a clear and explicit equivalence between \texttt{s}-rectangular $L_p$
robust MDPs and policy regularized MDPs that resemble very much policy entropy
regularized MDPs widely used in practice. Further, we dig into the policy
improvement step and concretely derive optimal robust Bellman operators for
\texttt{s}-rectangular $L_p$ robust MDPs. We find that the greedy/optimal
policies in \texttt{s}-rectangular $L_p$ robust MDPs are threshold policies
that play top $k$ actions whose $Q$ value is greater than some threshold
(value), proportional to the $(p-1)$th power of its advantage. In addition, we
show time complexity of (\texttt{sa} and \texttt{s}-rectangular) $L_p$ robust
MDPs is the same as non-robust MDPs up to some log factors. Our work greatly
extends the existing understanding of \texttt{s}-rectangular robust MDPs and
naturally generalizable to online settings.",2205.14327v2,https://arxiv.org/pdf/2205.14327v2
"MolScribe: Robust Molecular Structure Recognition with Image-To-Graph
  Generation","Yujie Qian, Jiang Guo, Zhengkai Tu, Zhening Li, Connor W. Coley, Regina Barzilay","Molecular structure recognition is the task of translating a molecular image
into its graph structure. Significant variation in drawing styles and
conventions exhibited in chemical literature poses a significant challenge for
automating this task. In this paper, we propose MolScribe, a novel
image-to-graph generation model that explicitly predicts atoms and bonds, along
with their geometric layouts, to construct the molecular structure. Our model
flexibly incorporates symbolic chemistry constraints to recognize chirality and
expand abbreviated structures. We further develop data augmentation strategies
to enhance the model robustness against domain shifts. In experiments on both
synthetic and realistic molecular images, MolScribe significantly outperforms
previous models, achieving 76-93% accuracy on public benchmarks. Chemists can
also easily verify MolScribe's prediction, informed by its confidence
estimation and atom-level alignment with the input image. MolScribe is publicly
available through Python and web interfaces:
https://github.com/thomas0809/MolScribe.",2205.14311v2,https://arxiv.org/pdf/2205.14311v2
Robust Phi-Divergence MDPs,"Chin Pang Ho, Marek Petrik, Wolfram Wiesemann","In recent years, robust Markov decision processes (MDPs) have emerged as a
prominent modeling framework for dynamic decision problems affected by
uncertainty. In contrast to classical MDPs, which only account for
stochasticity by modeling the dynamics through a stochastic process with a
known transition kernel, robust MDPs additionally account for ambiguity by
optimizing in view of the most adverse transition kernel from a prescribed
ambiguity set. In this paper, we develop a novel solution framework for robust
MDPs with s-rectangular ambiguity sets that decomposes the problem into a
sequence of robust Bellman updates and simplex projections. Exploiting the rich
structure present in the simplex projections corresponding to phi-divergence
ambiguity sets, we show that the associated s-rectangular robust MDPs can be
solved substantially faster than with state-of-the-art commercial solvers as
well as a recent first-order solution scheme, thus rendering them attractive
alternatives to classical MDPs in practical applications.",2205.14202v2,https://arxiv.org/pdf/2205.14202v2
Don't Explain Noise: Robust Counterfactuals for Randomized Ensembles,"Alexandre Forel, Axel Parmentier, Thibaut Vidal","Counterfactual explanations describe how to modify a feature vector in order
to flip the outcome of a trained classifier. Obtaining robust counterfactual
explanations is essential to provide valid algorithmic recourse and meaningful
explanations. We study the robustness of explanations of randomized ensembles,
which are always subject to algorithmic uncertainty even when the training data
is fixed. We formalize the generation of robust counterfactual explanations as
a probabilistic problem and show the link between the robustness of ensemble
models and the robustness of base learners. We develop a practical method with
good empirical performance and support it with theoretical guarantees for
ensembles of convex base learners. Our results show that existing methods give
surprisingly low robustness: the validity of naive counterfactuals is below
$50\%$ on most data sets and can fall to $20\%$ on problems with many features.
In contrast, our method achieves high robustness with only a small increase in
the distance from counterfactual explanations to their initial observations.",2205.14116v3,https://arxiv.org/pdf/2205.14116v3
Bayesian Robust Graph Contrastive Learning,"Yancheng Wang, Yingzhen Yang","Graph Neural Networks (GNNs) have been widely used to learn node
representations and with outstanding performance on various tasks such as node
classification. However, noise, which inevitably exists in real-world graph
data, would considerably degrade the performance of GNNs as the noise is easily
propagated via the graph structure. In this work, we propose a novel and robust
method, Bayesian Robust Graph Contrastive Learning (BRGCL), which trains a GNN
encoder to learn robust node representations. The BRGCL encoder is a completely
unsupervised encoder. Two steps are iteratively executed at each epoch of
training the BRGCL encoder: (1) estimating confident nodes and computing robust
cluster prototypes of node representations through a novel Bayesian
nonparametric method; (2) prototypical contrastive learning between the node
representations and the robust cluster prototypes. Experiments on public and
large-scale benchmarks demonstrate the superior performance of BRGCL and the
robustness of the learned node representations. The code of BRGCL is available
at \url{https://github.com/BRGCL-code/BRGCL-code}.",2205.14109v3,https://arxiv.org/pdf/2205.14109v3
"EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural
  Networks","Runlin Lei, Zhen Wang, Yaliang Li, Bolin Ding, Zhewei Wei","Graph Neural Networks (GNNs) have received extensive research attention for
their promising performance in graph machine learning. Despite their
extraordinary predictive accuracy, existing approaches, such as GCN and GPRGNN,
are not robust in the face of homophily changes on test graphs, rendering these
models vulnerable to graph structural attacks and with limited capacity in
generalizing to graphs of varied homophily levels. Although many methods have
been proposed to improve the robustness of GNN models, most of these techniques
are restricted to the spatial domain and employ complicated defense mechanisms,
such as learning new graph structures or calculating edge attentions. In this
paper, we study the problem of designing simple and robust GNN models in the
spectral domain. We propose EvenNet, a spectral GNN corresponding to an
even-polynomial graph filter. Based on our theoretical analysis in both spatial
and spectral domains, we demonstrate that EvenNet outperforms full-order models
in generalizing across homophilic and heterophilic graphs, implying that
ignoring odd-hop neighbors improves the robustness of GNNs. We conduct
experiments on both synthetic and real-world datasets to demonstrate the
effectiveness of EvenNet. Notably, EvenNet outperforms existing defense models
against structural attacks without introducing additional computational costs
and maintains competitiveness in traditional node classification tasks on
homophilic and heterophilic graphs.",2205.13892v2,https://arxiv.org/pdf/2205.13892v2
"Why Robust Generalization in Deep Learning is Difficult: Perspective of
  Expressive Power","Binghui Li, Jikai Jin, Han Zhong, John E. Hopcroft, Liwei Wang","It is well-known that modern neural networks are vulnerable to adversarial
examples. To mitigate this problem, a series of robust learning algorithms have
been proposed. However, although the robust training error can be near zero via
some methods, all existing algorithms lead to a high robust generalization
error. In this paper, we provide a theoretical understanding of this puzzling
phenomenon from the perspective of expressive power for deep neural networks.
Specifically, for binary classification problems with well-separated data, we
show that, for ReLU networks, while mild over-parameterization is sufficient
for high robust training accuracy, there exists a constant robust
generalization gap unless the size of the neural network is exponential in the
data dimension $d$. This result holds even if the data is linear separable
(which means achieving standard generalization is easy), and more generally for
any parameterized function classes as long as their VC dimension is at most
polynomial in the number of parameters. Moreover, we establish an improved
upper bound of $\exp({\mathcal{O}}(k))$ for the network size to achieve low
robust generalization error when the data lies on a manifold with intrinsic
dimension $k$ ($k \ll d$). Nonetheless, we also have a lower bound that grows
exponentially with respect to $k$ -- the curse of dimensionality is inevitable.
By demonstrating an exponential separation between the network size for
achieving low robust training and generalization error, our results reveal that
the hardness of robust generalization may stem from the expressive power of
practical models.",2205.13863v3,https://arxiv.org/pdf/2205.13863v3
RIGID: Robust Linear Regression with Missing Data,"Alireza Aghasi, MohammadJavad Feizollahi, Saeed Ghadimi","We present a robust framework to perform linear regression with missing
entries in the features. By considering an elliptical data distribution, and
specifically a multivariate normal model, we are able to conditionally
formulate a distribution for the missing entries and present a robust
framework, which minimizes the worst case error caused by the uncertainty about
the missing data. We show that the proposed formulation, which naturally takes
into account the dependency between different variables, ultimately reduces to
a convex program, for which a customized and scalable solver can be delivered.
In addition to a detailed analysis to deliver such solver, we also asymptoticly
analyze the behavior of the proposed framework, and present technical
discussions to estimate the required input parameters. We complement our
analysis with experiments performed on synthetic, semi-synthetic, and real
data, and show how the proposed formulation improves the prediction accuracy
and robustness, and outperforms the competing techniques.
  Missing data is a common problem associated with many datasets in machine
learning. With the significant increase in using robust optimization techniques
to train machine learning models, this paper presents a novel robust regression
framework that operates by minimizing the uncertainty associated with missing
data. The proposed approach allows training models with incomplete data, while
minimizing the impact of uncertainty associated with the unavailable data. The
ideas developed in this paper can be generalized beyond linear models and
elliptical data distributions.",2205.13635v3,https://arxiv.org/pdf/2205.13635v3
An Analytic Framework for Robust Training of Artificial Neural Networks,"Ramin Barati, Reza Safabakhsh, Mohammad Rahmati","The reliability of a learning model is key to the successful deployment of
machine learning in various industries. Creating a robust model, particularly
one unaffected by adversarial attacks, requires a comprehensive understanding
of the adversarial examples phenomenon. However, it is difficult to describe
the phenomenon due to the complicated nature of the problems in machine
learning. Consequently, many studies investigate the phenomenon by proposing a
simplified model of how adversarial examples occur and validate it by
predicting some aspect of the phenomenon. While these studies cover many
different characteristics of the adversarial examples, they have not reached a
holistic approach to the geometric and analytic modeling of the phenomenon.
This paper propose a formal framework to study the phenomenon in learning
theory and make use of complex analysis and holomorphicity to offer a robust
learning rule for artificial neural networks. With the help of complex
analysis, we can effortlessly move between geometric and analytic perspectives
of the phenomenon and offer further insights on the phenomenon by revealing its
connection with harmonic functions. Using our model, we can explain some of the
most intriguing characteristics of adversarial examples, including
transferability of adversarial examples, and pave the way for novel approaches
to mitigate the effects of the phenomenon.",2205.13502v2,https://arxiv.org/pdf/2205.13502v2
DeepJoint: Robust Survival Modelling Under Clinical Presence Shift,"Vincent Jeanselme, Glen Martin, Niels Peek, Matthew Sperrin, Brian Tom, Jessica Barrett","Observational data in medicine arise as a result of the complex interaction
between patients and the healthcare system. The sampling process is often
highly irregular and itself constitutes an informative process. When using such
data to develop prediction models, this phenomenon is often ignored, leading to
sub-optimal performance and generalisability of models when practices evolve.
We propose a multi-task recurrent neural network which models three clinical
presence dimensions -- namely the longitudinal, the inter-observation and the
missingness processes -- in parallel to the survival outcome. On a prediction
task using MIMIC III laboratory tests, explicit modelling of these three
processes showed improved performance in comparison to state-of-the-art
predictive models (C-index at 1 day horizon: 0.878). More importantly, the
proposed approach was more robust to change in the clinical presence setting,
demonstrated by performance comparison between patients admitted on weekdays
and weekends. This analysis demonstrates the importance of studying and
leveraging clinical presence to improve performance and create more
transportable clinical models.",2205.13481v1,https://arxiv.org/pdf/2205.13481v1
"Undersampling is a Minimax Optimal Robustness Intervention in
  Nonparametric Classification","Niladri S. Chatterji, Saminul Haque, Tatsunori Hashimoto","While a broad range of techniques have been proposed to tackle distribution
shift, the simple baseline of training on an $\textit{undersampled}$ balanced
dataset often achieves close to state-of-the-art-accuracy across several
popular benchmarks. This is rather surprising, since undersampling algorithms
discard excess majority group data. To understand this phenomenon, we ask if
learning is fundamentally constrained by a lack of minority group samples. We
prove that this is indeed the case in the setting of nonparametric binary
classification. Our results show that in the worst case, an algorithm cannot
outperform undersampling unless there is a high degree of overlap between the
train and test distributions (which is unlikely to be the case in real-world
datasets), or if the algorithm leverages additional structure about the
distribution shift. In particular, in the case of label shift we show that
there is always an undersampling algorithm that is minimax optimal. In the case
of group-covariate shift we show that there is an undersampling algorithm that
is minimax optimal when the overlap between the group distributions is small.
We also perform an experimental case study on a label shift dataset and find
that in line with our theory, the test accuracy of robust neural network
classifiers is constrained by the number of minority samples.",2205.13094v4,https://arxiv.org/pdf/2205.13094v4
How explainable are adversarially-robust CNNs?,"Mehdi Nourelahi, Lars Kotthoff, Peijie Chen, Anh Nguyen","Three important criteria of existing convolutional neural networks (CNNs) are
(1) test-set accuracy; (2) out-of-distribution accuracy; and (3)
explainability. While these criteria have been studied independently, their
relationship is unknown. For example, do CNNs that have a stronger
out-of-distribution performance have also stronger explainability? Furthermore,
most prior feature-importance studies only evaluate methods on 2-3 common
vanilla ImageNet-trained CNNs, leaving it unknown how these methods generalize
to CNNs of other architectures and training algorithms. Here, we perform the
first, large-scale evaluation of the relations of the three criteria using 9
feature-importance methods and 12 ImageNet-trained CNNs that are of 3 training
algorithms and 5 CNN architectures. We find several important insights and
recommendations for ML practitioners. First, adversarially robust CNNs have a
higher explainability score on gradient-based attribution methods (but not
CAM-based or perturbation-based methods). Second, AdvProp models, despite being
highly accurate more than both vanilla and robust models alone, are not
superior in explainability. Third, among 9 feature attribution methods tested,
GradCAM and RISE are consistently the best methods. Fourth, Insertion and
Deletion are biased towards vanilla and robust models respectively, due to
their strong correlation with the confidence score distributions of a CNN.
Fifth, we did not find a single CNN to be the best in all three criteria, which
interestingly suggests that CNNs are harder to interpret as they become more
accurate.",2205.13042v2,https://arxiv.org/pdf/2205.13042v2
BiT: Robustly Binarized Multi-distilled Transformer,"Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoorthi, Yashar Mehdad","Modern pre-trained transformers have rapidly advanced the state-of-the-art in
machine learning, but have also grown in parameters and computational
complexity, making them increasingly difficult to deploy in
resource-constrained environments. Binarization of the weights and activations
of the network can significantly alleviate these issues, however, is
technically challenging from an optimization perspective. In this work, we
identify a series of improvements that enables binary transformers at a much
higher accuracy than what was possible previously. These include a two-set
binarization scheme, a novel elastic binary activation function with learned
parameters, and a method to quantize a network to its limit by successively
distilling higher precision models into lower precision students. These
approaches allow for the first time, fully binarized transformer models that
are at a practical level of accuracy, approaching a full-precision BERT
baseline on the GLUE language understanding benchmark within as little as 5.9%.
Code and models are available at: https://github.com/facebookresearch/bit.",2205.13016v2,https://arxiv.org/pdf/2205.13016v2
"GARDNet: Robust Multi-View Network for Glaucoma Classification in Color
  Fundus Images","Ahmed Al Mahrooqi, Dmitrii Medvedev, Rand Muhtaseb, Mohammad Yaqub","Glaucoma is one of the most severe eye diseases, characterized by rapid
progression and leading to irreversible blindness. It is often the case that
diagnostics is carried out when one's sight has already significantly degraded
due to the lack of noticeable symptoms at early stage of the disease. Regular
glaucoma screenings of the population shall improve early-stage detection,
however the desirable frequency of etymological checkups is often not feasible
due to the excessive load imposed by manual diagnostics on limited number of
specialists. Considering the basic methodology to detect glaucoma is to analyze
fundus images for the optic-disc-to-optic-cup ratio, Machine Learning
algorithms can offer sophisticated methods for image processing and
classification. In our work, we propose an advanced image pre-processing
technique combined with a multi-view network of deep classification models to
categorize glaucoma. Our Glaucoma Automated Retinal Detection Network (GARDNet)
has been successfully tested on Rotterdam EyePACS AIROGS dataset with an AUC of
0.92, and then additionally fine-tuned and tested on RIM-ONE DL dataset with an
AUC of 0.9308 outperforming the state-of-the-art of 0.9272. Our code is
available on https://github.com/ahmed1996said/gardnet",2205.12902v3,https://arxiv.org/pdf/2205.12902v3
Robust Reinforcement Learning on Graphs for Logistics optimization,"Zangir Iklassov, Dmitrii Medvedev","Logistics optimization nowadays is becoming one of the hottest areas in the
AI community. In the past year, significant advancements in the domain were
achieved by representing the problem in a form of graph. Another promising area
of research was to apply reinforcement learning algorithms to the above task.
In our work, we made advantage of using both approaches and apply reinforcement
learning on a graph. To do that, we have analyzed the most recent results in
both fields and selected SOTA algorithms both from graph neural networks and
reinforcement learning. Then, we combined selected models on the problem of
AMOD systems optimization for the transportation network of New York city. Our
team compared three algorithms - GAT, Pro-CNN and PTDNet - to bring to the fore
the important nodes on a graph representation. Finally, we achieved SOTA
results on AMOD systems optimization problem employing PTDNet with GNN and
training them in reinforcement fashion.
  Keywords: Graph Neural Network (GNN), Logistics optimization, Reinforcement
Learning",2205.12888v1,https://arxiv.org/pdf/2205.12888v1
"An Empirical Study on Distribution Shift Robustness From the Perspective
  of Pre-Training and Data Augmentation","Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin, Xiangyang Ji, Antoni B. Chan","The performance of machine learning models under distribution shift has been
the focus of the community in recent years. Most of current methods have been
proposed to improve the robustness to distribution shift from the algorithmic
perspective, i.e., designing better training algorithms to help the
generalization in shifted test distributions. This paper studies the
distribution shift problem from the perspective of pre-training and data
augmentation, two important factors in the practice of deep learning that have
not been systematically investigated by existing work. By evaluating seven
pre-trained models, including ResNets and ViT's with self-supervision and
supervision mode, on five important distribution-shift datasets, from WILDS and
DomainBed benchmarks, with five different learning algorithms, we provide the
first comprehensive empirical study focusing on pre-training and data
augmentation. With our empirical result obtained from 1,330 models, we provide
the following main observations: 1) ERM combined with data augmentation can
achieve state-of-the-art performance if we choose a proper pre-trained model
respecting the data property; 2) specialized algorithms further improve the
robustness on top of ERM when handling a specific type of distribution shift,
e.g., GroupDRO for spurious correlation and CORAL for large-scale
out-of-distribution data; 3) Comparing different pre-training modes,
architectures and data sizes, we provide novel observations about pre-training
on distribution shift, which sheds light on designing or selecting pre-training
strategy for different kinds of distribution shifts. In summary, our empirical
study provides a comprehensive baseline for a wide range of pre-training models
fine-tuned with data augmentation, which potentially inspires research
exploiting the power of pre-training and data augmentation in the future of
distribution shift study.",2205.12753v1,https://arxiv.org/pdf/2205.12753v1
"RobustLR: Evaluating Robustness to Logical Perturbation in Deductive
  Reasoning","Soumya Sanyal, Zeyi Liao, Xiang Ren","Transformers have been shown to be able to perform deductive reasoning on a
logical rulebase containing rules and statements written in English natural
language. While the progress is promising, it is currently unclear if these
models indeed perform logical reasoning by understanding the underlying logical
semantics in the language. To this end, we propose RobustLR, a suite of
evaluation datasets that evaluate the robustness of these models to minimal
logical edits in rulebases and some standard logical equivalence conditions. In
our experiments with RoBERTa and T5, we find that the models trained in prior
works do not perform consistently on the different perturbations in RobustLR,
thus showing that the models are not robust to the proposed logical
perturbations. Further, we find that the models find it especially hard to
learn logical negation and disjunction operators. Overall, using our evaluation
sets, we demonstrate some shortcomings of the deductive reasoning-based
language models, which can eventually help towards designing better models for
logical reasoning over natural language. All the datasets and code base have
been made publicly available.",2205.12598v2,https://arxiv.org/pdf/2205.12598v2
"Linear Algorithms for Robust and Scalable Nonparametric Multiclass
  Probability Estimation","Liyun Zeng, Hao Helen Zhang","Multiclass probability estimation is the problem of estimating conditional
probabilities of a data point belonging to a class given its covariate
information. It has broad applications in statistical analysis and data
science. Recently a class of weighted Support Vector Machines (wSVMs) has been
developed to estimate class probabilities through ensemble learning for
$K$-class problems (Wu, Zhang and Liu, 2010; Wang, Zhang and Wu, 2019), where
$K$ is the number of classes. The estimators are robust and achieve high
accuracy for probability estimation, but their learning is implemented through
pairwise coupling, which demands polynomial time in $K$. In this paper, we
propose two new learning schemes, the baseline learning and the One-vs-All
(OVA) learning, to further improve wSVMs in terms of computational efficiency
and estimation accuracy. In particular, the baseline learning has optimal
computational complexity in the sense that it is linear in $K$. Though not
being most efficient in computation, the OVA offers the best estimation
accuracy among all the procedures under comparison. The resulting estimators
are distribution-free and shown to be consistent. We further conduct extensive
numerical experiments to demonstrate finite sample performance.",2205.12460v3,https://arxiv.org/pdf/2205.12460v3
"Certified Robustness Against Natural Language Attacks by Causal
  Intervention","Haiteng Zhao, Chang Ma, Xinshuai Dong, Anh Tuan Luu, Zhi-Hong Deng, Hanwang Zhang","Deep learning models have achieved great success in many fields, yet they are
vulnerable to adversarial examples. This paper follows a causal perspective to
look into the adversarial vulnerability and proposes Causal Intervention by
Semantic Smoothing (CISS), a novel framework towards robustness against natural
language attacks. Instead of merely fitting observational data, CISS learns
causal effects p(y|do(x)) by smoothing in the latent semantic space to make
robust predictions, which scales to deep architectures and avoids tedious
construction of noise customized for specific attacks. CISS is provably robust
against word substitution attacks, as well as empirically robust even when
perturbations are strengthened by unknown attack algorithms. For example, on
YELP, CISS surpasses the runner-up by 6.7% in terms of certified robustness
against word substitutions, and achieves 79.4% empirical robustness when
syntactic attacks are integrated.",2205.12331v3,https://arxiv.org/pdf/2205.12331v3
Robust 3D Object Detection in Cold Weather Conditions,"Aldi Piroli, Vinzenz Dallabetta, Marc Walessa, Daniel Meissner, Johannes Kopp, Klaus Dietmayer","Adverse weather conditions can negatively affect LiDAR-based object
detectors. In this work, we focus on the phenomenon of vehicle gas exhaust
condensation in cold weather conditions. This everyday effect can influence the
estimation of object sizes, orientations and introduce ghost object detections,
compromising the reliability of the state of the art object detectors. We
propose to solve this problem by using data augmentation and a novel training
loss term. To effectively train deep neural networks, a large set of labeled
data is needed. In case of adverse weather conditions, this process can be
extremely laborious and expensive. We address this issue in two steps: First,
we present a gas exhaust data generation method based on 3D surface
reconstruction and sampling which allows us to generate large sets of gas
exhaust clouds from a small pool of labeled data. Second, we introduce a point
cloud augmentation process that can be used to add gas exhaust to datasets
recorded in good weather conditions. Finally, we formulate a new training loss
term that leverages the augmented point cloud to increase object detection
robustness by penalizing predictions that include noise. In contrast to other
works, our method can be used with both grid-based and point-based detectors.
Moreover, since our approach does not require any network architecture changes,
inference times remain unchanged. Experimental results on real data show that
our proposed method greatly increases robustness to gas exhaust and noisy data.",2205.11925v2,https://arxiv.org/pdf/2205.11925v2
NFL: Robust Learned Index via Distribution Transformation,"Shangyu Wu, Yufei Cui, Jinghuan Yu, Xuan Sun, Tei-Wei Kuo, Chun Jason Xue","Recent works on learned index open a new direction for the indexing field.
The key insight of the learned index is to approximate the mapping between keys
and positions with piece-wise linear functions. Such methods require
partitioning key space for a better approximation. Although lots of heuristics
are proposed to improve the approximation quality, the bottleneck is that the
segmentation overheads could hinder the overall performance. This paper tackles
the approximation problem by applying a \textit{distribution transformation} to
the keys before constructing the learned index. A two-stage
Normalizing-Flow-based Learned index framework (NFL) is proposed, which first
transforms the original complex key distribution into a near-uniform
distribution, then builds a learned index leveraging the transformed keys. For
effective distribution transformation, we propose a Numerical Normalizing Flow
(Numerical NF). Based on the characteristics of the transformed keys, we
propose a robust After-Flow Learned Index (AFLI). To validate the performance,
comprehensive evaluations are conducted on both synthetic and real-world
workloads, which shows that the proposed NFL produces the highest throughput
and the lowest tail latency compared to the state-of-the-art learned indexes.",2205.11807v1,https://arxiv.org/pdf/2205.11807v1
"Byzantine-Robust Federated Learning with Optimal Statistical Rates and
  Privacy Guarantees","Banghua Zhu, Lun Wang, Qi Pang, Shuai Wang, Jiantao Jiao, Dawn Song, Michael I. Jordan","We propose Byzantine-robust federated learning protocols with nearly optimal
statistical rates. In contrast to prior work, our proposed protocols improve
the dimension dependence and achieve a tight statistical rate in terms of all
the parameters for strongly convex losses. We benchmark against competing
protocols and show the empirical superiority of the proposed protocols.
Finally, we remark that our protocols with bucketing can be naturally combined
with privacy-guaranteeing procedures to introduce security against a
semi-honest server. The code for evaluation is provided in
https://github.com/wanglun1996/secure-robust-federated-learning.",2205.11765v2,https://arxiv.org/pdf/2205.11765v2
"Alleviating Robust Overfitting of Adversarial Training With Consistency
  Regularization","Shudong Zhang, Haichang Gao, Tianwei Zhang, Yunyi Zhou, Zihui Wu","Adversarial training (AT) has proven to be one of the most effective ways to
defend Deep Neural Networks (DNNs) against adversarial attacks. However, the
phenomenon of robust overfitting, i.e., the robustness will drop sharply at a
certain stage, always exists during AT. It is of great importance to decrease
this robust generalization gap in order to obtain a robust model. In this
paper, we present an in-depth study towards the robust overfitting from a new
angle. We observe that consistency regularization, a popular technique in
semi-supervised learning, has a similar goal as AT and can be used to alleviate
robust overfitting. We empirically validate this observation, and find a
majority of prior solutions have implicit connections to consistency
regularization. Motivated by this, we introduce a new AT solution, which
integrates the consistency regularization and Mean Teacher (MT) strategy into
AT. Specifically, we introduce a teacher model, coming from the average weights
of the student models over the training steps. Then we design a consistency
loss function to make the prediction distribution of the student models over
adversarial examples consistent with that of the teacher model over clean
samples. Experiments show that our proposed method can effectively alleviate
robust overfitting and improve the robustness of DNN models against common
adversarial attacks.",2205.11744v1,https://arxiv.org/pdf/2205.11744v1
"Utilizing Language-Image Pretraining for Efficient and Robust Bilingual
  Word Alignment","Tuan Dinh, Jy-yong Sohn, Shashank Rajput, Timothy Ossowski, Yifei Ming, Junjie Hu, Dimitris Papailiopoulos, Kangwook Lee","Word translation without parallel corpora has become feasible, rivaling the
performance of supervised methods. Recent findings have shown that the accuracy
and robustness of unsupervised word translation (UWT) can be improved by making
use of visual observations, which are universal representations across
languages. In this work, we investigate the potential of using not only visual
observations but also pretrained language-image models for enabling a more
efficient and robust UWT. Specifically, we develop a novel UWT method dubbed
Word Alignment using Language-Image Pretraining (WALIP), which leverages visual
observations via the shared embedding space of images and texts provided by
CLIP models (Radford et al., 2021). WALIP has a two-step procedure. First, we
retrieve word pairs with high confidences of similarity, computed using our
proposed image-based fingerprints, which define the initial pivot for the word
alignment. Second, we apply our robust Procrustes algorithm to estimate the
linear mapping between two embedding spaces, which iteratively corrects and
refines the estimated alignment. Our extensive experiments show that WALIP
improves upon the state-of-the-art performance of bilingual word alignment for
a few language pairs across different word embeddings and displays great
robustness to the dissimilarity of language pairs or training corpora for two
word embeddings.",2205.11616v2,https://arxiv.org/pdf/2205.11616v2
"Robust and Agnostic Learning of Conditional Distributional Treatment
  Effects","Nathan Kallus, Miruna Oprescu","The conditional average treatment effect (CATE) is the best measure of
individual causal effects given baseline covariates. However, the CATE only
captures the (conditional) average, and can overlook risks and tail events,
which are important to treatment choice. In aggregate analyses, this is usually
addressed by measuring the distributional treatment effect (DTE), such as
differences in quantiles or tail expectations between treatment groups.
Hypothetically, one can similarly fit conditional quantile regressions in each
treatment group and take their difference, but this would not be robust to
misspecification or provide agnostic best-in-class predictions. We provide a
new robust and model-agnostic methodology for learning the conditional DTE
(CDTE) for a class of problems that includes conditional quantile treatment
effects, conditional super-quantile treatment effects, and conditional
treatment effects on coherent risk measures given by $f$-divergences. Our
method is based on constructing a special pseudo-outcome and regressing it on
covariates using any regression learner. Our method is model-agnostic in that
it can provide the best projection of CDTE onto the regression model class. Our
method is robust in that even if we learn these nuisances nonparametrically at
very slow rates, we can still learn CDTEs at rates that depend on the class
complexity and even conduct inferences on linear projections of CDTEs. We
investigate the behavior of our proposal in simulations, as well as in a case
study of 401(k) eligibility effects on wealth.",2205.11486v2,https://arxiv.org/pdf/2205.11486v2
"Logical Reasoning with Span-Level Predictions for Interpretable and
  Robust NLI Models","Joe Stacey, Pasquale Minervini, Haim Dubossarsky, Marek Rei","Current Natural Language Inference (NLI) models achieve impressive results,
sometimes outperforming humans when evaluating on in-distribution test sets.
However, as these models are known to learn from annotation artefacts and
dataset biases, it is unclear to what extent the models are learning the task
of NLI instead of learning from shallow heuristics in their training data. We
address this issue by introducing a logical reasoning framework for NLI,
creating highly transparent model decisions that are based on logical rules.
Unlike prior work, we show that improved interpretability can be achieved
without decreasing the predictive accuracy. We almost fully retain performance
on SNLI, while also identifying the exact hypothesis spans that are responsible
for each model prediction. Using the e-SNLI human explanations, we verify that
our model makes sensible decisions at a span level, despite not using any span
labels during training. We can further improve model performance and span-level
decisions by using the e-SNLI explanations during training. Finally, our model
is more robust in a reduced data setting. When training with only 1,000
examples, out-of-distribution performance improves on the MNLI matched and
mismatched validation sets by 13% and 16% relative to the baseline. Training
with fewer observations yields further improvements, both in-distribution and
out-of-distribution.",2205.11432v3,https://arxiv.org/pdf/2205.11432v3
"Training Efficient CNNS: Tweaking the Nuts and Bolts of Neural Networks
  for Lighter, Faster and Robust Models","Sabeesh Ethiraj, Bharath Kumar Bolla","Deep Learning has revolutionized the fields of computer vision, natural
language understanding, speech recognition, information retrieval and more.
Many techniques have evolved over the past decade that made models lighter,
faster, and robust with better generalization. However, many deep learning
practitioners persist with pre-trained models and architectures trained mostly
on standard datasets such as Imagenet, MS-COCO, IMDB-Wiki Dataset, and
Kinetics-700 and are either hesitant or unaware of redesigning the architecture
from scratch that will lead to better performance. This scenario leads to
inefficient models that are not suitable on various devices such as mobile,
edge, and fog. In addition, these conventional training methods are of concern
as they consume a lot of computing power. In this paper, we revisit various
SOTA techniques that deal with architecture efficiency (Global Average Pooling,
depth-wise convolutions & squeeze and excitation, Blurpool), learning rate
(Cyclical Learning Rate), data augmentation (Mixup, Cutout), label manipulation
(label smoothing), weight space manipulation (stochastic weight averaging), and
optimizer (sharpness aware minimization). We demonstrate how an efficient deep
convolution network can be built in a phased manner by sequentially reducing
the number of training parameters and using the techniques mentioned above. We
achieved a SOTA accuracy of 99.2% on MNIST data with just 1500 parameters and
an accuracy of 86.01% with just over 140K parameters on the CIFAR-10 dataset.",2205.12050v1,https://arxiv.org/pdf/2205.12050v1
Deep Image Retrieval is not Robust to Label Noise,"Stanislav Dereka, Ivan Karpukhin, Sergey Kolesnikov","Large-scale datasets are essential for the success of deep learning in image
retrieval. However, manual assessment errors and semi-supervised annotation
techniques can lead to label noise even in popular datasets. As previous works
primarily studied annotation quality in image classification tasks, it is still
unclear how label noise affects deep learning approaches to image retrieval. In
this work, we show that image retrieval methods are less robust to label noise
than image classification ones. Furthermore, we, for the first time,
investigate different types of label noise specific to image retrieval tasks
and study their effect on model performance.",2205.11195v1,https://arxiv.org/pdf/2205.11195v1
Squeeze Training for Adversarial Robustness,"Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen","The vulnerability of deep neural networks (DNNs) to adversarial examples has
attracted great attention in the machine learning community. The problem is
related to non-flatness and non-smoothness of normally obtained loss
landscapes. Training augmented with adversarial examples (a.k.a., adversarial
training) is considered as an effective remedy. In this paper, we highlight
that some collaborative examples, nearly perceptually indistinguishable from
both adversarial and benign examples yet show extremely lower prediction loss,
can be utilized to enhance adversarial training. A novel method is therefore
proposed to achieve new state-of-the-arts in adversarial robustness. Code:
https://github.com/qizhangli/ST-AT.",2205.11156v2,https://arxiv.org/pdf/2205.11156v2
"AutoJoin: Efficient Adversarial Training for Robust Maneuvering via
  Denoising Autoencoder and Joint Learning","Michael Villarreal, Bibek Poudel, Ryan Wickman, Yu Shen, Weizi Li","As a result of increasingly adopted machine learning algorithms and
ubiquitous sensors, many 'perception-to-control' systems are developed and
deployed. For these systems to be trustworthy, we need to improve their
robustness with adversarial training being one approach. We propose a
gradient-free adversarial training technique, called AutoJoin, which is a very
simple yet effective and efficient approach to produce robust models for
imaged-based maneuvering. Compared to other SOTA methods with testing on over
5M perturbed and clean images, AutoJoin achieves significant performance
increases up to the 40% range under gradient-free perturbations while improving
on clean performance up to 300%. Regarding efficiency, AutoJoin demonstrates
strong advantages over other SOTA techniques by saving up to 83% time per
training epoch and 90% training data. Although not the focus of AutoJoin, it
even demonstrates superb ability in defending gradient-based attacks. The core
idea of AutoJoin is to use a decoder attachment to the original regression
model creating a denoising autoencoder within the architecture. This
architecture allows the tasks 'maneuvering' and 'denoising sensor input' to be
jointly learnt and reinforce each other's performance.",2205.10933v2,https://arxiv.org/pdf/2205.10933v2
Test-Time Robust Personalization for Federated Learning,"Liangze Jiang, Tao Lin","Federated Learning (FL) is a machine learning paradigm where many clients
collaboratively learn a shared global model with decentralized training data.
Personalized FL additionally adapts the global model to different clients,
achieving promising results on consistent local training and test
distributions. However, for real-world personalized FL applications, it is
crucial to go one step further: robustifying FL models under the evolving local
test set during deployment, where various distribution shifts can arise. In
this work, we identify the pitfalls of existing works under test-time
distribution shifts and propose Federated Test-time Head Ensemble plus
tuning(FedTHE+), which personalizes FL models with robustness to various
test-time distribution shifts. We illustrate the advancement of FedTHE+ (and
its computationally efficient variant FedTHE) over strong competitors, by
training various neural architectures (CNN, ResNet, and Transformer) on CIFAR10
andImageNet with various test distributions. Along with this, we build a
benchmark for assessing the performance and robustness of personalized FL
methods during deployment. Code: https://github.com/LINs-lab/FedTHE.",2205.10920v4,https://arxiv.org/pdf/2205.10920v4
Federated Learning Aggregation: New Robust Algorithms with Guarantees,"Adnan Ben Mansour, Gaia Carenini, Alexandre Duplessis, David Naccache","Federated Learning has been recently proposed for distributed model training
at the edge. The principle of this approach is to aggregate models learned on
distributed clients to obtain a new more general ""average"" model (FedAvg). The
resulting model is then redistributed to clients for further training. To date,
the most popular federated learning algorithm uses coordinate-wise averaging of
the model parameters for aggregation. In this paper, we carry out a complete
general mathematical convergence analysis to evaluate aggregation strategies in
a federated learning framework. From this, we derive novel aggregation
algorithms which are able to modify their model architecture by differentiating
client contributions according to the value of their losses. Moreover, we go
beyond the assumptions introduced in theory, by evaluating the performance of
these strategies and by comparing them with the one of FedAvg in classification
tasks in both the IID and the Non-IID framework without additional hypothesis.",2205.10864v2,https://arxiv.org/pdf/2205.10864v2
Robust Quantity-Aware Aggregation for Federated Learning,"Jingwei Yi, Fangzhao Wu, Huishuai Zhang, Bin Zhu, Tao Qi, Guangzhong Sun, Xing Xie","Federated learning (FL) enables multiple clients to collaboratively train
models without sharing their local data, and becomes an important
privacy-preserving machine learning framework. However, classical FL faces
serious security and robustness problem, e.g., malicious clients can poison
model updates and at the same time claim large quantities to amplify the impact
of their model updates in the model aggregation. Existing defense methods for
FL, while all handling malicious model updates, either treat all quantities
benign or simply ignore/truncate the quantities of all clients. The former is
vulnerable to quantity-enhanced attack, while the latter leads to sub-optimal
performance since the local data on different clients is usually in
significantly different sizes. In this paper, we propose a robust
quantity-aware aggregation algorithm for federated learning, called FedRA, to
perform the aggregation with awareness of local data quantities while being
able to defend against quantity-enhanced attacks. More specifically, we propose
a method to filter malicious clients by jointly considering the uploaded model
updates and data quantities from different clients, and performing
quantity-aware weighted averaging on model updates from remaining clients.
Moreover, as the number of malicious clients participating in the federated
learning may dynamically change in different rounds, we also propose a
malicious client number estimator to predict how many suspicious clients should
be filtered in each round. Experiments on four public datasets demonstrate the
effectiveness of our FedRA method in defending FL against quantity-enhanced
attacks.",2205.10848v2,https://arxiv.org/pdf/2205.10848v2
"A Dirichlet Process Mixture of Robust Task Models for Scalable Lifelong
  Reinforcement Learning","Zhi Wang, Chunlin Chen, Daoyi Dong","While reinforcement learning (RL) algorithms are achieving state-of-the-art
performance in various challenging tasks, they can easily encounter
catastrophic forgetting or interference when faced with lifelong streaming
information. In the paper, we propose a scalable lifelong RL method that
dynamically expands the network capacity to accommodate new knowledge while
preventing past memories from being perturbed. We use a Dirichlet process
mixture to model the non-stationary task distribution, which captures task
relatedness by estimating the likelihood of task-to-cluster assignments and
clusters the task models in a latent space. We formulate the prior distribution
of the mixture as a Chinese restaurant process (CRP) that instantiates new
mixture components as needed. The update and expansion of the mixture are
governed by the Bayesian non-parametric framework with an expectation
maximization (EM) procedure, which dynamically adapts the model complexity
without explicit task boundaries or heuristics. Moreover, we use the domain
randomization technique to train robust prior parameters for the initialization
of each task model in the mixture, thus the resulting model can better
generalize and adapt to unseen tasks. With extensive experiments conducted on
robot navigation and locomotion domains, we show that our method successfully
facilitates scalable lifelong RL and outperforms relevant existing methods.",2205.10787v1,https://arxiv.org/pdf/2205.10787v1
Robust Flow-based Conformal Inference (FCI) with Statistical Guarantee,"Youhui Ye, Meimei Liu, Xin Xing","Conformal prediction aims to determine precise levels of confidence in
predictions for new objects using past experience. However, the commonly used
exchangeable assumptions between the training data and testing data limit its
usage in dealing with contaminated testing sets. In this paper, we develop a
novel flow-based conformal inference (FCI) method to build predictive sets and
infer outliers for complex and high-dimensional data. We leverage ideas from
adversarial flow to transfer the input data to a random vector with known
distributions. Our roundtrip transformation can map the input data to a
low-dimensional space, meanwhile reserving the conditional distribution of
input data given each class label, which enables us to construct a
non-conformity score for uncertainty quantification. Our approach is applicable
and robust when the testing data is contaminated. We evaluate our method,
robust flow-based conformal inference, on benchmark datasets. We find that it
produces effective predictive sets and accurate outlier detection and is more
powerful relative to competing approaches.",2205.10732v2,https://arxiv.org/pdf/2205.10732v2
Enriched Robust Multi-View Kernel Subspace Clustering,"Mengyuan Zhang, Kai Liu","Subspace clustering is to find underlying low-dimensional subspaces and
cluster the data points correctly. In this paper, we propose a novel multi-view
subspace clustering method. Most existing methods suffer from two critical
issues. First, they usually adopt a two-stage framework and isolate the
processes of affinity learning, multi-view information fusion and clustering.
Second, they assume the data lies in a linear subspace which may fail in
practice as most real-world datasets may have non-linearity structures. To
address the above issues, in this paper we propose a novel Enriched Robust
Multi-View Kernel Subspace Clustering framework where the consensus affinity
matrix is learned from both multi-view data and spectral clustering. Due to the
objective and constraints which is difficult to optimize, we propose an
iterative optimization method which is easy to implement and can yield closed
solution in each step. Extensive experiments have validated the superiority of
our method over state-of-the-art clustering methods.",2205.10495v1,https://arxiv.org/pdf/2205.10495v1
"Robust Sensible Adversarial Learning of Deep Neural Networks for Image
  Classification","Jungeum Kim, Xiao Wang","The idea of robustness is central and critical to modern statistical
analysis. However, despite the recent advances of deep neural networks (DNNs),
many studies have shown that DNNs are vulnerable to adversarial attacks. Making
imperceptible changes to an image can cause DNN models to make the wrong
classification with high confidence, such as classifying a benign mole as a
malignant tumor and a stop sign as a speed limit sign. The trade-off between
robustness and standard accuracy is common for DNN models. In this paper, we
introduce sensible adversarial learning and demonstrate the synergistic effect
between pursuits of standard natural accuracy and robustness. Specifically, we
define a sensible adversary which is useful for learning a robust model while
keeping high natural accuracy. We theoretically establish that the Bayes
classifier is the most robust multi-class classifier with the 0-1 loss under
sensible adversarial learning. We propose a novel and efficient algorithm that
trains a robust model using implicit loss truncation. We apply sensible
adversarial learning for large-scale image classification to a handwritten
digital image dataset called MNIST and an object recognition colored image
dataset called CIFAR10. We have performed an extensive comparative study to
compare our method with other competitive methods. Our experiments empirically
demonstrate that our method is not sensitive to its hyperparameter and does not
collapse even with a small model capacity while promoting robustness against
various attacks and keeping high natural accuracy.",2205.10457v1,https://arxiv.org/pdf/2205.10457v1
"The developmental trajectory of object recognition robustness: children
  are like small adults but unlike big deep neural networks","Lukas S. Huber, Robert Geirhos, Felix A. Wichmann","In laboratory object recognition tasks based on undistorted photographs, both
adult humans and Deep Neural Networks (DNNs) perform close to ceiling. Unlike
adults', whose object recognition performance is robust against a wide range of
image distortions, DNNs trained on standard ImageNet (1.3M images) perform
poorly on distorted images. However, the last two years have seen impressive
gains in DNN distortion robustness, predominantly achieved through
ever-increasing large-scale datasets$\unicode{x2014}$orders of magnitude larger
than ImageNet. While this simple brute-force approach is very effective in
achieving human-level robustness in DNNs, it raises the question of whether
human robustness, too, is simply due to extensive experience with (distorted)
visual input during childhood and beyond. Here we investigate this question by
comparing the core object recognition performance of 146 children (aged
4$\unicode{x2013}$15) against adults and against DNNs. We find, first, that
already 4$\unicode{x2013}$6 year-olds showed remarkable robustness to image
distortions and outperform DNNs trained on ImageNet. Second, we estimated the
number of $\unicode{x201C}$images$\unicode{x201D}$ children have been exposed
to during their lifetime. Compared to various DNNs, children's high robustness
requires relatively little data. Third, when recognizing objects
children$\unicode{x2014}$like adults but unlike DNNs$\unicode{x2014}$rely
heavily on shape but not on texture cues. Together our results suggest that the
remarkable robustness to distortions emerges early in the developmental
trajectory of human object recognition and is unlikely the result of a mere
accumulation of experience with distorted visual input. Even though current
DNNs match human performance regarding robustness they seem to rely on
different and more data-hungry strategies to do so.",2205.10144v1,https://arxiv.org/pdf/2205.10144v1
"Robust Expected Information Gain for Optimal Bayesian Experimental
  Design Using Ambiguity Sets","Jinwoo Go, Tobin Isaac","The ranking of experiments by expected information gain (EIG) in Bayesian
experimental design is sensitive to changes in the model's prior distribution,
and the approximation of EIG yielded by sampling will have errors similar to
the use of a perturbed prior. We define and analyze \emph{robust expected
information gain} (REIG), a modification of the objective in EIG maximization
by minimizing an affine relaxation of EIG over an ambiguity set of
distributions that are close to the original prior in KL-divergence. We show
that, when combined with a sampling-based approach to estimating EIG, REIG
corresponds to a `log-sum-exp' stabilization of the samples used to estimate
EIG, meaning that it can be efficiently implemented in practice. Numerical
tests combining REIG with variational nested Monte Carlo (VNMC), adaptive
contrastive estimation (ACE) and mutual information neural estimation (MINE)
suggest that in practice REIG also compensates for the variability of
under-sampled estimators.",2205.09914v1,https://arxiv.org/pdf/2205.09914v1
Residual Dynamic Mode Decomposition: Robust and verified Koopmanism,"Matthew J. Colbrook, Lorna J. Ayton, Máté Szőke","Dynamic Mode Decomposition (DMD) describes complex dynamic processes through
a hierarchy of simpler coherent features. DMD is regularly used to understand
the fundamental characteristics of turbulence and is closely related to Koopman
operators. However, verifying the decomposition, equivalently the computed
spectral features of Koopman operators, remains a major challenge due to the
infinite-dimensional nature of Koopman operators. Challenges include spurious
(unphysical) modes, and dealing with continuous spectra, both of which occur
regularly in turbulent flows. Residual Dynamic Mode Decomposition (ResDMD),
introduced by (Colbrook & Townsend 2021), overcomes some of these challenges
through the data-driven computation of residuals associated with the full
infinite-dimensional Koopman operator. ResDMD computes spectra and
pseudospectra of general Koopman operators with error control, and computes
smoothed approximations of spectral measures (including continuous spectra)
with explicit high-order convergence theorems. ResDMD thus provides robust and
verified Koopmanism. We implement ResDMD and demonstrate its application in a
variety of fluid dynamic situations, at varying Reynolds numbers, arising from
both numerical and experimental data. Examples include: vortex shedding behind
a cylinder; hot-wire data acquired in a turbulent boundary layer; particle
image velocimetry data focusing on a wall-jet flow; and acoustic pressure
signals of laser-induced plasma. We present some advantages of ResDMD, namely,
the ability to verifiably resolve non-linear, transient modes, and spectral
calculation with reduced broadening effects. We also discuss how a new modal
ordering based on residuals enables greater accuracy with a smaller dictionary
than the traditional modulus ordering. This paves the way for greater dynamic
compression of large datasets without sacrificing accuracy.",2205.09779v1,https://arxiv.org/pdf/2205.09779v1
Robust and Efficient Medical Imaging with Self-Supervision,"Shekoofeh Azizi, Laura Culp, Jan Freyberg, Basil Mustafa, Sebastien Baur, Simon Kornblith, Ting Chen, Patricia MacWilliams, S. Sara Mahdavi, Ellery Wulczyn, Boris Babenko, Megan Wilson, Aaron Loh, Po-Hsuan Cameron Chen, Yuan Liu, Pinal Bavishi, Scott Mayer McKinney, Jim Winkens, Abhijit Guha Roy, Zach Beaver, Fiona Ryan, Justin Krogue, Mozziyar Etemadi, Umesh Telang, Yun Liu, Lily Peng, Greg S. Corrado, Dale R. Webster, David Fleet, Geoffrey Hinton, Neil Houlsby, Alan Karthikesalingam, Mohammad Norouzi, Vivek Natarajan","Recent progress in Medical Artificial Intelligence (AI) has delivered systems
that can reach clinical expert level performance. However, such systems tend to
demonstrate sub-optimal ""out-of-distribution"" performance when evaluated in
clinical settings different from the training environment. A common mitigation
strategy is to develop separate systems for each clinical setting using
site-specific data [1]. However, this quickly becomes impractical as medical
data is time-consuming to acquire and expensive to annotate [2]. Thus, the
problem of ""data-efficient generalization"" presents an ongoing difficulty for
Medical AI development. Although progress in representation learning shows
promise, their benefits have not been rigorously studied, specifically for
out-of-distribution settings. To meet these challenges, we present REMEDIS, a
unified representation learning strategy to improve robustness and
data-efficiency of medical imaging AI. REMEDIS uses a generic combination of
large-scale supervised transfer learning with self-supervised learning and
requires little task-specific customization. We study a diverse range of
medical imaging tasks and simulate three realistic application scenarios using
retrospective data. REMEDIS exhibits significantly improved in-distribution
performance with up to 11.5% relative improvement in diagnostic accuracy over a
strong supervised baseline. More importantly, our strategy leads to strong
data-efficient generalization of medical imaging AI, matching strong supervised
baselines using between 1% to 33% of retraining data across tasks. These
results suggest that REMEDIS can significantly accelerate the life-cycle of
medical imaging AI development thereby presenting an important step forward for
medical imaging AI to deliver broad impact.",2205.09723v2,https://arxiv.org/pdf/2205.09723v2
"Distributed Multi-Agent Deep Reinforcement Learning for Robust
  Coordination against Noise","Yoshinari Motokawa, Toshiharu Sugawara","In multi-agent systems, noise reduction techniques are important for
improving the overall system reliability as agents are required to rely on
limited environmental information to develop cooperative and coordinated
behaviors with the surrounding agents. However, previous studies have often
applied centralized noise reduction methods to build robust and versatile
coordination in noisy multi-agent environments, while distributed and
decentralized autonomous agents are more plausible for real-world application.
In this paper, we introduce a \emph{distributed attentional actor architecture
model for a multi-agent system} (DA3-X), using which we demonstrate that agents
with DA3-X can selectively learn the noisy environment and behave
cooperatively. We experimentally evaluate the effectiveness of DA3-X by
comparing learning methods with and without DA3-X and show that agents with
DA3-X can achieve better performance than baseline agents. Furthermore, we
visualize heatmaps of \emph{attentional weights} from the DA3-X to analyze how
the decision-making process and coordinated behavior are influenced by noise.",2205.09705v1,https://arxiv.org/pdf/2205.09705v1
"Are Graph Representation Learning Methods Robust to Graph Sparsity and
  Asymmetric Node Information?","Pierre Sevestre, Marine Neyret","The growing popularity of Graph Representation Learning (GRL) methods has
resulted in the development of a large number of models applied to a miscellany
of domains. Behind this diversity of domains, there is a strong heterogeneity
of graphs, making it difficult to estimate the expected performance of a model
on a new graph, especially when the graph has distinctive characteristics that
have not been encountered in the benchmark yet. To address this, we have
developed an experimental pipeline, to assess the impact of a given property on
the models performances. In this paper, we use this pipeline to study the
effect of two specificities encountered on banks transactional graphs resulting
from the partial view a bank has on all the individuals and transactions
carried out on the market. These specific features are graph sparsity and
asymmetric node information. This study demonstrates the robustness of GRL
methods to these distinctive characteristics. We believe that this work can
ease the evaluation of GRL methods to specific characteristics and foster the
development of such methods on transactional graphs.",2205.09648v1,https://arxiv.org/pdf/2205.09648v1
"Improving Robustness against Real-World and Worst-Case Distribution
  Shifts through Decision Region Quantification","Leo Schwinn, Leon Bungert, An Nguyen, René Raab, Falk Pulsmeyer, Doina Precup, Björn Eskofier, Dario Zanca","The reliability of neural networks is essential for their use in
safety-critical applications. Existing approaches generally aim at improving
the robustness of neural networks to either real-world distribution shifts
(e.g., common corruptions and perturbations, spatial transformations, and
natural adversarial examples) or worst-case distribution shifts (e.g.,
optimized adversarial examples). In this work, we propose the Decision Region
Quantification (DRQ) algorithm to improve the robustness of any differentiable
pre-trained model against both real-world and worst-case distribution shifts in
the data. DRQ analyzes the robustness of local decision regions in the vicinity
of a given data point to make more reliable predictions. We theoretically
motivate the DRQ algorithm by showing that it effectively smooths spurious
local extrema in the decision surface. Furthermore, we propose an
implementation using targeted and untargeted adversarial attacks. An extensive
empirical evaluation shows that DRQ increases the robustness of adversarially
and non-adversarially trained models against real-world and worst-case
distribution shifts on several computer vision benchmark datasets.",2205.09619v1,https://arxiv.org/pdf/2205.09619v1
"Deep-learned orthogonal basis patterns for fast, noise-robust
  single-pixel imaging","Ritz Ann Aguilar, Damian Dailisan","Single-pixel imaging (SPI) is a novel, unconventional method that goes beyond
the notion of traditional cameras but can be computationally expensive and slow
for real-time applications. Deep learning has been proposed as an alternative
approach for solving the SPI reconstruction problem, but a detailed analysis of
its performance and generated basis patterns when used for SPI is limited. We
present a modified deep convolutional autoencoder network (DCAN) for SPI on
64x64 pixel images with up to 6.25% compression ratio and apply binary and
orthogonality regularizers during training. Training a DCAN with these
regularizers allows it to learn multiple measurement bases that have
combinations of binary or non-binary, and orthogonal or non-orthogonal
patterns. We compare the reconstruction quality, orthogonality of the patterns,
and robustness to noise of the resulting DCAN models to traditional SPI
reconstruction algorithms (such as Total Variation minimization and Fourier
Transform). Our DCAN models can be trained to be robust to noise while still
having fast enough reconstruction times (~3 ms per frame) to be viable for
real-time imaging.",2205.08736v1,https://arxiv.org/pdf/2205.08736v1
Robust Losses for Learning Value Functions,"Andrew Patterson, Victor Liao, Martha White","Most value function learning algorithms in reinforcement learning are based
on the mean squared (projected) Bellman error. However, squared errors are
known to be sensitive to outliers, both skewing the solution of the objective
and resulting in high-magnitude and high-variance gradients. To control these
high-magnitude updates, typical strategies in RL involve clipping gradients,
clipping rewards, rescaling rewards, or clipping errors. While these strategies
appear to be related to robust losses -- like the Huber loss -- they are built
on semi-gradient update rules which do not minimize a known loss. In this work,
we build on recent insights reformulating squared Bellman errors as a
saddlepoint optimization problem and propose a saddlepoint reformulation for a
Huber Bellman error and Absolute Bellman error. We start from a formalization
of robust losses, then derive sound gradient-based approaches to minimize these
losses in both the online off-policy prediction and control settings. We
characterize the solutions of the robust losses, providing insight into the
problem settings where the robust losses define notably better solutions than
the mean squared Bellman error. Finally, we show that the resulting
gradient-based algorithms are more stable, for both prediction and control,
with less sensitivity to meta-parameters.",2205.08464v2,https://arxiv.org/pdf/2205.08464v2
"Monotonicity Regularization: Improved Penalties and Novel Applications
  to Disentangled Representation Learning and Robust Classification","Joao Monteiro, Mohamed Osama Ahmed, Hossein Hajimirsadeghi, Greg Mori","We study settings where gradient penalties are used alongside risk
minimization with the goal of obtaining predictors satisfying different notions
of monotonicity. Specifically, we present two sets of contributions. In the
first part of the paper, we show that different choices of penalties define the
regions of the input space where the property is observed. As such, previous
methods result in models that are monotonic only in a small volume of the input
space. We thus propose an approach that uses mixtures of training instances and
random points to populate the space and enforce the penalty in a much larger
region. As a second set of contributions, we introduce regularization
strategies that enforce other notions of monotonicity in different settings. In
this case, we consider applications, such as image classification and
generative modeling, where monotonicity is not a hard constraint but can help
improve some aspects of the model. Namely, we show that inducing monotonicity
can be beneficial in applications such as: (1) allowing for controllable data
generation, (2) defining strategies to detect anomalous data, and (3)
generating explanations for predictions. Our proposed approaches do not
introduce relevant computational overhead while leading to efficient procedures
that provide extra benefits over baseline models.",2205.08247v1,https://arxiv.org/pdf/2205.08247v1
"Robust Perception Architecture Design for Automotive Cyber-Physical
  Systems","Joydeep Dey, Sudeep Pasricha","In emerging automotive cyber-physical systems (CPS), accurate environmental
perception is critical to achieving safety and performance goals. Enabling
robust perception for vehicles requires solving multiple complex problems
related to sensor selection/ placement, object detection, and sensor fusion.
Current methods address these problems in isolation, which leads to inefficient
solutions. We present PASTA, a novel framework for global co-optimization of
deep learning and sensing for dependable vehicle perception. Experimental
results with the Audi-TT and BMW-Minicooper vehicles show how PASTA can find
robust, vehicle-specific perception architecture solutions.",2205.08067v1,https://arxiv.org/pdf/2205.08067v1
"HelixADMET: a robust and endpoint extensible ADMET system incorporating
  self-supervised knowledge transfer","Shanzhuo Zhang, Zhiyuan Yan, Yueyang Huang, Lihang Liu, Donglong He, Wei Wang, Xiaomin Fang, Xiaonan Zhang, Fan Wang, Hua Wu, Haifeng Wang","Accurate ADMET (an abbreviation for ""absorption, distribution, metabolism,
excretion, and toxicity"") predictions can efficiently screen out undesirable
drug candidates in the early stage of drug discovery. In recent years, multiple
comprehensive ADMET systems that adopt advanced machine learning models have
been developed, providing services to estimate multiple endpoints. However,
those ADMET systems usually suffer from weak extrapolation ability. First, due
to the lack of labelled data for each endpoint, typical machine learning models
perform frail for the molecules with unobserved scaffolds. Second, most systems
only provide fixed built-in endpoints and cannot be customised to satisfy
various research requirements. To this end, we develop a robust and endpoint
extensible ADMET system, HelixADMET (H-ADMET). H-ADMET incorporates the concept
of self-supervised learning to produce a robust pre-trained model. The model is
then fine-tuned with a multi-task and multi-stage framework to transfer
knowledge between ADMET endpoints, auxiliary tasks, and self-supervised tasks.
Our results demonstrate that H-ADMET achieves an overall improvement of 4%,
compared with existing ADMET systems on comparable endpoints. Additionally, the
pre-trained model provided by H-ADMET can be fine-tuned to generate new and
customised ADMET endpoints, meeting various demands of drug research and
development requirements.",2205.08055v1,https://arxiv.org/pdf/2205.08055v1
Robust Testing in High-Dimensional Sparse Models,"Anand Jerry George, Clément L. Canonne","We consider the problem of robustly testing the norm of a high-dimensional
sparse signal vector under two different observation models. In the first
model, we are given $n$ i.i.d. samples from the distribution
$\mathcal{N}\left(\theta,I_d\right)$ (with unknown $\theta$), of which a small
fraction has been arbitrarily corrupted. Under the promise that
$\|\theta\|_0\le s$, we want to correctly distinguish whether $\|\theta\|_2=0$
or $\|\theta\|_2>\gamma$, for some input parameter $\gamma>0$. We show that any
algorithm for this task requires $n=\Omega\left(s\log\frac{ed}{s}\right)$
samples, which is tight up to logarithmic factors. We also extend our results
to other common notions of sparsity, namely, $\|\theta\|_q\le s$ for any $0 < q
< 2$. In the second observation model that we consider, the data is generated
according to a sparse linear regression model, where the covariates are i.i.d.
Gaussian and the regression coefficient (signal) is known to be $s$-sparse.
Here too we assume that an $\epsilon$-fraction of the data is arbitrarily
corrupted. We show that any algorithm that reliably tests the norm of the
regression coefficient requires at least $n=\Omega\left(\min(s\log
d,{1}/{\gamma^4})\right)$ samples. Our results show that the complexity of
testing in these two settings significantly increases under robustness
constraints. This is in line with the recent observations made in robust mean
testing and robust covariance testing.",2205.07488v2,https://arxiv.org/pdf/2205.07488v2
Robust Representation via Dynamic Feature Aggregation,"Haozhe Liu, Haoqin Ji, Yuexiang Li, Nanjun He, Haoqian Wu, Feng Liu, Linlin Shen, Yefeng Zheng","Deep convolutional neural network (CNN) based models are vulnerable to the
adversarial attacks. One of the possible reasons is that the embedding space of
CNN based model is sparse, resulting in a large space for the generation of
adversarial samples. In this study, we propose a method, denoted as Dynamic
Feature Aggregation, to compress the embedding space with a novel
regularization. Particularly, the convex combination between two samples are
regarded as the pivot for aggregation. In the embedding space, the selected
samples are guided to be similar to the representation of the pivot. On the
other side, to mitigate the trivial solution of such regularization, the last
fully-connected layer of the model is replaced by an orthogonal classifier, in
which the embedding codes for different classes are processed orthogonally and
separately. With the regularization and orthogonal classifier, a more compact
embedding space can be obtained, which accordingly improves the model
robustness against adversarial attacks. An averaging accuracy of 56.91% is
achieved by our method on CIFAR-10 against various attack methods, which
significantly surpasses a solid baseline (Mixup) by a margin of 37.31%. More
surprisingly, empirical results show that, the proposed method can also achieve
the state-of-the-art performance for out-of-distribution (OOD) detection, due
to the learned compact feature space. An F1 score of 0.937 is achieved by the
proposed method, when adopting CIFAR-10 as in-distribution (ID) dataset and
LSUN as OOD dataset. Code is available at
https://github.com/HaozheLiu-ST/DynamicFeatureAggregation.",2205.07466v1,https://arxiv.org/pdf/2205.07466v1
Policy Gradient Method For Robust Reinforcement Learning,"Yue Wang, Shaofeng Zou","This paper develops the first policy gradient method with global optimality
guarantee and complexity analysis for robust reinforcement learning under model
mismatch. Robust reinforcement learning is to learn a policy robust to model
mismatch between simulator and real environment. We first develop the robust
policy (sub-)gradient, which is applicable for any differentiable parametric
policy class. We show that the proposed robust policy gradient method converges
to the global optimum asymptotically under direct policy parameterization. We
further develop a smoothed robust policy gradient method and show that to
achieve an $\epsilon$-global optimum, the complexity is $\mathcal
O(\epsilon^{-3})$. We then extend our methodology to the general model-free
setting and design the robust actor-critic method with differentiable
parametric policy class and value function. We further characterize its
asymptotic convergence and sample complexity under the tabular setting.
Finally, we provide simulation results to demonstrate the robustness of our
methods.",2205.07344v1,https://arxiv.org/pdf/2205.07344v1
"RoMFAC: A robust mean-field actor-critic reinforcement learning against
  adversarial perturbations on states","Ziyuan Zhou, Guanjun Liu","Multi-agent deep reinforcement learning makes optimal decisions dependent on
system states observed by agents, but any uncertainty on the observations may
mislead agents to take wrong actions. The Mean-Field Actor-Critic reinforcement
learning (MFAC) is well-known in the multi-agent field since it can effectively
handle a scalability problem. However, it is sensitive to state perturbations
that can significantly degrade the team rewards. This work proposes a Robust
Mean-field Actor-Critic reinforcement learning (RoMFAC) that has two
innovations: 1) a new objective function of training actors, composed of a
\emph{policy gradient function} that is related to the expected cumulative
discount reward on sampled clean states and an \emph{action loss function} that
represents the difference between actions taken on clean and adversarial
states; and 2) a repetitive regularization of the action loss, ensuring the
trained actors to obtain excellent performance. Furthermore, this work proposes
a game model named a State-Adversarial Stochastic Game (SASG). Despite the Nash
equilibrium of SASG may not exist, adversarial perturbations to states in the
RoMFAC are proven to be defensible based on SASG. Experimental results show
that RoMFAC is robust against adversarial perturbations while maintaining its
competitive performance in environments without perturbations.",2205.07229v2,https://arxiv.org/pdf/2205.07229v2
"Sparsity-Aware Robust Normalized Subband Adaptive Filtering algorithms
  based on Alternating Optimization","Yi Yu, Zongxin Huang, Hongsen He, Yuriy Zakharov, Rodrigo C. de Lamare","This paper proposes a unified sparsity-aware robust normalized subband
adaptive filtering (SA-RNSAF) algorithm for identification of sparse systems
under impulsive noise. The proposed SA-RNSAF algorithm generalizes different
algorithms by defining the robust criterion and sparsity-aware penalty.
Furthermore, by alternating optimization of the parameters (AOP) of the
algorithm, including the step-size and the sparsity penalty weight, we develop
the AOP-SA-RNSAF algorithm, which not only exhibits fast convergence but also
obtains low steady-state misadjustment for sparse systems. Simulations in
various noise scenarios have verified that the proposed AOP-SA-RNSAF algorithm
outperforms existing techniques.",2205.07172v1,https://arxiv.org/pdf/2205.07172v1
"Robust Regularized Low-Rank Matrix Models for Regression and
  Classification","Hsin-Hsiung Huang, Feng Yu, Xing Fan, Teng Zhang","While matrix variate regression models have been studied in many existing
works, classical statistical and computational methods for the analysis of the
regression coefficient estimation are highly affected by high dimensional and
noisy matrix-valued predictors. To address these issues, this paper proposes a
framework of matrix variate regression models based on a rank constraint,
vector regularization (e.g., sparsity), and a general loss function with three
special cases considered: ordinary matrix regression, robust matrix regression,
and matrix logistic regression. We also propose an alternating projected
gradient descent algorithm. Based on analyzing our objective functions on
manifolds with bounded curvature, we show that the algorithm is guaranteed to
converge, all accumulation points of the iterates have estimation errors in the
order of $O(1/\sqrt{n})$ asymptotically and substantially attaining the minimax
rate. Our theoretical analysis can be applied to general optimization problems
on manifolds with bounded curvature and can be considered an important
technical contribution to this work. We validate the proposed method through
simulation studies and real image data examples.",2205.07106v1,https://arxiv.org/pdf/2205.07106v1
Evaluating Membership Inference Through Adversarial Robustness,"Zhaoxi Zhang, Leo Yu Zhang, Xufei Zheng, Bilal Hussain Abbasi, Shengshan Hu","The usage of deep learning is being escalated in many applications. Due to
its outstanding performance, it is being used in a variety of security and
privacy-sensitive areas in addition to conventional applications. One of the
key aspects of deep learning efficacy is to have abundant data. This trait
leads to the usage of data which can be highly sensitive and private, which in
turn causes wariness with regard to deep learning in the general public.
Membership inference attacks are considered lethal as they can be used to
figure out whether a piece of data belongs to the training dataset or not. This
can be problematic with regards to leakage of training data information and its
characteristics. To highlight the significance of these types of attacks, we
propose an enhanced methodology for membership inference attacks based on
adversarial robustness, by adjusting the directions of adversarial
perturbations through label smoothing under a white-box setting. We evaluate
our proposed method on three datasets: Fashion-MNIST, CIFAR-10, and CIFAR-100.
Our experimental results reveal that the performance of our method surpasses
that of the existing adversarial robustness-based method when attacking
normally trained models. Additionally, through comparing our technique with the
state-of-the-art metric-based membership inference methods, our proposed method
also shows better performance when attacking adversarially trained models. The
code for reproducing the results of this work is available at
\url{https://github.com/plll4zzx/Evaluating-Membership-Inference-Through-Adversarial-Robustness}.",2205.06986v1,https://arxiv.org/pdf/2205.06986v1
Robustness of Control Design via Bayesian Learning,"Nardos Ayele Ashenafi, Wankun Sirichotiyakul, Aykut C. Satici","In the realm of supervised learning, Bayesian learning has shown robust
predictive capabilities under input and parameter perturbations. Inspired by
these findings, we demonstrate the robustness properties of Bayesian learning
in the control search task. We seek to find a linear controller that stabilizes
a one-dimensional open-loop unstable stochastic system. We compare two methods
to deduce the controller: the first (deterministic) one assumes perfect
knowledge of system parameter and state, the second takes into account
uncertainties in both and employs Bayesian learning to compute a posterior
distribution for the controller.",2205.06896v1,https://arxiv.org/pdf/2205.06896v1
Smooth-Reduce: Leveraging Patches for Improved Certified Robustness,"Ameya Joshi, Minh Pham, Minsu Cho, Leonid Boytsov, Filipe Condessa, J. Zico Kolter, Chinmay Hegde","Randomized smoothing (RS) has been shown to be a fast, scalable technique for
certifying the robustness of deep neural network classifiers. However, methods
based on RS require augmenting data with large amounts of noise, which leads to
significant drops in accuracy. We propose a training-free, modified smoothing
approach, Smooth-Reduce, that leverages patching and aggregation to provide
improved classifier certificates. Our algorithm classifies overlapping patches
extracted from an input image, and aggregates the predicted logits to certify a
larger radius around the input. We study two aggregation schemes -- max and
mean -- and show that both approaches provide better certificates in terms of
certified accuracy, average certified radii and abstention rates as compared to
concurrent approaches. We also provide theoretical guarantees for such
certificates, and empirically show significant improvements over other
randomized smoothing methods that require expensive retraining. Further, we
extend our approach to videos and provide meaningful certificates for video
classifiers. A project page can be found at
https://nyu-dice-lab.github.io/SmoothReduce/",2205.06154v1,https://arxiv.org/pdf/2205.06154v1
"Sample Complexity Bounds for Robustly Learning Decision Lists against
  Evasion Attacks","Pascale Gourdeau, Varun Kanade, Marta Kwiatkowska, James Worrell","A fundamental problem in adversarial machine learning is to quantify how much
training data is needed in the presence of evasion attacks. In this paper we
address this issue within the framework of PAC learning, focusing on the class
of decision lists. Given that distributional assumptions are essential in the
adversarial setting, we work with probability distributions on the input data
that satisfy a Lipschitz condition: nearby points have similar probability. Our
key results illustrate that the adversary's budget (that is, the number of bits
it can perturb on each input) is a fundamental quantity in determining the
sample complexity of robust learning. Our first main result is a
sample-complexity lower bound: the class of monotone conjunctions (essentially
the simplest non-trivial hypothesis class on the Boolean hypercube) and any
superclass has sample complexity at least exponential in the adversary's
budget. Our second main result is a corresponding upper bound: for every fixed
$k$ the class of $k$-decision lists has polynomial sample complexity against a
$\log(n)$-bounded adversary. This sheds further light on the question of
whether an efficient PAC learning algorithm can always be used as an efficient
$\log(n)$-robust learning algorithm under the uniform distribution.",2205.06127v1,https://arxiv.org/pdf/2205.06127v1
"Towards Robust Unsupervised Disentanglement of Sequential Data -- A Case
  Study Using Music Audio","Yin-Jyun Luo, Sebastian Ewert, Simon Dixon","Disentangled sequential autoencoders (DSAEs) represent a class of
probabilistic graphical models that describes an observed sequence with dynamic
latent variables and a static latent variable. The former encode information at
a frame rate identical to the observation, while the latter globally governs
the entire sequence. This introduces an inductive bias and facilitates
unsupervised disentanglement of the underlying local and global factors. In
this paper, we show that the vanilla DSAE suffers from being sensitive to the
choice of model architecture and capacity of the dynamic latent variables, and
is prone to collapse the static latent variable. As a countermeasure, we
propose TS-DSAE, a two-stage training framework that first learns
sequence-level prior distributions, which are subsequently employed to
regularise the model and facilitate auxiliary objectives to promote
disentanglement. The proposed framework is fully unsupervised and robust
against the global factor collapse problem across a wide range of model
configurations. It also avoids typical solutions such as adversarial training
which usually involves laborious parameter tuning, and domain-specific data
augmentation. We conduct quantitative and qualitative evaluations to
demonstrate its robustness in terms of disentanglement on both artificial and
real-world music audio datasets.",2205.05871v2,https://arxiv.org/pdf/2205.05871v2
"Robustness Guarantees for Credal Bayesian Networks via Constraint
  Relaxation over Probabilistic Circuits","Hjalmar Wijk, Benjie Wang, Marta Kwiatkowska","In many domains, worst-case guarantees on the performance (e.g., prediction
accuracy) of a decision function subject to distributional shifts and
uncertainty about the environment are crucial. In this work we develop a method
to quantify the robustness of decision functions with respect to credal
Bayesian networks, formal parametric models of the environment where
uncertainty is expressed through credal sets on the parameters. In particular,
we address the maximum marginal probability (MARmax) problem, that is,
determining the greatest probability of an event (such as misclassification)
obtainable for parameters in the credal set. We develop a method to faithfully
transfer the problem into a constrained optimization problem on a probabilistic
circuit. By performing a simple constraint relaxation, we show how to obtain a
guaranteed upper bound on MARmax in linear time in the size of the circuit. We
further theoretically characterize this constraint relaxation in terms of the
original Bayesian network structure, which yields insight into the tightness of
the bound. We implement the method and provide experimental evidence that the
upper bound is often near tight and demonstrates improved scalability compared
to other methods.",2205.05793v1,https://arxiv.org/pdf/2205.05793v1
"Structured, flexible, and robust: benchmarking and improving large
  language models towards more human-like behavior in out-of-distribution
  reasoning tasks","Katherine M. Collins, Catherine Wong, Jiahai Feng, Megan Wei, Joshua B. Tenenbaum","Human language offers a powerful window into our thoughts -- we tell stories,
give explanations, and express our beliefs and goals through words. Abundant
evidence also suggests that language plays a developmental role in structuring
our learning. Here, we ask: how much of human-like thinking can be captured by
learning statistical patterns in language alone? We first contribute a new
challenge benchmark for comparing humans and distributional large language
models (LLMs). Our benchmark contains two problem-solving domains (planning and
explanation generation) and is designed to require generalization to new,
out-of-distribution problems expressed in language. We find that humans are far
more robust than LLMs on this benchmark. Next, we propose a hybrid
Parse-and-Solve model, which augments distributional LLMs with a structured
symbolic reasoning module. We find that this model shows more robust adaptation
to out-of-distribution planning problems, demonstrating the promise of hybrid
AI models for more human-like reasoning.",2205.05718v1,https://arxiv.org/pdf/2205.05718v1
Sibylvariant Transformations for Robust Text Classification,"Fabrice Harel-Canada, Muhammad Ali Gulzar, Nanyun Peng, Miryung Kim","The vast majority of text transformation techniques in NLP are inherently
limited in their ability to expand input space coverage due to an implicit
constraint to preserve the original class label. In this work, we propose the
notion of sibylvariance (SIB) to describe the broader set of transforms that
relax the label-preserving constraint, knowably vary the expected class, and
lead to significantly more diverse input distributions. We offer a unified
framework to organize all data transformations, including two types of SIB: (1)
Transmutations convert one discrete kind into another, (2) Mixture Mutations
blend two or more classes together. To explore the role of sibylvariance within
NLP, we implemented 41 text transformations, including several novel techniques
like Concept2Sentence and SentMix. Sibylvariance also enables a unique form of
adaptive training that generates new input mixtures for the most confused class
pairs, challenging the learner to differentiate with greater nuance. Our
experiments on six benchmark datasets strongly support the efficacy of
sibylvariance for generalization performance, defect detection, and adversarial
robustness.",2205.05137v1,https://arxiv.org/pdf/2205.05137v1
"Robust Data-Driven Output Feedback Control via Bootstrapped
  Multiplicative Noise","Benjamin Gravell, Iman Shames, Tyler Summers","We propose a robust data-driven output feedback control algorithm that
explicitly incorporates inherent finite-sample model estimate uncertainties
into the control design. The algorithm has three components: (1) a subspace
identification nominal model estimator; (2) a bootstrap resampling method that
quantifies non-asymptotic variance of the nominal model estimate; and (3) a
non-conventional robust control design method comprising a coupled optimal
dynamic output feedback filter and controller with multiplicative noise. A key
advantage of the proposed approach is that the system identification and robust
control design procedures both use stochastic uncertainty representations, so
that the actual inherent statistical estimation uncertainty directly aligns
with the uncertainty the robust controller is being designed against. Moreover,
the control design method accommodates a highly structured uncertainty
representation that can capture uncertainty shape more effectively than
existing approaches. We show through numerical experiments that the proposed
robust data-driven output feedback controller can significantly outperform a
certainty equivalent controller on various measures of sample complexity and
stability robustness.",2205.05119v1,https://arxiv.org/pdf/2205.05119v1
"StableDR: Stabilized Doubly Robust Learning for Recommendation on Data
  Missing Not at Random","Haoxuan Li, Chunyuan Zheng, Peng Wu","In recommender systems, users always choose the favorite items to rate, which
leads to data missing not at random and poses a great challenge for unbiased
evaluation and learning of prediction models. Currently, the doubly robust (DR)
methods have been widely studied and demonstrate superior performance. However,
in this paper, we show that DR methods are unstable and have unbounded bias,
variance, and generalization bounds to extremely small propensities. Moreover,
the fact that DR relies more on extrapolation will lead to suboptimal
performance. To address the above limitations while retaining double
robustness, we propose a stabilized doubly robust (StableDR) learning approach
with a weaker reliance on extrapolation. Theoretical analysis shows that
StableDR has bounded bias, variance, and generalization error bound
simultaneously under inaccurate imputed errors and arbitrarily small
propensities. In addition, we propose a novel learning approach for StableDR
that updates the imputation, propensity, and prediction models cyclically,
achieving more stable and accurate predictions. Extensive experiments show that
our approaches significantly outperform the existing methods.",2205.04701v3,https://arxiv.org/pdf/2205.04701v3
Robust Learning of Parsimonious Deep Neural Networks,"Valentin Frank Ingmar Guenter, Athanasios Sideris","We propose a simultaneous learning and pruning algorithm capable of
identifying and eliminating irrelevant structures in a neural network during
the early stages of training. Thus, the computational cost of subsequent
training iterations, besides that of inference, is considerably reduced. Our
method, based on variational inference principles using Gaussian scale mixture
priors on neural network weights, learns the variational posterior distribution
of Bernoulli random variables multiplying the units/filters similarly to
adaptive dropout. Our algorithm, ensures that the Bernoulli parameters
practically converge to either 0 or 1, establishing a deterministic final
network. We analytically derive a novel hyper-prior distribution over the prior
parameters that is crucial for their optimal selection and leads to consistent
pruning levels and prediction accuracy regardless of weight initialization or
the size of the starting network. We prove the convergence properties of our
algorithm establishing theoretical and practical pruning conditions. We
evaluate the proposed algorithm on the MNIST and CIFAR-10 data sets and the
commonly used fully connected and convolutional LeNet and VGG16 architectures.
The simulations show that our method achieves pruning levels on par with
state-of the-art methods for structured pruning, while maintaining better
test-accuracy and more importantly in a manner robust with respect to network
initialization and initial size.",2205.04650v3,https://arxiv.org/pdf/2205.04650v3
"How Does Frequency Bias Affect the Robustness of Neural Image
  Classifiers against Common Corruption and Adversarial Perturbations?","Alvin Chan, Yew-Soon Ong, Clement Tan","Model robustness is vital for the reliable deployment of machine learning
models in real-world applications. Recent studies have shown that data
augmentation can result in model over-relying on features in the low-frequency
domain, sacrificing performance against low-frequency corruptions, highlighting
a connection between frequency and robustness. Here, we take one step further
to more directly study the frequency bias of a model through the lens of its
Jacobians and its implication to model robustness. To achieve this, we propose
Jacobian frequency regularization for models' Jacobians to have a larger ratio
of low-frequency components. Through experiments on four image datasets, we
show that biasing classifiers towards low (high)-frequency components can bring
performance gain against high (low)-frequency corruption and adversarial
perturbation, albeit with a tradeoff in performance for low (high)-frequency
corruption. Our approach elucidates a more direct connection between the
frequency bias and robustness of deep learning models.",2205.04533v1,https://arxiv.org/pdf/2205.04533v1
"Robustness of Humans and Machines on Object Recognition with Extreme
  Image Transformations","Dakarai Crowder, Girik Malik","Recent neural network architectures have claimed to explain data from the
human visual cortex. Their demonstrated performance is however still limited by
the dependence on exploiting low-level features for solving visual tasks. This
strategy limits their performance in case of out-of-distribution/adversarial
data. Humans, meanwhile learn abstract concepts and are mostly unaffected by
even extreme image distortions. Humans and networks employ strikingly different
strategies to solve visual tasks. To probe this, we introduce a novel set of
image transforms and evaluate humans and networks on an object recognition
task. We found performance for a few common networks quickly decreases while
humans are able to recognize objects with a high accuracy.",2205.05167v2,https://arxiv.org/pdf/2205.05167v2
"""The World Is Its Own Best Model"": Robust Real-World Manipulation
  Through Online Behavior Selection","Manuel Baum, Oliver Brock","Robotic manipulation behavior should be robust to disturbances that violate
high-level task-structure. Such robustness can be achieved by constantly
monitoring the environment to observe the discrete high-level state of the
task. This is possible because different phases of a task are characterized by
different sensor patterns and by monitoring these patterns a robot can decide
which controllers to execute in the moment. This relaxes assumptions about the
temporal sequence of those controllers and makes behavior robust to unforeseen
disturbances. We implement this idea as probabilistic filter over discrete
states where each state is direcly associated with a controller. Based on this
framework we present a robotic system that is able to open a drawer and grasp
tennis balls from it in a surprisingly robust way.",2205.04172v1,https://arxiv.org/pdf/2205.04172v1
Hardware-Robust In-RRAM-Computing for Object Detection,"Yu-Hsiang Chiang, Cheng En Ni, Yun Sung, Tuo-Hung Hou, Tian-Sheuan Chang, Shyh Jye Jou","In-memory computing is becoming a popular architecture for deep-learning
hardware accelerators recently due to its highly parallel computing, low power,
and low area cost. However, in-RRAM computing (IRC) suffered from large device
variation and numerous nonideal effects in hardware. Although previous
approaches including these effects in model training successfully improved
variation tolerance, they only considered part of the nonideal effects and
relatively simple classification tasks. This paper proposes a joint hardware
and software optimization strategy to design a hardware-robust IRC macro for
object detection. We lower the cell current by using a low word-line voltage to
enable a complete convolution calculation in one operation that minimizes the
impact of nonlinear addition. We also implement ternary weight mapping and
remove batch normalization for better tolerance against device variation, sense
amplifier variation, and IR drop problem. An extra bias is included to overcome
the limitation of the current sensing range. The proposed approach has been
successfully applied to a complex object detection task with only 3.85\% mAP
drop, whereas a naive design suffers catastrophic failure under these nonideal
effects.",2205.03996v1,https://arxiv.org/pdf/2205.03996v1
"Robust (Controlled) Table-to-Text Generation with Structure-Aware
  Equivariance Learning","Fei Wang, Zhewei Xu, Pedro Szekely, Muhao Chen","Controlled table-to-text generation seeks to generate natural language
descriptions for highlighted subparts of a table. Previous SOTA systems still
employ a sequence-to-sequence generation method, which merely captures the
table as a linear structure and is brittle when table layouts change. We seek
to go beyond this paradigm by (1) effectively expressing the relations of
content pieces in the table, and (2) making our model robust to
content-invariant structural transformations. Accordingly, we propose an
equivariance learning framework, which encodes tables with a structure-aware
self-attention mechanism. This prunes the full self-attention structure into an
order-invariant graph attention that captures the connected graph structure of
cells belonging to the same row or column, and it differentiates between
relevant cells and irrelevant cells from the structural perspective. Our
framework also modifies the positional encoding mechanism to preserve the
relative position of tokens in the same cell but enforce position invariance
among different cells. Our technology is free to be plugged into existing
table-to-text generation models, and has improved T5-based models to offer
better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo,
we preserve promising performance, while previous SOTA systems, even with
transformation-based data augmentation, have seen significant performance
drops. Our code is available at https://github.com/luka-group/Lattice.",2205.03972v1,https://arxiv.org/pdf/2205.03972v1
RoViST:Learning Robust Metrics for Visual Storytelling,"Eileen Wang, Caren Han, Josiah Poon","Visual storytelling (VST) is the task of generating a story paragraph that
describes a given image sequence. Most existing storytelling approaches have
evaluated their models using traditional natural language generation metrics
like BLEU or CIDEr. However, such metrics based on n-gram matching tend to have
poor correlation with human evaluation scores and do not explicitly consider
other criteria necessary for storytelling such as sentence structure or topic
coherence. Moreover, a single score is not enough to assess a story as it does
not inform us about what specific errors were made by the model. In this paper,
we propose 3 evaluation metrics sets that analyses which aspects we would look
for in a good story: 1) visual grounding, 2) coherence, and 3) non-redundancy.
We measure the reliability of our metric sets by analysing its correlation with
human judgement scores on a sample of machine stories obtained from 4
state-of-the-arts models trained on the Visual Storytelling Dataset (VIST). Our
metric sets outperforms other metrics on human correlation, and could be served
as a learning based evaluation metric set that is complementary to existing
rule-based metrics.",2205.03774v1,https://arxiv.org/pdf/2205.03774v1
"Designing Robust Biotechnological Processes Regarding Variabilities
  using Multi-Objective Optimization Applied to a Biopharmaceutical Seed Train
  Design","Tanja Hernández Rodríguez, Anton Sekulic, Markus Lange-Hegermann, Björn Frahm","Development and optimization of biopharmaceutical production processes with
cell cultures is cost- and time-consuming and often performed rather
empirically. Efficient optimization of multiple-objectives like process time,
viable cell density, number of operating steps & cultivation scales, required
medium, amount of product as well as product quality depicts a promising
approach. This contribution presents a workflow which couples uncertainty-based
upstream simulation and Bayes optimization using Gaussian processes. Its
application is demonstrated in a simulation case study for a relevant
industrial task in process development, the design of a robust cell culture
expansion process (seed train), meaning that despite uncertainties and
variabilities concerning cell growth, low variations of viable cell density
during the seed train are obtained. Compared to a non-optimized reference seed
train, the optimized process showed much lower deviation rates regarding viable
cell densities (<~10% instead of 41.7%) using 5 or 4 shake flask scales and
seed train duration could be reduced by 56 h from 576 h to 520 h. Overall, it
is shown that applying Bayes optimization allows for optimization of a
multi-objective optimization function with several optimizable input variables
and under a considerable amount of constraints with a low computational effort.
This approach provides the potential to be used in form of a decision tool,
e.g. for the choice of an optimal and robust seed train design or for further
optimization tasks within process development.",2205.03261v1,https://arxiv.org/pdf/2205.03261v1
"Can collaborative learning be private, robust and scalable?","Dmitrii Usynin, Helena Klause, Johannes C. Paetzold, Daniel Rueckert, Georgios Kaissis","In federated learning for medical image analysis, the safety of the learning
protocol is paramount. Such settings can often be compromised by adversaries
that target either the private data used by the federation or the integrity of
the model itself. This requires the medical imaging community to develop
mechanisms to train collaborative models that are private and robust against
adversarial data. In response to these challenges, we propose a practical
open-source framework to study the effectiveness of combining differential
privacy, model compression and adversarial training to improve the robustness
of models against adversarial samples under train- and inference-time attacks.
Using our framework, we achieve competitive model performance, a significant
reduction in model's size and an improved empirical adversarial robustness
without a severe performance degradation, critical in medical image analysis.",2205.02652v2,https://arxiv.org/pdf/2205.02652v2
"M2R2: Missing-Modality Robust emotion Recognition framework with
  iterative data augmentation",Ning Wang,"This paper deals with the utterance-level modalities missing problem with
uncertain patterns on emotion recognition in conversation (ERC) task. Present
models generally predict the speaker's emotions by its current utterance and
context, which is degraded by modality missing considerably. Our work proposes
a framework Missing-Modality Robust emotion Recognition (M2R2), which trains
emotion recognition model with iterative data augmentation by learned common
representation. Firstly, a network called Party Attentive Network (PANet) is
designed to classify emotions, which tracks all the speakers' states and
context. Attention mechanism between speaker with other participants and
dialogue topic is used to decentralize dependence on multi-time and multi-party
utterances instead of the possible incomplete one. Moreover, the Common
Representation Learning (CRL) problem is defined for modality-missing problem.
Data imputation methods improved by the adversarial strategy are used here to
construct extra features to augment data. Extensive experiments and case
studies validate the effectiveness of our methods over baselines for
modality-missing emotion recognition on two different datasets.",2205.02524v1,https://arxiv.org/pdf/2205.02524v1
"Structural Extensions of Basis Pursuit: Guarantees on Adversarial
  Robustness","Dávid Szeghy, Mahmoud Aslan, Áron Fóthi, Balázs Mészáros, Zoltán Ádám Milacski, András Lőrincz","While deep neural networks are sensitive to adversarial noise, sparse coding
using the Basis Pursuit (BP) method is robust against such attacks, including
its multi-layer extensions. We prove that the stability theorem of BP holds
upon the following generalizations: (i) the regularization procedure can be
separated into disjoint groups with different weights, (ii) neurons or full
layers may form groups, and (iii) the regularizer takes various generalized
forms of the $\ell_1$ norm. This result provides the proof for the
architectural generalizations of Cazenavette et al. (2021), including (iv) an
approximation of the complete architecture as a shallow sparse coding network.
Due to this approximation, we settled to experimenting with shallow networks
and studied their robustness against the Iterative Fast Gradient Sign Method on
a synthetic dataset and MNIST. We introduce classification based on the
$\ell_2$ norms of the groups and show numerically that it can be accurate and
offers considerable speedups. In this family, linear transformer shows the best
performance. Based on the theoretical results and the numerical simulations, we
highlight numerical matters that may improve performance further.",2205.08955v1,https://arxiv.org/pdf/2205.08955v1
"Sequential Importance Sampling for Hybrid Model Bayesian Inference to
  Support Bioprocess Mechanism Learning and Robust Control","Wei Xie, Keqi Wang, Hua Zheng, Ben Feng","Driven by the critical needs of biomanufacturing 4.0, we introduce a
probabilistic knowledge graph hybrid model characterizing the risk- and
science-based understanding of bioprocess mechanisms. It can faithfully capture
the important properties, including nonlinear reactions, partially observed
state, and nonstationary dynamics. Given very limited real process
observations, we derive a posterior distribution quantifying model estimation
uncertainty. To avoid the evaluation of intractable likelihoods, Approximate
Bayesian Computation sampling with Sequential Monte Carlo (ABC-SMC) is utilized
to approximate the posterior distribution. Under high stochastic and model
uncertainties, it is computationally expensive to match output trajectories.
Therefore, we create a linear Gaussian dynamic Bayesian network (LG-DBN)
auxiliary likelihood-based ABC-SMC approach. Through matching the summary
statistics driven through LG-DBN likelihood that can capture critical
interactions and variations, the proposed algorithm can accelerate hybrid model
inference, support process monitoring, and facilitate mechanism learning and
robust control.",2205.02410v4,https://arxiv.org/pdf/2205.02410v4
Robust Conversational Agents against Imperceptible Toxicity Triggers,"Ninareh Mehrabi, Ahmad Beirami, Fred Morstatter, Aram Galstyan","Warning: this paper contains content that maybe offensive or upsetting.
Recent research in Natural Language Processing (NLP) has advanced the
development of various toxicity detection models with the intention of
identifying and mitigating toxic language from existing systems. Despite the
abundance of research in this area, less attention has been given to
adversarial attacks that force the system to generate toxic language and the
defense against them. Existing work to generate such attacks is either based on
human-generated attacks which is costly and not scalable or, in case of
automatic attacks, the attack vector does not conform to human-like language,
which can be detected using a language model loss. In this work, we propose
attacks against conversational agents that are imperceptible, i.e., they fit
the conversation in terms of coherency, relevancy, and fluency, while they are
effective and scalable, i.e., they can automatically trigger the system into
generating toxic language. We then propose a defense mechanism against such
attacks which not only mitigates the attack but also attempts to maintain the
conversational flow. Through automatic and human evaluations, we show that our
defense is effective at avoiding toxic language generation even against
imperceptible toxicity triggers while the generated language fits the
conversation in terms of coherency and relevancy. Lastly, we establish the
generalizability of such a defense mechanism on language generation models
beyond conversational agents.",2205.02392v1,https://arxiv.org/pdf/2205.02392v1
"Machine Learning based Framework for Robust Price-Sensitivity Estimation
  with Application to Airline Pricing","Ravi Kumar, Shahin Boluki, Karl Isler, Jonas Rauch, Darius Walczak","We consider the problem of dynamic pricing of a product in the presence of
feature-dependent price sensitivity. Developing practical algorithms that can
estimate price elasticities robustly, especially when information about no
purchases (losses) is not available, to drive such automated pricing systems is
a challenge faced by many industries. Based on the Poisson semi-parametric
approach, we construct a flexible yet interpretable demand model where the
price related part is parametric while the remaining (nuisance) part of the
model is non-parametric and can be modeled via sophisticated machine learning
(ML) techniques. The estimation of price-sensitivity parameters of this model
via direct one-stage regression techniques may lead to biased estimates due to
regularization. To address this concern, we propose a two-stage estimation
methodology which makes the estimation of the price-sensitivity parameters
robust to biases in the estimators of the nuisance parameters of the model. In
the first-stage we construct estimators of observed purchases and prices given
the feature vector using sophisticated ML estimators such as deep neural
networks. Utilizing the estimators from the first-stage, in the second-stage we
leverage a Bayesian dynamic generalized linear model to estimate the
price-sensitivity parameters. We test the performance of the proposed
estimation schemes on simulated and real sales transaction data from the
Airline industry. Our numerical studies demonstrate that our proposed two-stage
approach reduces the estimation error in price-sensitivity parameters from 25\%
to 4\% in realistic simulation settings. The two-stage estimation techniques
proposed in this work allows practitioners to leverage modern ML techniques to
robustly estimate price-sensitivities while still maintaining interpretability
and allowing ease of validation of its various constituent parts.",2205.01875v2,https://arxiv.org/pdf/2205.01875v2
"Data Determines Distributional Robustness in Contrastive Language Image
  Pre-training (CLIP)","Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, Ludwig Schmidt","Contrastively trained language-image models such as CLIP, ALIGN, and BASIC
have demonstrated unprecedented robustness to multiple challenging natural
distribution shifts. Since these language-image models differ from previous
training approaches in several ways, an important question is what causes the
large robustness gains. We answer this question via a systematic experimental
investigation. Concretely, we study five different possible causes for the
robustness gains: (i) the training set size, (ii) the training distribution,
(iii) language supervision at training time, (iv) language supervision at test
time, and (v) the contrastive loss function. Our experiments show that the more
diverse training distribution is the main cause for the robustness gains, with
the other factors contributing little to no robustness. Beyond our experimental
results, we also introduce ImageNet-Captions, a version of ImageNet with
original text annotations from Flickr, to enable further controlled experiments
of language-image training.",2205.01397v2,https://arxiv.org/pdf/2205.01397v2
FedRN: Exploiting k-Reliable Neighbors Towards Robust Federated Learning,"SangMook Kim, Wonyoung Shin, Soohyuk Jang, Hwanjun Song, Se-Young Yun","Robustness is becoming another important challenge of federated learning in
that the data collection process in each client is naturally accompanied by
noisy labels. However, it is far more complex and challenging owing to varying
levels of data heterogeneity and noise over clients, which exacerbates the
client-to-client performance discrepancy. In this work, we propose a robust
federated learning method called FedRN, which exploits k-reliable neighbors
with high data expertise or similarity. Our method helps mitigate the gap
between low- and high-performance clients by training only with a selected set
of clean examples, identified by their ensembled mixture models. We demonstrate
the superiority of FedRN via extensive evaluations on three real-world or
synthetic benchmark datasets. Compared with existing robust training methods,
the results show that FedRN significantly improves the test accuracy in the
presence of noisy labels.",2205.01310v2,https://arxiv.org/pdf/2205.01310v2
"Performance Weighting for Robust Federated Learning Against Corrupted
  Sources","Dimitris Stripelis, Marcin Abram, Jose Luis Ambite","Federated Learning has emerged as a dominant computational paradigm for
distributed machine learning. Its unique data privacy properties allow us to
collaboratively train models while offering participating clients certain
privacy-preserving guarantees. However, in real-world applications, a federated
environment may consist of a mixture of benevolent and malicious clients, with
the latter aiming to corrupt and degrade federated model's performance.
Different corruption schemes may be applied such as model poisoning and data
corruption. Here, we focus on the latter, the susceptibility of federated
learning to various data corruption attacks. We show that the standard global
aggregation scheme of local weights is inefficient in the presence of corrupted
clients. To mitigate this problem, we propose a class of task-oriented
performance-based methods computed over a distributed validation dataset with
the goal to detect and mitigate corrupted clients. Specifically, we construct a
robust weight aggregation scheme based on geometric mean and demonstrate its
effectiveness under random label shuffling and targeted label flipping attacks.",2205.01184v1,https://arxiv.org/pdf/2205.01184v1
DDDM: a Brain-Inspired Framework for Robust Classification,"Xiyuan Chen, Xingyu Li, Yi Zhou, Tianming Yang","Despite their outstanding performance in a broad spectrum of real-world
tasks, deep artificial neural networks are sensitive to input noises,
particularly adversarial perturbations. On the contrary, human and animal
brains are much less vulnerable. In contrast to the one-shot inference
performed by most deep neural networks, the brain often solves decision-making
with an evidence accumulation mechanism that may trade time for accuracy when
facing noisy inputs. The mechanism is well described by the Drift-Diffusion
Model (DDM). In the DDM, decision-making is modeled as a process in which noisy
evidence is accumulated toward a threshold. Drawing inspiration from the DDM,
we propose the Dropout-based Drift-Diffusion Model (DDDM) that combines
test-phase dropout and the DDM for improving the robustness for arbitrary
neural networks. The dropouts create temporally uncorrelated noises in the
network that counter perturbations, while the evidence accumulation mechanism
guarantees a reasonable decision accuracy. Neural networks enhanced with the
DDDM tested in image, speech, and text classification tasks all significantly
outperform their native counterparts, demonstrating the DDDM as a task-agnostic
defense against adversarial attacks.",2205.10117v1,https://arxiv.org/pdf/2205.10117v1
"A Short and General Duality Proof for Wasserstein Distributionally
  Robust Optimization","Luhao Zhang, Jincheng Yang, Rui Gao","We present a general duality result for Wasserstein distributionally robust
optimization that holds for any Kantorovich transport cost, measurable loss
function, and nominal probability distribution. Assuming an interchangeability
principle inherent in existing duality results, our proof only uses
one-dimensional convex analysis. Furthermore, we demonstrate that the
interchangeability principle holds if and only if certain measurable projection
and weak measurable selection conditions are satisfied. To illustrate the
broader applicability of our approach, we provide a rigorous treatment of
duality results in distributionally robust Markov decision processes and
distributionally robust multistage stochastic programming. Additionally, we
extend our analysis to other problems such as infinity-Wasserstein
distributionally robust optimization, risk-averse optimization, and globalized
distributionally robust counterpart.",2205.00362v4,https://arxiv.org/pdf/2205.00362v4
"Bridging Differential Privacy and Byzantine-Robustness via Model
  Aggregation","Heng Zhu, Qing Ling","This paper aims at jointly addressing two seemly conflicting issues in
federated learning: differential privacy (DP) and Byzantine-robustness, which
are particularly challenging when the distributed data are non-i.i.d.
(independent and identically distributed). The standard DP mechanisms add noise
to the transmitted messages, and entangles with robust stochastic gradient
aggregation to defend against Byzantine attacks. In this paper, we decouple the
two issues via robust stochastic model aggregation, in the sense that our
proposed DP mechanisms and the defense against Byzantine attacks have separated
influence on the learning performance. Leveraging robust stochastic model
aggregation, at each iteration, each worker calculates the difference between
the local model and the global one, followed by sending the element-wise signs
to the master node, which enables robustness to Byzantine attacks. Further, we
design two DP mechanisms to perturb the uploaded signs for the purpose of
privacy preservation, and prove that they are $(\epsilon,0)$-DP by exploiting
the properties of noise distributions. With the tools of Moreau envelop and
proximal point projection, we establish the convergence of the proposed
algorithm when the cost function is nonconvex. We analyze the trade-off between
privacy preservation and learning performance, and show that the influence of
our proposed DP mechanisms is decoupled with that of robust stochastic model
aggregation. Numerical experiments demonstrate the effectiveness of the
proposed algorithm.",2205.00107v2,https://arxiv.org/pdf/2205.00107v2
"How Robust is Neural Machine Translation to Language Imbalance in
  Multilingual Tokenizer Training?","Shiyue Zhang, Vishrav Chaudhary, Naman Goyal, James Cross, Guillaume Wenzek, Mohit Bansal, Francisco Guzman","A multilingual tokenizer is a fundamental component of multilingual neural
machine translation. It is trained from a multilingual corpus. Since a skewed
data distribution is considered to be harmful, a sampling strategy is usually
used to balance languages in the corpus. However, few works have systematically
answered how language imbalance in tokenizer training affects downstream
performance. In this work, we analyze how translation performance changes as
the data ratios among languages vary in the tokenizer training corpus. We find
that while relatively better performance is often observed when languages are
more equally sampled, the downstream performance is more robust to language
imbalance than we usually expected. Two features, UNK rate and closeness to the
character level, can warn of poor downstream performance before performing the
task. We also distinguish language sampling for tokenizer training from
sampling for model training and show that the model is more sensitive to the
latter.",2204.14268v2,https://arxiv.org/pdf/2204.14268v2
Robust Solutions for Multi-Defender Stackelberg Security Games,"Dolev Mutzari, Yonatan Aumann, Sarit Kraus","Multi-defender Stackelberg Security Games (MSSG) have recently gained
increasing attention in the literature. However, the solutions offered to date
are highly sensitive, wherein even small perturbations in the attacker's
utility or slight uncertainties thereof can dramatically change the defenders'
resulting payoffs and alter the equilibrium. In this paper, we introduce a
robust model for MSSGs, which admits solutions that are resistant to small
perturbations or uncertainties in the game's parameters. First, we formally
define the notion of robustness, as well as the robust MSSG model. Then, for
the non-cooperative setting, we prove the existence of a robust approximate
equilibrium in any such game, and provide an efficient construction thereof.
For the cooperative setting, we show that any such game admits a robust
approximate alpha-core, provide an efficient construction thereof, and prove
that stronger types of the core may be empty. Interestingly, the robust
solutions can substantially increase the defenders' utilities over those of the
non-robust ones.",2204.14000v2,https://arxiv.org/pdf/2204.14000v2
"RoSA: A Robust Self-Aligned Framework for Node-Node Graph Contrastive
  Learning","Yun Zhu, Jianhao Guo, Fei Wu, Siliang Tang","Graph contrastive learning has gained significant progress recently. However,
existing works have rarely explored non-aligned node-node contrasting. In this
paper, we propose a novel graph contrastive learning method named RoSA that
focuses on utilizing non-aligned augmented views for node-level representation
learning. First, we leverage the earth mover's distance to model the minimum
effort to transform the distribution of one view to the other as our
contrastive objective, which does not require alignment between views. Then we
introduce adversarial training as an auxiliary method to increase sampling
diversity and enhance the robustness of our model. Experimental results show
that RoSA outperforms a series of graph contrastive learning frameworks on
homophilous, non-homophilous and dynamic graphs, which validates the
effectiveness of our work. To the best of our awareness, RoSA is the first work
focuses on the non-aligned node-node graph contrastive learning problem. Our
codes are available at:
\href{https://github.com/ZhuYun97/RoSA}{\texttt{https://github.com/ZhuYun97/RoSA}}",2204.13846v1,https://arxiv.org/pdf/2204.13846v1
Formulating Robustness Against Unforeseen Attacks,"Sihui Dai, Saeed Mahloujifar, Prateek Mittal","Existing defenses against adversarial examples such as adversarial training
typically assume that the adversary will conform to a specific or known threat
model, such as $\ell_p$ perturbations within a fixed budget. In this paper, we
focus on the scenario where there is a mismatch in the threat model assumed by
the defense during training, and the actual capabilities of the adversary at
test time. We ask the question: if the learner trains against a specific
""source"" threat model, when can we expect robustness to generalize to a
stronger unknown ""target"" threat model during test-time? Our key contribution
is to formally define the problem of learning and generalization with an
unforeseen adversary, which helps us reason about the increase in adversarial
risk from the conventional perspective of a known adversary. Applying our
framework, we derive a generalization bound which relates the generalization
gap between source and target threat models to variation of the feature
extractor, which measures the expected maximum difference between extracted
features across a given threat model. Based on our generalization bound, we
propose variation regularization (VR) which reduces variation of the feature
extractor across the source threat model during training. We empirically
demonstrate that using VR can lead to improved generalization to unforeseen
attacks during test-time, and combining VR with perceptual adversarial training
(Laidlaw et al., 2021) achieves state-of-the-art robustness on unforeseen
attacks. Our code is publicly available at
https://github.com/inspire-group/variation-regularization.",2204.13779v3,https://arxiv.org/pdf/2204.13779v3
"KING: Generating Safety-Critical Driving Scenarios for Robust Imitation
  via Kinematics Gradients","Niklas Hanselmann, Katrin Renz, Kashyap Chitta, Apratim Bhattacharyya, Andreas Geiger","Simulators offer the possibility of safe, low-cost development of
self-driving systems. However, current driving simulators exhibit na\""ive
behavior models for background traffic. Hand-tuned scenarios are typically
added during simulation to induce safety-critical situations. An alternative
approach is to adversarially perturb the background traffic trajectories. In
this paper, we study this approach to safety-critical driving scenario
generation using the CARLA simulator. We use a kinematic bicycle model as a
proxy to the simulator's true dynamics and observe that gradients through this
proxy model are sufficient for optimizing the background traffic trajectories.
Based on this finding, we propose KING, which generates safety-critical driving
scenarios with a 20% higher success rate than black-box optimization. By
solving the scenarios generated by KING using a privileged rule-based expert
algorithm, we obtain training data for an imitation learning policy. After
fine-tuning on this new data, we show that the policy becomes better at
avoiding collisions. Importantly, our generated data leads to reduced
collisions on both held-out scenarios generated via KING as well as traditional
hand-crafted scenarios, demonstrating improved robustness.",2204.13683v1,https://arxiv.org/pdf/2204.13683v1
"Improving the Robustness of Federated Learning for Severely Imbalanced
  Datasets","Debasrita Chakraborty, Ashish Ghosh","With the ever increasing data deluge and the success of deep neural networks,
the research of distributed deep learning has become pronounced. Two common
approaches to achieve this distributed learning is synchronous and asynchronous
weight update. In this manuscript, we have explored very simplistic synchronous
weight update mechanisms. It has been seen that with an increasing number of
worker nodes, the performance degrades drastically. This effect has been
studied in the context of extreme imbalanced classification (e.g. outlier
detection). In practical cases, the assumed conditions of i.i.d. may not be
fulfilled. There may also arise global class imbalance situations like that of
outlier detection where the local servers receive severely imbalanced data and
may not get any samples from the minority class. In that case, the DNNs in the
local servers will get completely biased towards the majority class that they
receive. This would highly impact the learning at the parameter server (which
practically does not see any data). It has been observed that in a parallel
setting if one uses the existing federated weight update mechanisms at the
parameter server, the performance degrades drastically with the increasing
number of worker nodes. This is mainly because, with the increasing number of
nodes, there is a high chance that one worker node gets a very small portion of
the data, either not enough to train the model without overfitting or having a
highly imbalanced class distribution. The chapter, hence, proposes a workaround
to this problem by introducing the concept of adaptive cost-sensitive momentum
averaging. It is seen that for the proposed system, there was no to minimal
degradation in performance while most of the other methods hit their bottom
performance before that.",2204.13414v1,https://arxiv.org/pdf/2204.13414v1
"Robust stabilization of polytopic systems via fast and reliable neural
  network-based approximations","Filippo Fabiani, Paul J. Goulart","We consider the design of fast and reliable neural network (NN)-based
approximations of traditional stabilizing controllers for linear systems with
polytopic uncertainty, including control laws with variable structure and those
based on a (minimal) selection policy. Building upon recent approaches for the
design of reliable control surrogates with guaranteed structural properties, we
develop a systematic procedure to certify the closed-loop stability and
performance of a linear uncertain system when a trained rectified linear unit
(ReLU)-based approximation replaces such traditional controllers. First, we
provide a sufficient condition, which involves the worst-case approximation
error between ReLU-based and traditional controller-based state-to-input
mappings, ensuring that the system is ultimately bounded within a set with
adjustable size and convergence rate. Then, we develop an offline,
mixed-integer optimization-based method that allows us to compute that quantity
exactly.",2204.13209v2,https://arxiv.org/pdf/2204.13209v2
"Variational Kalman Filtering with Hinf-Based Correction for Robust
  Bayesian Learning in High Dimensions","Niladri Das, Jed A. Duersch, Thomas A. Catanach","In this paper, we address the problem of convergence of sequential
variational inference filter (VIF) through the application of a robust
variational objective and Hinf-norm based correction for a linear Gaussian
system. As the dimension of state or parameter space grows, performing the full
Kalman update with the dense covariance matrix for a large scale system
requires increased storage and computational complexity, making it impractical.
The VIF approach, based on mean-field Gaussian variational inference, reduces
this burden through the variational approximation to the covariance usually in
the form of a diagonal covariance approximation. The challenge is to retain
convergence and correct for biases introduced by the sequential VIF steps. We
desire a framework that improves feasibility while still maintaining reasonable
proximity to the optimal Kalman filter as data is assimilated. To accomplish
this goal, a Hinf-norm based optimization perturbs the VIF covariance matrix to
improve robustness. This yields a novel VIF- Hinf recursion that employs
consecutive variational inference and Hinf based optimization steps. We explore
the development of this method and investigate a numerical example to
illustrate the effectiveness of the proposed filter.",2204.13089v1,https://arxiv.org/pdf/2204.13089v1
RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning,"Marc Rigter, Bruno Lacerda, Nick Hawes","Offline reinforcement learning (RL) aims to find performant policies from
logged data without further environment interaction. Model-based algorithms,
which learn a model of the environment from the dataset and perform
conservative policy optimisation within that model, have emerged as a promising
approach to this problem. In this work, we present Robust Adversarial
Model-Based Offline RL (RAMBO), a novel approach to model-based offline RL. We
formulate the problem as a two-player zero sum game against an adversarial
environment model. The model is trained to minimise the value function while
still accurately predicting the transitions in the dataset, forcing the policy
to act conservatively in areas not covered by the dataset. To approximately
solve the two-player game, we alternate between optimising the policy and
adversarially optimising the model. The problem formulation that we address is
theoretically grounded, resulting in a probably approximately correct (PAC)
performance guarantee and a pessimistic value function which lower bounds the
value function in the true environment. We evaluate our approach on widely
studied offline RL benchmarks, and demonstrate that it outperforms existing
state-of-the-art baselines.",2204.12581v3,https://arxiv.org/pdf/2204.12581v3
Streaming Algorithms for High-Dimensional Robust Statistics,"Ilias Diakonikolas, Daniel M. Kane, Ankit Pensia, Thanasis Pittas","We study high-dimensional robust statistics tasks in the streaming model. A
recent line of work obtained computationally efficient algorithms for a range
of high-dimensional robust estimation tasks. Unfortunately, all previous
algorithms require storing the entire dataset, incurring memory at least
quadratic in the dimension. In this work, we develop the first efficient
streaming algorithms for high-dimensional robust statistics with near-optimal
memory requirements (up to logarithmic factors). Our main result is for the
task of high-dimensional robust mean estimation in (a strengthening of) Huber's
contamination model. We give an efficient single-pass streaming algorithm for
this task with near-optimal error guarantees and space complexity nearly-linear
in the dimension. As a corollary, we obtain streaming algorithms with
near-optimal space complexity for several more complex tasks, including robust
covariance estimation, robust regression, and more generally robust stochastic
optimization.",2204.12399v2,https://arxiv.org/pdf/2204.12399v2
Robust Dual-Graph Regularized Moving Object Detection,"Jing Qin, Ruilong Shen, Ruihan Zhu, Biyun Xie","Moving object detection and its associated background-foreground separation
have been widely used in a lot of applications, including computer vision,
transportation and surveillance. Due to the presence of the static background,
a video can be naturally decomposed into a low-rank background and a sparse
foreground. Many regularization techniques, such as matrix nuclear norm, have
been imposed on the background. In the meanwhile, sparsity or smoothness based
regularizations, such as total variation and $\ell_1$, can be imposed on the
foreground. Moreover, graph Laplacians are further imposed to capture the
complicated geometry of background images. Recently, weighted regularization
techniques including the weighted nuclear norm regularization have been
proposed in the image processing community to promote adaptive sparsity while
achieving efficient performance. In this paper, we propose a robust dual-graph
regularized moving object detection model based on the weighted nuclear norm
regularization, which is solved by the alternating direction method of
multipliers (ADMM). Numerical experiments on body movement data sets have
demonstrated the effectiveness of this method in separating moving objects from
background, and the great potential in robotic applications.",2204.11939v1,https://arxiv.org/pdf/2204.11939v1
Can Rationalization Improve Robustness?,"Howard Chen, Jacqueline He, Karthik Narasimhan, Danqi Chen","A growing line of work has investigated the development of neural NLP models
that can produce rationales--subsets of input that can explain their model
predictions. In this paper, we ask whether such rationale models can also
provide robustness to adversarial attacks in addition to their interpretable
nature. Since these models need to first generate rationales (""rationalizer"")
before making predictions (""predictor""), they have the potential to ignore
noise or adversarially added text by simply masking it out of the generated
rationale. To this end, we systematically generate various types of 'AddText'
attacks for both token and sentence-level rationalization tasks, and perform an
extensive empirical evaluation of state-of-the-art rationale models across five
different tasks. Our experiments reveal that the rationale models show the
promise to improve robustness, while they struggle in certain scenarios--when
the rationalizer is sensitive to positional bias or lexical choices of attack
text. Further, leveraging human rationale as supervision does not always
translate to better performance. Our study is a first step towards exploring
the interplay between interpretability and robustness in the
rationalize-then-predict framework.",2204.11790v2,https://arxiv.org/pdf/2204.11790v2
A Simple Structure For Building A Robust Model,"Xiao Tan, Jingbo Gao, Ruolin Li","As deep learning applications, especially programs of computer vision, are
increasingly deployed in our lives, we have to think more urgently about the
security of these applications.One effective way to improve the security of
deep learning models is to perform adversarial training, which allows the model
to be compatible with samples that are deliberately created for use in
attacking the model.Based on this, we propose a simple architecture to build a
model with a certain degree of robustness, which improves the robustness of the
trained network by adding an adversarial sample detection network for
cooperative training. At the same time, we design a new data sampling strategy
that incorporates multiple existing attacks, allowing the model to adapt to
many different adversarial attacks with a single training.We conducted some
experiments to test the effectiveness of this design based on Cifar10 dataset,
and the results indicate that it has some degree of positive effect on the
robustness of the model.Our code could be found at
https://github.com/dowdyboy/simple_structure_for_robust_model .",2204.11596v2,https://arxiv.org/pdf/2204.11596v2
"Robust Self-Augmentation for Named Entity Recognition with Meta
  Reweighting","Linzhi Wu, Pengjun Xie, Jie Zhou, Meishan Zhang, Chunping Ma, Guangwei Xu, Min Zhang","Self-augmentation has received increasing research interest recently to
improve named entity recognition (NER) performance in low-resource scenarios.
Token substitution and mixup are two feasible heterogeneous self-augmentation
techniques for NER that can achieve effective performance with certain
specialized efforts. Noticeably, self-augmentation may introduce potentially
noisy augmented data. Prior research has mainly resorted to heuristic
rule-based constraints to reduce the noise for specific self-augmentation
methods individually. In this paper, we revisit these two typical
self-augmentation methods for NER, and propose a unified meta-reweighting
strategy for them to achieve a natural integration. Our method is easily
extensible, imposing little effort on a specific self-augmentation method.
Experiments on different Chinese and English NER benchmarks show that our token
substitution and mixup method, as well as their integration, can achieve
effective performance improvement. Based on the meta-reweighting mechanism, we
can enhance the advantages of the self-augmentation techniques without much
extra effort.",2204.11406v4,https://arxiv.org/pdf/2204.11406v4
"Improving Deep Learning Model Robustness Against Adversarial Attack by
  Increasing the Network Capacity","Marco Marchetti, Edmond S. L. Ho","Nowadays, we are more and more reliant on Deep Learning (DL) models and thus
it is essential to safeguard the security of these systems. This paper explores
the security issues in Deep Learning and analyses, through the use of
experiments, the way forward to build more resilient models. Experiments are
conducted to identify the strengths and weaknesses of a new approach to improve
the robustness of DL models against adversarial attacks. The results show
improvements and new ideas that can be used as recommendations for researchers
and practitioners to create increasingly better DL algorithms.",2204.11357v1,https://arxiv.org/pdf/2204.11357v1
How Sampling Impacts the Robustness of Stochastic Neural Networks,"Sina Däubener, Asja Fischer","Stochastic neural networks (SNNs) are random functions whose predictions are
gained by averaging over multiple realizations. Consequently, a gradient-based
adversarial example is calculated based on one set of samples and its
classification on another set. In this paper, we derive a sufficient condition
for such a stochastic prediction to be robust against a given sample-based
attack. This allows us to identify the factors that lead to an increased
robustness of SNNs and gives theoretical explanations for: (i) the well known
observation, that increasing the amount of samples drawn for the estimation of
adversarial examples increases the attack's strength, (ii) why increasing the
number of samples during an attack can not fully reduce the effect of
stochasticity, (iii) why the sample size during inference does not influence
the robustness, and (iv) why a higher gradient variance and a shorter expected
value of the gradient relates to a higher robustness. Our theoretical findings
give a unified view on the mechanisms underlying previously proposed approaches
for increasing attack strengths or model robustness and are verified by an
extensive empirical analysis.",2204.10839v2,https://arxiv.org/pdf/2204.10839v2
"Application of Federated Learning in Building a Robust COVID-19 Chest
  X-ray Classification Model","Amartya Bhattacharya, Manish Gawali, Jitesh Seth, Viraj Kulkarni","While developing artificial intelligence (AI)-based algorithms to solve
problems, the amount of data plays a pivotal role - large amount of data helps
the researchers and engineers to develop robust AI algorithms. In the case of
building AI-based models for problems related to medical imaging, these data
need to be transferred from the medical institutions where they were acquired
to the organizations developing the algorithms. This movement of data involves
time-consuming formalities like complying with HIPAA, GDPR, etc.There is also a
risk of patients' private data getting leaked, compromising their
confidentiality. One solution to these problems is using the Federated Learning
framework.
  Federated Learning (FL) helps AI models to generalize better and create a
robust AI model by using data from different sources having different
distributions and data characteristics without moving all the data to a central
server. In our paper, we apply the FL framework for training a deep learning
model to solve a binary classification problem of predicting the presence or
absence of COVID-19. We took three different sources of data and trained
individual models on each source. Then we trained an FL model on the complete
data and compared all the model performances. We demonstrated that the FL model
performs better than the individual models. Moreover, the FL model performed at
par with the model trained on all the data combined at a central server. Thus
Federated Learning leads to generalized AI models without the cost of data
transfer and regulatory overhead.",2204.10505v1,https://arxiv.org/pdf/2204.10505v1
"Dynamic Ensemble Bayesian Filter for Robust Control of a Human
  Brain-machine Interface","Yu Qi, Xinyun Zhu, Kedi Xu, Feixiao Ren, Hongjie Jiang, Junming Zhu, Jianmin Zhang, Gang Pan, Yueming Wang","Objective: Brain-machine interfaces (BMIs) aim to provide direct brain
control of devices such as prostheses and computer cursors, which have
demonstrated great potential for mobility restoration. One major limitation of
current BMIs lies in the unstable performance in online control due to the
variability of neural signals, which seriously hinders the clinical
availability of BMIs. Method: To deal with the neural variability in online BMI
control, we propose a dynamic ensemble Bayesian filter (DyEnsemble). DyEnsemble
extends Bayesian filters with a dynamic measurement model, which adjusts its
parameters in time adaptively with neural changes. This is achieved by learning
a pool of candidate functions and dynamically weighting and assembling them
according to neural signals. In this way, DyEnsemble copes with variability in
signals and improves the robustness of online control. Results: Online BMI
experiments with a human participant demonstrate that, compared with the
velocity Kalman filter, DyEnsemble significantly improves the control accuracy
(increases the success rate by 13.9% and reduces the reach time by 13.5% in the
random target pursuit task) and robustness (performs more stably over different
experiment days). Conclusion: Our results demonstrate the superiority of
DyEnsemble in online BMI control. Significance: DyEnsemble frames a novel and
flexible framework for robust neural decoding, which is beneficial to different
neural decoding applications.",2204.11840v1,https://arxiv.org/pdf/2204.11840v1
"Testing robustness of predictions of trained classifiers against
  naturally occurring perturbations","Sebastian Scher, Andreas Trügler","Correctly quantifying the robustness of machine learning models is a central
aspect in judging their suitability for specific tasks, and ultimately, for
generating trust in them. We address the problem of finding the robustness of
individual predictions. We show both theoretically and with empirical examples
that a method based on counterfactuals that was previously proposed for this is
insufficient, as it is not a valid metric for determining the robustness
against perturbations that occur ``naturally'', outside specific adversarial
attack scenarios. We propose a flexible approach that models possible
perturbations in input data individually for each application. This is then
combined with a probabilistic approach that computes the likelihood that a
``real-world'' perturbation will change a prediction, thus giving quantitative
information of the robustness of individual predictions of the trained machine
learning model. The method does not require access to the internals of the
classifier and thus in principle works for any black-box model. It is, however,
based on Monte-Carlo sampling and thus only suited for input spaces with small
dimensions. We illustrate our approach on the Iris and the Ionosphere datasets,
on an application predicting fog at an airport, and on analytically solvable
cases.",2204.10046v2,https://arxiv.org/pdf/2204.10046v2
Is Neuron Coverage Needed to Make Person Detection More Robust?,"Svetlana Pavlitskaya, Şiyar Yıkmış, J. Marius Zöllner","The growing use of deep neural networks (DNNs) in safety- and
security-critical areas like autonomous driving raises the need for their
systematic testing. Coverage-guided testing (CGT) is an approach that applies
mutation or fuzzing according to a predefined coverage metric to find inputs
that cause misbehavior. With the introduction of a neuron coverage metric, CGT
has also recently been applied to DNNs. In this work, we apply CGT to the task
of person detection in crowded scenes. The proposed pipeline uses YOLOv3 for
person detection and includes finding DNN bugs via sampling and mutation, and
subsequent DNN retraining on the updated training set. To be a bug, we require
a mutated image to cause a significant performance drop compared to a clean
input. In accordance with the CGT, we also consider an additional requirement
of increased coverage in the bug definition. In order to explore several types
of robustness, our approach includes natural image transformations,
corruptions, and adversarial examples generated with the Daedalus attack. The
proposed framework has uncovered several thousand cases of incorrect DNN
behavior. The relative change in mAP performance of the retrained models
reached on average between 26.21\% and 64.24\% for different robustness types.
However, we have found no evidence that the investigated coverage metrics can
be advantageously used to improve robustness.",2204.10027v1,https://arxiv.org/pdf/2204.10027v1
"Improved Group Robustness via Classifier Retraining on Independent
  Splits","Thien Hang Nguyen, Hongyang R. Zhang, Huy Le Nguyen","Deep neural networks trained by minimizing the average risk can achieve
strong average performance. Still, their performance for a subgroup may degrade
if the subgroup is underrepresented in the overall data population. Group
distributionally robust optimization (Sagawa et al., 2020a), or group DRO in
short, is a widely used baseline for learning models with strong worst-group
performance. We note that this method requires group labels for every example
at training time and can overfit to small groups, requiring strong
regularization. Given a limited amount of group labels at training time, Just
Train Twice (Liu et al., 2021), or JTT in short, is a two-stage method that
infers a pseudo group label for every unlabeled example first, then applies
group DRO based on the inferred group labels. The inference process is also
sensitive to overfitting, sometimes involving additional hyperparameters. This
paper designs a simple method based on the idea of classifier retraining on
independent splits of the training data. We find that using a novel
sample-splitting procedure achieves robust worst-group performance in the
fine-tuning step. When evaluated on benchmark image and text classification
tasks, our approach consistently performs favorably to group DRO, JTT, and
other strong baselines when either group labels are available during training
or are only given in validation sets. Importantly, our method only relies on a
single hyperparameter, which adjusts the fraction of labels used for training
feature extractors vs. training classification layers. We justify the rationale
of our splitting scheme with a generalization-bound analysis of the worst-group
loss.",2204.09583v3,https://arxiv.org/pdf/2204.09583v3
NFormer: Robust Person Re-identification with Neighbor Transformer,"Haochen Wang, Jiayi Shen, Yongtuo Liu, Yan Gao, Efstratios Gavves","Person re-identification aims to retrieve persons in highly varying settings
across different cameras and scenarios, in which robust and discriminative
representation learning is crucial. Most research considers learning
representations from single images, ignoring any potential interactions between
them. However, due to the high intra-identity variations, ignoring such
interactions typically leads to outlier features. To tackle this issue, we
propose a Neighbor Transformer Network, or NFormer, which explicitly models
interactions across all input images, thus suppressing outlier features and
leading to more robust representations overall. As modelling interactions
between enormous amount of images is a massive task with lots of distractors,
NFormer introduces two novel modules, the Landmark Agent Attention, and the
Reciprocal Neighbor Softmax. Specifically, the Landmark Agent Attention
efficiently models the relation map between images by a low-rank factorization
with a few landmarks in feature space. Moreover, the Reciprocal Neighbor
Softmax achieves sparse attention to relevant -- rather than all -- neighbors
only, which alleviates interference of irrelevant representations and further
relieves the computational burden. In experiments on four large-scale datasets,
NFormer achieves a new state-of-the-art. The code is released at
\url{https://github.com/haochenheheda/NFormer}.",2204.09331v1,https://arxiv.org/pdf/2204.09331v1
"Robustness Testing of Data and Knowledge Driven Anomaly Detection in
  Cyber-Physical Systems","Xugui Zhou, Maxfield Kouzel, Homa Alemzadeh","The growing complexity of Cyber-Physical Systems (CPS) and challenges in
ensuring safety and security have led to the increasing use of deep learning
methods for accurate and scalable anomaly detection. However, machine learning
(ML) models often suffer from low performance in predicting unexpected data and
are vulnerable to accidental or malicious perturbations. Although robustness
testing of deep learning models has been extensively explored in applications
such as image classification and speech recognition, less attention has been
paid to ML-driven safety monitoring in CPS. This paper presents the preliminary
results on evaluating the robustness of ML-based anomaly detection methods in
safety-critical CPS against two types of accidental and malicious input
perturbations, generated using a Gaussian-based noise model and the Fast
Gradient Sign Method (FGSM). We test the hypothesis of whether integrating the
domain knowledge (e.g., on unsafe system behavior) with the ML models can
improve the robustness of anomaly detection without sacrificing accuracy and
transparency. Experimental results with two case studies of Artificial Pancreas
Systems (APS) for diabetes management show that ML-based safety monitors
trained with domain knowledge can reduce on average up to 54.2% of robustness
error and keep the average F1 scores high while improving transparency.",2204.09183v2,https://arxiv.org/pdf/2204.09183v2
Jacobian Ensembles Improve Robustness Trade-offs to Adversarial Attacks,"Kenneth T. Co, David Martinez-Rego, Zhongyuan Hau, Emil C. Lupu","Deep neural networks have become an integral part of our software
infrastructure and are being deployed in many widely-used and safety-critical
applications. However, their integration into many systems also brings with it
the vulnerability to test time attacks in the form of Universal Adversarial
Perturbations (UAPs). UAPs are a class of perturbations that when applied to
any input causes model misclassification. Although there is an ongoing effort
to defend models against these adversarial attacks, it is often difficult to
reconcile the trade-offs in model accuracy and robustness to adversarial
attacks. Jacobian regularization has been shown to improve the robustness of
models against UAPs, whilst model ensembles have been widely adopted to improve
both predictive performance and model robustness. In this work, we propose a
novel approach, Jacobian Ensembles-a combination of Jacobian regularization and
model ensembles to significantly increase the robustness against UAPs whilst
maintaining or improving model accuracy. Our results show that Jacobian
Ensembles achieves previously unseen levels of accuracy and robustness, greatly
improving over previous methods that tend to skew towards only either accuracy
or robustness.",2204.08726v1,https://arxiv.org/pdf/2204.08726v1
"A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy,
  Robustness, Fairness, and Explainability","Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Junjie Xu, Zhimeng Guo, Hui Liu, Jiliang Tang, Suhang Wang","Graph Neural Networks (GNNs) have made rapid developments in the recent
years. Due to their great ability in modeling graph-structured data, GNNs are
vastly used in various applications, including high-stakes scenarios such as
financial analysis, traffic predictions, and drug discovery. Despite their
great potential in benefiting humans in the real world, recent study shows that
GNNs can leak private information, are vulnerable to adversarial attacks, can
inherit and magnify societal bias from training data and lack interpretability,
which have risk of causing unintentional harm to the users and society. For
example, existing works demonstrate that attackers can fool the GNNs to give
the outcome they desire with unnoticeable perturbation on training graph. GNNs
trained on social networks may embed the discrimination in their decision
process, strengthening the undesirable societal bias. Consequently, trustworthy
GNNs in various aspects are emerging to prevent the harm from GNN models and
increase the users' trust in GNNs. In this paper, we give a comprehensive
survey of GNNs in the computational aspects of privacy, robustness, fairness,
and explainability. For each aspect, we give the taxonomy of the related
methods and formulate the general frameworks for the multiple categories of
trustworthy GNNs. We also discuss the future research directions of each aspect
and connections between these aspects to help achieve trustworthiness.",2204.08570v2,https://arxiv.org/pdf/2204.08570v2
"Robust, Nonparametric, Efficient Decomposition of Spectral Peaks under
  Distortion and Interference","Kaan Gokcesu, Hakan Gokcesu","We propose a decomposition method for the spectral peaks in an observed
frequency spectrum, which is efficiently acquired by utilizing the Fast Fourier
Transform. In contrast to the traditional methods of waveform fitting on the
spectrum, we optimize the problem from a more robust perspective. We model the
peaks in spectrum as pseudo-symmetric functions, where the only constraint is a
nonincreasing behavior around a central frequency when the distance increases.
Our approach is more robust against arbitrary distortion, interference and
noise on the spectrum that may be caused by an observation system. The time
complexity of our method is linear, i.e., $O(N)$ per extracted spectral peak.
Moreover, the decomposed spectral peaks show a pseudo-orthogonal behavior,
where they conform to a power preserving equality.",2204.08411v1,https://arxiv.org/pdf/2204.08411v1
"StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in
  Texts","Zhengxiang Shi, Qiang Zhang, Aldo Lipani","Inferring spatial relations in natural language is a crucial ability an
intelligent system should possess. The bAbI dataset tries to capture tasks
relevant to this domain (task 17 and 19). However, these tasks have several
limitations. Most importantly, they are limited to fixed expressions, they are
limited in the number of reasoning steps required to solve them, and they fail
to test the robustness of models to input that contains irrelevant or redundant
information. In this paper, we present a new Question-Answering dataset called
StepGame for robust multi-hop spatial reasoning in texts. Our experiments
demonstrate that state-of-the-art models on the bAbI dataset struggle on the
StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented
Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental
results on both datasets show that our model outperforms all the baselines with
superior generalization and robustness performance.",2204.08292v1,https://arxiv.org/pdf/2204.08292v1
"Towards Comprehensive Testing on the Robustness of Cooperative
  Multi-agent Reinforcement Learning","Jun Guo, Yonghong Chen, Yihang Hao, Zixin Yin, Yin Yu, Simin Li","While deep neural networks (DNNs) have strengthened the performance of
cooperative multi-agent reinforcement learning (c-MARL), the agent policy can
be easily perturbed by adversarial examples. Considering the safety critical
applications of c-MARL, such as traffic management, power management and
unmanned aerial vehicle control, it is crucial to test the robustness of c-MARL
algorithm before it was deployed in reality. Existing adversarial attacks for
MARL could be used for testing, but is limited to one robustness aspects (e.g.,
reward, state, action), while c-MARL model could be attacked from any aspect.
To overcome the challenge, we propose MARLSafe, the first robustness testing
framework for c-MARL algorithms. First, motivated by Markov Decision Process
(MDP), MARLSafe consider the robustness of c-MARL algorithms comprehensively
from three aspects, namely state robustness, action robustness and reward
robustness. Any c-MARL algorithm must simultaneously satisfy these robustness
aspects to be considered secure. Second, due to the scarceness of c-MARL
attack, we propose c-MARL attacks as robustness testing algorithms from
multiple aspects. Experiments on \textit{SMAC} environment reveals that many
state-of-the-art c-MARL algorithms are of low robustness in all aspect,
pointing out the urgent need to test and enhance robustness of c-MARL
algorithms.",2204.07932v1,https://arxiv.org/pdf/2204.07932v1
"A Robust and Scalable Attention Guided Deep Learning Framework for
  Movement Quality Assessment","Aditya Kanade, Mansi Sharma, Manivannan Muniyandi","Physical rehabilitation programs frequently begin with a brief stay in the
hospital and continue with home-based rehabilitation. Lack of feedback on
exercise correctness is a significant issue in home-based rehabilitation.
Automated movement quality assessment (MQA) using skeletal movement data
(hereafter referred to as skeletal data) collected via depth imaging devices
can assist with home-based rehabilitation by providing the necessary
quantitative feedback. This paper aims to use recent advances in deep learning
to address the problem of MQA. Movement quality score generation is an
essential component of MQA. We propose three novel skeletal data augmentation
schemes. We show that using the proposed augmentations for generating movement
quality scores result in significant performance boosts over existing methods.
Finally, we propose a novel transformer based architecture for MQA. Four novel
feature extractors are proposed and studied that allow the transformer network
to operate on skeletal data. We show that adding the attention mechanism in the
design of the proposed feature extractor allows the transformer network to pay
attention to specific body parts that make a significant contribution towards
executing a movement. We report an improvement in movement quality score
prediction of 12% on UI-PRMD dataset and 21% on KIMORE dataset compared to the
existing methods.",2204.07840v1,https://arxiv.org/pdf/2204.07840v1
"Robust PCA Unrolling Network for Super-resolution Vessel Extraction in
  X-ray Coronary Angiography","Binjie Qin, Haohao Mao, Yiming Liu, Jun Zhao, Yisong Lv, Yueqi Zhu, Song Ding, Xu Chen","Although robust PCA has been increasingly adopted to extract vessels from
X-ray coronary angiography (XCA) images, challenging problems such as
inefficient vessel-sparsity modelling, noisy and dynamic background artefacts,
and high computational cost still remain unsolved. Therefore, we propose a
novel robust PCA unrolling network with sparse feature selection for
super-resolution XCA vessel imaging. Being embedded within a patch-wise
spatiotemporal super-resolution framework that is built upon a pooling layer
and a convolutional long short-term memory network, the proposed network can
not only gradually prune complex vessel-like artefacts and noisy backgrounds in
XCA during network training but also iteratively learn and select the
high-level spatiotemporal semantic information of moving contrast agents
flowing in the XCA-imaged vessels. The experimental results show that the
proposed method significantly outperforms state-of-the-art methods, especially
in the imaging of the vessel network and its distal vessels, by restoring the
intensity and geometry profiles of heterogeneous vessels against complex and
dynamic backgrounds.",2204.08466v2,https://arxiv.org/pdf/2204.08466v2
"DRFLM: Distributionally Robust Federated Learning with Inter-client
  Noise via Local Mixup","Bingzhe Wu, Zhipeng Liang, Yuxuan Han, Yatao Bian, Peilin Zhao, Junzhou Huang","Recently, federated learning has emerged as a promising approach for training
a global model using data from multiple organizations without leaking their raw
data. Nevertheless, directly applying federated learning to real-world tasks
faces two challenges: (1) heterogeneity in the data among different
organizations; and (2) data noises inside individual organizations.
  In this paper, we propose a general framework to solve the above two
challenges simultaneously. Specifically, we propose using distributionally
robust optimization to mitigate the negative effects caused by data
heterogeneity paradigm to sample clients based on a learnable distribution at
each iteration. Additionally, we observe that this optimization paradigm is
easily affected by data noises inside local clients, which has a significant
performance degradation in terms of global model prediction accuracy. To solve
this problem, we propose to incorporate mixup techniques into the local
training process of federated learning. We further provide comprehensive
theoretical analysis including robustness analysis, convergence analysis, and
generalization ability. Furthermore, we conduct empirical studies across
different drug discovery tasks, such as ADMET property prediction and
drug-target affinity prediction.",2204.07742v1,https://arxiv.org/pdf/2204.07742v1
Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners,"Shashank Gupta, Subhabrata Mukherjee, Krishan Subudhi, Eduardo Gonzalez, Damien Jose, Ahmed H. Awadallah, Jianfeng Gao","Traditional multi-task learning (MTL) methods use dense networks that use the
same set of shared weights across several different tasks. This often creates
interference where two or more tasks compete to pull model parameters in
different directions. In this work, we study whether sparsely activated
Mixture-of-Experts (MoE) improve multi-task learning by specializing some
weights for learning shared representations and using the others for learning
task-specific information. To this end, we devise task-aware gating functions
to route examples from different tasks to specialized experts which share
subsets of network weights conditioned on the task. This results in a sparsely
activated multi-task model with a large number of parameters, but with the same
computational cost as that of a dense model. We demonstrate such sparse
networks to improve multi-task learning along three key dimensions: (i)
transfer to low-resource tasks from related tasks in the training mixture; (ii)
sample-efficient generalization to tasks not seen during training by making use
of task-aware routing from seen related tasks; (iii) robustness to the addition
of unrelated tasks by avoiding catastrophic forgetting of existing tasks.",2204.07689v1,https://arxiv.org/pdf/2204.07689v1
"Perfectly Balanced: Improving Transfer and Robustness of Supervised
  Contrastive Learning","Mayee F. Chen, Daniel Y. Fu, Avanika Narayan, Michael Zhang, Zhao Song, Kayvon Fatahalian, Christopher Ré","An ideal learned representation should display transferability and
robustness. Supervised contrastive learning (SupCon) is a promising method for
training accurate models, but produces representations that do not capture
these properties due to class collapse -- when all points in a class map to the
same representation. Recent work suggests that ""spreading out"" these
representations improves them, but the precise mechanism is poorly understood.
We argue that creating spread alone is insufficient for better representations,
since spread is invariant to permutations within classes. Instead, both the
correct degree of spread and a mechanism for breaking this invariance are
necessary. We first prove that adding a weighted class-conditional InfoNCE loss
to SupCon controls the degree of spread. Next, we study three mechanisms to
break permutation invariance: using a constrained encoder, adding a
class-conditional autoencoder, and using data augmentation. We show that the
latter two encourage clustering of latent subclasses under more realistic
conditions than the former. Using these insights, we show that adding a
properly-weighted class-conditional InfoNCE loss and a class-conditional
autoencoder to SupCon achieves 11.1 points of lift on coarse-to-fine transfer
across 5 standard datasets and 4.7 points on worst-group robustness on 3
datasets, setting state-of-the-art on CelebA by 11.5 points.",2204.07596v2,https://arxiv.org/pdf/2204.07596v2
"Revisiting the Adversarial Robustness-Accuracy Tradeoff in Robot
  Learning","Mathias Lechner, Alexander Amini, Daniela Rus, Thomas A. Henzinger","Adversarial training (i.e., training on adversarially perturbed input data)
is a well-studied method for making neural networks robust to potential
adversarial attacks during inference. However, the improved robustness does not
come for free but rather is accompanied by a decrease in overall model accuracy
and performance. Recent work has shown that, in practical robot learning
applications, the effects of adversarial training do not pose a fair trade-off
but inflict a net loss when measured in holistic robot performance. This work
revisits the robustness-accuracy trade-off in robot learning by systematically
analyzing if recent advances in robust training methods and theory in
conjunction with adversarial robot learning, are capable of making adversarial
training suitable for real-world robot applications. We evaluate three
different robot learning tasks ranging from autonomous driving in a
high-fidelity environment amenable to sim-to-real deployment to mobile robot
navigation and gesture recognition. Our results demonstrate that, while these
techniques make incremental improvements on the trade-off on a relative scale,
the negative impact on the nominal accuracy caused by adversarial training
still outweighs the improved robustness by an order of magnitude. We conclude
that although progress is happening, further advances in robust learning
methods are necessary before they can benefit robot learning tasks in practice.",2204.07373v2,https://arxiv.org/pdf/2204.07373v2
"Scalable and Robust Self-Learning for Skill Routing in Large-Scale
  Conversational AI Systems","Mohammad Kachuee, Jinseok Nam, Sarthak Ahuja, Jin-Myung Won, Sungjin Lee","Skill routing is an important component in large-scale conversational
systems. In contrast to traditional rule-based skill routing, state-of-the-art
systems use a model-based approach to enable natural conversations. To provide
supervision signal required to train such models, ideas such as human
annotation, replication of a rule-based system, relabeling based on user
paraphrases, and bandit-based learning were suggested. However, these
approaches: (a) do not scale in terms of the number of skills and skill
on-boarding, (b) require a very costly expert annotation/rule-design, (c)
introduce risks in the user experience with each model update. In this paper,
we present a scalable self-learning approach to explore routing alternatives
without causing abrupt policy changes that break the user experience, learn
from the user interaction, and incrementally improve the routing via frequent
model refreshes. To enable such robust frequent model updates, we suggest a
simple and effective approach that ensures controlled policy updates for
individual domains, followed by an off-policy evaluation for making deployment
decisions without any need for lengthy A/B experimentation. We conduct various
offline and online A/B experiments on a commercial large-scale conversational
system to demonstrate the effectiveness of the proposed method in real-world
production settings.",2204.07135v1,https://arxiv.org/pdf/2204.07135v1
"Q-TART: Quickly Training for Adversarial Robustness and
  in-Transferability","Madan Ravi Ganesh, Salimeh Yasaei Sekeh, Jason J. Corso","Raw deep neural network (DNN) performance is not enough; in real-world
settings, computational load, training efficiency and adversarial security are
just as or even more important. We propose to simultaneously tackle
Performance, Efficiency, and Robustness, using our proposed algorithm Q-TART,
Quickly Train for Adversarial Robustness and in-Transferability. Q-TART follows
the intuition that samples highly susceptible to noise strongly affect the
decision boundaries learned by DNNs, which in turn degrades their performance
and adversarial susceptibility. By identifying and removing such samples, we
demonstrate improved performance and adversarial robustness while using only a
subset of the training data. Through our experiments we highlight Q-TART's high
performance across multiple Dataset-DNN combinations, including ImageNet, and
provide insights into the complementary behavior of Q-TART alongside existing
adversarial training approaches to increase robustness by over 1.3% while using
up to 17.9% less training time.",2204.07024v1,https://arxiv.org/pdf/2204.07024v1
"From Environmental Sound Representation to Robustness of 2D CNN Models
  Against Adversarial Attacks","Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich","This paper investigates the impact of different standard environmental sound
representations (spectrograms) on the recognition performance and adversarial
attack robustness of a victim residual convolutional neural network, namely
ResNet-18. Our main motivation for focusing on such a front-end classifier
rather than other complex architectures is balancing recognition accuracy and
the total number of training parameters. Herein, we measure the impact of
different settings required for generating more informative Mel-frequency
cepstral coefficient (MFCC), short-time Fourier transform (STFT), and discrete
wavelet transform (DWT) representations on our front-end model. This
measurement involves comparing the classification performance over the
adversarial robustness. We demonstrate an inverse relationship between
recognition accuracy and model robustness against six benchmarking attack
algorithms on the balance of average budgets allocated by the adversary and the
attack cost. Moreover, our experimental results have shown that while the
ResNet-18 model trained on DWT spectrograms achieves a high recognition
accuracy, attacking this model is relatively more costly for the adversary than
other 2D representations. We also report some results on different
convolutional neural network architectures such as ResNet-34, ResNet-56,
AlexNet, and GoogLeNet, SB-CNN, and LSTM-based.",2204.07018v1,https://arxiv.org/pdf/2204.07018v1
Distributionally Robust Models with Parametric Likelihood Ratios,"Paul Michel, Tatsunori Hashimoto, Graham Neubig","As machine learning models are deployed ever more broadly, it becomes
increasingly important that they are not only able to perform well on their
training distribution, but also yield accurate predictions when confronted with
distribution shift. The Distributionally Robust Optimization (DRO) framework
proposes to address this issue by training models to minimize their expected
risk under a collection of distributions, to imitate test-time shifts. This is
most commonly achieved by instance-level re-weighting of the training objective
to emulate the likelihood ratio with possible test distributions, which allows
for estimating their empirical risk via importance sampling (assuming that they
are subpopulations of the training distribution). However, re-weighting schemes
in the literature are usually limited due to the difficulty of keeping the
optimization problem tractable and the complexity of enforcing normalization
constraints. In this paper, we show that three simple ideas -- mini-batch level
normalization, a KL penalty and simultaneous gradient updates -- allow us to
train models with DRO using a broader class of parametric likelihood ratios. In
a series of experiments on both image and text classification benchmarks, we
find that models trained with the resulting parametric adversaries are
consistently more robust to subpopulation shifts when compared to other DRO
approaches, and that the method performs reliably well with little
hyper-parameter tuning. Code to reproduce our experiments can be found at
https://github.com/pmichel31415/P-DRO.",2204.06340v1,https://arxiv.org/pdf/2204.06340v1
"Towards A Critical Evaluation of Robustness for Deep Learning Backdoor
  Countermeasures","Huming Qiu, Hua Ma, Zhi Zhang, Alsharif Abuadbba, Wei Kang, Anmin Fu, Yansong Gao","Since Deep Learning (DL) backdoor attacks have been revealed as one of the
most insidious adversarial attacks, a number of countermeasures have been
developed with certain assumptions defined in their respective threat models.
However, the robustness of these countermeasures is inadvertently ignored,
which can introduce severe consequences, e.g., a countermeasure can be misused
and result in a false implication of backdoor detection.
  For the first time, we critically examine the robustness of existing backdoor
countermeasures with an initial focus on three influential model-inspection
ones that are Neural Cleanse (S&P'19), ABS (CCS'19), and MNTD (S&P'21).
Although the three countermeasures claim that they work well under their
respective threat models, they have inherent unexplored non-robust cases
depending on factors such as given tasks, model architectures, datasets, and
defense hyper-parameter, which are \textit{not even rooted from delicate
adaptive attacks}. We demonstrate how to trivially bypass them aligned with
their respective threat models by simply varying aforementioned factors.
Particularly, for each defense, formal proofs or empirical studies are used to
reveal its two non-robust cases where it is not as robust as it claims or
expects, especially the recent MNTD. This work highlights the necessity of
thoroughly evaluating the robustness of backdoor countermeasures to avoid their
misleading security implications in unknown non-robust cases.",2204.06273v1,https://arxiv.org/pdf/2204.06273v1
Toward Robust Spiking Neural Network Against Adversarial Perturbation,"Ling Liang, Kaidi Xu, Xing Hu, Lei Deng, Yuan Xie","As spiking neural networks (SNNs) are deployed increasingly in real-world
efficiency critical applications, the security concerns in SNNs attract more
attention. Currently, researchers have already demonstrated an SNN can be
attacked with adversarial examples. How to build a robust SNN becomes an urgent
issue. Recently, many studies apply certified training in artificial neural
networks (ANNs), which can improve the robustness of an NN model promisely.
However, existing certifications cannot transfer to SNNs directly because of
the distinct neuron behavior and input formats for SNNs. In this work, we first
design S-IBP and S-CROWN that tackle the non-linear functions in SNNs' neuron
modeling. Then, we formalize the boundaries for both digital and spike inputs.
Finally, we demonstrate the efficiency of our proposed robust training method
in different datasets and model architectures. Based on our experiment, we can
achieve a maximum $37.7\%$ attack error reduction with $3.7\%$ original
accuracy loss. To the best of our knowledge, this is the first analysis on
robust training of SNNs.",2205.01625v1,https://arxiv.org/pdf/2205.01625v1
"A Robust Learning Rule for Soft-Bounded Memristive Synapses Competitive
  with Supervised Learning in Standard Spiking Neural Networks","Thomas F. Tiotto, Jelmer P. Borst, Niels A. Taatgen","Memristive devices are a class of circuit elements that shows great promise
as future building block for brain-inspired computing. One influential view in
theoretical neuroscience sees the brain as a function-computing device: given
input signals, the brain applies a function in order to generate new internal
states and motor outputs. Therefore, being able to approximate functions is a
fundamental axiom to build upon for future brain research and to derive more
efficient computational machines. In this work we apply a novel supervised
learning algorithm - based on controlling niobium-doped strontium titanate
memristive synapses - to learning non-trivial multidimensional functions. By
implementing our method into the spiking neural network simulator Nengo, we
show that we are able to at least match the performance obtained when using
ideal, linear synapses and - in doing so - that this kind of memristive device
can be harnessed as computational substrate to move towards more efficient,
brain-inspired computing.",2204.05682v1,https://arxiv.org/pdf/2204.05682v1
"A Simple Approach to Adversarial Robustness in Few-shot Image
  Classification","Akshayvarun Subramanya, Hamed Pirsiavash","Few-shot image classification, where the goal is to generalize to tasks with
limited labeled data, has seen great progress over the years. However, the
classifiers are vulnerable to adversarial examples, posing a question regarding
their generalization capabilities. Recent works have tried to combine
meta-learning approaches with adversarial training to improve the robustness of
few-shot classifiers. We show that a simple transfer-learning based approach
can be used to train adversarially robust few-shot classifiers. We also present
a method for novel classification task based on calibrating the centroid of the
few-shot category towards the base classes. We show that standard adversarial
training on base categories along with calibrated centroid-based classifier in
the novel categories, outperforms or is on-par with state-of-the-art advanced
methods on standard benchmarks for few-shot learning. Our method is simple,
easy to scale, and with little effort can lead to robust few-shot classifiers.
Code is available here: \url{https://github.com/UCDvision/Simple_few_shot.git}",2204.05432v1,https://arxiv.org/pdf/2204.05432v1
"From Modern CNNs to Vision Transformers: Assessing the Performance,
  Robustness, and Classification Strategies of Deep Learning Models in
  Histopathology","Maximilian Springenberg, Annika Frommholz, Markus Wenzel, Eva Weicken, Jackie Ma, Nils Strodthoff","While machine learning is currently transforming the field of histopathology,
the domain lacks a comprehensive evaluation of state-of-the-art models based on
essential but complementary quality requirements beyond a mere classification
accuracy. In order to fill this gap, we developed a new methodology to
extensively evaluate a wide range of classification models, including recent
vision transformers, and convolutional neural networks such as: ConvNeXt,
ResNet (BiT), Inception, ViT and Swin transformer, with and without supervised
or self-supervised pretraining. We thoroughly tested the models on five widely
used histopathology datasets containing whole slide images of breast, gastric,
and colorectal cancer and developed a novel approach using an image-to-image
translation model to assess the robustness of a cancer classification model
against stain variations. Further, we extended existing interpretability
methods to previously unstudied models and systematically reveal insights of
the models' classifications strategies that can be transferred to future model
architectures.",2204.05044v2,https://arxiv.org/pdf/2204.05044v2
"Enhancing the Robustness, Efficiency, and Diversity of Differentiable
  Architecture Search","Chao Li, Jia Ning, Han Hu, Kun He","Differentiable architecture search (DARTS) has attracted much attention due
to its simplicity and significant improvement in efficiency. However, the
excessive accumulation of the skip connection makes it suffer from long-term
weak stability and low robustness. Many works attempt to restrict the
accumulation of skip connections by indicators or manual design, however, these
methods are susceptible to thresholds and human priors. In this work, we
suggest a more subtle and direct approach that removes skip connections from
the operation space. Then, by introducing an adaptive channel allocation
strategy, we redesign the DARTS framework to automatically refill the skip
connections in the evaluation stage, resolving the performance degradation
caused by the absence of skip connections. Our method, dubbed
Adaptive-Channel-Allocation-DARTS (ACA-DRATS), could eliminate the
inconsistency in operation strength and significantly expand the architecture
diversity. We continue to explore smaller search space under our framework, and
offer a direct search on the entire ImageNet dataset. Experiments show that
ACA-DRATS improves the search stability and significantly speeds up DARTS by
more than ten times while yielding higher accuracy.",2204.04681v1,https://arxiv.org/pdf/2204.04681v1
"Robust Cross-Modal Representation Learning with Progressive
  Self-Distillation","Alex Andonian, Shixing Chen, Raffay Hamid","The learning objective of vision-language approach of CLIP does not
effectively account for the noisy many-to-many correspondences found in
web-harvested image captioning datasets, which contributes to its compute and
data inefficiency. To address this challenge, we introduce a novel training
framework based on cross-modal contrastive learning that uses progressive
self-distillation and soft image-text alignments to more efficiently learn
robust representations from noisy data. Our model distills its own knowledge to
dynamically generate soft-alignment targets for a subset of images and captions
in every minibatch, which are then used to update its parameters. Extensive
evaluation across 14 benchmark datasets shows that our method consistently
outperforms its CLIP counterpart in multiple settings, including: (a) zero-shot
classification, (b) linear probe transfer, and (c) image-text retrieval,
without incurring added computational cost. Analysis using an ImageNet-based
robustness test-bed reveals that our method offers better effective robustness
to natural distribution shifts compared to both ImageNet-trained models and
CLIP itself. Lastly, pretraining with datasets spanning two orders of magnitude
in size shows that our improvements over CLIP tend to scale with number of
training examples.",2204.04588v1,https://arxiv.org/pdf/2204.04588v1
"Self-Labeling Refinement for Robust Representation Learning with
  Bootstrap Your Own Latent","Siddhant Garg, Dhruval Jain","In this work, we have worked towards two major goals. Firstly, we have
investigated the importance of Batch Normalisation (BN) layers in a
non-contrastive representation learning framework called Bootstrap Your Own
Latent (BYOL). We conducted several experiments to conclude that BN layers are
not necessary for representation learning in BYOL. Moreover, BYOL only learns
from the positive pairs of images but ignores other semantically similar images
in the same input batch. For the second goal, we have introduced two new loss
functions to determine the semantically similar pairs in the same input batch
of images and reduce the distance between their representations. These loss
functions are Cross-Cosine Similarity Loss (CCSL) and Cross-Sigmoid Similarity
Loss (CSSL). Using the proposed loss functions, we are able to surpass the
performance of Vanilla BYOL (71.04%) by training the BYOL framework using CCSL
loss (76.87%) on the STL10 dataset. BYOL trained using CSSL loss performs
comparably with Vanilla BYOL.",2204.04545v1,https://arxiv.org/pdf/2204.04545v1
"Study of Robust Sparsity-Aware RLS algorithms with Jointly-Optimized
  Parameters for Impulsive Noise Environments","Y. Yu, L. Lu, Y. Zakharov, R. C. de Lamare, B. Chen","This paper proposes a unified sparsity-aware robust recursive least-squares
RLS (S-RRLS) algorithm for the identification of sparse systems under impulsive
noise. The proposed algorithm generalizes multiple algorithms only by replacing
the specified criterion of robustness and sparsity-aware penalty. Furthermore,
by jointly optimizing the forgetting factor and the sparsity penalty parameter,
we develop the jointly-optimized S-RRLS (JO-S-RRLS) algorithm, which not only
exhibits low misadjustment but also can track well sudden changes of a sparse
system. Simulations in impulsive noise scenarios demonstrate that the proposed
S-RRLS and JO-S-RRLS algorithms outperform existing techniques.",2204.08990v1,https://arxiv.org/pdf/2204.08990v1
Evaluating the Adversarial Robustness for Fourier Neural Operators,"Abolaji D. Adesoji, Pin-Yu Chen","In recent years, Machine-Learning (ML)-driven approaches have been widely
used in scientific discovery domains. Among them, the Fourier Neural Operator
(FNO) was the first to simulate turbulent flow with zero-shot super-resolution
and superior accuracy, which significantly improves the speed when compared to
traditional partial differential equation (PDE) solvers. To inspect the
trustworthiness, we provide the first study on the adversarial robustness of
scientific discovery models by generating adversarial examples for FNO, based
on norm-bounded data input perturbations. Evaluated on the mean squared error
between the FNO model's output and the PDE solver's output, our results show
that the model's robustness degrades rapidly with increasing perturbation
levels, particularly in non-simplistic cases like the 2D Darcy and the Navier
cases. Our research provides a sensitivity analysis tool and evaluation
principles for assessing the adversarial robustness of ML-based scientific
discovery models.",2204.04259v1,https://arxiv.org/pdf/2204.04259v1
"Backdoor Attack against NLP models with Robustness-Aware Perturbation
  defense","Shaik Mohammed Maqsood, Viveros Manuela Ceron, Addluri GowthamKrishna","Backdoor attack intends to embed hidden backdoor into deep neural networks
(DNNs), such that the attacked model performs well on benign samples, whereas
its prediction will be maliciously changed if the hidden backdoor is activated
by the attacker defined trigger. This threat could happen when the training
process is not fully controlled, such as training on third-party data-sets or
adopting third-party models. There has been a lot of research and different
methods to defend such type of backdoor attacks, one being robustness-aware
perturbation-based defense method. This method mainly exploits big gap of
robustness between poisoned and clean samples. In our work, we break this
defense by controlling the robustness gap between poisoned and clean samples
using adversarial training step.",2204.05758v1,https://arxiv.org/pdf/2204.05758v1
Does Robustness on ImageNet Transfer to Downstream Tasks?,"Yutaro Yamada, Mayu Otani","As clean ImageNet accuracy nears its ceiling, the research community is
increasingly more concerned about robust accuracy under distributional shifts.
While a variety of methods have been proposed to robustify neural networks,
these techniques often target models trained on ImageNet classification. At the
same time, it is a common practice to use ImageNet pretrained backbones for
downstream tasks such as object detection, semantic segmentation, and image
classification from different domains. This raises a question: Can these robust
image classifiers transfer robustness to downstream tasks? For object detection
and semantic segmentation, we find that a vanilla Swin Transformer, a variant
of Vision Transformer tailored for dense prediction tasks, transfers robustness
better than Convolutional Neural Networks that are trained to be robust to the
corrupted version of ImageNet. For CIFAR10 classification, we find that models
that are robustified for ImageNet do not retain robustness when fully
fine-tuned. These findings suggest that current robustification techniques tend
to emphasize ImageNet evaluations. Moreover, network architecture is a strong
source of robustness when we consider transfer learning.",2204.03934v1,https://arxiv.org/pdf/2204.03934v1
Using Multiple Self-Supervised Tasks Improves Model Robustness,"Matthew Lawhon, Chengzhi Mao, Junfeng Yang","Deep networks achieve state-of-the-art performance on computer vision tasks,
yet they fail under adversarial attacks that are imperceptible to humans. In
this paper, we propose a novel defense that can dynamically adapt the input
using the intrinsic structure from multiple self-supervised tasks. By
simultaneously using many self-supervised tasks, our defense avoids
over-fitting the adapted image to one specific self-supervised task and
restores more intrinsic structure in the image compared to a single
self-supervised task approach. Our approach further improves robustness and
clean accuracy significantly compared to the state-of-the-art single task
self-supervised defense. Our work is the first to connect multiple
self-supervised tasks to robustness, and suggests that we can achieve better
robustness with more intrinsic signal from visual data.",2204.03714v1,https://arxiv.org/pdf/2204.03714v1
"Imitating, Fast and Slow: Robust learning from demonstrations via
  decision-time planning","Carl Qi, Pieter Abbeel, Aditya Grover","The goal of imitation learning is to mimic expert behavior from
demonstrations, without access to an explicit reward signal. A popular class of
approach infers the (unknown) reward function via inverse reinforcement
learning (IRL) followed by maximizing this reward function via reinforcement
learning (RL). The policies learned via these approaches are however very
brittle in practice and deteriorate quickly even with small test-time
perturbations due to compounding errors. We propose Imitation with Planning at
Test-time (IMPLANT), a new meta-algorithm for imitation learning that utilizes
decision-time planning to correct for compounding errors of any base imitation
policy. In contrast to existing approaches, we retain both the imitation policy
and the rewards model at decision-time, thereby benefiting from the learning
signal of the two components. Empirically, we demonstrate that IMPLANT
significantly outperforms benchmark imitation learning approaches on standard
control environments and excels at zero-shot generalization when subject to
challenging perturbations in test-time dynamics.",2204.03597v2,https://arxiv.org/pdf/2204.03597v2
"FedADMM: A Robust Federated Deep Learning Framework with Adaptivity to
  System Heterogeneity","Yonghai Gong, Yichuan Li, Nikolaos M. Freris","Federated Learning (FL) is an emerging framework for distributed processing
of large data volumes by edge devices subject to limited communication
bandwidths, heterogeneity in data distributions and computational resources, as
well as privacy considerations. In this paper, we introduce a new FL protocol
termed FedADMM based on primal-dual optimization. The proposed method leverages
dual variables to tackle statistical heterogeneity, and accommodates system
heterogeneity by tolerating variable amount of work performed by clients.
FedADMM maintains identical communication costs per round as FedAvg/Prox, and
generalizes them via the augmented Lagrangian. A convergence proof is
established for nonconvex objectives, under no restrictions in terms of data
dissimilarity or number of participants per round of the algorithm. We
demonstrate the merits through extensive experiments on real datasets, under
both IID and non-IID data distributions across clients. FedADMM consistently
outperforms all baseline methods in terms of communication efficiency, with the
number of rounds needed to reach a prescribed accuracy reduced by up to 87%.
The algorithm effectively adapts to heterogeneous data distributions through
the use of dual variables, without the need for hyperparameter tuning, and its
advantages are more pronounced in large-scale systems.",2204.03529v2,https://arxiv.org/pdf/2204.03529v2
Self-supervised learning for robust voice cloning,"Konstantinos Klapsas, Nikolaos Ellinas, Karolos Nikitaras, Georgios Vamvoukakis, Panos Kakoulidis, Konstantinos Markopoulos, Spyros Raptis, June Sig Sung, Gunu Jho, Aimilios Chalamandaris, Pirros Tsiakoulis","Voice cloning is a difficult task which requires robust and informative
features incorporated in a high quality TTS system in order to effectively copy
an unseen speaker's voice. In our work, we utilize features learned in a
self-supervised framework via the Bootstrap Your Own Latent (BYOL) method,
which is shown to produce high quality speech representations when specific
audio augmentations are applied to the vanilla algorithm. We further extend the
augmentations in the training procedure to aid the resulting features to
capture the speaker identity and to make them robust to noise and acoustic
conditions. The learned features are used as pre-trained utterance-level
embeddings and as inputs to a Non-Attentive Tacotron based architecture, aiming
to achieve multispeaker speech synthesis without utilizing additional speaker
features. This method enables us to train our model in an unlabeled
multispeaker dataset as well as use unseen speaker embeddings to copy a
speaker's voice. Subjective and objective evaluations are used to validate the
proposed model, as well as the robustness to the acoustic conditions of the
target utterance.",2204.03421v2,https://arxiv.org/pdf/2204.03421v2
Robust Event-Driven Interactions in Cooperative Multi-Agent Learning,"Daniel Jarne Ornia, Manuel Mazo Jr","We present an approach to reduce the communication required between agents in
a Multi-Agent learning system by exploiting the inherent robustness of the
underlying Markov Decision Process. We compute so-called robustness surrogate
functions (off-line), that give agents a conservative indication of how far
their state measurements can deviate before they need to update other agents in
the system. This results in fully distributed decision functions, enabling
agents to decide when it is necessary to update others. We derive bounds on the
optimality of the resulting systems in terms of the discounted sum of rewards
obtained, and show these bounds are a function of the design parameters.
Additionally, we extend the results for the case where the robustness surrogate
functions are learned from data, and present experimental results demonstrating
a significant reduction in communication events between agents.",2204.03361v2,https://arxiv.org/pdf/2204.03361v2
"Robust and Explainable Autoencoders for Unsupervised Time Series Outlier
  Detection---Extended Version","Tung Kieu, Bin Yang, Chenjuan Guo, Christian S. Jensen, Yan Zhao, Feiteng Huang, Kai Zheng","Time series data occurs widely, and outlier detection is a fundamental
problem in data mining, which has numerous applications. Existing
autoencoder-based approaches deliver state-of-the-art performance on
challenging real-world data but are vulnerable to outliers and exhibit low
explainability. To address these two limitations, we propose robust and
explainable unsupervised autoencoder frameworks that decompose an input time
series into a clean time series and an outlier time series using autoencoders.
Improved explainability is achieved because clean time series are better
explained with easy-to-understand patterns such as trends and periodicities. We
provide insight into this by means of a post-hoc explainability analysis and
empirical studies. In addition, since outliers are separated from clean time
series iteratively, our approach offers improved robustness to outliers, which
in turn improves accuracy. We evaluate our approach on five real-world datasets
and report improvements over the state-of-the-art approaches in terms of
robustness and explainability.
  This is an extended version of ""Robust and Explainable Autoencoders for
Unsupervised Time Series Outlier Detection"", to appear in IEEE ICDE 2022.",2204.03341v1,https://arxiv.org/pdf/2204.03341v1
"Last Layer Re-Training is Sufficient for Robustness to Spurious
  Correlations","Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson","Neural network classifiers can largely rely on simple spurious features, such
as backgrounds, to make predictions. However, even in these cases, we show that
they still often learn core features associated with the desired attributes of
the data, contrary to recent findings. Inspired by this insight, we demonstrate
that simple last layer retraining can match or outperform state-of-the-art
approaches on spurious correlation benchmarks, but with profoundly lower
complexity and computational expenses. Moreover, we show that last layer
retraining on large ImageNet-trained models can also significantly reduce
reliance on background and texture information, improving robustness to
covariate shift, after only minutes of training on a single GPU.",2204.02937v2,https://arxiv.org/pdf/2204.02937v2
"Neural Network-augmented Kalman Filtering for Robust Online Speech
  Dereverberation in Noisy Reverberant Environments","Jean-Marie Lemercier, Joachim Thiemann, Raphael Koning, Timo Gerkmann","In this paper, a neural network-augmented algorithm for noise-robust online
dereverberation with a Kalman filtering variant of the weighted prediction
error (WPE) method is proposed. The filter stochastic variations are predicted
by a deep neural network (DNN) trained end-to-end using the filter residual
error and signal characteristics. The presented framework allows for robust
dereverberation on a single-channel noisy reverberant dataset similar to
WHAMR!. The Kalman filtering WPE introduces distortions in the enhanced signal
when predicting the filter variations from the residual error only, if the
target speech power spectral density is not perfectly known and the observation
is noisy. The proposed approach avoids these distortions by correcting the
filter variations estimation in a data-driven way, increasing the robustness of
the method to noisy scenarios. Furthermore, it yields a strong dereverberation
and denoising performance compared to a DNN-supported recursive least squares
variant of WPE, especially for highly noisy inputs.",2204.02741v2,https://arxiv.org/pdf/2204.02741v2
"Distilling Robust and Non-Robust Features in Adversarial Examples by
  Information Bottleneck","Junho Kim, Byung-Kwan Lee, Yong Man Ro","Adversarial examples, generated by carefully crafted perturbation, have
attracted considerable attention in research fields. Recent works have argued
that the existence of the robust and non-robust features is a primary cause of
the adversarial examples, and investigated their internal interactions in the
feature space. In this paper, we propose a way of explicitly distilling feature
representation into the robust and non-robust features, using Information
Bottleneck. Specifically, we inject noise variation to each feature unit and
evaluate the information flow in the feature representation to dichotomize
feature units either robust or non-robust, based on the noise variation
magnitude. Through comprehensive experiments, we demonstrate that the distilled
features are highly correlated with adversarial prediction, and they have
human-perceptible semantic information by themselves. Furthermore, we present
an attack mechanism intensifying the gradient of non-robust features that is
directly related to the model prediction, and validate its effectiveness of
breaking model robustness.",2204.02735v1,https://arxiv.org/pdf/2204.02735v1
"Training-Free Robust Multimodal Learning via Sample-Wise Jacobian
  Regularization","Zhengqi Gao, Sucheng Ren, Zihui Xue, Siting Li, Hang Zhao","Multimodal fusion emerges as an appealing technique to improve model
performances on many tasks. Nevertheless, the robustness of such fusion methods
is rarely involved in the present literature. In this paper, we propose a
training-free robust late-fusion method by exploiting conditional independence
assumption and Jacobian regularization. Our key is to minimize the Frobenius
norm of a Jacobian matrix, where the resulting optimization problem is relaxed
to a tractable Sylvester equation. Furthermore, we provide a theoretical error
bound of our method and some insights about the function of the extra modality.
Several numerical experiments on AV-MNIST, RAVDESS, and VGGsound demonstrate
the efficacy of our method under both adversarial attacks and random
corruptions.",2204.02485v1,https://arxiv.org/pdf/2204.02485v1
Adversarial Robustness through the Lens of Convolutional Filters,"Paul Gavrikov, Janis Keuper","Deep learning models are intrinsically sensitive to distribution shifts in
the input data. In particular, small, barely perceivable perturbations to the
input data can force models to make wrong predictions with high confidence. An
common defense mechanism is regularization through adversarial training which
injects worst-case perturbations back into training to strengthen the decision
boundaries, and to reduce overfitting. In this context, we perform an
investigation of 3x3 convolution filters that form in adversarially-trained
models. Filters are extracted from 71 public models of the linf-RobustBench
CIFAR-10/100 and ImageNet1k leaderboard and compared to filters extracted from
models built on the same architectures but trained without robust
regularization. We observe that adversarially-robust models appear to form more
diverse, less sparse, and more orthogonal convolution filters than their normal
counterparts. The largest differences between robust and normal models are
found in the deepest layers, and the very first convolution layer, which
consistently and predominantly forms filters that can partially eliminate
perturbations, irrespective of the architecture. Data & Project website:
https://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens",2204.02481v1,https://arxiv.org/pdf/2204.02481v1
"Hear No Evil: Towards Adversarial Robustness of Automatic Speech
  Recognition via Multi-Task Learning","Nilaksh Das, Duen Horng Chau","As automatic speech recognition (ASR) systems are now being widely deployed
in the wild, the increasing threat of adversarial attacks raises serious
questions about the security and reliability of using such systems. On the
other hand, multi-task learning (MTL) has shown success in training models that
can resist adversarial attacks in the computer vision domain. In this work, we
investigate the impact of performing such multi-task learning on the
adversarial robustness of ASR models in the speech domain. We conduct extensive
MTL experimentation by combining semantically diverse tasks such as accent
classification and ASR, and evaluate a wide range of adversarial settings. Our
thorough analysis reveals that performing MTL with semantically diverse tasks
consistently makes it harder for an adversarial attack to succeed. We also
discuss in detail the serious pitfalls and their related remedies that have a
significant impact on the robustness of MTL models. Our proposed MTL approach
shows considerable absolute improvements in adversarially targeted WER ranging
from 17.25 up to 59.90 compared to single-task learning baselines (attention
decoder and CTC respectively). Ours is the first in-depth study that uncovers
adversarial robustness gains from multi-task learning for ASR.",2204.02381v1,https://arxiv.org/pdf/2204.02381v1
"Nearly minimax robust estimator of the mean vector by iterative spectral
  dimension reduction","Amir-Hossein Bateni, Arshak Minasyan, Arnak S. Dalalyan","We study the problem of robust estimation of the mean vector of a
sub-Gaussian distribution. We introduce an estimator based on spectral
dimension reduction (SDR) and establish a finite sample upper bound on its
error that is minimax-optimal up to a logarithmic factor. Furthermore, we prove
that the breakdown point of the SDR estimator is equal to $1/2$, the highest
possible value of the breakdown point. In addition, the SDR estimator is
equivariant by similarity transforms and has low computational complexity. More
precisely, in the case of $n$ vectors of dimension $p$ -- at most $\varepsilon
n$ out of which are adversarially corrupted -- the SDR estimator has a squared
error of order $\big(\frac{r_\Sigma}{n} +
\varepsilon^2\log(1/\varepsilon)\big){\log p}$ and a running time of order $p^3
+ n p^2$. Here, $r_\Sigma\le p$ is the effective rank of the covariance matrix
of the reference distribution. Another advantage of the SDR estimator is that
it does not require knowledge of the contamination rate and does not involve
sample splitting. We also investigate extensions of the proposed algorithm and
of the obtained results in the case of (partially) unknown covariance matrix.",2204.02323v1,https://arxiv.org/pdf/2204.02323v1
"Feature robustness and sex differences in medical imaging: a case study
  in MRI-based Alzheimer's disease detection","Eike Petersen, Aasa Feragen, Maria Luise da Costa Zemsch, Anders Henriksen, Oskar Eiler Wiese Christensen, Melanie Ganz","Convolutional neural networks have enabled significant improvements in
medical image-based diagnosis. It is, however, increasingly clear that these
models are susceptible to performance degradation when facing spurious
correlations and dataset shift, leading, e.g., to underperformance on
underrepresented patient groups. In this paper, we compare two classification
schemes on the ADNI MRI dataset: a simple logistic regression model using
manually selected volumetric features, and a convolutional neural network
trained on 3D MRI data. We assess the robustness of the trained models in the
face of varying dataset splits, training set sex composition, and stage of
disease. In contrast to earlier work in other imaging modalities, we do not
observe a clear pattern of improved model performance for the majority group in
the training dataset. Instead, while logistic regression is fully robust to
dataset composition, we find that CNN performance is generally improved for
both male and female subjects when including more female subjects in the
training dataset. We hypothesize that this might be due to inherent differences
in the pathology of the two sexes. Moreover, in our analysis, the logistic
regression model outperforms the 3D CNN, emphasizing the utility of manual
feature specification based on prior knowledge, and the need for more robust
automatic feature selection.",2204.01737v3,https://arxiv.org/pdf/2204.01737v3
Robust Stuttering Detection via Multi-task and Adversarial Learning,"Shakeel Ahmad Sheikh, Md Sahidullah, Fabrice Hirsch, Slim Ouni","By automatic detection and identification of stuttering, speech pathologists
can track the progression of disfluencies of persons who stutter (PWS). In this
paper, we investigate the impact of multi-task (MTL) and adversarial learning
(ADV) to learn robust stutter features. This is the first-ever preliminary
study where MTL and ADV have been employed in stuttering identification (SI).
We evaluate our system on the SEP-28k stuttering dataset consisting of 20 hours
(approx) of data from 385 podcasts. Our methods show promising results and
outperform the baseline in various disfluency classes. We achieve up to 10%,
6.78%, and 2% improvement in repetitions, blocks, and interjections
respectively over the baseline.",2204.01735v1,https://arxiv.org/pdf/2204.01735v1
Byzantine-Robust Federated Linear Bandits,"Ali Jadbabaie, Haochuan Li, Jian Qian, Yi Tian","In this paper, we study a linear bandit optimization problem in a federated
setting where a large collection of distributed agents collaboratively learn a
common linear bandit model. Standard federated learning algorithms applied to
this setting are vulnerable to Byzantine attacks on even a small fraction of
agents. We propose a novel algorithm with a robust aggregation oracle that
utilizes the geometric median. We prove that our proposed algorithm is robust
to Byzantine attacks on fewer than half of agents and achieves a sublinear
$\tilde{\mathcal{O}}({T^{3/4}})$ regret with $\mathcal{O}(\sqrt{T})$ steps of
communication in $T$ steps. Moreover, we make our algorithm differentially
private via a tree-based mechanism. Finally, if the level of corruption is
known to be small, we show that using the geometric median of mean oracle for
robust aggregation further improves the regret bound.",2204.01155v1,https://arxiv.org/pdf/2204.01155v1
"Adversarially robust segmentation models learn perceptually-aligned
  gradients",Pedro Sandoval-Segura,"The effects of adversarial training on semantic segmentation networks has not
been thoroughly explored. While previous work has shown that
adversarially-trained image classifiers can be used to perform image synthesis,
we have yet to understand how best to leverage an adversarially-trained
segmentation network to do the same. Using a simple optimizer, we demonstrate
that adversarially-trained semantic segmentation networks can be used to
perform image inpainting and generation. Our experiments demonstrate that
adversarially-trained segmentation networks are more robust and indeed exhibit
perceptually-aligned gradients which help in producing plausible image
inpaintings. We seek to place additional weight behind the hypothesis that
adversarially robust models exhibit gradients that are more
perceptually-aligned with human vision. Through image synthesis, we argue that
perceptually-aligned gradients promote a better understanding of a neural
network's learned representations and aid in making neural networks more
interpretable.",2204.01099v1,https://arxiv.org/pdf/2204.01099v1
Supervised Robustness-preserving Data-free Neural Network Pruning,"Mark Huasong Meng, Guangdong Bai, Sin Gee Teo, Jin Song Dong","When deploying pre-trained neural network models in real-world applications,
model consumers often encounter resource-constraint platforms such as mobile
and smart devices. They typically use the pruning technique to reduce the size
and complexity of the model, generating a lighter one with less resource
consumption. Nonetheless, most existing pruning methods are proposed with the
premise that the model after being pruned has a chance to be fine-tuned or even
retrained based on the original training data. This may be unrealistic in
practice, as the data controllers are often reluctant to provide their model
consumers with the original data. In this work, we study the neural network
pruning in the data-free context, aiming to yield lightweight models that are
not only accurate in prediction but also robust against undesired inputs in
open-world deployments. Considering the absence of the fine-tuning and
retraining that can fix the mis-pruned units, we replace the traditional
aggressive one-shot strategy with a conservative one that treats the pruning as
a progressive process. We propose a pruning method based on stochastic
optimization that uses robustness-related metrics to guide the pruning process.
Our method is implemented as a Python program and evaluated with a series of
experiments on diverse neural network models. The experimental results show
that it significantly outperforms existing one-shot data-free pruning
approaches in terms of robustness preservation and accuracy.",2204.00783v2,https://arxiv.org/pdf/2204.00783v2
"Towards Robust and Accurate Myoelectric Controller Design based on
  Multi-objective Optimization using Evolutionary Computation","Ahmed Aqeel Shaikh, Anand Kumar Mukhopadhyay, Soumyajit Poddar, Suman Samui","Myoelectric pattern recognition is one of the important aspects in the design
of the control strategy for various applications including upper-limb
prostheses and bio-robotic hand movement systems. The current work has proposed
an approach to design an energy-efficient EMG-based controller by considering a
kernelized SVM classifier for decoding the information of surface
electromyography (sEMG) signals to infer the underlying muscle movements. In
order to achieve the optimized performance of the EMG-based controller, our
main strategy of classifier design is to reduce the false movements of the
overall system (when the EMG-based controller is at the `Rest' position). To
this end, we have formulated the training algorithm of the proposed supervised
learning system as a general constrained multi-objective optimization problem.
An elitist multi-objective evolutionary algorithm $-$ the non-dominated sorting
genetic algorithm II (NSGA-II) has been used to tune the hyperparameters of
SVM. We have presented the experimental results by performing the experiments
on a dataset consisting of the sEMG signals collected from eleven subjects at
five different upper limb positions. Furthermore, the performance of the
trained models based on the two-objective metrics, namely classification
accuracy, and false-negative have been evaluated on two different test sets to
examine the generalization capability of the proposed training approach while
implementing limb-position invariant EMG classification. It is evident from the
presented result that the proposed approach provides much more flexibility to
the designer in selecting the parameters of the classifier to optimize the
energy efficiency of the EMG-based controller.",2204.02179v3,https://arxiv.org/pdf/2204.02179v3
"Hysteresis-Based RL: Robustifying Reinforcement Learning-based Control
  Policies via Hybrid Control","Jan de Priester, Ricardo G. Sanfelice, Nathan van de Wouw","Reinforcement learning (RL) is a promising approach for deriving control
policies for complex systems. As we show in two control problems, the derived
policies from using the Proximal Policy Optimization (PPO) and Deep Q-Network
(DQN) algorithms may lack robustness guarantees. Motivated by these issues, we
propose a new hybrid algorithm, which we call Hysteresis-Based RL (HyRL),
augmenting an existing RL algorithm with hysteresis switching and two stages of
learning. We illustrate its properties in two examples for which PPO and DQN
fail.",2204.00654v1,https://arxiv.org/pdf/2204.00654v1
Robust and Efficient Aggregation for Distributed Learning,"Stefan Vlaski, Christian Schroth, Michael Muma, Abdelhak M. Zoubir","Distributed learning paradigms, such as federated and decentralized learning,
allow for the coordination of models across a collection of agents, and without
the need to exchange raw data. Instead, agents compute model updates locally
based on their available data, and subsequently share the update model with a
parameter server or their peers. This is followed by an aggregation step, which
traditionally takes the form of a (weighted) average. Distributed learning
schemes based on averaging are known to be susceptible to outliers. A single
malicious agent is able to drive an averaging-based distributed learning
algorithm to an arbitrarily poor model. This has motivated the development of
robust aggregation schemes, which are based on variations of the median and
trimmed mean. While such procedures ensure robustness to outliers and malicious
behavior, they come at the cost of significantly reduced sample efficiency.
This means that current robust aggregation schemes require significantly higher
agent participation rates to achieve a given level of performance than their
mean-based counterparts in non-contaminated settings. In this work we remedy
this drawback by developing statistically efficient and robust aggregation
schemes for distributed learning.",2204.00586v1,https://arxiv.org/pdf/2204.00586v1
"Separate and conquer heuristic allows robust mining of contrast sets in
  classification, regression, and survival data","Adam Gudyś, Marek Sikora, Łukasz Wróbel","Identifying differences between groups is one of the most important knowledge
discovery problems. The procedure, also known as contrast sets mining, is
applied in a wide range of areas like medicine, industry, or economics.
  In the paper we present RuleKit-CS, an algorithm for contrast set mining
based on separate and conquer - a well established heuristic for decision rule
induction. Multiple passes accompanied with an attribute penalization scheme
provide contrast sets describing same examples with different attributes,
distinguishing presented approach from the standard separate and conquer. The
algorithm was also generalized for regression and survival data allowing
identification of contrast sets whose label attribute/survival prognosis is
consistent with the label/prognosis for the predefined contrast groups. This
feature, not provided by the existing approaches, further extends the usability
of RuleKit-CS.
  Experiments on over 130 data sets from various areas and detailed analysis of
selected cases confirmed RuleKit-CS to be a useful tool for discovering
differences between defined groups. The algorithm was implemented as a part of
the RuleKit suite available at GitHub under GNU AGPL 3 licence
(https://github.com/adaa-polsl/RuleKit).
  Keywords: contrast sets, separate and conquer, regression, survival",2204.00497v3,https://arxiv.org/pdf/2204.00497v3
"Robust and Accurate -- Compositional Architectures for Randomized
  Smoothing","Miklós Z. Horváth, Mark Niklas Müller, Marc Fischer, Martin Vechev","Randomized Smoothing (RS) is considered the state-of-the-art approach to
obtain certifiably robust models for challenging tasks. However, current RS
approaches drastically decrease standard accuracy on unperturbed data, severely
limiting their real-world utility. To address this limitation, we propose a
compositional architecture, ACES, which certifiably decides on a per-sample
basis whether to use a smoothed model yielding predictions with guarantees or a
more accurate standard model without guarantees. This, in contrast to prior
approaches, enables both high standard accuracies and significant provable
robustness. On challenging tasks such as ImageNet, we obtain, e.g., $80.0\%$
natural accuracy and $28.2\%$ certifiable accuracy against $\ell_2$
perturbations with $r=1.0$. We release our code and models at
https://github.com/eth-sri/aces.",2204.00487v1,https://arxiv.org/pdf/2204.00487v1
"Comparative Analysis of Interval Reachability for Robust Implicit and
  Feedforward Neural Networks","Alexander Davydov, Saber Jafarpour, Matthew Abate, Francesco Bullo, Samuel Coogan","We use interval reachability analysis to obtain robustness guarantees for
implicit neural networks (INNs). INNs are a class of implicit learning models
that use implicit equations as layers and have been shown to exhibit several
notable benefits over traditional deep neural networks. We first establish that
tight inclusion functions of neural networks, which provide the tightest
rectangular over-approximation of an input-output map, lead to sharper
robustness guarantees than the well-studied robustness measures of local
Lipschitz constants. Like Lipschitz constants, tight inclusions functions are
computationally challenging to obtain, and we thus propose using mixed
monotonicity and contraction theory to obtain computationally efficient
estimates of tight inclusion functions for INNs. We show that our approach
performs at least as well as, and generally better than, applying
state-of-the-art interval bound propagation methods to INNs. We design a novel
optimization problem for training robust INNs and we provide empirical evidence
that suitably-trained INNs can be more robust than comparably-trained
feedforward networks.",2204.00187v1,https://arxiv.org/pdf/2204.00187v1
Adaptive Mean-Residue Loss for Robust Facial Age Estimation,"Ziyuan Zhao, Peisheng Qian, Yubo Hou, Zeng Zeng","Automated facial age estimation has diverse real-world applications in
multimedia analysis, e.g., video surveillance, and human-computer interaction.
However, due to the randomness and ambiguity of the aging process, age
assessment is challenging. Most research work over the topic regards the task
as one of age regression, classification, and ranking problems, and cannot well
leverage age distribution in representing labels with age ambiguity. In this
work, we propose a simple yet effective loss function for robust facial age
estimation via distribution learning, i.e., adaptive mean-residue loss, in
which, the mean loss penalizes the difference between the estimated age
distribution's mean and the ground-truth age, whereas the residue loss
penalizes the entropy of age probability out of dynamic top-K in the
distribution. Experimental results in the datasets FG-NET and CLAP2016 have
validated the effectiveness of the proposed loss. Our code is available at
https://github.com/jacobzhaoziyuan/AMR-Loss.",2203.17156v1,https://arxiv.org/pdf/2203.17156v1
"Doubly-Robust Estimation for Correcting Position-Bias in Click Feedback
  for Unbiased Learning to Rank",Harrie Oosterhuis,"Clicks on rankings suffer from position-bias: generally items on lower ranks
are less likely to be examined - and thus clicked - by users, in spite of their
actual preferences between items. The prevalent approach to unbiased
click-based learning-to-rank (LTR) is based on counterfactual
inverse-propensity-scoring (IPS) estimation. In contrast with general
reinforcement learning, counterfactual doubly-robust (DR) estimation has not
been applied to click-based LTR in previous literature. In this paper, we
introduce a novel DR estimator that is the first DR approach specifically
designed for position-bias. The difficulty with position-bias is that the
treatment - user examination - is not directly observable in click data. As a
solution, our estimator uses the expected treatment per rank, instead of the
actual treatment that existing DR estimators use. Our novel DR estimator has
more robust unbiasedness conditions than the existing IPS approach, and in
addition, provides enormous decreases in variance: our experimental results
indicate it requires several orders of magnitude fewer datapoints to converge
at optimal performance. For the unbiased LTR field, our DR estimator
contributes both increases in state-of-the-art performance and the most robust
theoretical guarantees of all known LTR estimators.",2203.17118v5,https://arxiv.org/pdf/2203.17118v5
RobIn: A Robust Interpretable Deep Network for Schizophrenia Diagnosis,"Daniel Organisciak, Hubert P. H. Shum, Ephraim Nwoye, Wai Lok Woo","Schizophrenia is a severe mental health condition that requires a long and
complicated diagnostic process. However, early diagnosis is vital to control
symptoms. Deep learning has recently become a popular way to analyse and
interpret medical data. Past attempts to use deep learning for schizophrenia
diagnosis from brain-imaging data have shown promise but suffer from a large
training-application gap - it is difficult to apply lab research to the real
world. We propose to reduce this training-application gap by focusing on
readily accessible data. We collect a data set of psychiatric observations of
patients based on DSM-5 criteria. Because similar data is already recorded in
all mental health clinics that diagnose schizophrenia using DSM-5, our method
could be easily integrated into current processes as a tool to assist
clinicians, whilst abiding by formal diagnostic criteria. To facilitate
real-world usage of our system, we show that it is interpretable and robust.
Understanding how a machine learning tool reaches its diagnosis is essential to
allow clinicians to trust that diagnosis. To interpret the framework, we fuse
two complementary attention mechanisms, 'squeeze and excitation' and
'self-attention', to determine global attribute importance and attribute
interactivity, respectively. The model uses these importance scores to make
decisions. This allows clinicians to understand how a diagnosis was reached,
improving trust in the model. Because machine learning models often struggle to
generalise to data from different sources, we perform experiments with
augmented test data to evaluate the model's applicability to the real world. We
find that our model is more robust to perturbations, and should therefore
perform better in a clinical setting. It achieves 98% accuracy with 10-fold
cross-validation.",2203.17085v1,https://arxiv.org/pdf/2203.17085v1
Robust Meta-Reinforcement Learning with Curriculum-Based Task Sampling,"Morio Matsumoto, Hiroya Matsuba, Toshihiro Kujirai","Meta-reinforcement learning (meta-RL) acquires meta-policies that show good
performance for tasks in a wide task distribution. However, conventional
meta-RL, which learns meta-policies by randomly sampling tasks, has been
reported to show meta-overfitting for certain tasks, especially for easy tasks
where an agent can easily get high scores. To reduce effects of the
meta-overfitting, we considered meta-RL with curriculum-based task sampling.
Our method is Robust Meta Reinforcement Learning with Guided Task Sampling
(RMRL-GTS), which is an effective method that restricts task sampling based on
scores and epochs. We show that in order to achieve robust meta-RL, it is
necessary not only to intensively sample tasks with poor scores, but also to
restrict and expand the task regions of the tasks to be sampled.",2203.16801v1,https://arxiv.org/pdf/2203.16801v1
"Robust Disentangled Variational Speech Representation Learning for
  Zero-shot Voice Conversion","Jiachen Lian, Chunlei Zhang, Dong Yu","Traditional studies on voice conversion (VC) have made progress with parallel
training data and known speakers. Good voice conversion quality is obtained by
exploring better alignment modules or expressive mapping functions. In this
study, we investigate zero-shot VC from a novel perspective of self-supervised
disentangled speech representation learning. Specifically, we achieve the
disentanglement by balancing the information flow between global speaker
representation and time-varying content representation in a sequential
variational autoencoder (VAE). A zero-shot voice conversion is performed by
feeding an arbitrary speaker embedding and content embeddings to the VAE
decoder. Besides that, an on-the-fly data augmentation training strategy is
applied to make the learned representation noise invariant. On TIMIT and VCTK
datasets, we achieve state-of-the-art performance on both objective evaluation,
i.e., speaker verification (SV) on speaker embedding and content embedding, and
subjective evaluation, i.e., voice naturalness and similarity, and remains to
be robust even with noisy source/target utterances.",2203.16705v1,https://arxiv.org/pdf/2203.16705v1
"Data-driven Prediction of Relevant Scenarios for Robust Combinatorial
  Optimization","Marc Goerigk, Jannis Kurtz","We study iterative methods for (two-stage) robust combinatorial optimization
problems with discrete uncertainty. We propose a machine-learning-based
heuristic to determine starting scenarios that provide strong lower bounds. To
this end, we design dimension-independent features and train a Random Forest
Classifier on small-dimensional instances. Experiments show that our method
improves the solution process for larger instances than contained in the
training set and also provides a feature importance-score which gives insights
into the role of scenario properties.",2203.16642v2,https://arxiv.org/pdf/2203.16642v2
"Improving Distortion Robustness of Self-supervised Speech Processing
  Tasks with Domain Adaptation","Kuan Po Huang, Yu-Kuan Fu, Yu Zhang, Hung-yi Lee","Speech distortions are a long-standing problem that degrades the performance
of supervisely trained speech processing models. It is high time that we
enhance the robustness of speech processing models to obtain good performance
when encountering speech distortions while not hurting the original performance
on clean speech. In this work, we propose to improve the robustness of speech
processing models by domain adversarial training (DAT). We conducted
experiments based on the SUPERB framework on five different speech processing
tasks. In case we do not always have knowledge of the distortion types for
speech data, we analyzed the binary-domain and multi-domain settings, where the
former treats all distorted speech as one domain, and the latter views
different distortions as different domains. In contrast to supervised training
methods, we obtained promising results in target domains where speech data is
distorted with different distortions including new unseen distortions
introduced during testing.",2203.16104v2,https://arxiv.org/pdf/2203.16104v2
"NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image
  Caption Generation Models","Simin Chen, Zihe Song, Mirazul Haque, Cong Liu, Wei Yang","Neural image caption generation (NICG) models have received massive attention
from the research community due to their excellent performance in visual
understanding. Existing work focuses on improving NICG model accuracy while
efficiency is less explored. However, many real-world applications require
real-time feedback, which highly relies on the efficiency of NICG models.
Recent research observed that the efficiency of NICG models could vary for
different inputs. This observation brings in a new attack surface of NICG
models, i.e., An adversary might be able to slightly change inputs to cause the
NICG models to consume more computational resources. To further understand such
efficiency-oriented threats, we propose a new attack approach, NICGSlowDown, to
evaluate the efficiency robustness of NICG models. Our experimental results
show that NICGSlowDown can generate images with human-unnoticeable
perturbations that will increase the NICG model latency up to 483.86%. We hope
this research could raise the community's concern about the efficiency
robustness of NICG models.",2203.15859v1,https://arxiv.org/pdf/2203.15859v1
"Smooth Robust Tensor Completion for Background/Foreground Separation
  with Missing Pixels: Novel Algorithm with Convergence Guarantee","Bo Shen, Weijun Xie, Zhenyu Kong","The objective of this study is to address the problem of
background/foreground separation with missing pixels by combining the video
acquisition, video recovery, background/foreground separation into a single
framework. To achieve this, a smooth robust tensor completion (SRTC) model is
proposed to recover the data and decompose it into the static background and
smooth foreground, respectively. Specifically, the static background is modeled
by the low-rank tucker decomposition and the smooth foreground (moving objects)
is modeled by the spatiotemporal continuity, which is enforced by the total
variation regularization. An efficient algorithm based on tensor proximal
alternating minimization (tenPAM) is implemented to solve the proposed model
with global convergence guarantee under very mild conditions. Extensive
experiments on real data demonstrate that the proposed method significantly
outperforms the state-of-the-art approaches for background/foreground
separation with missing pixels.",2203.16328v2,https://arxiv.org/pdf/2203.16328v2
LiDAR Snowfall Simulation for Robust 3D Object Detection,"Martin Hahner, Christos Sakaridis, Mario Bijelic, Felix Heide, Fisher Yu, Dengxin Dai, Luc Van Gool","3D object detection is a central task for applications such as autonomous
driving, in which the system needs to localize and classify surrounding traffic
agents, even in the presence of adverse weather. In this paper, we address the
problem of LiDAR-based 3D object detection under snowfall. Due to the
difficulty of collecting and annotating training data in this setting, we
propose a physically based method to simulate the effect of snowfall on real
clear-weather LiDAR point clouds. Our method samples snow particles in 2D space
for each LiDAR line and uses the induced geometry to modify the measurement for
each LiDAR beam accordingly. Moreover, as snowfall often causes wetness on the
ground, we also simulate ground wetness on LiDAR point clouds. We use our
simulation to generate partially synthetic snowy LiDAR data and leverage these
data for training 3D object detection models that are robust to snowfall. We
conduct an extensive evaluation using several state-of-the-art 3D object
detection methods and show that our simulation consistently yields significant
performance gains on the real snowy STF dataset compared to clear-weather
baselines and competing simulation approaches, while not sacrificing
performance in clear weather. Our code is available at
www.github.com/SysCV/LiDAR_snow_sim.",2203.15118v2,https://arxiv.org/pdf/2203.15118v2
Robust Speaker Recognition with Transformers Using wav2vec 2.0,"Sergey Novoselov, Galina Lavrentyeva, Anastasia Avdeeva, Vladimir Volokhov, Aleksei Gusev","Recent advances in unsupervised speech representation learning discover new
approaches and provide new state-of-the-art for diverse types of speech
processing tasks. This paper presents an investigation of using wav2vec 2.0
deep speech representations for the speaker recognition task. The proposed
fine-tuning procedure of wav2vec 2.0 with simple TDNN and statistic pooling
back-end using additive angular margin loss allows to obtain deep speaker
embedding extractor that is well-generalized across different domains. It is
concluded that Contrastive Predictive Coding pretraining scheme efficiently
utilizes the power of unlabeled data, and thus opens the door to powerful
transformer-based speaker recognition systems. The experimental results
obtained in this study demonstrate that fine-tuning can be done on relatively
small sets and a clean version of data. Using data augmentation during
fine-tuning provides additional performance gains in speaker verification. In
this study speaker recognition systems were analyzed on a wide range of
well-known verification protocols: VoxCeleb1 cleaned test set, NIST SRE 18
development set, NIST SRE 2016 and NIST SRE 2019 evaluation set, VOiCES
evaluation set, NIST 2021 SRE, and CTS challenges sets.",2203.15095v1,https://arxiv.org/pdf/2203.15095v1
Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition,"Yuchen Hu, Nana Hou, Chen Chen, Eng Siong Chng","Automatic speech recognition (ASR) systems degrade significantly under noisy
conditions. Recently, speech enhancement (SE) is introduced as front-end to
reduce noise for ASR, but it also suppresses some important speech information,
i.e., over-suppression. To alleviate this, we propose a dual-path style
learning approach for end-to-end noise-robust speech recognition (DPSL-ASR).
Specifically, we first introduce clean speech feature along with the fused
feature from IFF-Net as dual-path inputs to recover the suppressed information.
Then, we propose style learning to map the fused feature close to clean
feature, in order to learn latent speech information from the latter, i.e.,
clean ""speech style"". Furthermore, we also minimize the distance of final ASR
outputs in two paths to improve noise-robustness. Experiments show that the
proposed approach achieves relative word error rate (WER) reductions of 10.6%
and 8.6% over the best IFF-Net baseline, on RATS and CHiME-4 datasets
respectively.",2203.14838v3,https://arxiv.org/pdf/2203.14838v3
Robust and Energy-efficient PPG-based Heart-Rate Monitoring,"Matteo Risso, Alessio Burrello, Daniele Jahier Pagliari, Simone Benatti, Enrico Macii, Luca Benini, Massimo Poncino","A wrist-worn PPG sensor coupled with a lightweight algorithm can run on a MCU
to enable non-invasive and comfortable monitoring, but ensuring robust
PPG-based heart-rate monitoring in the presence of motion artifacts is still an
open challenge. Recent state-of-the-art algorithms combine PPG and inertial
signals to mitigate the effect of motion artifacts. However, these approaches
suffer from limited generality. Moreover, their deployment on MCU-based edge
nodes has not been investigated. In this work, we tackle both the
aforementioned problems by proposing the use of hardware-friendly Temporal
Convolutional Networks (TCN) for PPG-based heart estimation. Starting from a
single ""seed"" TCN, we leverage an automatic Neural Architecture Search (NAS)
approach to derive a rich family of models. Among them, we obtain a TCN that
outperforms the previous state-of-the-art on the largest PPG dataset available
(PPGDalia), achieving a Mean Absolute Error (MAE) of just 3.84 Beats Per Minute
(BPM). Furthermore, we tested also a set of smaller yet still accurate (MAE of
5.64 - 6.29 BPM) networks that can be deployed on a commercial MCU (STM32L4)
which require as few as 5k parameters and reach a latency of 17.1 ms consuming
just 0.21 mJ per inference.",2203.16339v1,https://arxiv.org/pdf/2203.16339v1
"Robust Unlearnable Examples: Protecting Data Against Adversarial
  Learning","Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, Dacheng Tao","The tremendous amount of accessible data in cyberspace face the risk of being
unauthorized used for training deep learning models. To address this concern,
methods are proposed to make data unlearnable for deep learning models by
adding a type of error-minimizing noise. However, such conferred unlearnability
is found fragile to adversarial training. In this paper, we design new methods
to generate robust unlearnable examples that are protected from adversarial
training. We first find that the vanilla error-minimizing noise, which
suppresses the informative knowledge of data via minimizing the corresponding
training loss, could not effectively minimize the adversarial training loss.
This explains the vulnerability of error-minimizing noise in adversarial
training. Based on the observation, robust error-minimizing noise is then
introduced to reduce the adversarial training loss. Experiments show that the
unlearnability brought by robust error-minimizing noise can effectively protect
data from adversarial training in various scenarios. The code is available at
\url{https://github.com/fshp971/robust-unlearnable-examples}.",2203.14533v1,https://arxiv.org/pdf/2203.14533v1
"How to Robustify Black-Box ML Models? A Zeroth-Order Optimization
  Perspective","Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jinfeng Yi, Mingyi Hong, Shiyu Chang, Sijia Liu","The lack of adversarial robustness has been recognized as an important issue
for state-of-the-art machine learning (ML) models, e.g., deep neural networks
(DNNs). Thereby, robustifying ML models against adversarial attacks is now a
major focus of research. However, nearly all existing defense methods,
particularly for robust training, made the white-box assumption that the
defender has the access to the details of an ML model (or its surrogate
alternatives if available), e.g., its architectures and parameters. Beyond
existing works, in this paper we aim to address the problem of black-box
defense: How to robustify a black-box model using just input queries and output
feedback? Such a problem arises in practical scenarios, where the owner of the
predictive model is reluctant to share model information in order to preserve
privacy. To this end, we propose a general notion of defensive operation that
can be applied to black-box models, and design it through the lens of denoised
smoothing (DS), a first-order (FO) certified defense technique. To allow the
design of merely using model queries, we further integrate DS with the
zeroth-order (gradient-free) optimization. However, a direct implementation of
zeroth-order (ZO) optimization suffers a high variance of gradient estimates,
and thus leads to ineffective defense. To tackle this problem, we next propose
to prepend an autoencoder (AE) to a given (black-box) model so that DS can be
trained using variance-reduced ZO optimization. We term the eventual defense as
ZO-AE-DS. In practice, we empirically show that ZO-AE- DS can achieve improved
accuracy, certified robustness, and query complexity over existing baselines.
And the effectiveness of our approach is justified under both image
classification and image reconstruction tasks. Codes are available at
https://github.com/damon-demon/Black-Box-Defense.",2203.14195v1,https://arxiv.org/pdf/2203.14195v1
"A Robust Optimization Method for Label Noisy Datasets Based on Adaptive
  Threshold: Adaptive-k","Enes Dedeoglu, Himmet Toprak Kesgin, Mehmet Fatih Amasyali","SGD does not produce robust results on datasets with label noise. Because the
gradients calculated according to the losses of the noisy samples cause the
optimization process to go in the wrong direction. In this paper, as an
alternative to SGD, we recommend using samples with loss less than a threshold
value determined during the optimization process, instead of using all samples
in the mini-batch. Our proposed method, Adaptive-k, aims to exclude label noise
samples from the optimization process and make the process robust. On noisy
datasets, we found that using a threshold-based approach, such as Adaptive-k,
produces better results than using all samples or a fixed number of low-loss
samples in the mini-batch. Based on our theoretical analysis and experimental
results, we show that the Adaptive-k method is closest to the performance of
the oracle, in which noisy samples are entirely removed from the dataset.
Adaptive-k is a simple but effective method. It does not require prior
knowledge of the noise ratio of the dataset, does not require additional model
training, and does not increase training time significantly. The code for
Adaptive-k is available at https://github.com/enesdedeoglu-TR/Adaptive-k",2203.14165v1,https://arxiv.org/pdf/2203.14165v1
"Efficient Global Robustness Certification of Neural Networks via
  Interleaving Twin-Network Encoding","Zhilu Wang, Chao Huang, Qi Zhu","The robustness of deep neural networks has received significant interest
recently, especially when being deployed in safety-critical systems, as it is
important to analyze how sensitive the model output is under input
perturbations. While most previous works focused on the local robustness
property around an input sample, the studies of the global robustness property,
which bounds the maximum output change under perturbations over the entire
input space, are still lacking. In this work, we formulate the global
robustness certification for neural networks with ReLU activation functions as
a mixed-integer linear programming (MILP) problem, and present an efficient
approach to address it. Our approach includes a novel interleaving twin-network
encoding scheme, where two copies of the neural network are encoded
side-by-side with extra interleaving dependencies added between them, and an
over-approximation algorithm leveraging relaxation and refinement techniques to
reduce complexity. Experiments demonstrate the timing efficiency of our work
when compared with previous global robustness certification methods and the
tightness of our over-approximation. A case study of closed-loop control safety
verification is conducted, and demonstrates the importance and practicality of
our approach for certifying the global robustness of neural networks in
safety-critical systems.",2203.14141v1,https://arxiv.org/pdf/2203.14141v1
Robust No-Regret Learning in Min-Max Stackelberg Games,"Denizalp Goktas, Jiayi Zhao, Amy Greenwald","The behavior of no-regret learning algorithms is well understood in
two-player min-max (i.e, zero-sum) games. In this paper, we investigate the
behavior of no-regret learning in min-max games with dependent strategy sets,
where the strategy of the first player constrains the behavior of the second.
Such games are best understood as sequential, i.e., min-max Stackelberg, games.
We consider two settings, one in which only the first player chooses their
actions using a no-regret algorithm while the second player best responds, and
one in which both players use no-regret algorithms. For the former case, we
show that no-regret dynamics converge to a Stackelberg equilibrium. For the
latter case, we introduce a new type of regret, which we call Lagrangian
regret, and show that if both players minimize their Lagrangian regrets, then
play converges to a Stackelberg equilibrium. We then observe that online mirror
descent (OMD) dynamics in these two settings correspond respectively to a known
nested (i.e., sequential) gradient descent-ascent (GDA) algorithm and a new
simultaneous GDA-like algorithm, thereby establishing convergence of these
algorithms to Stackelberg equilibrium. Finally, we analyze the robustness of
OMD dynamics to perturbations by investigating online min-max Stackelberg
games. We prove that OMD dynamics are robust for a large class of online
min-max games with independent strategy sets. In the dependent case, we
demonstrate the robustness of OMD dynamics experimentally by simulating them in
online Fisher markets, a canonical example of a min-max Stackelberg game with
dependent strategy sets.",2203.14126v2,https://arxiv.org/pdf/2203.14126v2
"A Survey of Robust Adversarial Training in Pattern Recognition:
  Fundamental, Theory, and Methodologies","Zhuang Qian, Kaizhu Huang, Qiu-Feng Wang, Xu-Yao Zhang","In the last a few decades, deep neural networks have achieved remarkable
success in machine learning, computer vision, and pattern recognition. Recent
studies however show that neural networks (both shallow and deep) may be easily
fooled by certain imperceptibly perturbed input samples called adversarial
examples. Such security vulnerability has resulted in a large body of research
in recent years because real-world threats could be introduced due to vast
applications of neural networks. To address the robustness issue to adversarial
examples particularly in pattern recognition, robust adversarial training has
become one mainstream. Various ideas, methods, and applications have boomed in
the field. Yet, a deep understanding of adversarial training including
characteristics, interpretations, theories, and connections among different
models has still remained elusive. In this paper, we present a comprehensive
survey trying to offer a systematic and structured investigation on robust
adversarial training in pattern recognition. We start with fundamentals
including definition, notations, and properties of adversarial examples. We
then introduce a unified theoretical framework for defending against
adversarial samples - robust adversarial training with visualizations and
interpretations on why adversarial training can lead to model robustness.
Connections will be also established between adversarial training and other
traditional learning theories. After that, we summarize, review, and discuss
various methodologies with adversarial attack and defense/training algorithms
in a structured way. Finally, we present analysis, outlook, and remarks of
adversarial training.",2203.14046v1,https://arxiv.org/pdf/2203.14046v1
Improving Robustness of Jet Tagging Algorithms with Adversarial Training,"Annika Stein, Xavier Coubez, Spandan Mondal, Andrzej Novak, Alexander Schmidt","Deep learning is a standard tool in the field of high-energy physics,
facilitating considerable sensitivity enhancements for numerous analysis
strategies. In particular, in identification of physics objects, such as jet
flavor tagging, complex neural network architectures play a major role.
However, these methods are reliant on accurate simulations. Mismodeling can
lead to non-negligible differences in performance in data that need to be
measured and calibrated against. We investigate the classifier response to
input data with injected mismodelings and probe the vulnerability of flavor
tagging algorithms via application of adversarial attacks. Subsequently, we
present an adversarial training strategy that mitigates the impact of such
simulated attacks and improves the classifier robustness. We examine the
relationship between performance and vulnerability and show that this method
constitutes a promising approach to reduce the vulnerability to poor modeling.",2203.13890v2,https://arxiv.org/pdf/2203.13890v2
"Robust deep learning for eye fundus images: Bridging real and synthetic
  data for enhancing generalization","Guilherme C. Oliveira, Gustavo H. Rosa, Daniel C. G. Pedronette, João P. Papa, Himeesh Kumar, Leandro A. Passos, Dinesh Kumar","Deep learning applications for assessing medical images are limited because
the datasets are often small and imbalanced. The use of synthetic data has been
proposed in the literature, but neither a robust comparison of the different
methods nor generalizability has been reported. Our approach integrates a
retinal image quality assessment model and StyleGAN2 architecture to enhance
Age-related Macular Degeneration (AMD) detection capabilities and improve
generalizability. This work compares ten different Generative Adversarial
Network (GAN) architectures to generate synthetic eye-fundus images with and
without AMD. We combined subsets of three public databases (iChallenge-AMD,
ODIR-2019, and RIADD) to form a single training and test set. We employed the
STARE dataset for external validation, ensuring a comprehensive assessment of
the proposed approach. The results show that StyleGAN2 reached the lowest
Frechet Inception Distance (166.17), and clinicians could not accurately
differentiate between real and synthetic images. ResNet-18 architecture
obtained the best performance with 85% accuracy and outperformed the two human
experts (80% and 75%) in detecting AMD fundus images. The accuracy rates were
82.8% for the test set and 81.3% for the STARE dataset, demonstrating the
model's generalizability. The proposed methodology for synthetic medical image
generation has been validated for robustness and accuracy, with free access to
its code for further research and development in this field.",2203.13856v2,https://arxiv.org/pdf/2203.13856v2
Speech-enhanced and Noise-aware Networks for Robust Speech Recognition,"Hung-Shin Lee, Pin-Yuan Chen, Yao-Fei Cheng, Yu Tsao, Hsin-Min Wang","Compensation for channel mismatch and noise interference is essential for
robust automatic speech recognition. Enhanced speech has been introduced into
the multi-condition training of acoustic models to improve their generalization
ability. In this paper, a noise-aware training framework based on two cascaded
neural structures is proposed to jointly optimize speech enhancement and speech
recognition. The feature enhancement module is composed of a multi-task
autoencoder, where noisy speech is decomposed into clean speech and noise. By
concatenating its enhanced, noise-aware, and noisy features for each frame, the
acoustic-modeling module maps each feature-augmented frame into a triphone
state by optimizing the lattice-free maximum mutual information and cross
entropy between the predicted and actual state sequences. On top of the
factorized time delay neural network (TDNN-F) and its convolutional variant
(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error
rate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared
with the best existing systems that use bigram and trigram language models for
decoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction
of 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based
system also outperforms the baseline CNN-TDNNF system on the AMI task.",2203.13696v3,https://arxiv.org/pdf/2203.13696v3
Distributionally Robust Optimization via Ball Oracle Acceleration,"Yair Carmon, Danielle Hausler","We develop and analyze algorithms for distributionally robust optimization
(DRO) of convex losses. In particular, we consider group-structured and bounded
$f$-divergence uncertainty sets. Our approach relies on an accelerated method
that queries a ball optimization oracle, i.e., a subroutine that minimizes the
objective within a small ball around the query point. Our main contribution is
efficient implementations of this oracle for DRO objectives. For DRO with $N$
non-smooth loss functions, the resulting algorithms find an $\epsilon$-accurate
solution with $\widetilde{O}\left(N\epsilon^{-2/3} + \epsilon^{-2}\right)$
first-order oracle queries to individual loss functions. Compared to existing
algorithms for this problem, we improve complexity by a factor of up to
$\epsilon^{-4/3}$.",2203.13225v1,https://arxiv.org/pdf/2203.13225v1
Kernel Robust Hypothesis Testing,"Zhongchang Sun, Shaofeng Zou","The problem of robust hypothesis testing is studied, where under the null and
the alternative hypotheses, the data-generating distributions are assumed to be
in some uncertainty sets, and the goal is to design a test that performs well
under the worst-case distributions over the uncertainty sets. In this paper,
uncertainty sets are constructed in a data-driven manner using kernel method,
i.e., they are centered around empirical distributions of training samples from
the null and alternative hypotheses, respectively; and are constrained via the
distance between kernel mean embeddings of distributions in the reproducing
kernel Hilbert space, i.e., maximum mean discrepancy (MMD). The Bayesian
setting and the Neyman-Pearson setting are investigated. For the Bayesian
setting where the goal is to minimize the worst-case error probability, an
optimal test is firstly obtained when the alphabet is finite. When the alphabet
is infinite, a tractable approximation is proposed to quantify the worst-case
average error probability, and a kernel smoothing method is further applied to
design test that generalizes to unseen samples. A direct robust kernel test is
also proposed and proved to be exponentially consistent. For the Neyman-Pearson
setting, where the goal is to minimize the worst-case probability of miss
detection subject to a constraint on the worst-case probability of false alarm,
an efficient robust kernel test is proposed and is shown to be asymptotically
optimal. Numerical results are provided to demonstrate the performance of the
proposed robust tests.",2203.12777v3,https://arxiv.org/pdf/2203.12777v3
"Contextual Model Aggregation for Fast and Robust Federated Learning in
  Edge Computing","Hung T. Nguyen, H. Vincent Poor, Mung Chiang","Federated learning is a prime candidate for distributed machine learning at
the network edge due to the low communication complexity and privacy protection
among other attractive properties. However, existing algorithms face issues
with slow convergence and/or robustness of performance due to the considerable
heterogeneity of data distribution, computation and communication capability at
the edge. In this work, we tackle both of these issues by focusing on the key
component of model aggregation in federated learning systems and studying
optimal algorithms to perform this task. Particularly, we propose a contextual
aggregation scheme that achieves the optimal context-dependent bound on loss
reduction in each round of optimization. The aforementioned context-dependent
bound is derived from the particular participating devices in that round and an
assumption on smoothness of the overall loss function. We show that this
aggregation leads to a definite reduction of loss function at every round.
Furthermore, we can integrate our aggregation with many existing algorithms to
obtain the contextual versions. Our experimental results demonstrate
significant improvements in convergence speed and robustness of the contextual
versions compared to the original algorithms. We also consider different
variants of the contextual aggregation and show robust performance even in the
most extreme settings.",2203.12738v1,https://arxiv.org/pdf/2203.12738v1
RILI: Robustly Influencing Latent Intent,"Sagar Parekh, Soheil Habibian, Dylan P. Losey","When robots interact with human partners, often these partners change their
behavior in response to the robot. On the one hand this is challenging because
the robot must learn to coordinate with a dynamic partner. But on the other
hand -- if the robot understands these dynamics -- it can harness its own
behavior, influence the human, and guide the team towards effective
collaboration. Prior research enables robots to learn to influence other robots
or simulated agents. In this paper we extend these learning approaches to now
influence humans. What makes humans especially hard to influence is that -- not
only do humans react to the robot -- but the way a single user reacts to the
robot may change over time, and different humans will respond to the same robot
behavior in different ways. We therefore propose a robust approach that learns
to influence changing partner dynamics. Our method first trains with a set of
partners across repeated interactions, and learns to predict the current
partner's behavior based on the previous states, actions, and rewards. Next, we
rapidly adapt to new partners by sampling trajectories the robot learned with
the original partners, and then leveraging those existing behaviors to
influence the new partner dynamics. We compare our resulting algorithm to
state-of-the-art baselines across simulated environments and a user study where
the robot and participants collaborate to build towers. We find that our
approach outperforms the alternatives, even when the partner follows new or
unexpected dynamics. Videos of the user study are available here:
https://youtu.be/lYsWM8An18g",2203.12705v2,https://arxiv.org/pdf/2203.12705v2
Enhancing Classifier Conservativeness and Robustness by Polynomiality,"Ziqi Wang, Marco Loog","We illustrate the detrimental effect, such as overconfident decisions, that
exponential behavior can have in methods like classical LDA and logistic
regression. We then show how polynomiality can remedy the situation. This,
among others, leads purposefully to random-level performance in the tails, away
from the bulk of the training data. A directly related, simple, yet important
technical novelty we subsequently present is softRmax: a reasoned alternative
to the standard softmax function employed in contemporary (deep) neural
networks. It is derived through linking the standard softmax to Gaussian
class-conditional models, as employed in LDA, and replacing those by a
polynomial alternative. We show that two aspects of softRmax, conservativeness
and inherent gradient regularization, lead to robustness against adversarial
attacks without gradient obfuscation.",2203.12693v1,https://arxiv.org/pdf/2203.12693v1
MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data,"George Close, Thomas Hain, Stefan Goetze","Training of speech enhancement systems often does not incorporate knowledge
of human perception and thus can lead to unnatural sounding results.
Incorporating psychoacoustically motivated speech perception metrics as part of
model training via a predictor network has recently gained interest. However,
the performance of such predictors is limited by the distribution of metric
scores that appear in the training data. In this work, we propose MetricGAN+/-
(an extension of MetricGAN+, one such metric-motivated system) which introduces
an additional network - a ""de-generator"" which attempts to improve the
robustness of the prediction network (and by extension of the generator) by
ensuring observation of a wider range of metric scores in training.
Experimental results on the VoiceBank-DEMAND dataset show relative improvement
in PESQ score of 3.8% (3.05 vs 3.22 PESQ score), as well as better
generalisation to unseen noise and speech.",2203.12369v5,https://arxiv.org/pdf/2203.12369v5
"Biceph-Net: A robust and lightweight framework for the diagnosis of
  Alzheimer's disease using 2D-MRI scans and deep similarity learning","A. H. Rashid, A. Gupta, J. Gupta, M. Tanveer","Alzheimer's Disease (AD) is a neurodegenerative disease that is one of the
significant causes of death in the elderly population. Many deep learning
techniques have been proposed to diagnose AD using Magnetic Resonance Imaging
(MRI) scans. Predicting AD using 2D slices extracted from 3D MRI scans is
challenging as the inter-slice information gets lost. To this end, we propose a
novel and lightweight framework termed 'Biceph-Net' for AD diagnosis using 2D
MRI scans that model both the intra-slice and inter-slice information.
Biceph-Net has been experimentally shown to perform similar to other
Spatio-temporal neural networks while being computationally more efficient.
Biceph-Net is also superior in performance compared to vanilla 2D convolutional
neural networks (CNN) for AD diagnosis using 2D MRI slices. Biceph-Net also has
an inbuilt neighbourhood-based model interpretation feature that can be
exploited to understand the classification decision taken by the network.
Biceph-Net experimentally achieves a test accuracy of 100% in the
classification of Cognitively Normal (CN) vs AD, 98.16% for Mild Cognitive
Impairment (MCI) vs AD, and 97.80% for CN vs MCI vs AD.",2203.12197v1,https://arxiv.org/pdf/2203.12197v1
"Out of Distribution Detection, Generalization, and Robustness Triangle
  with Maximum Probability Theorem","Amir Emad Marvasti, Ehsan Emad Marvasti, Ulas Bagci","Maximum Probability Framework, powered by Maximum Probability Theorem, is a
recent theoretical development in artificial intelligence, aiming to formally
define probabilistic models, guiding development of objective functions, and
regularization of probabilistic models. MPT uses the probability distribution
that the models assume on random variables to provide an upper bound on the
probability of the model. We apply MPT to challenging out-of-distribution (OOD)
detection problems in computer vision by incorporating MPT as a regularization
scheme in the training of CNNs and their energy-based variants. We demonstrate
the effectiveness of the proposed method on 1080 trained models, with varying
hyperparameters, and conclude that the MPT-based regularization strategy
stabilizes and improves the generalization and robustness of base models in
addition to enhanced OOD performance on CIFAR10, CIFAR100, and MNIST datasets.",2203.12145v2,https://arxiv.org/pdf/2203.12145v2
"Wasserstein Distributionally Robust Optimization with Wasserstein
  Barycenters","Tim Tsz-Kit Lau, Han Liu","In many applications in statistics and machine learning, the availability of
data samples from multiple possibly heterogeneous sources has become
increasingly prevalent. On the other hand, in distributionally robust
optimization, we seek data-driven decisions which perform well under the most
adverse distribution from a nominal distribution constructed from data samples
within a certain discrepancy of probability distributions. However, it remains
unclear how to achieve such distributional robustness in model learning and
estimation when data samples from multiple sources are available. In this work,
we propose constructing the nominal distribution in optimal transport-based
distributionally robust optimization problems through the notion of Wasserstein
barycenter as an aggregation of data samples from multiple sources. Under
specific choices of the loss function, the proposed formulation admits a
tractable reformulation as a finite convex program, with powerful finite-sample
and asymptotic guarantees. As an illustrative example, we demonstrate with the
problem of distributionally robust sparse inverse covariance matrix estimation
for zero-mean Gaussian random vectors that our proposed scheme outperforms
other widely used estimators in both the low- and high-dimensional regimes.",2203.12136v2,https://arxiv.org/pdf/2203.12136v2
"Review of Metrics to Measure the Stability, Robustness and Resilience of
  Reinforcement Learning",Laura L. Pullum,"Reinforcement learning has received significant interest in recent years, due
primarily to the successes of deep reinforcement learning at solving many
challenging tasks such as playing Chess, Go and online computer games. However,
with the increasing focus on reinforcement learning, applications outside of
gaming and simulated environments require understanding the robustness,
stability, and resilience of reinforcement learning methods. To this end, we
conducted a comprehensive literature review to characterize the available
literature on these three behaviors as they pertain to reinforcement learning.
We classify the quantitative and theoretical approaches used to indicate or
measure robustness, stability, and resilience behaviors. In addition, we
identified the action or event to which the quantitative approaches were
attempting to be stable, robust, or resilient. Finally, we provide a decision
tree useful for selecting metrics to quantify the behaviors. We believe that
this is the first comprehensive review of stability, robustness and resilience
specifically geared towards reinforcement learning.",2203.12048v1,https://arxiv.org/pdf/2203.12048v1
"On the (Non-)Robustness of Two-Layer Neural Networks in Different
  Learning Regimes","Elvis Dohmatob, Alberto Bietti","Neural networks are known to be highly sensitive to adversarial examples.
These may arise due to different factors, such as random initialization, or
spurious correlations in the learning problem. To better understand these
factors, we provide a precise study of the adversarial robustness in different
scenarios, from initialization to the end of training in different regimes, as
well as intermediate scenarios, where initialization still plays a role due to
""lazy"" training. We consider over-parameterized networks in high dimensions
with quadratic targets and infinite samples. Our analysis allows us to identify
new tradeoffs between approximation (as measured via test error) and
robustness, whereby robustness can only get worse when test error improves, and
vice versa. We also show how linearized lazy training regimes can worsen
robustness, due to improperly scaled random initialization. Our theoretical
results are illustrated with numerical experiments.",2203.11864v2,https://arxiv.org/pdf/2203.11864v2
Robust Classification using Contractive Hamiltonian Neural ODEs,"Muhammad Zakwan, Liang Xu, Giancarlo Ferrari-Trecate","Deep neural networks can be fragile and sensitive to small input
perturbations that might cause a significant change in the output. In this
paper, we employ contraction theory to improve the robustness of neural ODEs
(NODEs). A dynamical system is contractive if all solutions with different
initial conditions converge to each other exponentially fast. As a consequence,
perturbations in initial conditions become less and less relevant over time.
Since in NODEs the input data corresponds to the initial condition of dynamical
systems, we show contractivity can mitigate the effect of input perturbations.
More precisely, inspired by NODEs with Hamiltonian dynamics, we propose a class
of contractive Hamiltonian NODEs (CH-NODEs). By properly tuning a scalar
parameter, CH-NODEs ensure contractivity by design and can be trained using
standard backpropagation. Moreover, CH-NODEs enjoy built-in guarantees of
non-exploding gradients, which ensure a well-posed training process. Finally,
we demonstrate the robustness of CH-NODEs on the MNIST image classification
problem with noisy test data.",2203.11805v3,https://arxiv.org/pdf/2203.11805v3
Exploring High-Order Structure for Robust Graph Structure Learning,"Guangqian Yang, Yibing Zhan, Jinlong Li, Baosheng Yu, Liu Liu, Fengxiang He","Recent studies show that Graph Neural Networks (GNNs) are vulnerable to
adversarial attack, i.e., an imperceptible structure perturbation can fool GNNs
to make wrong predictions. Some researches explore specific properties of clean
graphs such as the feature smoothness to defense the attack, but the analysis
of it has not been well-studied. In this paper, we analyze the adversarial
attack on graphs from the perspective of feature smoothness which further
contributes to an efficient new adversarial defensive algorithm for GNNs. We
discover that the effect of the high-order graph structure is a smoother filter
for processing graph structures. Intuitively, the high-order graph structure
denotes the path number between nodes, where larger number indicates closer
connection, so it naturally contributes to defense the adversarial
perturbation. Further, we propose a novel algorithm that incorporates the
high-order structural information into the graph structure learning. We perform
experiments on three popular benchmark datasets, Cora, Citeseer and Polblogs.
Extensive experiments demonstrate the effectiveness of our method for defending
against graph adversarial attacks.",2203.11492v1,https://arxiv.org/pdf/2203.11492v1
"Robust Pivoting: Exploiting Frictional Stability Using Bilevel
  Optimization","Yuki Shirai, Devesh K. Jha, Arvind Raghunathan, Diego Romeres","Generalizable manipulation requires that robots be able to interact with
novel objects and environment. This requirement makes manipulation extremely
challenging as a robot has to reason about complex frictional interaction with
uncertainty in physical properties of the object. In this paper, we study
robust optimization for control of pivoting manipulation in the presence of
uncertainties. We present insights about how friction can be exploited to
compensate for the inaccuracies in the estimates of the physical properties
during manipulation. In particular, we derive analytical expressions for
stability margin provided by friction during pivoting manipulation. This margin
is then used in a bilevel trajectory optimization algorithm to design a
controller that maximizes this stability margin to provide robustness against
uncertainty in physical properties of the object. We demonstrate our proposed
method using a 6 DoF manipulator for manipulating several different objects.",2203.11412v1,https://arxiv.org/pdf/2203.11412v1
On The Robustness of Offensive Language Classifiers,"Jonathan Rusert, Zubair Shafiq, Padmini Srinivasan","Social media platforms are deploying machine learning based offensive
language classification systems to combat hateful, racist, and other forms of
offensive speech at scale. However, despite their real-world deployment, we do
not yet comprehensively understand the extent to which offensive language
classifiers are robust against adversarial attacks. Prior work in this space is
limited to studying robustness of offensive language classifiers against
primitive attacks such as misspellings and extraneous spaces. To address this
gap, we systematically analyze the robustness of state-of-the-art offensive
language classifiers against more crafty adversarial attacks that leverage
greedy- and attention-based word selection and context-aware embeddings for
word replacement. Our results on multiple datasets show that these crafty
adversarial attacks can degrade the accuracy of offensive language classifiers
by more than 50% while also being able to preserve the readability and meaning
of the modified text.",2203.11331v1,https://arxiv.org/pdf/2203.11331v1
"A Learning Convolutional Neural Network Approach for Network Robustness
  Prediction","Yang Lou, Ruizi Wu, Junli Li, Lin Wang, Xiang Li, Guanrong Chen","Network robustness is critical for various societal and industrial networks
again malicious attacks. In particular, connectivity robustness and
controllability robustness reflect how well a networked system can maintain its
connectedness and controllability against destructive attacks, which can be
quantified by a sequence of values that record the remaining connectivity and
controllability of the network after a sequence of node- or edge-removal
attacks. Traditionally, robustness is determined by attack simulations, which
are computationally very time-consuming or even practically infeasible. In this
paper, an improved method for network robustness prediction is developed based
on learning feature representation using convolutional neural network
(LFR-CNN). In this scheme, higher-dimensional network data are compressed to
lower-dimensional representations, and then passed to a CNN to perform
robustness prediction. Extensive experimental studies on both synthetic and
real-world networks, both directed and undirected, demonstrate that 1) the
proposed LFR-CNN performs better than other two state-of-the-art prediction
methods, with significantly lower prediction errors; 2) LFR-CNN is insensitive
to the variation of the network size, which significantly extends its
applicability; 3) although LFR-CNN needs more time to perform feature learning,
it can achieve accurate prediction faster than attack simulations; 4) LFR-CNN
not only can accurately predict network robustness, but also provides a good
indicator for connectivity robustness, better than the classical spectral
measures.",2203.10552v1,https://arxiv.org/pdf/2203.10552v1
Robust Action Gap Increasing with Clipped Advantage Learning,"Zhe Zhang, Yaozhong Gan, Xiaoyang Tan","Advantage Learning (AL) seeks to increase the action gap between the optimal
action and its competitors, so as to improve the robustness to estimation
errors. However, the method becomes problematic when the optimal action induced
by the approximated value function does not agree with the true optimal action.
In this paper, we present a novel method, named clipped Advantage Learning
(clipped AL), to address this issue. The method is inspired by our observation
that increasing the action gap blindly for all given samples while not taking
their necessities into account could accumulate more errors in the performance
loss bound, leading to a slow value convergence, and to avoid that, we should
adjust the advantage value adaptively. We show that our simple clipped AL
operator not only enjoys fast convergence guarantee but also retains proper
action gaps, hence achieving a good balance between the large action gap and
the fast convergence. The feasibility and effectiveness of the proposed method
are verified empirically on several RL benchmarks with promising performance.",2203.11677v1,https://arxiv.org/pdf/2203.11677v1
Build a Robust QA System with Transformer-based Mixture of Experts,"Yu Qing Zhou, Xixuan Julie Liu, Yuanzhe Dong","In this paper, we aim to build a robust question answering system that can
adapt to out-of-domain datasets. A single network may overfit to the
superficial correlation in the training distribution, but with a meaningful
number of expert sub-networks, a gating network that selects a sparse
combination of experts for each input, and careful balance on the importance of
expert sub-networks, the Mixture-of-Experts (MoE) model allows us to train a
multi-task learner that can be generalized to out-of-domain datasets. We also
explore the possibility of bringing the MoE layers up to the middle of the
DistilBERT and replacing the dense feed-forward network with a
sparsely-activated switch FFN layers, similar to the Switch Transformer
architecture, which simplifies the MoE routing algorithm with reduced
communication and computational costs. In addition to model architectures, we
explore techniques of data augmentation including Easy Data Augmentation (EDA)
and back translation, to create more meaningful variance among the small
out-of-domain training data, therefore boosting the performance and robustness
of our models. In this paper, we show that our combination of best architecture
and data augmentation techniques achieves a 53.477 F1 score in the
out-of-domain evaluation, which is a 9.52% performance gain over the baseline.
On the final test set, we reported a higher 59.506 F1 and 41.651 EM. We
successfully demonstrate the effectiveness of Mixture-of-Expert architecture in
a Robust QA task.",2204.09598v1,https://arxiv.org/pdf/2204.09598v1
"A Study on Robustness to Perturbations for Representations of
  Environmental Sound","Sangeeta Srivastava, Ho-Hsiang Wu, Joao Rulff, Magdalena Fuentes, Mark Cartwright, Claudio Silva, Anish Arora, Juan Pablo Bello","Audio applications involving environmental sound analysis increasingly use
general-purpose audio representations, also known as embeddings, for transfer
learning. Recently, Holistic Evaluation of Audio Representations (HEAR)
evaluated twenty-nine embedding models on nineteen diverse tasks. However, the
evaluation's effectiveness depends on the variation already captured within a
given dataset. Therefore, for a given data domain, it is unclear how the
representations would be affected by the variations caused by myriad
microphones' range and acoustic conditions -- commonly known as channel
effects. We aim to extend HEAR to evaluate invariance to channel effects in
this work. To accomplish this, we imitate channel effects by injecting
perturbations to the audio signal and measure the shift in the new (perturbed)
embeddings with three distance measures, making the evaluation domain-dependent
but not task-dependent. Combined with the downstream performance, it helps us
make a more informed prediction of how robust the embeddings are to the channel
effects. We evaluate two embeddings -- YAMNet, and OpenL3 on monophonic
(UrbanSound8K) and polyphonic (SONYC-UST) urban datasets. We show that one
distance measure does not suffice in such task-independent evaluation. Although
Fr\'echet Audio Distance (FAD) correlates with the trend of the performance
drop in the downstream task most accurately, we show that we need to study FAD
in conjunction with the other distances to get a clear understanding of the
overall effect of the perturbation. In terms of the embedding performance, we
find OpenL3 to be more robust than YAMNet, which aligns with the HEAR
evaluation.",2203.10425v3,https://arxiv.org/pdf/2203.10425v3
On Robust Prefix-Tuning for Text Classification,"Zonghan Yang, Yang Liu","Recently, prefix-tuning has gained increasing attention as a
parameter-efficient finetuning method for large-scale pretrained language
models. The method keeps the pretrained models fixed and only updates the
prefix token parameters for each downstream task. Despite being lightweight and
modular, prefix-tuning still lacks robustness to textual adversarial attacks.
However, most currently developed defense techniques necessitate auxiliary
model update and storage, which inevitably hamper the modularity and low
storage of prefix-tuning. In this work, we propose a robust prefix-tuning
framework that preserves the efficiency and modularity of prefix-tuning. The
core idea of our framework is leveraging the layerwise activations of the
language model by correctly-classified training data as the standard for
additional prefix finetuning. During the test phase, an extra batch-level
prefix is tuned for each batch and added to the original prefix for robustness
enhancement. Extensive experiments on three text classification benchmarks show
that our framework substantially improves robustness over several strong
baselines against five textual attacks of different types while maintaining
comparable accuracy on clean texts. We also interpret our robust prefix-tuning
framework from the optimal control perspective and pose several directions for
future research.",2203.10378v1,https://arxiv.org/pdf/2203.10378v1
"Distinguishing Non-natural from Natural Adversarial Samples for More
  Robust Pre-trained Language Model","Jiayi Wang, Rongzhou Bao, Zhuosheng Zhang, Hai Zhao","Recently, the problem of robustness of pre-trained language models (PrLMs)
has received increasing research interest. Latest studies on adversarial
attacks achieve high attack success rates against PrLMs, claiming that PrLMs
are not robust. However, we find that the adversarial samples that PrLMs fail
are mostly non-natural and do not appear in reality. We question the validity
of current evaluation of robustness of PrLMs based on these non-natural
adversarial samples and propose an anomaly detector to evaluate the robustness
of PrLMs with more natural adversarial samples. We also investigate two
applications of the anomaly detector: (1) In data augmentation, we employ the
anomaly detector to force generating augmented data that are distinguished as
non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply
the anomaly detector to a defense framework to enhance the robustness of PrLMs.
It can be used to defend all types of attacks and achieves higher accuracy on
both adversarial samples and compliant samples than other defense frameworks.",2203.11199v1,https://arxiv.org/pdf/2203.11199v1
FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,"Soumya Sanyal, Harman Singh, Xiang Ren","Transformers have been shown to be able to perform deductive reasoning on a
logical rulebase containing rules and statements written in natural language.
Recent works show that such models can also produce the reasoning steps (i.e.,
the proof graph) that emulate the model's logical reasoning process. Currently,
these black-box models generate both the proof graph and intermediate
inferences within the same model and thus may be unfaithful. In this work, we
frame the deductive logical reasoning task by defining three modular
components: rule selection, fact selection, and knowledge composition. The rule
and fact selection steps select the candidate rule and facts to be used and
then the knowledge composition combines them to generate new inferences. This
ensures model faithfulness by assured causal relation from the proof step to
the inference reasoning. To test our framework, we propose FaiRR (Faithful and
Robust Reasoner) where the above three components are independently modeled by
transformers. We observe that FaiRR is robust to novel language perturbations,
and is faster at inference than previous works on existing reasoning datasets.
Additionally, in contrast to black-box generative models, the errors made by
FaiRR are more interpretable due to the modular approach.",2203.10261v1,https://arxiv.org/pdf/2203.10261v1
"TDR-CL: Targeted Doubly Robust Collaborative Learning for Debiased
  Recommendations","Haoxuan Li, Yan Lyu, Chunyuan Zheng, Peng Wu","Bias is a common problem inherent in recommender systems, which is entangled
with users' preferences and poses a great challenge to unbiased learning. For
debiasing tasks, the doubly robust (DR) method and its variants show superior
performance due to the double robustness property, that is, DR is unbiased when
either imputed errors or learned propensities are accurate. However, our
theoretical analysis reveals that DR usually has a large variance. Meanwhile,
DR would suffer unexpectedly large bias and poor generalization caused by
inaccurate imputed errors and learned propensities, which usually occur in
practice. In this paper, we propose a principled approach that can effectively
reduce bias and variance simultaneously for existing DR approaches when the
error imputation model is misspecified. In addition, we further propose a novel
semi-parametric collaborative learning approach that decomposes imputed errors
into parametric and nonparametric parts and updates them collaboratively,
resulting in more accurate predictions. Both theoretical analysis and
experiments demonstrate the superiority of the proposed methods compared with
existing debiasing methods.",2203.10258v3,https://arxiv.org/pdf/2203.10258v3
Self-Ensemble Adversarial Training for Improved Robustness,"Hongjun Wang, Yisen Wang","Due to numerous breakthroughs in real-world applications brought by machine
intelligence, deep neural networks (DNNs) are widely employed in critical
applications. However, predictions of DNNs are easily manipulated with
imperceptible adversarial perturbations, which impedes the further deployment
of DNNs and may result in profound security and privacy implications. By
incorporating adversarial samples into the training data pool, adversarial
training is the strongest principled strategy against various adversarial
attacks among all sorts of defense methods. Recent works mainly focus on
developing new loss functions or regularizers, attempting to find the unique
optimal point in the weight space. But none of them taps the potentials of
classifiers obtained from standard adversarial training, especially states on
the searching trajectory of training. In this work, we are dedicated to the
weight states of models through the training process and devise a simple but
powerful \emph{Self-Ensemble Adversarial Training} (SEAT) method for yielding a
robust classifier by averaging weights of history models. This considerably
improves the robustness of the target model against several well known
adversarial attacks, even merely utilizing the naive cross-entropy loss to
supervise. We also discuss the relationship between the ensemble of predictions
from different adversarially trained models and the prediction of
weight-ensembled models, as well as provide theoretical and empirical evidence
that the proposed self-ensemble method provides a smoother loss landscape and
better robustness than both individual models and the ensemble of predictions
from different classifiers. We further analyze a subtle but fatal issue in the
general settings for the self-ensemble model, which causes the deterioration of
the weight-ensembled method in the late phases.",2203.09678v2,https://arxiv.org/pdf/2203.09678v2
"Learning Distributionally Robust Models at Scale via Composite
  Optimization","Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, Amin Karbasi","To train machine learning models that are robust to distribution shifts in
the data, distributionally robust optimization (DRO) has been proven very
effective. However, the existing approaches to learning a distributionally
robust model either require solving complex optimization problems such as
semidefinite programming or a first-order method whose convergence scales
linearly with the number of data samples -- which hinders their scalability to
large datasets. In this paper, we show how different variants of DRO are simply
instances of a finite-sum composite optimization for which we provide scalable
methods. We also provide empirical results that demonstrate the effectiveness
of our proposed algorithm with respect to the prior art in order to learn
robust models from very large datasets.",2203.09607v1,https://arxiv.org/pdf/2203.09607v1
Stochastic and Private Nonconvex Outlier-Robust PCA,"Tyler Maunu, Chenyu Yu, Gilad Lerman","We develop theoretically guaranteed stochastic methods for outlier-robust
PCA. Outlier-robust PCA seeks an underlying low-dimensional linear subspace
from a dataset that is corrupted with outliers. We are able to show that our
methods, which involve stochastic geodesic gradient descent over the
Grassmannian manifold, converge and recover an underlying subspace in various
regimes through the development of a novel convergence analysis. The main
application of this method is an effective differentially private algorithm for
outlier-robust PCA that uses a Gaussian noise mechanism within the stochastic
gradient method. Our results emphasize the advantages of the nonconvex methods
over another convex approach to solving this problem in the differentially
private setting. Experiments on synthetic and stylized data verify these
results.",2203.09276v1,https://arxiv.org/pdf/2203.09276v1
RoMe: A Robust Metric for Evaluating Natural Language Generation,"Md Rashad Al Hasan Rony, Liubov Kovriguina, Debanjan Chaudhuri, Ricardo Usbeck, Jens Lehmann","Evaluating Natural Language Generation (NLG) systems is a challenging task.
Firstly, the metric should ensure that the generated hypothesis reflects the
reference's semantics. Secondly, it should consider the grammatical quality of
the generated sentence. Thirdly, it should be robust enough to handle various
surface forms of the generated sentence. Thus, an effective evaluation metric
has to be multifaceted. In this paper, we propose an automatic evaluation
metric incorporating several core aspects of natural language understanding
(language competence, syntactic and semantic variation). Our proposed metric,
RoMe, is trained on language features such as semantic similarity combined with
tree edit distance and grammatical acceptability, using a self-supervised
neural network to assess the overall quality of the generated sentence.
Moreover, we perform an extensive robustness analysis of the state-of-the-art
methods and RoMe. Empirical results suggest that RoMe has a stronger
correlation to human judgment over state-of-the-art metrics in evaluating
system-generated sentences across several NLG tasks.",2203.09183v1,https://arxiv.org/pdf/2203.09183v1
Are Vision Transformers Robust to Spurious Correlations?,"Soumya Suvra Ghosal, Yifei Ming, Yixuan Li","Deep neural networks may be susceptible to learning spurious correlations
that hold on average but not in atypical test samples. As with the recent
emergence of vision transformer (ViT) models, it remains underexplored how
spurious correlations are manifested in such architectures. In this paper, we
systematically investigate the robustness of vision transformers to spurious
correlations on three challenging benchmark datasets and compare their
performance with popular CNNs. Our study reveals that when pre-trained on a
sufficiently large dataset, ViT models are more robust to spurious correlations
than CNNs. Key to their success is the ability to generalize better from the
examples where spurious correlations do not hold. Further, we perform extensive
ablations and experiments to understand the role of the self-attention
mechanism in providing robustness under spuriously correlated environments. We
hope that our work will inspire future research on further understanding the
robustness of ViT models.",2203.09125v1,https://arxiv.org/pdf/2203.09125v1
"DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression
  for Real-World Clinical Applications","Somaye Hashemifar, Claudia Iriondo, Evan Casey, Mohsen Hejrati, for Alzheimer's Disease Neuroimaging Initiative","The ability to predict the future trajectory of a patient is a key step
toward the development of therapeutics for complex diseases such as Alzheimer's
disease (AD). However, most machine learning approaches developed for
prediction of disease progression are either single-task or single-modality
models, which can not be directly adopted to our setting involving multi-task
learning with high dimensional images. Moreover, most of those approaches are
trained on a single dataset (i.e. cohort), which can not be generalized to
other cohorts. We propose a novel multimodal multi-task deep learning model to
predict AD progression by analyzing longitudinal clinical and neuroimaging data
from multiple cohorts. Our proposed model integrates high dimensional MRI
features from a 3D convolutional neural network with other data modalities,
including clinical and demographic information, to predict the future
trajectory of patients. Our model employs an adversarial loss to alleviate the
study-specific imaging bias, in particular the inter-study domain shifts. In
addition, a Sharpness-Aware Minimization (SAM) optimization technique is
applied to further improve model generalization. The proposed model is trained
and tested on various datasets in order to evaluate and validate the results.
Our results showed that 1) our model yields significant improvement over the
baseline models, and 2) models using extracted neuroimaging features from 3D
convolutional neural network outperform the same models when applied to
MRI-derived volumetric features.",2203.09096v5,https://arxiv.org/pdf/2203.09096v5
"On the Convergence of Certified Robust Training with Interval Bound
  Propagation","Yihan Wang, Zhouxing Shi, Quanquan Gu, Cho-Jui Hsieh","Interval Bound Propagation (IBP) is so far the base of state-of-the-art
methods for training neural networks with certifiable robustness guarantees
when potential adversarial perturbations present, while the convergence of IBP
training remains unknown in existing literature. In this paper, we present a
theoretical analysis on the convergence of IBP training. With an
overparameterized assumption, we analyze the convergence of IBP robust
training. We show that when using IBP training to train a randomly initialized
two-layer ReLU neural network with logistic loss, gradient descent can linearly
converge to zero robust training error with a high probability if we have
sufficiently small perturbation radius and large network width.",2203.08961v1,https://arxiv.org/pdf/2203.08961v1
"Robustness through Cognitive Dissociation Mitigation in Contrastive
  Adversarial Training","Adir Rahamim, Itay Naeh","In this paper, we introduce a novel neural network training framework that
increases model's adversarial robustness to adversarial attacks while
maintaining high clean accuracy by combining contrastive learning (CL) with
adversarial training (AT). We propose to improve model robustness to
adversarial attacks by learning feature representations that are consistent
under both data augmentations and adversarial perturbations. We leverage
contrastive learning to improve adversarial robustness by considering an
adversarial example as another positive example, and aim to maximize the
similarity between random augmentations of data samples and their adversarial
example, while constantly updating the classification head in order to avoid a
cognitive dissociation between the classification head and the embedding space.
This dissociation is caused by the fact that CL updates the network up to the
embedding space, while freezing the classification head which is used to
generate new positive adversarial examples. We validate our method, Contrastive
Learning with Adversarial Features(CLAF), on the CIFAR-10 dataset on which it
outperforms both robust accuracy and clean accuracy over alternative supervised
and self-supervised adversarial learning methods.",2203.08959v3,https://arxiv.org/pdf/2203.08959v3
Provable Adversarial Robustness for Fractional Lp Threat Models,"Alexander Levine, Soheil Feizi","In recent years, researchers have extensively studied adversarial robustness
in a variety of threat models, including L_0, L_1, L_2, and L_infinity-norm
bounded adversarial attacks. However, attacks bounded by fractional L_p ""norms""
(quasi-norms defined by the L_p distance with 0<p<1) have yet to be thoroughly
considered. We proactively propose a defense with several desirable properties:
it provides provable (certified) robustness, scales to ImageNet, and yields
deterministic (rather than high-probability) certified guarantees when applied
to quantized data (e.g., images). Our technique for fractional L_p robustness
constructs expressive, deep classifiers that are globally Lipschitz with
respect to the L_p^p metric, for any 0<p<1. However, our method is even more
general: we can construct classifiers which are globally Lipschitz with respect
to any metric defined as the sum of concave functions of components. Our
approach builds on a recent work, Levine and Feizi (2021), which provides a
provable defense against L_1 attacks. However, we demonstrate that our proposed
guarantees are highly non-vacuous, compared to the trivial solution of using
(Levine and Feizi, 2021) directly and applying norm inequalities. Code is
available at https://github.com/alevine0/fractionalLpRobustness.",2203.08945v1,https://arxiv.org/pdf/2203.08945v1
"Understanding robustness and generalization of artificial neural
  networks through Fourier masks","Nikos Karantzas, Emma Besier, Josue Ortega Caro, Xaq Pitkow, Andreas S. Tolias, Ankit B. Patel, Fabio Anselmi","Despite the enormous success of artificial neural networks (ANNs) in many
disciplines, the characterization of their computations and the origin of key
properties such as generalization and robustness remain open questions. Recent
literature suggests that robust networks with good generalization properties
tend to be biased towards processing low frequencies in images. To explore the
frequency bias hypothesis further, we develop an algorithm that allows us to
learn modulatory masks highlighting the essential input frequencies needed for
preserving a trained network's performance. We achieve this by imposing
invariance in the loss with respect to such modulations in the input
frequencies. We first use our method to test the low-frequency preference
hypothesis of adversarially trained or data-augmented networks. Our results
suggest that adversarially robust networks indeed exhibit a low-frequency bias
but we find this bias is also dependent on directions in frequency space.
However, this is not necessarily true for other types of data augmentation. Our
results also indicate that the essential frequencies in question are
effectively the ones used to achieve generalization in the first place.
Surprisingly, images seen through these modulatory masks are not recognizable
and resemble texture-like patterns.",2203.08822v1,https://arxiv.org/pdf/2203.08822v1
"COPA: Certifying Robust Policies for Offline Reinforcement Learning
  against Poisoning Attacks","Fan Wu, Linyi Li, Chejian Xu, Huan Zhang, Bhavya Kailkhura, Krishnaram Kenthapadi, Ding Zhao, Bo Li","As reinforcement learning (RL) has achieved near human-level performance in a
variety of tasks, its robustness has raised great attention. While a vast body
of research has explored test-time (evasion) attacks in RL and corresponding
defenses, its robustness against training-time (poisoning) attacks remains
largely unanswered. In this work, we focus on certifying the robustness of
offline RL in the presence of poisoning attacks, where a subset of training
trajectories could be arbitrarily manipulated. We propose the first
certification framework, COPA, to certify the number of poisoning trajectories
that can be tolerated regarding different certification criteria. Given the
complex structure of RL, we propose two certification criteria: per-state
action stability and cumulative reward bound. To further improve the
certification, we propose new partition and aggregation protocols to train
robust policies. We further prove that some of the proposed certification
methods are theoretically tight and some are NP-Complete problems. We leverage
COPA to certify three RL environments trained with different algorithms and
conclude: (1) The proposed robust aggregation protocols such as temporal
aggregation can significantly improve the certifications; (2) Our certification
for both per-state action stability and cumulative reward bound are efficient
and tight; (3) The certification for different training algorithms and
environments are different, implying their intrinsic robustness properties. All
experimental results are available at https://copa-leaderboard.github.io.",2203.08398v1,https://arxiv.org/pdf/2203.08398v1
"Improved Multi-label Classification under Temporal Concept Drift:
  Rethinking Group-Robust Algorithms in a Label-Wise Setting","Ilias Chalkidis, Anders Søgaard","In document classification for, e.g., legal and biomedical text, we often
deal with hundreds of classes, including very infrequent ones, as well as
temporal concept drift caused by the influence of real world events, e.g.,
policy changes, conflicts, or pandemics. Class imbalance and drift can
sometimes be mitigated by resampling the training data to simulate (or
compensate for) a known target distribution, but what if the target
distribution is determined by unknown future events? Instead of simply
resampling uniformly to hedge our bets, we focus on the underlying optimization
algorithms used to train such document classifiers and evaluate several
group-robust optimization algorithms, initially proposed to mitigate
group-level disparities. Reframing group-robust algorithms as adaptation
algorithms under concept drift, we find that Invariant Risk Minimization and
Spectral Decoupling outperform sampling-based approaches to class imbalance and
concept drift, and lead to much better performance on minority classes. The
effect is more pronounced the larger the label set.",2203.07856v1,https://arxiv.org/pdf/2203.07856v1
"Generalized but not Robust? Comparing the Effects of Data Modification
  Methods on Out-of-Domain Generalization and Adversarial Robustness","Tejas Gokhale, Swaroop Mishra, Man Luo, Bhavdeep Singh Sachdeva, Chitta Baral","Data modification, either via additional training datasets, data
augmentation, debiasing, and dataset filtering, has been proposed as an
effective solution for generalizing to out-of-domain (OOD) inputs, in both
natural language processing and computer vision literature. However, the effect
of data modification on adversarial robustness remains unclear. In this work,
we conduct a comprehensive study of common data modification strategies and
evaluate not only their in-domain and OOD performance, but also their
adversarial robustness (AR). We also present results on a two-dimensional
synthetic dataset to visualize the effect of each method on the training
distribution. This work serves as an empirical study towards understanding the
relationship between generalizing to unseen domains and defending against
adversarial perturbations. Our findings suggest that more data (either via
additional datasets or data augmentation) benefits both OOD accuracy and AR.
However, data filtering (previously shown to improve OOD accuracy on natural
language inference) hurts OOD accuracy on other tasks such as question
answering and image classification. We provide insights from our experiments to
inform future work in this direction.",2203.07653v1,https://arxiv.org/pdf/2203.07653v1
Task-Agnostic Robust Representation Learning,"A. Tuan Nguyen, Ser Nam Lim, Philip Torr","It has been reported that deep learning models are extremely vulnerable to
small but intentionally chosen perturbations of its input. In particular, a
deep network, despite its near-optimal accuracy on the clean images, often
mis-classifies an image with a worst-case but humanly imperceptible
perturbation (so-called adversarial examples). To tackle this problem, a great
amount of research has been done to study the training procedure of a network
to improve its robustness. However, most of the research so far has focused on
the case of supervised learning. With the increasing popularity of
self-supervised learning methods, it is also important to study and improve the
robustness of their resulting representation on the downstream tasks. In this
paper, we study the problem of robust representation learning with unlabeled
data in a task-agnostic manner. Specifically, we first derive an upper bound on
the adversarial loss of a prediction model (which is based on the learned
representation) on any downstream task, using its loss on the clean data and a
robustness regularizer. Moreover, the regularizer is task-independent, thus we
propose to minimize it directly during the representation learning phase to
make the downstream prediction model more robust. Extensive experiments show
that our method achieves preferable adversarial performance compared to
relevant baselines.",2203.07596v1,https://arxiv.org/pdf/2203.07596v1
"Convolutional-Recurrent Neural Network Proxy for Robust Optimization and
  Closed-Loop Reservoir Management","Yong Do Kim, Louis J. Durlofsky","Production optimization under geological uncertainty is computationally
expensive, as a large number of well control schedules must be evaluated over
multiple geological realizations. In this work, a convolutional-recurrent
neural network (CNN-RNN) proxy model is developed to predict well-by-well oil
and water rates, for given time-varying well bottom-hole pressure (BHP)
schedules, for each realization in an ensemble. This capability enables the
estimation of the objective function and nonlinear constraint values required
for robust optimization. The proxy model represents an extension of a recently
developed long short-term memory (LSTM) RNN proxy designed to predict well
rates for a single geomodel. A CNN is introduced here to processes permeability
realizations, and this provides the initial states for the RNN. The CNN-RNN
proxy is trained using simulation results for 300 different sets of BHP
schedules and permeability realizations. We demonstrate proxy accuracy for
oil-water flow through multiple realizations of 3D multi-Gaussian permeability
models. The proxy is then incorporated into a closed-loop reservoir management
(CLRM) workflow, where it is used with particle swarm optimization and a
filter-based method for nonlinear constraint satisfaction. History matching is
achieved using an adjoint-gradient-based procedure. The proxy model is shown to
perform well in this setting for five different (synthetic) `true' models.
Improved net present value along with constraint satisfaction and uncertainty
reduction are observed with CLRM. For the robust production optimization steps,
the proxy provides O(100) runtime speedup over simulation-based optimization.",2203.07524v2,https://arxiv.org/pdf/2203.07524v2
On the benefits of knowledge distillation for adversarial robustness,"Javier Maroto, Guillermo Ortiz-Jiménez, Pascal Frossard","Knowledge distillation is normally used to compress a big network, or
teacher, onto a smaller one, the student, by training it to match its outputs.
Recently, some works have shown that robustness against adversarial attacks can
also be distilled effectively to achieve good rates of robustness on
mobile-friendly models. In this work, however, we take a different point of
view, and show that knowledge distillation can be used directly to boost the
performance of state-of-the-art models in adversarial robustness. In this
sense, we present a thorough analysis and provide general guidelines to distill
knowledge from a robust teacher and boost the clean and adversarial performance
of a student model even further. To that end, we present Adversarial Knowledge
Distillation (AKD), a new framework to improve a model's robust performance,
consisting on adversarially training a student on a mixture of the original
labels and the teacher outputs. Through carefully controlled ablation studies,
we show that using early-stopping, model ensembles and weak adversarial
training are key techniques to maximize performance of the student, and show
that these insights generalize across different robust distillation techniques.
Finally, we provide insights on the effect of robust knowledge distillation on
the dynamics of the student network, and show that AKD mostly improves the
calibration of the network and modify its training dynamics on samples that the
model finds difficult to learn, or even memorize.",2203.07159v1,https://arxiv.org/pdf/2203.07159v1
"Conquering Ghosts: Relation Learning for Information Reliability
  Representation and End-to-End Robust Navigation","Kefan Jin, Xingyao Han","Environmental disturbances, such as sensor data noises, various lighting
conditions, challenging weathers and external adversarial perturbations, are
inevitable in real self-driving applications. Existing researches and testings
have shown that they can severely influence the vehicles perception ability and
performance, one of the main issue is the false positive detection, i.e., the
ghost object which is not real existed or occurs in the wrong position (such as
a non-existent vehicle). Traditional navigation methods tend to avoid every
detected objects for safety, however, avoiding a ghost object may lead the
vehicle into a even more dangerous situation, such as a sudden break on the
highway. Considering the various disturbance types, it is difficult to address
this issue at the perceptual aspect. A potential solution is to detect the
ghost through relation learning among the whole scenario and develop an
integrated end-to-end navigation system. Our underlying logic is that the
behavior of all vehicles in the scene is influenced by their neighbors, and
normal vehicles behave in a logical way, while ghost vehicles do not. By
learning the spatio-temporal relation among surrounding vehicles, an
information reliability representation is learned for each detected vehicle and
then a robot navigation network is developed. In contrast to existing works, we
encourage the network to learn how to represent the reliability and how to
aggregate all the information with uncertainties by itself, thus increasing the
efficiency and generalizability. To the best of the authors knowledge, this
paper provides the first work on using graph relation learning to achieve
end-to-end robust navigation in the presence of ghost vehicles. Simulation
results in the CARLA platform demonstrate the feasibility and effectiveness of
the proposed method in various scenarios.",2203.09952v3,https://arxiv.org/pdf/2203.09952v3
"Probabilistically Robust Recourse: Navigating the Trade-offs between
  Costs and Robustness in Algorithmic Recourse","Martin Pawelczyk, Teresa Datta, Johannes van-den-Heuvel, Gjergji Kasneci, Himabindu Lakkaraju","As machine learning models are increasingly being employed to make
consequential decisions in real-world settings, it becomes critical to ensure
that individuals who are adversely impacted (e.g., loan denied) by the
predictions of these models are provided with a means for recourse. While
several approaches have been proposed to construct recourses for affected
individuals, the recourses output by these methods either achieve low costs
(i.e., ease-of-implementation) or robustness to small perturbations (i.e.,
noisy implementations of recourses), but not both due to the inherent
trade-offs between the recourse costs and robustness. Furthermore, prior
approaches do not provide end users with any agency over navigating the
aforementioned trade-offs. In this work, we address the above challenges by
proposing the first algorithmic framework which enables users to effectively
manage the recourse cost vs. robustness trade-offs. More specifically, our
framework Probabilistically ROBust rEcourse (\texttt{PROBE}) lets users choose
the probability with which a recourse could get invalidated (recourse
invalidation rate) if small changes are made to the recourse i.e., the recourse
is implemented somewhat noisily. To this end, we propose a novel objective
function which simultaneously minimizes the gap between the achieved
(resulting) and desired recourse invalidation rates, minimizes recourse costs,
and also ensures that the resulting recourse achieves a positive model
prediction. We develop novel theoretical results to characterize the recourse
invalidation rates corresponding to any given instance w.r.t. different classes
of underlying models (e.g., linear models, tree based models etc.), and
leverage these results to efficiently optimize the proposed objective.
Experimental evaluation with multiple real world datasets demonstrates the
efficacy of the proposed framework.",2203.06768v4,https://arxiv.org/pdf/2203.06768v4
Context-LSTM: a robust classifier for video detection on UCF101,"Dengshan Li, Rujing Wang","Video detection and human action recognition may be computationally
expensive, and need a long time to train models. In this paper, we were
intended to reduce the training time and the GPU memory usage of video
detection, and achieved a competitive detection accuracy. Other research works
such as Two-stream, C3D, TSN have shown excellent performance on UCF101. Here,
we used a LSTM structure simply for video detection. We used a simple structure
to perform a competitive top-1 accuracy on the entire validation dataset of
UCF101. The LSTM structure is named Context-LSTM, since it may process the deep
temporal features. The Context-LSTM may simulate the human recognition system.
We cascaded the LSTM blocks in PyTorch and connected the cell state flow and
hidden output flow. At the connection of the blocks, we used ReLU, Batch
Normalization, and MaxPooling functions. The Context-LSTM could reduce the
training time and the GPU memory usage, while keeping a state-of-the-art top-1
accuracy on UCF101 entire validation dataset, show a robust performance on
video action detection.",2203.06610v1,https://arxiv.org/pdf/2203.06610v1
"Policy Learning for Robust Markov Decision Process with a Mismatched
  Generative Model","Jialian Li, Tongzheng Ren, Dong Yan, Hang Su, Jun Zhu","In high-stake scenarios like medical treatment and auto-piloting, it's risky
or even infeasible to collect online experimental data to train the agent.
Simulation-based training can alleviate this issue, but may suffer from its
inherent mismatches from the simulator and real environment. It is therefore
imperative to utilize the simulator to learn a robust policy for the real-world
deployment. In this work, we consider policy learning for Robust Markov
Decision Processes (RMDP), where the agent tries to seek a robust policy with
respect to unexpected perturbations on the environments. Specifically, we focus
on the setting where the training environment can be characterized as a
generative model and a constrained perturbation can be added to the model
during testing. Our goal is to identify a near-optimal robust policy for the
perturbed testing environment, which introduces additional technical
difficulties as we need to simultaneously estimate the training environment
uncertainty from samples and find the worst-case perturbation for testing. To
solve this issue, we propose a generic method which formalizes the perturbation
as an opponent to obtain a two-player zero-sum game, and further show that the
Nash Equilibrium corresponds to the robust policy. We prove that, with a
polynomial number of samples from the generative model, our algorithm can find
a near-optimal robust policy with a high probability. Our method is able to
deal with general perturbations under some mild assumptions and can also be
extended to more complex problems like robust partial observable Markov
decision process, thanks to the game-theoretical formulation.",2203.06587v2,https://arxiv.org/pdf/2203.06587v2
"A Robust Approach for the Decomposition of High-Energy-Consuming
  Industrial Loads with Deep Learning","Jia Cui, Yonghui Jin, Renzhe Yu, Martin Onyeka Okoye, Yang Li, Junyou Yang, Shunjiang Wang","The knowledge of the users' electricity consumption pattern is an important
coordinating mechanism between the utility company and the electricity
consumers in terms of key decision makings. The load decomposition is therefore
crucial to reveal the underlying relationship between the load consumption and
its characteristics. However, load decomposition is conventionally performed on
the residential and commercial loads, and adequate consideration has not been
given to the high-energy-consuming industrial loads leading to inefficient
results. This paper thus focuses on the load decomposition of the industrial
park loads (IPL). The commonly used parameters in a conventional method are
however inapplicable in high-energy-consuming industrial loads. Therefore, a
more robust approach is developed comprising a three-algorithm model to achieve
this goal on the IPL. First, the improved variational mode decomposition (IVMD)
algorithm is introduced to denoise the training data of the IPL and improve its
stability. Secondly, the convolutional neural network (CNN) and simple
recurrent units (SRU) joint algorithms are used to achieve a non-intrusive and
non-invasive decomposition process of the IPL using a double-layer deep
learning network based on the IPL characteristics. Specifically, CNN is used to
extract the IPL data characteristics while the improved long and short-term
memory (LSTM) network, SRU, is adopted to develop the decomposition model and
further train the load data. Through the robust decomposition process, the
underlying relationship in the load consumption is extracted. The results
obtained from the numerical examples show that this approach outperforms the
state-of-the-art in the conventional decomposition process.",2203.07075v1,https://arxiv.org/pdf/2203.07075v1
Learning-based Localizability Estimation for Robust LiDAR Localization,"Julian Nubert, Etienne Walther, Shehryar Khattak, Marco Hutter","LiDAR-based localization and mapping is one of the core components in many
modern robotic systems due to the direct integration of range and geometry,
allowing for precise motion estimation and generation of high quality maps in
real-time. Yet, as a consequence of insufficient environmental constraints
present in the scene, this dependence on geometry can result in localization
failure, happening in self-symmetric surroundings such as tunnels. This work
addresses precisely this issue by proposing a neural network-based estimation
approach for detecting (non-)localizability during robot operation. Special
attention is given to the localizability of scan-to-scan registration, as it is
a crucial component in many LiDAR odometry estimation pipelines. In contrast to
previous, mostly traditional detection approaches, the proposed method enables
early detection of failure by estimating the localizability on raw sensor
measurements without evaluating the underlying registration optimization.
Moreover, previous approaches remain limited in their ability to generalize
across environments and sensor types, as heuristic-tuning of degeneracy
detection thresholds is required. The proposed approach avoids this problem by
learning from a collection of different environments, allowing the network to
function over various scenarios. Furthermore, the network is trained
exclusively on simulated data, avoiding arduous data collection in challenging
and degenerate, often hard-to-access, environments. The presented method is
tested during field experiments conducted across challenging environments and
on two different sensor types without any modifications. The observed detection
performance is on par with state-of-the-art methods after environment-specific
threshold tuning.",2203.05698v2,https://arxiv.org/pdf/2203.05698v2
"Attacks as Defenses: Designing Robust Audio CAPTCHAs Using Attacks on
  Automatic Speech Recognition Systems","Hadi Abdullah, Aditya Karlekar, Saurabh Prasad, Muhammad Sajidur Rahman, Logan Blue, Luke A. Bauer, Vincent Bindschaedler, Patrick Traynor","Audio CAPTCHAs are supposed to provide a strong defense for online resources;
however, advances in speech-to-text mechanisms have rendered these defenses
ineffective. Audio CAPTCHAs cannot simply be abandoned, as they are
specifically named by the W3C as important enablers of accessibility.
Accordingly, demonstrably more robust audio CAPTCHAs are important to the
future of a secure and accessible Web. We look to recent literature on attacks
on speech-to-text systems for inspiration for the construction of robust,
principle-driven audio defenses. We begin by comparing 20 recent attack papers,
classifying and measuring their suitability to serve as the basis of new
""robust to transcription"" but ""easy for humans to understand"" CAPTCHAs. After
showing that none of these attacks alone are sufficient, we propose a new
mechanism that is both comparatively intelligible (evaluated through a user
study) and hard to automatically transcribe (i.e., $P({\rm transcription}) = 4
\times 10^{-5}$). Finally, we demonstrate that our audio samples have a high
probability of being detected as CAPTCHAs when given to speech-to-text systems
($P({\rm evasion}) = 1.77 \times 10^{-4}$). In so doing, we not only
demonstrate a CAPTCHA that is approximately four orders of magnitude more
difficult to crack, but that such systems can be designed based on the insights
gained from attack papers using the differences between the ways that humans
and computers process audio.",2203.05408v1,https://arxiv.org/pdf/2203.05408v1
"Robustness Analysis of Classification Using Recurrent Neural Networks
  with Perturbed Sequential Input","Guangyi Liu, Arash Amini, Martin Takac, Nader Motee","For a given stable recurrent neural network (RNN) that is trained to perform
a classification task using sequential inputs, we quantify explicit robustness
bounds as a function of trainable weight matrices. The sequential inputs can be
perturbed in various ways, e.g., streaming images can be deformed due to robot
motion or imperfect camera lens. Using the notion of the Voronoi diagram and
Lipschitz properties of stable RNNs, we provide a thorough analysis and
characterize the maximum allowable perturbations while guaranteeing the full
accuracy of the classification task. We illustrate and validate our theoretical
results using a map dataset with clouds as well as the MNIST dataset.",2203.05403v1,https://arxiv.org/pdf/2203.05403v1
"Exploiting the Potential of Datasets: A Data-Centric Approach for Model
  Robustness","Yiqi Zhong, Lei Wu, Xianming Liu, Junjun Jiang","Robustness of deep neural networks (DNNs) to malicious perturbations is a hot
topic in trustworthy AI. Existing techniques obtain robust models given fixed
datasets, either by modifying model structures, or by optimizing the process of
inference or training. While significant improvements have been made, the
possibility of constructing a high-quality dataset for model robustness remain
unexplored. Follow the campaign of data-centric AI launched by Andrew Ng, we
propose a novel algorithm for dataset enhancement that works well for many
existing DNN models to improve robustness. Transferable adversarial examples
and 14 kinds of common corruptions are included in our optimized dataset. In
the data-centric robust learning competition hosted by Alibaba Group and
Tsinghua University, our algorithm came third out of more than 3000 competitors
in the first stage while we ranked fourth in the second stage. Our code is
available at \url{https://github.com/hncszyq/tianchi_challenge}.",2203.05323v1,https://arxiv.org/pdf/2203.05323v1
"Robust Federated Learning Against Adversarial Attacks for Speech Emotion
  Recognition","Yi Chang, Sofiane Laridi, Zhao Ren, Gregory Palmer, Björn W. Schuller, Marco Fisichella","Due to the development of machine learning and speech processing, speech
emotion recognition has been a popular research topic in recent years. However,
the speech data cannot be protected when it is uploaded and processed on
servers in the internet-of-things applications of speech emotion recognition.
Furthermore, deep neural networks have proven to be vulnerable to
human-indistinguishable adversarial perturbations. The adversarial attacks
generated from the perturbations may result in deep neural networks wrongly
predicting the emotional states. We propose a novel federated adversarial
learning framework for protecting both data and deep neural networks. The
proposed framework consists of i) federated learning for data privacy, and ii)
adversarial training at the training stage and randomisation at the testing
stage for model robustness. The experiments show that our proposed framework
can effectively protect the speech data locally and improve the model
robustness against a series of adversarial attacks.",2203.04696v1,https://arxiv.org/pdf/2203.04696v1
"Robust Multi-Task Learning and Online Refinement for Spacecraft Pose
  Estimation across Domain Gap","Tae Ha Park, Simone D'Amico","This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural
Network (CNN) for pose estimation of noncooperative spacecraft across domain
gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared
multi-scale feature encoder and multiple prediction heads that perform
different tasks on a shared feature output. These tasks are all related to
detection and pose estimation of a target spacecraft from an image, such as
prediction of pre-defined satellite keypoints, direct pose regression, and
binary segmentation of the satellite foreground. It is shown that by jointly
training on different yet related tasks with extensive data augmentations on
synthetic images only, the shared encoder learns features that are common
across image domains that have fundamentally different visual characteristics
compared to synthetic images. This work also introduces Online Domain
Refinement (ODR) which refines the parameters of the normalization layers of
SPNv2 on the target domain images online at deployment. Specifically, ODR
performs self-supervised entropy minimization of the predicted satellite
foreground, thereby improving the CNN's performance on the target domain images
without their pose labels and with minimal computational efforts. The GitHub
repository for SPNv2 is available at https://github.com/tpark94/spnv2.",2203.04275v6,https://arxiv.org/pdf/2203.04275v6
"Adaptative Perturbation Patterns: Realistic Adversarial Learning for
  Robust Intrusion Detection","João Vitorino, Nuno Oliveira, Isabel Praça","Adversarial attacks pose a major threat to machine learning and to the
systems that rely on it. In the cybersecurity domain, adversarial cyber-attack
examples capable of evading detection are especially concerning. Nonetheless,
an example generated for a domain with tabular data must be realistic within
that domain. This work establishes the fundamental constraint levels required
to achieve realism and introduces the Adaptative Perturbation Pattern Method
(A2PM) to fulfill these constraints in a gray-box setting. A2PM relies on
pattern sequences that are independently adapted to the characteristics of each
class to create valid and coherent data perturbations. The proposed method was
evaluated in a cybersecurity case study with two scenarios: Enterprise and
Internet of Things (IoT) networks. Multilayer Perceptron (MLP) and Random
Forest (RF) classifiers were created with regular and adversarial training,
using the CIC-IDS2017 and IoT-23 datasets. In each scenario, targeted and
untargeted attacks were performed against the classifiers, and the generated
examples were compared with the original network traffic flows to assess their
realism. The obtained results demonstrate that A2PM provides a scalable
generation of realistic adversarial examples, which can be advantageous for
both adversarial training and attacks.",2203.04234v2,https://arxiv.org/pdf/2203.04234v2
Robustly-reliable learners under poisoning attacks,"Maria-Florina Balcan, Avrim Blum, Steve Hanneke, Dravyansh Sharma","Data poisoning attacks, in which an adversary corrupts a training set with
the goal of inducing specific desired mistakes, have raised substantial
concern: even just the possibility of such an attack can make a user no longer
trust the results of a learning system. In this work, we show how to achieve
strong robustness guarantees in the face of such attacks across multiple axes.
  We provide robustly-reliable predictions, in which the predicted label is
guaranteed to be correct so long as the adversary has not exceeded a given
corruption budget, even in the presence of instance targeted attacks, where the
adversary knows the test example in advance and aims to cause a specific
failure on that example. Our guarantees are substantially stronger than those
in prior approaches, which were only able to provide certificates that the
prediction of the learning algorithm does not change, as opposed to certifying
that the prediction is correct, as we are able to achieve in our work.
Remarkably, we provide a complete characterization of learnability in this
setting, in particular, nearly-tight matching upper and lower bounds on the
region that can be certified, as well as efficient algorithms for computing
this region given an ERM oracle. Moreover, for the case of linear separators
over logconcave distributions, we provide efficient truly polynomial time
algorithms (i.e., non-oracle algorithms) for such robustly-reliable
predictions.
  We also extend these results to the active setting where the algorithm
adaptively asks for labels of specific informative examples, and the difficulty
is that the adversary might even be adaptive to this interaction, as well as to
the agnostic learning setting where there is no perfect classifier even over
the uncorrupted data.",2203.04160v1,https://arxiv.org/pdf/2203.04160v1
"Plumeria at SemEval-2022 Task 6: Robust Approaches for Sarcasm Detection
  for English and Arabic Using Transformers and Data Augmentation","Shubham Kumar Nigam, Mosab Shaheen","This paper describes our submission to SemEval-2022 Task 6 on sarcasm
detection and its five subtasks for English and Arabic. Sarcasm conveys a
meaning which contradicts the literal meaning, and it is mainly found on social
networks. It has a significant role in understanding the intention of the user.
For detecting sarcasm, we used deep learning techniques based on transformers
due to its success in the field of Natural Language Processing (NLP) without
the need for feature engineering. The datasets were taken from tweets. We
created new datasets by augmenting with external data or by using word
embeddings and repetition of instances. Experiments were done on the datasets
with different types of preprocessing because it is crucial in this task. The
rank of our team was consistent across four subtasks (fourth rank in three
subtasks and sixth rank in one subtask); whereas other teams might be in the
top ranks for some subtasks but rank drastically less in other subtasks. This
implies the robustness and stability of the models and the techniques we used.",2203.04111v1,https://arxiv.org/pdf/2203.04111v1
Geodesic Multi-Modal Mixup for Robust Fine-Tuning,"Changdae Oh, Junhyuk So, Hoyoon Byun, YongTaek Lim, Minchul Shin, Jong-June Jeon, Kyungwoo Song","Pre-trained multi-modal models, such as CLIP, provide transferable embeddings
and show promising results in diverse applications. However, the analysis of
learned multi-modal embeddings is relatively unexplored, and the embedding
transferability can be improved. In this work, we observe that CLIP holds
separated embedding subspaces for two different modalities, and then we
investigate it through the lens of uniformity-alignment to measure the quality
of learned representation. Both theoretically and empirically, we show that
CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack
of alignment and uniformity might restrict the transferability and robustness
of embeddings. To this end, we devise a new fine-tuning method for robust
representation equipping better alignment and uniformity. First, we propose a
Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to
generate hard negative samples on the hypersphere. Then, we fine-tune the model
on hard negatives as well as original negatives and positives with contrastive
loss. Based on the theoretical analysis about hardness guarantee and limiting
behavior, we justify the use of our method. Extensive experiments on retrieval,
calibration, few- or zero-shot classification (under distribution shift),
embedding arithmetic, and image captioning further show that our method
provides transferable representations, enabling robust model adaptation on
diverse tasks. Code: https://github.com/changdaeoh/multimodal-mixup",2203.03897v4,https://arxiv.org/pdf/2203.03897v4
"Towards Efficient Data-Centric Robust Machine Learning with Noise-based
  Augmentation","Xiaogeng Liu, Haoyu Wang, Yechao Zhang, Fangzhou Wu, Shengshan Hu","The data-centric machine learning aims to find effective ways to build
appropriate datasets which can improve the performance of AI models. In this
paper, we mainly focus on designing an efficient data-centric scheme to improve
robustness for models towards unforeseen malicious inputs in the black-box test
settings. Specifically, we introduce a noised-based data augmentation method
which is composed of Gaussian Noise, Salt-and-Pepper noise, and the PGD
adversarial perturbations. The proposed method is built on lightweight
algorithms and proved highly effective based on comprehensive evaluations,
showing good efficiency on computation cost and robustness enhancement. In
addition, we share our insights about the data-centric robust machine learning
gained from our experiments.",2203.03810v1,https://arxiv.org/pdf/2203.03810v1
Robustness and Usefulness in AI Explanation Methods,Erick Galinkin,"Explainability in machine learning has become incredibly important as machine
learning-powered systems become ubiquitous and both regulation and public
sentiment begin to demand an understanding of how these systems make decisions.
As a result, a number of explanation methods have begun to receive widespread
adoption. This work summarizes, compares, and contrasts three popular
explanation methods: LIME, SmoothGrad, and SHAP. We evaluate these methods with
respect to: robustness, in the sense of sample complexity and stability;
understandability, in the sense that provided explanations are consistent with
user expectations; and usability, in the sense that the explanations allow for
the model to be modified based on the output. This work concludes that current
explanation methods are insufficient; that putting faith in and adopting these
methods may actually be worse than simply not using them.",2203.03729v1,https://arxiv.org/pdf/2203.03729v1
Shift-Robust Node Classification via Graph Adversarial Clustering,"Qi Zhu, Chao Zhang, Chanyoung Park, Carl Yang, Jiawei Han","Graph Neural Networks (GNNs) are de facto node classification models in graph
structured data. However, during testing-time, these algorithms assume no data
shift, i.e., $\Pr_\text{train}(X,Y) = \Pr_\text{test}(X,Y)$. Domain adaption
methods can be adopted for data shift, yet most of them are designed to only
encourage similar feature distribution between source and target data.
Conditional shift on classes can still affect such adaption. Fortunately, graph
yields graph homophily across different data distributions. In response, we
propose Shift-Robust Node Classification (SRNC) to address these limitations.
We introduce an unsupervised cluster GNN on target graph to group the similar
nodes by graph homophily. An adversarial loss with label information on source
graph is used upon clustering objective. Then a shift-robust classifier is
optimized on training graph and adversarial samples on target graph, which are
generated by cluster GNN. We conduct experiments on both open-set shift and
representation-shift, which demonstrates the superior accuracy of SRNC on
generalizing to test graph with data shift. SRNC is consistently better than
previous SoTA domain adaption algorithm on graph that progressively use model
predictions on target graph for training.",2203.15802v1,https://arxiv.org/pdf/2203.15802v1
"ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness
  against Adversarial Patches","Maura Pintor, Daniele Angioni, Angelo Sotgiu, Luca Demetrio, Ambra Demontis, Battista Biggio, Fabio Roli","Adversarial patches are optimized contiguous pixel blocks in an input image
that cause a machine-learning model to misclassify it. However, their
optimization is computationally demanding, and requires careful hyperparameter
tuning, potentially leading to suboptimal robustness evaluations. To overcome
these issues, we propose ImageNet-Patch, a dataset to benchmark
machine-learning models against adversarial patches. It consists of a set of
patches, optimized to generalize across different models, and readily
applicable to ImageNet data after preprocessing them with affine
transformations. This process enables an approximate yet faster robustness
evaluation, leveraging the transferability of adversarial perturbations. We
showcase the usefulness of this dataset by testing the effectiveness of the
computed patches against 127 models. We conclude by discussing how our dataset
could be used as a benchmark for robustness, and how our methodology can be
generalized to other domains. We open source our dataset and evaluation code at
https://github.com/pralab/ImageNet-Patch.",2203.04412v1,https://arxiv.org/pdf/2203.04412v1
"Robust Modeling of Unknown Dynamical Systems via Ensemble Averaged
  Learning","Victor Churchill, Steve Manns, Zhen Chen, Dongbin Xiu","Recent work has focused on data-driven learning of the evolution of unknown
systems via deep neural networks (DNNs), with the goal of conducting long time
prediction of the evolution of the unknown system. Training a DNN with low
generalization error is a particularly important task in this case as error is
accumulated over time. Because of the inherent randomness in DNN training,
chiefly in stochastic optimization, there is uncertainty in the resulting
prediction, and therefore in the generalization error. Hence, the
generalization error can be viewed as a random variable with some probability
distribution. Well-trained DNNs, particularly those with many hyperparameters,
typically result in probability distributions for generalization error with low
bias but high variance. High variance causes variability and unpredictably in
the results of a trained DNN. This paper presents a computational technique
which decreases the variance of the generalization error, thereby improving the
reliability of the DNN model to generalize consistently. In the proposed
ensemble averaging method, multiple models are independently trained and model
predictions are averaged at each time step. A mathematical foundation for the
method is presented, including results regarding the distribution of the local
truncation error. In addition, three time-dependent differential equation
problems are considered as numerical examples, demonstrating the effectiveness
of the method to decrease variance of DNN predictions generally.",2203.03458v1,https://arxiv.org/pdf/2203.03458v1
"Knowledge Transfer in Deep Reinforcement Learning for Slice-Aware
  Mobility Robustness Optimization","Qi Liao, Tianlun Hu, Dan Wellington","The legacy mobility robustness optimization (MRO) in self-organizing networks
aims at improving handover performance by optimizing cell-specific handover
parameters. However, such solutions cannot satisfy the needs of next-generation
network with network slicing, because it only guarantees the received signal
strength but not the per-slice service quality. To provide the truly seamless
mobility service, we propose a deep reinforcement learning-based slice-aware
mobility robustness optimization (SAMRO) approach, which improves handover
performance with per-slice service assurance by optimizing slice-specific
handover parameters. Moreover, to allow safe and sample efficient online
training, we develop a two-step transfer learning scheme: 1) regularized
offline reinforcement learning, and 2) effective online fine-tuning with mixed
experience replay. System-level simulations show that compared against the
legacy MRO algorithms, SAMRO significantly improves slice-aware service
continuation while optimizing the handover performance.",2203.03227v1,https://arxiv.org/pdf/2203.03227v1
"Bandits Corrupted by Nature: Lower Bounds on Regret and Robust
  Optimistic Algorithm","Debabrota Basu, Odalric-Ambrym Maillard, Timothée Mathieu","We study the corrupted bandit problem, i.e. a stochastic multi-armed bandit
problem with $k$ unknown reward distributions, which are heavy-tailed and
corrupted by a history-independent adversary or Nature. To be specific, the
reward obtained by playing an arm comes from corresponding heavy-tailed reward
distribution with probability $1-\varepsilon \in (0.5,1]$ and an arbitrary
corruption distribution of unbounded support with probability $\varepsilon \in
[0,0.5)$.
  First, we provide $\textit{a problem-dependent lower bound on the regret}$ of
any corrupted bandit algorithm. The lower bounds indicate that the corrupted
bandit problem is harder than the classical stochastic bandit problem with
sub-Gaussian or heavy-tail rewards.
  Following that, we propose a novel UCB-type algorithm for corrupted bandits,
namely HubUCB, that builds on Huber's estimator for robust mean estimation.
Leveraging a novel concentration inequality of Huber's estimator, we prove that
HubUCB achieves a near-optimal regret upper bound.
  Since computing Huber's estimator has quadratic complexity, we further
introduce a sequential version of Huber's estimator that exhibits linear
complexity. We leverage this sequential estimator to design SeqHubUCB that
enjoys similar regret guarantees while reducing the computational burden.
  Finally, we experimentally illustrate the efficiency of HubUCB and SeqHubUCB
in solving corrupted bandits for different reward distributions and different
levels of corruptions.",2203.03186v2,https://arxiv.org/pdf/2203.03186v2
"Detecting data-driven robust statistical arbitrage strategies with deep
  neural networks","Ariel Neufeld, Julian Sester, Daiying Yin","We present an approach, based on deep neural networks, that allows
identifying robust statistical arbitrage strategies in financial markets.
Robust statistical arbitrage strategies refer to trading strategies that enable
profitable trading under model ambiguity. The presented novel methodology
allows to consider a large amount of underlying securities simultaneously and
does not depend on the identification of cointegrated pairs of assets, hence it
is applicable on high-dimensional financial markets or in markets where
classical pairs trading approaches fail. Moreover, we provide a method to build
an ambiguity set of admissible probability measures that can be derived from
observed market data. Thus, the approach can be considered as being model-free
and entirely data-driven. We showcase the applicability of our method by
providing empirical investigations with highly profitable trading performances
even in 50 dimensions, during financial crises, and when the cointegration
relationship between asset pairs stops to persist.",2203.03179v4,https://arxiv.org/pdf/2203.03179v4
"$A^{3}D$: A Platform of Searching for Robust Neural Architectures and
  Efficient Adversarial Attacks","Jialiang Sun, Wen Yao, Tingsong Jiang, Chao Li, Xiaoqian Chen","The robustness of deep neural networks (DNN) models has attracted increasing
attention due to the urgent need for security in many applications. Numerous
existing open-sourced tools or platforms are developed to evaluate the
robustness of DNN models by ensembling the majority of adversarial attack or
defense algorithms. Unfortunately, current platforms do not possess the ability
to optimize the architectures of DNN models or the configuration of adversarial
attacks to further enhance the robustness of models or the performance of
adversarial attacks. To alleviate these problems, in this paper, we first
propose a novel platform called auto adversarial attack and defense ($A^{3}D$),
which can help search for robust neural network architectures and efficient
adversarial attacks. In $A^{3}D$, we employ multiple neural architecture search
methods, which consider different robustness evaluation metrics, including four
types of noises: adversarial noise, natural noise, system noise, and quantified
metrics, resulting in finding robust architectures. Besides, we propose a
mathematical model for auto adversarial attack, and provide multiple
optimization algorithms to search for efficient adversarial attacks. In
addition, we combine auto adversarial attack and defense together to form a
unified framework. Among auto adversarial defense, the searched efficient
attack can be used as the new robustness evaluation to further enhance the
robustness. In auto adversarial attack, the searched robust architectures can
be utilized as the threat model to help find stronger adversarial attacks.
Experiments on CIFAR10, CIFAR100, and ImageNet datasets demonstrate the
feasibility and effectiveness of the proposed platform, which can also provide
a benchmark and toolkit for researchers in the application of automated machine
learning in evaluating and improving the DNN model robustnesses.",2203.03128v2,https://arxiv.org/pdf/2203.03128v2
A Robust Framework of Chromosome Straightening with ViT-Patch GAN,"Sifan Song, Jinfeng Wang, Fengrui Cheng, Qirui Cao, Yihan Zuo, Yongteng Lei, Ruomai Yang, Chunxiao Yang, Frans Coenen, Jia Meng, Kang Dang, Jionglong Su","Chromosomes carry the genetic information of humans. They exhibit non-rigid
and non-articulated nature with varying degrees of curvature. Chromosome
straightening is an important step for subsequent karyotype construction,
pathological diagnosis and cytogenetic map development. However, robust
chromosome straightening remains challenging, due to the unavailability of
training images, distorted chromosome details and shapes after straightening,
as well as poor generalization capability. In this paper, we propose a novel
architecture, ViT-Patch GAN, consisting of a self-learned motion transformation
generator and a Vision Transformer-based patch (ViT-Patch) discriminator. The
generator learns the motion representation of chromosomes for straightening.
With the help of the ViT-Patch discriminator, the straightened chromosomes
retain more shape and banding pattern details. The experimental results show
that the proposed method achieves better performance on Fr\'echet Inception
Distance (FID), Learned Perceptual Image Patch Similarity (LPIPS) and
downstream chromosome classification accuracy, and shows excellent
generalization capability on a large dataset.",2203.02901v2,https://arxiv.org/pdf/2203.02901v2
"Distributional Hardness Against Preconditioned Lasso via Erasure-Robust
  Designs","Jonathan A. Kelner, Frederic Koehler, Raghu Meka, Dhruv Rohatgi","Sparse linear regression with ill-conditioned Gaussian random designs is
widely believed to exhibit a statistical/computational gap, but there is
surprisingly little formal evidence for this belief, even in the form of
examples that are hard for restricted classes of algorithms. Recent work has
shown that, for certain covariance matrices, the broad class of Preconditioned
Lasso programs provably cannot succeed on polylogarithmically sparse signals
with a sublinear number of samples. However, this lower bound only shows that
for every preconditioner, there exists at least one signal that it fails to
recover successfully. This leaves open the possibility that, for example,
trying multiple different preconditioners solves every sparse linear regression
problem.
  In this work, we prove a stronger lower bound that overcomes this issue. For
an appropriate covariance matrix, we construct a single signal distribution on
which any invertibly-preconditioned Lasso program fails with high probability,
unless it receives a linear number of samples.
  Surprisingly, at the heart of our lower bound is a new positive result in
compressed sensing. We show that standard sparse random designs are with high
probability robust to adversarial measurement erasures, in the sense that if
$b$ measurements are erased, then all but $O(b)$ of the coordinates of the
signal are still information-theoretically identifiable. To our knowledge, this
is the first time that partial recoverability of arbitrary sparse signals under
erasures has been studied in compressed sensing.",2203.02824v1,https://arxiv.org/pdf/2203.02824v1
A Robust Spectral Algorithm for Overcomplete Tensor Decomposition,"Samuel B. Hopkins, Tselil Schramm, Jonathan Shi","We give a spectral algorithm for decomposing overcomplete order-4 tensors, so
long as their components satisfy an algebraic non-degeneracy condition that
holds for nearly all (all but an algebraic set of measure $0$) tensors over
$(\mathbb{R}^d)^{\otimes 4}$ with rank $n \le d^2$. Our algorithm is robust to
adversarial perturbations of bounded spectral norm.
  Our algorithm is inspired by one which uses the sum-of-squares semidefinite
programming hierarchy (Ma, Shi, and Steurer STOC'16, arXiv:1610.01980), and we
achieve comparable robustness and overcompleteness guarantees under similar
algebraic assumptions. However, our algorithm avoids semidefinite programming
and may be implemented as a series of basic linear-algebraic operations. We
consequently obtain a much faster running time than semidefinite programming
methods: our algorithm runs in time $\tilde O(n^2d^3) \le \tilde O(d^7)$, which
is subquadratic in the input size $d^4$ (where we have suppressed factors
related to the condition number of the input tensor).",2203.02790v1,https://arxiv.org/pdf/2203.02790v1
"Towards Robust Part-aware Instance Segmentation for Industrial Bin
  Picking","Yidan Feng, Biqi Yang, Xianzhi Li, Chi-Wing Fu, Rui Cao, Kai Chen, Qi Dou, Mingqiang Wei, Yun-Hui Liu, Pheng-Ann Heng","Industrial bin picking is a challenging task that requires accurate and
robust segmentation of individual object instances. Particularly, industrial
objects can have irregular shapes, that is, thin and concave, whereas in
bin-picking scenarios, objects are often closely packed with strong occlusion.
To address these challenges, we formulate a novel part-aware instance
segmentation pipeline. The key idea is to decompose industrial objects into
correlated approximate convex parts and enhance the object-level segmentation
with part-level segmentation. We design a part-aware network to predict part
masks and part-to-part offsets, followed by a part aggregation module to
assemble the recognized parts into instances. To guide the network learning, we
also propose an automatic label decoupling scheme to generate ground-truth
part-level labels from instance-level labels. Finally, we contribute the first
instance segmentation dataset, which contains a variety of industrial objects
that are thin and have non-trivial shapes. Extensive experimental results on
various industrial objects demonstrate that our method can achieve the best
segmentation results compared with the state-of-the-art approaches.",2203.02767v1,https://arxiv.org/pdf/2203.02767v1
"Chance-Constrained Optimization in Contact-Rich Systems for Robust
  Manipulation","Yuki Shirai, Devesh K. Jha, Arvind Raghunathan, Diego Romeres","This paper presents a chance-constrained formulation for robust trajectory
optimization during manipulation. In particular, we present a
chance-constrained optimization for Stochastic Discrete-time Linear
Complementarity Systems (SDLCS). To solve the optimization problem, we
formulate Mixed-Integer Quadratic Programming with Chance Constraints (MIQPCC).
In our formulation, we explicitly consider joint chance constraints for
complementarity as well as states to capture the stochastic evolution of
dynamics. We evaluate robustness of our optimized trajectories in simulation on
several systems. The proposed approach outperforms some recent approaches for
robust trajectory optimization for SDLCS.",2203.02616v1,https://arxiv.org/pdf/2203.02616v1
"Sparsity-Inducing Categorical Prior Improves Robustness of the
  Information Bottleneck","Anirban Samaddar, Sandeep Madireddy, Prasanna Balaprakash, Tapabrata Maiti, Gustavo de los Campos, Ian Fischer","The information bottleneck framework provides a systematic approach to
learning representations that compress nuisance information in the input and
extract semantically meaningful information about predictions. However, the
choice of a prior distribution that fixes the dimensionality across all the
data can restrict the flexibility of this approach for learning robust
representations. We present a novel sparsity-inducing spike-slab categorical
prior that uses sparsity as a mechanism to provide the flexibility that allows
each data point to learn its own dimension distribution. In addition, it
provides a mechanism for learning a joint distribution of the latent variable
and the sparsity and hence can account for the complete uncertainty in the
latent space. Through a series of experiments using in-distribution and
out-of-distribution learning scenarios on the MNIST, CIFAR-10, and ImageNet
data, we show that the proposed approach improves accuracy and robustness
compared to traditional fixed-dimensional priors, as well as other sparsity
induction mechanisms for latent variable models proposed in the literature.",2203.02592v2,https://arxiv.org/pdf/2203.02592v2
No More Than 6ft Apart: Robust K-Means via Radius Upper Bounds,"Ahmed Imtiaz Humayun, Randall Balestriero, Anastasios Kyrillidis, Richard Baraniuk","Centroid based clustering methods such as k-means, k-medoids and k-centers
are heavily applied as a go-to tool in exploratory data analysis. In many
cases, those methods are used to obtain representative centroids of the data
manifold for visualization or summarization of a dataset. Real world datasets
often contain inherent abnormalities, e.g., repeated samples and sampling bias,
that manifest imbalanced clustering. We propose to remedy such a scenario by
introducing a maximal radius constraint $r$ on the clusters formed by the
centroids, i.e., samples from the same cluster should not be more than $2r$
apart in terms of $\ell_2$ distance. We achieve this constraint by solving a
semi-definite program, followed by a linear assignment problem with quadratic
constraints. Through qualitative results, we show that our proposed method is
robust towards dataset imbalances and sampling artifacts. To the best of our
knowledge, ours is the first constrained k-means clustering method with hard
radius constraints. Codes at https://bit.ly/kmeans-constrained",2203.02502v2,https://arxiv.org/pdf/2203.02502v2
"AutoMO-Mixer: An automated multi-objective Mixer model for balanced,
  safe and robust prediction in medicine","Xi Chen, Jiahuan Lv, Dehua Feng, Xuanqin Mou, Ling Bai, Shu Zhang, Zhiguo Zhou","Accurately identifying patient's status through medical images plays an
important role in diagnosis and treatment. Artificial intelligence (AI),
especially the deep learning, has achieved great success in many fields.
However, more reliable AI model is needed in image guided diagnosis and
therapy. To achieve this goal, developing a balanced, safe and robust model
with a unified framework is desirable. In this study, a new unified model
termed as automated multi-objective Mixer (AutoMO-Mixer) model was developed,
which utilized a recent developed multiple layer perceptron Mixer (MLP-Mixer)
as base. To build a balanced model, sensitivity and specificity were considered
as the objective functions simultaneously in training stage. Meanwhile, a new
evidential reasoning based on entropy was developed to achieve a safe and
robust model in testing stage. The experiment on an optical coherence
tomography dataset demonstrated that AutoMO-Mixer can obtain safer, more
balanced, and robust results compared with MLP-Mixer and other available
models.",2203.02384v1,https://arxiv.org/pdf/2203.02384v1
Distributionally Robust Bayesian Optimization with $\varphi$-divergences,"Hisham Husain, Vu Nguyen, Anton van den Hengel","The study of robustness has received much attention due to its inevitability
in data-driven settings where many systems face uncertainty. One such example
of concern is Bayesian Optimization (BO), where uncertainty is multi-faceted,
yet there only exists a limited number of works dedicated to this direction. In
particular, there is the work of Kirschner et al. (2020), which bridges the
existing literature of Distributionally Robust Optimization (DRO) by casting
the BO problem from the lens of DRO. While this work is pioneering, it
admittedly suffers from various practical shortcomings such as finite contexts
assumptions, leaving behind the main question Can one devise a computationally
tractable algorithm for solving this DRO-BO problem? In this work, we tackle
this question to a large degree of generality by considering robustness against
data-shift in $\varphi$-divergences, which subsumes many popular choices, such
as the $\chi^2$-divergence, Total Variation, and the extant Kullback-Leibler
(KL) divergence. We show that the DRO-BO problem in this setting is equivalent
to a finite-dimensional optimization problem which, even in the continuous
context setting, can be easily implemented with provable sublinear regret
bounds. We then show experimentally that our method surpasses existing methods,
attesting to the theoretical results.",2203.02128v5,https://arxiv.org/pdf/2203.02128v5
Adversarial Patterns: Building Robust Android Malware Classifiers,"Dipkamal Bhusal, Nidhi Rastogi","Machine learning models are increasingly being adopted across various fields,
such as medicine, business, autonomous vehicles, and cybersecurity, to analyze
vast amounts of data, detect patterns, and make predictions or recommendations.
In the field of cybersecurity, these models have made significant improvements
in malware detection. However, despite their ability to understand complex
patterns from unstructured data, these models are susceptible to adversarial
attacks that perform slight modifications in malware samples, leading to
misclassification from malignant to benign. Numerous defense approaches have
been proposed to either detect such adversarial attacks or improve model
robustness. These approaches have resulted in a multitude of attack and defense
techniques and the emergence of a field known as `adversarial machine
learning.' In this survey paper, we provide a comprehensive review of
adversarial machine learning in the context of Android malware classifiers.
Android is the most widely used operating system globally and is an easy target
for malicious agents. The paper first presents an extensive background on
Android malware classifiers, followed by an examination of the latest
advancements in adversarial attacks and defenses. Finally, the paper provides
guidelines for designing robust malware classifiers and outlines research
directions for the future.",2203.02121v2,https://arxiv.org/pdf/2203.02121v2
Why adversarial training can hurt robust accuracy,"Jacob Clarysse, Julia Hörrmann, Fanny Yang","Machine learning classifiers with high test accuracy often perform poorly
under adversarial attacks. It is commonly believed that adversarial training
alleviates this issue. In this paper, we demonstrate that, surprisingly, the
opposite may be true -- Even though adversarial training helps when enough data
is available, it may hurt robust generalization in the small sample size
regime. We first prove this phenomenon for a high-dimensional linear
classification setting with noiseless observations. Our proof provides
explanatory insights that may also transfer to feature learning models.
Further, we observe in experiments on standard image datasets that the same
behavior occurs for perceptible attacks that effectively reduce class
information such as mask attacks and object corruptions.",2203.02006v1,https://arxiv.org/pdf/2203.02006v1
Robustness and Adaptation to Hidden Factors of Variation,"William Paul, Philippe Burlina","We tackle here a specific, still not widely addressed aspect, of AI
robustness, which consists of seeking invariance / insensitivity of model
performance to hidden factors of variations in the data. Towards this end, we
employ a two step strategy that a) does unsupervised discovery, via generative
models, of sensitive factors that cause models to under-perform, and b)
intervenes models to make their performance invariant to these sensitive
factors' influence. We consider 3 separate interventions for robustness,
including: data augmentation, semantic consistency, and adversarial alignment.
We evaluate our method using metrics that measure trade offs between invariance
(insensitivity) and overall performance (utility) and show the benefits of our
method for 3 settings (unsupervised, semi-supervised and generalization).",2203.01864v1,https://arxiv.org/pdf/2203.01864v1
"Robust PAC$^m$: Training Ensemble Models Under Misspecification and
  Outliers","Matteo Zecchin, Sangwoo Park, Osvaldo Simeone, Marios Kountouris, David Gesbert","Standard Bayesian learning is known to have suboptimal generalization
capabilities under misspecification and in the presence of outliers. PAC-Bayes
theory demonstrates that the free energy criterion minimized by Bayesian
learning is a bound on the generalization error for Gibbs predictors (i.e., for
single models drawn at random from the posterior) under the assumption of
sampling distributions uncontaminated by outliers. This viewpoint provides a
justification for the limitations of Bayesian learning when the model is
misspecified, requiring ensembling, and when data is affected by outliers. In
recent work, PAC-Bayes bounds -- referred to as PAC$^m$ -- were derived to
introduce free energy metrics that account for the performance of ensemble
predictors, obtaining enhanced performance under misspecification. This work
presents a novel robust free energy criterion that combines the generalized
logarithm score function with PAC$^m$ ensemble bounds. The proposed free energy
training criterion produces predictive distributions that are able to
concurrently counteract the detrimental effects of misspecification -- with
respect to both likelihood and prior distribution -- and outliers.",2203.01859v3,https://arxiv.org/pdf/2203.01859v3
"On Practical Reinforcement Learning: Provable Robustness, Scalability,
  and Statistical Efficiency",Thanh Nguyen-Tang,"This thesis rigorously studies fundamental reinforcement learning (RL)
methods in modern practical considerations, including robust RL, distributional
RL, and offline RL with neural function approximation. The thesis first
prepares the readers with an overall overview of RL and key technical
background in statistics and optimization. In each of the settings, the thesis
motivates the problems to be studied, reviews the current literature, provides
computationally efficient algorithms with provable efficiency guarantees, and
concludes with future research directions. The thesis makes fundamental
contributions to the three settings above, both algorithmically, theoretically,
and empirically, while staying relevant to practical considerations.",2203.01758v1,https://arxiv.org/pdf/2203.01758v1
"Detection of Word Adversarial Examples in Text Classification: Benchmark
  and Baseline via Robust Density Estimation","KiYoon Yoo, Jangho Kim, Jiho Jang, Nojun Kwak","Word-level adversarial attacks have shown success in NLP models, drastically
decreasing the performance of transformer-based models in recent years. As a
countermeasure, adversarial defense has been explored, but relatively few
efforts have been made to detect adversarial examples. However, detecting
adversarial examples may be crucial for automated tasks (e.g. review sentiment
analysis) that wish to amass information about a certain population and
additionally be a step towards a robust defense system. To this end, we release
a dataset for four popular attack methods on four datasets and four models to
encourage further research in this field. Along with it, we propose a
competitive baseline based on density estimation that has the highest AUC on 29
out of 30 dataset-attack-model combinations. Source code is available in
https://github.com/anoymous92874838/text-adv-detection.",2203.01677v1,https://arxiv.org/pdf/2203.01677v1
"Ensemble Methods for Robust Support Vector Machines using Integer
  Programming",Jannis Kurtz,"In this work we study binary classification problems where we assume that our
training data is subject to uncertainty, i.e. the precise data points are not
known. To tackle this issue in the field of robust machine learning the aim is
to develop models which are robust against small perturbations in the training
data. We study robust support vector machines (SVM) and extend the classical
approach by an ensemble method which iteratively solves a non-robust SVM on
different perturbations of the dataset, where the perturbations are derived by
an adversarial problem. Afterwards for classification of an unknown data point
we perform a majority vote of all calculated SVM solutions. We study three
different variants for the adversarial problem, the exact problem, a relaxed
variant and an efficient heuristic variant. While the exact and the relaxed
variant can be modeled using integer programming formulations, the heuristic
one can be implemented by an easy and efficient algorithm. All derived methods
are tested on random and realistic datasets and the results indicate that the
derived ensemble methods have a much more stable behaviour when changing the
protection level compared to the classical robust SVM model.",2203.01606v1,https://arxiv.org/pdf/2203.01606v1
Semi-supervised Learning using Robust Loss,"Wenhui Cui, Haleh Akrami, Anand A. Joshi, Richard M. Leahy","The amount of manually labeled data is limited in medical applications, so
semi-supervised learning and automatic labeling strategies can be an asset for
training deep neural networks. However, the quality of the automatically
generated labels can be uneven and inferior to manual labels. In this paper, we
suggest a semi-supervised training strategy for leveraging both manually
labeled data and extra unlabeled data. In contrast to the existing approaches,
we apply robust loss for the automated labeled data to automatically compensate
for the uneven data quality using a teacher-student framework. First, we
generate pseudo-labels for unlabeled data using a teacher model pre-trained on
labeled data. These pseudo-labels are noisy, and using them along with labeled
data for training a deep neural network can severely degrade learned feature
representations and the generalization of the network. Here we mitigate the
effect of these pseudo-labels by using robust loss functions. Specifically, we
use three robust loss functions, namely beta cross-entropy, symmetric
cross-entropy, and generalized cross-entropy. We show that our proposed
strategy improves the model performance by compensating for the uneven quality
of labels in image classification as well as segmentation applications.",2203.01524v1,https://arxiv.org/pdf/2203.01524v1
"BatchFormer: Learning to Explore Sample Relationships for Robust
  Representation Learning","Zhi Hou, Baosheng Yu, Dacheng Tao","Despite the success of deep neural networks, there are still many challenges
in deep representation learning due to the data scarcity issues such as data
imbalance, unseen distribution, and domain shift. To address the
above-mentioned issues, a variety of methods have been devised to explore the
sample relationships in a vanilla way (i.e., from the perspectives of either
the input or the loss function), failing to explore the internal structure of
deep neural networks for learning with sample relationships. Inspired by this,
we propose to enable deep neural networks themselves with the ability to learn
the sample relationships from each mini-batch. Specifically, we introduce a
batch transformer module or BatchFormer, which is then applied into the batch
dimension of each mini-batch to implicitly explore sample relationships during
training. By doing this, the proposed method enables the collaboration of
different samples, e.g., the head-class samples can also contribute to the
learning of the tail classes for long-tailed recognition. Furthermore, to
mitigate the gap between training and testing, we share the classifier between
with or without the BatchFormer during training, which can thus be removed
during testing. We perform extensive experiments on over ten datasets and the
proposed method achieves significant improvements on different data scarcity
applications without any bells and whistles, including the tasks of long-tailed
recognition, compositional zero-shot learning, domain generalization, and
contrastive learning. Code will be made publicly available at
https://github.com/zhihou7/BatchFormer.",2203.01522v2,https://arxiv.org/pdf/2203.01522v2
"Correct-N-Contrast: A Contrastive Approach for Improving Robustness to
  Spurious Correlations","Michael Zhang, Nimit S. Sohoni, Hongyang R. Zhang, Chelsea Finn, Christopher Ré","Spurious correlations pose a major challenge for robust machine learning.
Models trained with empirical risk minimization (ERM) may learn to rely on
correlations between class labels and spurious attributes, leading to poor
performance on data groups without these correlations. This is particularly
challenging to address when spurious attribute labels are unavailable. To
improve worst-group performance on spuriously correlated data without training
attribute labels, we propose Correct-N-Contrast (CNC), a contrastive approach
to directly learn representations robust to spurious correlations. As ERM
models can be good spurious attribute predictors, CNC works by (1) using a
trained ERM model's outputs to identify samples with the same class but
dissimilar spurious features, and (2) training a robust model with contrastive
learning to learn similar representations for same-class samples. To support
CNC, we introduce new connections between worst-group error and a
representation alignment loss that CNC aims to minimize. We empirically observe
that worst-group error closely tracks with alignment loss, and prove that the
alignment loss over a class helps upper-bound the class's worst-group vs.
average error gap. On popular benchmarks, CNC reduces alignment loss
drastically, and achieves state-of-the-art worst-group accuracy by 3.6% average
absolute lift. CNC is also competitive with oracle methods that require group
labels.",2203.01517v1,https://arxiv.org/pdf/2203.01517v1
Enhancing Adversarial Robustness for Deep Metric Learning,"Mo Zhou, Vishal M. Patel","Owing to security implications of adversarial vulnerability, adversarial
robustness of deep metric learning models has to be improved. In order to avoid
model collapse due to excessively hard examples, the existing defenses dismiss
the min-max adversarial training, but instead learn from a weak adversary
inefficiently. Conversely, we propose Hardness Manipulation to efficiently
perturb the training triplet till a specified level of hardness for adversarial
training, according to a harder benign triplet or a pseudo-hardness function.
It is flexible since regular training and min-max adversarial training are its
boundary cases. Besides, Gradual Adversary, a family of pseudo-hardness
functions is proposed to gradually increase the specified hardness level during
training for a better balance between performance and robustness. Additionally,
an Intra-Class Structure loss term among benign and adversarial examples
further improves model robustness and efficiency. Comprehensive experimental
results suggest that the proposed method, although simple in its form,
overwhelmingly outperforms the state-of-the-art defenses in terms of
robustness, training efficiency, as well as performance on benign examples.",2203.01439v1,https://arxiv.org/pdf/2203.01439v1
"Detecting Chronic Kidney Disease(CKD) at the Initial Stage: A Novel
  Hybrid Feature-selection Method and Robust Data Preparation Pipeline for
  Different ML Techniques","Md. Taufiqul Haque Khan Tusar, Md. Touhidul Islam, Foyjul Islam Raju","Chronic Kidney Disease (CKD) has infected almost 800 million people around
the world. Around 1.7 million people die each year because of it. Detecting CKD
in the initial stage is essential for saving millions of lives. Many
researchers have applied distinct Machine Learning (ML) methods to detect CKD
at an early stage, but detailed studies are still missing. We present a
structured and thorough method for dealing with the complexities of medical
data with optimal performance. Besides, this study will assist researchers in
producing clear ideas on the medical data preparation pipeline. In this paper,
we applied KNN Imputation to impute missing values, Local Outlier Factor to
remove outliers, SMOTE to handle data imbalance, K-stratified K-fold
Cross-validation to validate the ML models, and a novel hybrid feature
selection method to remove redundant features. Applied algorithms in this study
are Support Vector Machine, Gaussian Naive Bayes, Decision Tree, Random Forest,
Logistic Regression, K-Nearest Neighbor, Gradient Boosting, Adaptive Boosting,
and Extreme Gradient Boosting. Finally, the Random Forest can detect CKD with
100% accuracy without any data leakage.",2203.01394v1,https://arxiv.org/pdf/2203.01394v1
"Adversarial Robustness of Neural-Statistical Features in Detection of
  Generative Transformers","Evan Crothers, Nathalie Japkowicz, Herna Viktor, Paula Branco","The detection of computer-generated text is an area of rapidly increasing
significance as nascent generative models allow for efficient creation of
compelling human-like text, which may be abused for the purposes of spam,
disinformation, phishing, or online influence campaigns. Past work has studied
detection of current state-of-the-art models, but despite a developing threat
landscape, there has been minimal analysis of the robustness of detection
methods to adversarial attacks. To this end, we evaluate neural and non-neural
approaches on their ability to detect computer-generated text, their robustness
against text adversarial attacks, and the impact that successful adversarial
attacks have on human judgement of text quality. We find that while statistical
features underperform neural features, statistical features provide additional
adversarial robustness that can be leveraged in ensemble detection models. In
the process, we find that previously effective complex phrasal features for
detection of computer-generated text hold little predictive power against
contemporary generative models, and identify promising statistical features to
use instead. Finally, we pioneer the usage of $\Delta$MAUVE as a proxy measure
for human judgement of adversarial text quality.",2203.07983v1,https://arxiv.org/pdf/2203.07983v1
"Robust Portfolio Design and Stock Price Prediction Using an Optimized
  LSTM Model","Jaydip Sen, Saikat Mondal, Gourab Nath","Accurate prediction of future prices of stocks is a difficult task to
perform. Even more challenging is to design an optimized portfolio with weights
allocated to the stocks in a way that optimizes its return and the risk. This
paper presents a systematic approach towards building two types of portfolios,
optimum risk, and eigen, for four critical economic sectors of India. The
prices of the stocks are extracted from the web from Jan 1, 2016, to Dec 31,
2020. Sector-wise portfolios are built based on their ten most significant
stocks. An LSTM model is also designed for predicting future stock prices. Six
months after the construction of the portfolios, i.e., on Jul 1, 2021, the
actual returns and the LSTM-predicted returns for the portfolios are computed.
A comparison of the predicted and the actual returns indicate a high accuracy
level of the LSTM model.",2204.01850v1,https://arxiv.org/pdf/2204.01850v1
"Naturally-meaningful and efficient descriptors: machine learning of
  material properties based on robust one-shot ab initio descriptors","Sherif Abdulkader Tawfik, Salvy P. Russo","Establishing a data-driven pipeline for the discovery of novel materials
requires the engineering of material features that can be feasibly calculated
and can be applied to predict a material's target properties. Here we propose a
new class of descriptors for describing crystal structures, which we term
Robust One-Shot Ab initio (ROSA) descriptors. ROSA is computationally cheap and
is shown to accurately predict a range of material properties. These simple and
intuitive class of descriptors are generated from the energetics of a material
at a low level of theory using an incomplete ab initio calculation. We
demonstrate how the incorporation of ROSA descriptors in ML-based property
prediction leads to accurate predictions over a wide range of crystals,
amorphized crystals, metal-organic frameworks and molecules. We believe that
the low computational cost and ease of use of these descriptors will
significantly improve ML-based predictions.",2203.03392v2,https://arxiv.org/pdf/2203.03392v2
"CD-GAN: a robust fusion-based generative adversarial network for
  unsupervised remote sensing change detection with heterogeneous sensors","Jin-Ju Wang, Nicolas Dobigeon, Marie Chabert, Ding-Cheng Wang, Ting-Zhu Huang, Jie Huang","In the context of Earth observation, change detection boils down to comparing
images acquired at different times by sensors of possibly different spatial
and/or spectral resolutions or different modalities (e.g., optical or radar).
Even when considering only optical images, this task has proven to be
challenging as soon as the sensors differ by their spatial and/or spectral
resolutions. This paper proposes a novel unsupervised change detection method
dedicated to images acquired by such so-called heterogeneous optical sensors.
It capitalizes on recent advances which formulate the change detection task
into a robust fusion framework. Adopting this formulation, the work reported in
this paper shows that any off-the-shelf network trained beforehand to fuse
optical images of different spatial and/or spectral resolutions can be easily
complemented with a network of the same architecture and embedded into an
adversarial framework to perform change detection. A comparison with
state-of-the-art change detection methods demonstrates the versatility and the
effectiveness of the proposed approach.",2203.00948v4,https://arxiv.org/pdf/2203.00948v4
Canonical foliations of neural networks: application to robustness,"Eliot Tron, Nicolas Couellan, Stéphane Puechmorel","Deep learning models are known to be vulnerable to adversarial attacks.
Adversarial learning is therefore becoming a crucial task. We propose a new
vision on neural network robustness using Riemannian geometry and foliation
theory. The idea is illustrated by creating a new adversarial attack that takes
into account the curvature of the data space. This new adversarial attack
called the two-step spectral attack is a piece-wise linear approximation of a
geodesic in the data space. The data space is treated as a (degenerate)
Riemannian manifold equipped with the pullback of the Fisher Information Metric
(FIM) of the neural network. In most cases, this metric is only semi-definite
and its kernel becomes a central object to study. A canonical foliation is
derived from this kernel. The curvature of transverse leaves gives the
appropriate correction to get a two-step approximation of the geodesic and
hence a new efficient adversarial attack. The method is first illustrated on a
2D toy example in order to visualize the neural network foliation and the
corresponding attacks. Next, experiments on the MNIST dataset with the proposed
technique and a state of the art attack presented in Zhao et al. (2019) are
reported. The result show that the proposed attack is more efficient at all
levels of available budget for the attack (norm of the attack), confirming that
the curvature of the transverse neural network FIM foliation plays an important
role in the robustness of neural networks.",2203.00922v2,https://arxiv.org/pdf/2203.00922v2
"Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor
  Perturbation","Wei Dai, Daniel Berleant","Accuracies of deep learning (DL) classifiers are often unstable in that they
may change significantly when retested on adversarial images, imperfect images,
or perturbed images. This paper adds to the fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. To measure
robust DL classifiers, previous research reported on single-factor corruption.
We created comprehensive 69 benchmarking image sets, including a clean set,
sets with single factor perturbations, and sets with two-factor perturbation
conditions. The state-of-the-art two-factor perturbation includes (a) two
digital perturbations (salt & pepper noise and Gaussian noise) applied in both
sequences, and (b) one digital perturbation (salt & pepper noise) and a
geometric perturbation (rotation) applied in both sequences. Previous research
evaluating DL classifiers has often used top-1/top-5 accuracy. We innovate a
new two-dimensional, statistical matrix to evaluating robustness of DL
classifiers. Also, we introduce a new visualization tool, including minimum
accuracy, maximum accuracy, mean accuracies, and coefficient of variation (CV),
for benchmarking robustness of DL classifiers. Comparing with single factor
corruption, we first report that using two-factor perturbed images improves
both robustness and accuracy of DL classifiers. All source codes and related
image sets are shared on the Website at http://cslinux.semo.edu/david/data to
support future academic research and industry projects.",2203.01323v1,https://arxiv.org/pdf/2203.01323v1
Adversarially Robust Learning with Tolerance,"Hassan Ashtiani, Vinayak Pathak, Ruth Urner","We initiate the study of tolerant adversarial PAC-learning with respect to
metric perturbation sets. In adversarial PAC-learning, an adversary is allowed
to replace a test point $x$ with an arbitrary point in a closed ball of radius
$r$ centered at $x$. In the tolerant version, the error of the learner is
compared with the best achievable error with respect to a slightly larger
perturbation radius $(1+\gamma)r$. This simple tweak helps us bridge the gap
between theory and practice and obtain the first PAC-type guarantees for
algorithmic techniques that are popular in practice.
  Our first result concerns the widely-used ``perturb-and-smooth'' approach for
adversarial learning. For perturbation sets with doubling dimension $d$, we
show that a variant of these approaches PAC-learns any hypothesis class
$\mathcal{H}$ with VC-dimension $v$ in the $\gamma$-tolerant adversarial
setting with $O\left(\frac{v(1+1/\gamma)^{O(d)}}{\varepsilon}\right)$ samples.
This is in contrast to the traditional (non-tolerant) setting in which, as we
show, the perturb-and-smooth approach can provably fail.
  Our second result shows that one can PAC-learn the same class using
$\widetilde{O}\left(\frac{d.v\log(1+1/\gamma)}{\varepsilon^2}\right)$ samples
even in the agnostic setting. This result is based on a novel compression-based
algorithm, and achieves a linear dependence on the doubling dimension as well
as the VC-dimension. This is in contrast to the non-tolerant setting where
there is no known sample complexity upper bound that depend polynomially on the
VC-dimension.",2203.00849v2,https://arxiv.org/pdf/2203.00849v2
A Conformer Based Acoustic Model for Robust Automatic Speech Recognition,"Yufeng Yang, Peidong Wang, DeLiang Wang","This study addresses robust automatic speech recognition (ASR) by introducing
a Conformer-based acoustic model. The proposed model builds on the wide
residual bi-directional long short-term memory network (WRBN) with
utterance-wise dropout and iterative speaker adaptation, but employs a
Conformer encoder instead of the recurrent network. The Conformer encoder uses
a convolution-augmented attention mechanism for acoustic modeling. The proposed
system is evaluated on the monaural ASR task of the CHiME-4 corpus. Coupled
with utterance-wise normalization and speaker adaptation, our model achieves
$6.25\%$ word error rate, which outperforms WRBN by $8.4\%$ relatively. In
addition, the proposed Conformer-based model is $18.3\%$ smaller in model size
and reduces total training time by $79.6\%$.",2203.00725v3,https://arxiv.org/pdf/2203.00725v3
Learning Robust Real-Time Cultural Transmission without Human Data,"Cultural General Intelligence Team, Avishkar Bhoopchand, Bethanie Brownfield, Adrian Collister, Agustin Dal Lago, Ashley Edwards, Richard Everett, Alexandre Frechette, Yanko Gitahy Oliveira, Edward Hughes, Kory W. Mathewson, Piermaria Mendolicchio, Julia Pawar, Miruna Pislar, Alex Platonov, Evan Senter, Sukhdeep Singh, Alexander Zacherl, Lei M. Zhang","Cultural transmission is the domain-general social skill that allows agents
to acquire and use information from each other in real-time with high fidelity
and recall. In humans, it is the inheritance process that powers cumulative
cultural evolution, expanding our skills, tools and knowledge across
generations. We provide a method for generating zero-shot, high recall cultural
transmission in artificially intelligent agents. Our agents succeed at
real-time cultural transmission from humans in novel contexts without using any
pre-collected human data. We identify a surprisingly simple set of ingredients
sufficient for generating cultural transmission and develop an evaluation
methodology for rigorously assessing it. This paves the way for cultural
evolution as an algorithm for developing artificial general intelligence.",2203.00715v1,https://arxiv.org/pdf/2203.00715v1
Global-Local Regularization Via Distributional Robustness,"Hoang Phan, Trung Le, Trung Phung, Tuan Anh Bui, Nhat Ho, Dinh Phung","Despite superior performance in many situations, deep neural networks are
often vulnerable to adversarial examples and distribution shifts, limiting
model generalization ability in real-world applications. To alleviate these
problems, recent approaches leverage distributional robustness optimization
(DRO) to find the most challenging distribution, and then minimize loss
function over this most challenging distribution. Regardless of achieving some
improvements, these DRO approaches have some obvious limitations. First, they
purely focus on local regularization to strengthen model robustness, missing a
global regularization effect which is useful in many real-world applications
(e.g., domain adaptation, domain generalization, and adversarial machine
learning). Second, the loss functions in the existing DRO approaches operate in
only the most challenging distribution, hence decouple with the original
distribution, leading to a restrictive modeling capability. In this paper, we
propose a novel regularization technique, following the veins of
Wasserstein-based DRO framework. Specifically, we define a particular joint
distribution and Wasserstein-based uncertainty, allowing us to couple the
original and most challenging distributions for enhancing modeling capability
and applying both local and global regularizations. Empirical studies on
different learning problems demonstrate that our proposed approach
significantly outperforms the existing regularization approaches in various
domains: semi-supervised learning, domain adaptation, domain generalization,
and adversarial machine learning.",2203.00553v3,https://arxiv.org/pdf/2203.00553v3
"Bayesian Optimisation for Robust Model Predictive Control under Model
  Parameter Uncertainty","Rel Guzman, Rafael Oliveira, Fabio Ramos","We propose an adaptive optimisation approach for tuning stochastic model
predictive control (MPC) hyper-parameters while jointly estimating probability
distributions of the transition model parameters based on performance rewards.
In particular, we develop a Bayesian optimisation (BO) algorithm with a
heteroscedastic noise model to deal with varying noise across the MPC
hyper-parameter and dynamics model parameter spaces. Typical homoscedastic
noise models are unrealistic for tuning MPC since stochastic controllers are
inherently noisy, and the level of noise is affected by their hyper-parameter
settings. We evaluate the proposed optimisation algorithm in simulated control
and robotics tasks where we jointly infer control and dynamics parameters.
Experimental results demonstrate that our approach leads to higher cumulative
rewards and more stable controllers.",2203.00551v3,https://arxiv.org/pdf/2203.00551v3
A Domain-Theoretic Framework for Robustness Analysis of Neural Networks,"Can Zhou, Razin A. Shaikh, Yiran Li, Amin Farjudian","A domain-theoretic framework is presented for validated robustness analysis
of neural networks. First, global robustness of a general class of networks is
analyzed. Then, using the fact that Edalat's domain-theoretic L-derivative
coincides with Clarke's generalized gradient, the framework is extended for
attack-agnostic local robustness analysis. The proposed framework is ideal for
designing algorithms which are correct by construction. This claim is
exemplified by developing a validated algorithm for estimation of Lipschitz
constant of feedforward regressors. The completeness of the algorithm is proved
over differentiable networks, and also over general position ReLU networks.
Computability results are obtained within the framework of effectively given
domains. Using the proposed domain model, differentiable and non-differentiable
networks can be analyzed uniformly. The validated algorithm is implemented
using arbitrary-precision interval arithmetic, and the results of some
experiments are presented. The software implementation is truly validated, as
it handles floating-point errors as well.",2203.00295v3,https://arxiv.org/pdf/2203.00295v3
Robust Multi-Agent Bandits Over Undirected Graphs,"Daniel Vial, Sanjay Shakkottai, R. Srikant","We consider a multi-agent multi-armed bandit setting in which $n$ honest
agents collaborate over a network to minimize regret but $m$ malicious agents
can disrupt learning arbitrarily. Assuming the network is the complete graph,
existing algorithms incur $O( (m + K/n) \log (T) / \Delta )$ regret in this
setting, where $K$ is the number of arms and $\Delta$ is the arm gap. For $m
\ll K$, this improves over the single-agent baseline regret of
$O(K\log(T)/\Delta)$.
  In this work, we show the situation is murkier beyond the case of a complete
graph. In particular, we prove that if the state-of-the-art algorithm is used
on the undirected line graph, honest agents can suffer (nearly) linear regret
until time is doubly exponential in $K$ and $n$. In light of this negative
result, we propose a new algorithm for which the $i$-th agent has regret $O( (
d_{\text{mal}}(i) + K/n) \log(T)/\Delta)$ on any connected and undirected
graph, where $d_{\text{mal}}(i)$ is the number of $i$'s neighbors who are
malicious. Thus, we generalize existing regret bounds beyond the complete graph
(where $d_{\text{mal}}(i) = m$), and show the effect of malicious agents is
entirely local (in the sense that only the $d_{\text{mal}}(i)$ malicious agents
directly connected to $i$ affect its long-term regret).",2203.00076v2,https://arxiv.org/pdf/2203.00076v2
Robust Training under Label Noise by Over-parameterization,"Sheng Liu, Zhihui Zhu, Qing Qu, Chong You","Recently, over-parameterized deep networks, with increasingly more network
parameters than training samples, have dominated the performances of modern
machine learning. However, when the training data is corrupted, it has been
well-known that over-parameterized networks tend to overfit and do not
generalize. In this work, we propose a principled approach for robust training
of over-parameterized deep networks in classification tasks where a proportion
of training labels are corrupted. The main idea is yet very simple: label noise
is sparse and incoherent with the network learned from clean data, so we model
the noise and learn to separate it from the data. Specifically, we model the
label noise via another sparse over-parameterization term, and exploit implicit
algorithmic regularizations to recover and separate the underlying corruptions.
Remarkably, when trained using such a simple method in practice, we demonstrate
state-of-the-art test accuracy against label noise on a variety of real
datasets. Furthermore, our experimental results are corroborated by theory on
simplified linear models, showing that exact separation between sparse noise
and low-rank data can be achieved under incoherent conditions. The work opens
many interesting directions for improving over-parameterized models by using
sparse over-parameterization and implicit regularization.",2202.14026v2,https://arxiv.org/pdf/2202.14026v2
"Probing the Robustness of Trained Metrics for Conversational Dialogue
  Systems","Jan Deriu, Don Tuggener, Pius von Däniken, Mark Cieliebak","This paper introduces an adversarial method to stress-test trained metrics to
evaluate conversational dialogue systems. The method leverages Reinforcement
Learning to find response strategies that elicit optimal scores from the
trained metrics. We apply our method to test recently proposed trained metrics.
We find that they all are susceptible to giving high scores to responses
generated by relatively simple and obviously flawed strategies that our method
converges on. For instance, simply copying parts of the conversation context to
form a response yields competitive scores or even outperforms responses written
by humans.",2202.13887v1,https://arxiv.org/pdf/2202.13887v1
"Towards Robust Stacked Capsule Autoencoder with Hybrid Adversarial
  Training","Jiazhu Dai, Siwei Xiong","Capsule networks (CapsNets) are new neural networks that classify images
based on the spatial relationships of features. By analyzing the pose of
features and their relative positions, it is more capable to recognize images
after affine transformation. The stacked capsule autoencoder (SCAE) is a
state-of-the-art CapsNet, and achieved unsupervised classification of CapsNets
for the first time. However, the security vulnerabilities and the robustness of
the SCAE has rarely been explored. In this paper, we propose an evasion attack
against SCAE, where the attacker can generate adversarial perturbations based
on reducing the contribution of the object capsules in SCAE related to the
original category of the image. The adversarial perturbations are then applied
to the original images, and the perturbed images will be misclassified.
Furthermore, we propose a defense method called Hybrid Adversarial Training
(HAT) against such evasion attacks. HAT makes use of adversarial training and
adversarial distillation to achieve better robustness and stability. We
evaluate the defense method and the experimental results show that the refined
SCAE model can achieve 82.14% classification accuracy under evasion attack. The
source code is available at https://github.com/FrostbiteXSW/SCAE_Defense.",2202.13755v2,https://arxiv.org/pdf/2202.13755v2
On the Robustness of CountSketch to Adaptive Inputs,"Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, Moshe Shechner, Uri Stemmer","CountSketch is a popular dimensionality reduction technique that maps vectors
to a lower dimension using randomized linear measurements. The sketch supports
recovering $\ell_2$-heavy hitters of a vector (entries with $v[i]^2 \geq
\frac{1}{k}\|\boldsymbol{v}\|^2_2$). We study the robustness of the sketch in
adaptive settings where input vectors may depend on the output from prior
inputs. Adaptive settings arise in processes with feedback or with adversarial
attacks. We show that the classic estimator is not robust, and can be attacked
with a number of queries of the order of the sketch size. We propose a robust
estimator (for a slightly modified sketch) that allows for quadratic number of
queries in the sketch size, which is an improvement factor of $\sqrt{k}$ (for
$k$ heavy hitters) over prior work.",2202.13736v1,https://arxiv.org/pdf/2202.13736v1
Evaluating the Adversarial Robustness of Adaptive Test-time Defenses,"Francesco Croce, Sven Gowal, Thomas Brunner, Evan Shelhamer, Matthias Hein, Taylan Cemgil","Adaptive defenses, which optimize at test time, promise to improve
adversarial robustness. We categorize such adaptive test-time defenses, explain
their potential benefits and drawbacks, and evaluate a representative variety
of the latest adaptive defenses for image classification. Unfortunately, none
significantly improve upon static defenses when subjected to our careful case
study evaluation. Some even weaken the underlying static model while
simultaneously increasing inference computation. While these results are
disappointing, we still believe that adaptive test-time defenses are a
promising avenue of research and, as such, we provide recommendations for their
thorough evaluation. We extend the checklist of Carlini et al. (2019) by
providing concrete steps specific to adaptive defenses.",2202.13711v2,https://arxiv.org/pdf/2202.13711v2
"Synergistic Network Learning and Label Correction for Noise-robust Image
  Classification","Chen Gong, Kong Bin, Eric J. Seibel, Xin Wang, Youbing Yin, Qi Song","Large training datasets almost always contain examples with inaccurate or
incorrect labels. Deep Neural Networks (DNNs) tend to overfit training label
noise, resulting in poorer model performance in practice. To address this
problem, we propose a robust label correction framework combining the ideas of
small loss selection and noise correction, which learns network parameters and
reassigns ground truth labels iteratively. Taking the expertise of DNNs to
learn meaningful patterns before fitting noise, our framework first trains two
networks over the current dataset with small loss selection. Based on the
classification loss and agreement loss of two networks, we can measure the
confidence of training data. More and more confident samples are selected for
label correction during the learning process. We demonstrate our method on both
synthetic and real-world datasets with different noise types and rates,
including CIFAR-10, CIFAR-100 and Clothing1M, where our method outperforms the
baseline approaches.",2202.13472v1,https://arxiv.org/pdf/2202.13472v1
"A Unified Wasserstein Distributional Robustness Framework for
  Adversarial Training","Tuan Anh Bui, Trung Le, Quan Tran, He Zhao, Dinh Phung","It is well-known that deep neural networks (DNNs) are susceptible to
adversarial attacks, exposing a severe fragility of deep learning systems. As
the result, adversarial training (AT) method, by incorporating adversarial
examples during training, represents a natural and effective approach to
strengthen the robustness of a DNN-based classifier. However, most AT-based
methods, notably PGD-AT and TRADES, typically seek a pointwise adversary that
generates the worst-case adversarial example by independently perturbing each
data sample, as a way to ""probe"" the vulnerability of the classifier. Arguably,
there are unexplored benefits in considering such adversarial effects from an
entire distribution. To this end, this paper presents a unified framework that
connects Wasserstein distributional robustness with current state-of-the-art AT
methods. We introduce a new Wasserstein cost function and a new series of risk
functions, with which we show that standard AT methods are special cases of
their counterparts in our framework. This connection leads to an intuitive
relaxation and generalization of existing AT methods and facilitates the
development of a new family of distributional robustness AT-based algorithms.
Extensive experiments show that our distributional robustness AT algorithms
robustify further their standard AT counterparts in various settings.",2202.13437v1,https://arxiv.org/pdf/2202.13437v1
"Robust Continual Learning through a Comprehensively Progressive Bayesian
  Neural Network","Guo Yang, Cheryl Sze Yin Wong, Ramasamy Savitha","This work proposes a comprehensively progressive Bayesian neural network for
robust continual learning of a sequence of tasks. A Bayesian neural network is
progressively pruned and grown such that there are sufficient network resources
to represent a sequence of tasks, while the network does not explode. It starts
with the contention that similar tasks should have the same number of total
network resources, to ensure fair representation of all tasks in a continual
learning scenario. Thus, as the data for new task streams in, sufficient
neurons are added to the network such that the total number of neurons in each
layer of the network, including the shared representations with previous tasks
and individual task related representation, are equal for all tasks. The
weights that are redundant at the end of training each task are also pruned
through re-initialization, in order to be efficiently utilized in the
subsequent task. Thus, the network grows progressively, but ensures effective
utilization of network resources. We refer to our proposed method as 'Robust
Continual Learning through a Comprehensively Progressive Bayesian Neural
Network (RCL-CPB)' and evaluate the proposed approach on the MNIST data set,
under three different continual learning scenarios. Further to this, we
evaluate the performance of RCL-CPB on a homogeneous sequence of tasks using
split CIFAR100 (20 tasks of 5 classes each), and a heterogeneous sequence of
tasks using MNIST, SVHN and CIFAR10 data sets. The demonstrations and the
performance results show that the proposed strategies for progressive BNN
enable robust continual learning.",2202.13369v1,https://arxiv.org/pdf/2202.13369v1
Towards Robust Off-policy Learning for Runtime Uncertainty,"Da Xu, Yuting Ye, Chuanwei Ruan, Bo Yang","Off-policy learning plays a pivotal role in optimizing and evaluating
policies prior to the online deployment. However, during the real-time serving,
we observe varieties of interventions and constraints that cause inconsistency
between the online and offline settings, which we summarize and term as runtime
uncertainty. Such uncertainty cannot be learned from the logged data due to its
abnormality and rareness nature. To assert a certain level of robustness, we
perturb the off-policy estimators along an adversarial direction in view of the
runtime uncertainty. It allows the resulting estimators to be robust not only
to observed but also unexpected runtime uncertainties. Leveraging this idea, we
bring runtime-uncertainty robustness to three major off-policy learning
methods: the inverse propensity score method, reward-model method, and doubly
robust method. We theoretically justify the robustness of our methods to
runtime uncertainty, and demonstrate their effectiveness using both the
simulation and the real-world online experiments.",2202.13337v1,https://arxiv.org/pdf/2202.13337v1
Bayesian Robust Tensor Ring Model for Incomplete Multiway Data,"Zhenhao Huang, Yuning Qiu, Xinqi Chen, Weijun Sun, Guoxu Zhou","Robust tensor completion (RTC) aims to recover a low-rank tensor from its
incomplete observation with outlier corruption. The recently proposed tensor
ring (TR) model has demonstrated superiority in solving the RTC problem.
However, the existing methods either require a pre-assigned TR rank or
aggressively pursue the minimum TR rank, thereby often leading to biased
solutions in the presence of noise. In this paper, a Bayesian robust tensor
ring decomposition (BRTR) method is proposed to give more accurate solutions to
the RTC problem, which can avoid exquisite selection of the TR rank and penalty
parameters. A variational Bayesian (VB) algorithm is developed to infer the
probability distribution of posteriors. During the learning process, BRTR can
prune off slices of core tensor with marginal components, resulting in
automatic TR rank detection. Extensive experiments show that BRTR can achieve
significantly improved performance than other state-of-the-art methods.",2202.13321v2,https://arxiv.org/pdf/2202.13321v2
"Towards Scalable and Robust Structured Bandits: A Meta-Learning
  Framework","Runzhe Wan, Lin Ge, Rui Song","Online learning in large-scale structured bandits is known to be challenging
due to the curse of dimensionality. In this paper, we propose a unified
meta-learning framework for a general class of structured bandit problems where
the parameter space can be factorized to item-level. The novel bandit algorithm
is general to be applied to many popular problems,scalable to the huge
parameter and action spaces, and robust to the specification of the
generalization model. At the core of this framework is a Bayesian hierarchical
model that allows information sharing among items via their features, upon
which we design a meta Thompson sampling algorithm. Three representative
examples are discussed thoroughly. Both theoretical analysis and numerical
results support the usefulness of the proposed method.",2202.13227v1,https://arxiv.org/pdf/2202.13227v1
Adversarial robustness of sparse local Lipschitz predictors,"Ramchandran Muthukumar, Jeremias Sulam","This work studies the adversarial robustness of parametric functions composed
of a linear predictor and a non-linear representation map. % that satisfies
certain stability condition. Our analysis relies on \emph{sparse local
Lipschitzness} (SLL), an extension of local Lipschitz continuity that better
captures the stability and reduced effective dimensionality of predictors upon
local perturbations. SLL functions preserve a certain degree of structure,
given by the sparsity pattern in the representation map, and include several
popular hypothesis classes, such as piece-wise linear models, Lasso and its
variants, and deep feed-forward \relu networks. % are sparse local Lipschitz.
We provide a tighter robustness certificate on the minimal energy of an
adversarial example, as well as tighter data-dependent non-uniform bounds on
the robust generalization error of these predictors. We instantiate these
results for the case of deep neural networks and provide numerical evidence
that supports our results, shedding new insights into natural regularization
strategies to increase the robustness of these models.",2202.13216v2,https://arxiv.org/pdf/2202.13216v2
ASSIST: Towards Label Noise-Robust Dialogue State Tracking,"Fanghua Ye, Yue Feng, Emine Yilmaz","The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state
tracking (DST). However, substantial noise has been discovered in its state
annotations. Such noise brings about huge challenges for training DST models
robustly. Although several refined versions, including MultiWOZ 2.1-2.4, have
been published recently, there are still lots of noisy labels, especially in
the training set. Besides, it is costly to rectify all the problematic
annotations. In this paper, instead of improving the annotation quality
further, we propose a general framework, named ASSIST (lAbel noiSe-robuSt
dIalogue State Tracking), to train DST models robustly from noisy labels.
ASSIST first generates pseudo labels for each sample in the training set by
using an auxiliary model trained on a small clean dataset, then puts the
generated pseudo labels and vanilla noisy labels together to train the primary
model. We show the validity of ASSIST theoretically. Experimental results also
demonstrate that ASSIST improves the joint goal accuracy of DST by up to
$28.16\%$ on MultiWOZ 2.0 and $8.41\%$ on MultiWOZ 2.4, compared to using only
the vanilla noisy labels.",2202.13024v2,https://arxiv.org/pdf/2202.13024v2
ARIA: Adversarially Robust Image Attribution for Content Provenance,"Maksym Andriushchenko, Xiaoyang Rebecca Li, Geoffrey Oxholm, Thomas Gittings, Tu Bui, Nicolas Flammarion, John Collomosse","Image attribution -- matching an image back to a trusted source -- is an
emerging tool in the fight against online misinformation. Deep visual
fingerprinting models have recently been explored for this purpose. However,
they are not robust to tiny input perturbations known as adversarial examples.
First we illustrate how to generate valid adversarial images that can easily
cause incorrect image attribution. Then we describe an approach to prevent
imperceptible adversarial attacks on deep visual fingerprinting models, via
robust contrastive learning. The proposed training procedure leverages training
on $\ell_\infty$-bounded adversarial examples, it is conceptually simple and
incurs only a small computational overhead. The resulting models are
substantially more robust, are accurate even on unperturbed images, and perform
well even over a database with millions of images. In particular, we achieve
91.6% standard and 85.1% adversarial recall under $\ell_\infty$-bounded
perturbations on manipulated images compared to 80.1% and 0.0% from prior work.
We also show that robustness generalizes to other types of imperceptible
perturbations unseen during training. Finally, we show how to train an
adversarially robust image comparator model for detecting editorial changes in
matched images.",2202.12860v1,https://arxiv.org/pdf/2202.12860v1
"A Robust Multi-Objective Bayesian Optimization Framework Considering
  Input Uncertainty","J. Qing, I. Couckuyt, T. Dhaene","Bayesian optimization is a popular tool for data-efficient optimization of
expensive objective functions. In real-life applications like engineering
design, the designer often wants to take multiple objectives as well as input
uncertainty into account to find a set of robust solutions. While this is an
active topic in single-objective Bayesian optimization, it is less investigated
in the multi-objective case. We introduce a novel Bayesian optimization
framework to efficiently perform multi-objective optimization considering input
uncertainty. We propose a robust Gaussian Process model to infer the Bayes risk
criterion to quantify robustness, and we develop a two-stage Bayesian
optimization process to search for a robust Pareto frontier. The complete
framework supports various distributions of the input uncertainty and takes
full advantage of parallel computing. We demonstrate the effectiveness of the
framework through numerical benchmarks.",2202.12848v1,https://arxiv.org/pdf/2202.12848v1
"Understanding Adversarial Robustness from Feature Maps of Convolutional
  Layers","Cong Xu, Wei Zhang, Jun Wang, Min Yang","The adversarial robustness of a neural network mainly relies on two factors:
model capacity and anti-perturbation ability. In this paper, we study the
anti-perturbation ability of the network from the feature maps of convolutional
layers. Our theoretical analysis discovers that larger convolutional feature
maps before average pooling can contribute to better resistance to
perturbations, but the conclusion is not true for max pooling. It brings new
inspiration to the design of robust neural networks and urges us to apply these
findings to improve existing architectures. The proposed modifications are very
simple and only require upsampling the inputs or slightly modifying the stride
configurations of downsampling operators. We verify our approaches on several
benchmark neural network architectures, including AlexNet, VGG, RestNet18, and
PreActResNet18. Non-trivial improvements in terms of both natural accuracy and
adversarial robustness can be achieved under various attack and defense
mechanisms. The code is available at \url{https://github.com/MTandHJ/rcm}.",2202.12435v2,https://arxiv.org/pdf/2202.12435v2
"Fourier-Based Augmentations for Improved Robustness and Uncertainty
  Calibration","Ryan Soklaski, Michael Yee, Theodoros Tsiligkaridis","Diverse data augmentation strategies are a natural approach to improving
robustness in computer vision models against unforeseen shifts in data
distribution. However, the ability to tailor such strategies to inoculate a
model against specific classes of corruptions or attacks -- without incurring
substantial losses in robustness against other classes of corruptions --
remains elusive. In this work, we successfully harden a model against
Fourier-based attacks, while producing superior-to-AugMix accuracy and
calibration results on both the CIFAR-10-C and CIFAR-100-C datasets;
classification error is reduced by over ten percentage points for some
high-severity noise and digital-type corruptions. We achieve this by
incorporating Fourier-basis perturbations in the AugMix image-augmentation
framework. Thus we demonstrate that the AugMix framework can be tailored to
effectively target particular distribution shifts, while boosting overall model
robustness.",2202.12412v1,https://arxiv.org/pdf/2202.12412v1
Towards Effective and Robust Neural Trojan Defenses via Input Filtering,"Kien Do, Haripriya Harikumar, Hung Le, Dung Nguyen, Truyen Tran, Santu Rana, Dang Nguyen, Willy Susilo, Svetha Venkatesh","Trojan attacks on deep neural networks are both dangerous and surreptitious.
Over the past few years, Trojan attacks have advanced from using only a single
input-agnostic trigger and targeting only one class to using multiple,
input-specific triggers and targeting multiple classes. However, Trojan
defenses have not caught up with this development. Most defense methods still
make inadequate assumptions about Trojan triggers and target classes, thus, can
be easily circumvented by modern Trojan attacks. To deal with this problem, we
propose two novel ""filtering"" defenses called Variational Input Filtering (VIF)
and Adversarial Input Filtering (AIF) which leverage lossy data compression and
adversarial learning respectively to effectively purify potential Trojan
triggers in the input at run time without making assumptions about the number
of triggers/target classes or the input dependence property of triggers. In
addition, we introduce a new defense mechanism called
""Filtering-then-Contrasting"" (FtC) which helps avoid the drop in classification
accuracy on clean data caused by ""filtering"", and combine it with VIF/AIF to
derive new defenses of this kind. Extensive experimental results and ablation
studies show that our proposed defenses significantly outperform well-known
baseline defenses in mitigating five advanced Trojan attacks including two
recent state-of-the-art while being quite robust to small amounts of training
data and large-norm triggers.",2202.12154v5,https://arxiv.org/pdf/2202.12154v5
Robust Probabilistic Time Series Forecasting,"TaeHo Yoon, Youngsuk Park, Ernest K. Ryu, Yuyang Wang","Probabilistic time series forecasting has played critical role in
decision-making processes due to its capability to quantify uncertainties. Deep
forecasting models, however, could be prone to input perturbations, and the
notion of such perturbations, together with that of robustness, has not even
been completely established in the regime of probabilistic forecasting. In this
work, we propose a framework for robust probabilistic time series forecasting.
First, we generalize the concept of adversarial input perturbations, based on
which we formulate the concept of robustness in terms of bounded Wasserstein
deviation. Then we extend the randomized smoothing technique to attain robust
probabilistic forecasters with theoretical robustness certificates against
certain classes of adversarial perturbations. Lastly, extensive experiments
demonstrate that our methods are empirically effective in enhancing the
forecast quality under additive adversarial attacks and forecast consistency
under supplement of noisy observations.",2202.11910v1,https://arxiv.org/pdf/2202.11910v1
"Improving Robustness of Convolutional Neural Networks Using Element-Wise
  Activation Scaling","Zhi-Yuan Zhang, Di Liu","Recent works reveal that re-calibrating the intermediate activation of
adversarial examples can improve the adversarial robustness of a CNN model. The
state of the arts [Baiet al., 2021] and [Yanet al., 2021] explores this feature
at the channel level, i.e. the activation of a channel is uniformly scaled by a
factor. In this paper, we investigate the intermediate activation manipulation
at a more fine-grained level. Instead of uniformly scaling the activation, we
individually adjust each element within an activation and thus propose
Element-Wise Activation Scaling, dubbed EWAS, to improve CNNs' adversarial
robustness. Experimental results on ResNet-18 and WideResNet with CIFAR10 and
SVHN show that EWAS significantly improves the robustness accuracy. Especially
for ResNet18 on CIFAR10, EWAS increases the adversarial accuracy by 37.65% to
82.35% against C&W attack. EWAS is simple yet very effective in terms of
improving robustness. The codes are anonymously available at
https://anonymous.4open.science/r/EWAS-DD64.",2202.11898v1,https://arxiv.org/pdf/2202.11898v1
"Robust Federated Learning with Connectivity Failures: A
  Semi-Decentralized Framework with Collaborative Relaying","Michal Yemini, Rajarshi Saha, Emre Ozfatura, Deniz Gündüz, Andrea J. Goldsmith","Intermittent connectivity of clients to the parameter server (PS) is a major
bottleneck in federated edge learning frameworks. The lack of constant
connectivity induces a large generalization gap, especially when the local data
distribution amongst clients exhibits heterogeneity. To overcome intermittent
communication outages between clients and the central PS, we introduce the
concept of collaborative relaying wherein the participating clients relay their
neighbors' local updates to the PS in order to boost the participation of
clients with poor connectivity to the PS. We propose a semi-decentralized
federated learning framework in which at every communication round, each client
initially computes a local consensus of a subset of its neighboring clients'
updates, and eventually transmits to the PS a weighted average of its own
update and those of its neighbors'. We appropriately optimize these local
consensus weights to ensure that the global update at the PS is unbiased with
minimal variance - consequently improving the convergence rate. Numerical
evaluations on the CIFAR-10 dataset demonstrate that our collaborative relaying
approach outperforms federated averaging-based benchmarks for learning over
intermittently-connected networks such as when the clients communicate over
millimeter wave channels with intermittent blockages.",2202.11850v2,https://arxiv.org/pdf/2202.11850v2
"DC and SA: Robust and Efficient Hyperparameter Optimization of
  Multi-subnetwork Deep Learning Models","Alex H. Treacher, Albert Montillo","We present two novel hyperparameter optimization strategies for optimization
of deep learning models with a modular architecture constructed of multiple
subnetworks. As complex networks with multiple subnetworks become more
frequently applied in machine learning, hyperparameter optimization methods are
required to efficiently optimize their hyperparameters. Existing hyperparameter
searches are general, and can be used to optimize such networks, however, by
exploiting the multi-subnetwork architecture, these searches can be sped up
substantially. The proposed methods offer faster convergence to a
better-performing final model. To demonstrate this, we propose 2 independent
approaches to enhance these prior algorithms: 1) a divide-and-conquer approach,
in which the best subnetworks of top-performing models are combined, allowing
for more rapid sampling of the hyperparameter search space. 2) A subnetwork
adaptive approach that distributes computational resources based on the
importance of each subnetwork, allowing more intelligent resource allocation.
These approaches can be flexibily applied to many hyperparameter optimization
algorithms. To illustrate this, we combine our approaches with the
commonly-used Bayesian optimization method. Our approaches are then tested
against both synthetic examples and real-world examples and applied to multiple
network types including convolutional neural networks and dense feed forward
neural networks. Our approaches show an increased optimization efficiency of up
to 23.62x, and a final performance boost of up to 3.5% accuracy for
classification and 4.4 MSE for regression, when compared to comparable BO
approach.",2202.11841v1,https://arxiv.org/pdf/2202.11841v1
A Law of Robustness beyond Isoperimetry,"Yihan Wu, Heng Huang, Hongyang Zhang","We study the robust interpolation problem of arbitrary data distributions
supported on a bounded space and propose a two-fold law of robustness. Robust
interpolation refers to the problem of interpolating $n$ noisy training data
points in $\mathbb{R}^d$ by a Lipschitz function. Although this problem has
been well understood when the samples are drawn from an isoperimetry
distribution, much remains unknown concerning its performance under generic or
even the worst-case distributions. We prove a Lipschitzness lower bound
$\Omega(\sqrt{n/p})$ of the interpolating neural network with $p$ parameters on
arbitrary data distributions. With this result, we validate the law of
robustness conjecture in prior work by Bubeck, Li, and Nagaraj on two-layer
neural networks with polynomial weights. We then extend our result to arbitrary
interpolating approximators and prove a Lipschitzness lower bound
$\Omega(n^{1/d})$ for robust interpolation. Our results demonstrate a two-fold
law of robustness: i) we show the potential benefit of overparametrization for
smooth data interpolation when $n=\mathrm{poly}(d)$, and ii) we disprove the
potential existence of an $O(1)$-Lipschitz robust interpolating function when
$n=\exp(\omega(d))$.",2202.11592v2,https://arxiv.org/pdf/2202.11592v2
Robust Geometric Metric Learning,"Antoine Collas, Arnaud Breloy, Guillaume Ginolhac, Chengfang Ren, Jean-Philippe Ovarlez","This paper proposes new algorithms for the metric learning problem. We start
by noticing that several classical metric learning formulations from the
literature can be viewed as modified covariance matrix estimation problems.
Leveraging this point of view, a general approach, called Robust Geometric
Metric Learning (RGML), is then studied. This method aims at simultaneously
estimating the covariance matrix of each class while shrinking them towards
their (unknown) barycenter. We focus on two specific costs functions: one
associated with the Gaussian likelihood (RGML Gaussian), and one with Tyler's M
-estimator (RGML Tyler). In both, the barycenter is defined with the Riemannian
distance, which enjoys nice properties of geodesic convexity and affine
invariance. The optimization is performed using the Riemannian geometry of
symmetric positive definite matrices and its submanifold of unit determinant.
Finally, the performance of RGML is asserted on real datasets. Strong
performance is exhibited while being robust to mislabeled data.",2202.11550v3,https://arxiv.org/pdf/2202.11550v3
"Robust Hierarchical Patterns for identifying MDD patients: A Multisite
  Study","Dushyant Sahoo, Mathilde Antoniades, Cynthia H. Y. Fu, Christos Davatzikos","Many supervised machine learning frameworks have been proposed for disease
classification using functional magnetic resonance imaging (fMRI) data,
producing important biomarkers. More recently, data pooling has flourished,
making the result generalizable across a large population. But, this success
depends on the population diversity and variability introduced due to the
pooling of the data that is not a primary research interest. Here, we look at
hierarchical Sparse Connectivity Patterns (hSCPs) as biomarkers for major
depressive disorder (MDD). We propose a novel model based on hSCPs to predict
MDD patients from functional connectivity matrices extracted from resting-state
fMRI data. Our model consists of three coupled terms. The first term decomposes
connectivity matrices into hierarchical low-rank sparse components
corresponding to synchronous patterns across the human brain. These components
are then combined via patient-specific weights capturing heterogeneity in the
data. The second term is a classification loss that uses the patient-specific
weights to classify MDD patients from healthy ones. Both of these terms are
combined with the third term, a robustness loss function to improve the
reproducibility of hSCPs. This reduces the variability introduced due to site
and population diversity (age and sex) on the predictive accuracy and pattern
stability in a large dataset pooled from five different sites. Our results show
the impact of diversity on prediction performance. Our model can reduce
diversity and improve the predictive and generalizing capability of the
components. Finally, our results show that our proposed model can robustly
identify clinically relevant patterns characteristic of MDD with high
reproducibility.",2202.11144v1,https://arxiv.org/pdf/2202.11144v1
"Nonconvex Extension of Generalized Huber Loss for Robust Learning and
  Pseudo-Mode Statistics","Kaan Gokcesu, Hakan Gokcesu","We propose an extended generalization of the pseudo Huber loss formulation.
We show that using the log-exp transform together with the logistic function,
we can create a loss which combines the desirable properties of the strictly
convex losses with robust loss functions. With this formulation, we show that a
linear convergence algorithm can be utilized to find a minimizer. We further
discuss the creation of a quasi-convex composite loss and provide a
derivative-free exponential convergence rate algorithm.",2202.11141v1,https://arxiv.org/pdf/2202.11141v1
Robust and Provable Guarantees for Sparse Random Embeddings,"Maciej Skorski, Alessandro Temperoni, Martin Theobald","In this work, we improve upon the guarantees for sparse random embeddings, as
they were recently provided and analyzed by Freksen at al. (NIPS'18) and
Jagadeesan (NIPS'19). Specifically, we show that (a) our bounds are explicit as
opposed to the asymptotic guarantees provided previously, and (b) our bounds
are guaranteed to be sharper by practically significant constants across a wide
range of parameters, including the dimensionality, sparsity and dispersion of
the data. Moreover, we empirically demonstrate that our bounds significantly
outperform prior works on a wide range of real-world datasets, such as
collections of images, text documents represented as bags-of-words, and text
sequences vectorized by neural embeddings. Behind our numerical improvements
are techniques of broader interest, which improve upon key steps of previous
analyses in terms of (c) tighter estimates for certain types of quadratic
chaos, (d) establishing extreme properties of sparse linear forms, and (e)
improvements on bounds for the estimation of sums of independent random
variables.",2202.10815v1,https://arxiv.org/pdf/2202.10815v1
"Partial Identification with Noisy Covariates: A Robust Optimization
  Approach","Wenshuo Guo, Mingzhang Yin, Yixin Wang, Michael I. Jordan","Causal inference from observational datasets often relies on measuring and
adjusting for covariates. In practice, measurements of the covariates can often
be noisy and/or biased, or only measurements of their proxies may be available.
Directly adjusting for these imperfect measurements of the covariates can lead
to biased causal estimates. Moreover, without additional assumptions, the
causal effects are not point-identifiable due to the noise in these
measurements. To this end, we study the partial identification of causal
effects given noisy covariates, under a user-specified assumption on the noise
level. The key observation is that we can formulate the identification of the
average treatment effects (ATE) as a robust optimization problem. This
formulation leads to an efficient robust optimization algorithm that bounds the
ATE with noisy covariates. We show that this robust optimization approach can
extend a wide range of causal adjustment methods to perform partial
identification, including backdoor adjustment, inverse propensity score
weighting, double machine learning, and front door adjustment. Across synthetic
and real datasets, we find that this approach provides ATE bounds with a higher
coverage probability than existing methods.",2202.10665v1,https://arxiv.org/pdf/2202.10665v1
"Differential Secrecy for Distributed Data and Applications to Robust
  Differentially Secure Vector Summation",Kunal Talwar,"Computing the noisy sum of real-valued vectors is an important primitive in
differentially private learning and statistics. In private federated learning
applications, these vectors are held by client devices, leading to a
distributed summation problem. Standard Secure Multiparty Computation (SMC)
protocols for this problem are susceptible to poisoning attacks, where a client
may have a large influence on the sum, without being detected.
  In this work, we propose a poisoning-robust private summation protocol in the
multiple-server setting, recently studied in PRIO. We present a protocol for
vector summation that verifies that the Euclidean norm of each contribution is
approximately bounded. We show that by relaxing the security constraint in SMC
to a differential privacy like guarantee, one can improve over PRIO in terms of
communication requirements as well as the client-side computation. Unlike SMC
algorithms that inevitably cast integers to elements of a large finite field,
our algorithms work over integers/reals, which may allow for additional
efficiencies.",2202.10618v1,https://arxiv.org/pdf/2202.10618v1
"Semi-Implicit Hybrid Gradient Methods with Application to Adversarial
  Robustness","Beomsu Kim, Junghoon Seo","Adversarial examples, crafted by adding imperceptible perturbations to
natural inputs, can easily fool deep neural networks (DNNs). One of the most
successful methods for training adversarially robust DNNs is solving a
nonconvex-nonconcave minimax problem with an adversarial training (AT)
algorithm. However, among the many AT algorithms, only Dynamic AT (DAT) and You
Only Propagate Once (YOPO) guarantee convergence to a stationary point. In this
work, we generalize the stochastic primal-dual hybrid gradient algorithm to
develop semi-implicit hybrid gradient methods (SI-HGs) for finding stationary
points of nonconvex-nonconcave minimax problems. SI-HGs have the convergence
rate $O(1/K)$, which improves upon the rate $O(1/K^{1/2})$ of DAT and YOPO. We
devise a practical variant of SI-HGs, and show that it outperforms other AT
algorithms in terms of convergence speed and robustness.",2202.10523v1,https://arxiv.org/pdf/2202.10523v1
"Ligandformer: A Graph Neural Network for Predicting Compound Property
  with Robust Interpretation","Jinjiang Guo, Qi Liu, Han Guo, Xi Lu","Robust and efficient interpretation of QSAR methods is quite useful to
validate AI prediction rationales with subjective opinion (chemist or biologist
expertise), understand sophisticated chemical or biological process mechanisms,
and provide heuristic ideas for structure optimization in pharmaceutical
industry. For this purpose, we construct a multi-layer self-attention based
Graph Neural Network framework, namely Ligandformer, for predicting compound
property with interpretation. Ligandformer integrates attention maps on
compound structure from different network blocks. The integrated attention map
reflects the machine's local interest on compound structure, and indicates the
relationship between predicted compound property and its structure. This work
mainly contributes to three aspects: 1. Ligandformer directly opens the
black-box of deep learning methods, providing local prediction rationales on
chemical structures. 2. Ligandformer gives robust prediction in different
experimental rounds, overcoming the ubiquitous prediction instability of deep
learning methods. 3. Ligandformer can be generalized to predict different
chemical or biological properties with high performance. Furthermore,
Ligandformer can simultaneously output specific property score and visible
attention map on structure, which can support researchers to investigate
chemical or biological property and optimize structure efficiently. Our
framework outperforms over counterparts in terms of accuracy, robustness and
generalization, and can be applied in complex system study.",2202.10873v3,https://arxiv.org/pdf/2202.10873v3
"r-G2P: Evaluating and Enhancing Robustness of Grapheme to Phoneme
  Conversion by Controlled noise introducing and Contextual information
  incorporation","Chendong Zhao, Jianzong Wang, Xiaoyang Qu, Haoqian Wang, Jing Xiao","Grapheme-to-phoneme (G2P) conversion is the process of converting the written
form of words to their pronunciations. It has an important role for
text-to-speech (TTS) synthesis and automatic speech recognition (ASR) systems.
In this paper, we aim to evaluate and enhance the robustness of G2P models. We
show that neural G2P models are extremely sensitive to orthographical
variations in graphemes like spelling mistakes. To solve this problem, we
propose three controlled noise introducing methods to synthesize noisy training
data. Moreover, we incorporate the contextual information with the baseline and
propose a robust training strategy to stabilize the training process. The
experimental results demonstrate that our proposed robust G2P model (r-G2P)
outperforms the baseline significantly (-2.73\% WER on Dict-based benchmarks
and -9.09\% WER on Real-world sources).",2202.11194v1,https://arxiv.org/pdf/2202.11194v1
Robustness and Accuracy Could Be Reconcilable by (Proper) Definition,"Tianyu Pang, Min Lin, Xiao Yang, Jun Zhu, Shuicheng Yan","The trade-off between robustness and accuracy has been widely studied in the
adversarial literature. Although still controversial, the prevailing view is
that this trade-off is inherent, either empirically or theoretically. Thus, we
dig for the origin of this trade-off in adversarial training and find that it
may stem from the improperly defined robust error, which imposes an inductive
bias of local invariance -- an overcorrection towards smoothness. Given this,
we advocate employing local equivariance to describe the ideal behavior of a
robust model, leading to a self-consistent robust error named SCORE. By
definition, SCORE facilitates the reconciliation between robustness and
accuracy, while still handling the worst-case uncertainty via robust
optimization. By simply substituting KL divergence with variants of distance
metrics, SCORE can be efficiently minimized. Empirically, our models achieve
top-rank performance on RobustBench under AutoAttack. Besides, SCORE provides
instructive insights for explaining the overfitting phenomenon and semantic
input gradients observed on robust models. Code is available at
https://github.com/P2333/SCORE.",2202.10103v2,https://arxiv.org/pdf/2202.10103v2
"Transferring Adversarial Robustness Through Robust Representation
  Matching","Pratik Vaishnavi, Kevin Eykholt, Amir Rahmati","With the widespread use of machine learning, concerns over its security and
reliability have become prevalent. As such, many have developed defenses to
harden neural networks against adversarial examples, imperceptibly perturbed
inputs that are reliably misclassified. Adversarial training in which
adversarial examples are generated and used during training is one of the few
known defenses able to reliably withstand such attacks against neural networks.
However, adversarial training imposes a significant training overhead and
scales poorly with model complexity and input dimension. In this paper, we
propose Robust Representation Matching (RRM), a low-cost method to transfer the
robustness of an adversarially trained model to a new model being trained for
the same task irrespective of architectural differences. Inspired by
student-teacher learning, our method introduces a novel training loss that
encourages the student to learn the teacher's robust representations. Compared
to prior works, RRM is superior with respect to both model performance and
adversarial training time. On CIFAR-10, RRM trains a robust model $\sim
1.8\times$ faster than the state-of-the-art. Furthermore, RRM remains effective
on higher-dimensional datasets. On Restricted-ImageNet, RRM trains a ResNet50
model $\sim 18\times$ faster than standard adversarial training.",2202.09994v2,https://arxiv.org/pdf/2202.09994v2
Mining Robust Default Configurations for Resource-constrained AutoML,"Moe Kayali, Chi Wang","Automatic machine learning (AutoML) is a key enabler of the mass deployment
of the next generation of machine learning systems. A key desideratum for
future ML systems is the automatic selection of models and hyperparameters. We
present a novel method of selecting performant configurations for a given task
by performing offline autoML and mining over a diverse set of tasks. By mining
the training tasks, we can select a compact portfolio of configurations that
perform well over a wide variety of tasks, as well as learn a strategy to
select portfolio configurations for yet-unseen tasks. The algorithm runs in a
zero-shot manner, that is without training any models online except the chosen
one. In a compute- or time-constrained setting, this virtually instant
selection is highly performant. Further, we show that our approach is effective
for warm-starting existing autoML platforms. In both settings, we demonstrate
an improvement on the state-of-the-art by testing over 62 classification and
regression datasets. We also demonstrate the utility of recommending
data-dependent default configurations that outperform widely used hand-crafted
defaults.",2202.09927v1,https://arxiv.org/pdf/2202.09927v1
"Sparsity Winning Twice: Better Robust Generalization from More Efficient
  Training","Tianlong Chen, Zhenyu Zhang, Pengjun Wang, Santosh Balachandra, Haoyu Ma, Zehao Wang, Zhangyang Wang","Recent studies demonstrate that deep networks, even robustified by the
state-of-the-art adversarial training (AT), still suffer from large robust
generalization gaps, in addition to the much more expensive training costs than
standard training. In this paper, we investigate this intriguing problem from a
new perspective, i.e., injecting appropriate forms of sparsity during
adversarial training. We introduce two alternatives for sparse adversarial
training: (i) static sparsity, by leveraging recent results from the lottery
ticket hypothesis to identify critical sparse subnetworks arising from the
early training; (ii) dynamic sparsity, by allowing the sparse subnetwork to
adaptively adjust its connectivity pattern (while sticking to the same sparsity
ratio) throughout training. We find both static and dynamic sparse methods to
yield win-win: substantially shrinking the robust generalization gap and
alleviating the robust overfitting, meanwhile significantly saving training and
inference FLOPs. Extensive experiments validate our proposals with multiple
network architectures on diverse datasets, including CIFAR-10/100 and
Tiny-ImageNet. For example, our methods reduce robust generalization gap and
overfitting by 34.44% and 4.02%, with comparable robust/standard accuracy
boosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR-100 with
ResNet-18. Besides, our approaches can be organically combined with existing
regularizers, establishing new state-of-the-art results in AT. Codes are
available in https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization.",2202.09844v3,https://arxiv.org/pdf/2202.09844v3
"Overparametrization improves robustness against adversarial attacks: A
  replication study",Ali Borji,"Overparametrization has become a de facto standard in machine learning.
Despite numerous efforts, our understanding of how and where
overparametrization helps model accuracy and robustness is still limited. To
this end, here we conduct an empirical investigation to systemically study and
replicate previous findings in this area, in particular the study by Madry et
al. Together with this study, our findings support the ""universal law of
robustness"" recently proposed by Bubeck et al. We argue that while critical for
robust perception, overparametrization may not be enough to achieve full
robustness and smarter architectures e.g. the ones implemented by the human
visual cortex) seem inevitable.",2202.09735v1,https://arxiv.org/pdf/2202.09735v1
Understanding Robust Generalization in Learning Regular Languages,"Soham Dan, Osbert Bastani, Dan Roth","A key feature of human intelligence is the ability to generalize beyond the
training distribution, for instance, parsing longer sentences than seen in the
past. Currently, deep neural networks struggle to generalize robustly to such
shifts in the data distribution. We study robust generalization in the context
of using recurrent neural networks (RNNs) to learn regular languages. We
hypothesize that standard end-to-end modeling strategies cannot generalize well
to systematic distribution shifts and propose a compositional strategy to
address this. We compare an end-to-end strategy that maps strings to labels
with a compositional strategy that predicts the structure of the deterministic
finite-state automaton (DFA) that accepts the regular language. We
theoretically prove that the compositional strategy generalizes significantly
better than the end-to-end strategy. In our experiments, we implement the
compositional strategy via an auxiliary task where the goal is to predict the
intermediate states visited by the DFA when parsing a string. Our empirical
results support our hypothesis, showing that auxiliary tasks can enable robust
generalization. Interestingly, the end-to-end RNN generalizes significantly
better than the theoretical lower bound, suggesting that it is able to achieve
at least some degree of robust generalization.",2202.09717v1,https://arxiv.org/pdf/2202.09717v1
Doubly Robust Distributionally Robust Off-Policy Evaluation and Learning,"Nathan Kallus, Xiaojie Mao, Kaiwen Wang, Zhengyuan Zhou","Off-policy evaluation and learning (OPE/L) use offline observational data to
make better decisions, which is crucial in applications where online
experimentation is limited. However, depending entirely on logged data, OPE/L
is sensitive to environment distribution shifts -- discrepancies between the
data-generating environment and that where policies are deployed.
\citet{si2020distributional} proposed distributionally robust OPE/L (DROPE/L)
to address this, but the proposal relies on inverse-propensity weighting, whose
estimation error and regret will deteriorate if propensities are
nonparametrically estimated and whose variance is suboptimal even if not. For
standard, non-robust, OPE/L, this is solved by doubly robust (DR) methods, but
they do not naturally extend to the more complex DROPE/L, which involves a
worst-case expectation. In this paper, we propose the first DR algorithms for
DROPE/L with KL-divergence uncertainty sets. For evaluation, we propose
Localized Doubly Robust DROPE (LDR$^2$OPE) and show that it achieves
semiparametric efficiency under weak product rates conditions. Thanks to a
localization technique, LDR$^2$OPE only requires fitting a small number of
regressions, just like DR methods for standard OPE. For learning, we propose
Continuum Doubly Robust DROPL (CDR$^2$OPL) and show that, under a product rate
condition involving a continuum of regressions, it enjoys a fast regret rate of
$\mathcal{O}\left(N^{-1/2}\right)$ even when unknown propensities are
nonparametrically estimated. We empirically validate our algorithms in
simulations and further extend our results to general $f$-divergence
uncertainty sets.",2202.09667v2,https://arxiv.org/pdf/2202.09667v2
"Robust Reinforcement Learning as a Stackelberg Game via
  Adaptively-Regularized Adversarial Training","Peide Huang, Mengdi Xu, Fei Fang, Ding Zhao","Robust Reinforcement Learning (RL) focuses on improving performances under
model errors or adversarial attacks, which facilitates the real-life deployment
of RL agents. Robust Adversarial Reinforcement Learning (RARL) is one of the
most popular frameworks for robust RL. However, most of the existing literature
models RARL as a zero-sum simultaneous game with Nash equilibrium as the
solution concept, which could overlook the sequential nature of RL deployments,
produce overly conservative agents, and induce training instability. In this
paper, we introduce a novel hierarchical formulation of robust RL - a
general-sum Stackelberg game model called RRL-Stack - to formalize the
sequential nature and provide extra flexibility for robust training. We develop
the Stackelberg Policy Gradient algorithm to solve RRL-Stack, leveraging the
Stackelberg learning dynamics by considering the adversary's response. Our
method generates challenging yet solvable adversarial environments which
benefit RL agents' robust learning. Our algorithm demonstrates better training
stability and robustness against different testing conditions in the
single-agent robotics control and multi-agent highway merging tasks.",2202.09514v2,https://arxiv.org/pdf/2202.09514v2
"Attacks, Defenses, And Tools: A Framework To Facilitate Robust AI/ML
  Systems","Mohamad Fazelnia, Igor Khokhlov, Mehdi Mirakhorli","Software systems are increasingly relying on Artificial Intelligence (AI) and
Machine Learning (ML) components. The emerging popularity of AI techniques in
various application domains attracts malicious actors and adversaries.
Therefore, the developers of AI-enabled software systems need to take into
account various novel cyber-attacks and vulnerabilities that these systems may
be susceptible to. This paper presents a framework to characterize attacks and
weaknesses associated with AI-enabled systems and provide mitigation techniques
and defense strategies. This framework aims to support software designers in
taking proactive measures in developing AI-enabled software, understanding the
attack surface of such systems, and developing products that are resilient to
various emerging attacks associated with ML. The developed framework covers a
broad spectrum of attacks, mitigation techniques, and defensive and offensive
tools. In this paper, we demonstrate the framework architecture and its major
components, describe their attributes, and discuss the long-term goals of this
research.",2202.09465v1,https://arxiv.org/pdf/2202.09465v1
Learning Representations Robust to Group Shifts and Adversarial Examples,"Ming-Chang Chiu, Xuezhe Ma","Despite the high performance achieved by deep neural networks on various
tasks, extensive studies have demonstrated that small tweaks in the input could
fail the model predictions. This issue of deep neural networks has led to a
number of methods to improve model robustness, including adversarial training
and distributionally robust optimization. Though both of these two methods are
geared towards learning robust models, they have essentially different
motivations: adversarial training attempts to train deep neural networks
against perturbations, while distributional robust optimization aims at
improving model performance on the most difficult ""uncertain distributions"". In
this work, we propose an algorithm that combines adversarial training and group
distribution robust optimization to improve robust representation learning.
Experiments on three image benchmark datasets illustrate that the proposed
method achieves superior results on robust metrics without sacrificing much of
the standard measures.",2202.09446v1,https://arxiv.org/pdf/2202.09446v1
"Exploring Adversarially Robust Training for Unsupervised Domain
  Adaptation","Shao-Yuan Lo, Vishal M. Patel","Unsupervised Domain Adaptation (UDA) methods aim to transfer knowledge from a
labeled source domain to an unlabeled target domain. UDA has been extensively
studied in the computer vision literature. Deep networks have been shown to be
vulnerable to adversarial attacks. However, very little focus is devoted to
improving the adversarial robustness of deep UDA models, causing serious
concerns about model reliability. Adversarial Training (AT) has been considered
to be the most successful adversarial defense approach. Nevertheless,
conventional AT requires ground-truth labels to generate adversarial examples
and train models, which limits its effectiveness in the unlabeled target
domain. In this paper, we aim to explore AT to robustify UDA models: How to
enhance the unlabeled data robustness via AT while learning domain-invariant
features for UDA? To answer this question, we provide a systematic study into
multiple AT variants that can potentially be applied to UDA. Moreover, we
propose a novel Adversarially Robust Training method for UDA accordingly,
referred to as ARTUDA. Extensive experiments on multiple adversarial attacks
and UDA benchmarks show that ARTUDA consistently improves the adversarial
robustness of UDA models. Code is available at
https://github.com/shaoyuanlo/ARTUDA",2202.09300v2,https://arxiv.org/pdf/2202.09300v2
"Critical Checkpoints for Evaluating Defence Models Against Adversarial
  Attack and Robustness","Kanak Tekwani, Manojkumar Parmar","From past couple of years there is a cycle of researchers proposing a defence
model for adversaries in machine learning which is arguably defensible to most
of the existing attacks in restricted condition (they evaluate on some bounded
inputs or datasets). And then shortly another set of researcher finding the
vulnerabilities in that defence model and breaking it by proposing a stronger
attack model. Some common flaws are been noticed in the past defence models
that were broken in very short time. Defence models being broken so easily is a
point of concern as decision of many crucial activities are taken with the help
of machine learning models. So there is an utter need of some defence
checkpoints that any researcher should keep in mind while evaluating the
soundness of technique and declaring it to be decent defence technique. In this
paper, we have suggested few checkpoints that should be taken into
consideration while building and evaluating the soundness of defence models.
All these points are recommended after observing why some past defence models
failed and how some model remained adamant and proved their soundness against
some of the very strong attacks.",2202.09039v1,https://arxiv.org/pdf/2202.09039v1
"Rethinking Machine Learning Robustness via its Link with the
  Out-of-Distribution Problem","Abderrahmen Amich, Birhanu Eshete","Despite multiple efforts made towards robust machine learning (ML) models,
their vulnerability to adversarial examples remains a challenging problem that
calls for rethinking the defense strategy. In this paper, we take a step back
and investigate the causes behind ML models' susceptibility to adversarial
examples. In particular, we focus on exploring the cause-effect link between
adversarial examples and the out-of-distribution (OOD) problem. To that end, we
propose an OOD generalization method that stands against both adversary-induced
and natural distribution shifts. Through an OOD to in-distribution mapping
intuition, our approach translates OOD inputs to the data distribution used to
train and test the model. Through extensive experiments on three benchmark
image datasets of different scales (MNIST, CIFAR10, and ImageNet) and by
leveraging image-to-image translation methods, we confirm that the adversarial
examples problem is a special case of the wider OOD generalization problem.
Across all datasets, we show that our translation-based approach consistently
improves robustness to OOD adversarial inputs and outperforms state-of-the-art
defenses by a significant margin, while preserving the exact accuracy on benign
(in-distribution) data. Furthermore, our method generalizes on naturally OOD
inputs such as darker or sharper images",2202.08944v1,https://arxiv.org/pdf/2202.08944v1
"Global Convergence of Sub-gradient Method for Robust Matrix Recovery:
  Small Initialization, Noisy Measurements, and Over-parameterization","Jianhao Ma, Salar Fattahi","In this work, we study the performance of sub-gradient method (SubGM) on a
natural nonconvex and nonsmooth formulation of low-rank matrix recovery with
$\ell_1$-loss, where the goal is to recover a low-rank matrix from a limited
number of measurements, a subset of which may be grossly corrupted with noise.
We study a scenario where the rank of the true solution is unknown and
over-estimated instead. The over-estimation of the rank gives rise to an
over-parameterized model in which there are more degrees of freedom than
needed. Such over-parameterization may lead to overfitting, or adversely affect
the performance of the algorithm. We prove that a simple SubGM with small
initialization is agnostic to both over-parameterization and noise in the
measurements. In particular, we show that small initialization nullifies the
effect of over-parameterization on the performance of SubGM, leading to an
exponential improvement in its convergence rate. Moreover, we provide the first
unifying framework for analyzing the behavior of SubGM under both outlier and
Gaussian noise models, showing that SubGM converges to the true solution, even
under arbitrarily large and arbitrarily dense noise values, and--perhaps
surprisingly--even if the globally optimal solutions do not correspond to the
ground truth. At the core of our results is a robust variant of restricted
isometry property, called Sign-RIP, which controls the deviation of the
sub-differential of the $\ell_1$-loss from that of an ideal, expected loss. As
a byproduct of our results, we consider a subclass of robust low-rank matrix
recovery with Gaussian measurements, and show that the number of required
samples to guarantee the global convergence of SubGM is independent of the
over-parameterized rank.",2202.08788v1,https://arxiv.org/pdf/2202.08788v1
Robust SVM Optimization in Banach spaces,"Mohammed Sbihi, Nicolas Couellan","We address the issue of binary classification in Banach spaces in presence of
uncertainty. We show that a number of results from classical support vector
machines theory can be appropriately generalised to their robust counterpart in
Banach spaces. These include the Representer Theorem, strong duality for the
associated Optimization problem as well as their geometric interpretation.
Furthermore, we propose a game theoretic interpretation by expressing a Nash
equilibrium problem formulation for the more general problem of finding the
closest points in two closed convex sets when the underlying space is reflexive
and smooth.",2202.08567v1,https://arxiv.org/pdf/2202.08567v1
Robust Reinforcement Learning via Genetic Curriculum,"Yeeho Song, Jeff Schneider","Achieving robust performance is crucial when applying deep reinforcement
learning (RL) in safety critical systems. Some of the state of the art
approaches try to address the problem with adversarial agents, but these agents
often require expert supervision to fine tune and prevent the adversary from
becoming too challenging to the trainee agent. While other approaches involve
automatically adjusting environment setups during training, they have been
limited to simple environments where low-dimensional encodings can be used.
Inspired by these approaches, we propose genetic curriculum, an algorithm that
automatically identifies scenarios in which the agent currently fails and
generates an associated curriculum to help the agent learn to solve the
scenarios and acquire more robust behaviors. As a non-parametric optimizer, our
approach uses a raw, non-fixed encoding of scenarios, reducing the need for
expert supervision and allowing our algorithm to adapt to the changing
performance of the agent. Our empirical studies show improvement in robustness
over the existing state of the art algorithms, providing training curricula
that result in agents being 2 - 8x times less likely to fail without
sacrificing cumulative reward. We include an ablation study and share insights
on why our algorithm outperforms prior approaches.",2202.08393v1,https://arxiv.org/pdf/2202.08393v1
"Vision Models Are More Robust And Fair When Pretrained On Uncurated
  Images Without Supervision","Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Ishan Misra, Levent Sagun, Armand Joulin, Piotr Bojanowski","Discriminative self-supervised learning allows training models on any random
group of internet images, and possibly recover salient information that helps
differentiate between the images. Applied to ImageNet, this leads to object
centric features that perform on par with supervised features on most
object-centric downstream tasks. In this work, we question if using this
ability, we can learn any salient and more representative information present
in diverse unbounded set of images from across the globe. To do so, we train
models on billions of random images without any data pre-processing or prior
assumptions about what we want the model to learn. We scale our model size to
dense 10 billion parameters to avoid underfitting on a large data size. We
extensively study and validate our model performance on over 50 benchmarks
including fairness, robustness to distribution shift, geographical diversity,
fine grained recognition, image copy detection and many image classification
datasets. The resulting model, not only captures well semantic information, it
also captures information about artistic style and learns salient information
such as geolocations and multilingual word embeddings based on visual content
only. More importantly, we discover that such model is more robust, more fair,
less harmful and less biased than supervised models or models trained on object
centric datasets such as ImageNet.",2202.08360v2,https://arxiv.org/pdf/2202.08360v2
"Phase Aberration Robust Beamformer for Planewave US Using
  Self-Supervised Learning","Shujaat Khan, Jaeyoung Huh, Jong Chul Ye","Ultrasound (US) is widely used for clinical imaging applications thanks to
its real-time and non-invasive nature. However, its lesion detectability is
often limited in many applications due to the phase aberration artefact caused
by variations in the speed of sound (SoS) within body parts. To address this,
here we propose a novel self-supervised 3D CNN that enables phase aberration
robust plane-wave imaging. Instead of aiming at estimating the SoS distribution
as in conventional methods, our approach is unique in that the network is
trained in a self-supervised manner to robustly generate a high-quality image
from various phase aberrated images by modeling the variation in the speed of
sound as stochastic. Experimental results using real measurements from
tissue-mimicking phantom and \textit{in vivo} scans confirmed that the proposed
method can significantly reduce the phase aberration artifacts and improve the
visual quality of deep scans.",2202.08262v1,https://arxiv.org/pdf/2202.08262v1
"Robust Nonparametric Distribution Forecast with Backtest-based Bootstrap
  and Adaptive Residual Selection","Longshaokan Wang, Lingda Wang, Mina Georgieva, Paulo Machado, Abinaya Ulagappa, Safwan Ahmed, Yan Lu, Arjun Bakshi, Farhad Ghassemi","Distribution forecast can quantify forecast uncertainty and provide various
forecast scenarios with their corresponding estimated probabilities. Accurate
distribution forecast is crucial for planning - for example when making
production capacity or inventory allocation decisions. We propose a practical
and robust distribution forecast framework that relies on backtest-based
bootstrap and adaptive residual selection. The proposed approach is robust to
the choice of the underlying forecasting model, accounts for uncertainty around
the input covariates, and relaxes the independence between residuals and
covariates assumption. It reduces the Absolute Coverage Error by more than 63%
compared to the classic bootstrap approaches and by 2% - 32% compared to a
variety of State-of-the-Art deep learning approaches on in-house product sales
data and M4-hourly competition data.",2202.07955v1,https://arxiv.org/pdf/2202.07955v1
Robust Multi-Objective Bayesian Optimization Under Input Noise,"Samuel Daulton, Sait Cakmak, Maximilian Balandat, Michael A. Osborne, Enlu Zhou, Eytan Bakshy","Bayesian optimization (BO) is a sample-efficient approach for tuning design
parameters to optimize expensive-to-evaluate, black-box performance metrics. In
many manufacturing processes, the design parameters are subject to random input
noise, resulting in a product that is often less performant than expected.
Although BO methods have been proposed for optimizing a single objective under
input noise, no existing method addresses the practical scenario where there
are multiple objectives that are sensitive to input perturbations. In this
work, we propose the first multi-objective BO method that is robust to input
noise. We formalize our goal as optimizing the multivariate value-at-risk
(MVaR), a risk measure of the uncertain objectives. Since directly optimizing
MVaR is computationally infeasible in many settings, we propose a scalable,
theoretically-grounded approach for optimizing MVaR using random
scalarizations. Empirically, we find that our approach significantly
outperforms alternative methods and efficiently identifies optimal robust
designs that will satisfy specifications across multiple metrics with high
probability.",2202.07549v4,https://arxiv.org/pdf/2202.07549v4
A precortical module for robust CNNs to light variations,"R. Fioresi, J. Petkovic","We present a simple mathematical model for the mammalian low visual pathway,
taking into account its key elements: retina, lateral geniculate nucleus (LGN),
primary visual cortex (V1). The analogies between the cortical level of the
visual system and the structure of popular CNNs, used in image classification
tasks, suggests the introduction of an additional preliminary convolutional
module inspired to precortical neuronal circuits to improve robustness with
respect to global light intensity and contrast variations in the input images.
We validate our hypothesis on the popular databases MNIST, FashionMNIST and
SVHN, obtaining significantly more robust CNNs with respect to these
variations, once such extra module is added.",2202.07432v2,https://arxiv.org/pdf/2202.07432v2
"Unreasonable Effectiveness of Last Hidden Layer Activations for
  Adversarial Robustness","Omer Faruk Tuna, Ferhat Ozgur Catak, M. Taner Eskil","In standard Deep Neural Network (DNN) based classifiers, the general
convention is to omit the activation function in the last (output) layer and
directly apply the softmax function on the logits to get the probability scores
of each class. In this type of architectures, the loss value of the classifier
against any output class is directly proportional to the difference between the
final probability score and the label value of the associated class. Standard
White-box adversarial evasion attacks, whether targeted or untargeted, mainly
try to exploit the gradient of the model loss function to craft adversarial
samples and fool the model. In this study, we show both mathematically and
experimentally that using some widely known activation functions in the output
layer of the model with high temperature values has the effect of zeroing out
the gradients for both targeted and untargeted attack cases, preventing
attackers from exploiting the model's loss function to craft adversarial
samples. We've experimentally verified the efficacy of our approach on MNIST
(Digit), CIFAR10 datasets. Detailed experiments confirmed that our approach
substantially improves robustness against gradient-based targeted and
untargeted attack threats. And, we showed that the increased non-linearity at
the output layer has some additional benefits against some other attack methods
like Deepfool attack.",2202.07342v2,https://arxiv.org/pdf/2202.07342v2
User-Oriented Robust Reinforcement Learning,"Haoyi You, Beichen Yu, Haiming Jin, Zhaoxing Yang, Jiahui Sun","Recently, improving the robustness of policies across different environments
attracts increasing attention in the reinforcement learning (RL) community.
Existing robust RL methods mostly aim to achieve the max-min robustness by
optimizing the policy's performance in the worst-case environment. However, in
practice, a user that uses an RL policy may have different preferences over its
performance across environments. Clearly, the aforementioned max-min robustness
is oftentimes too conservative to satisfy user preference. Therefore, in this
paper, we integrate user preference into policy learning in robust RL, and
propose a novel User-Oriented Robust RL (UOR-RL) framework. Specifically, we
define a new User-Oriented Robustness (UOR) metric for RL, which allocates
different weights to the environments according to user preference and
generalizes the max-min robustness metric. To optimize the UOR metric, we
develop two different UOR-RL training algorithms for the scenarios with or
without a priori known environment distribution, respectively. Theoretically,
we prove that our UOR-RL training algorithms converge to near-optimal policies
even with inaccurate or completely no knowledge about the environment
distribution. Furthermore, we carry out extensive experimental evaluations in 4
MuJoCo tasks. The experimental results demonstrate that UOR-RL is comparable to
the state-of-the-art baselines under the average and worst-case performance
metrics, and more importantly establishes new state-of-the-art performance
under the UOR metric.",2202.07301v4,https://arxiv.org/pdf/2202.07301v4
"Learning to Solve Routing Problems via Distributionally Robust
  Optimization","Yuan Jiang, Yaoxin Wu, Zhiguang Cao, Jie Zhang","Recent deep models for solving routing problems always assume a single
distribution of nodes for training, which severely impairs their
cross-distribution generalization ability. In this paper, we exploit group
distributionally robust optimization (group DRO) to tackle this issue, where we
jointly optimize the weights for different groups of distributions and the
parameters for the deep model in an interleaved manner during training. We also
design a module based on convolutional neural network, which allows the deep
model to learn more informative latent pattern among the nodes. We evaluate the
proposed approach on two types of well-known deep models including GCN and
POMO. The experimental results on the randomly synthesized instances and the
ones from two benchmark dataset (i.e., TSPLib and CVRPLib) demonstrate that our
approach could significantly improve the cross-distribution generalization
performance over the original models.",2202.07241v1,https://arxiv.org/pdf/2202.07241v1
Holistic Adversarial Robustness of Deep Learning Models,"Pin-Yu Chen, Sijia Liu","Adversarial robustness studies the worst-case performance of a machine
learning model to ensure safety and reliability. With the proliferation of
deep-learning-based technology, the potential risks associated with model
development and deployment can be amplified and become dreadful
vulnerabilities. This paper provides a comprehensive overview of research
topics and foundational principles of research methods for adversarial
robustness of deep learning models, including attacks, defenses, verification,
and novel applications.",2202.07201v3,https://arxiv.org/pdf/2202.07201v3
"TURF: A Two-factor, Universal, Robust, Fast Distribution Learning
  Algorithm","Yi Hao, Ayush Jain, Alon Orlitsky, Vaishakh Ravindrakumar","Approximating distributions from their samples is a canonical
statistical-learning problem. One of its most powerful and successful
modalities approximates every distribution to an $\ell_1$ distance essentially
at most a constant times larger than its closest $t$-piece degree-$d$
polynomial, where $t\ge1$ and $d\ge0$. Letting $c_{t,d}$ denote the smallest
such factor, clearly $c_{1,0}=1$, and it can be shown that $c_{t,d}\ge 2$ for
all other $t$ and $d$. Yet current computationally efficient algorithms show
only $c_{t,1}\le 2.25$ and the bound rises quickly to $c_{t,d}\le 3$ for $d\ge
9$. We derive a near-linear-time and essentially sample-optimal estimator that
establishes $c_{t,d}=2$ for all $(t,d)\ne(1,0)$. Additionally, for many
practical distributions, the lowest approximation distance is achieved by
polynomials with vastly varying number of pieces. We provide a method that
estimates this number near-optimally, hence helps approach the best possible
approximation. Experiments combining the two techniques confirm improved
performance over existing methodologies.",2202.07172v2,https://arxiv.org/pdf/2202.07172v2
Robust Policy Learning over Multiple Uncertainty Sets,"Annie Xie, Shagun Sodhani, Chelsea Finn, Joelle Pineau, Amy Zhang","Reinforcement learning (RL) agents need to be robust to variations in
safety-critical environments. While system identification methods provide a way
to infer the variation from online experience, they can fail in settings where
fast identification is not possible. Another dominant approach is robust RL
which produces a policy that can handle worst-case scenarios, but these methods
are generally designed to achieve robustness to a single uncertainty set that
must be specified at train time. Towards a more general solution, we formulate
the multi-set robustness problem to learn a policy robust to different
perturbation sets. We then design an algorithm that enjoys the benefits of both
system identification and robust RL: it reduces uncertainty where possible
given a few interactions, but can still act robustly with respect to the
remaining uncertainty. On a diverse set of control tasks, our approach
demonstrates improved worst-case performance on new environments compared to
prior methods based on system identification and on robust RL alone.",2202.07013v2,https://arxiv.org/pdf/2202.07013v2
Unlabeled Data Help: Minimax Analysis and Adversarial Robustness,"Yue Xing, Qifan Song, Guang Cheng","The recent proposed self-supervised learning (SSL) approaches successfully
demonstrate the great potential of supplementing learning algorithms with
additional unlabeled data. However, it is still unclear whether the existing
SSL algorithms can fully utilize the information of both labelled and unlabeled
data. This paper gives an affirmative answer for the reconstruction-based SSL
algorithm \citep{lee2020predicting} under several statistical models. While
existing literature only focuses on establishing the upper bound of the
convergence rate, we provide a rigorous minimax analysis, and successfully
justify the rate-optimality of the reconstruction-based SSL algorithm under
different data generation models. Furthermore, we incorporate the
reconstruction-based SSL into the existing adversarial training algorithms and
show that learning from unlabeled data helps improve the robustness.",2202.06996v1,https://arxiv.org/pdf/2202.06996v1
"Robust alignment of cross-session recordings of neural population
  activity by behaviour via unsupervised domain adaptation","Justin Jude, Matthew G Perich, Lee E Miller, Matthias H Hennig","Neural population activity relating to behaviour is assumed to be inherently
low-dimensional despite the observed high dimensionality of data recorded using
multi-electrode arrays. Therefore, predicting behaviour from neural population
recordings has been shown to be most effective when using latent variable
models. Over time however, the activity of single neurons can drift, and
different neurons will be recorded due to movement of implanted neural probes.
This means that a decoder trained to predict behaviour on one day performs
worse when tested on a different day. On the other hand, evidence suggests that
the latent dynamics underlying behaviour may be stable even over months and
years. Based on this idea, we introduce a model capable of inferring
behaviourally relevant latent dynamics from previously unseen data recorded
from the same animal, without any need for decoder recalibration. We show that
unsupervised domain adaptation combined with a sequential variational
autoencoder, trained on several sessions, can achieve good generalisation to
unseen data and correctly predict behaviour where conventional methods fail.
Our results further support the hypothesis that behaviour-related neural
dynamics are low-dimensional and stable over time, and will enable more
effective and flexible use of brain computer interface technologies.",2202.06159v2,https://arxiv.org/pdf/2202.06159v2
"TATTOOED: A Robust Deep Neural Network Watermarking Scheme based on
  Spread-Spectrum Channel Coding","Giulio Pagnotta, Dorjan Hitaj, Briland Hitaj, Fernando Perez-Cruz, Luigi V. Mancini","Watermarking of deep neural networks (DNNs) has gained significant traction
in recent years, with numerous (watermarking) strategies being proposed as
mechanisms that can help verify the ownership of a DNN in scenarios where these
models are obtained without the permission of the owner. However, a growing
body of work has demonstrated that existing watermarking mechanisms are highly
susceptible to removal techniques, such as fine-tuning, parameter pruning, or
shuffling. In this paper, we build upon extensive prior work on covert
(military) communication and propose TATTOOED, a novel DNN watermarking
technique that is robust to existing threats. We demonstrate that using
TATTOOED as their watermarking mechanisms, the DNN owner can successfully
obtain the watermark and verify model ownership even in scenarios where 99% of
model parameters are altered. Furthermore, we show that TATTOOED is easy to
employ in training pipelines, and has negligible impact on model performance.",2202.06091v3,https://arxiv.org/pdf/2202.06091v3
"RoPGen: Towards Robust Code Authorship Attribution via Automatic Coding
  Style Transformation","Zhen Li, Guenevere, Chen, Chen Chen, Yayi Zou, Shouhuai Xu","Source code authorship attribution is an important problem often encountered
in applications such as software forensics, bug fixing, and software quality
analysis. Recent studies show that current source code authorship attribution
methods can be compromised by attackers exploiting adversarial examples and
coding style manipulation. This calls for robust solutions to the problem of
code authorship attribution. In this paper, we initiate the study on making
Deep Learning (DL)-based code authorship attribution robust. We propose an
innovative framework called Robust coding style Patterns Generation (RoPGen),
which essentially learns authors' unique coding style patterns that are hard
for attackers to manipulate or imitate. The key idea is to combine data
augmentation and gradient augmentation at the adversarial training phase. This
effectively increases the diversity of training examples, generates meaningful
perturbations to gradients of deep neural networks, and learns diversified
representations of coding styles. We evaluate the effectiveness of RoPGen using
four datasets of programs written in C, C++, and Java. Experimental results
show that RoPGen can significantly improve the robustness of DL-based code
authorship attribution, by respectively reducing 22.8% and 41.0% of the success
rate of targeted and untargeted attacks on average.",2202.06043v1,https://arxiv.org/pdf/2202.06043v1
Robust Learning from Observation with Model Misspecification,"Luca Viano, Yu-Ting Huang, Parameswaran Kamalaruban, Craig Innes, Subramanian Ramamoorthy, Adrian Weller","Imitation learning (IL) is a popular paradigm for training policies in
robotic systems when specifying the reward function is difficult. However,
despite the success of IL algorithms, they impose the somewhat unrealistic
requirement that the expert demonstrations must come from the same domain in
which a new imitator policy is to be learned. We consider a practical setting,
where (i) state-only expert demonstrations from the real (deployment)
environment are given to the learner, (ii) the imitation learner policy is
trained in a simulation (training) environment whose transition dynamics is
slightly different from the real environment, and (iii) the learner does not
have any access to the real environment during the training phase beyond the
batch of demonstrations given. Most of the current IL methods, such as
generative adversarial imitation learning and its state-only variants, fail to
imitate the optimal expert behavior under the above setting. By leveraging
insights from the Robust reinforcement learning (RL) literature and building on
recent adversarial imitation approaches, we propose a robust IL algorithm to
learn policies that can effectively transfer to the real environment without
fine-tuning. Furthermore, we empirically demonstrate on continuous-control
benchmarks that our method outperforms the state-of-the-art state-only IL
method in terms of the zero-shot transfer performance in the real environment
and robust performance under different testing conditions.",2202.06003v2,https://arxiv.org/pdf/2202.06003v2
Robust Deep Semi-Supervised Learning: A Brief Introduction,"Lan-Zhe Guo, Zhi Zhou, Yu-Feng Li","Semi-supervised learning (SSL) is the branch of machine learning that aims to
improve learning performance by leveraging unlabeled data when labels are
insufficient. Recently, SSL with deep models has proven to be successful on
standard benchmark tasks. However, they are still vulnerable to various
robustness threats in real-world applications as these benchmarks provide
perfect unlabeled data, while in realistic scenarios, unlabeled data could be
corrupted. Many researchers have pointed out that after exploiting corrupted
unlabeled data, SSL suffers severe performance degradation problems. Thus,
there is an urgent need to develop SSL algorithms that could work robustly with
corrupted unlabeled data. To fully understand robust SSL, we conduct a survey
study. We first clarify a formal definition of robust SSL from the perspective
of machine learning. Then, we classify the robustness threats into three
categories: i) distribution corruption, i.e., unlabeled data distribution is
mismatched with labeled data; ii) feature corruption, i.e., the features of
unlabeled examples are adversarially attacked; and iii) label corruption, i.e.,
the label distribution of unlabeled data is imbalanced. Under this unified
taxonomy, we provide a thorough review and discussion of recent works that
focus on these issues. Finally, we propose possible promising directions within
robust SSL to provide insights for future research.",2202.05975v2,https://arxiv.org/pdf/2202.05975v2
"Boosting Barely Robust Learners: A New Perspective on Adversarial
  Robustness","Avrim Blum, Omar Montasser, Greg Shakhnarovich, Hongyang Zhang","We present an oracle-efficient algorithm for boosting the adversarial
robustness of barely robust learners. Barely robust learning algorithms learn
predictors that are adversarially robust only on a small fraction $\beta \ll 1$
of the data distribution. Our proposed notion of barely robust learning
requires robustness with respect to a ""larger"" perturbation set; which we show
is necessary for strongly robust learning, and that weaker relaxations are not
sufficient for strongly robust learning. Our results reveal a qualitative and
quantitative equivalence between two seemingly unrelated problems: strongly
robust learning and barely robust learning.",2202.05920v1,https://arxiv.org/pdf/2202.05920v1
Distributionally Robust Data Join,"Pranjal Awasthi, Christopher Jung, Jamie Morgenstern","Suppose we are given two datasets: a labeled dataset and unlabeled dataset
which also has additional auxiliary features not present in the first dataset.
What is the most principled way to use these datasets together to construct a
predictor?
  The answer should depend upon whether these datasets are generated by the
same or different distributions over their mutual feature sets, and how similar
the test distribution will be to either of those distributions. In many
applications, the two datasets will likely follow different distributions, but
both may be close to the test distribution. We introduce the problem of
building a predictor which minimizes the maximum loss over all probability
distributions over the original features, auxiliary features, and binary
labels, whose Wasserstein distance is $r_1$ away from the empirical
distribution over the labeled dataset and $r_2$ away from that of the unlabeled
dataset. This can be thought of as a generalization of distributionally robust
optimization (DRO), which allows for two data sources, one of which is
unlabeled and may contain auxiliary features.",2202.05797v2,https://arxiv.org/pdf/2202.05797v2
"SleepPPG-Net: a deep learning algorithm for robust sleep staging from
  continuous photoplethysmography","Kevin Kotzen, Peter H. Charlton, Sharon Salabi, Lea Amar, Amir Landesberg, Joachim A. Behar","Introduction: Sleep staging is an essential component in the diagnosis of
sleep disorders and management of sleep health. It is traditionally measured in
a clinical setting and requires a labor-intensive labeling process. We
hypothesize that it is possible to perform robust 4-class sleep staging using
the raw photoplethysmography (PPG) time series and modern advances in deep
learning (DL). Methods: We used two publicly available sleep databases that
included raw PPG recordings, totalling 2,374 patients and 23,055 hours. We
developed SleepPPG-Net, a DL model for 4-class sleep staging from the raw PPG
time series. SleepPPG-Net was trained end-to-end and consists of a residual
convolutional network for automatic feature extraction and a temporal
convolutional network to capture long-range contextual information. We
benchmarked the performance of SleepPPG-Net against models based on the
best-reported state-of-the-art (SOTA) algorithms. Results: When benchmarked on
a held-out test set, SleepPPG-Net obtained a median Cohen's Kappa ($\kappa$)
score of 0.75 against 0.69 for the best SOTA approach. SleepPPG-Net showed good
generalization performance to an external database, obtaining a $\kappa$ score
of 0.74 after transfer learning. Perspective: Overall, SleepPPG-Net provides
new SOTA performance. In addition, performance is high enough to open the path
to the development of wearables that meet the requirements for usage in
clinical applications such as the diagnosis and monitoring of obstructive sleep
apnea.",2202.05735v4,https://arxiv.org/pdf/2202.05735v4
"CMW-Net: Learning a Class-Aware Sample Weighting Mapping for Robust Deep
  Learning","Jun Shu, Xiang Yuan, Deyu Meng, Zongben Xu","Modern deep neural networks can easily overfit to biased training data
containing corrupted labels or class imbalance. Sample re-weighting methods are
popularly used to alleviate this data bias issue. Most current methods,
however, require to manually pre-specify the weighting schemes as well as their
additional hyper-parameters relying on the characteristics of the investigated
problem and training data. This makes them fairly hard to be generally applied
in practical scenarios, due to their significant complexities and inter-class
variations of data bias situations. To address this issue, we propose a
meta-model capable of adaptively learning an explicit weighting scheme directly
from data. Specifically, by seeing each training class as a separate learning
task, our method aims to extract an explicit weighting function with sample
loss and task/class feature as input, and sample weight as output, expecting to
impose adaptively varying weighting schemes to different sample classes based
on their own intrinsic bias characteristics. Synthetic and real data
experiments substantiate the capability of our method on achieving proper
weighting schemes in various data bias cases, like the class imbalance,
feature-independent and dependent label noise scenarios, and more complicated
bias scenarios beyond conventional cases. Besides, the task-transferability of
the learned weighting scheme is also substantiated, by readily deploying the
weighting function learned on relatively smaller-scale CIFAR-10 dataset on much
larger-scale full WebVision dataset. A performance gain can be readily achieved
compared with previous SOAT ones without additional hyper-parameter tuning and
meta gradient descent step. The general availability of our method for multiple
robust deep learning issues, including partial-label learning, semi-supervised
learning and selective classification, has also been validated.",2202.05613v3,https://arxiv.org/pdf/2202.05613v3
"Fast and Robust Sparsity Learning over Networks: A Decentralized
  Surrogate Median Regression Approach","Weidong Liu, Xiaojun Mao, Xin Zhang","Decentralized sparsity learning has attracted a significant amount of
attention recently due to its rapidly growing applications. To obtain the
robust and sparse estimators, a natural idea is to adopt the non-smooth median
loss combined with a $\ell_1$ sparsity regularizer. However, most of the
existing methods suffer from slow convergence performance caused by the {\em
double} non-smooth objective. To accelerate the computation, in this paper, we
proposed a decentralized surrogate median regression (deSMR) method for
efficiently solving the decentralized sparsity learning problem. We show that
our proposed algorithm enjoys a linear convergence rate with a simple
implementation. We also investigate the statistical guarantee, and it shows
that our proposed estimator achieves a near-oracle convergence rate without any
restriction on the number of network nodes. Moreover, we establish the
theoretical results for sparse support recovery. Thorough numerical experiments
and real data study are provided to demonstrate the effectiveness of our
method.",2202.05498v1,https://arxiv.org/pdf/2202.05498v1
"Concurrent Training of a Control Policy and a State Estimator for
  Dynamic and Robust Legged Locomotion","Gwanghyeon Ji, Juhyeok Mun, Hyeongjun Kim, Jemin Hwangbo","In this paper, we propose a locomotion training framework where a control
policy and a state estimator are trained concurrently. The framework consists
of a policy network which outputs the desired joint positions and a state
estimation network which outputs estimates of the robot's states such as the
base linear velocity, foot height, and contact probability. We exploit a fast
simulation environment to train the networks and the trained networks are
transferred to the real robot. The trained policy and state estimator are
capable of traversing diverse terrains such as a hill, slippery plate, and
bumpy road. We also demonstrate that the learned policy can run at up to 3.75
m/s on normal flat ground and 3.54 m/s on a slippery plate with the coefficient
of friction of 0.22.",2202.05481v2,https://arxiv.org/pdf/2202.05481v2
Robust estimation algorithms don't need to know the corruption level,"Ayush Jain, Alon Orlitsky, Vaishakh Ravindrakumar","Real data are rarely pure. Hence the past half-century has seen great
interest in robust estimation algorithms that perform well even when part of
the data is corrupt. However, their vast majority approach optimal accuracy
only when given a tight upper bound on the fraction of corrupt data. Such
bounds are not available in practice, resulting in weak guarantees and often
poor performance. This brief note abstracts the complex and pervasive
robustness problem into a simple geometric puzzle. It then applies the puzzle's
solution to derive a universal meta technique that converts any robust
estimation algorithm requiring a tight corruption-level upper bound to achieve
its optimal accuracy into one achieving essentially the same accuracy without
using any upper bounds.",2202.05453v1,https://arxiv.org/pdf/2202.05453v1
"Minimax Regret Optimization for Robust Machine Learning under
  Distribution Shift","Alekh Agarwal, Tong Zhang","In this paper, we consider learning scenarios where the learned model is
evaluated under an unknown test distribution which potentially differs from the
training distribution (i.e. distribution shift). The learner has access to a
family of weight functions such that the test distribution is a reweighting of
the training distribution under one of these functions, a setting typically
studied under the name of Distributionally Robust Optimization (DRO). We
consider the problem of deriving regret bounds in the classical learning theory
setting, and require that the resulting regret bounds hold uniformly for all
potential test distributions. We show that the DRO formulation does not
guarantee uniformly small regret under distribution shift. We instead propose
an alternative method called Minimax Regret Optimization (MRO), and show that
under suitable conditions this method achieves uniformly low regret across all
test distributions. We also adapt our technique to have stronger guarantees
when the test distributions are heterogeneous in their similarity to the
training data. Given the widespead optimization of worst case risks in current
approaches to robust machine learning, we believe that MRO can be a strong
alternative to address distribution shift scenarios.",2202.05436v1,https://arxiv.org/pdf/2202.05436v1
"A Characterization of Semi-Supervised Adversarially-Robust PAC
  Learnability","Idan Attias, Steve Hanneke, Yishay Mansour","We study the problem of learning an adversarially robust predictor to test
time attacks in the semi-supervised PAC model. We address the question of how
many labeled and unlabeled examples are required to ensure learning. We show
that having enough unlabeled data (the size of a labeled sample that a
fully-supervised method would require), the labeled sample complexity can be
arbitrarily smaller compared to previous works, and is sharply characterized by
a different complexity measure. We prove nearly matching upper and lower bounds
on this sample complexity. This shows that there is a significant benefit in
semi-supervised robust learning even in the worst-case distribution-free model,
and establishes a gap between the supervised and semi-supervised label
complexities which is known not to hold in standard non-robust PAC learning.",2202.05420v3,https://arxiv.org/pdf/2202.05420v3
"Accountability in an Algorithmic Society: Relationality, Responsibility,
  and Robustness in Machine Learning","A. Feder Cooper, Emanuel Moss, Benjamin Laufer, Helen Nissenbaum","In 1996, Accountability in a Computerized Society [95] issued a clarion call
concerning the erosion of accountability in society due to the ubiquitous
delegation of consequential functions to computerized systems. Nissenbaum [95]
described four barriers to accountability that computerization presented, which
we revisit in relation to the ascendance of data-driven algorithmic
systems--i.e., machine learning or artificial intelligence--to uncover new
challenges for accountability that these systems present. Nissenbaum's original
paper grounded discussion of the barriers in moral philosophy; we bring this
analysis together with recent scholarship on relational accountability
frameworks and discuss how the barriers present difficulties for instantiating
a unified moral, relational framework in practice for data-driven algorithmic
systems. We conclude by discussing ways of weakening the barriers in order to
do so.",2202.05338v3,https://arxiv.org/pdf/2202.05338v3
Adaptive and Robust Multi-Task Learning,"Yaqi Duan, Kaizheng Wang","We study the multi-task learning problem that aims to simultaneously analyze
multiple datasets collected from different sources and learn one model for each
of them. We propose a family of adaptive methods that automatically utilize
possible similarities among those tasks while carefully handling their
differences. We derive sharp statistical guarantees for the methods and prove
their robustness against outlier tasks. Numerical experiments on synthetic and
real datasets demonstrate the efficacy of our new methods.",2202.05250v4,https://arxiv.org/pdf/2202.05250v4
Learnable Nonlinear Compression for Robust Speaker Verification,"Xuechen Liu, Md Sahidullah, Tomi Kinnunen","In this study, we focus on nonlinear compression methods in spectral features
for speaker verification based on deep neural network. We consider different
kinds of channel-dependent (CD) nonlinear compression methods optimized in a
data-driven manner. Our methods are based on power nonlinearities and dynamic
range compression (DRC). We also propose multi-regime (MR) design on the
nonlinearities, at improving robustness. Results on VoxCeleb1 and VoxMovies
data demonstrate improvements brought by proposed compression methods over both
the commonly-used logarithm and their static counterparts, especially for ones
based on power function. While CD generalization improves performance on
VoxCeleb1, MR provides more robustness on VoxMovies, with a maximum relative
equal error rate reduction of 21.6%.",2202.05236v1,https://arxiv.org/pdf/2202.05236v1
Deadwooding: Robust Global Pruning for Deep Neural Networks,"Sawinder Kaur, Ferdinando Fioretto, Asif Salekin","The ability of Deep Neural Networks to approximate highly complex functions
is key to their success. This benefit, however, comes at the expense of a large
model size, which challenges its deployment in resource-constrained
environments. Pruning is an effective technique used to limit this issue, but
often comes at the cost of reduced accuracy and adversarial robustness. This
paper addresses these shortcomings and introduces Deadwooding, a novel global
pruning technique that exploits a Lagrangian Dual method to encourage model
sparsity while retaining accuracy and ensuring robustness. The resulting model
is shown to significantly outperform the state-of-the-art studies in measures
of robustness and accuracy.",2202.05226v4,https://arxiv.org/pdf/2202.05226v4
"Feature-level augmentation to improve robustness of deep neural networks
  to affine transformations","Adrian Sandru, Mariana-Iuliana Georgescu, Radu Tudor Ionescu","Recent studies revealed that convolutional neural networks do not generalize
well to small image transformations, e.g. rotations by a few degrees or
translations of a few pixels. To improve the robustness to such
transformations, we propose to introduce data augmentation at intermediate
layers of the neural architecture, in addition to the common data augmentation
applied on the input images. By introducing small perturbations to activation
maps (features) at various levels, we develop the capacity of the neural
network to cope with such transformations. We conduct experiments on three
image classification benchmarks (Tiny ImageNet, Caltech-256 and Food-101),
considering two different convolutional architectures (ResNet-18 and
DenseNet-121). When compared with two state-of-the-art stabilization methods,
the empirical results show that our approach consistently attains the best
trade-off between accuracy and mean flip rate.",2202.05152v4,https://arxiv.org/pdf/2202.05152v4
Robust Graph Representation Learning for Local Corruption Recovery,"Bingxin Zhou, Yuanhong Jiang, Yu Guang Wang, Jingwei Liang, Junbin Gao, Shirui Pan, Xiaoqun Zhang","The performance of graph representation learning is affected by the quality
of graph input. While existing research usually pursues a globally smoothed
graph embedding, we believe the rarely observed anomalies are as well harmful
to an accurate prediction. This work establishes a graph learning scheme that
automatically detects (locally) corrupted feature attributes and recovers
robust embedding for prediction tasks. The detection operation leverages a
graph autoencoder, which does not make any assumptions about the distribution
of the local corruptions. It pinpoints the positions of the anomalous node
attributes in an unbiased mask matrix, where robust estimations are recovered
with sparsity promoting regularizer. The optimizer approaches a new embedding
that is sparse in the framelet domain and conditionally close to input
observations. Extensive experiments are provided to validate our proposed model
can recover a robust graph representation from black-box poisoning and achieve
excellent performance.",2202.04936v4,https://arxiv.org/pdf/2202.04936v4
"Proceedings of the Robust Artificial Intelligence System Assurance
  (RAISA) Workshop 2022","Olivia Brown, Brad Dillman","The Robust Artificial Intelligence System Assurance (RAISA) workshop will
focus on research, development and application of robust artificial
intelligence (AI) and machine learning (ML) systems. Rather than studying
robustness with respect to particular ML algorithms, our approach will be to
explore robustness assurance at the system architecture level, during both
development and deployment, and within the human-machine teaming context. While
the research community is converging on robust solutions for individual AI
models in specific scenarios, the problem of evaluating and assuring the
robustness of an AI system across its entire life cycle is much more complex.
Moreover, the operational context in which AI systems are deployed necessitates
consideration of robustness and its relation to principles of fairness,
privacy, and explainability.",2202.04787v1,https://arxiv.org/pdf/2202.04787v1
"Robust Bayesian Inference for Simulator-based Models via the MMD
  Posterior Bootstrap","Charita Dellaporta, Jeremias Knoblauch, Theodoros Damoulas, François-Xavier Briol","Simulator-based models are models for which the likelihood is intractable but
simulation of synthetic data is possible. They are often used to describe
complex real-world phenomena, and as such can often be misspecified in
practice. Unfortunately, existing Bayesian approaches for simulators are known
to perform poorly in those cases. In this paper, we propose a novel algorithm
based on the posterior bootstrap and maximum mean discrepancy estimators. This
leads to a highly-parallelisable Bayesian inference algorithm with strong
robustness properties. This is demonstrated through an in-depth theoretical
study which includes generalisation bounds and proofs of frequentist
consistency and robustness of our posterior. The approach is then assessed on a
range of examples including a g-and-k distribution and a toggle-switch model.",2202.04744v3,https://arxiv.org/pdf/2202.04744v3
"Optimal Hyperparameters and Structure Setting of Multi-Objective Robust
  CNN Systems via Generalized Taguchi Method and Objective Vector Norm","Sheng-Guo Wang, Shanshan Jiang","Recently, Machine Learning (ML), Artificial Intelligence (AI), and
Convolutional Neural Network (CNN) have made huge progress with broad
applications, where their systems have deep learning structures and a large
number of hyperparameters that determine the quality and performance of the
CNNs and AI systems. These systems may have multi-objective ML and AI
performance needs. There is a key requirement to find the optimal
hyperparameters and structures for multi-objective robust optimal CNN systems.
This paper proposes a generalized Taguchi approach to effectively determine the
optimal hyperparameters and structure for the multi-objective robust optimal
CNN systems via their objective performance vector norm. The proposed approach
and methods are applied to a CNN classification system with the original ResNet
for CIFAR-10 dataset as a demonstration and validation, which shows the
proposed methods are highly effective to achieve an optimal accuracy rate of
the original ResNet on CIFAR-10.",2202.04567v2,https://arxiv.org/pdf/2202.04567v2
Gradient Methods Provably Converge to Non-Robust Networks,"Gal Vardi, Gilad Yehudai, Ohad Shamir","Despite a great deal of research, it is still unclear why neural networks are
so susceptible to adversarial examples. In this work, we identify natural
settings where depth-$2$ ReLU networks trained with gradient flow are provably
non-robust (susceptible to small adversarial $\ell_2$-perturbations), even when
robust networks that classify the training dataset correctly exist. Perhaps
surprisingly, we show that the well-known implicit bias towards margin
maximization induces bias towards non-robust networks, by proving that every
network which satisfies the KKT conditions of the max-margin problem is
non-robust.",2202.04347v2,https://arxiv.org/pdf/2202.04347v2
"A Data-Driven Approach to Robust Hypothesis Testing Using Sinkhorn
  Uncertainty Sets","Jie Wang, Yao Xie","Hypothesis testing for small-sample scenarios is a practically important
problem. In this paper, we investigate the robust hypothesis testing problem in
a data-driven manner, where we seek the worst-case detector over distributional
uncertainty sets centered around the empirical distribution from samples using
Sinkhorn distance. Compared with the Wasserstein robust test, the corresponding
least favorable distributions are supported beyond the training samples, which
provides a more flexible detector. Various numerical experiments are conducted
on both synthetic and real datasets to validate the competitive performances of
our proposed method.",2202.04258v3,https://arxiv.org/pdf/2202.04258v3
"Learning Robust Convolutional Neural Networks with Relevant Feature
  Focusing via Explanations","Kazuki Adachi, Shin'ya Yamaguchi","Existing image recognition techniques based on convolutional neural networks
(CNNs) basically assume that the training and test datasets are sampled from
i.i.d distributions. However, this assumption is easily broken in the real
world because of the distribution shift that occurs when the co-occurrence
relations between objects and backgrounds in input images change. Under this
type of distribution shift, CNNs learn to focus on features that are not
task-relevant, such as backgrounds from the training data, and degrade their
accuracy on the test data. To tackle this problem, we propose relevant feature
focusing (ReFF). ReFF detects task-relevant features and regularizes CNNs via
explanation outputs (e.g., Grad-CAM). Since ReFF is composed of post-hoc
explanation modules, it can be easily applied to off-the-shelf CNNs.
Furthermore, ReFF requires no additional inference cost at test time because it
is only used for regularization while training. We demonstrate that CNNs
trained with ReFF focus on features relevant to the target task and that ReFF
improves the test-time accuracy.",2202.04237v2,https://arxiv.org/pdf/2202.04237v2
"Are Transformers More Robust? Towards Exact Robustness Verification for
  Transformers","Brian Hsuan-Cheng Liao, Chih-Hong Cheng, Hasan Esen, Alois Knoll","As an emerging type of Neural Networks (NNs), Transformers are used in many
domains ranging from Natural Language Processing to Autonomous Driving. In this
paper, we study the robustness problem of Transformers, a key characteristic as
low robustness may cause safety concerns. Specifically, we focus on
Sparsemax-based Transformers and reduce the finding of their maximum robustness
to a Mixed Integer Quadratically Constrained Programming (MIQCP) problem. We
also design two pre-processing heuristics that can be embedded in the MIQCP
encoding and substantially accelerate its solving. We then conduct experiments
using the application of Land Departure Warning to compare the robustness of
Sparsemax-based Transformers against that of the more conventional
Multi-Layer-Perceptron (MLP) NNs. To our surprise, Transformers are not
necessarily more robust, leading to profound considerations in selecting
appropriate NN architectures for safety-critical domain applications.",2202.03932v4,https://arxiv.org/pdf/2202.03932v4
Robust Hybrid Learning With Expert Augmentation,"Antoine Wehenkel, Jens Behrmann, Hsiang Hsu, Guillermo Sapiro, Gilles Louppe, Jörn-Henrik Jacobsen","Hybrid modelling reduces the misspecification of expert models by combining
them with machine learning (ML) components learned from data. Similarly to many
ML algorithms, hybrid model performance guarantees are limited to the training
distribution. Leveraging the insight that the expert model is usually valid
even outside the training domain, we overcome this limitation by introducing a
hybrid data augmentation strategy termed \textit{expert augmentation}. Based on
a probabilistic formalization of hybrid modelling, we demonstrate that expert
augmentation, which can be incorporated into existing hybrid systems, improves
generalization. We empirically validate the expert augmentation on three
controlled experiments modelling dynamical systems with ordinary and partial
differential equations. Finally, we assess the potential real-world
applicability of expert augmentation on a dataset of a real double pendulum.",2202.03881v3,https://arxiv.org/pdf/2202.03881v3
"Robust, Deep, and Reinforcement Learning for Management of Communication
  and Power Networks",Alireza Sadeghi,"This thesis develops data-driven machine learning algorithms to managing and
optimizing the next-generation highly complex cyberphysical systems, which
desperately need ground-breaking control, monitoring, and decision making
schemes that can guarantee robustness, scalability, and situational awareness.
The present thesis first develops principled methods to make generic machine
learning models robust against distributional uncertainties and adversarial
data. Particular focus will be on parametric models where some training data
are being used to learn a parametric model. The developed framework is of high
interest especially when training and testing data are drawn from ""slightly""
different distribution. We then introduce distributionally robust learning
frameworks to minimize the worst-case expected loss over a prescribed ambiguity
set of training distributions quantified via Wasserstein distance. Later, we
build on this robust framework to design robust semi-supervised learning over
graph methods. The second part of this thesis aspires to fully unleash the
potential of next-generation wired and wireless networks, where we design
""smart"" network entities using (deep) reinforcement learning approaches.
Finally, this thesis enhances the power system operation and control. Our
contribution is on sustainable distribution grids with high penetration of
renewable sources and demand response programs. To account for unanticipated
and rapidly changing renewable generation and load consumption scenarios, we
specifically delegate reactive power compensation to both utility-owned control
devices (e.g., capacitor banks), as well as smart inverters of distributed
generation units with cyber-capabilities.",2202.05395v1,https://arxiv.org/pdf/2202.05395v1
FrePGAN: Robust Deepfake Detection Using Frequency-level Perturbations,"Yonghyun Jeong, Doyeon Kim, Youngmin Ro, Jongwon Choi","Various deepfake detectors have been proposed, but challenges still exist to
detect images of unknown categories or GAN models outside of the training
settings. Such issues arise from the overfitting issue, which we discover from
our own analysis and the previous studies to originate from the frequency-level
artifacts in generated images. We find that ignoring the frequency-level
artifacts can improve the detector's generalization across various GAN models,
but it can reduce the model's performance for the trained GAN models. Thus, we
design a framework to generalize the deepfake detector for both the known and
unseen GAN models. Our framework generates the frequency-level perturbation
maps to make the generated images indistinguishable from the real images. By
updating the deepfake detector along with the training of the perturbation
generator, our model is trained to detect the frequency-level artifacts at the
initial iterations and consider the image-level irregularities at the last
iterations. For experiments, we design new test scenarios varying from the
training settings in GAN models, color manipulations, and object categories.
Numerous experiments validate the state-of-the-art performance of our deepfake
detector.",2202.03347v1,https://arxiv.org/pdf/2202.03347v1
Robust Semantic Communications Against Semantic Noise,"Qiyu Hu, Guangyi Zhang, Zhijin Qin, Yunlong Cai, Guanding Yu, Geoffrey Ye Li","Although the semantic communications have exhibited satisfactory performance
in a large number of tasks, the impact of semantic noise and the robustness of
the systems have not been well investigated. Semantic noise is a particular
kind of noise in semantic communication systems, which refers to the misleading
between the intended semantic symbols and received ones. In this paper, we
first propose a framework for the robust end-to-end semantic communication
systems to combat the semantic noise. Particularly, we analyze the causes of
semantic noise and propose a practical method to generate it. To remove the
effect of semantic noise, adversarial training is proposed to incorporate the
samples with semantic noise in the training dataset. Then, the masked
autoencoder (MAE) is designed as the architecture of a robust semantic
communication system, where a portion of the input is masked. To further
improve the robustness of semantic communication systems, we firstly employ the
vector quantization-variational autoencoder (VQ-VAE) to design a discrete
codebook shared by the transmitter and the receiver for encoded feature
representation. Thus, the transmitter simply needs to transmit the indices of
these features in the codebook. Simulation results show that our proposed
method significantly improves the robustness of semantic communication systems
against semantic noise with significant reduction on the transmission overhead.",2202.03338v2,https://arxiv.org/pdf/2202.03338v2
Distributionally Robust Fair Principal Components via Geodesic Descents,"Hieu Vu, Toan Tran, Man-Chung Yue, Viet Anh Nguyen","Principal component analysis is a simple yet useful dimensionality reduction
technique in modern machine learning pipelines. In consequential domains such
as college admission, healthcare and credit approval, it is imperative to take
into account emerging criteria such as the fairness and the robustness of the
learned projection. In this paper, we propose a distributionally robust
optimization problem for principal component analysis which internalizes a
fairness criterion in the objective function. The learned projection thus
balances the trade-off between the total reconstruction error and the
reconstruction error gap between subgroups, taken in the min-max sense over all
distributions in a moment-based ambiguity set. The resulting optimization
problem over the Stiefel manifold can be efficiently solved by a Riemannian
subgradient descent algorithm with a sub-linear convergence rate. Our
experimental results on real-world datasets show the merits of our proposed
method over state-of-the-art baselines.",2202.03071v1,https://arxiv.org/pdf/2202.03071v1
Robust Anomaly Detection for Time-series Data,"Min Hu, Yi Wang, Xiaowei Feng, Shengchen Zhou, Zhaoyu Wu, Yuan Qin","Time-series anomaly detection plays a vital role in monitoring complex
operation conditions. However, the detection accuracy of existing approaches is
heavily influenced by pattern distribution, existence of multiple normal
patterns, dynamical features representation, and parameter settings. For the
purpose of improving the robustness and guaranteeing the accuracy, this
research combined the strengths of negative selection, unthresholded recurrence
plots, and an extreme learning machine autoencoder and then proposed robust
anomaly detection for time-series data (RADTD), which can automatically learn
dynamical features in time series and recognize anomalies with low label
dependency and high robustness. Yahoo benchmark datasets and three tunneling
engineering simulation experiments were used to evaluate the performance of
RADTD. The experiments showed that in benchmark datasets RADTD possessed higher
accuracy and robustness than recurrence qualification analysis and extreme
learning machine autoencoder, respectively, and that RADTD accurately detected
the occurrence of tunneling settlement accidents, indicating its remarkable
performance in accuracy and robustness.",2202.02721v1,https://arxiv.org/pdf/2202.02721v1
"Memory Defense: More Robust Classification via a Memory-Masking
  Autoencoder","Eashan Adhikarla, Dan Luo, Brian D. Davison","Many deep neural networks are susceptible to minute perturbations of images
that have been carefully crafted to cause misclassification. Ideally, a robust
classifier would be immune to small variations in input images, and a number of
defensive approaches have been created as a result. One method would be to
discern a latent representation which could ignore small changes to the input.
However, typical autoencoders easily mingle inter-class latent representations
when there are strong similarities between classes, making it harder for a
decoder to accurately project the image back to the original high-dimensional
space. We propose a novel framework, Memory Defense, an augmented classifier
with a memory-masking autoencoder to counter this challenge. By masking other
classes, the autoencoder learns class-specific independent latent
representations. We test the model's robustness against four widely used
attacks. Experiments on the Fashion-MNIST & CIFAR-10 datasets demonstrate the
superiority of our model. We make available our source code at GitHub
repository: https://github.com/eashanadhikarla/MemDefense",2202.02595v1,https://arxiv.org/pdf/2202.02595v1
Adversarial Detector with Robust Classifier,"Takayuki Osakabe, Maungmaung Aprilpyone, Sayaka Shiota, Hitoshi Kiya","Deep neural network (DNN) models are wellknown to easily misclassify
prediction results by using input images with small perturbations, called
adversarial examples. In this paper, we propose a novel adversarial detector,
which consists of a robust classifier and a plain one, to highly detect
adversarial examples. The proposed adversarial detector is carried out in
accordance with the logits of plain and robust classifiers. In an experiment,
the proposed detector is demonstrated to outperform a state-of-the-art detector
without any robust classifier.",2202.02503v1,https://arxiv.org/pdf/2202.02503v1
"HENRI: High Efficiency Negotiation-based Robust Interface for
  Multi-party Multi-issue Negotiation over the Internet","Saurabh Deochake, Shashank Kanth, Subhadip Chakraborty, Suresh Sarode, Vidyasagar Potdar, Debajyoti Mukhopadhyay","This paper proposes a framework for a full fledged negotiation system that
allows multi party multi issue negotiation. It focuses on the negotiation
protocol to be observed and provides a platform for concurrent and independent
negotiation on individual issues using the concept of multi threading. It
depicts the architecture of an agent detailing its components. The paper sets
forth a hierarchical pattern for the multiple issues concerning every party.
The system also provides enhancements such as the time-to-live counters for
every advertisement, refinement of utility considering non-functional
attributes, prioritization of issues, by assigning weights to issues.",2202.02430v1,https://arxiv.org/pdf/2202.02430v1
Robust Linear Regression for General Feature Distribution,"Tom Norman, Nir Weinberger, Kfir Y. Levy","We investigate robust linear regression where data may be contaminated by an
oblivious adversary, i.e., an adversary than may know the data distribution but
is otherwise oblivious to the realizations of the data samples. This model has
been previously analyzed under strong assumptions. Concretely, $\textbf{(i)}$
all previous works assume that the covariance matrix of the features is
positive definite; and $\textbf{(ii)}$ most of them assume that the features
are centered (i.e. zero mean). Additionally, all previous works make additional
restrictive assumption, e.g., assuming that the features are Gaussian or that
the corruptions are symmetrically distributed.
  In this work we go beyond these assumptions and investigate robust regression
under a more general set of assumptions: $\textbf{(i)}$ we allow the covariance
matrix to be either positive definite or positive semi definite,
$\textbf{(ii)}$ we do not necessarily assume that the features are centered,
$\textbf{(iii)}$ we make no further assumption beyond boundedness
(sub-Gaussianity) of features and measurement noise.
  Under these assumption we analyze a natural SGD variant for this problem and
show that it enjoys a fast convergence rate when the covariance matrix is
positive definite. In the positive semi definite case we show that there are
two regimes: if the features are centered we can obtain a standard convergence
rate; otherwise the adversary can cause any learner to fail arbitrarily.",2202.02080v1,https://arxiv.org/pdf/2202.02080v1
Robust Vector Quantized-Variational Autoencoder,"Chieh-Hsin Lai, Dongmian Zou, Gilad Lerman","Image generative models can learn the distributions of the training data and
consequently generate examples by sampling from these distributions. However,
when the training dataset is corrupted with outliers, generative models will
likely produce examples that are also similar to the outliers. In fact, a small
portion of outliers may induce state-of-the-art generative models, such as
Vector Quantized-Variational AutoEncoder (VQ-VAE), to learn a significant mode
from the outliers. To mitigate this problem, we propose a robust generative
model based on VQ-VAE, which we name Robust VQ-VAE (RVQ-VAE). In order to
achieve robustness, RVQ-VAE uses two separate codebooks for the inliers and
outliers. To ensure the codebooks embed the correct components, we iteratively
update the sets of inliers and outliers during each training epoch. To ensure
that the encoded data points are matched to the correct codebooks, we quantize
using a weighted Euclidean distance, whose weights are determined by
directional variances of the codebooks. Both codebooks, together with the
encoder and decoder, are trained jointly according to the reconstruction loss
and the quantization loss. We experimentally demonstrate that RVQ-VAE is able
to generate examples from inliers even if a large portion of the training data
points are corrupted.",2202.01987v2,https://arxiv.org/pdf/2202.01987v2
"A Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian
  Process Bandits","Ilija Bogunovic, Zihan Li, Andreas Krause, Jonathan Scarlett","We consider the sequential optimization of an unknown, continuous, and
expensive to evaluate reward function, from noisy and adversarially corrupted
observed rewards. When the corruption attacks are subject to a suitable budget
$C$ and the function lives in a Reproducing Kernel Hilbert Space (RKHS), the
problem can be posed as corrupted Gaussian process (GP) bandit optimization. We
propose a novel robust elimination-type algorithm that runs in epochs, combines
exploration with infrequent switching to select a small subset of actions, and
plays each action for multiple time instants. Our algorithm, Robust GP Phased
Elimination (RGP-PE), successfully balances robustness to corruptions with
exploration and exploitation such that its performance degrades minimally in
the presence (or absence) of adversarial corruptions. When $T$ is the number of
samples and $\gamma_T$ is the maximal information gain, the
corruption-dependent term in our regret bound is $O(C \gamma_T^{3/2})$, which
is significantly tighter than the existing $O(C \sqrt{T \gamma_T})$ for several
commonly-considered kernels. We perform the first empirical study of robustness
in the corrupted GP bandit setting, and show that our algorithm is robust
against a variety of adversarial attacks.",2202.01850v2,https://arxiv.org/pdf/2202.01850v2
"Adversarially Robust Models may not Transfer Better: Sufficient
  Conditions for Domain Transferability from the View of Regularization","Xiaojun Xu, Jacky Yibo Zhang, Evelyn Ma, Danny Son, Oluwasanmi Koyejo, Bo Li","Machine learning (ML) robustness and domain generalization are fundamentally
correlated: they essentially concern data distribution shifts under adversarial
and natural settings, respectively. On one hand, recent studies show that more
robust (adversarially trained) models are more generalizable. On the other
hand, there is a lack of theoretical understanding of their fundamental
connections. In this paper, we explore the relationship between regularization
and domain transferability considering different factors such as norm
regularization and data augmentations (DA). We propose a general theoretical
framework proving that factors involving the model function class
regularization are sufficient conditions for relative domain transferability.
Our analysis implies that ``robustness"" is neither necessary nor sufficient for
transferability; rather, regularization is a more fundamental perspective for
understanding domain transferability. We then discuss popular DA protocols
(including adversarial training) and show when they can be viewed as the
function class regularization under certain conditions and therefore improve
generalization. We conduct extensive experiments to verify our theoretical
findings and show several counterexamples where robustness and generalization
are negatively correlated on different datasets.",2202.01832v2,https://arxiv.org/pdf/2202.01832v2
"RipsNet: a general architecture for fast and robust estimation of the
  persistent homology of point clouds","Thibault de Surrel, Felix Hensel, Mathieu Carrière, Théo Lacombe, Yuichi Ike, Hiroaki Kurihara, Marc Glisse, Frédéric Chazal","The use of topological descriptors in modern machine learning applications,
such as Persistence Diagrams (PDs) arising from Topological Data Analysis
(TDA), has shown great potential in various domains. However, their practical
use in applications is often hindered by two major limitations: the
computational complexity required to compute such descriptors exactly, and
their sensitivity to even low-level proportions of outliers. In this work, we
propose to bypass these two burdens in a data-driven setting by entrusting the
estimation of (vectorization of) PDs built on top of point clouds to a neural
network architecture that we call RipsNet. Once trained on a given data set,
RipsNet can estimate topological descriptors on test data very efficiently with
generalization capacity. Furthermore, we prove that RipsNet is robust to input
perturbations in terms of the 1-Wasserstein distance, a major improvement over
the standard computation of PDs that only enjoys Hausdorff stability, yielding
RipsNet to substantially outperform exactly-computed PDs in noisy settings. We
showcase the use of RipsNet on both synthetic and real-world data. Our
open-source implementation is publicly available at
https://github.com/hensel-f/ripsnet and will be included in the Gudhi library.",2202.01725v2,https://arxiv.org/pdf/2202.01725v2
Robust Audio Anomaly Detection,"Wo Jae Lee, Karim Helwani, Arvindh Krishnaswamy, Srikanth Tenneti","We propose an outlier robust multivariate time series model which can be used
for detecting previously unseen anomalous sounds based on noisy training data.
The presented approach doesn't assume the presence of labeled anomalies in the
training dataset and uses a novel deep neural network architecture to learn the
temporal dynamics of the multivariate time series at multiple resolutions while
being robust to contaminations in the training dataset. The temporal dynamics
are modeled using recurrent layers augmented with attention mechanism. These
recurrent layers are built on top of convolutional layers allowing the network
to extract features at multiple resolutions. The output of the network is an
outlier robust probability density function modeling the conditional
probability of future samples given the time series history. State-of-the-art
approaches using other multiresolution architectures are contrasted with our
proposed approach. We validate our solution using publicly available machine
sound datasets. We demonstrate the effectiveness of our approach in anomaly
detection by comparing against several state-of-the-art models.",2202.01784v1,https://arxiv.org/pdf/2202.01784v1
"Doubly Robust Off-Policy Evaluation for Ranking Policies under the
  Cascade Behavior Model","Haruka Kiyohara, Yuta Saito, Tatsuya Matsuhiro, Yusuke Narita, Nobuyuki Shimizu, Yasuo Yamamoto","In real-world recommender systems and search engines, optimizing ranking
decisions to present a ranked list of relevant items is critical. Off-policy
evaluation (OPE) for ranking policies is thus gaining a growing interest
because it enables performance estimation of new ranking policies using only
logged data. Although OPE in contextual bandits has been studied extensively,
its naive application to the ranking setting faces a critical variance issue
due to the huge item space. To tackle this problem, previous studies introduce
some assumptions on user behavior to make the combinatorial item space
tractable. However, an unrealistic assumption may, in turn, cause serious bias.
Therefore, appropriately controlling the bias-variance tradeoff by imposing a
reasonable assumption is the key for success in OPE of ranking policies. To
achieve a well-balanced bias-variance tradeoff, we propose the Cascade Doubly
Robust estimator building on the cascade assumption, which assumes that a user
interacts with items sequentially from the top position in a ranking. We show
that the proposed estimator is unbiased in more cases compared to existing
estimators that make stronger assumptions. Furthermore, compared to a previous
estimator based on the same cascade assumption, the proposed estimator reduces
the variance by leveraging a control variate. Comprehensive experiments on both
synthetic and real-world data demonstrate that our estimator leads to more
accurate OPE than existing estimators in a variety of settings.",2202.01562v1,https://arxiv.org/pdf/2202.01562v1
Byzantine-Robust Decentralized Learning via ClippedGossip,"Lie He, Sai Praneeth Karimireddy, Martin Jaggi","In this paper, we study the challenging task of Byzantine-robust
decentralized training on arbitrary communication graphs. Unlike federated
learning where workers communicate through a server, workers in the
decentralized environment can only talk to their neighbors, making it harder to
reach consensus and benefit from collaborative training. To address these
issues, we propose a ClippedGossip algorithm for Byzantine-robust consensus and
optimization, which is the first to provably converge to a
$O(\delta_{\max}\zeta^2/\gamma^2)$ neighborhood of the stationary point for
non-convex objectives under standard assumptions. Finally, we demonstrate the
encouraging empirical performance of ClippedGossip under a large number of
attacks.",2202.01545v2,https://arxiv.org/pdf/2202.01545v2
"Data Heterogeneity-Robust Federated Learning via Group Client Selection
  in Industrial IoT","Zonghang Li, Yihong He, Hongfang Yu, Jiawen Kang, Xiaoping Li, Zenglin Xu, Dusit Niyato","Nowadays, the industrial Internet of Things (IIoT) has played an integral
role in Industry 4.0 and produced massive amounts of data for industrial
intelligence. These data locate on decentralized devices in modern factories.
To protect the confidentiality of industrial data, federated learning (FL) was
introduced to collaboratively train shared machine learning models. However,
the local data collected by different devices skew in class distribution and
degrade industrial FL performance. This challenge has been widely studied at
the mobile edge, but they ignored the rapidly changing streaming data and
clustering nature of factory devices, and more seriously, they may threaten
data security. In this paper, we propose FedGS, which is a hierarchical
cloud-edge-end FL framework for 5G empowered industries, to improve industrial
FL performance on non-i.i.d. data. Taking advantage of naturally clustered
factory devices, FedGS uses a gradient-based binary permutation algorithm
(GBP-CS) to select a subset of devices within each factory and build
homogeneous super nodes participating in FL training. Then, we propose a
compound-step synchronization protocol to coordinate the training process
within and among these super nodes, which shows great robustness against data
heterogeneity. The proposed methods are time-efficient and can adapt to dynamic
environments, without exposing confidential industrial data in risky
manipulation. We prove that FedGS has better convergence performance than
FedAvg and give a relaxed condition under which FedGS is more
communication-efficient. Extensive experiments show that FedGS improves
accuracy by 3.5% and reduces training rounds by 59% on average, confirming its
superior effectiveness and efficiency on non-i.i.d. data.",2202.01512v1,https://arxiv.org/pdf/2202.01512v1
Maximum Likelihood Uncertainty Estimation: Robustness to Outliers,"Deebul S. Nair, Nico Hochgeschwender, Miguel A. Olivares-Mendez","We benchmark the robustness of maximum likelihood based uncertainty
estimation methods to outliers in training data for regression tasks. Outliers
or noisy labels in training data results in degraded performances as well as
incorrect estimation of uncertainty. We propose the use of a heavy-tailed
distribution (Laplace distribution) to improve the robustness to outliers. This
property is evaluated using standard regression benchmarks and on a
high-dimensional regression task of monocular depth estimation, both containing
outliers. In particular, heavy-tailed distribution based maximum likelihood
provides better uncertainty estimates, better separation in uncertainty for
out-of-distribution data, as well as better detection of adversarial attacks in
the presence of outliers.",2202.03870v1,https://arxiv.org/pdf/2202.03870v1
Robust Binary Models by Pruning Randomly-initialized Networks,"Chen Liu, Ziqi Zhao, Sabine Süsstrunk, Mathieu Salzmann","Robustness to adversarial attacks was shown to require a larger model
capacity, and thus a larger memory footprint. In this paper, we introduce an
approach to obtain robust yet compact models by pruning randomly-initialized
binary networks. Unlike adversarial training, which learns the model
parameters, we initialize the model parameters as either +1 or -1, keep them
fixed, and find a subnetwork structure that is robust to attacks. Our method
confirms the Strong Lottery Ticket Hypothesis in the presence of adversarial
attacks, and extends this to binary networks. Furthermore, it yields more
compact networks with competitive performance than existing works by 1)
adaptively pruning different network layers; 2) exploiting an effective binary
initialization scheme; 3) incorporating a last batch normalization layer to
improve training stability. Our experiments demonstrate that our approach not
only always outperforms the state-of-the-art robust binary networks, but also
can achieve accuracy better than full-precision ones on some datasets. Finally,
we show the structured patterns of our pruned binary networks.",2202.01341v2,https://arxiv.org/pdf/2202.01341v2
"Robust Estimation for Nonparametric Families via Generative Adversarial
  Networks","Banghua Zhu, Jiantao Jiao, Michael I. Jordan","We provide a general framework for designing Generative Adversarial Networks
(GANs) to solve high dimensional robust statistics problems, which aim at
estimating unknown parameter of the true distribution given adversarially
corrupted samples. Prior work focus on the problem of robust mean and
covariance estimation when the true distribution lies in the family of Gaussian
distributions or elliptical distributions, and analyze depth or scoring rule
based GAN losses for the problem. Our work extend these to robust mean
estimation, second moment estimation, and robust linear regression when the
true distribution only has bounded Orlicz norms, which includes the broad
family of sub-Gaussian, sub-Exponential and bounded moment distributions. We
also provide a different set of sufficient conditions for the GAN loss to work:
we only require its induced distance function to be a cumulative density
function of some light-tailed distribution, which is easily satisfied by neural
networks with sigmoid activation. In terms of techniques, our proposed GAN
losses can be viewed as a smoothed and generalized Kolmogorov-Smirnov distance,
which overcomes the computational intractability of the original
Kolmogorov-Smirnov distance used in the prior work.",2202.01269v1,https://arxiv.org/pdf/2202.01269v1
NoisyMix: Boosting Model Robustness to Common Corruptions,"N. Benjamin Erichson, Soon Hoe Lim, Winnie Xu, Francisco Utrera, Ziang Cao, Michael W. Mahoney","For many real-world applications, obtaining stable and robust statistical
performance is more important than simply achieving state-of-the-art predictive
test accuracy, and thus robustness of neural networks is an increasingly
important topic. Relatedly, data augmentation schemes have been shown to
improve robustness with respect to input perturbations and domain shifts.
Motivated by this, we introduce NoisyMix, a novel training scheme that promotes
stability as well as leverages noisy augmentations in input and feature space
to improve both model robustness and in-domain accuracy. NoisyMix produces
models that are consistently more robust and that provide well-calibrated
estimates of class membership probabilities. We demonstrate the benefits of
NoisyMix on a range of benchmark datasets, including ImageNet-C, ImageNet-R,
and ImageNet-P. Moreover, we provide theory to understand implicit
regularization and robustness of NoisyMix.",2202.01263v2,https://arxiv.org/pdf/2202.01263v2
"Probabilistically Robust Learning: Balancing Average- and Worst-case
  Performance","Alexander Robey, Luiz F. O. Chamon, George J. Pappas, Hamed Hassani","Many of the successes of machine learning are based on minimizing an averaged
loss function. However, it is well-known that this paradigm suffers from
robustness issues that hinder its applicability in safety-critical domains.
These issues are often addressed by training against worst-case perturbations
of data, a technique known as adversarial training. Although empirically
effective, adversarial training can be overly conservative, leading to
unfavorable trade-offs between nominal performance and robustness. To this end,
in this paper we propose a framework called probabilistic robustness that
bridges the gap between the accurate, yet brittle average case and the robust,
yet conservative worst case by enforcing robustness to most rather than to all
perturbations. From a theoretical point of view, this framework overcomes the
trade-offs between the performance and the sample-complexity of worst-case and
average-case learning. From a practical point of view, we propose a novel
algorithm based on risk-aware optimization that effectively balances average-
and worst-case performance at a considerably lower computational cost relative
to adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate
the advantages of this framework on the spectrum from average- to worst-case
robustness.",2202.01136v3,https://arxiv.org/pdf/2202.01136v3
Robust Training of Neural Networks Using Scale Invariant Architectures,"Zhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer, Sashank J. Reddi, Sanjiv Kumar","In contrast to SGD, adaptive gradient methods like Adam allow robust training
of modern deep networks, especially large language models. However, the use of
adaptivity not only comes at the cost of extra memory but also raises the
fundamental question: can non-adaptive methods like SGD enjoy similar benefits?
In this paper, we provide an affirmative answer to this question by proposing
to achieve both robust and memory-efficient training via the following general
recipe: (1) modify the architecture and make it scale invariant, i.e. the scale
of parameter doesn't affect the output of the network, (2) train with SGD and
weight decay, and optionally (3) clip the global gradient norm proportional to
weight norm multiplied by $\sqrt{\tfrac{2\lambda}{\eta}}$, where $\eta$ is
learning rate and $\lambda$ is weight decay. We show that this general approach
is robust to rescaling of parameter and loss by proving that its convergence
only depends logarithmically on the scale of initialization and loss, whereas
the standard SGD might not even converge for many initializations. Following
our recipe, we design a scale invariant version of BERT, called SIBERT, which
when trained simply by vanilla SGD achieves performance comparable to BERT
trained by adaptive methods like Adam on downstream tasks.",2202.00980v2,https://arxiv.org/pdf/2202.00980v2
"Understanding The Robustness of Self-supervised Learning Through Topic
  Modeling","Zeping Luo, Shiyou Wu, Cindy Weng, Mo Zhou, Rong Ge","Self-supervised learning has significantly improved the performance of many
NLP tasks. However, how can self-supervised learning discover useful
representations, and why is it better than traditional approaches such as
probabilistic models are still largely unknown. In this paper, we focus on the
context of topic modeling and highlight a key advantage of self-supervised
learning - when applied to data generated by topic models, self-supervised
learning can be oblivious to the specific model, and hence is less susceptible
to model misspecification. In particular, we prove that commonly used
self-supervised objectives based on reconstruction or contrastive samples can
both recover useful posterior information for general topic models.
Empirically, we show that the same objectives can perform on par with posterior
inference using the correct model, while outperforming posterior inference
using misspecified models.",2203.03539v2,https://arxiv.org/pdf/2203.03539v2
"Finding Biological Plausibility for Adversarially Robust Features via
  Metameric Tasks","Anne Harrington, Arturo Deza","Recent work suggests that representations learned by adversarially robust
networks are more human perceptually-aligned than non-robust networks via image
manipulations. Despite appearing closer to human visual perception, it is
unclear if the constraints in robust DNN representations match biological
constraints found in human vision. Human vision seems to rely on
texture-based/summary statistic representations in the periphery, which have
been shown to explain phenomena such as crowding and performance on visual
search tasks. To understand how adversarially robust
optimizations/representations compare to human vision, we performed a
psychophysics experiment using a set of metameric discrimination tasks where we
evaluated how well human observers could distinguish between images synthesized
to match adversarially robust representations compared to non-robust
representations and a texture synthesis model of peripheral vision (Texforms).
We found that the discriminability of robust representation and texture model
images decreased to near chance performance as stimuli were presented farther
in the periphery. Moreover, performance on robust and texture-model images
showed similar trends within participants, while performance on non-robust
representations changed minimally across the visual field. These results
together suggest that (1) adversarially robust representations capture
peripheral computation better than non-robust representations and (2) robust
representations capture peripheral computation similar to current
state-of-the-art texture peripheral vision models. More broadly, our findings
support the idea that localized texture summary statistic representations may
drive human invariance to adversarial perturbations and that the incorporation
of such representations in DNNs could give rise to useful properties like
adversarial robustness.",2202.00838v2,https://arxiv.org/pdf/2202.00838v2
"Deep Learning for Ultrasound Speed-of-Sound Reconstruction: Impacts of
  Training Data Diversity on Stability and Robustness","Farnaz Khun Jush, Markus Biele, Peter M. Dueppenbecker, Andreas Maier","Ultrasound b-mode imaging is a qualitative approach and diagnostic quality
strongly depends on operators' training and experience. Quantitative approaches
can provide information about tissue properties; therefore, can be used for
identifying various tissue types, e.g., speed-of-sound in the tissue can be
used as a biomarker for tissue malignancy, especially in breast imaging. Recent
studies showed the possibility of speed-of-sound reconstruction using deep
neural networks that are fully trained on simulated data. However, because of
the ever-present domain shift between simulated and measured data, the
stability and performance of these models in real setups are still under
debate. In prior works, for training data generation, tissue structures were
modeled as simplified geometrical structures which does not reflect the
complexity of the real tissues. In this study, we proposed a new simulation
setup for training data generation based on Tomosynthesis images. We combined
our approach with the simplified geometrical model and investigated the impacts
of training data diversity on the stability and robustness of an existing
network architecture. We studied the sensitivity of the trained network to
different simulation parameters, e.g., echogenicity, number of scatterers,
noise, and geometry. We showed that the network trained with the joint set of
data is more stable on out-of-domain simulated data as well as measured phantom
data.",2202.01208v2,https://arxiv.org/pdf/2202.01208v2
"Improving deep neural network generalization and robustness to
  background bias via layer-wise relevance propagation optimization","Pedro R. A. S. Bassi, Sergio S. J. Dertkigil, Andrea Cavalli","Features in images' backgrounds can spuriously correlate with the images'
classes, representing background bias. They can influence the classifier's
decisions, causing shortcut learning (Clever Hans effect). The phenomenon
generates deep neural networks (DNNs) that perform well on standard evaluation
datasets but generalize poorly to real-world data. Layer-wise Relevance
Propagation (LRP) explains DNNs' decisions. Here, we show that the optimization
of LRP heatmaps can minimize the background bias influence on deep classifiers,
hindering shortcut learning. By not increasing run-time computational cost, the
approach is light and fast. Furthermore, it applies to virtually any
classification architecture. After injecting synthetic bias in images'
backgrounds, we compared our approach (dubbed ISNet) to eight state-of-the-art
DNNs, quantitatively demonstrating its superior robustness to background bias.
Mixed datasets are common for COVID-19 and tuberculosis classification with
chest X-rays, fostering background bias. By focusing on the lungs, the ISNet
reduced shortcut learning. Thus, its generalization performance on external
(out-of-distribution) test databases significantly surpassed all implemented
benchmark models.",2202.00232v7,https://arxiv.org/pdf/2202.00232v7
"Studying the Robustness of Anti-adversarial Federated Learning Models
  Detecting Cyberattacks in IoT Spectrum Sensors","Pedro Miguel Sánchez Sánchez, Alberto Huertas Celdrán, Timo Schenk, Adrian Lars Benjamin Iten, Gérôme Bovet, Gregorio Martínez Pérez, Burkhard Stiller","Device fingerprinting combined with Machine and Deep Learning (ML/DL) report
promising performance when detecting cyberattacks targeting data managed by
resource-constrained spectrum sensors. However, the amount of data needed to
train models and the privacy concerns of such scenarios limit the applicability
of centralized ML/DL-based approaches. Federated learning (FL) addresses these
limitations by creating federated and privacy-preserving models. However, FL is
vulnerable to malicious participants, and the impact of adversarial attacks on
federated models detecting spectrum sensing data falsification (SSDF) attacks
on spectrum sensors has not been studied. To address this challenge, the first
contribution of this work is the creation of a novel dataset suitable for FL
and modeling the behavior (usage of CPU, memory, or file system, among others)
of resource-constrained spectrum sensors affected by different SSDF attacks.
The second contribution is a pool of experiments analyzing and comparing the
robustness of federated models according to i) three families of spectrum
sensors, ii) eight SSDF attacks, iii) four scenarios dealing with unsupervised
(anomaly detection) and supervised (binary classification) federated models,
iv) up to 33% of malicious participants implementing data and model poisoning
attacks, and v) four aggregation functions acting as anti-adversarial
mechanisms to increase the models robustness.",2202.00137v1,https://arxiv.org/pdf/2202.00137v1
Robust supervised learning with coordinate gradient descent,"Stéphane Gaïffas, Ibrahim Merad","This paper considers the problem of supervised learning with linear methods
when both features and labels can be corrupted, either in the form of heavy
tailed data and/or corrupted rows. We introduce a combination of coordinate
gradient descent as a learning algorithm together with robust estimators of the
partial derivatives. This leads to robust statistical learning methods that
have a numerical complexity nearly identical to non-robust ones based on
empirical risk minimization. The main idea is simple: while robust learning
with gradient descent requires the computational cost of robustly estimating
the whole gradient to update all parameters, a parameter can be updated
immediately using a robust estimator of a single partial derivative in
coordinate gradient descent. We prove upper bounds on the generalization error
of the algorithms derived from this idea, that control both the optimization
and statistical errors with and without a strong convexity assumption of the
risk. Finally, we propose an efficient implementation of this approach in a new
python library called linlearn, and demonstrate through extensive numerical
experiments that our approach introduces a new interesting compromise between
robustness, statistical performance and numerical efficiency for this problem.",2201.13372v1,https://arxiv.org/pdf/2201.13372v1
Can Adversarial Training Be Manipulated By Non-Robust Features?,"Lue Tao, Lei Feng, Hongxin Wei, Jinfeng Yi, Sheng-Jun Huang, Songcan Chen","Adversarial training, originally designed to resist test-time adversarial
examples, has shown to be promising in mitigating training-time availability
attacks. This defense ability, however, is challenged in this paper. We
identify a novel threat model named stability attack, which aims to hinder
robust availability by slightly manipulating the training data. Under this
threat, we show that adversarial training using a conventional defense budget
$\epsilon$ provably fails to provide test robustness in a simple statistical
setting, where the non-robust features of the training data can be reinforced
by $\epsilon$-bounded perturbation. Further, we analyze the necessity of
enlarging the defense budget to counter stability attacks. Finally,
comprehensive experiments demonstrate that stability attacks are harmful on
benchmark datasets, and thus the adaptive defense is necessary to maintain
robustness. Our code is available at
https://github.com/TLMichael/Hypocritical-Perturbation.",2201.13329v4,https://arxiv.org/pdf/2201.13329v4
Adversarial Robustness in Deep Learning: Attacks on Fragile Neurons,"Chandresh Pravin, Ivan Martino, Giuseppe Nicosia, Varun Ojha","We identify fragile and robust neurons of deep learning architectures using
nodal dropouts of the first convolutional layer. Using an adversarial targeting
algorithm, we correlate these neurons with the distribution of adversarial
attacks on the network. Adversarial robustness of neural networks has gained
significant attention in recent times and highlights intrinsic weaknesses of
deep learning networks against carefully constructed distortion applied to
input images. In this paper, we evaluate the robustness of state-of-the-art
image classification models trained on the MNIST and CIFAR10 datasets against
the fast gradient sign method attack, a simple yet effective method of
deceiving neural networks. Our method identifies the specific neurons of a
network that are most affected by the adversarial attack being applied. We,
therefore, propose to make fragile neurons more robust against these attacks by
compressing features within robust neurons and amplifying the fragile neurons
proportionally.",2201.12347v1,https://arxiv.org/pdf/2201.12347v1
Deletion Robust Submodular Maximization over Matroids,"Paul Dütting, Federico Fusco, Silvio Lattanzi, Ashkan Norouzi-Fard, Morteza Zadimoghaddam","Maximizing a monotone submodular function is a fundamental task in machine
learning. In this paper, we study the deletion robust version of the problem
under the classic matroids constraint. Here the goal is to extract a small size
summary of the dataset that contains a high value independent set even after an
adversary deleted some elements. We present constant-factor approximation
algorithms, whose space complexity depends on the rank $k$ of the matroid and
the number $d$ of deleted elements. In the centralized setting we present a
$(3.582+O(\varepsilon))$-approximation algorithm with summary size $O(k +
\frac{d \log k}{\varepsilon^2})$. In the streaming setting we provide a
$(5.582+O(\varepsilon))$-approximation algorithm with summary size and memory
$O(k + \frac{d \log k}{\varepsilon^2})$. We complement our theoretical results
with an in-depth experimental analysis showing the effectiveness of our
algorithms on real-world datasets.",2201.13128v1,https://arxiv.org/pdf/2201.13128v1
"Bayesian Optimization for Distributionally Robust Chance-constrained
  Problem","Yu Inatsu, Shion Takeno, Masayuki Karasuyama, Ichiro Takeuchi","In black-box function optimization, we need to consider not only controllable
design variables but also uncontrollable stochastic environment variables. In
such cases, it is necessary to solve the optimization problem by taking into
account the uncertainty of the environmental variables. Chance-constrained (CC)
problem, the problem of maximizing the expected value under a certain level of
constraint satisfaction probability, is one of the practically important
problems in the presence of environmental variables. In this study, we consider
distributionally robust CC (DRCC) problem and propose a novel DRCC Bayesian
optimization method for the case where the distribution of the environmental
variables cannot be precisely specified. We show that the proposed method can
find an arbitrary accurate solution with high probability in a finite number of
trials, and confirm the usefulness of the proposed method through numerical
experiments.",2201.13112v2,https://arxiv.org/pdf/2201.13112v2
"Learning Robust Representation through Graph Adversarial Contrastive
  Learning","Jiayan Guo, Shangyang Li, Yue Zhao, Yan Zhang","Existing studies show that node representations generated by graph neural
networks (GNNs) are vulnerable to adversarial attacks, such as unnoticeable
perturbations of adjacent matrix and node features. Thus, it is requisite to
learn robust representations in graph neural networks. To improve the
robustness of graph representation learning, we propose a novel Graph
Adversarial Contrastive Learning framework (GraphACL) by introducing
adversarial augmentations into graph self-supervised learning. In this
framework, we maximize the mutual information between local and global
representations of a perturbed graph and its adversarial augmentations, where
the adversarial graphs can be generated in either supervised or unsupervised
approaches. Based on the Information Bottleneck Principle, we theoretically
prove that our method could obtain a much tighter bound, thus improving the
robustness of graph representation learning. Empirically, we evaluate several
methods on a range of node classification benchmarks and the results
demonstrate GraphACL could achieve comparable accuracy over previous supervised
methods.",2201.13025v1,https://arxiv.org/pdf/2201.13025v1
On the Robustness of Quality Measures for GANs,"Motasem Alfarra, Juan C. Pérez, Anna Frühstück, Philip H. S. Torr, Peter Wonka, Bernard Ghanem","This work evaluates the robustness of quality measures of generative models
such as Inception Score (IS) and Fr\'echet Inception Distance (FID). Analogous
to the vulnerability of deep models against a variety of adversarial attacks,
we show that such metrics can also be manipulated by additive pixel
perturbations. Our experiments indicate that one can generate a distribution of
images with very high scores but low perceptual quality. Conversely, one can
optimize for small imperceptible perturbations that, when added to real world
images, deteriorate their scores. We further extend our evaluation to
generative models themselves, including the state of the art network
StyleGANv2. We show the vulnerability of both the generative model and the FID
against additive perturbations in the latent space. Finally, we show that the
FID can be robustified by simply replacing the standard Inception with a robust
Inception. We validate the effectiveness of the robustified metric through
extensive experiments, showing it is more robust against manipulation.",2201.13019v2,https://arxiv.org/pdf/2201.13019v2
The Geometry of Robust Value Functions,"Kaixin Wang, Navdeep Kumar, Kuangqi Zhou, Bryan Hooi, Jiashi Feng, Shie Mannor","The space of value functions is a fundamental concept in reinforcement
learning. Characterizing its geometric properties may provide insights for
optimization and representation. Existing works mainly focus on the value space
for Markov Decision Processes (MDPs). In this paper, we study the geometry of
the robust value space for the more general Robust MDPs (RMDPs) setting, where
transition uncertainties are considered. Specifically, since we find it hard to
directly adapt prior approaches to RMDPs, we start with revisiting the
non-robust case, and introduce a new perspective that enables us to
characterize both the non-robust and robust value space in a similar fashion.
The key of this perspective is to decompose the value space, in a state-wise
manner, into unions of hypersurfaces. Through our analysis, we show that the
robust value space is determined by a set of conic hypersurfaces, each of which
contains the robust values of all policies that agree on one state.
Furthermore, we find that taking only extreme points in the uncertainty set is
sufficient to determine the robust value space. Finally, we discuss some other
aspects about the robust value space, including its non-convexity and policy
agreement on multiple states.",2201.12929v2,https://arxiv.org/pdf/2201.12929v2
"GARNET: Reduced-Rank Topology Learning for Robust and Scalable Graph
  Neural Networks","Chenhui Deng, Xiuyu Li, Zhuo Feng, Zhiru Zhang","Graph neural networks (GNNs) have been increasingly deployed in various
applications that involve learning on non-Euclidean data. However, recent
studies show that GNNs are vulnerable to graph adversarial attacks. Although
there are several defense methods to improve GNN robustness by eliminating
adversarial components, they may also impair the underlying clean graph
structure that contributes to GNN training. In addition, few of those defense
models can scale to large graphs due to their high computational complexity and
memory usage. In this paper, we propose GARNET, a scalable spectral method to
boost the adversarial robustness of GNN models. GARNET first leverages weighted
spectral embedding to construct a base graph, which is not only resistant to
adversarial attacks but also contains critical (clean) graph structure for GNN
training. Next, GARNET further refines the base graph by pruning additional
uncritical edges based on probabilistic graphical model. GARNET has been
evaluated on various datasets, including a large graph with millions of nodes.
Our extensive experiment results show that GARNET achieves adversarial accuracy
improvement and runtime speedup over state-of-the-art GNN (defense) models by
up to 13.27% and 14.7x, respectively.",2201.12741v6,https://arxiv.org/pdf/2201.12741v6
Towards Robust Deep Active Learning for Scientific Computing,"Simiao Ren, Yang Deng, Willie J. Padilla, Jordan Malof","Deep learning (DL) is revolutionizing the scientific computing community. To
reduce the data gap, active learning has been identified as a promising
solution for DL in the scientific computing community. However, the deep active
learning (DAL) literature is dominated by image classification problems and
pool-based methods. Here we investigate the robustness of pool-based DAL
methods for scientific computing problems (dominated by regression) where DNNs
are increasingly used. We show that modern pool-based DAL methods all share an
untunable hyperparameter, termed the pool ratio, denoted $\gamma$, which is
often assumed to be known apriori in the literature. We evaluate the
performance of five state-of-the-art DAL methods on six benchmark problems if
we assume $\gamma$ is \textit{not} known - a more realistic assumption for
scientific computing problems. Our results indicate that this reduces the
performance of modern DAL methods and that they sometimes can even perform
worse than random sampling, creating significant uncertainty when used in
real-world settings. To overcome this limitation we propose, to our knowledge,
the first query synthesis DAL method for regression, termed NA-QBC. NA-QBC
removes the sensitive $\gamma$ hyperparameter and we find that, on average, it
outperforms the other DAL methods on our benchmark problems. Crucially, NA-QBC
always outperforms random sampling, providing more robust performance benefits.",2201.12632v2,https://arxiv.org/pdf/2201.12632v2
Robust Imitation Learning from Corrupted Demonstrations,"Liu Liu, Ziyang Tang, Lanqing Li, Dijun Luo","We consider offline Imitation Learning from corrupted demonstrations where a
constant fraction of data can be noise or even arbitrary outliers. Classical
approaches such as Behavior Cloning assumes that demonstrations are collected
by an presumably optimal expert, hence may fail drastically when learning from
corrupted demonstrations. We propose a novel robust algorithm by minimizing a
Median-of-Means (MOM) objective which guarantees the accurate estimation of
policy, even in the presence of constant fraction of outliers. Our theoretical
analysis shows that our robust method in the corrupted setting enjoys nearly
the same error scaling and sample complexity guarantees as the classical
Behavior Cloning in the expert demonstration setting. Our experiments on
continuous-control benchmarks validate that our method exhibits the predicted
robustness and effectiveness, and achieves competitive results compared to
existing imitation learning methods.",2201.12594v1,https://arxiv.org/pdf/2201.12594v1
"Investigating Why Contrastive Learning Benefits Robustness Against Label
  Noise","Yihao Xue, Kyle Whitecross, Baharan Mirzasoleiman","Self-supervised Contrastive Learning (CL) has been recently shown to be very
effective in preventing deep networks from overfitting noisy labels. Despite
its empirical success, the theoretical understanding of the effect of
contrastive learning on boosting robustness is very limited. In this work, we
rigorously prove that the representation matrix learned by contrastive learning
boosts robustness, by having: (i) one prominent singular value corresponding to
each sub-class in the data, and significantly smaller remaining singular
values; and (ii) {a large alignment between the prominent singular vectors and
the clean labels of each sub-class. The above properties enable a linear layer
trained on such representations to effectively learn the clean labels without
overfitting the noise.} We further show that the low-rank structure of the
Jacobian of deep networks pre-trained with contrastive learning allows them to
achieve a superior performance initially, when fine-tuned on noisy labels.
Finally, we demonstrate that the initial robustness provided by contrastive
learning enables robust training methods to achieve state-of-the-art
performance under extreme noise levels, e.g., an average of 27.18\% and 15.58\%
increase in accuracy on CIFAR-10 and CIFAR-100 with 80\% symmetric noisy
labels, and 4.11\% increase in accuracy on WebVision.",2201.12498v4,https://arxiv.org/pdf/2201.12498v4
"REET: Robustness Evaluation and Enhancement Toolbox for Computational
  Pathology","Alex Foote, Amina Asif, Nasir Rajpoot, Fayyaz Minhas","Motivation: Digitization of pathology laboratories through digital slide
scanners and advances in deep learning approaches for objective histological
assessment have resulted in rapid progress in the field of computational
pathology (CPath) with wide-ranging applications in medical and pharmaceutical
research as well as clinical workflows. However, the estimation of robustness
of CPath models to variations in input images is an open problem with a
significant impact on the down-stream practical applicability, deployment and
acceptability of these approaches. Furthermore, development of domain-specific
strategies for enhancement of robustness of such models is of prime importance
as well.
  Implementation and Availability: In this work, we propose the first
domain-specific Robustness Evaluation and Enhancement Toolbox (REET) for
computational pathology applications. It provides a suite of algorithmic
strategies for enabling robustness assessment of predictive models with respect
to specialized image transformations such as staining, compression, focusing,
blurring, changes in spatial resolution, brightness variations, geometric
changes as well as pixel-level adversarial perturbations. Furthermore, REET
also enables efficient and robust training of deep learning pipelines in
computational pathology. REET is implemented in Python and is available at the
following URL: https://github.com/alexjfoote/reetoolbox.
  Contact: Fayyaz.minhas@warwick.ac.uk",2201.12311v1,https://arxiv.org/pdf/2201.12311v1
"Benchmarking Robustness of 3D Point Cloud Recognition Against Common
  Corruptions","Jiachen Sun, Qingzhao Zhang, Bhavya Kailkhura, Zhiding Yu, Chaowei Xiao, Z. Morley Mao","Deep neural networks on 3D point cloud data have been widely used in the real
world, especially in safety-critical applications. However, their robustness
against corruptions is less studied. In this paper, we present ModelNet40-C,
the first comprehensive benchmark on 3D point cloud corruption robustness,
consisting of 15 common and realistic corruptions. Our evaluation shows a
significant gap between the performances on ModelNet40 and ModelNet40-C for
state-of-the-art (SOTA) models. To reduce the gap, we propose a simple but
effective method by combining PointCutMix-R and TENT after evaluating a wide
range of augmentation and test-time adaptation strategies. We identify a number
of critical insights for future studies on corruption robustness in point cloud
recognition. For instance, we unveil that Transformer-based architectures with
proper training recipes achieve the strongest robustness. We hope our in-depth
analysis will motivate the development of robust training strategies or
architecture designs in the 3D point cloud domain. Our codebase and dataset are
included in https://github.com/jiachens/ModelNet40-C",2201.12296v1,https://arxiv.org/pdf/2201.12296v1
Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks,"Lukas Struppek, Dominik Hintersdorf, Antonio De Almeida Correia, Antonia Adler, Kristian Kersting","Model inversion attacks (MIAs) aim to create synthetic images that reflect
the class-wise characteristics from a target classifier's private training data
by exploiting the model's learned knowledge. Previous research has developed
generative MIAs that use generative adversarial networks (GANs) as image priors
tailored to a specific target model. This makes the attacks time- and
resource-consuming, inflexible, and susceptible to distributional shifts
between datasets. To overcome these drawbacks, we present Plug & Play Attacks,
which relax the dependency between the target model and image prior, and enable
the use of a single GAN to attack a wide range of targets, requiring only minor
adjustments to the attack. Moreover, we show that powerful MIAs are possible
even with publicly available pre-trained GANs and under strong distributional
shifts, for which previous approaches fail to produce meaningful results. Our
extensive evaluation confirms the improved robustness and flexibility of Plug &
Play Attacks and their ability to create high-quality images revealing
sensitive class characteristics.",2201.12179v4,https://arxiv.org/pdf/2201.12179v4
"A Robust and Flexible EM Algorithm for Mixtures of Elliptical
  Distributions with Missing Data","Florian Mouret, Alexandre Hippert-Ferrer, Frédéric Pascal, Jean-Yves Tourneret","This paper tackles the problem of missing data imputation for noisy and
non-Gaussian data. A classical imputation method, the Expectation Maximization
(EM) algorithm for Gaussian mixture models, has shown interesting properties
when compared to other popular approaches such as those based on k-nearest
neighbors or on multiple imputations by chained equations. However, Gaussian
mixture models are known to be non-robust to heterogeneous data, which can lead
to poor estimation performance when the data is contaminated by outliers or
follows non-Gaussian distributions. To overcome this issue, a new EM algorithm
is investigated for mixtures of elliptical distributions with the property of
handling potential missing data. This paper shows that this problem reduces to
the estimation of a mixture of Angular Gaussian distributions under generic
assumptions (i.e., each sample is drawn from a mixture of elliptical
distributions, which is possibly different for one sample to another). In that
case, the complete-data likelihood associated with mixtures of elliptical
distributions is well adapted to the EM framework with missing data thanks to
its conditional distribution, which is shown to be a multivariate
$t$-distribution. Experimental results on synthetic data demonstrate that the
proposed algorithm is robust to outliers and can be used with non-Gaussian
data. Furthermore, experiments conducted on real-world datasets show that this
algorithm is very competitive when compared to other classical imputation
methods.",2201.12020v4,https://arxiv.org/pdf/2201.12020v4
Robust Augmentation for Multivariate Time Series Classification,"Hong Yang, Travis Desell","Neural networks are capable of learning powerful representations of data, but
they are susceptible to overfitting due to the number of parameters. This is
particularly challenging in the domain of time series classification, where
datasets may contain fewer than 100 training examples. In this paper, we show
that the simple methods of cutout, cutmix, mixup, and window warp improve the
robustness and overall performance in a statistically significant way for
convolutional, recurrent, and self-attention based architectures for time
series classification. We evaluate these methods on 26 datasets from the
University of East Anglia Multivariate Time Series Classification (UEA MTSC)
archive and analyze how these methods perform on different types of time series
data.. We show that the InceptionTime network with augmentation improves
accuracy by 1% to 45% in 18 different datasets compared to without
augmentation. We also show that augmentation improves accuracy for recurrent
and self attention based architectures.",2201.11739v1,https://arxiv.org/pdf/2201.11739v1
How Robust are Discriminatively Trained Zero-Shot Learning Models?,"Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, Pinar Duygulu","Data shift robustness has been primarily investigated from a fully supervised
perspective, and robustness of zero-shot learning (ZSL) models have been
largely neglected. In this paper, we present novel analyses on the robustness
of discriminative ZSL to image corruptions. We subject several ZSL models to a
large set of common corruptions and defenses. In order to realize the
corruption analysis, we curate and release the first ZSL corruption robustness
datasets SUN-C, CUB-C and AWA2-C. We analyse our results by taking into account
the dataset characteristics, class imbalance, class transitions between seen
and unseen classes and the discrepancies between ZSL and GZSL performances. Our
results show that discriminative ZSL suffers from corruptions and this trend is
further exacerbated by the severe class imbalance and model weakness inherent
in ZSL methods. We then combine our findings with those based on adversarial
attacks in ZSL, and highlight the different effects of corruptions and
adversarial examples, such as the pseudo-robustness effect present under
adversarial attacks. We also obtain new strong baselines for both models with
the defense methods. Finally, our experiments show that although existing
methods to improve robustness somewhat work for ZSL models, they do not produce
a tangible effect.",2201.10972v2,https://arxiv.org/pdf/2201.10972v2
"Improving robustness and calibration in ensembles with diversity
  regularization","Hendrik Alexander Mehrtens, Camila González, Anirban Mukhopadhyay","Calibration and uncertainty estimation are crucial topics in high-risk
environments. We introduce a new diversity regularizer for classification tasks
that uses out-of-distribution samples and increases the overall accuracy,
calibration and out-of-distribution detection capabilities of ensembles.
Following the recent interest in the diversity of ensembles, we systematically
evaluate the viability of explicitly regularizing ensemble diversity to improve
calibration on in-distribution data as well as under dataset shift. We
demonstrate that diversity regularization is highly beneficial in
architectures, where weights are partially shared between the individual
members and even allows to use fewer ensemble members to reach the same level
of robustness. Experiments on CIFAR-10, CIFAR-100, and SVHN show that
regularizing diversity can have a significant impact on calibration and
robustness, as well as out-of-distribution detection.",2201.10908v1,https://arxiv.org/pdf/2201.10908v1
An Efficient and Robust System for Vertically Federated Random Forest,"Houpu Yao, Jiazhou Wang, Peng Dai, Liefeng Bo, Yanqing Chen","As there is a growing interest in utilizing data across multiple resources to
build better machine learning models, many vertically federated learning
algorithms have been proposed to preserve the data privacy of the participating
organizations. However, the efficiency of existing vertically federated
learning algorithms remains to be a big problem, especially when applied to
large-scale real-world datasets. In this paper, we present a fast, accurate,
scalable and yet robust system for vertically federated random forest. With
extensive optimization, we achieved $5\times$ and $83\times$ speed up over the
SOTA SecureBoost model \cite{cheng2019secureboost} for training and serving
tasks. Moreover, the proposed system can achieve similar accuracy but with
favorable scalability and partition tolerance. Our code has been made public to
facilitate the development of the community and the protection of user data
privacy.",2201.10761v1,https://arxiv.org/pdf/2201.10761v1
PaRT: Parallel Learning Towards Robust and Transparent AI,"Mahsa Paknezhad, Hamsawardhini Rengarajan, Chenghao Yuan, Sujanya Suresh, Manas Gupta, Savitha Ramasamy, Hwee Kuan Lee","This paper takes a parallel learning approach for robust and transparent AI.
A deep neural network is trained in parallel on multiple tasks, where each task
is trained only on a subset of the network resources. Each subset consists of
network segments, that can be combined and shared across specific tasks. Tasks
can share resources with other tasks, while having independent task-related
network resources. Therefore, the trained network can share similar
representations across various tasks, while also enabling independent
task-related representations. The above allows for some crucial outcomes. (1)
The parallel nature of our approach negates the issue of catastrophic
forgetting. (2) The sharing of segments uses network resources more
efficiently. (3) We show that the network does indeed use learned knowledge
from some tasks in other tasks, through shared representations. (4) Through
examination of individual task-related and shared representations, the model
offers transparency in the network and in the relationships across tasks in a
multi-task setting. Evaluation of the proposed approach against complex
competing approaches such as Continual Learning, Neural Architecture Search,
and Multi-task learning shows that it is capable of learning robust
representations. This is the first effort to train a DL model on multiple tasks
in parallel. Our code is available at https://github.com/MahsaPaknezhad/PaRT",2201.09534v2,https://arxiv.org/pdf/2201.09534v2
Efficient and Robust Classification for Sparse Attacks,"Mark Beliaev, Payam Delgosha, Hamed Hassani, Ramtin Pedarsani","In the past two decades we have seen the popularity of neural networks
increase in conjunction with their classification accuracy. Parallel to this,
we have also witnessed how fragile the very same prediction models are: tiny
perturbations to the inputs can cause misclassification errors throughout
entire datasets. In this paper, we consider perturbations bounded by the
$\ell_0$--norm, which have been shown as effective attacks in the domains of
image-recognition, natural language processing, and malware-detection. To this
end, we propose a novel defense method that consists of ""truncation"" and
""adversarial training"". We then theoretically study the Gaussian mixture
setting and prove the asymptotic optimality of our proposed classifier.
Motivated by the insights we obtain, we extend these components to neural
network classifiers. We conduct numerical experiments in the domain of computer
vision using the MNIST and CIFAR datasets, demonstrating significant
improvement for the robust classification error of neural networks.",2201.09369v1,https://arxiv.org/pdf/2201.09369v1
"Implicit Bias of Projected Subgradient Method Gives Provable Robust
  Recovery of Subspaces of Unknown Codimension","Paris V. Giampouras, Benjamin D. Haeffele, René Vidal","Robust subspace recovery (RSR) is a fundamental problem in robust
representation learning. Here we focus on a recently proposed RSR method termed
Dual Principal Component Pursuit (DPCP) approach, which aims to recover a basis
of the orthogonal complement of the subspace and is amenable to handling
subspaces of high relative dimension. Prior work has shown that DPCP can
provably recover the correct subspace in the presence of outliers, as long as
the true dimension of the subspace is known. We show that DPCP can provably
solve RSR problems in the {\it unknown} subspace dimension regime, as long as
orthogonality constraints -- adopted in previous DPCP formulations -- are
relaxed and random initialization is used instead of spectral one. Namely, we
propose a very simple algorithm based on running multiple instances of a
projected sub-gradient descent method (PSGM), with each problem instance
seeking to find one vector in the null space of the subspace. We theoretically
prove that under mild conditions this approach will succeed with high
probability. In particular, we show that 1) all of the problem instances will
converge to a vector in the nullspace of the subspace and 2) the ensemble of
problem instance solutions will be sufficiently diverse to fully span the
nullspace of the subspace thus also revealing its true unknown codimension. We
provide empirical results that corroborate our theoretical results and showcase
the remarkable implicit rank regularization behavior of PSGM algorithm that
allows us to perform RSR without being aware of the subspace dimension.",2201.09079v1,https://arxiv.org/pdf/2201.09079v1
"On the Robustness of Sparse Counterfactual Explanations to Adverse
  Perturbations","Marco Virgolin, Saverio Fracaros","Counterfactual explanations (CEs) are a powerful means for understanding how
decisions made by algorithms can be changed. Researchers have proposed a number
of desiderata that CEs should meet to be practically useful, such as requiring
minimal effort to enact, or complying with causal models. We consider a further
aspect to improve the usability of CEs: robustness to adverse perturbations,
which may naturally happen due to unfortunate circumstances. Since CEs
typically prescribe a sparse form of intervention (i.e., only a subset of the
features should be changed), we study the effect of addressing robustness
separately for the features that are recommended to be changed and those that
are not. Our definitions are workable in that they can be incorporated as
penalty terms in the loss functions that are used for discovering CEs. To
experiment with robustness, we create and release code where five data sets
(commonly used in the field of fair and explainable machine learning) have been
enriched with feature-specific annotations that can be used to sample
meaningful perturbations. Our experiments show that CEs are often not robust
and, if adverse perturbations take place (even if not worst-case), the
intervention they prescribe may require a much larger cost than anticipated, or
even become impossible. However, accounting for robustness in the search
process, which can be done rather easily, allows discovering robust CEs
systematically. Robust CEs make additional intervention to contrast
perturbations much less costly than non-robust CEs. We also find that
robustness is easier to achieve for the features to change, posing an important
point of consideration for the choice of what counterfactual explanation is
best for the user. Our code is available at:
https://github.com/marcovirgolin/robust-counterfactuals.",2201.09051v3,https://arxiv.org/pdf/2201.09051v3
"Uncertainty-aware deep learning methods for robust diabetic retinopathy
  classification","Joel Jaskari, Jaakko Sahlsten, Theodoros Damoulas, Jeremias Knoblauch, Simo Särkkä, Leo Kärkkäinen, Kustaa Hietala, Kimmo Kaski","Automatic classification of diabetic retinopathy from retinal images has been
widely studied using deep neural networks with impressive results. However,
there is a clinical need for estimation of the uncertainty in the
classifications, a shortcoming of modern neural networks. Recently, approximate
Bayesian deep learning methods have been proposed for the task but the studies
have only considered the binary referable/non-referable diabetic retinopathy
classification applied to benchmark datasets. We present novel results by
systematically investigating a clinical dataset and a clinically relevant
5-class classification scheme, in addition to benchmark datasets and the binary
classification scheme. Moreover, we derive a connection between uncertainty
measures and classifier risk, from which we develop a new uncertainty measure.
We observe that the previously proposed entropy-based uncertainty measure
generalizes to the clinical dataset on the binary classification scheme but not
on the 5-class scheme, whereas our new uncertainty measure generalizes to the
latter case.",2201.09042v2,https://arxiv.org/pdf/2201.09042v2
"PiCO+: Contrastive Label Disambiguation for Robust Partial Label
  Learning","Haobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, Junbo Zhao","Partial label learning (PLL) is an important problem that allows each
training example to be labeled with a coarse candidate set, which well suits
many real-world data annotation scenarios with label ambiguity. Despite the
promise, the performance of PLL often lags behind the supervised counterpart.
In this work, we bridge the gap by addressing two key research challenges in
PLL -- representation learning and label disambiguation -- in one coherent
framework. Specifically, our proposed framework PiCO consists of a contrastive
learning module along with a novel class prototype-based label disambiguation
algorithm. PiCO produces closely aligned representations for examples from the
same classes and facilitates label disambiguation. Theoretically, we show that
these two components are mutually beneficial, and can be rigorously justified
from an expectation-maximization (EM) algorithm perspective. Moreover, we study
a challenging yet practical noisy partial label learning setup, where the
ground-truth may not be included in the candidate set. To remedy this problem,
we present an extension PiCO+ that performs distance-based clean sample
selection and learns robust classifiers by a semi-supervised contrastive
learning algorithm. Extensive experiments demonstrate that our proposed methods
significantly outperform the current state-of-the-art approaches in standard
and noisy PLL tasks and even achieve comparable results to fully supervised
learning.",2201.08984v3,https://arxiv.org/pdf/2201.08984v3
Deep Q-learning: a robust control approach,"Balazs Varga, Balazs Kulcsar, Morteza Haghir Chehreghani","In this paper, we place deep Q-learning into a control-oriented perspective
and study its learning dynamics with well-established techniques from robust
control. We formulate an uncertain linear time-invariant model by means of the
neural tangent kernel to describe learning. We show the instability of learning
and analyze the agent's behavior in frequency-domain. Then, we ensure
convergence via robust controllers acting as dynamical rewards in the loss
function. We synthesize three controllers: state-feedback gain scheduling H2,
dynamic Hinf, and constant gain Hinf controllers. Setting up the learning agent
with a control-oriented tuning methodology is more transparent and has
well-established literature compared to the heuristics in reinforcement
learning. In addition, our approach does not use a target network and
randomized replay memory. The role of the target network is overtaken by the
control input, which also exploits the temporal dependency of samples (opposed
to a randomized memory buffer). Numerical simulations in different OpenAI Gym
environments suggest that the Hinf controlled learning performs slightly better
than Double deep Q-learning.",2201.08610v2,https://arxiv.org/pdf/2201.08610v2
"Toward Enhanced Robustness in Unsupervised Graph Representation
  Learning: A Graph Information Bottleneck Perspective","Jihong Wang, Minnan Luo, Jundong Li, Ziqi Liu, Jun Zhou, Qinghua Zheng","Recent studies have revealed that GNNs are vulnerable to adversarial attacks.
Most existing robust graph learning methods measure model robustness based on
label information, rendering them infeasible when label information is not
available. A straightforward direction is to employ the widely used Infomax
technique from typical Unsupervised Graph Representation Learning (UGRL) to
learn robust unsupervised representations. Nonetheless, directly transplanting
the Infomax technique from typical UGRL to robust UGRL may involve a biased
assumption. In light of the limitation of Infomax, we propose a novel unbiased
robust UGRL method called Robust Graph Information Bottleneck (RGIB), which is
grounded in the Information Bottleneck (IB) principle. Our RGIB attempts to
learn robust node representations against adversarial perturbations by
preserving the original information in the benign graph while eliminating the
adversarial information in the adversarial graph. There are mainly two
challenges to optimize RGIB: 1) high complexity of adversarial attack to
perturb node features and graph structure jointly in the training procedure; 2)
mutual information estimation upon adversarially attacked graphs. To tackle
these problems, we further propose an efficient adversarial training strategy
with only feature perturbations and an effective mutual information estimator
with subgraph-level summary. Moreover, we theoretically establish a connection
between our proposed RGIB and the robustness of downstream classifiers,
revealing that RGIB can provide a lower bound on the adversarial risk of
downstream classifiers. Extensive experiments over several benchmarks and
downstream tasks demonstrate the effectiveness and superiority of our proposed
method.",2201.08557v2,https://arxiv.org/pdf/2201.08557v2
"AdaTerm: Adaptive T-Distribution Estimated Robust Moments for
  Noise-Robust Stochastic Gradient Optimization","Wendyam Eric Lionel Ilboudo, Taisuke Kobayashi, Takamitsu Matsubara","With the increasing practicality of deep learning applications, practitioners
are inevitably faced with datasets corrupted by noise from various sources such
as measurement errors, mislabeling, and estimated surrogate inputs/outputs that
can adversely impact the optimization results. It is a common practice to
improve the optimization algorithm's robustness to noise, since this algorithm
is ultimately in charge of updating the network parameters. Previous studies
revealed that the first-order moment used in Adam-like stochastic gradient
descent optimizers can be modified based on the Student's t-distribution. While
this modification led to noise-resistant updates, the other associated
statistics remained unchanged, resulting in inconsistencies in the assumed
models. In this paper, we propose AdaTerm, a novel approach that incorporates
the Student's t-distribution to derive not only the first-order moment but also
all the associated statistics. This provides a unified treatment of the
optimization process, offering a comprehensive framework under the statistical
model of the t-distribution for the first time. The proposed approach offers
several advantages over previously proposed approaches, including reduced
hyperparameters and improved robustness and adaptability. This noise-adaptive
behavior contributes to AdaTerm's exceptional learning performance, as
demonstrated through various optimization problems with different and/or
unknown noise ratios. Furthermore, we introduce a new technique for deriving a
theoretical regret bound without relying on AMSGrad, providing a valuable
contribution to the field",2201.06714v4,https://arxiv.org/pdf/2201.06714v4
"Planning Not to Talk: Multiagent Systems that are Robust to
  Communication Loss","Mustafa O. Karabag, Cyrus Neary, Ufuk Topcu","In a cooperative multiagent system, a collection of agents executes a joint
policy in order to achieve some common objective. The successful deployment of
such systems hinges on the availability of reliable inter-agent communication.
However, many sources of potential disruption to communication exist in
practice, such as radio interference, hardware failure, and adversarial
attacks. In this work, we develop joint policies for cooperative multiagent
systems that are robust to potential losses in communication. More
specifically, we develop joint policies for cooperative Markov games with
reach-avoid objectives. First, we propose an algorithm for the decentralized
execution of joint policies during periods of communication loss. Next, we use
the total correlation of the state-action process induced by a joint policy as
a measure of the intrinsic dependencies between the agents. We then use this
measure to lower-bound the performance of a joint policy when communication is
lost. Finally, we present an algorithm that maximizes a proxy to this lower
bound in order to synthesize minimum-dependency joint policies that are robust
to communication loss. Numerical experiments show that the proposed
minimum-dependency policies require minimal coordination between the agents
while incurring little to no loss in performance; the total correlation value
of the synthesized policy is one fifth of the total correlation value of the
baseline policy which does not take potential communication losses into
account. As a result, the performance of the minimum-dependency policies
remains consistently high regardless of whether or not communication is
available. By contrast, the performance of the baseline policy decreases by
twenty percent when communication is lost.",2201.06619v1,https://arxiv.org/pdf/2201.06619v1
AugLy: Data Augmentations for Robustness,"Zoe Papakipos, Joanna Bitton","We introduce AugLy, a data augmentation library with a focus on adversarial
robustness. AugLy provides a wide array of augmentations for multiple
modalities (audio, image, text, & video). These augmentations were inspired by
those that real users perform on social media platforms, some of which were not
already supported by existing data augmentation libraries. AugLy can be used
for any purpose where data augmentations are useful, but it is particularly
well-suited for evaluating robustness and systematically generating adversarial
attacks. In this paper we present how AugLy works, benchmark it compared
against existing libraries, and use it to evaluate the robustness of various
state-of-the-art models to showcase AugLy's utility. The AugLy repository can
be found at https://github.com/facebookresearch/AugLy.",2201.06494v1,https://arxiv.org/pdf/2201.06494v1
"Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial
  Examples Against Traffic Sign Recognition Systems","Wei Jia, Zhaojun Lu, Haichun Zhang, Zhenglin Liu, Jie Wang, Gang Qu","Adversarial Examples (AEs) can deceive Deep Neural Networks (DNNs) and have
received a lot of attention recently. However, majority of the research on AEs
is in the digital domain and the adversarial patches are static, which is very
different from many real-world DNN applications such as Traffic Sign
Recognition (TSR) systems in autonomous vehicles. In TSR systems, object
detectors use DNNs to process streaming video in real time. From the view of
object detectors, the traffic sign`s position and quality of the video are
continuously changing, rendering the digital AEs ineffective in the physical
world.
  In this paper, we propose a systematic pipeline to generate robust physical
AEs against real-world object detectors. Robustness is achieved in three ways.
First, we simulate the in-vehicle cameras by extending the distribution of
image transformations with the blur transformation and the resolution
transformation. Second, we design the single and multiple bounding boxes
filters to improve the efficiency of the perturbation training. Third, we
consider four representative attack vectors, namely Hiding Attack, Appearance
Attack, Non-Target Attack and Target Attack.
  We perform a comprehensive set of experiments under a variety of
environmental conditions, and considering illuminations in sunny and cloudy
weather as well as at night. The experimental results show that the physical
AEs generated from our pipeline are effective and robust when attacking the
YOLO v5 based TSR system. The attacks have good transferability and can deceive
other state-of-the-art object detectors. We launched HA and NTA on a brand-new
2021 model vehicle. Both attacks are successful in fooling the TSR system,
which could be a life-threatening case for autonomous vehicles. Finally, we
discuss three defense mechanisms based on image preprocessing, AEs detection,
and model enhancing.",2201.06192v1,https://arxiv.org/pdf/2201.06192v1
Common Phone: A Multilingual Dataset for Robust Acoustic Modelling,"Philipp Klumpp, Tomás Arias-Vergara, Paula Andrea Pérez-Toro, Elmar Nöth, Juan Rafael Orozco-Arroyave","Current state of the art acoustic models can easily comprise more than 100
million parameters. This growing complexity demands larger training datasets to
maintain a decent generalization of the final decision function. An ideal
dataset is not necessarily large in size, but large with respect to the amount
of unique speakers, utilized hardware and varying recording conditions. This
enables a machine learning model to explore as much of the domain-specific
input space as possible during parameter estimation. This work introduces
Common Phone, a gender-balanced, multilingual corpus recorded from more than
11.000 contributors via Mozilla's Common Voice project. It comprises around 116
hours of speech enriched with automatically generated phonetic segmentation. A
Wav2Vec 2.0 acoustic model was trained with the Common Phone to perform
phonetic symbol recognition and validate the quality of the generated phonetic
annotation. The architecture achieved a PER of 18.1 % on the entire test set,
computed with all 101 unique phonetic symbols, showing slight differences
between the individual languages. We conclude that Common Phone provides
sufficient variability and reliable phonetic annotation to help bridging the
gap between research and application of acoustic models.",2201.05912v2,https://arxiv.org/pdf/2201.05912v2
"Robust uncertainty estimates with out-of-distribution pseudo-inputs
  training","Pierre Segonne, Yevgen Zainchkovskyy, Søren Hauberg","Probabilistic models often use neural networks to control their predictive
uncertainty. However, when making out-of-distribution (OOD)} predictions, the
often-uncontrollable extrapolation properties of neural networks yield poor
uncertainty predictions. Such models then don't know what they don't know,
which directly limits their robustness w.r.t unexpected inputs. To counter
this, we propose to explicitly train the uncertainty predictor where we are not
given data to make it reliable. As one cannot train without data, we provide
mechanisms for generating pseudo-inputs in informative low-density regions of
the input space, and show how to leverage these in a practical Bayesian
framework that casts a prior distribution over the model uncertainty. With a
holistic evaluation, we demonstrate that this yields robust and interpretable
predictions of uncertainty while retaining state-of-the-art performance on
diverse tasks such as regression and generative modelling",2201.05890v1,https://arxiv.org/pdf/2201.05890v1
"Concise Logarithmic Loss Function for Robust Training of Anomaly
  Detection Model",YeongHyeon Park,"Recently, deep learning-based algorithms are widely adopted due to the
advantage of being able to establish anomaly detection models without or with
minimal domain knowledge of the task. Instead, to train the artificial neural
network more stable, it should be better to define the appropriate neural
network structure or the loss function. For the training anomaly detection
model, the mean squared error (MSE) function is adopted widely. On the other
hand, the novel loss function, logarithmic mean squared error (LMSE), is
proposed in this paper to train the neural network more stable. This study
covers a variety of comparisons from mathematical comparisons, visualization in
the differential domain for backpropagation, loss convergence in the training
process, and anomaly detection performance. In an overall view, LMSE is
superior to the existing MSE function in terms of strongness of loss
convergence, anomaly detection performance. The LMSE function is expected to be
applicable for training not only the anomaly detection model but also the
general generative neural network.",2201.05748v2,https://arxiv.org/pdf/2201.05748v2
"Precise Stock Price Prediction for Robust Portfolio Design from Selected
  Sectors of the Indian Stock Market","Jaydip Sen, Ashwin Kumar R S, Geetha Joseph, Kaushik Muthukrishnan, Koushik Tulasi, Praveen Varukolu","Stock price prediction is a challenging task and a lot of propositions exist
in the literature in this area. Portfolio construction is a process of choosing
a group of stocks and investing in them optimally to maximize the return while
minimizing the risk. Since the time when Markowitz proposed the Modern
Portfolio Theory, several advancements have happened in the area of building
efficient portfolios. An investor can get the best benefit out of the stock
market if the investor invests in an efficient portfolio and could take the buy
or sell decision in advance, by estimating the future asset value of the
portfolio with a high level of precision. In this project, we have built an
efficient portfolio and to predict the future asset value by means of
individual stock price prediction of the stocks in the portfolio. As part of
building an efficient portfolio we have studied multiple portfolio optimization
methods beginning with the Modern Portfolio theory. We have built the minimum
variance portfolio and optimal risk portfolio for all the five chosen sectors
by using past daily stock prices over the past five years as the training data,
and have also conducted back testing to check the performance of the portfolio.
A comparative study of minimum variance portfolio and optimal risk portfolio
with equal weight portfolio is done by backtesting.",2201.05570v1,https://arxiv.org/pdf/2201.05570v1
"The curse of overparametrization in adversarial training: Precise
  analysis of robust generalization for random features regression","Hamed Hassani, Adel Javanmard","Successful deep learning models often involve training neural network
architectures that contain more parameters than the number of training samples.
Such overparametrized models have been extensively studied in recent years, and
the virtues of overparametrization have been established from both the
statistical perspective, via the double-descent phenomenon, and the
computational perspective via the structural properties of the optimization
landscape.
  Despite the remarkable success of deep learning architectures in the
overparametrized regime, it is also well known that these models are highly
vulnerable to small adversarial perturbations in their inputs. Even when
adversarially trained, their performance on perturbed inputs (robust
generalization) is considerably worse than their best attainable performance on
benign inputs (standard generalization). It is thus imperative to understand
how overparametrization fundamentally affects robustness.
  In this paper, we will provide a precise characterization of the role of
overparametrization on robustness by focusing on random features regression
models (two-layer neural networks with random first layer weights). We consider
a regime where the sample size, the input dimension and the number of
parameters grow in proportion to each other, and derive an asymptotically exact
formula for the robust generalization error when the model is adversarially
trained. Our developed theory reveals the nontrivial effect of
overparametrization on robustness and indicates that for adversarially trained
random features models, high overparametrization can hurt robust
generalization.",2201.05149v2,https://arxiv.org/pdf/2201.05149v2
"A robust kernel machine regression towards biomarker selection in
  multi-omics datasets of osteoporosis for drug discovery","Md Ashad Alam, Hui Shen, Hong-Wen Deng","Many statistical machine approaches could ultimately highlight novel features
of the etiology of complex diseases by analyzing multi-omics data. However,
they are sensitive to some deviations in distribution when the observed samples
are potentially contaminated with adversarial corrupted outliers (e.g., a
fictional data distribution). Likewise, statistical advances lag in supporting
comprehensive data-driven analyses of complex multi-omics data integration. We
propose a novel non-linear M-estimator-based approach, ""robust kernel machine
regression (RobKMR),"" to improve the robustness of statistical machine
regression and the diversity of fictional data to examine the higher-order
composite effect of multi-omics datasets. We address a robust kernel-centered
Gram matrix to estimate the model parameters accurately. We also propose a
robust score test to assess the marginal and joint Hadamard product of features
from multi-omics data. We apply our proposed approach to a multi-omics dataset
of osteoporosis (OP) from Caucasian females. Experiments demonstrate that the
proposed approach effectively identifies the inter-related risk factors of OP.
With solid evidence (p-value = 0.00001), biological validations, network-based
analysis, causal inference, and drug repurposing, the selected three triplets
((DKK1, SMTN, DRGX), (MTND5, FASTKD2, CSMD3), (MTND5, COG3, CSMD3)) are
significant biomarkers and directly relate to BMD. Overall, the top three
selected genes (DKK1, MTND5, FASTKD2) and one gene (SIDT1 at p-value= 0.001)
significantly bond with four drugs- Tacrolimus, Ibandronate, Alendronate, and
Bazedoxifene out of 30 candidates for drug repurposing in OP. Further, the
proposed approach can be applied to any disease model where multi-omics
datasets are available.",2201.05060v1,https://arxiv.org/pdf/2201.05060v1
Certifiable Robustness for Nearest Neighbor Classifiers,"Austen Z. Fan, Paraschos Koutris","ML models are typically trained using large datasets of high quality.
However, training datasets often contain inconsistent or incomplete data. To
tackle this issue, one solution is to develop algorithms that can check whether
a prediction of a model is certifiably robust. Given a learning algorithm that
produces a classifier and given an example at test time, a classification
outcome is certifiably robust if it is predicted by every model trained across
all possible worlds (repairs) of the uncertain (inconsistent) dataset. This
notion of robustness falls naturally under the framework of certain answers. In
this paper, we study the complexity of certifying robustness for a simple but
widely deployed classification algorithm, $k$-Nearest Neighbors ($k$-NN). Our
main focus is on inconsistent datasets when the integrity constraints are
functional dependencies (FDs). For this setting, we establish a dichotomy in
the complexity of certifying robustness w.r.t. the set of FDs: the problem
either admits a polynomial time algorithm, or it is coNP-hard. Additionally, we
exhibit a similar dichotomy for the counting version of the problem, where the
goal is to count the number of possible worlds that predict a certain label. As
a byproduct of our study, we also establish the complexity of a problem related
to finding an optimal subset repair that may be of independent interest.",2201.04770v2,https://arxiv.org/pdf/2201.04770v2
"Adversarially Robust Classification by Conditional Generative Model
  Inversion","Mitra Alirezaei, Tolga Tasdizen","Most adversarial attack defense methods rely on obfuscating gradients. These
methods are successful in defending against gradient-based attacks; however,
they are easily circumvented by attacks which either do not use the gradient or
by attacks which approximate and use the corrected gradient. Defenses that do
not obfuscate gradients such as adversarial training exist, but these
approaches generally make assumptions about the attack such as its magnitude.
We propose a classification model that does not obfuscate gradients and is
robust by construction without assuming prior knowledge about the attack. Our
method casts classification as an optimization problem where we ""invert"" a
conditional generator trained on unperturbed, natural images to find the class
that generates the closest sample to the query image. We hypothesize that a
potential source of brittleness against adversarial attacks is the
high-to-low-dimensional nature of feed-forward classifiers which allows an
adversary to find small perturbations in the input space that lead to large
changes in the output space. On the other hand, a generative model is typically
a low-to-high-dimensional mapping. While the method is related to Defense-GAN,
the use of a conditional generative model and inversion in our model instead of
the feed-forward classifier is a critical difference. Unlike Defense-GAN, which
was shown to generate obfuscated gradients that are easily circumvented, we
show that our method does not obfuscate gradients. We demonstrate that our
model is extremely robust against black-box attacks and has improved robustness
against white-box attacks compared to naturally trained, feed-forward
classifiers.",2201.04733v1,https://arxiv.org/pdf/2201.04733v1
RGRecSys: A Toolkit for Robustness Evaluation of Recommender Systems,"Zohreh Ovaisi, Shelby Heinecke, Jia Li, Yongfeng Zhang, Elena Zheleva, Caiming Xiong","Robust machine learning is an increasingly important topic that focuses on
developing models resilient to various forms of imperfect data. Due to the
pervasiveness of recommender systems in online technologies, researchers have
carried out several robustness studies focusing on data sparsity and profile
injection attacks. Instead, we propose a more holistic view of robustness for
recommender systems that encompasses multiple dimensions - robustness with
respect to sub-populations, transformations, distributional disparity, attack,
and data sparsity. While there are several libraries that allow users to
compare different recommender system models, there is no software library for
comprehensive robustness evaluation of recommender system models under
different scenarios. As our main contribution, we present a robustness
evaluation toolkit, Robustness Gym for RecSys (RGRecSys --
https://www.github.com/salesforce/RGRecSys), that allows us to quickly and
uniformly evaluate the robustness of recommender system models.",2201.04399v1,https://arxiv.org/pdf/2201.04399v1
Towards Adversarially Robust Deep Image Denoising,"Hanshu Yan, Jingfeng Zhang, Jiashi Feng, Masashi Sugiyama, Vincent Y. F. Tan","This work systematically investigates the adversarial robustness of deep
image denoisers (DIDs), i.e, how well DIDs can recover the ground truth from
noisy observations degraded by adversarial perturbations. Firstly, to evaluate
DIDs' robustness, we propose a novel adversarial attack, namely
Observation-based Zero-mean Attack ({\sc ObsAtk}), to craft adversarial
zero-mean perturbations on given noisy images. We find that existing DIDs are
vulnerable to the adversarial noise generated by {\sc ObsAtk}. Secondly, to
robustify DIDs, we propose an adversarial training strategy, hybrid adversarial
training ({\sc HAT}), that jointly trains DIDs with adversarial and
non-adversarial noisy data to ensure that the reconstruction quality is high
and the denoisers around non-adversarial data are locally smooth. The resultant
DIDs can effectively remove various types of synthetic and adversarial noise.
We also uncover that the robustness of DIDs benefits their generalization
capability on unseen real-world noise. Indeed, {\sc HAT}-trained DIDs can
recover high-quality clean images from real-world noise even without training
on real noisy data. Extensive experiments on benchmark datasets, including
Set68, PolyU, and SIDD, corroborate the effectiveness of {\sc ObsAtk} and {\sc
HAT}.",2201.04397v2,https://arxiv.org/pdf/2201.04397v2
Robust Contrastive Learning against Noisy Views,"Ching-Yao Chuang, R Devon Hjelm, Xin Wang, Vibhav Vineet, Neel Joshi, Antonio Torralba, Stefanie Jegelka, Yale Song","Contrastive learning relies on an assumption that positive pairs contain
related views, e.g., patches of an image or co-occurring multimodal signals of
a video, that share certain underlying information about an instance. But what
if this assumption is violated? The literature suggests that contrastive
learning produces suboptimal representations in the presence of noisy views,
e.g., false positive pairs with no apparent shared information. In this work,
we propose a new contrastive loss function that is robust against noisy views.
We provide rigorous theoretical justifications by showing connections to robust
symmetric losses for noisy binary classification and by establishing a new
contrastive bound for mutual information maximization based on the Wasserstein
distance measure. The proposed loss is completely modality-agnostic and a
simple drop-in replacement for the InfoNCE loss, which makes it easy to apply
to existing contrastive frameworks. We show that our approach provides
consistent improvements over the state-of-the-art on image, video, and graph
contrastive learning benchmarks that exhibit a variety of real-world noise
patterns.",2201.04309v1,https://arxiv.org/pdf/2201.04309v1
"Learning Robust Policies for Generalized Debris Capture with an
  Automated Tether-Net System","Chen Zeng, Grant Hecht, Prajit KrisshnaKumar, Raj K. Shah, Souma Chowdhury, Eleonora M. Botta","Tether-net launched from a chaser spacecraft provides a promising method to
capture and dispose of large space debris in orbit. This tether-net system is
subject to several sources of uncertainty in sensing and actuation that affect
the performance of its net launch and closing control. Earlier
reliability-based optimization approaches to design control actions however
remain challenging and computationally prohibitive to generalize over varying
launch scenarios and target (debris) state relative to the chaser. To search
for a general and reliable control policy, this paper presents a reinforcement
learning framework that integrates a proximal policy optimization (PPO2)
approach with net dynamics simulations. The latter allows evaluating the
episodes of net-based target capture, and estimate the capture quality index
that serves as the reward feedback to PPO2. Here, the learned policy is
designed to model the timing of the net closing action based on the state of
the moving net and the target, under any given launch scenario. A stochastic
state transition model is considered in order to incorporate synthetic
uncertainties in state estimation and launch actuation. Along with notable
reward improvement during training, the trained policy demonstrates capture
performance (over a wide range of launch/target scenarios) that is close to
that obtained with reliability-based optimization run over an individual
scenario.",2201.04180v1,https://arxiv.org/pdf/2201.04180v1
"Quasi-Framelets: Robust Graph Neural Networks via Adaptive Framelet
  Convolution","Mengxi Yang, Dai Shi, Xuebin Zheng, Jie Yin, Junbin Gao","This paper aims to provide a novel design of a multiscale framelet
convolution for spectral graph neural networks (GNNs). While current spectral
methods excel in various graph learning tasks, they often lack the flexibility
to adapt to noisy, incomplete, or perturbed graph signals, making them fragile
in such conditions. Our newly proposed framelet convolution addresses these
limitations by decomposing graph data into low-pass and high-pass spectra
through a finely-tuned multiscale approach. Our approach directly designs
filtering functions within the spectral domain, allowing for precise control
over the spectral components. The proposed design excels in filtering out
unwanted spectral information and significantly reduces the adverse effects of
noisy graph signals. Our approach not only enhances the robustness of GNNs but
also preserves crucial graph features and structures. Through extensive
experiments on diverse, real-world graph datasets, we demonstrate that our
framelet convolution achieves superior performance in node classification
tasks. It exhibits remarkable resilience to noisy data and adversarial attacks,
highlighting its potential as a robust solution for real-world graph
applications. This advancement opens new avenues for more adaptive and reliable
spectral GNN architectures.",2201.04728v2,https://arxiv.org/pdf/2201.04728v2
Towards Group Robustness in the presence of Partial Group Labels,"Vishnu Suresh Lokhande, Kihyuk Sohn, Jinsung Yoon, Madeleine Udell, Chen-Yu Lee, Tomas Pfister","Learning invariant representations is an important requirement when training
machine learning models that are driven by spurious correlations in the
datasets. These spurious correlations, between input samples and the target
labels, wrongly direct the neural network predictions resulting in poor
performance on certain groups, especially the minority groups. Robust training
against these spurious correlations requires the knowledge of group membership
for every sample. Such a requirement is impractical in situations where the
data labeling efforts for minority or rare groups are significantly laborious
or where the individuals comprising the dataset choose to conceal sensitive
information. On the other hand, the presence of such data collection efforts
results in datasets that contain partially labeled group information. Recent
works have tackled the fully unsupervised scenario where no labels for groups
are available. Thus, we aim to fill the missing gap in the literature by
tackling a more realistic setting that can leverage partially available
sensitive or group information during training. First, we construct a
constraint set and derive a high probability bound for the group assignment to
belong to the set. Second, we propose an algorithm that optimizes for the
worst-off group assignments from the constraint set. Through experiments on
image and tabular datasets, we show improvements in the minority group's
performance while preserving overall aggregate accuracy across groups.",2201.03668v1,https://arxiv.org/pdf/2201.03668v1
"Iterative training of robust k-space interpolation networks for improved
  image reconstruction with limited scan specific training samples","Peter Dawood, Felix Breuer, Paul R. Burd, István Homolya, Johannes Oberberger, Peter M. Jakob, Martin Blaimer","Purpose: To evaluate an iterative learning approach for enhanced performance
of Robust Artificial-neural-networks for K-space Interpolation (RAKI), when
only a limited amount of training data (auto-calibration signals, ACS) are
available for accelerated standard 2D imaging. Methods: In a first step, the
RAKI model was optimized for the case of strongly limited training data amount.
In the iterative learning approach (termed iterative RAKI), the optimized RAKI
model is initially trained using original and augmented ACS obtained from a
linear parallel imaging reconstruction. Subsequently, the RAKI convolution
filters are refined iteratively using original and augmented ACS extracted from
the previous RAKI reconstruction. Evaluation was carried out on 200
retrospectively undersampled in-vivo datasets from the fastMRI neuro database
with different contrast settings. Results: For limited training data (18 and 22
ACS lines for R=4 and R=5, respectively), iterative RAKI outperforms standard
RAKI by reducing residual artefacts and yields strong noise suppression when
compared to standard parallel imaging, underlined by quantitative
reconstruction quality metrics. In combination with a phase constraint, further
reconstruction improvements can be achieved. Additionally, iterative RAKI shows
better performance than both GRAPPA and RAKI in case of pre-scan calibration
with varying contrast between training- and undersampled data. Conclusion: The
iterative learning approach with RAKI benefits from standard RAKIs well known
noise suppression feature but requires less original training data for the
accurate reconstruction of standard 2D images thereby improving net
acceleration.",2201.03560v2,https://arxiv.org/pdf/2201.03560v2
"Non-Asymptotic Guarantees for Robust Statistical Learning under Infinite
  Variance Assumption","Lihu Xu, Fang Yao, Qiuran Yao, Huiming Zhang","There has been a surge of interest in developing robust estimators for models
with heavy-tailed and bounded variance data in statistics and machine learning,
while few works impose unbounded variance. This paper proposes two type of
robust estimators, the ridge log-truncated M-estimator and the elastic net
log-truncated M-estimator. The first estimator is applied to convex regressions
such as quantile regression and generalized linear models, while the other one
is applied to high dimensional non-convex learning problems such as regressions
via
  deep neural networks. Simulations and real data analysis demonstrate the
{robustness} of log-truncated estimations over standard estimations.",2201.03182v2,https://arxiv.org/pdf/2201.03182v2
"Robust and Resource-Efficient Data-Free Knowledge Distillation by
  Generative Pseudo Replay","Kuluhan Binici, Shivam Aggarwal, Nam Trung Pham, Karianto Leman, Tulika Mitra","Data-Free Knowledge Distillation (KD) allows knowledge transfer from a
trained neural network (teacher) to a more compact one (student) in the absence
of original training data. Existing works use a validation set to monitor the
accuracy of the student over real data and report the highest performance
throughout the entire process. However, validation data may not be available at
distillation time either, making it infeasible to record the student snapshot
that achieved the peak accuracy. Therefore, a practical data-free KD method
should be robust and ideally provide monotonically increasing student accuracy
during distillation. This is challenging because the student experiences
knowledge degradation due to the distribution shift of the synthetic data. A
straightforward approach to overcome this issue is to store and rehearse the
generated samples periodically, which increases the memory footprint and
creates privacy concerns. We propose to model the distribution of the
previously observed synthetic samples with a generative network. In particular,
we design a Variational Autoencoder (VAE) with a training objective that is
customized to learn the synthetic data representations optimally. The student
is rehearsed by the generative pseudo replay technique, with samples produced
by the VAE. Hence knowledge degradation can be prevented without storing any
samples. Experiments on image classification benchmarks show that our method
optimizes the expected value of the distilled model accuracy while eliminating
the large memory overhead incurred by the sample-storing methods.",2201.03019v3,https://arxiv.org/pdf/2201.03019v3
"A multi-scale sampling method for accurate and robust deep neural
  network to predict combustion chemical kinetics","Tianhan Zhang, Yuxiao Yi, Yifan Xu, Zhi X. Chen, Yaoyu Zhang, Weinan E, Zhi-Qin John Xu","Machine learning has long been considered as a black box for predicting
combustion chemical kinetics due to the extremely large number of parameters
and the lack of evaluation standards and reproducibility. The current work aims
to understand two basic questions regarding the deep neural network (DNN)
method: what data the DNN needs and how general the DNN method can be. Sampling
and preprocessing determine the DNN training dataset, further affect DNN
prediction ability. The current work proposes using Box-Cox transformation
(BCT) to preprocess the combustion data. In addition, this work compares
different sampling methods with or without preprocessing, including the Monte
Carlo method, manifold sampling, generative neural network method (cycle-GAN),
and newly-proposed multi-scale sampling. Our results reveal that the DNN
trained by the manifold data can capture the chemical kinetics in limited
configurations but cannot remain robust toward perturbation, which is
inevitable for the DNN coupled with the flow field. The Monte Carlo and
cycle-GAN samplings can cover a wider phase space but fail to capture
small-scale intermediate species, producing poor prediction results. A
three-hidden-layer DNN, based on the multi-scale method without specific flame
simulation data, allows predicting chemical kinetics in various scenarios and
being stable during the temporal evolutions. This single DNN is readily
implemented with several CFD codes and validated in various combustors,
including (1). zero-dimensional autoignition, (2). one-dimensional freely
propagating flame, (3). two-dimensional jet flame with triple-flame structure,
and (4). three-dimensional turbulent lifted flames. The results demonstrate the
satisfying accuracy and generalization ability of the pre-trained DNN. The
Fortran and Python versions of DNN and example code are attached in the
supplementary for reproducibility.",2201.03549v2,https://arxiv.org/pdf/2201.03549v2
"Robust classification with flexible discriminant analysis in
  heterogeneous data","Pierre Houdouin, Frédéric Pascal, Matthieu Jonckheere, Andrew Wang","Linear and Quadratic Discriminant Analysis are well-known classical methods
but can heavily suffer from non-Gaussian distributions and/or contaminated
datasets, mainly because of the underlying Gaussian assumption that is not
robust. To fill this gap, this paper presents a new robust discriminant
analysis where each data point is drawn by its own arbitrary Elliptically
Symmetrical (ES) distribution and its own arbitrary scale parameter. Such a
model allows for possibly very heterogeneous, independent but non-identically
distributed samples. After deriving a new decision rule, it is shown that
maximum-likelihood parameter estimation and classification are very simple,
fast and robust compared to state-of-the-art methods.",2201.02967v1,https://arxiv.org/pdf/2201.02967v1
"On robust risk-based active-learning algorithms for enhanced decision
  support","Aidan J. Hughes, Lawrence A. Bull, Paul Gardner, Nikolaos Dervilis, Keith Worden","Classification models are a fundamental component of physical-asset
management technologies such as structural health monitoring (SHM) systems and
digital twins. Previous work introduced risk-based active learning, an online
approach for the development of statistical classifiers that takes into account
the decision-support context in which they are applied. Decision-making is
considered by preferentially querying data labels according to expected value
of perfect information (EVPI). Although several benefits are gained by adopting
a risk-based active learning approach, including improved decision-making
performance, the algorithms suffer from issues relating to sampling bias as a
result of the guided querying process. This sampling bias ultimately manifests
as a decline in decision-making performance during the later stages of active
learning, which in turn corresponds to lost resource/utility.
  The current paper proposes two novel approaches to counteract the effects of
sampling bias: semi-supervised learning, and discriminative classification
models. These approaches are first visualised using a synthetic dataset, then
subsequently applied to an experimental case study, specifically, the Z24
Bridge dataset. The semi-supervised learning approach is shown to have variable
performance; with robustness to sampling bias dependent on the suitability of
the generative distributions selected for the model with respect to each
dataset. In contrast, the discriminative classifiers are shown to have
excellent robustness to the effects of sampling bias. Moreover, it was found
that the number of inspections made during a monitoring campaign, and therefore
resource expenditure, could be reduced with the careful selection of the
statistical classifiers used within a decision-supporting monitoring system.",2201.02555v2,https://arxiv.org/pdf/2201.02555v2
Learning to be adversarially robust and differentially private,"Jamie Hayes, Borja Balle, M. Pawan Kumar","We study the difficulties in learning that arise from robust and
differentially private optimization. We first study convergence of gradient
descent based adversarial training with differential privacy, taking a simple
binary classification task on linearly separable data as an illustrative
example. We compare the gap between adversarial and nominal risk in both
private and non-private settings, showing that the data dimensionality
dependent term introduced by private optimization compounds the difficulties of
learning a robust model. After this, we discuss what parts of adversarial
training and differential privacy hurt optimization, identifying that the size
of adversarial perturbation and clipping norm in differential privacy both
increase the curvature of the loss landscape, implying poorer generalization
performance.",2201.02265v1,https://arxiv.org/pdf/2201.02265v1
"Robust Linear Predictions: Analyses of Uniform Concentration, Fast Rates
  and Model Misspecification","Saptarshi Chakraborty, Debolina Paul, Swagatam Das","The problem of linear predictions has been extensively studied for the past
century under pretty generalized frameworks. Recent advances in the robust
statistics literature allow us to analyze robust versions of classical linear
models through the prism of Median of Means (MoM). Combining these approaches
in a piecemeal way might lead to ad-hoc procedures, and the restricted
theoretical conclusions that underpin each individual contribution may no
longer be valid. To meet these challenges coherently, in this study, we offer a
unified robust framework that includes a broad variety of linear prediction
problems on a Hilbert space, coupled with a generic class of loss functions.
Notably, we do not require any assumptions on the distribution of the outlying
data points ($\mathcal{O}$) nor the compactness of the support of the inlying
ones ($\mathcal{I}$). Under mild conditions on the dual norm, we show that for
misspecification level $\epsilon$, these estimators achieve an error rate of
$O(\max\left\{|\mathcal{O}|^{1/2}n^{-1/2}, |\mathcal{I}|^{1/2}n^{-1}
\right\}+\epsilon)$, matching the best-known rates in literature. This rate is
slightly slower than the classical rates of $O(n^{-1/2})$, indicating that we
need to pay a price in terms of error rates to obtain robust estimates.
Additionally, we show that this rate can be improved to achieve so-called ""fast
rates"" under additional assumptions.",2201.01973v2,https://arxiv.org/pdf/2201.01973v2
"On the Real-World Adversarial Robustness of Real-Time Semantic
  Segmentation Models for Autonomous Driving","Giulio Rossolini, Federico Nesti, Gianluca D'Amico, Saasha Nair, Alessandro Biondi, Giorgio Buttazzo","The existence of real-world adversarial examples (commonly in the form of
patches) poses a serious threat for the use of deep learning models in
safety-critical computer vision tasks such as visual perception in autonomous
driving. This paper presents an extensive evaluation of the robustness of
semantic segmentation models when attacked with different types of adversarial
patches, including digital, simulated, and physical ones. A novel loss function
is proposed to improve the capabilities of attackers in inducing a
misclassification of pixels. Also, a novel attack strategy is presented to
improve the Expectation Over Transformation method for placing a patch in the
scene. Finally, a state-of-the-art method for detecting adversarial patch is
first extended to cope with semantic segmentation models, then improved to
obtain real-time performance, and eventually evaluated in real-world scenarios.
Experimental results reveal that, even though the adversarial effect is visible
with both digital and real-world attacks, its impact is often spatially
confined to areas of the image around the patch. This opens to further
questions about the spatial robustness of real-time semantic segmentation
models.",2201.01850v1,https://arxiv.org/pdf/2201.01850v1
Robust Self-Supervised Audio-Visual Speech Recognition,"Bowen Shi, Wei-Ning Hsu, Abdelrahman Mohamed","Audio-based automatic speech recognition (ASR) degrades significantly in
noisy environments and is particularly vulnerable to interfering speech, as the
model cannot determine which speaker to transcribe. Audio-visual speech
recognition (AVSR) systems improve robustness by complementing the audio stream
with the visual information that is invariant to noise and helps the model
focus on the desired speaker. However, previous AVSR work focused solely on the
supervised learning setup; hence the progress was hindered by the amount of
labeled data available. In this work, we present a self-supervised AVSR
framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art
audio-visual speech representation learning model. On the largest available
AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by
~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in
the presence of babble noise, while reducing the WER of an audio-based model by
over 75% (25.8% vs. 5.8%) on average.",2201.01763v3,https://arxiv.org/pdf/2201.01763v3
"Towards Understanding Quality Challenges of the Federated Learning for
  Neural Networks: A First Look from the Lens of Robustness","Amin Eslami Abyane, Derui Zhu, Roberto Souza, Lei Ma, Hadi Hemmati","Federated learning (FL) is a distributed learning paradigm that preserves
users' data privacy while leveraging the entire dataset of all participants. In
FL, multiple models are trained independently on the clients and aggregated
centrally to update a global model in an iterative process. Although this
approach is excellent at preserving privacy, FL still suffers from quality
issues such as attacks or byzantine faults. Recent attempts have been made to
address such quality challenges on the robust aggregation techniques for FL.
However, the effectiveness of state-of-the-art (SOTA) robust FL techniques is
still unclear and lacks a comprehensive study. Therefore, to better understand
the current quality status and challenges of these SOTA FL techniques in the
presence of attacks and faults, we perform a large-scale empirical study to
investigate the SOTA FL's quality from multiple angles of attacks, simulated
faults (via mutation operators), and aggregation (defense) methods. In
particular, we study FL's performance on the image classification tasks and use
DNNs as our model type. Furthermore, we perform our study on two generic image
datasets and one real-world federated medical image dataset. We also
investigate the effect of the proportion of affected clients and the dataset
distribution factors on the robustness of FL. After a large-scale analysis with
496 configurations, we find that most mutators on each user have a negligible
effect on the final model in the generic datasets, and only one of them is
effective in the medical dataset. Furthermore, we show that model poisoning
attacks are more effective than data poisoning attacks. Moreover, choosing the
most robust FL aggregator depends on the attacks and datasets. Finally, we
illustrate that a simple ensemble of aggregators achieves a more robust
solution than any single aggregator and is the best choice in 75% of the cases.",2201.01409v2,https://arxiv.org/pdf/2201.01409v2
"Corrupting Data to Remove Deceptive Perturbation: Using Preprocessing
  Method to Improve System Robustness","Hieu Le, Hans Walker, Dung Tran, Peter Chin","Although deep neural networks have achieved great performance on
classification tasks, recent studies showed that well trained networks can be
fooled by adding subtle noises. This paper introduces a new approach to improve
neural network robustness by applying the recovery process on top of the
naturally trained classifier. In this approach, images will be intentionally
corrupted by some significant operator and then be recovered before passing
through the classifiers. SARGAN -- an extension on Generative Adversarial
Networks (GAN) is capable of denoising radar signals. This paper will show that
SARGAN can also recover corrupted images by removing the adversarial effects.
Our results show that this approach does improve the performance of naturally
trained networks.",2201.01399v1,https://arxiv.org/pdf/2201.01399v1
"Robust Natural Language Processing: Recent Advances, Challenges, and
  Future Directions","Marwan Omar, Soohyeon Choi, DaeHun Nyang, David Mohaisen","Recent natural language processing (NLP) techniques have accomplished high
performance on benchmark datasets, primarily due to the significant improvement
in the performance of deep learning. The advances in the research community
have led to great enhancements in state-of-the-art production systems for NLP
tasks, such as virtual assistants, speech recognition, and sentiment analysis.
However, such NLP systems still often fail when tested with adversarial
attacks. The initial lack of robustness exposed troubling gaps in current
models' language understanding capabilities, creating problems when NLP systems
are deployed in real life. In this paper, we present a structured overview of
NLP robustness research by summarizing the literature in a systemic way across
various dimensions. We then take a deep-dive into the various dimensions of
robustness, across techniques, metrics, embeddings, and benchmarks. Finally, we
argue that robustness should be multi-dimensional, provide insights into
current research, identify gaps in the literature to suggest directions worth
pursuing to address these gaps.",2201.00768v1,https://arxiv.org/pdf/2201.00768v1
"Robust Semi-supervised Federated Learning for Images Automatic
  Recognition in Internet of Drones","Zhe Zhang, Shiyao Ma, Zhaohui Yang, Zehui Xiong, Jiawen Kang, Yi Wu, Kejia Zhang, Dusit Niyato","Air access networks have been recognized as a significant driver of various
Internet of Things (IoT) services and applications. In particular, the aerial
computing network infrastructure centered on the Internet of Drones has set off
a new revolution in automatic image recognition. This emerging technology
relies on sharing ground truth labeled data between Unmanned Aerial Vehicle
(UAV) swarms to train a high-quality automatic image recognition model.
However, such an approach will bring data privacy and data availability
challenges. To address these issues, we first present a Semi-supervised
Federated Learning (SSFL) framework for privacy-preserving UAV image
recognition. Specifically, we propose model parameters mixing strategy to
improve the naive combination of FL and semi-supervised learning methods under
two realistic scenarios (labels-at-client and labels-at-server), which is
referred to as Federated Mixing (FedMix). Furthermore, there are significant
differences in the number, features, and distribution of local data collected
by UAVs using different camera modules in different environments, i.e.,
statistical heterogeneity. To alleviate the statistical heterogeneity problem,
we propose an aggregation rule based on the frequency of the client's
participation in training, namely the FedFreq aggregation rule, which can
adjust the weight of the corresponding local model according to its frequency.
Numerical results demonstrate that the performance of our proposed method is
significantly better than those of the current baseline and is robust to
different non-IID levels of client data.",2201.01230v1,https://arxiv.org/pdf/2201.01230v1
Testing the Robustness of a BiLSTM-based Structural Story Classifier,"Aftab Hussain, Sai Durga Prasad Nanduri, Sneha Seenuvasavarathan","The growing prevalence of counterfeit stories on the internet has fostered
significant interest towards fast and scalable detection of fake news in the
machine learning community. While several machine learning techniques for this
purpose have emerged, we observe that there is a need to evaluate the impact of
noise on these techniques' performance, where noise constitutes news articles
being mistakenly labeled as fake (or real). This work takes a step in that
direction, where we examine the impact of noise on a state-of-the-art,
structural model based on BiLSTM (Bidirectional Long-Short Term Model) for fake
news detection, Hierarchical Discourse-level Structure for Fake News Detection
by Karimi and Tang (Reference no. 9).",2201.02733v2,https://arxiv.org/pdf/2201.02733v2
Improving Out-of-Distribution Robustness via Selective Augmentation,"Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, Chelsea Finn","Machine learning algorithms typically assume that training and test examples
are drawn from the same distribution. However, distribution shift is a common
problem in real-world applications and can cause models to perform dramatically
worse at test time. In this paper, we specifically consider the problems of
subpopulation shifts (e.g., imbalanced data) and domain shifts. While prior
works often seek to explicitly regularize internal representations or
predictors of the model to be domain invariant, we instead aim to learn
invariant predictors without restricting the model's internal representations
or predictors. This leads to a simple mixup-based technique which learns
invariant predictors via selective augmentation called LISA. LISA selectively
interpolates samples either with the same labels but different domains or with
the same domain but different labels. Empirically, we study the effectiveness
of LISA on nine benchmarks ranging from subpopulation shifts to domain shifts,
and we find that LISA consistently outperforms other state-of-the-art methods
and leads to more invariant predictors. We further analyze a linear setting and
theoretically show how LISA leads to a smaller worst-group error.",2201.00299v3,https://arxiv.org/pdf/2201.00299v3
Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels,"Enyan Dai, Wei Jin, Hui Liu, Suhang Wang","Graph Neural Networks (GNNs) have shown their great ability in modeling graph
structured data. However, real-world graphs usually contain structure noises
and have limited labeled nodes. The performance of GNNs would drop
significantly when trained on such graphs, which hinders the adoption of GNNs
on many applications. Thus, it is important to develop noise-resistant GNNs
with limited labeled nodes. However, the work on this is rather limited.
Therefore, we study a novel problem of developing robust GNNs on noisy graphs
with limited labeled nodes. Our analysis shows that both the noisy edges and
limited labeled nodes could harm the message-passing mechanism of GNNs. To
mitigate these issues, we propose a novel framework which adopts the noisy
edges as supervision to learn a denoised and dense graph, which can down-weight
or eliminate noisy edges and facilitate message passing of GNNs to alleviate
the issue of limited labeled nodes. The generated edges are further used to
regularize the predictions of unlabeled nodes with label smoothness to better
train GNNs. Experimental results on real-world datasets demonstrate the
robustness of the proposed framework on noisy graphs with limited labeled
nodes.",2201.00232v1,https://arxiv.org/pdf/2201.00232v1
"Rethinking Feature Uncertainty in Stochastic Neural Networks for
  Adversarial Robustness","Hao Yang, Min Wang, Zhengfei Yu, Yun Zhou","It is well-known that deep neural networks (DNNs) have shown remarkable
success in many fields. However, when adding an imperceptible magnitude
perturbation on the model input, the model performance might get rapid
decrease. To address this issue, a randomness technique has been proposed
recently, named Stochastic Neural Networks (SNNs). Specifically, SNNs inject
randomness into the model to defend against unseen attacks and improve the
adversarial robustness. However, existed studies on SNNs mainly focus on
injecting fixed or learnable noises to model weights/activations. In this
paper, we find that the existed SNNs performances are largely bottlenecked by
the feature representation ability. Surprisingly, simply maximizing the
variance per dimension of the feature distribution leads to a considerable
boost beyond all previous methods, which we named maximize feature distribution
variance stochastic neural network (MFDV-SNN). Extensive experiments on
well-known white- and black-box attacks show that MFDV-SNN achieves a
significant improvement over existing methods, which indicates that it is a
simple but effective method to improve model robustness.",2201.00148v1,https://arxiv.org/pdf/2201.00148v1
