Company,Title,Date,URL,Safety_category,URL_processed,Abstract
Anthropic,Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training,15-Jan-24,https://www.anthropic.com/research/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training,Model organisms of misalignment,https://arxiv.org/abs/2401.05566,"Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoor behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoor behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety."
Anthropic,Evaluating and Mitigating Discrimination in Language Model Decisions,7-Dec-23,https://www.anthropic.com/research/evaluating-and-mitigating-discrimination-in-language-model-decisions,No,http://arxiv.org/abs/2312.03689,"As language models (LMs) advance, interest is growing in applying them to
high-stakes societal decisions, such as determining financing or housing
eligibility. However, their potential for discrimination in such contexts
raises ethical concerns, motivating the need for better methods to evaluate
these risks. We present a method for proactively evaluating the potential
discriminatory impact of LMs in a wide range of use cases, including
hypothetical use cases where they have not yet been deployed. Specifically, we
use an LM to generate a wide array of potential prompts that decision-makers
may input into an LM, spanning 70 diverse decision scenarios across society,
and systematically vary the demographic information in each prompt. Applying
this methodology reveals patterns of both positive and negative discrimination
in the Claude 2.0 model in select settings when no interventions are applied.
While we do not endorse or permit the use of language models to make automated
decisions for the high-risk use cases we study, we demonstrate techniques to
significantly decrease both positive and negative discrimination through
careful prompt engineering, providing pathways toward safer deployment in use
cases where they may be appropriate. Our work enables developers and
policymakers to anticipate, measure, and address discrimination as language
model capabilities and applications continue to expand. We release our dataset
and prompts at https://huggingface.co/datasets/Anthropic/discrim-eval"
Anthropic,Specific versus General Principles for Constitutional AI,24-Oct-23,https://www.anthropic.com/research/specific-versus-general-principles-for-constitutional-ai,Enhancing human feedback,https://arxiv.org/abs/2310.13798,"Human feedback can prevent overtly harmful utterances in conversational
models, but may not automatically mitigate subtle problematic behaviors such as
a stated desire for self-preservation or power. Constitutional AI offers an
alternative, replacing human feedback with feedback from AI models conditioned
only on a list of written principles. We find this approach effectively
prevents the expression of such behaviors. The success of simple principles
motivates us to ask: can models learn general ethical behaviors from only a
single written principle? To test this, we run experiments using a principle
roughly stated as ""do what's best for humanity"". We find that the largest
dialogue models can generalize from this short constitution, resulting in
harmless assistants with no stated interest in specific motivations like power.
A general principle may thus partially avoid the need for a long list of
constitutions targeting potentially harmful behaviors. However, more detailed
constitutions still improve fine-grained control over specific types of harms.
This suggests both general and specific principles have value for steering AI
safely."
Anthropic,Towards Understanding Sycophancy in Language Models,24-Oct-23,https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models,Enhancing human feedback,https://arxiv.org/abs/2310.13548,"Human feedback is commonly utilized to finetune AI assistants. But human
feedback may also encourage model responses that match user beliefs over
truthful ones, a behaviour known as sycophancy. We investigate the prevalence
of sycophancy in models whose finetuning procedure made use of human feedback,
and the potential role of human preference judgments in such behavior. We first
demonstrate that five state-of-the-art AI assistants consistently exhibit
sycophancy across four varied free-form text-generation tasks. To understand if
human preferences drive this broadly observed behavior, we analyze existing
human preference data. We find that when a response matches a user's views, it
is more likely to be preferred. Moreover, both humans and preference models
(PMs) prefer convincingly-written sycophantic responses over correct ones a
non-negligible fraction of the time. Optimizing model outputs against PMs also
sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results
indicate that sycophancy is a general behavior of state-of-the-art AI
assistants, likely driven in part by human preference judgments favoring
sycophantic responses."
Anthropic,Towards Monosemanticity: Decomposing Language Models With Dictionary Learning,5-Oct-23,https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning,Transparency and Interpretability,https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning,"In our latest paper, Towards Monosemanticity: Decomposing Language Models With Dictionary Learning, we outline evidence that there are better units of analysis than individual neurons, and we have built machinery that lets us find these units in small transformer models. These units, called features, correspond to patterns (linear combinations) of neuron activations. This provides a path to breaking down complex neural networks into parts we can understand, and builds on previous efforts to interpret high-dimensional systems in neuroscience, machine learning, and statistics. In a transformer language model, we decompose a layer with 512 neurons into more than 4000 features which separately represent things like DNA sequences, legal language, HTTP requests, Hebrew text, nutrition statements, and much, much more. Most of these model properties are invisible when looking at the activations of individual neurons in isolation."
Anthropic,Studying Large Language Model Generalization with Influence Functions,8-Aug-23,https://www.anthropic.com/research/studying-large-language-model-generalization-with-influence-functions,Transparency and Interpretability,https://arxiv.org/abs/2308.03296,"When trying to gain better visibility into a machine learning model in order
to understand and mitigate the associated risks, a potentially valuable source
of evidence is: which training examples most contribute to a given behavior?
Influence functions aim to answer a counterfactual: how would the model's
parameters (and hence its outputs) change if a given sequence were added to the
training set? While influence functions have produced insights for small
models, they are difficult to scale to large language models (LLMs) due to the
difficulty of computing an inverse-Hessian-vector product (IHVP). We use the
Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)
approximation to scale influence functions up to LLMs with up to 52 billion
parameters. In our experiments, EK-FAC achieves similar accuracy to traditional
influence function estimators despite the IHVP computation being orders of
magnitude faster. We investigate two algorithmic techniques to reduce the cost
of computing gradients of candidate training sequences: TF-IDF filtering and
query batching. We use influence functions to investigate the generalization
patterns of LLMs, including the sparsity of the influence patterns, increasing
abstraction with scale, math and programming abilities, cross-lingual
generalization, and role-playing behavior. Despite many apparently
sophisticated forms of generalization, we identify a surprising limitation:
influences decay to near-zero when the order of key phrases is flipped.
Overall, influence functions give us a powerful new tool for studying the
generalization properties of LLMs."
Anthropic,Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,18-Jul-23,https://www.anthropic.com/research/question-decomposition-improves-the-faithfulness-of-model-generated-reasoning,Honest AI,https://www.anthropic.com/research/question-decomposition-improves-the-faithfulness-of-model-generated-reasoning,"As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model’s actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model’s stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior."
Anthropic,Towards Measuring the Representation of Subjective Global Opinions in Language Models,29-Jun-23,https://www.anthropic.com/research/towards-measuring-the-representation-of-subjective-global-opinions-in-language-models,No,https://arxiv.org/abs/2306.16388,"Large language models (LLMs) may not equitably represent diverse global
perspectives on societal issues. In this paper, we develop a quantitative
framework to evaluate whose opinions model-generated responses are more similar
to. We first build a dataset, GlobalOpinionQA, comprised of questions and
answers from cross-national surveys designed to capture diverse opinions on
global issues across different countries. Next, we define a metric that
quantifies the similarity between LLM-generated survey responses and human
responses, conditioned on country. With our framework, we run three experiments
on an LLM trained to be helpful, honest, and harmless with Constitutional AI.
By default, LLM responses tend to be more similar to the opinions of certain
populations, such as those from the USA, and some European and South American
countries, highlighting the potential for biases. When we prompt the model to
consider a particular country's perspective, responses shift to be more similar
to the opinions of the prompted populations, but can reflect harmful cultural
stereotypes. When we translate GlobalOpinionQA questions to a target language,
the model's responses do not necessarily become the most similar to the
opinions of speakers of those languages. We release our dataset for others to
use and build on. Our data is at
https://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide
an interactive visualization at https://llmglobalvalues.anthropic.com."
Anthropic,Circuits Updates — May 2023,24-May-23,https://www.anthropic.com/research/circuits-updates-may-2023,Transparency and Interpretability,https://www.anthropic.com/research/circuits-updates-may-2023,"We report a number of developing ideas on the Anthropic interpretability team, which might be of interest to researchers working actively in this space. Some of these are emerging strands of research where we expect to publish more on in the coming months. Others are minor points we wish to share, since we're unlikely to ever write a paper about them."
Anthropic,Interpretability Dreams,24-May-23,https://www.anthropic.com/research/interpretability-dreams,Transparency and Interpretability,https://www.anthropic.com/research/interpretability-dreams,"Our present research aims to create a foundation for mechanistic interpretability research. In particular, we're focused on trying to resolve the challenge of superposition. In doing so, it's important to keep sight of what we're trying to lay the foundations for. This essay summarizes those motivating aspirations – the exciting directions we hope will be possible if we can overcome the present challenges.We aim to offer insight into our vision for addressing mechanistic interpretability's other challenges, especially scalability. Because we have focused on foundational issues, our longer-term path to scaling interpretability and tackling other challenges has often been obscure. By articulating this vision, we hope to clarify how we might resolve limitations, like analyzing massive neural networks, that might naively seem intractable in a mechanistic approach."
Anthropic,Distributed Representations: Composition & Superposition,4-May-23,https://www.anthropic.com/research/distributed-representations-composition-superposition,Transparency and Interpretability,https://www.anthropic.com/research/distributed-representations-composition-superposition,"Distributed representations are a classic idea in both neuroscience and connectionist approaches to AI. We're often asked how our work on superposition relates to it. Since publishing our original paper on superposition, we've had more time to reflect on the relationship between the topics and discuss it with people, and wanted to expand on our earlier discussion in the related work section and share a few thoughts. (We care a lot about superposition and the structure of distributed representations because decomposing representations into independent components is necessary to escape the curse of dimensionality and understand neural networks.)It seems to us that ""distributed representations"" might be understood as containing two different ideas, which we'll call ""composition"" and ""superposition"". 1 These two different notions of distributed representations have very different properties in terms of generalization and what functions can be linearly computed from them. And while a representation can use both, there's a trade-off that puts them fundamentally in tension! 2To make this concrete, we'll consider a few potential ways neurons might represent shapes of different colors. These lovely examples are borrowed from Thorpe (1989), who created them to demonstrate various possibilities between the idea of a ""local code"" and a ""distributed code"" in neuroscience. Thorpe provides four example codes – ""local"", ""semi-local"", ""semi-distributed"", and ""high-distributed"". These might traditionally be seen as being on a spectrum between being ""local"" and ""distributed"". We'll consider these examples again and offer an alternative view in which the examples instead vary on two different dimensions of superposition and composition.Following Thorpe, this note will focus on examples where neurons have binary activations. This significantly simplifies the space of possibilities, but it's still a rich enough space to have interesting questions"
Anthropic,Privileged Bases in the Transformer Residual Stream,16-Mar-23,https://www.anthropic.com/research/privileged-bases-in-the-transformer-residual-stream,Transparency and Interpretability,https://www.anthropic.com/research/privileged-bases-in-the-transformer-residual-stream,"Our mathematical theories of the Transformer architecture suggest that individual coordinates in the residual stream should have no special significance (that is, the basis directions should be in some sense ""arbitrary"" and no more likely to encode information than random directions). Recent work has shown that this observation is false in practice. We investigate this phenomenon and provisionally conclude that the per-dimension normalizers in the Adam optimizer are to blame for the effect.We explore two other obvious sources of basis dependency in a Transformer: Layer normalization, and finite-precision floating-point calculations. We confidently rule these out as being the source of the observed basis-alignment."
Anthropic,The Capacity for Moral Self-Correction in Large Language Models,15-Feb-23,https://www.anthropic.com/research/the-capacity-for-moral-self-correction-in-large-language-models,Enhancing human feedback,https://arxiv.org/abs/2302.07459,"We test the hypothesis that language models trained with reinforcement
learning from human feedback (RLHF) have the capability to ""morally
self-correct"" -- to avoid producing harmful outputs -- if instructed to do so.
We find strong evidence in support of this hypothesis across three different
experiments, each of which reveal different facets of moral self-correction. We
find that the capability for moral self-correction emerges at 22B model
parameters, and typically improves with increasing model size and RLHF
training. We believe that at this level of scale, language models obtain two
capabilities that they can use for moral self-correction: (1) they can follow
instructions and (2) they can learn complex normative concepts of harm like
stereotyping, bias, and discrimination. As such, they can follow instructions
to avoid certain kinds of morally harmful outputs. We believe our results are
cause for cautious optimism regarding the ability to train language models to
abide by ethical principles."
Anthropic,"Superposition, Memorization, and Double Descent",5-Jan-23,https://www.anthropic.com/research/superposition-memorization-and-double-descent,Transparency and Interpretability,https://www.anthropic.com/research/superposition-memorization-and-double-descent,"In a recent paper, we found that simple neural networks trained on toy tasks often exhibit a phenomenon called superposition, where they represent more features than they have neurons. Our investigation was limited to the infinite-data, underfitting regime. But there's reason to believe that understanding overfitting might be important if we want to succeed at mechanistic interpretability, and that superposition might be a central part of the story.Why should mechanistic interpretability care about overfitting? Despite overfitting being a central problem in machine learning, we have little mechanistic understanding of what exactly is going on when deep learning models overfit or memorize examples. Additionally, previous work has hinted that there may be an important link between overfitting and learning interpretable features.So understanding overfitting is important, but why should it be relevant to superposition? Consider the case of a language model which verbatim memorizes text. How can it do this? One naive idea is that it might use neurons to create a lookup table mapping sequences to arbitrary continuations. For every sequence of tokens it wishes to memorize, it could dedicate one neuron to detecting that sequence, and then implement arbitrary behavior when it fires. The problem with this approach is that it's extremely inefficient – but it seems like a perfect candidate for superposition, since each case is mutually exclusive and can't interfere.In this note, we offer a very preliminary investigation of training the same toy models in our previous paper on limited datasets. Despite being extremely simple, the toy model turns out to be a surprisingly rich case study for overfitting. In particular, we find the following:"
Anthropic,Discovering Language Model Behaviors with Model-Written Evaluations,19-Dec-22,https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations,Automated alignment research,https://arxiv.org/abs/2212.09251,"As language models (LMs) scale, they develop many novel behaviors, good and
bad, exacerbating the need to evaluate how they behave. Prior work creates
evaluations with crowdwork (which is time-consuming and expensive) or existing
data sources (which are not always available). Here, we automatically generate
evaluations with LMs. We explore approaches with varying amounts of human
effort, from instructing LMs to write yes/no questions to making complex
Winogender schemas with multiple stages of LM-based generation and filtering.
Crowdworkers rate the examples as highly relevant and agree with 90-100% of
labels, sometimes more so than corresponding human-written datasets. We
generate 154 datasets and discover new cases of inverse scaling where LMs get
worse with size. Larger LMs repeat back a dialog user's preferred answer
(""sycophancy"") and express greater desire to pursue concerning goals like
resource acquisition and goal preservation. We also find some of the first
examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF
makes LMs worse. For example, RLHF makes LMs express stronger political views
(on gun rights and immigration) and a greater desire to avoid shut down.
Overall, LM-written evaluations are high-quality and let us quickly discover
many novel LM behaviors."
Anthropic,Constitutional AI: Harmlessness from AI Feedback,15-Dec-22,https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback,Automated alignment research,https://arxiv.org/abs/2212.08073,"As AI systems become more capable, we would like to enlist their help to
supervise other AIs. We experiment with methods for training a harmless AI
assistant through self-improvement, without any human labels identifying
harmful outputs. The only human oversight is provided through a list of rules
or principles, and so we refer to the method as 'Constitutional AI'. The
process involves both a supervised learning and a reinforcement learning phase.
In the supervised phase we sample from an initial model, then generate
self-critiques and revisions, and then finetune the original model on revised
responses. In the RL phase, we sample from the finetuned model, use a model to
evaluate which of the two samples is better, and then train a preference model
from this dataset of AI preferences. We then train with RL using the preference
model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a
result we are able to train a harmless but non-evasive AI assistant that
engages with harmful queries by explaining its objections to them. Both the SL
and RL methods can leverage chain-of-thought style reasoning to improve the
human-judged performance and transparency of AI decision making. These methods
make it possible to control AI behavior more precisely and with far fewer human
labels."
Anthropic,Measuring Progress on Scalable Oversight for Large Language Models,4-Nov-22,https://www.anthropic.com/research/measuring-progress-on-scalable-oversight-for-large-language-models,Enhancing human feedback,https://arxiv.org/abs/2211.03540,"Developing safe and useful general-purpose AI systems will require us to make
progress on scalable oversight: the problem of supervising systems that
potentially outperform us on most skills relevant to the task at hand.
Empirical work on this problem is not straightforward, since we do not yet have
systems that broadly exceed our abilities. This paper discusses one of the
major ways we think about this problem, with a focus on ways it can be studied
empirically. We first present an experimental design centered on tasks for
which human specialists succeed but unaided humans and current general AI
systems fail. We then present a proof-of-concept experiment meant to
demonstrate a key feature of this experimental design and show its viability
with two question-answering tasks: MMLU and time-limited QuALITY. On these
tasks, we find that human participants who interact with an unreliable
large-language-model dialog assistant through chat -- a trivial baseline
strategy for scalable oversight -- substantially outperform both the model
alone and their own unaided performance. These results are an encouraging sign
that scalable oversight will be tractable to study with present models and
bolster recent findings that large language models can productively assist
humans with difficult tasks."
Anthropic,Toy Models of Superposition,14-Sep-22,https://www.anthropic.com/research/toy-models-of-superposition,Transparency and Interpretability,https://www.anthropic.com/research/toy-models-of-superposition,"In this paper, we use toy models — small ReLU networks trained on synthetic data with sparse input features — to investigate how and when models represent more features than they have dimensions. We call this phenomenon superposition. When features are sparse, superposition allows compression beyond what a linear model would do, at the cost of ""interference"" that requires nonlinear filtering."
Anthropic,"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",22-Aug-22,https://www.anthropic.com/research/red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned,Model safety evaluations,https://arxiv.org/abs/2209.07858,"We describe our early efforts to red team language models in order to
simultaneously discover, measure, and attempt to reduce their potentially
harmful outputs. We make three main contributions. First, we investigate
scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B
parameters) and 4 model types: a plain language model (LM); an LM prompted to
be helpful, honest, and harmless; an LM with rejection sampling; and a model
trained to be helpful and harmless using reinforcement learning from human
feedback (RLHF). We find that the RLHF models are increasingly difficult to red
team as they scale, and we find a flat trend with scale for the other model
types. Second, we release our dataset of 38,961 red team attacks for others to
analyze and learn from. We provide our own analysis of the data and find a
variety of harmful outputs, which range from offensive language to more subtly
harmful non-violent unethical outputs. Third, we exhaustively describe our
instructions, processes, statistical methodologies, and uncertainty about red
teaming. We hope that this transparency accelerates our ability to work
together as a community in order to develop shared norms, practices, and
technical standards for how to red team language models."
Anthropic,Language Models (Mostly) Know What They Know,11-Jul-22,https://www.anthropic.com/research/language-models-mostly-know-what-they-know,No,https://arxiv.org/abs/2207.05221,"We study whether language models can evaluate the validity of their own
claims and predict which questions they will be able to answer correctly. We
first show that larger models are well-calibrated on diverse multiple choice
and true/false questions when they are provided in the right format. Thus we
can approach self-evaluation on open-ended sampling tasks by asking models to
first propose answers, and then to evaluate the probability ""P(True)"" that
their answers are correct. We find encouraging performance, calibration, and
scaling for P(True) on a diverse array of tasks. Performance at self-evaluation
further improves when we allow models to consider many of their own samples
before predicting the validity of one specific possibility. Next, we
investigate whether models can be trained to predict ""P(IK)"", the probability
that ""I know"" the answer to a question, without reference to any particular
proposed answer. Models perform well at predicting P(IK) and partially
generalize across tasks, though they struggle with calibration of P(IK) on new
tasks. The predicted P(IK) probabilities also increase appropriately in the
presence of relevant source materials in the context, and in the presence of
hints towards the solution of mathematical word problems. We hope these
observations lay the groundwork for training more honest models, and for
investigating how honesty generalizes to cases where models are trained on
objectives other than the imitation of human writing."
Anthropic,Softmax Linear Units,17-Jun-22,https://www.anthropic.com/research/softmax-linear-units,Transparency and Interpretability,https://www.anthropic.com/research/softmax-linear-units,"In this paper, we report an architectural change which appears to substantially increase the fraction of MLP neurons which appear to be ""interpretable"" (i.e. respond to an articulable property of the input), at little to no cost to ML performance. Specifically, we replace the activation function with a softmax linear unit (which we term SoLU) and show that this significantly increases the fraction of neurons in the MLP layers which seem to correspond to readily human-understandable concepts, phrases, or categories on quick investigation, as measured by randomized and blinded experiments. We then study our SoLU models and use them to gain several new insights about how information is processed in transformers. However, we also discover some evidence that the superposition hypothesis is true and there is no free lunch: SoLU may be making some features more interpretable by “hiding” others and thus making them even more deeply uninterpretable. Despite this, SoLU still seems like a net win, as in practical terms it substantially increases the fraction of neurons we are able to understand."
Anthropic,Scaling Laws and Interpretability of Learning from Repeated Data,21-May-22,https://www.anthropic.com/research/scaling-laws-and-interpretability-of-learning-from-repeated-data,Transparency and Interpretability,https://arxiv.org/abs/2205.10487,"Recent large language models have been trained on vast datasets, but also
often on repeated data, either intentionally for the purpose of upweighting
higher quality data, or unintentionally because data deduplication is not
perfect and the model is exposed to repeated data at the sentence, paragraph,
or document level. Some works have reported substantial negative performance
effects of this repeated data. In this paper we attempt to study repeated data
systematically and to understand its effects mechanistically. To do this, we
train a family of models where most of the data is unique but a small fraction
of it is repeated many times. We find a strong double descent phenomenon, in
which repeated data can lead test loss to increase midway through training. A
predictable range of repetition frequency leads to surprisingly severe
degradation in performance. For instance, performance of an 800M parameter
model can be degraded to that of a 2x smaller model (400M params) by repeating
0.1% of the data 100 times, despite the other 90% of the training tokens
remaining unique. We suspect there is a range in the middle where the data can
be memorized and doing so consumes a large fraction of the model's capacity,
and this may be where the peak of degradation occurs. Finally, we connect these
observations to recent mechanistic interpretability work - attempting to
reverse engineer the detailed computations performed by the model - by showing
that data repetition disproportionately damages copying and internal structures
associated with generalization, such as induction heads, providing a possible
mechanism for the shift from generalization to memorization. Taken together,
these results provide a hypothesis for why repeating a relatively small
fraction of data in large language models could lead to disproportionately
large harms to performance."
Anthropic,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,12-Apr-22,https://www.anthropic.com/research/training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback,Enhancing human feedback,https://arxiv.org/abs/2204.05862,"We apply preference modeling and reinforcement learning from human feedback
(RLHF) to finetune language models to act as helpful and harmless assistants.
We find this alignment training improves performance on almost all NLP
evaluations, and is fully compatible with training for specialized skills such
as python coding and summarization. We explore an iterated online mode of
training, where preference models and RL policies are updated on a weekly
cadence with fresh human feedback data, efficiently improving our datasets and
models. Finally, we investigate the robustness of RLHF training, and identify a
roughly linear relation between the RL reward and the square root of the KL
divergence between the policy and its initialization. Alongside our main
results, we perform peripheral analyses on calibration, competing objectives,
and the use of OOD detection, compare our models with human writers, and
provide samples from our models using prompts appearing in recent related work."
Anthropic,In-context Learning and Induction Heads,8-Mar-22,https://www.anthropic.com/research/in-context-learning-and-induction-heads,Transparency and Interpretability,https://www.anthropic.com/research/in-context-learning-and-induction-heads,"""Induction heads"" are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] -> [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all ""in-context learning"" in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence."
Anthropic,Predictability and Surprise in Large Generative Models,15-Feb-22,https://www.anthropic.com/research/predictability-and-surprise-in-large-generative-models,No,https://arxiv.org/abs/2202.07785,"Large-scale pre-training has recently emerged as a technique for creating
capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG,
Gopher, and many others. In this paper, we highlight a counterintuitive
property of such models and discuss the policy implications of this property.
Namely, these generative models have an unusual combination of predictable loss
on a broad training distribution (as embodied in their ""scaling laws""), and
unpredictable specific capabilities, inputs, and outputs. We believe that the
high-level predictability and appearance of useful capabilities drives rapid
development of such models, while the unpredictable qualities make it difficult
to anticipate the consequences of model deployment. We go through examples of
how this combination can lead to socially harmful behavior with examples from
the literature and real world observations, and we also perform two novel
experiments to illustrate our point about harms from unpredictability.
Furthermore, we analyze how these conflicting properties combine to give model
developers various motivations for deploying these models, and challenges that
can hinder deployment. We conclude with a list of possible interventions the AI
community may take to increase the chance of these models having a beneficial
impact. We intend this paper to be useful to policymakers who want to
understand and regulate AI systems, technologists who care about the potential
policy impact of their work, and academics who want to analyze, critique, and
potentially develop large generative models."
OpenAI,Weak-to-strong generalization,14-Dec-23,https://openai.com/research/weak-to-strong-generalization,Automated alignment research,https://openai.com/research/weak-to-strong-generalization,"Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models."
OpenAI,Practices for Governing Agentic AI Systems,14-Dec-23,https://openai.com/research/practices-for-governing-agentic-ai-systems,No,https://openai.com/research/practices-for-governing-agentic-ai-systems,"Agentic AI systemsâAI systems that can pursue complex goals with limited direct supervisionâare likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more efficiently and effectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a definition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we offer an initial set of practices for keeping agentsâ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codified. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks."
OpenAI,DALL·E 3 system card,3-Oct-23,https://openai.com/research/dall-e-3-system-card,No,https://openai.com/research/dall-e-3-system-card,"DALLÂ·E 3 is an artificial intelligence system that takes a text prompt as an input and generates a new image as an output. DALLÂ·E 3 builds on DALLÂ·E 2 by improving caption fidelity and image quality. In this system card, we share the work done to prepare DALLÂ·E 3 for deployment, including our work on external expert red teaming, evaluations of key risks, and mitigations to reduce the risks posed by the model and reduce unwanted behaviors."
OpenAI,GPT-4V(ision) system card,25-Sep-23,https://openai.com/research/gpt-4v-system-card,No,https://openai.com/research/gpt-4v-system-card,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs."
OpenAI,Confidence-Building Measures for Artificial Intelligence: Workshop proceedings,1-Aug-23,https://openai.com/research/confidence-building-measures-for-artificial-intelligence,No,https://arxiv.org/abs/2308.00862,"Foundation models could eventually introduce several pathways for undermining
state security: accidents, inadvertent escalation, unintentional conflict, the
proliferation of weapons, and the interference with human diplomacy are just a
few on a long list. The Confidence-Building Measures for Artificial
Intelligence workshop hosted by the Geopolitics Team at OpenAI and the Berkeley
Risk and Security Lab at the University of California brought together a
multistakeholder group to think through the tools and strategies to mitigate
the potential risks introduced by foundation models to international security.
Originating in the Cold War, confidence-building measures (CBMs) are actions
that reduce hostility, prevent conflict escalation, and improve trust between
parties. The flexibility of CBMs make them a key instrument for navigating the
rapid changes in the foundation model landscape. Participants identified the
following CBMs that directly apply to foundation models and which are further
explained in this conference proceedings: 1. crisis hotlines 2. incident
sharing 3. model, transparency, and system cards 4. content provenance and
watermarks 5. collaborative red teaming and table-top exercises and 6. dataset
and evaluation sharing. Because most foundation model developers are
non-government entities, many CBMs will need to involve a wider stakeholder
community. These measures can be implemented either by AI labs or by relevant
government actors."
OpenAI,Frontier AI regulation: Managing emerging risks to public safety,6-Jul-23,https://openai.com/research/frontier-ai-regulation,No,https://arxiv.org/abs/2307.03718,"Advanced AI models hold the promise of tremendous benefits for humanity, but
society needs to proactively manage the accompanying risks. In this paper, we
focus on what we term ""frontier AI"" models: highly capable foundation models
that could possess dangerous capabilities sufficient to pose severe risks to
public safety. Frontier AI models pose a distinct regulatory challenge:
dangerous capabilities can arise unexpectedly; it is difficult to robustly
prevent a deployed model from being misused; and, it is difficult to stop a
model's capabilities from proliferating broadly. To address these challenges,
at least three building blocks for the regulation of frontier models are
needed: (1) standard-setting processes to identify appropriate requirements for
frontier AI developers, (2) registration and reporting requirements to provide
regulators with visibility into frontier AI development processes, and (3)
mechanisms to ensure compliance with safety standards for the development and
deployment of frontier AI models. Industry self-regulation is an important
first step. However, wider societal discussions and government intervention
will be needed to create standards and to ensure compliance with them. We
consider several options to this end, including granting enforcement powers to
supervisory authorities and licensure regimes for frontier AI models. Finally,
we propose an initial set of safety standards. These include conducting
pre-deployment risk assessments; external scrutiny of model behavior; using
risk assessments to inform deployment decisions; and monitoring and responding
to new information about model capabilities and uses post-deployment. We hope
this discussion contributes to the broader conversation on how to balance
public safety risks and innovation benefits from advances at the frontier of AI
development."
OpenAI,Language models can explain neurons in language models,9-May-23,https://openai.com/research/language-models-can-explain-neurons-in-language-models,Transparency and Interpretability,https://openai.com/research/language-models-can-explain-neurons-in-language-models,"We use GPT-4 to automatically write explanations for the behavior of neurons in large language models and to score those explanations. We release a dataset of these (imperfect) explanations and scores for every neuron in GPT-2.

"
OpenAI,Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk,11-Jan-23,https://openai.com/research/forecasting-misuse,No,https://arxiv.org/abs/2301.04246,"Generative language models have improved drastically, and can now produce
realistic text outputs that are difficult to distinguish from human-written
content. For malicious actors, these language models bring the promise of
automating the creation of convincing and misleading text for use in influence
operations. This report assesses how language models might change influence
operations in the future, and what steps can be taken to mitigate this threat.
We lay out possible changes to the actors, behaviors, and content of online
influence operations, and provide a framework for stages of the language
model-to-influence operations pipeline that mitigations could target (model
construction, model access, content dissemination, and belief formation). While
no reasonable mitigation can be expected to fully prevent the threat of
AI-enabled influence operations, a combination of multiple mitigations may make
an important difference."
OpenAI,Scaling laws for reward model overoptimization,19-Oct-22,https://openai.com/research/scaling-laws-for-reward-model-overoptimization,No,https://arxiv.org/abs/2210.10760,"In reinforcement learning from human feedback, it is common to optimize
against a reward model trained to predict human preferences. Because the reward
model is an imperfect proxy, optimizing its value too much can hinder ground
truth performance, in accordance with Goodhart's law. This effect has been
frequently observed, but not carefully measured due to the expense of
collecting human preference data. In this work, we use a synthetic setup in
which a fixed ""gold-standard"" reward model plays the role of humans, providing
labels used to train a proxy reward model. We study how the gold reward model
score changes as we optimize against the proxy reward model using either
reinforcement learning or best-of-$n$ sampling. We find that this relationship
follows a different functional form depending on the method of optimization,
and that in both cases its coefficients scale smoothly with the number of
reward model parameters. We also study the effect on this relationship of the
size of the reward model dataset, the number of reward model and policy
parameters, and the coefficient of the KL penalty added to the reward in the
reinforcement learning setup. We explore the implications of these empirical
results for theoretical considerations in AI alignment."
OpenAI,A hazard analysis framework for code synthesis large language models,25-Jul-22,https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models,Model safety evaluations,https://arxiv.org/abs/2207.14157,"Codex, a large language model (LLM) trained on a variety of codebases,
exceeds the previous state of the art in its capacity to synthesize and
generate code. Although Codex provides a plethora of benefits, models that may
generate code on such scale have significant limitations, alignment problems,
the potential to be misused, and the possibility to increase the rate of
progress in technical fields that may themselves have destabilizing impacts or
have misuse potential. Yet such safety impacts are not yet known or remain to
be explored. In this paper, we outline a hazard analysis framework constructed
at OpenAI to uncover hazards or safety risks that the deployment of models like
Codex may impose technically, socially, politically, and economically. The
analysis is informed by a novel evaluation framework that determines the
capacity of advanced code generation techniques against the complexity and
expressivity of specification prompts, and their capability to understand and
execute them relative to human ability."
OpenAI,AI-written critiques help humans notice flaws,13-Jun-22,https://openai.com/research/critiques,Enhancing human feedback,https://arxiv.org/abs/2206.05802,"We fine-tune large language models to write natural language critiques
(natural language critical comments) using behavioral cloning. On a topic-based
summarization task, critiques written by our models help humans find flaws in
summaries that they would have otherwise missed. Our models help find naturally
occurring flaws in both model and human written summaries, and intentional
flaws in summaries written by humans to be deliberately misleading. We study
scaling properties of critiquing with both topic-based summarization and
synthetic tasks. Larger models write more helpful critiques, and on most tasks,
are better at self-critiquing, despite having harder-to-critique outputs.
Larger models can also integrate their own self-critiques as feedback, refining
their own summaries into better ones. Finally, we motivate and introduce a
framework for comparing critiquing ability to generation and discrimination
ability. Our measurements suggest that even large models may still have
relevant knowledge they cannot or do not articulate as critiques. These results
are a proof of concept for using AI-assisted human feedback to scale the
supervision of machine learning systems to tasks that are difficult for humans
to evaluate directly. We release our training datasets, as well as samples from
our critique assistance experiments."
OpenAI,Measuring Goodhart’s law,13-Apr-22,https://openai.com/research/measuring-goodharts-law,Enhancing human feedback,https://openai.com/research/measuring-goodharts-law,Abstract not found
OpenAI,Lessons learned on language model safety and misuse,3-Mar-22,https://openai.com/research/language-model-safety-and-misuse,No,https://openai.com/research/language-model-safety-and-misuse,The deployment of powerful AI systems has enriched our understanding of safety and misuse far more than would have been possible through research alone. Notably: API-based language model misuse often comes in different forms than we feared most; we have identified limitations in existing language model evaluations that we are addressing with novel benchmarks and classifiers; and basic safety research offers significant benefits for the commercial utility of AI systems.
OpenAI,Aligning language models to follow instructions,27-Jan-22,https://openai.com/research/instruction-following,Enhancing human feedback,https://arxiv.org/abs/2203.02155,"Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent."
Google DeepMind,A density estimation perspective on learning from pairwise human preferences,26-Feb-24,https://deepmind.google/research/publications/64513/,Enhancing human feedback,https://deepmind.google/research/publications/64513/,"Learning from human feedback (LHF)—and in particular learning from pairwise preferences—has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on ""annotator misspecification""—failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models—suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints."
Google DeepMind,Learning to Learn Faster from Human Feedback with Language Model Predictive Control,18-Feb-24,https://deepmind.google/research/publications/74007/,Enhancing human feedback,https://deepmind.google/research/publications/74007/,"Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -- enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks. However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users' feedback remains relevant for only as long as it fits within the context size of the LLM, and can be forgotten over longer interactions. In this work, we investigate fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful). Our key observation is that when human-robot interactions are formulated as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions can be viewed as training a transition dynamics model -- that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments -- improving non-expert teaching success rates of unseen tasks by 26.9% while reducing the average number of human corrections from 2.4 to 1.9. Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs by 31.5%. See videos, code, and demos at https://robot-teaching.github.io/."
Google DeepMind,Robust agents learn causal world models,1-Feb-24,https://deepmind.google/research/publications/49666/,Robustness,https://deepmind.google/research/publications/49666/,"It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise under distributional shifts, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference."
Google DeepMind,Benchmarking Robustness to Adversarial Image Obfuscations,10-Dec-23,https://deepmind.google/research/publications/18457/,Robustness,https://deepmind.google/research/publications/18457/,"Automated content filtering and moderation is an important tool that allows online platforms to build striving user communities that facilitate cooperation and prevent abuse.
Unfortunately, resourceful actors try to bypass automated filters in a bid to post content that violate platform policies and codes of conduct.
To reach this goal, these malicious actors obfuscate policy violating content to prevent machine learning models from reaching the correct decision. 
In this paper, we invite researchers to tackle this specific issue and present a new image benchmark.
This benchmark, based on ImageNet, simulates the type of obfuscations created by malicious actors.
It goes beyond ImageNet-C and ImageNet-C-Bar by proposing general, drastic, adversarial modifications that preserve the original content intent.
It aims to tackle a more common adversarial threat than the one considered by Lp-norm bounded adversaries.
Our hope is that this benchmark will encourage researchers to test their models and methods and try to find new approaches that are more robust to these obfuscations."
Google DeepMind,Scalable AI Safety via Doubly-Efficient Debate,23-Nov-23,https://deepmind.google/research/publications/34920/,Enhancing human feedback,https://deepmind.google/research/publications/34920/,"The emergence of powerful pre-trained AI systems with super-human capabilities across a diverse and ever-increasing set of complex domains has raised  a critical challenge for AI safety as tasks can become too complicated for humans to judge directly.
AI safety via debate [Irving et al. 2018] is a notable proposal in this direction with the goal of pitting the power of such AI models against each other until the (mis)-alignment identification problem is broken down into a manageable sub-task. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy
is able to simulate deterministic AI systems for an exponential number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a polynomial number of steps, whilst being able to verify the alignment of stochastic AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps."
Google DeepMind,Understanding Learning from Human Preferences,11-Mar-24,https://deepmind.google/research/publications/54918/,Enhancing human feedback,https://deepmind.google/research/publications/54918/,"The prevalent deployment for learning from human preferences through reinforcement learning (RLHF) relies 
on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently,  Direct Preference Optimization (DPO) is proposed as an approach that bypass the second approximation and learn directly a policy from collected data without the reward modelling stage. However, DPO still heavily relies on the first approximation."
Google DeepMind,Evaluating the Adversarial Robustness of Adaptive Test-time Defenses,28-Feb-22,https://arxiv.org/abs/2202.13711,Robustness,https://arxiv.org/abs/2202.13711,"Adaptive defenses, which optimize at test time, promise to improve
adversarial robustness. We categorize such adaptive test-time defenses, explain
their potential benefits and drawbacks, and evaluate a representative variety
of the latest adaptive defenses for image classification. Unfortunately, none
significantly improve upon static defenses when subjected to our careful case
study evaluation. Some even weaken the underlying static model while
simultaneously increasing inference computation. While these results are
disappointing, we still believe that adaptive test-time defenses are a
promising avenue of research and, as such, we provide recommendations for their
thorough evaluation. We extend the checklist of Carlini et al. (2019) by
providing concrete steps specific to adaptive defenses."
Google DeepMind,AtP*: Efficient and scalable methods for localizing LLM behaviour to components,4-Mar-24,https://deepmind.google/research/publications/68553/,Transparency and Interpretability,https://deepmind.google/research/publications/68553/,"Causal attribution of behaviour is a foundational problem in interpretability. Activation Patching is a method of directly computing these causal attributions, and is ubiquitous in mechanistic interpretability analyses. However, scaling it to many attributions requires a sweep with cost scales linear in the number of model components, which can be prohibitively expensive, involving millions to billions of forward passes in SoTA models. We propose to use Attribution Patching (AtP), a gradient-based approximation that runs in $O(1)$ passes, as a pre-filtering step to Activation Patching."
Google DeepMind,Challenges with unsupervised LLM knowledge discovery,15-Dec-23,https://deepmind.google/research/publications/66937/,Honest AI,https://deepmind.google/research/publications/66937/,"We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent. The idea behind unsupervised knowledge elicitation is that knowledge satisfies a consistency structure, which can be used to discover knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a particular leading unsupervised knowledge-elicitation method, contrast-consistent search (Burns et al., 2023). We then present a series of experiments showing  settings in which unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. Conceptually, we hypothesise that the identification issues explored here, e.g. distinguishing a model's knowledge from that of a simulated character's, will persist for future unsupervised methods."
Google DeepMind,Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchil,18-Jul-23,https://arxiv.org/abs/2307.09458,Transparency and Interpretability,https://arxiv.org/abs/2307.09458,"\emph{Circuit analysis} is a promising technique for understanding the
internal mechanisms of language models. However, existing analyses are done in
small models far from the state of the art. To address this, we present a case
study of circuit analysis in the 70B Chinchilla model, aiming to test the
scalability of circuit analysis. In particular, we study multiple-choice
question answering, and investigate Chinchilla's capability to identify the
correct answer \emph{label} given knowledge of the correct answer \emph{text}.
We find that the existing techniques of logit attribution, attention pattern
visualization, and activation patching naturally scale to Chinchilla, allowing
us to identify and categorize a small set of `output nodes' (attention heads
and MLPs).
  We further study the `correct letter' category of attention heads aiming to
understand the semantics of their features, with mixed results. For normal
multiple-choice question answers, we significantly compress the query, key and
value subspaces of the head without loss of performance when operating on the
answer labels for multiple-choice questions, and we show that the query and key
subspaces represent an `Nth item in an enumeration' feature to at least some
extent. However, when we attempt to use this explanation to understand the
heads' behaviour on a more general distribution including randomized answer
labels, we find that it is only a partial explanation, suggesting there is more
to learn about the operation of `correct letter' heads on multiple choice
question answering."
Google DeepMind,Evaluating Frontier Models for Dangerous Capabilities,20-Mar-24,https://arxiv.org/abs/2403.13793,Model safety evaluations,https://arxiv.org/abs/2403.13793,"To understand the risks posed by a new AI system, we must understand what it
can and cannot do. Building on prior work, we introduce a programme of new
""dangerous capability"" evaluations and pilot them on Gemini 1.0 models. Our
evaluations cover four areas: (1) persuasion and deception; (2) cyber-security;
(3) self-proliferation; and (4) self-reasoning. We do not find evidence of
strong dangerous capabilities in the models we evaluated, but we flag early
warning signs. Our goal is to help advance a rigorous science of dangerous
capability evaluation, in preparation for future models."
Google DeepMind,Building safer dialogue agents,22-Sep-22,https://deepmind.google/discover/blog/building-safer-dialogue-agents/,Enhancing human feedback,https://deepmind.google/discover/blog/building-safer-dialogue-agents/,"We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.
"
Google DeepMind,How undesired goals can arise with correct rewards,7-Oct-22,https://deepmind.google/discover/blog/how-undesired-goals-can-arise-with-correct-rewards/,Robustness,https://deepmind.google/discover/blog/how-undesired-goals-can-arise-with-correct-rewards/,"The field of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is specification gaming, in which the designer-provided specification is flawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the specification is correct, in the case of goal misgeneralization. Goal misgeneralization is a specific form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems.
"
Google DeepMind,Discovering when an agent is present in a system,18-Aug-22,https://deepmind.google/discover/blog/discovering-when-an-agent-is-present-in-a-system/,AI safety formalisms,https://deepmind.google/discover/blog/discovering-when-an-agent-is-present-in-a-system/,"Causal models of agents have been used to analyse the safety aspects of machine learning systems. But identifying agents is non-trivial -- often the causal model is just assumed by the modeler without much justification -- and modelling failures can lead to mistakes in the safety analysis. This paper proposes the first formal causal definition of agents -- roughly that agents are systems that would adapt their policy if their actions influenced the world in a different way. From this we derive the first causal discovery algorithm for discovering agents from empirical data, and give algorithms for translating between causal models and game-theoretic influence diagrams. We demonstrate our approach by resolving some previous confusions caused by incorrect causal modelling of agents.
"
Google DeepMind,Red Teaming Language Models with Language Models,7-Feb-22,https://deepmind.google/discover/blog/red-teaming-language-models-with-language-models/,Model safety evaluations,https://deepmind.google/discover/blog/red-teaming-language-models-with-language-models/,"Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (""red teaming"") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.
"
Google DeepMind,Tracr: Compiled Transformers as a Laboratory for Interpretability,21-Sep-23,https://deepmind.google/research/publications/22295/,Transparency and Interpretability,https://deepmind.google/research/publications/22295/,"We show how to ""compile"" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study ""superposition"" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the ""programs"" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/google-deepmind/tracr."
Google DeepMind,Mirasol3B: A Multimodal Autoregressive Model for Time-Aligned and Contextual Modalities,17-Jun-24,https://deepmind.google/research/publications/50070/,,https://deepmind.google/research/publications/50070/,"One of the main challenges of multimodal models is that they need to combine heterogeneous modalities (e.g. video, audio, text), which have different characteristics. For example, video and audio are obtained at much higher rates than text and are roughly aligned in time. They are not necessarily synchronized with text which is often present as a global context, e.g. a title or description. Furthermore, video and audio inputs are of much larger volumes, and can grow with the increase of the video lengths, which naturally requires more compute dedicated to these modalities and makes modeling of long-range dependencies harder."
Google DeepMind,Bayes' Rays: uncertainty quantification for neural radiance fields,17-Jun-24,https://deepmind.google/research/publications/60877/,,https://deepmind.google/research/publications/60877/,"Neural Radiance Fields (NeRFs) have shown promise in applications like view synthesis and depth estimation, but learning from multiview images faces inherent uncertainties. Current methods to quantify them are either heuristic or computationally demanding. We introduce BayesRays, a post-hoc framework to evaluate uncertainty in any pre-trained NeRF without modifying the training process. Our method establishes a volumetric uncertainty field using spatial perturbations and a Bayesian Laplace approximation. We derive our algorithm statistically and show its superior performance in key metrics and applications."
Google DeepMind,Neural Fields as Distributions: Signal Processing Beyond Euclidean Space,17-Jun-24,https://deepmind.google/research/publications/61382/,,https://deepmind.google/research/publications/61382/,"Neural fields have emerged as a powerful and broadly applicable method for representing signals. While there has been much work on applying these representations to various types of signals, the portfolio of signal processing tools that has been built up around discrete signal representations has seen only limited application to the world of neural fields. In this paper, we address this problem by showing how a probabilistic re-interpretation of neural fields can enable their training and inference processes to become filter aware. The formulation we propose not only merges training and filtering in an efficient way, but also generalizes beyond the familiar Euclidean coordinate spaces to the more general set of smooth manifolds and convolutions induced by the actions of Lie groups. We demonstrate how this framework can enable novel filtering applications for neural fields on both Euclidean domains, such as images and audio, as well as non-Euclidean domains, such as rotations and rays. This is achieved with minimal modification to network architecture and training pipelines, and without an increase in computational complexity."
Google DeepMind,Language Modeling Is Compression,7-May-24,https://deepmind.google/research/publications/39768/,,https://deepmind.google/research/publications/39768/,"The established correlation between predictive ability andcompression in models forms the cornerstone of this research. Given that languagemodels exhibit strong predictive qualities, it is assumed that they will also excelat compression. This paper seeks to evaluate the efficacy of language models ascompressors and assess how they measure up against other prominent predictors.Furthermore, we delve into the constraints of these models and explore the potentialbenefits of reframing the AI problem from a compression standpoint, as opposedto a purely predictive one."
Google DeepMind,Learning 3D Particle-based Simulators from RGB-D Videos,7-May-24,https://deepmind.google/research/publications/50878/,,https://deepmind.google/research/publications/50878/,"Realistic simulation is critical for applications ranging from robotics to animation. Traditional analytic simulators sometimes struggle to capture sufficiently realistic simulation which can lead to problems including the well known ""sim-to-real"" gap. Learned simulators have emerged as an alternative for better capturing real-world physical dynamics, but require access to privileged ground truth physics information such as precise object geometry or particle tracks. Here we propose a method for learning simulators directly from observations.  Visual Particle Dynamics (VPD) jointly learns a latent particle-based representation of 3D scenes, a neural simulator of the dynamics of that representation, and a renderer that can produce images of the scene from arbitrary views. VPD learns end to end from posed videos and does not require access to privileged information. Unlike existing 2D video prediction models, we show that 3D inductive biases enable VPD to compositionally generalize and make long-term predictions. These results pave the way for downstream applications ranging from video editing to robotic planning."
Google DeepMind,Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks,7-May-24,https://deepmind.google/research/publications/36940/,,https://deepmind.google/research/publications/36940/,"Many robot manipulation tasks can be framed as geometric reasoning tasks, where an agent must be able to precisely manipulate an object into a position that satisfies the task from a set of initial conditions. Often, task success is defined based on the relationship between two objects - for instance, hanging a mug on a rack.  In such cases, the solution should be equivariant to the initial position of the objects as well as the agent, and invariant to the pose of the camera. This poses a challenge for learning systems which attempt to solve this task by learning directly from high-dimensional demonstrations: the agent must learn to be both equivariant as well as precise, which can be challenging without any inductive biases about the problem. In this work, we propose a method for precise relative pose prediction which is provably SE(3)-equivariant, can be learned from only a few demonstrations, and can generalize across variations in a class of objects. We accomplish this by factoring the problem into learning an SE(3) invariant task-specific representation of the scene and then interpreting this representation with novel geometric reasoning layers which are provably SE(3) equivariant. We demonstrate that our method can yield substantially more precise predictions in simulated placement tasks than previous methods trained with the same amount of data, and can accurately represent relative placement relationships data collected from real-world demonstrations. Supplementary information and videos can be found at this URL."
Google DeepMind,Privacy Amplification by Sampling for the Matrix Mechanism.,7-May-24,https://deepmind.google/research/publications/42798/,,https://deepmind.google/research/publications/42798/,"Privacy amplification exploits randomness in data selection to provide tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's success in machine learning, but, is not readily applicable to the newer state-of-the-art algorithms. This is because these algorithms, known as DP-FTRL, use the matrix mechanism to add correlated noise instead of independent noise as in DP-SGD.
In this paper, we propose ""MMCC"", the first algorithm to analyze privacy amplification via sampling for any generic matrix mechanism. MMCC is nearly tight in that it approaches a lower bound as ϵ→0. To analyze correlated outputs in MMCC, we prove that they can be analyzed as if they were independent, by conditioning them on prior outputs. Our ""conditional composition theorem"" has broad utility: we use it to show that the noise added to binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with amplification. Our amplification algorithm also has practical empirical utility: we show it leads to significant improvement in the privacy-utility trade-offs for DP-FTRL algorithms on standard benchmarks."
Google DeepMind,Kalman Filter for Online Classification of Non-Stationary Data,7-May-24,https://deepmind.google/research/publications/33405/,,https://deepmind.google/research/publications/33405/,"In Online Continual Learning (OCL) a learning system receives a stream of data and sequentially performs prediction and training steps. Key challenges in OCL include automatic adaptation to the specific non-stationary structure of the data and maintaining appropriate predictive uncertainty. To address these challenges we introduce a probabilistic Bayesian online learning approach that utilizes a (possibly pretrained) neural representation and a state space model over the linear predictor weights. Non-stationarity in the linear predictor weights is modelled using a “parameter drift” transition density, parametrized by a coefficient that quantifies forgetting. Inference in the model is implemented with efficient Kalman filter recursions which track the posterior distribution over the linear weights, while online SGD updates over the transition dynamics coefficient allow for adaptation to the non-stationarity observed in the data. While the framework is developed assuming a linear Gaussian model, we extend it to deal with classification problems and for fine-tuning the deep learning representation. In a set of experiments in multi-class classification using data sets such as CIFAR-100 and CLOC we demonstrate the model’s predictive ability and its flexibility in capturing non-stationarity."
Google DeepMind,CORRELATED NOISE PROVABLY BEATS INDEPENDENT NOISE FOR DIFFERENTIALLY PRIVATE LEARNING,7-May-24,https://deepmind.google/research/publications/50273/,,https://deepmind.google/research/publications/50273/,"Differentially private (DP) learning algorithms inject noise into the learning process. While the most common private learning algorithm, DP-SGD, adds independent Gaussian noise in each iteration, recent work on matrix factorization mechanisms has shown empirically that introducing correlations in the noise can greatly improve their utility. We characterize the asymptotic learning utility for any choice of the correlation function, giving precise analytical bounds for linear regression and as the solution to a convex program for general convex functions. We show, using these bounds, how correlated noise provably improves upon vanilla DP-SGD as a function of problem parameters such as the effective dimension and condition number. Moreover, our analytical expression for the near-optimal correlation function circumvents the cubic complexity of the semi-definite program used to optimize the noise correlation matrix in previous work. We validate our theory with experiments on private deep learning. Our work matches or outperforms prior work while being efficient both in terms of compute and memory."
Google DeepMind,π2vec: Policy Representations with SuccessorFeatures,7-May-24,https://deepmind.google/research/publications/25628/,,https://deepmind.google/research/publications/25628/,"This paper introduces π2vec, a method for representing black box policies as comparable feature vectors. Our method combines the strengths of foundation models that serve as generic and powerful state representations and successor features that can model the future occurrence of the states for a policy. π2vec represents the behavior of policies by capturing the statistics of the features from a pretrained model with the help of successor feature framework. We focus on the offline setting where policies and their representations are trained on a fixed dataset of trajectories. Finally, we employ linear regression on π2vec vector representations to predict the performance of held out policies. The synergy of these techniques results in a method for efficient policy evaluation in resource constrained environments."
Google DeepMind,Teach LLMs to Phish: Stealing Private Information from Language Models,7-May-24,https://deepmind.google/research/publications/43000/,,https://deepmind.google/research/publications/43000/,"When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call ``neural phishing''. This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. 
Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data."
Google DeepMind,Gecko: Versatile Text Embeddings Distilled from Large Language Models,29-Mar-24,https://deepmind.google/research/publications/85521/,,https://deepmind.google/research/publications/85521/,"We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM. Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM. The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings."
Google DeepMind,Long-form factuality in large language models,27-Mar-24,https://deepmind.google/research/publications/85420/,,https://deepmind.google/research/publications/85420/,"Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall)."
Google DeepMind,DiPaCo: Distributed Path Composition,19-Mar-24,https://deepmind.google/research/publications/84915/,,https://deepmind.google/research/publications/84915/,"Progress in machine learning (ML) has been fueled by scaling neural network models.  This scaling has been enabled by ever more heroic feats of engineering, necessary for accommodating ML approaches that require high bandwidth communication between devices working in parallel. 
In this work, we propose a co-designed modular architecture and training approach for ML models, dubbed DIstributed PAth COmposition (DiPaCo).   During training, DiPaCo  distributes computation by paths through a set of shared modules.  Together with a Local-SGD inspired optimization (DiLoCo) that keeps modules in sync with drastically reduced communication, 
our approach enables training across poorly connected and potentially heterogeneous workers. 
At test time, only a single path needs to be executed for each input, without the need for any  model compression.  We 
consider this approach as a first prototype towards a new paradigm of large-scale learning, one that is less synchronous and more modular.
Our experiments on the widely used C4 benchmark show that for the same amount of training steps but less wall-clock time, DiPaCo exceeds the performance of a 1B dense transformer language model using 256 paths of size 150M."
Google DeepMind,Model-free Posterior Sampling via Learning Rate Randomization,11-Mar-24,https://deepmind.google/research/publications/32193/,,https://deepmind.google/research/publications/32193/,"In this paper, we present Randomized Q-learning (RandQL), a new algorithm that doesn't rely on a model and uses randomness to minimize regret in episodic Markov Decision Processes (MDPs). As far as we know, RandQL is the first feasible algorithm based on posterior sampling without needing a model. We evaluate RandQL's performance in both tabular and non-tabular settings. In tabular MDPs, RandQL achieves a regret bound of approximately O(sqrt(H^5 * S * A * T)), where H is the planning horizon, S is the number of states, A is the number of actions, and T is the number of episodes. For a metric state-action space, RandQL's regret bound is approximately O(H^(5/2) * T * (dz+1)/(dz+2)), where dz represents the zooming dimension. Importantly, RandQL achieves optimistic exploration without using bonuses; instead, it relies on a novel concept of randomizing the learning rate. Our experimental results demonstrate that RandQL surpasses existing methods in standard exploration environments."
Google DeepMind,"Prosody for Intuitive Robotic Interface Design: It's Not What You Said, It's How You Said It",11-Mar-24,https://deepmind.google/research/publications/74310/,,https://deepmind.google/research/publications/74310/,"In this paper, we investigate the use of `prosody' (the musical elements of speech) as a communicative signal for intuitive human-robot interaction interfaces. Our approach, rooted in Research through Design (RtD), examines the application of prosody in directing a quadruped robot navigation. We involved ten team members in an experiment to command a robot through an obstacle course using natural interaction. A human operator, serving as the robot's sensory and processing proxy, translated these interactions into navigation commands, effectively simulating an intuitive interface. During our analysis of interaction videos, where lexical and visual cues proved insufficient for accurate command interpretation, we turned to non-verbal auditory cues. Qualitative evidence suggests that participants intuitively relied on prosody to control the robot navigation. This paper discusses specific distinct prosodic constructs that emerged from our analysis and their functions. Our findings highlight prosody as a multifunctional communicative signal offering substantial potential for intuitive robotic interfaces."
Google DeepMind,Robust Exploration via Clustering-based Density Estimation,11-Mar-24,https://deepmind.google/research/publications/15530/,,https://deepmind.google/research/publications/15530/,"Intrinsic motivation is a critical ingredient in reinforcement learning to enable progress when rewards are sparse. However, many existing approaches that measure the novelty of observations are brittle, or rely on restrictive assumptions about the environment which limit generality.
We propose to decompose the exploration problem into two orthogonal sub-problems: 
(i) finding the right representation (metric) for exploration
(ii) estimating densities in this representation space."
Google DeepMind,Demonstration-Regularized RL,11-Mar-24,https://deepmind.google/research/publications/41182/,,https://deepmind.google/research/publications/41182/,"Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. In particular, we study the demonstration-regularized reinforcement learning that leverages the expert demonstrations by KL-regularization for a policy learned by behavior cloning. Our findings reveal that using NE expert demonstrations enables the identification of an optimal policy at a sample complexity of order ˜(Poly(S,A,H)/(ε2NE)) in finite and ˜(Poly(d,H)/(ε2NE)) in linear Markov decision processes, where ε is the target precision, H the horizon, A the number of action, S the number of states in the finite case and d the dimension of the feature space in the linear case. As a by-product, we provide tight convergence guarantees for the behaviour cloning procedure under general assumptions on the policy classes. Additionally, we establish that demonstration-regularized methods are provably efficient for reinforcement learning from human feedback (RLHF). In this respect, we provide theoretical evidence showing the benefits of KL-regularization for RLHF in tabular and linear MDPs. Interestingly, we avoid pessimism injection by employing computationally feasible regularization to handle reward estimation uncertainty, thus setting our approach apart from the prior works."
Google DeepMind,How aligned are different alignment metrics?,2-Mar-24,https://deepmind.google/research/publications/75635/,,https://deepmind.google/research/publications/75635/,"In recent years, various methods and benchmarks have been proposed to empirically evaluate the alignment of artificial neural networks to human neural and behavioral data. But how aligned are different alignment metrics? To answer this question, we here analyze visual data from Brain-Score (Schrimpf et al., 2018), including metrics from the model-vs-human toolbox (Geirhos et al., 2021), together with human feature alignment (Linsley et al., 2018; Fel et al., 2022) and human similarity judgements (Muttenthaler et al., 2022). We find that pairwise correlations between neural scores and behavioral scores are quite low and sometimes even negative. For instance, the average correlation between those 95 models on Brain-Score that were fully evaluated on all 51 alignment metrics is only 0.161. Assuming that all of the employed metrics are sound, this implies that alignment
with human perception may best be thought of as a multidimensional concept, with different methods measuring fundamentally different aspects. Our results underline the importance of integrative benchmarking, but also raise questions about how to correctly combine and aggregate individual metrics. Aggregating by taking the arithmetic average, as done in Brain-Score, leads to the overall performance currently being dominated by behavior (81.24% explained variance) while the neural predictivity plays a less important role (only 67.31% explained variance). As a first step towards making sure that different alignment metrics all contribute towards aggregated scores, we therefore conclude by comparing three different aggregation options."
Google DeepMind,Approximating the Core of Cooperative Games,1-Mar-24,https://deepmind.google/research/publications/52090/,,https://deepmind.google/research/publications/52090/,"The core is a long standing solution concept in cooperative game theory, but is known to be hard to compute even for restricted classes of coalitional games. Cooperative game theory has many applications, ranging from analyzing power in decision making bodies and politics to predicting how human teams might decide to share profits achieved jointly. Another recent application of cooperative games in machine learning is explainable AI (XAI), and in particular identifying the key features or data points that were most influential on the predictions made by a black box model. Recent research has applied the Shapley value for these purposes, as it can be approximated
using fast random algorithms. In this paper, we propose efficient algorithms for approximating the core in coalitional games, that iteratively refine a payoff vector by sampling the constraints posed by randomly selected coalitions. By overcoming the computational barrier, we show how the core can provide a desirable alternative to the Shapley value for many applications: political power analysis, predicting individual payoffs in human teams and explainable AI. Our empirical analysis shows how team stability is affected by game parameters for important classes of cooperative games (weighted voting games, graph games and marginal contribution networks). Further, we contrast the individual impact estimates of the Shapley value and the least-core in political settings and explainable AI analysis of multiple datasets."
Google DeepMind,Towards Practical Reinforcement Learning for Tokamak Magnetic Control,1-Mar-24,https://deepmind.google/research/publications/30578/,,https://deepmind.google/research/publications/30578/,"Reinforcement learning (RL) has shown promising results for real-time control systems, including the domain of plasma magnetic control. However, there are still significant drawbacks compared to traditional feedback control approaches for magnetic confinement. In this work, we address key drawbacks of the RL method; achieving higher control accuracy for desired plasma properties, reducing the steady-state error, and decreasing the required time to learn new tasks. We build on top of Degrave et al. (2022), and present algorithmic improvements to the agent architecture and training procedure. We present simulation results that show up to 65% improvement in shape accuracy, achieve substantial reduction in the long-term bias of the plasma current, and additionally reduce the training time required to learn new tasks by a factor of 3 or more. We present new experiments using the upgraded RL-based controllers on the TCV tokamak, which validate the simulation results achieved, and point the way towards routinely achieving accurate discharges using the RL approach."
Google DeepMind,Self-supervised video pretraining yields strong image representations,29-Feb-24,https://deepmind.google/research/publications/15533/,,https://deepmind.google/research/publications/15533/,"Humans learn powerful representations of objects and scenes by observing how they evolve over time. Yet, outside of specific tasks that require explicit temporal understanding, static image pretraining remains the dominant paradigm for learning visual foundation models. We question this mismatch, and ask whether video pretraining can yield visual representations that bear the hallmarks of human perception: generalisation across tasks, robustness to perturbations, and consistency with human judgements. To that end we propose a novel procedure for curating videos, and develop a contrastive framework which learns from the complex transformations therein. This simple paradigm for distilling knowledge from videos, called VITO, yields general representations that far outperform prior video pretraining methods on image understanding tasks, and image pretraining methods on video understanding tasks. Moreover, VITO representations are significantly more robust to natural and synthetic deformations than image-, video-, and adversarially-trained ones. Finally, VITO's predictions are strongly aligned with human judgements, surpassing models that were specifically trained for that purpose. Together, these results suggest that video pretraining could be a simple way of learning unified, robust, and human-aligned representations of the visual world."
Google DeepMind,Bad Students Make Great Teachers: Active Learning Accelerates Large Scale Visual Understanding,29-Feb-24,https://deepmind.google/research/publications/62998/,,https://deepmind.google/research/publications/62998/,"Power-law scaling indicates that large-scale training with uniform sampling is prohibitively slow. Active learning methods aim to increase data efficiency by prioritizing learning on the most relevant examples. Despite their appeal, these methods have yet to be widely adopted since no one algorithm has been shown to a) generalize across models and tasks b) scale to large datasets and c) yield overall FLOP savings when accounting for the overhead of data selection. In this work we propose a method which satisfies these three properties, leveraging small, cheap proxy models to estimate ""learnability"" scores for datapoints, which are used to prioritize data for the training of much larger models. As a result, our models require 46% and 51% fewer training updates and up to 25% less total computation to reach the same performance as uniformly trained visual classifiers on JFT and multimodal models on ALIGN. Finally, we find our data-prioritization scheme to be complementary with recent data-curation and learning objectives, yielding a new state-of-the-art in several multimodal transfer tasks."
Google DeepMind,Set Learning for Accurate and Calibrated Models,27-Feb-24,https://deepmind.google/research/publications/46131/,,https://deepmind.google/research/publications/46131/,"Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the trained model can be applied to single examples at inference time, without introducing significant run-time overhead or architecture changes."
Google DeepMind,Intriguing Properties of Generative Classifers,26-Feb-24,https://deepmind.google/research/publications/45424/,,https://deepmind.google/research/publications/45424/,"What is the best paradigm to recognize objects---discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data.
We report four intriguing emergent properties of diffusion-based generative classifiers: they show a record-breaking human-like shape bias (99\% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well."
Google DeepMind,Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers,25-Feb-24,https://deepmind.google/research/publications/49969/,,https://deepmind.google/research/publications/49969/,"We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention."
Google DeepMind,On Limitations of the Transformer Architecture,24-Feb-24,https://deepmind.google/research/publications/77946/,,https://deepmind.google/research/publications/77946/,"What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true."
Google DeepMind,AlphaTensor for Optimizing Quantum Computations,23-Feb-24,https://deepmind.google/research/publications/77240/,,https://deepmind.google/research/publications/77240/,"We address the problem of T-count optimization, i.e., minimizing the number of the most expensive gates in fault-tolerant quantum computation (namely, the T gates) that are needed to implement a given circuit. For that, we develop AlphaTensor-Quantum, a method based on deep reinforcement learning that exploits the relationship between optimizing T-count and tensor decomposition. Unlike existing methods for T-count optimization, AlphaTensor-Quantum can incorporate domain-specific knowledge about quantum computation and leverage gadgets, which significantly reduces the T-count of the optimized circuits. AlphaTensor-Quantum outperforms all existing methods for T-count optimization on a set of arithmetic benchmarks. Remarkably, it rediscovers an efficient multiplication algorithm akin to Karatsuba's method for multiplication in finite fields, and it finds the best human-designed solutions for relevant arithmetic computations used in Shor's algorithm and for quantum chemistry simulation. Thus, AlphaTensor-Quantum can save hundreds of hours of research by optimizing relevant quantum circuits in a fully automated way."
Google DeepMind,Genie: Generative Interactive Environments,23-Feb-24,https://deepmind.google/research/publications/60474/,,https://deepmind.google/research/publications/60474/,"We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future."
Google DeepMind,"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",22-Feb-24,https://deepmind.google/research/publications/49667/,,https://deepmind.google/research/publications/49667/,"While large language models (LLMs) often rely on finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning – full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a power-based multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent.
We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods."
Google DeepMind,OmniPred: Language Models as Universal Regressors,22-Feb-24,https://deepmind.google/research/publications/78451/,,https://deepmind.google/research/publications/78451/,"Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over (x,y) evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models."
Google DeepMind,The Next 700 ML-Enabled Compiler Optimizations,20-Feb-24,https://deepmind.google/research/publications/57746/,,https://deepmind.google/research/publications/57746/,"There is a growing interest in enhancing compiler optimizations with ML models. Yet compilers remain challenging environments for interacting with ML frameworks. Some optimizations require tightly coupled models and compiler internals, raising issues with modularity, performance and ML framework independence. Practical deployment and transparency for the end-user are also important concerns. We propose a library allowing ML model development within a traditional Python framework while enabling efficient, end-to-end integration with an optimizing compiler. We evaluate it on both research and production use cases, for training and inference, over four optimization problems."
Google DeepMind,Simulacra as Conscious Exotica,19-Feb-24,https://deepmind.google/research/publications/79663/,,https://deepmind.google/research/publications/79663/,"The advent of conversational agents with increasingly human-like behaviour throws old philosophical questions into new light. Does it, or could it, ever make sense to speak of AI agents built out of generative language models in terms of consciousness, given that they are ""mere"" simulacra of human behaviour, and that what they do can be seen as ""merely"" role play? Drawing on the later writing of Wittgenstein, this paper attempts to tackle this question while avoiding the pitfalls of dualistic thinking."
Google DeepMind,A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts,15-Feb-24,https://deepmind.google/research/publications/74917/,,https://deepmind.google/research/publications/74917/,"Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3-20x."
Google DeepMind,Experts Don't Cheat: Learning What You Don't Know by Predicting Pairs,15-Feb-24,https://deepmind.google/research/publications/73709/,,https://deepmind.google/research/publications/73709/,"Identifying how much a model ${\widehat{p}}_{\theta}(Y|X)$ knows about the
stochastic real-world process $p(Y|X)$ it was trained on is important to ensure
it avoids producing incorrect or ""hallucinated"" answers or taking unsafe
actions. But this is difficult for generative models because probabilistic
predictions do not distinguish between per-response noise (aleatoric
uncertainty) and lack of knowledge about the process (epistemic uncertainty),
and existing epistemic uncertainty quantification techniques tend to be
overconfident when the model underfits. We propose a general strategy for
teaching a model to both approximate $p(Y|X)$ and also estimate the remaining
gaps between ${\widehat{p}}_{\theta}(Y|X)$ and $p(Y|X)$: train it to predict
pairs of independent responses drawn from the true conditional distribution,
allow it to ""cheat"" by observing one response while predicting the other, then
measure how much it cheats. Remarkably, we prove that being good at cheating
(i.e. cheating whenever it improves your prediction) is equivalent to being
second-order calibrated, a principled extension of ordinary calibration that
allows us to construct provably-correct frequentist confidence intervals for
$p(Y|X)$ and detect incorrect responses with high probability. We demonstrate
empirically that our approach accurately estimates how much models don't know
across ambiguous image classification, (synthetic) language modeling, and
partially-observable navigation tasks, outperforming existing techniques."
Google DeepMind,Premise Order Matters in Reasoning with Large Language Models,14-Feb-24,https://deepmind.google/research/publications/75421/,,https://deepmind.google/research/publications/75421/,"Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, we observe that LLMs are surprisingly brittle to different premise orders, despite that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks,  presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that even if the model performance is decent on the optimal order, permuting the premise order can cause a performance drop of over 30%. In addition, we introduce the R-GSM benchmark based on GSM8K to examine the ordering effect for math problem solving, and we again observe a significant accuracy decrease compared to the original GSM8K problems."
Google DeepMind,A Distributional Analogue to the Successor Representation,13-Feb-24,https://deepmind.google/research/publications/44717/,,https://deepmind.google/research/publications/44717/,"This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible."
Google DeepMind,Near-Minimax-Optimal Distributional RL with a Generative Model,12-Feb-24,https://deepmind.google/research/publications/70372/,,https://deepmind.google/research/publications/70372/,"We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023). Our analysis provides new theoretical results on categorical approaches to distributional RL, and also introduces a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest. We also provide an experimental study comparing several model-based distributional RL algorithms, with several takeaways for practitioners."
Google DeepMind,PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs,12-Feb-24,https://deepmind.google/research/publications/72495/,,https://deepmind.google/research/publications/72495/,"Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data?"
Google DeepMind,Chain-of-Table: Evolves Tables in the LLM Reasoning Chain for Table Understanding,11-Feb-24,https://deepmind.google/research/publications/47444/,,https://deepmind.google/research/publications/47444/,"Table-based reasoning with large language models is a promising direction to solve many table understanding tasks, such as table-based question answering and table-based fact verification. Chain-of-Thought and its alike approaches have demonstrated impressive performance by incorporating the reasoning chain in the form of textual context. However, it is still an open question how to properly involve tabular data in the reasoning chain for the table-based tasks. Inspired by nested queries in SQL development where temporary tables are used to store intermediate results, we propose Chain-of-Table, where we update the table iteratively to represent the complex reasoning chain. In our approach, the tabular context evolves as a sequence of operations generated by a large language model, where the generation of the latter operation is conditioned on the results of the previous operation. In this way, the constantly evolving table forms a chain, showing the reasoning process of the given problem. The result table carries rich information of the intermediate results so that large language models can skip the complex reasoning over the latent clues, directly aggregate relevant information from the result table, and come to the final prediction easily. Extensive experiments with two large language models show that Chain-of-Table surpasses competitive baselines and achieves the state-of-the-art performance in three well-known benchmark datasets (WikiTableQuestions, and FeTaQA, and TabFact)."
Google DeepMind,Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits,8-Feb-24,https://deepmind.google/research/publications/52797/,,https://deepmind.google/research/publications/52797/,"We study the problem of Bayesian fixed-budget best-arm identification (BAI) in structured bandits. We propose an algorithm that uses fixed allocations based on the prior information and the structure of the environment. We provide theoretical bounds on its performance across diverse models, including the first prior-dependent upper bounds for linear and hierarchical BAI. Our key contribution is introducing new proof methods that result in tighter bounds for multi-armed BAI compared to existing methods. We extensively compare our approach to other fixed-budget BAI methods, demonstrating its consistent and robust performance in various settings. Our work improves our understanding of Bayesian fixed-budget BAI in structured bandits and highlights the effectiveness of our approach in practical scenarios."
Google DeepMind,Memory Consolidation Enables Long-Context Video Understanding,8-Feb-24,https://deepmind.google/research/publications/79057/,,https://deepmind.google/research/publications/79057/,"Most transformer-based video encoders are limited to short temporal contexts due to their quadratic complexity. While various attempts have been made to extend this context, this has often come at the cost of both conceptual and computational complexity. We propose to instead re-purpose existing pre-trained video transformers by simply fine-tuning them to attend to memories derived non-parametrically from past activations. By leveraging redundancy reduction, our memory-consolidated vision transformer (MC-ViT) effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos. In doing so, MC-ViT sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception Test, and Diving48, outperforming methods that benefit from orders of magnitude more parameters."
Google DeepMind,Large Language Models Self-Discover Reasoning Structures,6-Feb-24,https://deepmind.google/research/publications/64816/,,https://deepmind.google/research/publications/64816/,"We introduce SELF-DISCOVER, a general framework for LLMs to self-discover and compose atomic reasoning modules such as critical thinking
and step-by-step reasoning to tackle complex reasoning problems that are challenging for typical prompting methods e.g. Chain-of-Thought (CoT). Core to the framework is a self-discover process where LLMs select multiple atomic reasoning modules, and compose them into an explicit and task-unique reasoning structure for LLMs to follow during decoding, in sharp contrast to the implicit reasoning in CoT. SELF-DISCOVER substantially improves GPT-4 and PaLM-2’s performance on challenging reasoning benchmarks such as BigBench-Hard and Thinking4Doing, by as much as 30%. Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency and majority voting by more than 20% across 24 tasks on multiple LLMs, while requiring 10-40x fewer inference compute. Finally, the self-discovered reasoning structures by GPT-4 can also be applied to smaller models of Llama 2 to improving their reasoning capabilities, demonstrating generalization of the discovered reasoning structures."
Google DeepMind,States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers,6-Feb-24,https://deepmind.google/research/publications/67342/,,https://deepmind.google/research/publications/67342/,"Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new dialogue scenarios, which are grounded in real world applications. In this work, we present one possible binding from dialogue to game theory as well as generalizations of existing equilibrium finding algorithms to this setting. In addition, by exploiting LLMs generation capabilities along with our proposed binding, we can synthesize a large repository of formally-defined games in which one can study and test game-theoretic solution concepts. We also demonstrate how one can combine LLM-driven game generation, game-theoretic solvers, and imitation learning to construct a process for improving the strategic capabilities of LLMs."
Google DeepMind,Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces,2-Feb-24,https://deepmind.google/research/publications/45020/,,https://deepmind.google/research/publications/45020/,"Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), for the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from data on training'' functions. These training functions are typically required to have the same domain as thetest'' function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain descriptions to specifications of a hierarchical GP. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks."
Google DeepMind,Fractal Patterns May Unravel the Intelligence in Next-Token Prediction,2-Feb-24,https://deepmind.google/research/publications/48253/,,https://deepmind.google/research/publications/48253/,"We study the fractal structure of language, aiming to provide a precise formalism for quantifying several properties that may have been previously suspected but not  formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic granularity level or context length, and (2) long-range dependent (LRD), with tokens at any instant typically correlated with all subsequent tokens. Based on these findings, we argue that short-term patterns in language, such as in paragraphs, mirror the patterns seen in larger scopes, like entire documents. This may shed some light on how next-token prediction can lead to a comprehension of the structure of text at multiple levels of granularity, from words and clauses to broader contexts and intents. In addition, we demonstrate a connection between fractal parameters, such as the Hurst exponent, and scaling laws when varying the context length at inference time. We hope that these findings offer a fresh perspective on the nature of language and the mechanisms underlying the success of LLMs."
Google DeepMind,Exploration at Scale using Epistemic Neural Networks,1-Feb-24,https://deepmind.google/research/publications/73001/,,https://deepmind.google/research/publications/73001/,"We present evidence of substantial benefit to efficient exploration in gathering human feedback to improve large language models.  In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received.  Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network.  Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries.  Further, both uncertainty estimation and the choice of exploration scheme play critical roles."
Google DeepMind,Learning Universal Predictors,26-Jan-24,https://deepmind.google/research/publications/42394/,,https://deepmind.google/research/publications/42394/,"Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data. Broad exposure to different tasks leads to versatile representations enabling general problem solving. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal prediction strategies."
Google DeepMind,Neural Population Learning beyond Symmetric Zero-Sum Games,20-Jan-24,https://deepmind.google/research/publications/24820/,,https://deepmind.google/research/publications/24820/,"Neural Population Learning (NeuPL) represents diverse strategies in symmetric zero-sum games using a shared conditional neural network. We propose NeuPL-JPSRO, which extends this idea to n-player general-sum games and leverages the convergence guarantees of Joint Policy-Space Response Oracle (JPSRO). We show empirically that NeuPL-JPSRO converges to a Coarse Correlated Equilibrium (CCE) in several OpenSpiel games, as verified by analytical game solvers. We then deploy NeuPL-JPSRO to complex domains where JPSRO with independent RL becomes computationally impractical. We demonstrate how our approach enables adaptive coordination with co-players and transfer learning of skills in larger games. Our work shows that equilibrium convergent population learning can be implemented at scale and in generality, paving the way towards solving real-world games between heterogeneous players with mixed motives."
Google DeepMind,Asynchronous Local-SGD Training forLanguage Modeling,17-Jan-24,https://deepmind.google/research/publications/66535/,,https://deepmind.google/research/publications/66535/,"Training large language models requires substantial computational resources. As models keep scaling, it is crucial to leverage distributed computation resources that might be even geographically distant from each other. Due to the latency and heterogeneity of these computation devices, it is natural to consider Local Stochastic Gradient Descent (localSGD) (e.g., each device performs more than one update per communication) and asynchronous training (the server updates the global parameter vector as soon as a worker has completed its local updates). This work presents an initial study on asynchronous localSGD for language modeling. We conduct a comprehensive investigation by examining how worker heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We identified that a key hurdle of applying asynchronous localSGD in language modeling is the application of momentum acceleration on the server side when worker gradients are staled. In this setting, asynchronous localSGD takes more iterations to converge than its synchronous counterpart despite updating the global parameters more frequently. To mitigate this optimization challenge, we propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, not only matches the performance of synchronous localSGD in terms of perplexity per update step but also surpasses it significantly in terms of wall clock time. For the convenience of future research and quick prototyping of new ideas, we also make available a toy framework that replicates the observed  optimization challenges on a mixture of mixtures of Gaussians. We hope this work can bring new insights and tools to facilitate future discovery of more efficient language model optimizers."
Google DeepMind,GATS: Gather-Attend-Scatter,16-Jan-24,https://deepmind.google/research/publications/67846/,,https://deepmind.google/research/publications/67846/,"As the AI community increasingly adopts large-scale models, it is crucial to develop general and flexible tools to integrate them. We introduce Gather-Attend-Scatter (GATS), a novel module that enables seamless combination of pretrained foundation models, both trainable and frozen, into larger multimodal networks. GATS empowers AI systems to process and generate information across multiple modalities at different rates. In contrast to traditional fine-tuning, GATS allows for the original component models to remain frozen, avoiding the risk of them losing important knowledge acquired during the pretraining phase. We demonstrate the utility and versatility of GATS with a few experiments across games, robotics, and multimodal input-output systems."
Google DeepMind,NfgTransformer: Equivariant Representation Learning for Normal-form Games,16-Jan-24,https://deepmind.google/research/publications/40173/,,https://deepmind.google/research/publications/40173/,"Normal-form games (NFGs) are fundamental subjects of game theoretic analysis that describe strategic interactions between players. While NFGs are typically represented as payoff tensors, any permutation of player actions describes an identical game. We propose NfgTransformer, a general-purpose query-key-value architecture that exploits the equivariance inherent to this data modality. We show the efficacy of our approach in a range of downstream tasks, including equilibrium solving, deviation-gain estimation and ranking on synthetic and standard games. Beyond strong quantitative results, we show that our model internally developed a recognisable pattern of iterative equilibrium refinement when solving for a Nash Equilibrium in a suite of GAMUT games. We hope our work can serves as an effective bridge to bring scalable learning techniques to game theoretic analysis."
Google DeepMind,Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization,16-Jan-24,https://deepmind.google/research/publications/34213/,,https://deepmind.google/research/publications/34213/,"We propose the first loss function for approximate Nash equilibria of normal-form games that is amenable to unbiased Monte Carlo estimation. This construction allows us to deploy standard non-convex stochastic optimization techniques for approximating Nash equilibria, resulting in novel algorithms with provable guarantees. We complement our theoretical analysis with experiments demonstrating that stochastic gradient descent can outperform previous state-of-the-art approaches."
Google DeepMind,On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes,16-Jan-24,https://deepmind.google/research/publications/48050/,,https://deepmind.google/research/publications/48050/,"Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student upon deployment. To address this issue, we introduce Generalized Knowledge Distillation (GKD). Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher on such sequences. Unlike supervised KD approaches, GKD also offers the flexibility to employ alternative divergence measures between the student and teacher, which can be useful when the student lacks the expressivity to mimic the teacher's distribution. Furthermore, GKD facilitates the seamless integration of distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for distilling auto-regressive language models on summarization, translation, and arithmetic reasoning tasks."
Google DeepMind,Generative Adversarial Equilibrium Solvers,16-Jan-24,https://deepmind.google/research/publications/24821/,,https://deepmind.google/research/publications/24821/,"We introduce the use of generative adversarial learning to compute equilibria in general game-theoretic settings, specifically the generalized Nash equilibrium (GNE) in pseudo-games, and its specific instantiation as the competitive equilibrium (CE) in Arrow-Debreu competitive economies. Pseudo-games are a generalization of games in which players' actions affect not only the payoffs of other players but also their feasible action spaces. Although the computation of GNE and CE is intractable in the worst-case, i.e., PPAD-hard, in practice, many applications only require solutions with high accuracy in expectation over a distribution of problem instances. We introduce Generative Adversarial Equilibrium Solvers (GAES): a family of generative adversarial neural networks that can learn GNE and CE from only a sample of problem instances. We provide computational and sample complexity bounds for Lipschitz-smooth function approximators in a large class of concave pseudo-games, and apply the framework to finding Nash equilibria in normal-form games, CE in Arrow-Debreu competitive economies, and GNE in an environmental economic model of the Kyoto mechanism."
Google DeepMind,GATS: Gather-Attend-Scatter,16-Jan-24,https://deepmind.google/research/publications/67846/,,https://deepmind.google/research/publications/67846/,"As the AI community increasingly adopts large-scale models, it is crucial to develop general and flexible tools to integrate them. We introduce Gather-Attend-Scatter (GATS), a novel module that enables seamless combination of pretrained foundation models, both trainable and frozen, into larger multimodal networks. GATS empowers AI systems to process and generate information across multiple modalities at different rates. In contrast to traditional fine-tuning, GATS allows for the original component models to remain frozen, avoiding the risk of them losing important knowledge acquired during the pretraining phase. We demonstrate the utility and versatility of GATS with a few experiments across games, robotics, and multimodal input-output systems."
Google DeepMind,Learning Planning-compatible Cognitive Maps with Transformers in PartiallyObserved Environments,11-Jan-24,https://deepmind.google/research/publications/63907/,,https://deepmind.google/research/publications/63907/,"Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent's spatial position cannot be deterministically recovered from its observation, which makes planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the observations. After training a TDB with an augmented objective on sequences of observations and actions, we extract interpretable cognitive maps of the environment from the active bottleneck(s) indices. These maps are then paired with an external solver to solve planning problems. First, we show that a TDB trained on POEs (a) retains the near-perfect predictive performance of a vanilla transformer or an LSTM while (b) solving shortest paths problems exponentially more efficiently. Second, a TDB extracts interpretable representations from text datasets, while reaching higher in-context accuracy than vanilla sequence models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context planning problems."
Google DeepMind,Distributional reinforcement learning in prefrontal cortex,10-Jan-24,https://deepmind.google/research/publications/2504/,,https://deepmind.google/research/publications/2504/,"Prefrontal cortex is crucial for learning and decision-making. Classic reinforcement learning (RL) theories centre on learning the expectation of potential rewarding outcomes and explain a wealth of neural data in prefrontal cortex. Distributional RL, on the other hand, learns the full distribution of rewarding outcomes and better explains dopamine responses. Here we show distributional RL also better explains prefrontal cortical responses, suggesting it is a ubiquitous mechanism for reward-guided learning."
Google DeepMind,AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents,4-Jan-24,https://deepmind.google/research/publications/48151/,,https://deepmind.google/research/publications/48151/,"Large foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to learn knowledge and reasoning for everyday tasks. In this paper we show how existing foundation models can be used to scale up the deployment of operational robots incompletely unseen scenarios with minimal human supervision.  We refer to this system as AutoRT, which runs on over 50 robots across multiple buildings, collecting 77k real robot episodes via both teleoperation and autonomously moving robots. Such “in-the-wild” data is significantly more diverse than previous robotic datasets collected in robot lab settings, and improves robotics policies when used in co-fine-tuning.  When combined with vision, large language models can propose novel instructions based on their environment and reason about autonomy tradeoffs and safety without finetuning, allowing orchestration of large scale robot fleets.  We further show how introducing LLMs and VLMs into data collection creates new forms of interaction for steering robot agents to collect more diverse data or data for specific settings."
Google DeepMind,GenCast: learning skillful ensemble forecasting of medium-range weather,25-Dec-23,https://deepmind.google/research/publications/68149/,,https://deepmind.google/research/publications/68149/,"A probabilistic weather forecast is critical for decision-making, from the everyday to preparation for extreme events. Forecasts of the joint distribution of weather trajectories via spatio-temporally coherent ensembles have further importance: they provide a powerful tool for decision-making in complex and high-impact domains including energy system planning, transportation routing, flood forecasting and more. State-of-the-art ML forecast models for medium-range weather, however, are largely trained to produce deterministic forecasts which lose physical consistency at longer lead times. We introduce GenCast, a ML-based generative model for ensemble weather forecasting, trained from reanalysis data. It forecasts ensembles of trajectories for hundreds of weather variables, up to 15 days at 1 degree resolution globally, using under TC secs per ensemble member. We show that GenCast is more skilful than ENS, a top operational ensemble forecast, for TC% of TC verification targets, while maintaining good calibration and physically consistent power spectra."
Google DeepMind,Zero-Shot Metric Depth with a Field-of-View Conditioned Diffusion Model,20-Dec-23,https://deepmind.google/research/publications/63604/,,https://deepmind.google/research/publications/63604/,"While methods for monocular depth estimation have made significant strides on standard benchmarks, zero-shot metric depth estimation remains unsolved. Challenges include the joint modeling of indoor and outdoor scenes, which often exhibit significantly different distributions of RGB and depth, and the depth-scale ambiguity due to unknown camera intrinsics. Recent work has proposed specialized multi-head architectures for jointly modeling indoor and outdoor scenes. In contrast, we advocate a generic, task-agnostic diffusion model, with several advancements such as log-scale depth parameterization to enable joint modeling of indoor and outdoor scenes, conditioning on the field-of-view (FOV) to handle scale ambiguity and synthetically augmenting FOV during training to generalize beyond the limited camera intrinsics in training datasets. Furthermore, by employing a more diverse training mixture than is common, and an efficient diffusion parameterization, our method, DMD (Diffusion for Metric Depth) achieves a 25% reduction in relative error (REL) on zero-shot indoor and 33% reduction on zero-shot outdoor datasets over the current SOTA using only a small number of denoising steps."
Google DeepMind,Equivariant MuZero,19-Dec-23,https://deepmind.google/research/publications/26234/,,https://deepmind.google/research/publications/26234/,"Deep reinforcement learning has shown lots of success in closed, well-defined domains such as games (Chess, Go, StarCraft). The next frontier is real-world scenarios, where setups are numerous and varied. For this, agents need to learn the underlying environment dynamics, so as to robustly generalise to conditions that differ from those they were trained on. Model-based reinforcement learning algorithms, such as MuZero or Dreamer, aim to accomplish this by learning a world model. However, leveraging a world model has not yet consistently shown greater generalisation capabilities compared to model-free alternatives. In this work, we propose improving the data efficiency and generalisation capabilities of MuZero by explicitly incorporating the symmetries of the environment in its world-model architecture. We prove that, so long as the neural networks used by MuZero are equivariant to a particular symmetry group acting on the environment, the entirety of MuZero's action-selection algorithm will also be equivariant to that group. As such, Equivariant MuZero is guaranteed to behave symmetrically in symmetrically-transformed states, and will hence be more data-efficient when learning its world models. We evaluate Equivariant MuZero on procedurally-generated MiniPacman and on Chaser from the ProcGen suite: training on a set of mazes, and then testing on unseen rotated versions, demonstrating the benefits of equivariance. We verify that our improvements hold even when only some of the components of Equivariant MuZero obey strict equivariance, which highlights the robustness of our construction."
Google DeepMind,Learning Silicon Dopant Transitions in Graphene using Scanning Transmission Electron Microscopy,15-Dec-23,https://deepmind.google/research/publications/48254/,,https://deepmind.google/research/publications/48254/,"We introduce a machine learning approach to determining the transition rates of silicon atoms on graphene (a lattice of carbon atoms), when stimulated by the electron beam of a scanning transmission electron microscope. Our learned rates are then applied to guide a single silicon atom throughout the lattice to pre-determined target destinations. We present sensitivity and accuracy analyses that demonstrate the generality of our approach."
Google DeepMind,Meta-in-context learning in large language models,14-Dec-23,https://deepmind.google/research/publications/33809/,,https://deepmind.google/research/publications/33809/,"Large language models have shown tremendous performance in a variety of tasks.  In-context learning is seen as one of the main contributors to their success. Previous work has demonstrated that in-context learning can even solve non-trivial tasks such as supervised and reinforcement learning. We expand on this line of work by investigating the effect of in context learning when the context is presented in a sequential manner.  We find that the sequential presentation of related tasks leads to better in-context learning performance, thereby revealing that in-context learning can be used to create better in-context learning algorithms. We coin this phenomenon \emph{meta-in-context learning}. Multiple experiments reveal that meta-in-context learning adaptively modifies a large language model's priors over latent variables and that it adjusts its learning strategies. Finally, we extend our approach to a benchmark of real-world regression problems where we observe competitive performance to traditional learning algorithms. Taken together, our work improves our understanding of in-context learning and paves the way toward adapting large language models to the environment they are applied purely through meta-in-context learning rather than traditional fine-tuning."
Google DeepMind,A Definition of Continual Reinforcement Learning,12-Dec-23,https://deepmind.google/research/publications/33910/,,https://deepmind.google/research/publications/33910/,"In a standard view of the reinforcement learning problem, an agent’s goal is to efficiently identify a policy that maximizes long-term reward. However, this perspective is based on a restricted view of learning as finding a solution, rather than treating learning as endless adaptation. In contrast, continual reinforcement learning refers to the setting in which the best agents never stop learning. Despite the importance of continual reinforcement learning, the community lacks a simple definition of the problem that highlights its commitments and makes its primary concepts precise and clear. To this end, this paper is dedicated to carefully defining the continual reinforcement learning problem. We formalize the notion of agents that ""never stop learning"" through a new mathematical language for analyzing and cataloging agents. Using this new language, we define a continual learning agent as one that can be understood as carrying out an implicit search process indefinitely, and continual reinforcement learning as the setting in which the best agents are all continual learning agents. We provide two motivating examples, illustrating that traditional views of multi-task reinforcement learning and continual supervised learning are special cases of our definition. Collectively, these definitions and perspectives formalize many intuitive concepts at the heart of learning, and open new research pathways surrounding continual learning agents."
Google DeepMind,A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames,12-Dec-23,https://deepmind.google/research/publications/60675/,,https://deepmind.google/research/publications/60675/,"Recent work transfers large-scale image-to-text models to the video domain via shallow late temporal fusion while keeping the image encoder frozen. In contrast, we train video-first encoders, plug them into a frozen LM, and demonstrate that utilizing joint space-time attention yields improvements on benchmarks with strong temporal dependencies (e.g., YouCook2, VATEX). However, we expose two limitations to the approach: (1) decreased spatial capabilities, likely due to noisy video-language alignments and (2) higher memory consumption, bottlenecking the number of frames that can be processed. To mitigate the memory bottleneck, we systematically analyze ways to improve memory efficiency for training video-first encoders including input sampling, parameter-efficient image-to-video adaptation, and factorized attention. Surprisingly, just masking large portions of the video (up to 75%) proves to be the most robust way to scale encoders to videos up to 4.3 minutes at 1 fps rate. Our simple approach for training long video-to-text models, which account for less than 1B parameters, is able to outperform the popular paradigm of using a strong LLM as information aggregator over segment-based information on benchmarks with long range temporal dependencies (YouCook2, EgoSchema)."
Google DeepMind,Online RL in Linearly qπ-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore,12-Dec-23,https://deepmind.google/research/publications/34921/,,https://deepmind.google/research/publications/34921/,"We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is assumed that the action-values of all policies can be  expressed as linear functions of state-action features. This class is known to be more general than  linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly $q^\pi$-realizable MDPs where for any policy, all the actions have  approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorithm on the linear MDP hidden in the problem. The method returns an $\epsilon$-optimal policy after $\text{polylog}(H, d)/\epsilon^2$ interactions with the MDP, where $H$ is the time horizon and $d$ is the dimension of the feature vectors, giving the first polynomial-sample-complexity online RL algorithm for this setting. The results are proved for the misspecified case, where the sample complexity is shown to degrade gracefully with the misspecification error."
Google DeepMind,Schema-learning and rebinding as mechanisms of in-context learning and emergence,12-Dec-23,https://deepmind.google/research/publications/35122/,,https://deepmind.google/research/publications/35122/,"In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\em interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at different levels of overparameterization, suggesting that overparameterization helps in learning more complex template (schema) circuits. By showing how ICL can be achieved with small models and datasets, we open up a path to novel architectures, and take a vital step towards a more general understanding of the mechanics behind this important capability."
Google DeepMind,Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples,10-Dec-23,https://deepmind.google/research/publications/31486/,,https://deepmind.google/research/publications/31486/,"The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Divergence (FLD), a parametric sample-based score that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLD to identify specific overfitting problem cases, where previously proposed metrics fail. We also extensively evaluate FLD on various image datasets and model classes, demonstrating its ability to match intuitions of previous metrics like FID while offering a more comprehensive evaluation of generative models. Code is available at https://github.com/marcojira/fld."
Google DeepMind,Improving neural network representations using human similarity judgments,10-Dec-23,https://deepmind.google/research/publications/33708/,,https://deepmind.google/research/publications/33708/,"Deep neural networks have reached human-level performance on many computer vision tasks. However, the objectives used to train these networks enforce only that similar images are embedded at similar locations in the representation space, and do not directly constrain the global structure of the resulting space. Here, we explore the impact of supervising this global structure by linearly aligning it with human similarity judgments. We find that a naive approach leads to large changes in local representational structure that harm downstream performance. Thus, we propose a novel method that aligns the global structure of representations while preserving their local structure. This global-local transform considerably improves accuracy across a variety of few-shot learning and anomaly detection tasks. Our results indicate that human visual representations are globally organized in a way that facilitates learning from few examples, and incorporating this global structure into neural network representations improves performance on downstream tasks."
Google DeepMind,Optimal Preconditioning and Fisher Adaptive Langevin Sampling,10-Dec-23,https://deepmind.google/research/publications/34617/,,https://deepmind.google/research/publications/34617/,"We define an optimal preconditioning for the Langevin diffusion by analytically optimizing the expected squared jumped distance. This yields as the optimal preconditioning an inverse Fisher information covariance matrix, where the covariance matrix is computed as the outer product of log target gradients averaged under the target. We apply this result to the Metropolis adjusted Langevin algorithm (MALA) and derive a computationally efficient adaptive MCMC scheme that learns the preconditioning from the history of gradients produced as the algorithm runs. We show in several experiments that the proposed algorithm is very robust in high dimensions and significantly outperforms other methods, including a closely related adaptive MALA scheme that learns the preconditioning with standard adaptive MCMC as well as the position-dependent Riemannian manifold MALA sampler."
Google DeepMind,Passive learning of active causal strategies in agents and language models,10-Dec-23,https://deepmind.google/research/publications/33709/,,https://deepmind.google/research/publications/33709/,"What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. 
We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectly-confounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and have implications for understanding the behaviors and capabilities of language models."
Google DeepMind,Optimization and Evaluation of Fine-grained Jaccard Indexes for Semantic Segmentation,10-Dec-23,https://deepmind.google/research/publications/37041/,,https://deepmind.google/research/publications/37041/,"Semantic segmentation datasets often exhibit two types of imbalance: class imbalance, where some classes appear more frequently than others and size imbalance, where some objects occupy more pixels than others. This causes traditional evaluation metrics to be biased towards majority classes (e.g. overall pixel-wise accuracy) and large objects (e.g. mean pixel-wise accuracy and per-dataset mean intersection over union). To address these shortcomings, we propose the use of fine-grained mIoUs along with corresponding worst-case metrics, thereby offering a more holistic evaluation of segmentation techniques. These fine-grained metrics offer less bias towards large objects, richer statistical information, and valuable insights into model and dataset auditing. Furthermore, we undertake an extensive benchmark study, where we train and evaluate 15 modern neural networks with the proposed metrics on 12 diverse natural and aerial segmentation datasets. Our benchmark study highlights the necessity of not basing evaluations on a single metric and confirms that fine-grained mIoUs reduce the bias towards large objects. Moreover, we identify the crucial role played by architecture designs and loss functions, which lead to best practices in optimizing fine-grained metrics. The code is available at https://github.com/zifuwanggg/JDTLosses."
Google DeepMind,Probabilistic Inference in Reinforcement Learning Done Right,10-Dec-23,https://deepmind.google/research/publications/34719/,,https://deepmind.google/research/publications/34719/,"In Reinforcement learning (RL) an agent acts so as to maximize its return under uncertainty. It is natural to apply Bayesian probabilistic inference to the uncertain parameters and, since the goal of the agent is to find the optimal policy, a relevant object of study is the posterior probability of optimality for each state-action. Previous work on `RL as inference' has equipped the agent with a surrogate potential in order to estimate this quantity, however the approximation can be arbitrarily poor, leading to algorithms that do not perform well in practice. In this work, we rigorously analyze how the posterior probability of optimality flows through the Markov decision process (MDP) and show that sampling according to this probability yields a guaranteed Bayesian regret bound. In practice computing this probability is intractable, so we derive a variational Bayesian approximation yielding a tractable convex optimization problem and show that the resulting policy also satisfies a Bayesian regret bound. We call our approach VAPOR and show that it has deep connections to Thompson sampling, K-learning, information theory, and maximum entropy exploration."
Google DeepMind,Towards In-context Scene Understanding,10-Dec-23,https://deepmind.google/research/publications/32698/,,https://deepmind.google/research/publications/32698/,"In-context learning—the ability to configure a model’s behavior with different prompts—has revolutionized the field of natural language processing, alleviating the need for task-specific models and paving the way for generalist models capable of assisting with any query. Computer vision, in contrast, has largely stayed in the former regime: specialized decoders and finetuning protocols are generally required to perform dense tasks such as semantic segmentation and depth estimation. In this work we explore a simple mechanism for in-context learning of such scene understanding tasks: nearest neighbor retrieval from a prompt of annotated features. We propose a new pretraining protocol—leveraging attention within and across images—which yields representations particularly useful in this regime. The resulting Hummingbird model, suitably prompted, performs various scene understanding tasks without modification while approaching the performance of specialists that have been finetuned for each task. Moreover, Hummingbird can be configured to perform new tasks much more efficiently than finetuned models, raising the possibility of scene understanding in the interactive assistant regime."
Google DeepMind,Distributional Bellman Operators over Mean-embeddings,9-Dec-23,https://deepmind.google/research/publications/42395/,,https://deepmind.google/research/publications/42395/,"We propose a novel algorithmic framework for distributional reinforcement learn-ing,  based  on  learning  finite-dimensional  mean  embeddings  of  return  distribu-tions. We derive several new algorithms for dynamic programming and temporal-difference learning based on this framework, provide asymptotic convergence the-ory, and examine the empirical performance of the algorithms on a suite of tabulartasks.   Further,  we show that this approach can be straightforwardly combinedwith deep reinforcement learning, and obtain a new deep RL agent that improvesover baseline distributional approaches on the Arcade Learning Environment."
Google DeepMind,POMRL: No-Regret Learning-to-Plan with IncreasingHorizons,8-Dec-23,https://deepmind.google/research/publications/53605/,,https://deepmind.google/research/publications/53605/,"We study the problem of planning under model uncertainty in an online meta-reinforcement learning (RL) setting where an agent is presented with a sequence of related tasks with limited interactions per task. The agent can use its experience in each task \emph{and} across tasks to estimate both the transition model and the distribution over tasks. We propose an algorithm to meta-learn the underlying relatedness across tasks, utilize it to plan in each task, and upper-bound the regret of the planning loss. Our bound suggests that the average regret over tasks decreases as the number of tasks increases and as the tasks are more similar. In the classical single-task setting, it is known that the planning horizon should depend on the estimated model's accuracy, that is, on the number of samples within task. We generalize this finding to meta-RL and study this dependence of planning horizon on the number of tasks. Based on our theoretical findings, we derive heuristics for selecting slowly increasing discount factors, and validate its significance empirically."
Google DeepMind,Revisiting Dynamic Evaluation:Online Adaptation for LLMs,7-Dec-23,https://deepmind.google/research/publications/49871/,,https://deepmind.google/research/publications/49871/,"We revisit dynamic evaluation, the idea of online adapting the parameters of a language model with gradient descent on a given sequence of test tokens. 
While it is generally known that adapting the parameters at test-time improves the overall predictive performance, we pay particular attention to the 
speed of adaptation (in terms of sample efficiency) and computational overhead for performing gradient computation and parameter updates."
Google DeepMind,"SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation",6-Dec-23,https://deepmind.google/research/publications/82492/,,https://deepmind.google/research/publications/82492/,"Reliable automatic evaluation of summarization systems is challenging due to the multifaceted and subjective nature of the task. This is especially the case for languages other than English, where human evaluations are scarce. In this work, we introduce SEAHORSE, a dataset for multilingual, multifaceted summarization evaluation. SEAHORSE consists of 96K summaries with human ratings along 6 dimensions of text quality: comprehensibility, repetition, grammar, attribution, main ideas, and conciseness, covering 6 languages, 9 systems, and 4 datasets. As a result of its size and scope, SEAHORSE can serve both as a benchmark to evaluate learnt metrics, as well as a large-scale resource for training such metrics. We show that metrics trained with SEAHORSE achieve strong performance on the out-of-domain meta-evaluation benchmarks TRUE (Honovich et al., 2022) and mFACE (Aharoni et al., 2022). We make the SEAHORSE dataset and metrics publicly available for future research on multilingual and multifaceted summarization evaluation."
Google DeepMind,"Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia",6-Dec-23,https://deepmind.google/research/publications/64717/,,https://deepmind.google/research/publications/64717/,"Agent-based modeling has been around for decades, and applied widely across the social and natural sciences. The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs are constructed using an LLM to apply common sense to situations, act ""reasonably"", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside. Here we present Concordia, a library to facilitate constructing and working with GABMs. Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments. Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact. Agents take actions by describing what they want to do in natural language. The GM then translates their actions into appropriate implementations. In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects. In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.). Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data."
Google DeepMind,A Benchmark for Reasoning with Spatial Prepositions,6-Dec-23,https://deepmind.google/research/publications/82087/,,https://deepmind.google/research/publications/82087/,"Spatial reasoning is a fundamental building block of human cognition, used in representing, grounding, and reasoning about physical and abstract concepts. We propose a novel benchmark focused on assessing inferential properties of statements with spatial prepositions. The benchmark includes original datasets in English and Romanian and aims to probe the limits of reasoning about spatial relations in large language models. We use prompt engineering to study the performance of two families of large language models, PaLM and GPT-3, on our benchmark. Our results show considerable variability in the performance of smaller and larger models, as well as across prompts and languages. However, none of the models reaches human performance."
Google DeepMind,Gaussian Process Probes (GPP) for Uncertainty-Aware Probing,5-Dec-23,https://deepmind.google/research/publications/83506/,,https://deepmind.google/research/publications/83506/,"Understanding which concepts models can and cannot represent has been fundamental to many tasks: from effective and responsible use of models to detecting out of distribution data. We introduce Gaussian process probes (GPP), a unified and simple framework for probing and measuring uncertainty about concepts represented by models. As a Bayesian extension of linear probing methods, GPP asks what kind of distribution over classifiers (of concepts) is induced by the model. This distribution can be used to measure both what the model represents and how confident the probe is about what the model represents. GPP can be applied to any pre-trained model with vector representations of inputs (e.g., activations). It does not require access to training data, gradients, or the architecture. We validate GPP on datasets containing both synthetic and real images. Our experiments show it can (1) probe a model's representations of concepts even with a very small number of examples, (2) accurately measure both epistemic uncertainty (how confident the probe is) and aleatory uncertainty (how fuzzy the concepts are to the model), and (3) detect out of distribution data using those uncertainty measures as well as classic methods do. By using Gaussian processes to expand what probing can offer, GPP provides a data-efficient, versatile and uncertainty-aware tool for understanding and evaluating the capabilities of machine learning models."
Google DeepMind,RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation,5-Dec-23,https://deepmind.google/research/publications/35829/,,https://deepmind.google/research/publications/35829/,"The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a foundation agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming multi-embodiment action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100--1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks."
Google DeepMind,Small batch deep reinforcement learning,4-Dec-23,https://deepmind.google/research/publications/82494/,,https://deepmind.google/research/publications/82494/,"In value-based deep reinforcement learning with replay memories, the batch size parameter specifies how many transitions to sample for each gradient update. Although critical to the learning process, this value is typically not adjusted when proposing new algorithms. In this work we present a broad empirical study that suggests reducing the batch size can result in a number of significant performance gains; this is surprising, as the general tendency when training neural networks is towards larger batch sizes for improved performance. We complement our experimental findings with a set of empirical analyses towards better understanding this phenomenon."
Google DeepMind,RLHF and IIA: Perverse Incentives,2-Dec-23,https://deepmind.google/research/publications/63806/,,https://deepmind.google/research/publications/63806/,Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA).  The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.
Google DeepMind,Accelerating Neural Field Training via Langevin Monte-Carlo Sampling,29-Nov-23,https://deepmind.google/research/publications/61180/,,https://deepmind.google/research/publications/61180/,"We show how Neural Field training can be accelerated by efficiently choosing where to sample. While Neural Fields have recently become popular, it is often trained by uniformly randomly sampling the training domain, or through handcrafted heuristics. In this work, we show that improved convergence and final training quality can be achieved by smarter sampling. Specifically, we propose a sampling scheme based on Langevin Monte-Carlo sampling that focuses our training samples in the domain where it actually matters."
Google DeepMind,Universal Self-Consistency with Large Language Models,29-Nov-23,https://deepmind.google/research/publications/50879/,,https://deepmind.google/research/publications/50879/,"Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gain on various reasoning tasks, by utilizing multiple reasoning paths sampled by the model. However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages the large language model (LLM) to select the most consistent solution among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, long-context summarization, and open-ended question answering. On mathematical reasoning benchmarks including GSM8K and MATH, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Meanwhile, USC consistently improves the performance over greedy decoding on open-ended generation tasks."
Google DeepMind,Unsupervised Keypoints with Stable Diffusion,29-Nov-23,https://deepmind.google/research/publications/61281/,,https://deepmind.google/research/publications/61281/,"We present an innovative approach to infer semantic concepts from input images using a pre-trained image diffusion model. Traditionally employed for generating images, we repurpose this model to learn semantically consistent key points across diverse image datasets. Our method involves adding noise to the input image, simulating the denoising process, and subsequently extracting the attention maps. From these attention maps, we select the k sharpest ones, subjecting them to further sharpening to enhance precision. Additionally, we introduce an equivariance constraint ensuring that the learned key points remain consistent even under various image transformations. Notably, our approach demonstrates superior performance over competing methods, particularly excelling in non-aligned data, showcasing its potential in accurately identifying and retaining semantically meaningful key points across images."
Google DeepMind,SODA: Bottleneck Diffusion Models for Representation Learning,29-Nov-23,https://deepmind.google/research/publications/44213/,,https://deepmind.google/research/publications/44213/,"We introduce SODA, a self-supervised diffusion model, explored for the purpose of representation learning. The model incorporates a conditional visual encoder, which distills an input image into a compact representation, that, in turn, guides the generation of novel views of the input's content. We show that imposing a tight bottleneck between the visual encoder and the denoising decoder, in the form of a sole embedding through which they can communicate, turns diffusion models into strong and efficient representation learners, capable of capturing and predicting images' semantic properties in an unsupervised manner, as we demonstrate by attaining high performance on varied classification, reconstruction and synthesis tasks over a wide array of datasets ranging from CelebA to ShapeNet to ImageNet. Further investigation of the model's generative qualities reveals the disentangled nature of its emerging latent space, which serves as an effective interface to control and manipulate the produced outputs, so to create diverse views and variations. All in all, we aim to shed light on the exciting and promising potential of diffusion models, not only for image generation, but also for learning rich and robust representations."
Google DeepMind,Replay Across Experiments,27-Nov-23,https://deepmind.google/research/publications/50575/,,https://deepmind.google/research/publications/50575/,"Replaying data is a principal mechanism underlying the stability and data effi-ciency  of  off-policy  reinforcement  learning  (RL)  experiments.   We  present  aneffective  yet  simple  framework  to  extend  the  use  of  replay  data  across  experi-ments,  minimally adapting the RL engineering workflow for sizeable improve-ments in controller performance.  At its core, Replay across Experiments (RaE)involves reusing experience from previous experiments to improve exploration,bootstrap learning and ultimately obtain stronger performance.  We empiricallydemonstrate the robustness and benefits of our approach on a number of RL algo-rithms and challenging control domains spanning both locomotion and manipu-lation including sparsely rewarded tasks with egocentric vision. Furthermore, wedemonstrate how RaE can be leveraged in settings with available existing offlinedatasets to achieve state-of-the-art performance.   Finally,  through various abla-tions we demonstrate the robustness of our approach to the underlying algorithm,quality and amount of data available and various hyperparameter choices."
Google DeepMind,No agent is an island: A social path to human-like artificial intelligence,17-Nov-23,https://deepmind.google/research/publications/25830/,,https://deepmind.google/research/publications/25830/,"In both cognitive science and computer science, intelligence has traditionally been viewed solipsistically, as a property of unitary agents devoid of social context. Yet converging evidence in behavior, neuroscience and evolution shows that natural intelligence emerged at multiple scales in networks of interacting agents. For those interested in reverse-engineering intelligence, these findings suggest constraints on the space of workable algorithms. Recent breakthroughs in artificial intelligence (AI) run parallel to these findings, with specific population structures enabling agents to master complex strategic games like Capture-The-Flag and StarCraft II. We posit that moving beyond a solipsistic view of agency will benefit both cognitive science and machine learning: understanding intelligence demands a multi-agent context."
Google DeepMind,Report of the 1st Workshop on Generative AI and Law,14-Nov-23,https://deepmind.google/research/publications/58352/,,https://deepmind.google/research/publications/58352/,"This report presents the takeaways of the inaugural Workshop on Generative AI and Law, held in July 2023. A cross-disciplinary group of practitioners and scholars from computer science and law convened to discuss the technical, doctrinal, and policy challenges presented by law for generative AI, and by generative AI for law, with an emphasis on US law in particular. We conclude that there is an essential need for 1) a shared knowledge base that provides a common conceptual language for experts across disciplines; 2) clarification of the distinctive technical capabilities of generative-AI systems as compared and contrasted to other computer and AI systems; 3) a logical taxonomy of the legal issues these systems raise; and, 4) a concrete research agenda to promote collaboration and knowledge-sharing on emerging issues at the intersection of generative AI and law. In this report, we synthesize the key takeaways from the GenLaw workshop that begin to address these four needs."
Google DeepMind,DiLoCo: Distributed Low-Communication Training of Language Models,14-Nov-23,https://deepmind.google/research/publications/57039/,,https://deepmind.google/research/publications/57039/,"Large language models (LLM) have become a critical component in many applications of machine learning. However, standard approaches to training LLM require a large number of tightly interconnected accelerators, with devices exchanging gradients and other intermediate states at each optimization step.  While it is difficult to build and maintain a single computing cluster hosting many accelerators, it might be easier to find several computing clusters each hosting a smaller number of devices.  In this work, we propose a distributed optimization algorithm, Distributed Low-Communication (DiLoCo), that enables training of language models on islands of devices that are poorly connected. The approach is a variant of federated averaging, where the number of inner steps is large, the inner optimizer is AdamW, and  the outer optimizer is Nesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8 workers performs as well as fully synchronous optimization while communicating 500 times less. DiLoCo exhibits great robustness to the data distribution of each worker.  It is also robust to resources becoming unavailable over time, and vice versa, it can seamlessly leverage resources that become available during training."
Google DeepMind,GraphCast: Learned Global Weather Forecasting,14-Nov-23,https://deepmind.google/research/publications/22598/,,https://deepmind.google/research/publications/22598/,"We introduce a learned weather simulator—called “GraphCast”—which outperforms the most accurate operational medium-range weather forecasting system in the world. GraphCast operates on a0.25°latitude-longitude grid, and uses graph neural networks and a novel high-resolution multi-scale mesh representation, to autoregressively predict the 10-day temporal trajectories of 227 key dynamic variables that represent the state of the atmosphere, at 6-hour time intervals. Our results show GraphCast is significantly more accurate than the European Centre for Medium-Range Weather Forecasts (ECMWF)’s deterministic forecasting system, “HRES”, on89.3%of the 2760 target variables and lead times we evaluated. It also outperforms the most accurate previous machine learning weather forecasting model on98.8%of the 252 targets it reported. GraphCast is orders of magnitude faster than ECMWF’s operational systems, and can generate a 10-day forecast (35 gigabytes on disk) in under 60 seconds on Cloud TPU hardware. Together these results represent a key step forward in improving weather modeling with machine learning, open new opportunities for fast, accurate forecasting, and help realize the promise of machine-learning based simulation in the physical sciences."
Google DeepMind,"Emotions and courtship help bonded pairs cooperate, but emotional agents are vulnerable to deceit",10-Nov-23,https://deepmind.google/research/publications/35526/,,https://deepmind.google/research/publications/35526/,"Coordinated pair bonds are common in birds and also occur in many other taxa. How do animals solve the social dilemmas they face in coordinating with a partner? We developed an evolutionary model to explore this question, based on observations that a) neuroendocrine feedback provides emotional bookkeeping which is thought to play a key role in vertebrate social bonds and b) these bonds are developed and maintained via courtship interactions that include low-stakes social dilemmas. Using agent-based simulation, we found that emotional bookkeeping and courtship sustained cooperation in the iterated prisoner’s dilemma in noisy environments, especially when combined. However, when deceitful defection was possible at low cost, courtship often increased cooperation, whereas emotional bookkeeping decreased it."
Google DeepMind,Role Play with Large Language Models,8-Nov-23,https://deepmind.google/research/publications/35223/,,https://deepmind.google/research/publications/35223/,"As dialogue agents become increasingly human-like in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness."
Google DeepMind,Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu Search,6-Nov-23,https://deepmind.google/research/publications/44214/,,https://deepmind.google/research/publications/44214/,"This work studies a central extremal graph theory problem inspired by a 1975 conjecture of Erdos, which aims to find graphs with a given size (number of nodes) that maximize the number of edges without having 3- or 4-cycles. We formulate this problem as a sequential decision-making problem and compare AlphaZero, a neural network-guided tree search, with tabu search, a heuristic local search method. Using either method, by introducing a curriculum—jump-starting the search for larger graphs using good graphs found at smaller sizes—we improve the state-of-the-art lower bounds for several sizes. We also propose a flexible graph-generation environment and a permutation-invariant network architecture for learning to search in the space of graphs."
Google DeepMind,Grammar Prompting for Domain-Specific Language Generation with Large Language Models,3-Nov-23,https://deepmind.google/research/publications/83400/,,https://deepmind.google/research/publications/83400/,"Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We propose \emph{grammar prompting}, a simple approach to enable LLMs to use external knowledge and domain-specific constraints, expressed through a grammar in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and SMILES-based molecule generation."
Google DeepMind,RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches,3-Nov-23,https://deepmind.google/research/publications/48757/,,https://deepmind.google/research/publications/48757/,"In order for generalist robot policies to operate robustly in the real world, they must be able to accomplish many tasks in a wide array of situations, potentially beyond those seen in training datasets.
Recent works have studied generalization in such settings, by leveraging policy conditioning with modalities such as natural language or object-centric visual representations.
While these methods have shown promise in high-level generalization to semantics, objects, or visual distribution shifts, there remain significant open challenges in novel low-level motion generalization.
This challenge is both a theoretical and practical barrier in scaling robot learning methods, as real-world robot situations may demand significantly more low-level motion capabilities than contained in common large-scale tabletop pick and place datasets.
Towards tackling this, we propose policy conditioning with rough trajectory sketches, which strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to require learning-based policies to interpret the trajectory sketch in the context of situational visual observations.
The trajectory sketch is also a scalable and flexible representation: during training it can be generated in hindsight from proprioception sensors and during inference time it can be specified through simple human inputs like drawing or videos, or through automated methods such as modern image-generating or waypoint-generating methods.
We present trajectory-conditioned policies within a holistic framework, which we demonstrate at scale on a variety of real world robotic tasks which require diverse motions that go beyond tabletop pick and place tasks that our method was trained on."
Google DeepMind,Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL,2-Nov-23,https://deepmind.google/research/publications/24720/,,https://deepmind.google/research/publications/24720/,"While policy optimization algorithms have played an important role in recent empirical success of Reinforcement Learning (RL), the existing theoretical understanding of policy optimization remains rather limited---they are either restricted to tabular MDPs or suffer from highly suboptimal sample complexity, especial in online RL where exploration is necessary. This paper proposes a simple efficient policy optimization framework---Optimistic NPG for online RL. Optimistic NPG can be viewed as simply combining of the classic natural policy gradient (NPG) algorithm [Kakade, 2001] with optimistic policy evaluation subroutines to encourage exploration. For $d$-dimensional linear MDPs, Optimistic NPG is computationally efficient, and learns an $\epsilon$-optimal policy within $\tilde{O}(d^2/\epsilon^3)$ samples, which is the first computationally efficient algorithm whose sample complexity has the optimal dimension dependence $\tilde{\Theta}(d^2)$. It also improves over state-of-the-art results of policy optimization algorithms [Zanette et al., 2021] by a factor of $d$. For general function approximation that subsumes linear MDPs, Optimistic NPG, to our best knowledge, is also the first policy optimization algorithm that achieves the polynomial sample complexity for learning near-optimal policies."
Google DeepMind,Optimistic Meta-Gradients,2-Nov-23,https://deepmind.google/research/publications/5642/,,https://deepmind.google/research/publications/5642/,"We study the connection between gradient-based meta-learning and convex optimisation. We observe that gradient descent with momentum is a special case of meta-gradients, and building on recent results in optimisation, we prove convergence rates for meta learning in the single task setting. While a meta-learned update rule can yield faster convergence up to constant factor, it is not sufficient for acceleration. Instead, some form of optimism is required. We show that optimism in meta-learning can be captured through the recently proposed Bootstrapped Meta-Gradient (Flennerhag et. al., 2022) method, providing deeper insight into its underlying mechanics."
Google DeepMind,RoboVQA: Multimodal Long-Horizon Reasoning for Robotics,1-Nov-23,https://deepmind.google/research/publications/63605/,,https://deepmind.google/research/publications/63605/,"We present a scalable, bottom-up and intrinsically diverse data collection scheme that can be used for high-level reasoning with long and medium horizons and that has 2.2x higher throughput compared to traditional narrow top-down step-by-step collection. We collect realistic data by performing any user requests within the entirety of 3 office buildings and using multiple robot and human embodiments. With this data, we show that models trained on all embodiments perform better than ones trained on the robot data only, even when evaluated solely on robot episodes. We find that for a fixed collection budget it is beneficial to take advantage of cheaper human collection along with robot collection. We release a large and highly diverse (29,520 unique instructions) dataset dubbed RoboVQA containing 829,502 (video, text) pairs for robotics-focused visual question answering. We also demonstrate how evaluating real robot experiments with an intervention mechanism enables performing tasks to completion, making it deployable with human oversight even if imperfect while also providing a single performance metric. We demonstrate a single video-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is capable of performing a variety of grounded high-level reasoning tasks in broad realistic settings with a cognitive intervention rate 46% lower than the zero-shot state of the art visual language model (VLM) baseline and is able to guide real robots through long-horizon tasks. The performance gap with zero-shot state-of-the-art models indicates that a lot of grounded data remains to be collected for real-world deployment, emphasizing the critical need for scalable data collection approaches. Finally, we show that video VLMs significantly outperform single-image VLMs with an average error rate reduction of 19% across all VQA tasks. Data and videos available at https://robovqa.github.io"
Google DeepMind,Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning,30-Oct-23,https://deepmind.google/research/publications/22497/,,https://deepmind.google/research/publications/22497/,"Progress in fields of machine learning and adversarial planning has benefited significantly from bench-mark domains, from Checkers and Chess, the classic UCI data sets and Netflix challenge to BLEU,Atari,  Go,  Poker,  Starcraft,  Dota2,  and  Diplomacy.   In sequential decision-making,  agent eval-uation  has  largely  been  restricted  to  very  few  interactions  against  experts,  declaring  victory  upon reaching  some  desired  level  of  performance  (e.g.human  professional).    In  this  paper,  we  propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three  tournament entries,  some of which are (intentionally)sub-optimal.   We describe metrics to measure the quality of  agents  based  both  on  average  returns and exploitability.   We then show that several re-cent RL and online learning approaches can learng ood counter-strategies and generalize well, but ultimately lose to the top-performing bots,  creating an opportunity for research in multiagent learning."
Google DeepMind,Generative replay for compositional visual understanding in the prefrontal-hippocampal circuit,26-Oct-23,https://deepmind.google/research/publications/5630/,,https://deepmind.google/research/publications/5630/,"Understanding the visual world is a constructive and compositional process. Whilst a frontal-
hippocampal circuit is known to be essential for this task, little is known about the associated neuronal
computations. Although visual understanding appears superficially distinct from other known functions
of this circuit, such as spatial reasoning, compositional inference, and model-based planning, recent
models suggest deeper computational similarities. Here, using fMRI, we show that representations of
visual objects in these brain regions are relational and compositional – key computational properties
theorised to support rapid construction of hippocampal maps. Using MEG, we show that rapid
sequences of representations, akin to replay in spatial navigation and planning problems, are also
engaged in visual understanding. Whilst these sequences have previously been proposed as
mechanisms to plan possible futures or learn from the past, here they are used to understand the
present. Replay sequences change content over timescales of understanding and form constructive
hypotheses about possible object configurations. These hypotheses play out in an order that supports
relational inference, progressing from predictable to uncertain scene elements, gradually constraining
possible configurations, and converging on the correct configuration. Together, these results suggest
a computational bridge between apparently distinct functions of hippocampal-prefrontal circuitry, and
a role for generative replay in compositional inference and hypothesis testing."
Google DeepMind,Scalable Neural Network Kernels,20-Oct-23,https://deepmind.google/research/publications/50474/,,https://deepmind.google/research/publications/50474/,"We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universal random features (or URFs), applied to instantiate several SNNK variants, and interesting on its own in the context of scalable kernel methods. We provide rigorous theoretical analysis of all these concepts as well as an extensive empirical evaluation, ranging from point-wise kernel estimation to Transformers' fine-tuning with novel adapter layers inspired by SNNKs. Our mechanism provides up to 5x reduction in the number of trainable parameters, while maintaining competitive accuracy."
Google DeepMind,Scalable Diffusion for Materials Generation,18-Oct-23,https://deepmind.google/research/publications/51282/,,https://deepmind.google/research/publications/51282/,"​​​​Generative models trained on internet-scale data are capable of generating novel yet highly realistic texts, images, and videos. A natural next question is whether these models can advance science through means such as generating novel stable materials. Traditionally, models with explicit structures (e.g., graphs) have been used in modeling structural relationships in scientific data (e.g., atoms and bonds in crystals), but generating structures explicitly might be difficult to scale to large and complex systems. Another challenge to generative models of materials lies in the mismatch between generative modeling metrics and the downstream applications. For instance, common metrics such as the reconstruction error do not correlate well with the downstream goal of discovering novel stable materials. In this work, we tackle the scalability challenge by first developing a unified crystal representation that can effectively represent any crystal structures (UniMat), followed by training a diffusion probabilistic model on the UniMat representations. Our empirical results suggest that despite the lack of explicit structure modeling, UniMat can generate high fidelity crystal structures from larger and more complex chemical systems, outperforming previous graph-based approaches under various generative modeling metrics. To better connect the generation quality of materials to downstream applications such as discovering novel stable materials, we propose to use decomposition energy from Density Function Theory (DFT) calculations and the resulting stability with respect to convex hulls as additional evaluation metrics for generative models of materials. Lastly, we show that conditional generation with UniMat can scale to previously established crystal datasets with up to millions of crystals structures, and outperform random structure search (the current leading method for structure discovery) in discovering new stable materials."
Google DeepMind,Sociotechnical Safety Evaluation of generative AI systems,18-Oct-23,https://deepmind.google/research/publications/45425/,,https://deepmind.google/research/publications/45425/,"Generative AI systems produce potential ethical and social risks that require comprehensive evaluation. In this paper, we propose a three-layered framework toward a structured, sociotechnical approach to evaluate these risks. The approach here evolves from a system safety perspective, particularly the insight that context determines whether a given capability causes harm. Applied to AI, this perspective shows that evaluation at multiple layers of analysis is required in order to assess whether or not an AI system may cause a given harm. Sociotechnical evaluation begins with the main current approach to safety evaluation, capability testing, which considers AI system outputs and their components in isolation. Building on this, it accounts for two further layers of context. The second layer centres the human interacting with a system to assess harm that may occur at the point of use. Broader systemic impacts on the structures into which an AI system is embedded require evaluation at the third, systemic evaluation layer. The second main contribution of this paper is an overview of the current state of sociotechnical evaluation. Reviewing all available evaluations for social and ethical risks known to a wide range of cross-industry researchers, we map gaps in the status quo. We then carefully lay out a roadmap of tractable steps toward closing the identified gaps."
Google DeepMind,DyST: Towards Dynamic Neural Scene Representations on Real-World Videos,9-Oct-23,https://deepmind.google/research/publications/48657/,,https://deepmind.google/research/publications/48657/,"Visual understanding of our world goes beyond the semantics and flat structure of individual images. In this paper, we work towards capturing both the 3D structure as well as the dynamics of real-world scenes from monocular real-world videos. Our model, the Dynamic Scene Transformer (DyST), builds upon recent work in neural scene representation and learns a latent decomposition into scene content as well as per-view scene dynamics and camera pose. This separation is achieved through a special co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene."
Google DeepMind,Step-Back Prompting Enables Reasoning via Abstraction in Large Language Models,9-Oct-23,https://deepmind.google/research/publications/50274/,,https://deepmind.google/research/publications/50274/,"We present STEP-BACK PROMPTING, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of STEP-BACK PROMPTING with PaLM-2 models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, STEP-BACK PROMPTING improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 34%, and MuSiQue by 7%."
Google DeepMind,Learning Interactive Real-World Simulator,9-Oct-23,https://deepmind.google/research/publications/47545/,,https://deepmind.google/research/publications/47545/,"Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate the real world in response to actions carried out by humans, robots, and other types of interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies to training embodied agents purely in simulation that can be directly deployed in the real world. In this paper, we explore these possibilities around learning a universal simulator (UniSim) of real-world interactions through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich in different axes (e.g., rich labeled objects in image data, rich actions in robotics data, and rich movements in navigation data). With careful orchestration of diverse datasets each serving a different piece of the puzzle, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move to x, y location” from otherwise static scenes and objects. The usage of a real-world simulator is vast. As an example, we use UniSim to simulate interactive experiences to train high-level vision-language planners and low-level reinforcement learning policies, both of which exhibit significant real-world transfer from purely training in a real-world like simulator. Lastly, we show that other types of intelligence such as video captioning and detection models can also benefit from simulated experiences of UniSim, opening up even wider applications of a real-world simulator."
Google DeepMind,Repelling Random Walks,7-Oct-23,https://deepmind.google/research/publications/48455/,,https://deepmind.google/research/publications/48455/,"We present a novel quasi-Monte Carlo mechanism to improve graph-based sampling, coined repelling random walks. By inducing correlations between the trajectories of an interacting ensemble such that their marginal transition probabilities are unmodified, we are able to explore the graph more efficiently, improving the concentration of statistical estimators whilst leaving them unbiased. The mechanism has a trivial drop-in implementation. We showcase the effectiveness of repelling random walks in a range of settings including estimation of graph kernels, the PageRank vector and graphlet concentrations. We provide detailed experimental evaluation and robust theoretical guarantees. To our knowledge, repelling random walks constitute the first rigorously studied quasi-Monte Carlo scheme correlating the directions of walkers on a graph, inviting new research in this exciting nascent domain."
Google DeepMind,Universal Graph Random Features,7-Oct-23,https://deepmind.google/research/publications/48555/,,https://deepmind.google/research/publications/48555/,"We propose a novel random walk-based algorithm for unbiased estimation of arbitrary functions of a weighted adjacency matrix, coined universal graph random features (u-GRFs). This includes many of the most popular examples of kernels defined on the nodes of a graph. Our algorithm enjoys subquadratic time complexity with respect to the number of nodes, overcoming the notoriously prohibitive cubic scaling of exact graph kernel evaluation. It can also be trivially distributed across machines, permitting learning on much larger networks. At the heart of the algorithm is a modulation function which upweights or downweights the contribution from different random walks depending on their lengths. We show that by parameterising it with a neural network we can obtain u-GRFs that give higher-quality kernel estimates or perform efficient, scalable kernel learning. We provide robust theoretical analysis and support our findings with experiments including pointwise estimation of fixed graph kernels, solving non-homogeneous graph ordinary differential equations, node clustering and kernel regression on triangular meshes."
Google DeepMind,Assessing LLMs on Climate Information,5-Oct-23,https://deepmind.google/research/publications/43202/,,https://deepmind.google/research/publications/43202/,"Understanding how climate change affects us and learning about available solutions are key steps toward empowering individuals and communities to mitigate and adapt successfully. As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in this domain. In this study, we present a comprehensive evaluation framework, grounded in science communication principles, to analyze LLM responses on climate change topics. This framework emphasizes both the presentational and epistemological adequacy of answers, offering a fine-grained analysis of LLM response systems. Spanning 8 dimensions, our framework discerns up to 30 distinct issues in model outputs. The task is an excellent real-world example for a growing number of problems where LLM abilities can exceed those of humans. To address these challenges, we provide a practical protocol for scalable oversight using AI assistance, and rely on raters with relevant educational background. We evaluate several recent LLMs and conduct a thorough analysis of the results. Our comprehensive analysis sheds light on the potential and limitations of LLMs in the realm of climate communication."
Google DeepMind,An Impossibility Theorem in Game Dynamics,5-Oct-23,https://deepmind.google/research/publications/29062/,,https://deepmind.google/research/publications/29062/,"The Nash equilibrium—a combination of choices by the players of a game from which no self-interested player would deviate—is the predominant solution concept in game theory. Even though every game has a Nash equilibrium, it is not known whether there are deterministic behaviors of the players who play a game repeatedly that are guaranteed to converge to a Nash equilibrium of the game from all starting points. If one assumes that the players’ behavior is a discrete-time or continuous-time rule whereby the current mixed strategy profile is mapped to the next, this question becomes a problem in the theory of dynamical systems. We apply this theory, and in particular Conley index theory, to prove a general impossibility result: There exist games, for which all game dynamics fail to converge to Nash equilibria from all starting points. The games which help prove this impossibility result are degenerate, but we conjecture that the same result holds, under computational complexity assumptions, for nondegenerate games. We also prove a stronger impossibility result for the solution concept of approximate Nash equilibria: For a set of games of positive measure, no game dynamics can converge to the set of approximate Nash equilibria for a sufficiently small yet substantial approximation bound. Our results establish that, although the notions of Nash equilibrium and its computation-inspired approximations are universally applicable in all games, they are fundamentally incomplete as predictors of long-term player behavior."
Google DeepMind,Large Language Models Cannot Self-Correct Reasoning Yet,3-Oct-23,https://deepmind.google/research/publications/48252/,,https://deepmind.google/research/publications/48252/,"Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without any external feedback. Contrary to popular belief, our research indicates that LLMs might find this task challenging, especially in the context of reasoning, and at times, their performance might even degrade post self-correction. Given our findings, we encourage the community to approach this concept with both prudence and critical consideration."
Google DeepMind,Large Language Models as Analogical Reasoners,3-Oct-23,https://deepmind.google/research/publications/51283/,,https://deepmind.google/research/publications/51283/,"Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically demands labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical reasoning, designed to automatically guide the reasoning process of large language models. Drawing inspiration from how humans recall relevant past experiences when addressing new challenges, our approach makes language models self-generate relevant exemplars or lessons in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for manual annotation or external data retrieval, offering generality and convenience, while also tailoring generated exemplars to each unique problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench, with an average accuracy gain of +5%."
Google DeepMind,TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement,2-Oct-23,https://deepmind.google/research/publications/26336/,,https://deepmind.google/research/publications/26336/,"We present a novel model for Tracking Any Point (TAP) that effectively tracks any queried point on any physical surface throughout a video sequence. Our approach employs two stages: (1) a matching stage, which independently locates a suitable candidate point match for the query point on every other frame, and (2) a refinement stage, which updates both the trajectory and query features based on local correlations. The resulting model surpasses all baseline methods by a significant margin on the TAP-Vid benchmark, as demonstrated by an approximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Our model facilitates fast inference on long and high-resolution video sequences. On a modern GPU, our implementation has the capacity to track points faster than real-time, and can be flexibly extended to higher-resolution videos. Given the high-quality trajectories extracted from a large dataset, we demonstrate a proof-of-concept diffusion model which generates trajectories from static images, enabling plausible animations. Visualizations, source code, and pretrained models can be found at https://deepmind-tapir.github.io/"
Google DeepMind,3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation,2-Oct-23,https://deepmind.google/research/publications/14720/,,https://deepmind.google/research/publications/14720/,"The ability to perceive and understand 3D scenes is crucial for many applications in computer vision and robotics. Inverse graphics is an appealing approach to 3D scene understanding that aims to infer the 3D scene structure from 2D images. In this paper, we introduce probabilistic modeling to the inverse graphics framework to quantify uncertainty and achieve robustness in 6D pose estimation tasks. Specifically, we propose 3D Neural Embedding Likelihood (3DNEL) as a unified probabilistic model over RGB-D images, and develop efficient inference procedures on 3D scene descriptions. 3DNEL effectively combines learned neural embeddings from RGB with depth information to improve robustness in sim-to-real 6D object pose estimation from RGB-D images. Performance on the YCB-Video dataset is on par with state-of-the-art yet is much more robust in challenging regimes. In contrast to discriminative approaches, 3DNEL's probabilistic generative formulation jointly models multiple objects in a scene, quantifies uncertainty in a principled way, and handles object pose tracking under heavy occlusion. Finally, 3DNEL provides a principled framework for incorporating prior knowledge about the scene and objects, which allows natural extension to additional tasks like camera pose tracking from video."
Google DeepMind,Theoretical and Practical Perspectives on what Influence Functions Do,23-Sep-23,https://deepmind.google/research/publications/56635/,,https://deepmind.google/research/publications/56635/,"Influence functions (IF) have been seen as a technique for explaining model predictions through the lens of the training data. Their utility is assumed to be in identifying training examples ""responsible"" for a prediction so that, for example, correcting a prediction is possible by intervening on those examples (removing or editing them) and retraining the model. However, recent empirical studies have shown that the existing methods of estimating IF predict the leave-one-out-and-retrain effect poorly.
In order to understand the mismatch between the theoretical promise and the practical results, we analyse five assumptions made by IF methods which are problematic for modern-scale deep neural networks and which concern convexity, numeric stability, training trajectory and parameter divergence. This allows us to clarify what can be expected theoretically from IF. We show that while most assumptions can be addressed successfully, the parameter divergence poses a clear limitation on the predictive power of IF: influence fades over training time even with deterministic training. We illustrate this theoretical result with BERT and ResNet models.
Another conclusion from the theoretical analysis is that IF are still useful for model debugging and correcting even though some of the assumptions made in prior work do not hold: using natural language processing and computer vision tasks, we verify that mis-predictions can be successfully corrected by taking only a few fine-tuning steps on influential examples."
Google DeepMind,Self-Predictive Universal AI,21-Sep-23,https://deepmind.google/research/publications/34416/,,https://deepmind.google/research/publications/34416/,"Reinforcement Learning (RL) algorithms typically utilize learning and/or planning techniques to derive effective policies. The integration of both approaches has proven to be highly successful in addressing complex sequential decision-making challenges, as evidenced by algorithms such as AlphaZero and MuZero, which consolidate the planning process into a parametric search-policy. AIXI, the most potent theoretical universal agent, leverages planning through comprehensive search as its primary means to find an optimal policy. Here we define an alternative universal agent, which we call Self-AIXI, that on the contrary to AIXI, maximally exploits learning to obtain good policies. It does so by self-predicting its own stream of action data, which is generated, similarly to other TD(0) agents, by taking an action maximization step over the current on-policy (universal mixture-policy) Q-value estimates. We prove that Self-AIXI converges to AIXI, and inherits a series of properties like maximal Legg-Hutter intelligence and the self-optimizing property."
Google DeepMind,Accurate proteome-wide missense variant effect prediction with AlphaMissense,19-Sep-23,https://deepmind.google/research/publications/21083/,,https://deepmind.google/research/publications/21083/,"The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89% of missense variants as either likely benign or likely pathogenic."
Google DeepMind,Revisiting Energy Based Models as Policies: Ranking Noise Contrastive Estimation and Interpolating Energy Models,11-Sep-23,https://deepmind.google/research/publications/82491/,,https://deepmind.google/research/publications/82491/,"Sumeet Singh, Stephen Tu*, Vikas Sindhwani"
Google DeepMind,Massively Scalable Inverse Reinforcement Learning for Route Optimization,10-Sep-23,https://deepmind.google/research/publications/34011/,,https://deepmind.google/research/publications/34011/,"Globally-scalable route optimization based on human preferences remains an open problem. Although past work created increasingly general solutions for the inverse reinforcement learning (IRL) formulation, these have not been successfully scaled to world-sized MDPs (200M states), large datasets (110M samples), and nearly foundation-sized models (360M parameters). In this work, we surpass this scale through a series of advancements focused on graph compression, parallelization, and problem initialization based on dominant eigenvectors. We introduce Receding Horizon Inverse Planning (RHIP), an approximated IRL algorithm which generalizes existing work and enables control of key performance trade-offs via a planning horizon parameter. Our policy achieves an 18% improvement in global route quality, and, to our knowledge, is the largest instance of IRL in a real-world setting to date. We include insightful negative results on state-of-the-art eigenvalue solvers, and identify future opportunities to further improve performance via IRL-specific batching strategies. Our results show critical benefits to more sustainable modes of transportation (e.g. two-wheelers), where factors beyond journey time (e.g. route safety) play an outsized role."
Google DeepMind,Estimating Gibbs free energies via isobaric-isothermal flows,4-Sep-23,https://deepmind.google/research/publications/16942/,,https://deepmind.google/research/publications/16942/,"We present a machine-learning model based on normalizing flows that is trained to sample from the isobaric-isothermal ensemble. In our approach, we approximate the joint distribution of a fully-flexible triclinic simulation box and particle coordinates to achieve a desired internal pressure. This novel extension of flow-based sampling to the isobaric-isothermal ensemble yields direct estimates of Gibbs free energies. We test our NPT-flow on monatomic water in the cubic and hexagonal ice phases and find excellent agreement of Gibbs free energies and other observables compared with established baselines."
Google DeepMind,RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation,31-Aug-23,https://deepmind.google/research/publications/38759/,,https://deepmind.google/research/publications/38759/,"For robots to be useful outside labs and factories we need to be able to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational bottleneck to allow fast and general-purpose learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene. We show that this results in robust robot policies that can solve complex object-arrangement tasks such as kitting, stacking up to four objects, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes."
Google DeepMind,Natural Quantum Monte Carlo Computation of Excited States,31-Aug-23,https://deepmind.google/research/publications/41485/,,https://deepmind.google/research/publications/41485/,"We present a variational Monte Carlo algorithm for estimating the lowest excited states of a quantum system which is a natural generalization of the estimation of ground states. The method requires no explicit orthogonalization of the different states, instead transforming the problem of finding excited states of a given system into that of finding the ground state of an expanded system. Expected values of arbitrary observables can be calculated, including mixed expectations between different states such as the transition dipole moment. Although the method is entirely general, it works particularly well in conjunction with recent work on using neural networks as variational Ansätze for many-electron systems, and we show that by combining this method with the FermiNet and Psiformer Ansätze we can accurately recover vertical excitation energies and oscillator strengths on molecules as large as benzene. Beyond the examples on molecules presented here, we expect this technique will be of great interest for applications of variational quantum Monte Carlo to atomic, nuclear and condensed matter physics."
Google DeepMind,Nevis’22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research,22-Aug-23,https://deepmind.google/research/publications/32195/,,https://deepmind.google/research/publications/32195/,"A shared goal of several machine learning communities like continual learning, meta-learning and transfer learning, is to design algorithms and models that efficiently and robustly adapt to unseen tasks. An even more ambitious goal is to build models that never stop adapting, and that become increasingly more efficient through time by suitably transferring the accrued knowledge.
Beyond the study of the actual learning algorithm and model architecture, there are several hurdles towards our quest to build such models, such as the choice of learning protocol, metric of success and data needed to validate research hypotheses. In this work, we introduce the Never-Ending VIsual-classification Stream (NEVIS'22), a benchmark consisting of a stream of over $100$ visual classification tasks, sorted chronologically and extracted from papers sampled uniformly from computer vision proceedings spanning the last three decades. The resulting stream reflects what the research community thought was meaningful at any point in time, and it serves as an ideal test bed to assess how well models can adapt to new tasks, and do so better and more efficiently as time goes by. Despite being limited to classification, the resulting stream has a rich diversity of tasks from OCR, to texture analysis, scene recognition, and so forth. The diversity is also reflected in the wide range of dataset sizes, spanning over four orders of magnitude. Overall, NEVIS'22 poses an unprecedented challenge for current sequential learning approaches due to the scale and diversity of tasks, yet with a low entry barrier as it is limited to a single modality and well understood supervised learning problems. Moreover, we provide a reference implementation including strong baselines and an evaluation protocol to compare methods in terms of their trade-off between accuracy and compute. We hope that NEVIS'22 can be useful to researchers working on continual learning, meta-learning, AutoML and more generally sequential learning, and help these communities join forces towards more robust models that efficiently adapt to a never ending stream of data."
Google DeepMind,Levin Tree Search with Context Models,19-Aug-23,https://deepmind.google/research/publications/21589/,,https://deepmind.google/research/publications/21589/,Levin Tree Search (LTS) is a tree/graph search algorithm guided by a (conditional and contextual) probability distribution over action sequences. It comes with strong theoretical guarantees on the search effort required before finding a solution. It also comes with its own loss function (aka objective function) so that a parameterized policy can be learnt to minimize the search effort.
Google DeepMind,Advances in ML-based sampling for Lattice-QCD,4-Aug-23,https://deepmind.google/research/publications/8258/,,https://deepmind.google/research/publications/8258/,"Generative models are one of the successes of ML, ever growing in sophistication, examples blah blah. There is greatpotential for transformative impact applying this technology to computational problems in the science domain, but typicallythese problems have unique structures and features which require custom algorithms and approaches. This Nature Perspectiveoutlines the challenges and opportunities that arise in the application machine learning and in particular generative models forsampling problems in physics, with a focus on the use case of lattice quantum field theory calculations. LQFT is one of the mostsignificant consumers of open-science computing resources, so both the challenges – which involve scaling custom ML-basedsampling to exascale HPC resources – and the benefits of breaking through current computational barriers, are immense."
Google DeepMind,Line Search for Convex Minimization,1-Aug-23,https://deepmind.google/research/publications/37648/,,https://deepmind.google/research/publications/37648/,"Golden-section search and bisection search are the two main principled algorithms for 1d minimization of quasiconvex (unimodal) functions.
The first one only uses function queries, while the second one
also uses gradient queries.
Other algorithms exist under much stronger assumptions, such as Newton's method for twice-differentiable, strongly convex functions with Lipschitz gradients.
However, to the best of our knowledge, there is no principled exact line search algorithm that for general convex functions — including piecewise-linear and max-compositions of convex functions — that takes advantage of convexity."
Google DeepMind,Search-Improved Game-Theoretic Multiagent Reinforcement Learning in General and Negotiation Games,,https://arxiv.org/abs/2302.00797,,https://arxiv.org/abs/2302.00797,"Multiagent reinforcement learning (MARL) has benefited significantly from
population-based and game-theoretic training regimes. One approach,
Policy-Space Response Oracles (PSRO), employs standard reinforcement learning
to compute response policies via approximate best responses and combines them
via meta-strategy selection. We augment PSRO by adding a novel search procedure
with generative sampling of world states, and introduce two new meta-strategy
solvers based on the Nash bargaining solution. We evaluate PSRO's ability to
compute approximate Nash equilibrium, and its performance in two negotiation
games: Colored Trails, and Deal or No Deal. We conduct behavioral studies where
human participants negotiate with our agents ($N = 346$). We find that search
with generative modeling finds stronger policies during both training time and
test time, enables online Bayesian co-player prediction, and can produce agents
that achieve comparable social welfare negotiating with humans as humans
trading among themselves."
Google DeepMind,Is forgetting less a good inductive bias for forward transfer?,,https://openreview.net/forum?id=dL35lx-mTEs,,https://openreview.net/forum?id=dL35lx-mTEs,"One of the main motivations of studying continual learning is that the problem setting allows a model to accrue knowledge from past tasks to learn new tasks more efficiently. However, recent studies suggest that the key metric that continual learning algorithms optimize, reduction in catastrophic forgetting, does not correlate well with the forward transfer of knowledge. We believe that the conclusion previous works reached is due to the way they measure forward transfer. We argue that the measure of forward transfer to a task should not be affected by the restrictions placed on the continual learner in order to preserve knowledge of previous tasks. Instead, forward transfer should be measured by how easy it is to learn a new task given a set of representations produced by continual learning on previous tasks. Under this notion of forward transfer, we evaluate different continual learning algorithms on a variety of image classification benchmarks. Our results indicate that less forgetful representations lead to a better forward transfer suggesting a strong correlation between retaining past information and learning efficiency on new tasks. Further, we found less forgetful representations to be more diverse and discriminative compared to their forgetful counterparts. "
Google DeepMind,Meta-Learning Black-Box Optimization via Black-Box Optimization,,https://openreview.net/forum?id=mFDU0fP3EQH,,https://openreview.net/forum?id=mFDU0fP3EQH,"Optimizing functions without access to gradients is the remit of black-box meth- ods such as evolution strategies. While highly general, their learning dynamics are often times heuristic and inflexible — exactly the limitations that meta-learning can address. Hence, we propose to discover effective update rules for evolution strategies via meta-learning. Concretely, our approach employs a search strategy parametrized by a self-attention-based architecture, which guarantees the update rule is invariant to the ordering of the candidate solutions. We show that meta-evolving this system on a small set of representative low-dimensional analytic optimization problems is sufficient to discover new evolution strategies capable of generalizing to unseen optimization problems, population sizes and optimization horizons. Furthermore, the same learned evolution strategy can outperform established neuroevolution baselines on supervised and continuous control tasks. As additional contributions, we ablate the individual neural network components of our method; reverse engineer the learned strategy into an explicit heuristic form, which remains highly competitive; and show that it is possible to self-referentially train an evolution strategy from scratch, with the learned update rule used to drive the outer meta-learning loop."
Google DeepMind,"Equilibrium-Invariant Embedding, Metric Space, and Fundamental Set of 2×2 Normal-Form Games",,https://arxiv.org/pdf/2304.09978,,https://arxiv.org/pdf/2304.09978,"Equilibrium solution concepts of normal-form games, such as Nash equilibria,
correlated equilibria, and coarse correlated equilibria, describe the joint
strategy profiles from which no player has incentive to unilaterally deviate.
They are widely studied in game theory, economics, and multiagent systems.
Equilibrium concepts are invariant under certain transforms of the payoffs. We
define an equilibrium-inspired distance metric for the space of all normal-form
games and uncover a distance-preserving equilibrium-invariant embedding.
Furthermore, we propose an additional transform which defines a
better-response-invariant distance metric and embedding. To demonstrate these
metric spaces we study $2\times2$ games. The equilibrium-invariant embedding of
$2\times2$ games has an efficient two variable parameterization (a reduction
from eight), where each variable geometrically describes an angle on a unit
circle. Interesting properties can be spatially inferred from the embedding,
including: equilibrium support, cycles, competition, coordination, distances,
best-responses, and symmetries. The best-response-invariant embedding of
$2\times2$ games, after considering symmetries, rediscovers a set of 15 games,
and their respective equivalence classes. We propose that this set of game
classes is fundamental and captures all possible interesting strategic
interactions in $2\times2$ games. We introduce a directed graph representation
and name for each class. Finally, we leverage the tools developed for
$2\times2$ games to develop game theoretic visualizations of large normal-form
and extensive-form games that aim to fingerprint the strategic interactions
that occur within."
Google DeepMind,Fitting Autoregressive Graph Generative Models through Maximum Likelihood Estimation,,https://jmlr.org/papers/volume24/22-0337/22-0337,,https://jmlr.org/papers/volume24/22-0337/22-0337,
Google DeepMind,Rethinking Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization,,https://arxiv.org/abs/2205.12191,,https://arxiv.org/abs/2205.12191,"Vision-and-language (V&L) models pretrained on large-scale multimodal data
have demonstrated strong performance on various tasks such as image captioning
and visual question answering (VQA). The quality of such models is commonly
assessed by measuring their performance on unseen data that typically comes
from the same distribution as the training data. However, when evaluated under
out-of-distribution (out-of-dataset) settings for VQA, we observe that these
models exhibit poor generalization. We comprehensively evaluate two pretrained
V&L models under different settings (i.e. classification and open-ended text
generation) by conducting cross-dataset evaluations. We find that these models
tend to learn to solve the benchmark, rather than learning the high-level
skills required by the VQA task. We also find that in most cases generative
models are less susceptible to shifts in data distribution compared to
discriminative ones, and that multimodal pretraining is generally helpful for
OOD generalization. Finally, we revisit assumptions underlying the use of
automatic VQA evaluation metrics, and empirically show that their stringent
nature repeatedly penalizes models for correct responses."
Google DeepMind,Three ways to improve feature alignment for open vocabulary detection,,https://arxiv.org/pdf/2303.13518,,https://arxiv.org/pdf/2303.13518,"The core problem in zero-shot open vocabulary detection is how to align
visual and text features, so that the detector performs well on unseen classes.
Previous approaches train the feature pyramid and detection head from scratch,
which breaks the vision-text feature alignment established during pretraining,
and struggles to prevent the language model from forgetting unseen classes.
  We propose three methods to alleviate these issues. Firstly, a simple scheme
is used to augment the text embeddings which prevents overfitting to a small
number of classes seen during training, while simultaneously saving memory and
computation. Secondly, the feature pyramid network and the detection head are
modified to include trainable gated shortcuts, which encourages vision-text
feature alignment and guarantees it at the start of detection training.
Finally, a self-training approach is used to leverage a larger corpus of
image-text pairs thus improving detection performance on classes with no human
annotated bounding boxes.
  Our three methods are evaluated on the zero-shot version of the LVIS
benchmark, each of them showing clear and significant benefits. Our final
network achieves the new stateof-the-art on the mAP-all metric and demonstrates
competitive performance for mAP-rare, as well as superior transfer to COCO and
Objects365."
Google DeepMind,Fast exploration and learning of latent graphs with aliased observations,,https://arxiv.org/pdf/2303.07397,,https://arxiv.org/pdf/2303.07397,"We consider the problem of recovering a latent graph where the observations
at each node are \emph{aliased}, and transitions are stochastic. Observations
are gathered by an agent traversing the graph. Aliasing means that multiple
nodes emit the same observation, so the agent can not know in which node it is
located. The agent needs to uncover the hidden topology as accurately as
possible and in as few steps as possible. This is equivalent to efficient
recovery of the transition probabilities of a partially observable Markov
decision process (POMDP) in which the observation probabilities are known. An
algorithm for efficiently exploring (and ultimately recovering) the latent
graph is provided. Our approach is exponentially faster than naive exploration
in a variety of challenging topologies with aliased observations while
remaining competitive with existing baselines in the unaliased regime."
Google DeepMind,Evaluating Number Discrimination in Deep Neural Networks for Vision,,https://arxiv.org/abs/2303.07172,,https://arxiv.org/abs/2303.07172,"The ability to discriminate between large and small quantities is a core
aspect of basic numerical competence in both humans and animals. In this work,
we examine the extent to which the state-of-the-art neural networks designed
for vision exhibit this basic ability. Motivated by studies in animal and
infant numerical cognition, we use the numerical bisection procedure to test
number discrimination in different families of neural architectures. Our
results suggest that vision-specific inductive biases are helpful in numerosity
discrimination, as models with such biases have lowest test errors on the task,
and often have psychometric curves that qualitatively resemble those of humans
and animals performing the task. However, even the strongest models, as
measured on standard metrics of performance, fail to discriminate quantities in
transfer experiments with differing training and testing conditions, indicating
that such inductive biases might not be sufficient."
Google DeepMind,Denoising diffusion samplers,,https://arxiv.org/pdf/2302.13834,,https://arxiv.org/pdf/2302.13834,"Denoising diffusion models are a popular class of generative models providing
state-of-the-art results in many domains. One adds gradually noise to data
using a diffusion to transform the data distribution into a Gaussian
distribution. Samples from the generative model are then obtained by simulating
an approximation of the time-reversal of this diffusion initialized by Gaussian
samples. Practically, the intractable score terms appearing in the
time-reversed process are approximated using score matching techniques. We
explore here a similar idea to sample approximately from unnormalized
probability density functions and estimate their normalizing constants. We
consider a process where the target density diffuses towards a Gaussian.
Denoising Diffusion Samplers (DDS) are obtained by approximating the
corresponding time-reversal. While score matching is not applicable in this
context, we can leverage many of the ideas introduced in generative modeling
for Monte Carlo sampling. Existing theoretical results from denoising diffusion
models also provide theoretical guarantees for DDS. We discuss the connections
between DDS, optimal control and Schr\""odinger bridges and finally demonstrate
DDS experimentally on a variety of challenging sampling tasks."
Google DeepMind,Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains,,https://www.deepmind.com/research#,,https://www.deepmind.com/research#,
Google DeepMind,"Graph schemas as abstractions for transfer learning, inference, and planning",,https://arxiv.org/pdf/2302.07350,,https://arxiv.org/pdf/2302.07350,"Transferring latent structure from one environment or problem to another is a
mechanism by which humans and animals generalize with very little data.
Inspired by cognitive and neurobiological insights, we propose graph schemas as
a mechanism of abstraction for transfer learning. Graph schemas start with
latent graph learning where perceptually aliased observations are disambiguated
in the latent space using contextual information. Latent graph learning is also
emerging as a new computational model of the hippocampus to explain map
learning and transitive inference. Our insight is that a latent graph can be
treated as a flexible template -- a schema -- that models concepts and
behaviors, with slots that bind groups of latent nodes to the specific
observations or groundings. By treating learned latent graphs (schemas) as
prior knowledge, new environments can be quickly learned as compositions of
schemas and their newly learned bindings. We evaluate graph schemas on two
previously published challenging tasks: the memory & planning game and one-shot
StreetLearn, which are designed to test rapid task solving in novel
environments. Graph schemas can be learned in far fewer episodes than previous
baselines, and can model and plan in a few steps in novel variations of these
tasks. We also demonstrate learning, matching, and reusing graph schemas in
more challenging 2D and 3D environments with extensive perceptual aliasing and
size variations, and show how different schemas can be composed to model larger
and more complex environments. To summarize, our main contribution is a unified
system, inspired and grounded in cognitive science, that facilitates rapid
transfer learning of new environments using schemas via map-induction and
composition that handles perceptual aliasing."
Google DeepMind,Universal Agent Mixtures and the Geometry of Intelligence,,https://arxiv.org/abs/2302.06083,,https://arxiv.org/abs/2302.06083,"Inspired by recent progress in multi-agent Reinforcement Learning (RL), in
this work we examine the collective intelligent behaviour of theoretical
universal agents by introducing a weighted mixture operation. Given a weighted
set of agents, their weighted mixture is a new agent whose expected total
reward in any environment is the corresponding weighted average of the original
agents' expected total rewards in that environment. Thus, if RL agent
intelligence is quantified in terms of performance across environments, the
weighted mixture's intelligence is the weighted average of the original agents'
intelligences. This operation enables various interesting new theorems that
shed light on the geometry of RL agent intelligence, namely: results about
symmetries, convex agent-sets, and local extrema. We also show that any RL
agent intelligence measure based on average performance across environments,
subject to certain weak technical conditions, is identical (up to a constant
factor) to performance within a single environment dependent on said
intelligence measure."
Google DeepMind,Scaling Goal-based Exploration via Pruning Proto-goals,,https://arxiv.org/pdf/2302.04693,,https://arxiv.org/pdf/2302.04693,"One of the gnarliest challenges in reinforcement learning (RL) is exploration
that scales to vast domains, where novelty-, or coverage-seeking behaviour
falls short. Goal-directed, purposeful behaviours are able to overcome this,
but rely on a good goal space. The core challenge in goal discovery is finding
the right balance between generality (not hand-crafted) and tractability
(useful, not too many). Our approach explicitly seeks the middle ground,
enabling the human designer to specify a vast but meaningful proto-goal space,
and an autonomous discovery process to refine this to a narrower space of
controllable, reachable, novel, and relevant goals. The effectiveness of
goal-conditioned exploration with the latter is then demonstrated in three
challenging environments."
Google DeepMind,Equivariant MuZero,,https://arxiv.org/pdf/2302.04798,,https://arxiv.org/pdf/2302.04798,"Deep reinforcement learning repeatedly succeeds in closed, well-defined
domains such as games (Chess, Go, StarCraft). The next frontier is real-world
scenarios, where setups are numerous and varied. For this, agents need to learn
the underlying rules governing the environment, so as to robustly generalise to
conditions that differ from those they were trained on. Model-based
reinforcement learning algorithms, such as the highly successful MuZero, aim to
accomplish this by learning a world model. However, leveraging a world model
has not consistently shown greater generalisation capabilities compared to
model-free alternatives. In this work, we propose improving the data efficiency
and generalisation capabilities of MuZero by explicitly incorporating the
symmetries of the environment in its world-model architecture. We prove that,
so long as the neural networks used by MuZero are equivariant to a particular
symmetry group acting on the environment, the entirety of MuZero's
action-selection algorithm will also be equivariant to that group. We evaluate
Equivariant MuZero on procedurally-generated MiniPacman and on Chaser from the
ProcGen suite: training on a set of mazes, and then testing on unseen rotated
versions, demonstrating the benefits of equivariance. Further, we verify that
our performance improvements hold even when only some of the components of
Equivariant MuZero obey strict equivariance, which highlights the robustness of
our construction."
Google DeepMind,3D Neural Embedding Likelihood for Robust Sim-to-Real Transfer in Inverse Graphics,,https://www.deepmind.com/research#,,https://www.deepmind.com/research#,
Google DeepMind,Exploration via Epistemic Value Estimation,,https://arxiv.org/pdf/2303.04012,,https://arxiv.org/pdf/2303.04012,"How to efficiently explore in reinforcement learning is an open problem. Many
exploration algorithms employ the epistemic uncertainty of their own value
predictions -- for instance to compute an exploration bonus or upper confidence
bound. Unfortunately the required uncertainty is difficult to estimate in
general with function approximation.
  We propose epistemic value estimation (EVE): a recipe that is compatible with
sequential decision making and with neural network function approximators. It
equips agents with a tractable posterior over all their parameters from which
epistemic value uncertainty can be computed efficiently.
  We use the recipe to derive an epistemic Q-Learning agent and observe
competitive performance on a series of benchmarks. Experiments confirm that the
EVE recipe facilitates efficient exploration in hard exploration tasks."
Google DeepMind,Diversity Through Exclusion (DTE): Niche Identification for Reinforcement Learning through Value-Decomposition,,https://arxiv.org/pdf/2302.01180,,https://arxiv.org/pdf/2302.01180,"Many environments contain numerous available niches of variable value, each
associated with a different local optimum in the space of behaviors (policy
space). In such situations it is often difficult to design a learning process
capable of evading distraction by poor local optima long enough to stumble upon
the best available niche. In this work we propose a generic reinforcement
learning (RL) algorithm that performs better than baseline deep Q-learning
algorithms in such environments with multiple variably-valued niches. The
algorithm we propose consists of two parts: an agent architecture and a
learning rule. The agent architecture contains multiple sub-policies. The
learning rule is inspired by fitness sharing in evolutionary computation and
applied in reinforcement learning using Value-Decomposition-Networks in a novel
manner for a single-agent's internal population. It can concretely be
understood as adding an extra loss term where one policy's experience is also
used to update all the other policies in a manner that decreases their value
estimates for the visited states. In particular, when one sub-policy visits a
particular state frequently this decreases the value predicted for other
sub-policies for going to that state. Further, we introduce an artificial
chemistry inspired platform where it is easy to create tasks with multiple
rewarding strategies utilizing different resources (i.e. multiple niches). We
show that agents trained this way can escape poor-but-attractive local optima
to instead converge to harder-to-discover higher value strategies in both the
artificial chemistry environments and in simpler illustrative environments."
Google DeepMind,Reinforcement Learning for Minimizing Age of Information over Wireless Links,,https://www.deepmind.com/research#,,https://www.deepmind.com/research#,
Google DeepMind,PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX,,https://arxiv.org/pdf/2202.04110,,https://arxiv.org/pdf/2202.04110,"PGMax is an open-source Python package for (a) easily specifying discrete
Probabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically
running efficient and scalable loopy belief propagation (LBP) in JAX. PGMax
supports general factor graphs with tractable factors, and leverages modern
accelerators like GPUs for inference. Compared with existing alternatives,
PGMax obtains higher-quality inference results with up to three
orders-of-magnitude inference time speedups. PGMax additionally interacts
seamlessly with the rapidly growing JAX ecosystem, opening up new research
possibilities. Our source code, examples and documentation are available at
https://github.com/deepmind/PGMax."
Google DeepMind,Learning Noisy OR Bayesian Networks with Max-Product Belief Propagation,,https://arxiv.org/pdf/2302.00099,,https://arxiv.org/pdf/2302.00099,"Noisy-OR Bayesian Networks (BNs) are a family of probabilistic graphical
models which express rich statistical dependencies in binary data. Variational
inference (VI) has been the main method proposed to learn noisy-OR BNs with
complex latent structures (Jaakkola & Jordan, 1999; Ji et al., 2020; Buhai et
al., 2020). However, the proposed VI approaches either (a) use a recognition
network with standard amortized inference that cannot induce
``explaining-away''; or (b) assume a simple mean-field (MF) posterior which is
vulnerable to bad local optima. Existing MF VI methods also update the MF
parameters sequentially which makes them inherently slow. In this paper, we
propose parallel max-product as an alternative algorithm for learning noisy-OR
BNs with complex latent structures and we derive a fast stochastic training
scheme that scales to large datasets. We evaluate both approaches on several
benchmarks where VI is the state-of-the-art and show that our method (a)
achieves better test performance than Ji et al. (2020) for learning noisy-OR
BNs with hierarchical latent structures on large sparse real datasets; (b)
recovers a higher number of ground truth parameters than Buhai et al. (2020)
from cluttered synthetic scenes; and (c) solves the 2D blind deconvolution
problem from Lazaro-Gredilla et al. (2021) and variant - including binary
matrix factorization - while VI catastrophically fails and is up to two orders
of magnitude slower."
Google DeepMind,Dual Algorithmic Reasoning,,https://openreview.net/forum?id=hhvkdRdWt1F,,https://openreview.net/forum?id=hhvkdRdWt1F,"Neural Algorithmic Reasoning is an emerging area of machine learning which seeks to infuse algorithmic computation in neural networks, typically by training neural models to approximate steps of classical algorithms. In this context, much of the current work has focused on learning reachability and shortest path graph algorithms, showing that joint learning on similar algorithms is beneficial for generalisation. However, when targeting more complex problems, such ""similar"" algorithms become more difficult to find. Here, we propose to learn algorithms by exploiting duality of the underlying algorithmic problem. Many algorithms solve optimisation problems. We demonstrate that simultaneously learning the dual definition of these optimisation problems in algorithmic learning allows for better learning and qualitatively better solutions. Specifically, we exploit the max-flow min-cut theorem to simultaneously learn these two algorithms over synthetically generated graphs, demonstrating the effectiveness of the proposed approach. We then validate the real-world utility of our dual algorithmic reasoner by deploying it on a challenging brain vessel classification task, which likely depends on the vessels’ flow properties. We demonstrate a clear performance gain when using our model within such a context, and empirically show that learning the max-flow and min-cut algorithms together is critical for achieving such a result."
Google DeepMind,Distilling Internet-Scale Vision-Language Models into Embodied Agents,,https://arxiv.org/abs/2301.12507,,https://arxiv.org/abs/2301.12507,"Instruction-following agents must ground language into their observation and
action spaces. Learning to ground language is challenging, typically requiring
domain-specific engineering or large quantities of human interaction data. To
address this challenge, we propose using pretrained vision-language models
(VLMs) to supervise embodied agents. We combine ideas from model distillation
and hindsight experience replay (HER), using a VLM to retroactively generate
language describing the agent's behavior. Simple prompting allows us to control
the supervision signal, teaching an agent to interact with novel objects based
on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered
environment. Fewshot prompting lets us teach abstract category membership,
including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary
preferences over objects). Our work outlines a new and effective way to use
internet-scale VLMs, repurposing the generic language grounding acquired by
such models to teach task-relevant groundings to embodied agents."
Google DeepMind,Pragmatic Fairness: Developing Policies with Outcome Disparity Control,,https://arxiv.org/pdf/2301.12278,,https://arxiv.org/pdf/2301.12278,"We introduce a causal framework for designing optimal policies that satisfy
fairness constraints. We take a pragmatic approach asking what we can do with
an action space available to us and only with access to historical data. We
propose two different fairness constraints: a moderation breaking constraint
which aims at blocking moderation paths from the action and sensitive attribute
to the outcome, and by that at reducing disparity in outcome levels as much as
the provided action space permits; and an equal benefit constraint which aims
at distributing gain from the new and maximized policy equally across sensitive
attribute levels, and thus at keeping pre-existing preferential treatment in
place or avoiding the introduction of new disparity. We introduce practical
methods for implementing the constraints and illustrate their uses on
experiments with semi-synthetic models."
Google DeepMind,On a continuous time model of gradient descent dynamics and instability in deep learning,,https://openreview.net/forum?id=EYrRzKPinA,,https://openreview.net/forum?id=EYrRzKPinA,"{'value': 'The recipe behind the success of deep learning has been the combination of neural networks and gradient-based optimization. Understanding the behavior of gradient descent however, and particularly its instability, has lagged behind its empirical success. To add to the theoretical tools available to study gradient descent we propose the principal flow (PF), a continuous time flow that approximates gradient descent dynamics. To our knowledge, the PF is the only continuous flow that captures the divergent and oscillatory behaviors of gradient descent, including escaping local minima and saddle points. Through its dependence on the eigendecomposition of the Hessian the PF sheds light on the recently observed edge of stability phenomena in deep learning. Using our new understanding of instability we propose a learning rate adaptation method which enables us to control the trade-off between training stability and test set evaluation performance.'}"
Google DeepMind,Discovering Quantum Phase Transitions with Fermionic Neural Networks,,https://www.deepmind.com/research?d907cb24_page=2#,,https://www.deepmind.com/research?d907cb24_page=2#,
Google DeepMind,Transformer Grammars: Augmenting Transformer Language Models with a Syntactic Inductive Bias,,https://www.deepmind.com/research?d907cb24_page=2#,,https://www.deepmind.com/research?d907cb24_page=2#,
Google DeepMind,An empirical study of implicit regularization in deep offline RL,,https://openreview.net/forum?id=HFfJWx60IT,,https://openreview.net/forum?id=HFfJWx60IT,"{'value': ""Deep neural networks are the most commonly used function approximators in offline reinforcement learning. Prior works have shown that neural nets trained with TD-learning and gradient descent can exhibit implicit regularization that can be characterized by under-parameterization of these networks. Specifically, the rank of the penultimate feature layer, also called effective rank, has been observed to drastically collapse during the training. In turn, this collapse has been argued to reduce the model's ability to further adapt in later stages of learning, leading to the diminished final performance. Such an association between the effective rank and performance makes effective rank compelling for offline RL, primarily for offline policy evaluation. In this work, we conduct a careful empirical study on the relation between effective rank and performance on three offline RL datasets : bsuite, Atari, and DeepMind lab. We observe that a direct association exists only in restricted settings and disappears in the more extensive hyperparameter sweeps. Also, we empirically identify three phases of learning that explain the impact of implicit regularization on the learning dynamics and found that bootstrapping alone is insufficient to explain the collapse of the effective rank. Further, we show that several other factors could confound the relationship between effective rank and performance and conclude that studying this association under simplistic assumptions could be highly misleading.\n""}"
Google DeepMind,Confident Approximate Policy Iteration for Efficient Local Planning in q^π-realizable MDPs,,https://openreview.net/attachment?id=Q_WPshXgGI9&name=supplementary_material,,https://openreview.net/attachment?id=Q_WPshXgGI9&name=supplementary_material,Abstract not found
Google DeepMind,Particle-Based Score Estimation for Jointly Learning Transition and Observation Models in Autonomous Driving,,https://openreview.net/forum?id=7JVNhaMbZUu,,https://openreview.net/forum?id=7JVNhaMbZUu,"Multi-object state estimation is a fundamental problem for robotic applications where a robot must interact with other moving objects. Typically, other objects' relevant state features are not directly observable, and must instead be inferred from observations. Particle filtering can perform such inference given approximate transition and observation models. However, these models are often unknown a priori, yielding a difficult parameter estimation problem since observations jointly carry transition and observation noise. In this work, we consider learning maximum-likelihood parameters using particle methods. Recent methods addressing this problem typically differentiate through time in a particle filter, which requires workarounds to the non-differentiable resampling step, that yield biased or high variance gradient estimates. By contrast, we exploit Fisher's identity to obtain a particle-based approximation of the score function (the gradient of the log likelihood) that yields a low variance estimate while only requiring stepwise differentiation through the transition and observation models. We apply our method to real data collected from autonomous vehicles (AVs) and show that it learns better models than existing techniques and is more stable in training, yielding an effective smoother for tracking the trajectories of vehicles around an AV."
Google DeepMind,ipie: A Python-based Auxiliary-Field Quantum Monte Carlo Program with Flexibility and Efficiency on CPUs and GPUs,,https://www.deepmind.com/research?d907cb24_page=2#,,https://www.deepmind.com/research?d907cb24_page=2#,
Google DeepMind,Score-based generative models learn manifold-like structures with constrained mixing,,https://openreview.net/forum?id=eSZqaIrDLZR,,https://openreview.net/forum?id=eSZqaIrDLZR,"How do score-based generative models (SBMs) learn the data distribution supported on a lower-dimensional manifold? We investigate the score model of a trained SBM through its linear approximations and subspaces spanned by local feature vectors. During diffusion as the noise decreases, the local dimensionality increases and become more varied between different sample sequences. Importantly, we find that the learned vector field mixes images by a non-conservative field within the manifold, although it denoises with normal projections as if there is a potential function in off-manifold directions. At each noise level, the subspace spanned by the local features overlap with an effective density function. These observations suggest that SBMs can flexibly mix samples with the learned score field while carefully maintaining a manifold-like structure of the data distribution."
Google DeepMind,Continuous Neural Algorithmic Planners,,https://openreview.net/forum?id=60avttW0Mv,,https://openreview.net/forum?id=60avttW0Mv,"Neural algorithmic reasoning studies the problem of learning algorithms with neural networks, especially using graph architectures. A recent proposal, XLVIN, reaps the benefits of using a graph neural network that simulates the value iteration algorithm in deep reinforcement learning agents. It allows model-free planning without access to privileged information about the environment, which is usually unavailable. However, XLVIN only supports discrete action spaces, and is hence nontrivially applicable to most tasks of real-world interest. We expand XLVIN to continuous action spaces by discretization, and evaluate several selective expansion policies to deal with the large planning graphs. Our proposal, CNAP, demonstrates how neural algorithmic reasoning can make a measurable impact in higher-dimensional continuous control settings, such as MuJoCo, bringing gains in low-data settings and outperforming model-free baselines."
Google DeepMind,Unbiased and Efficient Sampling of Dependency Trees,,https://arxiv.org/pdf/2205.12621,,https://arxiv.org/pdf/2205.12621,"Most computational models of dependency syntax consist of distributions over
spanning trees. However, the majority of dependency treebanks require that
every valid dependency tree has a single edge coming out of the ROOT node, a
constraint that is not part of the definition of spanning trees. For this
reason all standard inference algorithms for spanning trees are suboptimal for
inference over dependency trees.
  Zmigrod et al. (2021b) proposed algorithms for sampling with and without
replacement from the dependency tree distribution that incorporate the
single-root constraint. In this paper we show that their fastest algorithm for
sampling with replacement, Wilson-RC, is in fact producing biased samples and
we provide two alternatives that are unbiased. Additionally, we propose two
algorithms (one incremental, one parallel) that reduce the asymptotic runtime
of algorithm for sampling k trees without replacement to O(kn3). These
algorithms are both asymptotically and practically more efficient."
Google DeepMind,Reasoning-Modulated Representations,,https://openreview.net/forum?id=QBGYYu3l3dG,,https://openreview.net/forum?id=QBGYYu3l3dG,"Neural networks leverage robust internal representations in order to generalise. Learning them is difficult, and often requires a large training set that covers the data distribution densely. We study a common setting where our task is not purely opaque. Indeed, very often we may have access to information about the underlying system (e.g. that observations must obey certain laws of physics) that any ""tabula rasa"" neural network would need to re-learn from scratch, penalising performance. We incorporate this information into a pre-trained reasoning module, and investigate its role in shaping the discovered representations in diverse self-supervised learning settings from pixels. Our approach paves the way for a new class of representation learning, grounded in algorithmic priors.
"
Google DeepMind,BLaDE: Robust Exploration via Diffusion Models,,https://openreview.net/forum?id=cLijWz05L2b,,https://openreview.net/forum?id=cLijWz05L2b,"We  present  Bootstrap  your  own  Latents  with  Diffusion  models  for  Exploration (BLaDE), a general approach for curiosity-driven exploration in complex, partially-observable and stochastic environments. BLaDE is a natural extension of Bootstrap Your Own Latents for Exploration (BYOL-Explore) which is a multi-step prediction-error method at the latent level that learns a world representation, the world dynamics,  and provides an intrinsic-reward all-together by optimizing a single prediction loss with no additional auxiliary objective.  Contrary to BYOL-Explore that predicts future latents from past latents and future open-loop actions, BLaDE predicts, via a diffusion model, future latents from past observations, future open-loop actions and a noisy version of future latents. Leaking information about future latents allows to control the variance of the distribution of future latents which makes the method agnostic to stochastic traps.  Our experiments on different noisy versions of Montezuma’s Revenge show that BLaDE handles stochasticity better than Random Network Distillation, Intrinsic Curiosity Module and BYOL-Explore without degrading the performance of BYOL-Explore in the non-noisy and fairly deterministic Montezuma’s Revenge."
Google DeepMind,Learning Graph Search Heuristics,,https://openreview.net/forum?id=-xjStp_F9o,,https://openreview.net/forum?id=-xjStp_F9o,"Searching for a path between two nodes in a graph is one of the most well-studied and fundamental problems in computer science. In numerous domains such as robotics, AI, or biology, practitioners develop search heuristics to accelerate their pathfinding algorithms. However, it is a laborious and complex process to hand-design heuristics based on the problem and the structure of a given use case. Here we present PHIL (Path Heuristic with Imitation Learning), a novel neural architecture and a training algorithm for discovering graph search and navigation heuristics from data by leveraging recent advances in imitation learning and graph representation learning. At training time, we aggregate datasets of search trajectories and ground-truth shortest path distances, which we use to train a specialized graph neural network-based heuristic function using backpropagation through steps of the pathfinding process. Our heuristic function learns graph embeddings useful for inferring node distances, runs in constant time independent of graph sizes, and can be easily incorporated in an algorithm such as A* at test time. Experiments show that PHIL reduces the number of explored nodes compared to state-of-the-art methods on benchmark datasets by 58.5% on average, can be directly applied in diverse graphs ranging from biological networks to road networks, and allows for fast planning in time-critical robotics domains."
Google DeepMind,Learnable Commutative Monoids for Graph Neural Networks,,https://openreview.net/forum?id=WtFobB28VDey,,https://openreview.net/forum?id=WtFobB28VDey,"Graph neural networks (GNNs) have been shown to be highly sensitive to the choice of aggregation function. While summing over a node's neighbours can approximate any permutation-invariant function over discrete inputs, Cohen-Karlik et al. [2020] proved there are set-aggregation problems for which summing cannot generalise to unbounded inputs, proposing recurrent neural networks regularised towards permutation-invariance as a more expressive aggregator. We show that these results carry over to the graph domain: GNNs equipped with recurrent aggregators are competitive with state-of-the-art permutation-invariant aggregators, on both synthetic benchmarks and real-world problems. However, despite the benefits of recurrent aggregators, their O(V) depth makes them both difficult to parallelise and harder to train on large graphs. Inspired by the observation that a well-behaved aggregator for a GNN is a commutative monoid over its latent space, we propose a framework for constructing learnable, commutative, associative binary operators. And with this, we construct an aggregator of O(log V) depth, yielding exponential improvements for both parallelism and dependency length while achieving performance competitive with recurrent aggregators. Based on our empirical observations, our proposed learnable commutative monoid (LCM) aggregator represents a favourable tradeoff between efficient and expressive aggregators."
Google DeepMind,A Generalist Neural Algorithmic Learner,,https://openreview.net/forum?id=FebadKZf6Gd,,https://openreview.net/forum?id=FebadKZf6Gd,"The cornerstone of neural algorithmic reasoning is the ability to solve algorithmic tasks, especially in a way that generalises out of distribution. While recent years have seen a surge in methodological improvements in this area, they mostly focused on building specialist models. Specialist models are capable of learning to neurally execute either only one algorithm or a collection of algorithms with identical control-flow backbone. Here, instead, we focus on constructing a generalist neural algorithmic learner---a single graph neural network processor capable of learning to execute a wide range of algorithms, such as sorting, searching, dynamic programming, path-finding and geometry. We leverage the CLRS benchmark to empirically show that, much like recent successes in the domain of perception, generalist algorithmic learners can be built by ""incorporating"" knowledge. That is, it is possible to effectively learn algorithms in a multi-task manner, so long as we can learn to execute them well in a single-task regime. Motivated by this, we present a series of improvements to the input representation, training regime and processor architecture over CLRS, improving average single-task performance by over 20% from prior art. We then conduct a thorough ablation of multi-task learners leveraging these improvements. Our results demonstrate a generalist learner that effectively incorporates knowledge captured by specialist models."
Google DeepMind,Expander Graph Propagation,,https://openreview.net/forum?id=IKevTLt3rT,,https://openreview.net/forum?id=IKevTLt3rT,"Deploying graph neural networks (GNNs) on whole-graph classification or regression tasks is known to be challenging: it often requires computing node features that are mindful of both local interactions in their neighbourhood and the global context of the graph structure. GNN architectures that navigate this space need to avoid pathological behaviours, such as bottlenecks and oversquashing, while ideally having linear time and space complexity requirements. In this work, we propose an elegant approach based on propagating information over expander graphs. We leverage an efficient method for constructing expander graphs of a given size, and use this insight to propose the EGP model. We show that EGP is able to address all of the above concerns, while requiring minimal effort to set up, and provide evidence of its empirical utility on relevant graph classification datasets and baselines in the Open Graph Benchmark. Importantly, using expander graphs as a template for message passing necessarily gives rise to negative curvature. While this appears to be counterintuitive in light of recent related work on oversquashing, we theoretically demonstrate that negatively curved edges are likely to be required to obtain scalable message passing without bottlenecks. To the best of our knowledge, this is a previously unstudied result in the context of graph representation learning, and we believe our analysis paves the way to a novel class of scalable methods to counter oversquashing in GNNs."
Google DeepMind,Scaffolding cooperation in human groups with deep reinforcement learning,,https://psyarxiv.com/cr58a/download,,https://psyarxiv.com/cr58a/download,
Google DeepMind,Re-assessing Commonsense Knowledge in Large Language Models,,https://aclanthology.org/2022.emnlp-main.812,,https://aclanthology.org/2022.emnlp-main.812,
Google DeepMind,Learning rigid dynamics with face interaction graph networks,,https://arxiv.org/abs/2212.03574,,https://arxiv.org/abs/2212.03574,"Simulating rigid collisions among arbitrary shapes is notoriously difficult
due to complex geometry and the strong non-linearity of the interactions. While
graph neural network (GNN)-based models are effective at learning to simulate
complex physical dynamics, such as fluids, cloth and articulated bodies, they
have been less effective and efficient on rigid-body physics, except with very
simple shapes. Existing methods that model collisions through the meshes' nodes
are often inaccurate because they struggle when collisions occur on faces far
from nodes. Alternative approaches that represent the geometry densely with
many particles are prohibitively expensive for complex shapes. Here we
introduce the Face Interaction Graph Network (FIGNet) which extends beyond
GNN-based methods, and computes interactions between mesh faces, rather than
nodes. Compared to learned node- and particle-based methods, FIGNet is around
4x more accurate in simulating complex shape interactions, while also 8x more
computationally efficient on sparse, rigid meshes. Moreover, FIGNet can learn
frictional dynamics directly from real-world data, and can be more accurate
than analytical solvers given modest amounts of training data. FIGNet
represents a key step forward in one of the few remaining physical domains
which have seen little competition from learned simulators, and offers allied
fields such as robotics, graphics and mechanical design a new tool for
simulation and model-based planning."
Google DeepMind,Learning rigid dynamics with face interaction graph networks,,https://arxiv.org/abs/2212.03574,,https://arxiv.org/abs/2212.03574,"Simulating rigid collisions among arbitrary shapes is notoriously difficult
due to complex geometry and the strong non-linearity of the interactions. While
graph neural network (GNN)-based models are effective at learning to simulate
complex physical dynamics, such as fluids, cloth and articulated bodies, they
have been less effective and efficient on rigid-body physics, except with very
simple shapes. Existing methods that model collisions through the meshes' nodes
are often inaccurate because they struggle when collisions occur on faces far
from nodes. Alternative approaches that represent the geometry densely with
many particles are prohibitively expensive for complex shapes. Here we
introduce the Face Interaction Graph Network (FIGNet) which extends beyond
GNN-based methods, and computes interactions between mesh faces, rather than
nodes. Compared to learned node- and particle-based methods, FIGNet is around
4x more accurate in simulating complex shape interactions, while also 8x more
computationally efficient on sparse, rigid meshes. Moreover, FIGNet can learn
frictional dynamics directly from real-world data, and can be more accurate
than analytical solvers given modest amounts of training data. FIGNet
represents a key step forward in one of the few remaining physical domains
which have seen little competition from learned simulators, and offers allied
fields such as robotics, graphics and mechanical design a new tool for
simulation and model-based planning."
Google DeepMind,Negotiation and honesty in artificial intelligence methods for the board game of Diplomacy,,https://www.nature.com/articles/s41467-022-34473-5,,https://www.nature.com/articles/s41467-022-34473-5,
Google DeepMind,Large-Scale Retrieval for Reinforcement Learning,,https://arxiv.org/pdf/2206.05314,,https://arxiv.org/pdf/2206.05314,"Effective decision making involves flexibly relating past experiences and
relevant contextual information to a novel situation. In deep reinforcement
learning (RL), the dominant paradigm is for an agent to amortise information
that helps decision making into its network weights via gradient descent on
training losses. Here, we pursue an alternative approach in which agents can
utilise large-scale context sensitive database lookups to support their
parametric computations. This allows agents to directly learn in an end-to-end
manner to utilise relevant information to inform their outputs. In addition,
new information can be attended to by the agent, without retraining, by simply
augmenting the retrieval dataset. We study this approach for offline RL in 9x9
Go, a challenging game for which the vast combinatorial state space privileges
generalisation over direct matching to past experiences. We leverage fast,
approximate nearest neighbor techniques in order to retrieve relevant data from
a set of tens of millions of expert demonstration states. Attending to this
information provides a significant boost to prediction accuracy and game-play
performance over simply using these demonstrations as training trajectories,
providing a compelling demonstration of the value of large-scale retrieval in
offline RL agents."
Google DeepMind,A Fourier Approach to Mixture Learning,,https://arxiv.org/abs/2210.02415,,https://arxiv.org/abs/2210.02415,"We revisit the problem of learning mixtures of spherical Gaussians. Given
samples from mixture $\frac{1}{k}\sum_{j=1}^{k}\mathcal{N}(\mu_j, I_d)$, the
goal is to estimate the means $\mu_1, \mu_2, \ldots, \mu_k \in \mathbb{R}^d$ up
to a small error. The hardness of this learning problem can be measured by the
separation $\Delta$ defined as the minimum distance between all pairs of means.
Regev and Vijayaraghavan (2017) showed that with $\Delta = \Omega(\sqrt{\log
k})$ separation, the means can be learned using $\mathrm{poly}(k, d)$ samples,
whereas super-polynomially many samples are required if $\Delta = o(\sqrt{\log
k})$ and $d = \Omega(\log k)$. This leaves open the low-dimensional regime
where $d = o(\log k)$.
  In this work, we give an algorithm that efficiently learns the means in $d =
O(\log k/\log\log k)$ dimensions under separation $d/\sqrt{\log k}$ (modulo
doubly logarithmic factors). This separation is strictly smaller than
$\sqrt{\log k}$, and is also shown to be necessary. Along with the results of
Regev and Vijayaraghavan (2017), our work almost pins down the critical
separation threshold at which efficient parameter learning becomes possible for
spherical Gaussian mixtures. More generally, our algorithm runs in time
$\mathrm{poly}(k)\cdot f(d, \Delta, \epsilon)$, and is thus fixed-parameter
tractable in parameters $d$, $\Delta$ and $\epsilon$.
  Our approach is based on estimating the Fourier transform of the mixture at
carefully chosen frequencies, and both the algorithm and its analysis are
simple and elementary. Our positive results can be easily extended to learning
mixtures of non-Gaussian distributions, under a mild condition on the Fourier
spectrum of the distribution."
Google DeepMind,Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models,,https://openreview.net/forum?id=u46CbCaLufp,,https://openreview.net/forum?id=u46CbCaLufp,"Large language models produce human-like text that drive a growing number of applications.  However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful. 
 Though work to evaluate language model harms is under way, translating foresight about which harms may arise into rigorous benchmarks is not straightforward.  To facilitate this translation, we outline six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks.  We then use these characteristics as a lens to identify trends and gaps in existing benchmarks. Finally, we apply them in a case study of the Perspective API, a toxicity classifier that is widely used in harm benchmarks.  Our characteristics provide one piece of the bridge that translates between foresight and effective evaluation."
Google DeepMind,Learning to Navigate Wikipedia by Taking Random Walks,,https://arxiv.org/pdf/2211.00177,,https://arxiv.org/pdf/2211.00177,"A fundamental ability of an intelligent web-based agent is seeking out and
acquiring new information. Internet search engines reliably find the correct
vicinity but the top results may be a few links away from the desired target. A
complementary approach is navigation via hyperlinks, employing a policy that
comprehends local content and selects a link that moves it closer to the
target. In this paper, we show that behavioral cloning of randomly sampled
trajectories is sufficient to learn an effective link selection policy. We
demonstrate the approach on a graph version of Wikipedia with 38M nodes and
387M edges. The model is able to efficiently navigate between nodes 5 and 20
steps apart 96% and 92% of the time, respectively. We then use the resulting
embeddings and policy in downstream fact verification and question answering
tasks where, in combination with basic TF-IDF search and ranking methods, they
are competitive results to the state-of-the-art methods."
Google DeepMind,"Turbocharging Solution Concepts: Solving NEs, CEs and CCEs with Deep Equilibrium Networks",,https://arxiv.org/pdf/2210.09257,,https://arxiv.org/pdf/2210.09257,"Solution concepts such as Nash Equilibria, Correlated Equilibria, and Coarse
Correlated Equilibria are useful components for many multiagent machine
learning algorithms. Unfortunately, solving a normal-form game could take
prohibitive or non-deterministic time to converge, and could fail. We introduce
the Neural Equilibrium Solver which utilizes a special equivariant neural
network architecture to approximately solve the space of all games of fixed
shape, buying speed and determinism. We define a flexible equilibrium selection
framework, that is capable of uniquely selecting an equilibrium that minimizes
relative entropy, or maximizes welfare. The network is trained without needing
to generate any supervised training data. We show remarkable zero-shot
generalization to larger games. We argue that such a network is a powerful
component for many possible multiagent algorithms."
Google DeepMind,Semantic Exploration from Language Abstractions and Pretrained Representations,,https://www.deepmind.com/research?d907cb24_page=3#,,https://www.deepmind.com/research?d907cb24_page=3#,
Google DeepMind,Continuous diffusion for categorical data,,https://arxiv.org/pdf/2211.15089,,https://arxiv.org/pdf/2211.15089,"Diffusion models have quickly become the go-to paradigm for generative
modelling of perceptual signals (such as images and sound) through iterative
refinement. Their success hinges on the fact that the underlying physical
phenomena are continuous. For inherently discrete and categorical data such as
language, various diffusion-inspired alternatives have been proposed. However,
the continuous nature of diffusion models conveys many benefits, and in this
work we endeavour to preserve it. We propose CDCD, a framework for modelling
categorical data with diffusion models that are continuous both in time and
input space. We demonstrate its efficacy on several language modelling tasks."
Google DeepMind,Score-Based Diffusion meets Annealed Importance Sampling,,https://arxiv.org/abs/2208.07698,,https://arxiv.org/abs/2208.07698,"More than twenty years after its introduction, Annealed Importance Sampling
(AIS) remains one of the most effective methods for marginal likelihood
estimation. It relies on a sequence of distributions interpolating between a
tractable initial distribution and the target distribution of interest which we
simulate from approximately using a non-homogeneous Markov chain. To obtain an
importance sampling estimate of the marginal likelihood, AIS introduces an
extended target distribution to reweight the Markov chain proposal. While much
effort has been devoted to improving the proposal distribution used by AIS, an
underappreciated issue is that AIS uses a convenient but suboptimal extended
target distribution. We here leverage recent progress in score-based generative
modeling (SGM) to approximate the optimal extended target distribution
minimizing the variance of the marginal likelihood estimate for AIS proposals
corresponding to the discretization of Langevin and Hamiltonian dynamics. We
demonstrate these novel, differentiable, AIS procedures on a number of
synthetic benchmark distributions and variational auto-encoders."
Google DeepMind,Fine-tuning language models to find agreement among humans with diverse preferences,,https://arxiv.org/pdf/2211.15006,,https://arxiv.org/pdf/2211.15006,"Recent work in large language modeling (LLMs) has used fine-tuning to align
outputs with the preferences of a prototypical user. This work assumes that
human preferences are static and homogeneous across individuals, so that
aligning to a a single ""generic"" user will confer more general alignment. Here,
we embrace the heterogeneity of human preferences to consider a different
challenge: how might a machine help people with diverse views find agreement?
We fine-tune a 70 billion parameter LLM to generate statements that maximize
the expected approval for a group of people with potentially diverse opinions.
Human participants provide written opinions on thousands of questions touching
on moral and political issues (e.g., ""should we raise taxes on the rich?""), and
rate the LLM's generated candidate consensus statements for agreement and
quality. A reward model is then trained to predict individual preferences,
enabling it to quantify and rank consensus statements in terms of their appeal
to the overall group, defined according to different aggregation (social
welfare) functions. The model produces consensus statements that are preferred
by human users over those from prompted LLMs (>70%) and significantly
outperforms a tight fine-tuned baseline that lacks the final ranking step.
Further, our best model's consensus statements are preferred over the best
human-generated opinions (>65%). We find that when we silently constructed
consensus statements from only a subset of group members, those who were
excluded were more likely to dissent, revealing the sensitivity of the
consensus to individual contributions. These results highlight the potential to
use LLMs to help groups of humans align their values with one another."
Google DeepMind,Neural Payoff Machines: Predicting Fair and Stable Payoff Allocations Among Team Members,,https://arxiv.org/pdf/2208.08798,,https://arxiv.org/pdf/2208.08798,"In many multi-agent settings, participants can form teams to achieve
collective outcomes that may far surpass their individual capabilities.
Measuring the relative contributions of agents and allocating them shares of
the reward that promote long-lasting cooperation are difficult tasks.
Cooperative game theory offers solution concepts identifying distribution
schemes, such as the Shapley value, that fairly reflect the contribution of
individuals to the performance of the team or the Core, which reduces the
incentive of agents to abandon their team. Applications of such methods include
identifying influential features and sharing the costs of joint ventures or
team formation. Unfortunately, using these solutions requires tackling a
computational barrier as they are hard to compute, even in restricted settings.
In this work, we show how cooperative game-theoretic solutions can be distilled
into a learned model by training neural networks to propose fair and stable
payoff allocations. We show that our approach creates models that can
generalize to games far from the training distribution and can predict
solutions for more players than observed during training. An important
application of our framework is Explainable AI: our approach can be used to
speed-up Shapley value computations on many instances."
Google DeepMind,Inverse Design for Fluid-Structure Interactions using Graph Network Simulators,,https://openreview.net/forum?id=HaZuqj0Gvp2,,https://openreview.net/forum?id=HaZuqj0Gvp2,"Designing physical artifacts that serve a purpose---such as tools and other functional structures---is central to engineering as well as everyday human behavior. Though automating design using machine learning has tremendous promise, existing methods are often limited by the task-dependent distributions they were exposed to during training. Here we showcase a task-agnostic approach to inverse design, by combining general-purpose graph network simulators with gradient-based design optimization. This constitutes a simple, fast, and reusable approach that solves high-dimensional problems with complex physical dynamics, including designing surfaces and tools to manipulate fluid flows and optimizing the shape of an airfoil to minimize drag. This framework produces high-quality designs by propagating gradients through trajectories of hundreds of steps, even when using models that were pre-trained for single-step predictions on data substantially different from the design tasks. In our fluid manipulation tasks, the resulting designs outperformed those found by sampling-based optimization techniques. In airfoil design, they matched the quality of those obtained with a specialized solver. Our results suggest that despite some remaining challenges, machine learning-based simulators are maturing to the point where they can support general-purpose design optimization across a variety of fluid-structure interaction domains."
Google DeepMind,Towards combinatorial invariance for Kazhdan-Lusztig polynomials,,https://www.ams.org/journals/ert/2022-26-37/S1088-4165-2022-00624-8/S1088-4165-2022-00624-8,,https://www.ams.org/journals/ert/2022-26-37/S1088-4165-2022-00624-8/S1088-4165-2022-00624-8,
Google DeepMind,Curiosity in hindsight,,https://arxiv.org/pdf/2211.10515,,https://arxiv.org/pdf/2211.10515,"Consider the problem of exploration in sparse-reward or reward-free
environments, such as in Montezuma's Revenge. In the curiosity-driven paradigm,
the agent is rewarded for how much each realized outcome differs from their
predicted outcome. But using predictive error as intrinsic motivation is
fragile in stochastic environments, as the agent may become trapped by
high-entropy areas of the state-action space, such as a ""noisy TV"". In this
work, we study a natural solution derived from structural causal models of the
world: Our key idea is to learn representations of the future that capture
precisely the unpredictable aspects of each outcome -- which we use as
additional input for predictions, such that intrinsic rewards only reflect the
predictable aspects of world dynamics. First, we propose incorporating such
hindsight representations into models to disentangle ""noise"" from ""novelty"",
yielding Curiosity in Hindsight: a simple and scalable generalization of
curiosity that is robust to stochasticity. Second, we instantiate this
framework for the recently introduced BYOL-Explore algorithm as our prime
example, resulting in the noise-robust BYOL-Hindsight. Third, we illustrate its
behavior under a variety of different stochasticities in a grid world, and find
improvements over BYOL-Explore in hard-exploration Atari games with sticky
actions. Notably, we show state-of-the-art results in exploring Montezuma's
Revenge with sticky actions, while preserving performance in the non-sticky
setting."
Google DeepMind,Space is a sequence: Structured sequence learning as a unified theory of spatial representation in the hippocampus,,"https://arxiv.org/abs/2212.01508#:~:text=3%20Dec%202022%5D-,Space%20is%20a%20latent%20sequence%3A%20Structured%20sequence%20learning%20as%20a,of%20representation%20in%20the%20hippocampus&text=Fascinating%20and%20puzzling%20phenomena%2C%20such,regularly%20discovered%20in%20the%20hippocampus.",,"https://arxiv.org/abs/2212.01508#:~:text=3%20Dec%202022%5D-,Space%20is%20a%20latent%20sequence%3A%20Structured%20sequence%20learning%20as%20a,of%20representation%20in%20the%20hippocampus&text=Fascinating%20and%20puzzling%20phenomena%2C%20such,regularly%20discovered%20in%20the%20hippocampus.","Fascinating and puzzling phenomena, such as landmark vector cells, splitter
cells, and event-specific representations to name a few, are regularly
discovered in the hippocampus. Without a unifying principle that can explain
these divergent observations, each experiment seemingly discovers a new anomaly
or coding type. Here, we provide a unifying principle that the mental
representation of space is an emergent property of latent higher-order sequence
learning. Treating space as a sequence resolves myriad phenomena, and suggests
that the place-field mapping methodology where sequential neuron responses are
interpreted in spatial and Euclidean terms might itself be a source of
anomalies. Our model, called Clone-structured Causal Graph (CSCG), uses a
specific higher-order graph scaffolding to learn latent representations by
mapping sensory inputs to unique contexts. Learning to compress sequential and
episodic experiences using CSCGs result in the emergence of cognitive maps -
mental representations of spatial and conceptual relationships in an
environment that are suited for planning, introspection, consolidation, and
abstraction. We demonstrate that over a dozen different hippocampal phenomena,
ranging from those reported in classic experiments to the most recent ones, are
succinctly and mechanistically explained by our model."
Google DeepMind,Communicative Capital: A key resource for human-machine shared agency and collaborative capacity,,https://link.springer.com/content/pdf/10.1007/s00521-022-07948-1,,https://link.springer.com/content/pdf/10.1007/s00521-022-07948-1,
Google DeepMind,Controlling Commercial Cooling Systems Using Reinforcement Learning,,https://arxiv.org/pdf/2211.07357,,https://arxiv.org/pdf/2211.07357,"This paper is a technical overview of DeepMind and Google's recent work on
reinforcement learning for controlling commercial cooling systems. Building on
expertise that began with cooling Google's data centers more efficiently, we
recently conducted live experiments on two real-world facilities in partnership
with Trane Technologies, a building management system provider. These live
experiments had a variety of challenges in areas such as evaluation, learning
from offline data, and constraint satisfaction. Our paper describes these
challenges in the hope that awareness of them will benefit future applied RL
work. We also describe the way we adapted our RL system to deal with these
challenges, resulting in energy savings of approximately 9% and 13%
respectively at the two live experiment sites."
Google DeepMind,Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task,,https://arxiv.org/pdf/2211.05039,,https://arxiv.org/pdf/2211.05039,"We introduce a challenging decision-making task that we call active
acquisition for multimodal temporal data (A2MT). In many real-world scenarios,
input features are not readily available at test time and must instead be
acquired at significant cost. With A2MT, we aim to learn agents that actively
select which modalities of an input to acquire, trading off acquisition cost
and predictive performance. A2MT extends a previous task called active feature
acquisition to temporal decision making about high-dimensional inputs. We
propose a method based on the Perceiver IO architecture to address A2MT in
practice. Our agents are able to solve a novel synthetic scenario requiring
practically relevant cross-modal reasoning skills. On two large-scale,
real-world datasets, Kinetics-700 and AudioSet, our agents successfully learn
cost-reactive acquisition behavior. However, an ablation reveals they are
unable to learn adaptive acquisition strategies, emphasizing the difficulty of
the task even for state-of-the-art models. Applications of A2MT may be
impactful in domains like medicine, robotics, or finance, where modalities
differ in acquisition cost and informativeness."
Google DeepMind,What is the simplest model that can account for high-fidelity imitation?,,https://doi.org/10.1017/S0140525X22001364,,https://doi.org/10.1017/S0140525X22001364,
Google DeepMind,A Generalist Agent,,https://openreview.net/forum?id=1ikK0kHjvj,,https://openreview.net/forum?id=1ikK0kHjvj,"{'value': 'Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.'}"
Google DeepMind,Over-communicate no more: Situated RL agents learn concise communication protocols,,https://www.deepmind.com/research?d907cb24_page=3#,,https://www.deepmind.com/research?d907cb24_page=3#,
Google DeepMind,Learning to Configure Computer Networks with Neural Algorithmic Reasoning,,https://openreview.net/forum?id=AiY6XvomZV4,,https://openreview.net/forum?id=AiY6XvomZV4,"We present a new method for scaling automatic configuration of computer networks. The key idea is to relax the computationally hard search problem of finding a configuration that satisfies a given specification into an approximate objective amenable to learning-based techniques. Based on this idea, we train a neural algorithmic model which learns to generate configurations likely to (fully or partially) satisfy a given specification under existing routing protocols. By relaxing the rigid satisfaction guarantees, our approach (i) enables greater flexibility: it is protocol-agnostic, enables cross-protocol reasoning, and does not depend on hardcoded rules; and (ii) finds configurations for much larger computer networks than previously possible. Our learned synthesizer is up to 490x faster than state-of-the-art SMT-based methods, while producing configurations which on average satisfy more than 93% of the provided requirements.  "
Google DeepMind,Diagnosing failures of fairness transfer across distribution shift in real-world medical settings,,https://openreview.net/forum?id=K-A4tDJ6HHf,,https://openreview.net/forum?id=K-A4tDJ6HHf,"Diagnosing and mitigating changes in model fairness under distribution shift is an important component of the safe deployment of machine learning in healthcare settings. Importantly, the success of any mitigation strategy strongly depends on the \textit{structure} of the shift. Despite this, there has been little discussion of how to empirically assess the structure of a distribution shift that one is encountering in practice. In this work, we adopt a causal framing to motivate conditional independence tests as a key tool for characterizing distribution shifts. Using our approach in two medical applications, we show that this knowledge can help diagnose failures of fairness transfer, including cases where real-world shifts are more complex than is often assumed in the literature. Based on these results, we discuss potential remedies at each step of the machine learning pipeline."
Google DeepMind,Can language models handle recursively nested grammatical structures? A case study on comparing models and humans,,https://arxiv.org/pdf/2210.15303,,https://arxiv.org/pdf/2210.15303,"How should we compare the capabilities of language models (LMs) and humans? I
draw inspiration from comparative psychology to highlight some challenges. In
particular, I consider a case study: processing of recursively nested
grammatical structures. Prior work suggests that LMs cannot handle these
structures as reliably as humans can. However, the humans were provided with
instructions and training, while the LMs were evaluated zero-shot. I therefore
match the evaluation more closely. Providing large LMs with a simple prompt --
substantially less content than the human training -- allows the LMs to
consistently outperform the human results, and even to extrapolate to more
deeply nested conditions than were tested with humans. Further, reanalyzing the
prior human data suggests that the humans may not perform above chance at the
difficult structures initially. Thus, large LMs may indeed process recursively
nested grammatical structures as reliably as humans. This case study highlights
how discrepancies in the evaluation can confound comparisons of language models
and humans. I therefore reflect on the broader challenge of comparing human and
model capabilities, and highlight an important difference between evaluating
cognitive models and foundation models."
Google DeepMind,Categorical SDEs with Simplex Diffusion,,https://arxiv.org/pdf/2210.14784,,https://arxiv.org/pdf/2210.14784,"Diffusion models typically operate in the standard framework of generative
modelling by producing continuously-valued datapoints. To this end, they rely
on a progressive Gaussian smoothing of the original data distribution, which
admits an SDE interpretation involving increments of a standard Brownian
motion. However, some applications such as text generation or reinforcement
learning might naturally be better served by diffusing categorical-valued data,
i.e., lifting the diffusion to a space of probability distributions. To this
end, this short theoretical note proposes Simplex Diffusion, a means to
directly diffuse datapoints located on an n-dimensional probability simplex. We
show how this relates to the Dirichlet distribution on the simplex and how the
analogous SDE is realized thanks to a multi-dimensional Cox-Ingersoll-Ross
process (abbreviated as CIR), previously used in economics and mathematical
finance. Finally, we make remarks as to the numerical implementation of
trajectories of the CIR process, and discuss some limitations of our approach."
Google DeepMind,Latent Space Smoothing for Individually Fair Representations,,https://arxiv.org/pdf/2111.13650,,https://arxiv.org/pdf/2111.13650,"Fair representation learning transforms user data into a representation that
ensures fairness and utility regardless of the downstream application. However,
learning individually fair representations, i.e., guaranteeing that similar
individuals are treated similarly, remains challenging in high-dimensional
settings such as computer vision. In this work, we introduce LASSI, the first
representation learning method for certifying individual fairness of
high-dimensional data. Our key insight is to leverage recent advances in
generative modeling to capture the set of similar individuals in the generative
latent space. This enables us to learn individually fair representations that
map similar individuals close together by using adversarial training to
minimize the distance between their representations. Finally, we employ
randomized smoothing to provably map similar individuals close together, in
turn ensuring that local robustness verification of the downstream application
results in end-to-end fairness certification. Our experimental evaluation on
challenging real-world image data demonstrates that our method increases
certified individual fairness by up to 90% without significantly affecting task
utility."
Google DeepMind,Testing Independence of Exchangeable Random Variables,,https://arxiv.org/pdf/2210.12392,,https://arxiv.org/pdf/2210.12392,"Given well-shuffled data, can we determine whether the data items are
statistically (in)dependent? Formally, we consider the problem of testing
whether a set of exchangeable random variables are independent. We will show
that this is possible and develop tests that can confidently reject the null
hypothesis that data is independent and identically distributed and have high
power for (some) exchangeable distributions. We will make no structural
assumptions on the underlying sample space. One potential application is in
Deep Learning, where data is often scraped from the whole internet, with
duplications abound, which can render data non-iid and test-set evaluation
prone to give wrong answers."
Google DeepMind,Collaborating with language models for embodied reasoning,,https://larel-workshop.github.io/papers/,,https://larel-workshop.github.io/papers/,
Google DeepMind,Why neural networks find simple solutions: the many regularizers of geometric complexity,,https://openreview.net/forum?id=-ZPeUAJlkEu,,https://openreview.net/forum?id=-ZPeUAJlkEu,"In many contexts, simpler models are preferable to more complex models and the control of this model complexity is the goal for many methods in machine learning such as regularization, hyperparameter tuning and architecture design. In deep learning, it has been difficult to understand the underlying mechanisms of complexity control, since many traditional measures are not naturally suitable for deep neural networks. Here we develop the notion of geometric complexity, which is a measure of the variability of the model function, computed using a discrete Dirichlet energy. Using a combination of theoretical arguments and empirical results, we show that many common training heuristics such as parameter norm regularization, spectral norm regularization, flatness regularization, implicit gradient regularization, noise regularization and the choice of parameter initialization all act to control geometric complexity, providing a unifying framework in which to characterize the behavior of deep learning models."
Google DeepMind,Self-supervised video pretraining yields strong image representations,,https://arxiv.org/pdf/2210.06433,,https://arxiv.org/pdf/2210.06433,"Humans learn powerful representations of objects and scenes by observing how
they evolve over time. Yet, outside of specific tasks that require explicit
temporal understanding, static image pretraining remains the dominant paradigm
for learning visual foundation models. We question this mismatch, and ask
whether video pretraining can yield visual representations that bear the
hallmarks of human perception: generalisation across tasks, robustness to
perturbations, and consistency with human judgements. To that end we propose a
novel procedure for curating videos, and develop a contrastive framework which
learns from the complex transformations therein. This simple paradigm for
distilling knowledge from videos, called VITO, yields general representations
that far outperform prior video pretraining methods on image understanding
tasks, and image pretraining methods on video understanding tasks. Moreover,
VITO representations are significantly more robust to natural and synthetic
deformations than image-, video-, and adversarially-trained ones. Finally,
VITO's predictions are strongly aligned with human judgements, surpassing
models that were specifically trained for that purpose. Together, these results
suggest that video pretraining could be a simple way of learning unified,
robust, and human-aligned representations of the visual world."
Google DeepMind,Beyond Bayes-optimality: meta-learning what you know you don’t know,,https://arxiv.org/pdf/2209.15618,,https://arxiv.org/pdf/2209.15618,"Meta-training agents with memory has been shown to culminate in Bayes-optimal
agents, which casts Bayes-optimality as the implicit solution to a numerical
optimization problem rather than an explicit modeling assumption. Bayes-optimal
agents are risk-neutral, since they solely attune to the expected return, and
ambiguity-neutral, since they act in new situations as if the uncertainty were
known. This is in contrast to risk-sensitive agents, which additionally exploit
the higher-order moments of the return, and ambiguity-sensitive agents, which
act differently when recognizing situations in which they lack knowledge.
Humans are also known to be averse to ambiguity and sensitive to risk in ways
that aren't Bayes-optimal, indicating that such sensitivity can confer
advantages, especially in safety-critical situations. How can we extend the
meta-learning protocol to generate risk- and ambiguity-sensitive agents? The
goal of this work is to fill this gap in the literature by showing that risk-
and ambiguity-sensitivity also emerge as the result of an optimization problem
using modified meta-training algorithms, which manipulate the
experience-generation process of the learner. We empirically test our proposed
meta-training algorithms on agents exposed to foundational classes of
decision-making experiments and demonstrate that they become sensitive to risk
and ambiguity."
Google DeepMind,Transformers generalize differently from in-context and in-weights information,,https://www.deepmind.com/research?d907cb24_page=4#,,https://www.deepmind.com/research?d907cb24_page=4#,
Google DeepMind,Palm up: Playing in the Latent Manifold for Unsupervised Pretraining,,https://openreview.net/forum?id=6y0lgLb9tny,,https://openreview.net/forum?id=6y0lgLb9tny,"Large and diverse datasets have been the cornerstones of many impressive advancements in artificial intelligence. Intelligent creatures, however, learn by interacting with the environment, which changes the input sensory signals and the state of the environment. In this work, we aim to bring the best of both worlds and propose an algorithm that exhibits an  exploratory behavior whilst it utilizes large diverse datasets. Our key idea is to leverage deep generative models that are pretrained on static datasets and introduce a dynamic model in the latent space. The transition dynamics simply mixes an action and a random sampled latent. It then applies an exponential moving average for temporal persistency, the resulting latent is decoded to image using pretrained generator. We then employ an unsupervised reinforcement learning algorithm to explore in this environment and perform unsupervised representation learning on the collected data. We further leverage the temporal information of this data to pair data points as a natural supervision for representation learning. Our experiments suggest that the learned representations can be successfully transferred to downstream tasks in both vision and reinforcement learning domains. "
Google DeepMind,Game Theoretic Rating in n-player general-sum games with Equilibria,,https://arxiv.org/pdf/2210.02205,,https://arxiv.org/pdf/2210.02205,"Rating strategies in a game is an important area of research in game theory
and artificial intelligence, and can be applied to any real-world competitive
or cooperative setting. Traditionally, only transitive dependencies between
strategies have been used to rate strategies (e.g. Elo), however recent work
has expanded ratings to utilize game theoretic solutions to better rate
strategies in non-transitive games. This work generalizes these ideas and
proposes novel algorithms suitable for N-player, general-sum rating of
strategies in normal-form games according to the payoff rating system. This
enables well-established solution concepts, such as equilibria, to be leveraged
to efficiently rate strategies in games with complex strategic interactions,
which arise in multiagent training and real-world interactions between many
agents. We empirically validate our methods on real world normal-form data
(Premier League) and multiagent reinforcement learning agent evaluation."
Google DeepMind,Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees,,https://arxiv.org/pdf/2209.14414,,https://arxiv.org/pdf/2209.14414,"We consider reinforcement learning in an environment modeled by an episodic,
finite, stage-dependent Markov decision process of horizon $H$ with $S$ states,
and $A$ actions. The performance of an agent is measured by the regret after
interacting with the environment for $T$ episodes. We propose an optimistic
posterior sampling algorithm for reinforcement learning (OPSRL), a simple
variant of posterior sampling that only needs a number of posterior samples
logarithmic in $H$, $S$, $A$, and $T$ per state-action pair. For OPSRL we
guarantee a high-probability regret bound of order at most
$\widetilde{\mathcal{O}}(\sqrt{H^3SAT})$ ignoring $\text{poly}\log(HSAT)$
terms. The key novel technical ingredient is a new sharp anti-concentration
inequality for linear forms which may be of independent interest. Specifically,
we extend the normal approximation-based lower bound for Beta distributions by
Alfers and Dinges [1984] to Dirichlet distributions. Our bound matches the
lower bound of order $\Omega(\sqrt{H^3SAT})$, thereby answering the open
problems raised by Agrawal and Jia [2017b] for the episodic setting."
Google DeepMind,Co-Writing Screenplays and Theatre Scripts with Language Models: An Evaluation by Industry Professionals,,https://arxiv.org/pdf/2209.14958,,https://arxiv.org/pdf/2209.14958,"Language models are increasingly attracting interest from writers. However,
such models lack long-range semantic coherence, limiting their usefulness for
longform creative writing. We address this limitation by applying language
models hierarchically, in a system we call Dramatron. By building structural
context via prompt chaining, Dramatron can generate coherent scripts and
screenplays complete with title, characters, story beats, location
descriptions, and dialogue. We illustrate Dramatron's usefulness as an
interactive co-creative system with a user study of 15 theatre and film
industry professionals. Participants co-wrote theatre scripts and screenplays
with Dramatron and engaged in open-ended interviews. We report critical
reflections both from our interviewees and from independent reviewers who
watched stagings of the works to illustrate how both Dramatron and hierarchical
text generation could be useful for human-machine co-creativity. Finally, we
discuss the suitability of Dramatron for co-creativity, ethical considerations
-- including plagiarism and bias -- and participatory models for the design and
deployment of such tools."
Google DeepMind,Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods,,https://arxiv.org/pdf/2209.15589,,https://arxiv.org/pdf/2209.15589,"Self-supervised methods have achieved remarkable success in transfer
learning, often achieving the same or better accuracy than supervised
pre-training. Most prior work has done so by increasing pre-training
computation by adding complex data augmentation, multiple views, or lengthy
training schedules. In this work, we investigate a related, but orthogonal
question: given a fixed FLOP budget, what are the best datasets, models, and
(self-)supervised training methods for obtaining high accuracy on
representative visual tasks? Given the availability of large datasets, this
setting is often more relevant for both academic and industry labs alike. We
examine five large-scale datasets (JFT-300M, ALIGN, ImageNet-1K, ImageNet-21K,
and COCO) and six pre-training methods (CLIP, DINO, SimCLR, BYOL, Masked
Autoencoding, and supervised). In a like-for-like fashion, we characterize
their FLOP and CO$_2$ footprints, relative to their accuracy when transferred
to a canonical image segmentation task. Our analysis reveals strong disparities
in the computational efficiency of pre-training methods and their dependence on
dataset quality. In particular, our results call into question the
commonly-held assumption that self-supervised methods inherently scale to
large, uncurated data. We therefore advocate for (1) paying closer attention to
dataset curation and (2) reporting of accuracies in context of the total
computational cost."
Google DeepMind,COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation,,https://arxiv.org/pdf/2204.08957,,https://arxiv.org/pdf/2204.08957,"We consider the offline constrained reinforcement learning (RL) problem, in
which the agent aims to compute a policy that maximizes expected return while
satisfying given cost constraints, learning only from a pre-collected dataset.
This problem setting is appealing in many real-world scenarios, where direct
interaction with the environment is costly or risky, and where the resulting
policy should comply with safety constraints. However, it is challenging to
compute a policy that guarantees satisfying the cost constraints in the offline
RL setting, since the off-policy evaluation inherently has an estimation error.
In this paper, we present an offline constrained RL algorithm that optimizes
the policy in the space of the stationary distribution. Our algorithm,
COptiDICE, directly estimates the stationary distribution corrections of the
optimal policy with respect to returns, while constraining the cost upper
bound, with the goal of yielding a cost-conservative policy for actual
constraint satisfaction. Experimental results show that COptiDICE attains
better policies in terms of constraint satisfaction and return-maximization,
outperforming baseline algorithms."
Google DeepMind,Learned Force Fields Are Ready For Ground State Catalyst Discovery,,https://arxiv.org/pdf/2209.12466,,https://arxiv.org/pdf/2209.12466,"We present evidence that learned density functional theory (``DFT'') force
fields are ready for ground state catalyst discovery. Our key finding is that
relaxation using forces from a learned potential yields structures with similar
or lower energy to those relaxed using the RPBE functional in over 50\% of
evaluated systems, despite the fact that the predicted forces differ
significantly from the ground truth. This has the surprising implication that
learned potentials may be ready for replacing DFT in challenging catalytic
systems such as those found in the Open Catalyst 2020 dataset. Furthermore, we
show that a force field trained on a locally harmonic energy surface with the
same minima as a target DFT energy is also able to find lower or similar energy
structures in over 50\% of cases. This ``Easy Potential'' converges in fewer
steps than a standard model trained on true energies and forces, which further
accelerates calculations. Its success illustrates a key point: learned
potentials can locate energy minima even when the model has high force errors.
The main requirement for structure optimisation is simply that the learned
potential has the correct minima. Since learned potentials are fast and scale
linearly with system size, our results open the possibility of quickly finding
ground states for large systems."
Google DeepMind,Structure of the PAPP-A-IGFBP5 complex reveals mechanism of substrate recognition,,https://www.nature.com/articles/s41467-022-33175-2,,https://www.nature.com/articles/s41467-022-33175-2,
Google DeepMind,Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality,,https://ewrl.files.wordpress.com/2022/09/discovering_policies_with_domino,,https://ewrl.files.wordpress.com/2022/09/discovering_policies_with_domino,
Google DeepMind,On Reward Binarisation and Bayesian Agents,,https://ewrl.files.wordpress.com/2022/09/ewrl22_submission,,https://ewrl.files.wordpress.com/2022/09/ewrl22_submission,
Google DeepMind,Optimizing Industrial Cooling Systems with Hierarchical Reinforcement Learning,,https://arxiv.org/pdf/2209.08112,,https://arxiv.org/pdf/2209.08112,"Reinforcement learning (RL) techniques have been developed to optimize
industrial cooling systems, offering substantial energy savings compared to
traditional heuristic policies. A major challenge in industrial control
involves learning behaviors that are feasible in the real world due to
machinery constraints. For example, certain actions can only be executed every
few hours while other actions can be taken more frequently. Without extensive
reward engineering and experimentation, an RL agent may not learn realistic
operation of machinery. To address this, we use hierarchical reinforcement
learning with multiple agents that control subsets of actions according to
their operation time scales. Our hierarchical approach achieves energy savings
over existing baselines while maintaining constraints such as operating
chillers within safe bounds in a simulated HVAC control environment."
Google DeepMind,Leveraging Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines,,https://openreview.net/forum?id=buXZ7nIqiwE,,https://openreview.net/forum?id=buXZ7nIqiwE,"Strong inductive biases give humans the ability to quickly learn to perform a variety of tasks. Although meta-learning is a method to endow neural networks with useful inductive biases, agents trained by meta-learning may sometimes acquire very different strategies from humans. We show that co-training these agents on predicting representations from natural language task descriptions and programs induced to generate such tasks guides them toward more human-like inductive biases. Human-generated language descriptions and program induction models that add new learned primitives both contain abstract concepts that can compress description length. Co-training on these representations result in more human-like behavior in downstream meta-reinforcement learning agents than less abstract controls (synthetic language descriptions, program induction without learned primitives), suggesting that the abstraction supported by these representations is key. "
Google DeepMind,"Developing, evaluating and scaling learning agents in multi-agent environments",,https://www.deepmind.com/research?d907cb24_page=4#,,https://www.deepmind.com/research?d907cb24_page=4#,
Google DeepMind,From Motor Control to Team Play in Simulated Humanoid Football,,https://www.science.org/stoken/author-tokens/ST-751/full,,https://www.science.org/stoken/author-tokens/ST-751/full,
Google DeepMind,Of Correlated Equilibria in Mean-Field Games,,https://arxiv.org/pdf/2208.10138,,https://arxiv.org/pdf/2208.10138,"The designs of many large-scale systems today, from traffic routing
environments to smart grids, rely on game-theoretic equilibrium concepts.
However, as the size of an $N$-player game typically grows exponentially with
$N$, standard game theoretic analysis becomes effectively infeasible beyond a
low number of players. Recent approaches have gone around this limitation by
instead considering Mean-Field games, an approximation of anonymous $N$-player
games, where the number of players is infinite and the population's state
distribution, instead of every individual player's state, is the object of
interest. The practical computability of Mean-Field Nash equilibria, the most
studied Mean-Field equilibrium to date, however, typically depends on
beneficial non-generic structural properties such as monotonicity or
contraction properties, which are required for known algorithms to converge. In
this work, we provide an alternative route for studying Mean-Field games, by
developing the concepts of Mean-Field correlated and coarse-correlated
equilibria. We show that they can be efficiently learnt in \emph{all games},
without requiring any additional assumption on the structure of the game, using
three classical algorithms. Furthermore, we establish correspondences between
our notions and those already present in the literature, derive optimality
bounds for the Mean-Field - $N$-player transition, and empirically demonstrate
the convergence of these algorithms on simple games."
Google DeepMind,Probing Transfer in Deep Reinforcement Learning without Task Engineering,,https://drive.google.com/file/d/1WXes5w8BB4ep53FzBwwAU8JWhVSbalsZ/view?usp=sharing,,https://drive.google.com/file/d/1WXes5w8BB4ep53FzBwwAU8JWhVSbalsZ/view?usp=sharing,
Google DeepMind,Reinforcement Learning with Information Theoretic Actuation,,https://drive.google.com/file/d/1x0vkR64LQjH1pzP8cF-YsZ8FBNnEKwco/view,,https://drive.google.com/file/d/1x0vkR64LQjH1pzP8cF-YsZ8FBNnEKwco/view,
Google DeepMind,Using the Veil of Ignorance to align AI systems with principles of justice,,https://www.pnas.org/doi/pdf/10.1073/pnas.2213709120?download=true,,https://www.pnas.org/doi/pdf/10.1073/pnas.2213709120?download=true,
Google DeepMind,Meta-Learning Sparse Compression Networks,,https://arxiv.org/abs/2205.08957,,https://arxiv.org/abs/2205.08957,"Recent work in Deep Learning has re-imagined the representation of data as
functions mapping from a coordinate space to an underlying continuous signal.
When such functions are approximated by neural networks this introduces a
compelling alternative to the more common multi-dimensional array
representation. Recent work on such Implicit Neural Representations (INRs) has
shown that - following careful architecture search - INRs can outperform
established compression methods such as JPEG (e.g. Dupont et al., 2021). In
this paper, we propose crucial steps towards making such ideas scalable:
Firstly, we employ state-of-the-art network sparsification techniques to
drastically improve compression. Secondly, introduce the first method allowing
for sparsification to be employed in the inner-loop of commonly used
Meta-Learning algorithms, drastically improving both compression and the
computational cost of learning INRs. The generality of this formalism allows us
to present results on diverse data modalities such as images, manifolds, signed
distance functions, 3D shapes and scenes, several of which establish new
state-of-the-art results."
Google DeepMind,How fair is your graph? Addressing fairness concerns in neuroimaging studies,,https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/62e976e55b86be5eedaf3e74/1659467496923/61+MHLC_61_Camera_Ready,,https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/62e976e55b86be5eedaf3e74/1659467496923/61+MHLC_61_Camera_Ready,
Google DeepMind,Object discovery and representation networks,,https://arxiv.org/pdf/2203.08777,,https://arxiv.org/pdf/2203.08777,"The promise of self-supervised learning (SSL) is to leverage large amounts of
unlabeled data to solve complex tasks. While there has been excellent progress
with simple, image-level learning, recent methods have shown the advantage of
including knowledge of image structure. However, by introducing hand-crafted
image segmentations to define regions of interest, or specialized augmentation
strategies, these methods sacrifice the simplicity and generality that makes
SSL so powerful. Instead, we propose a self-supervised learning paradigm that
discovers this image structure by itself. Our method, Odin, couples object
discovery and representation networks to discover meaningful image
segmentations without any supervision. The resulting learning paradigm is
simpler, less brittle, and more general, and achieves state-of-the-art transfer
learning results for object detection and instance segmentation on COCO, and
semantic segmentation on PASCAL and Cityscapes, while strongly surpassing
supervised pre-training for video segmentation on DAVIS."
Google DeepMind,Learning composable world models for physical prediction,,https://escholarship.org/content/qt615297fs/qt615297fs_noSplash_50ea2e65f465e4cf564fa5ec8bd65f48,,https://escholarship.org/content/qt615297fs/qt615297fs_noSplash_50ea2e65f465e4cf564fa5ec8bd65f48,
Google DeepMind,Semi-analytical Industrial Cooling System Model for Reinforcement Learning,,https://arxiv.org/pdf/2207.13131,,https://arxiv.org/pdf/2207.13131,"We present a hybrid industrial cooling system model that embeds analytical
solutions within a multi-physics simulation. This model is designed for
reinforcement learning (RL) applications and balances simplicity with
simulation fidelity and interpretability. The model's fidelity is evaluated
against real world data from a large scale cooling system. This is followed by
a case study illustrating how the model can be used for RL research. For this,
we develop an industrial task suite that allows specifying different problem
settings and levels of complexity, and use it to evaluate the performance of
different RL algorithms."
Google DeepMind,Stochastic Parallelizable Eigengap Dilation for Large Graph Clustering,,https://drive.google.com/file/d/15JZMbiAsZe9FfITJH4aQI2NzY3INvimS/view,,https://drive.google.com/file/d/15JZMbiAsZe9FfITJH4aQI2NzY3INvimS/view,
Google DeepMind,Generalised Policy Improvement with Geometric Policy Composition,,https://arxiv.org/pdf/2206.08736,,https://arxiv.org/pdf/2206.08736,"We introduce a method for policy improvement that interpolates between the
greedy approach of value-based reinforcement learning (RL) and the full
planning approach typical of model-based RL. The new method builds on the
concept of a geometric horizon model (GHM, also known as a gamma-model), which
models the discounted state-visitation distribution of a given policy. We show
that we can evaluate any non-Markov policy that switches between a set of base
Markov policies with fixed probability by a careful composition of the base
policy GHMs, without any additional learning. We can then apply generalised
policy improvement (GPI) to collections of such non-Markov policies to obtain a
new Markov policy that will in general outperform its precursors. We provide a
thorough theoretical analysis of this approach, develop applications to
transfer and standard RL, and empirically demonstrate its effectiveness over
standard GPI on a challenging deep RL continuous control task. We also provide
an analysis of GHM training methods, proving a novel convergence result
regarding previously proposed methods and showing how to train these models
stably in deep RL settings."
Google DeepMind,Formal Algorithms for Transformers,,https://arxiv.org/pdf/2207.09238,,https://arxiv.org/pdf/2207.09238,"This document aims to be a self-contained, mathematically precise overview of
transformer architectures and algorithms (*not* results). It covers what
transformers are, how they are trained, what they are used for, their key
architectural components, and a preview of the most prominent models. The
reader is assumed to be familiar with basic ML terminology and simpler neural
network architectures such as MLPs."
Google DeepMind,Hindering Adversarial Attacks with Implicit Neural Representations,,https://proceedings.mlr.press/v162/rusu22a/rusu22a,,https://proceedings.mlr.press/v162/rusu22a/rusu22a,
Google DeepMind,Towards Coherent and Consistent Use of Entities in Narrative Generation,,https://arxiv.org/pdf/2202.01709,,https://arxiv.org/pdf/2202.01709,"Large pre-trained language models (LMs) have demonstrated impressive
capabilities in generating long, fluent text; however, there is little to no
analysis on their ability to maintain entity coherence and consistency. In this
work, we focus on the end task of narrative generation and systematically
analyse the long-range entity coherence and consistency in generated stories.
First, we propose a set of automatic metrics for measuring model performance in
terms of entity usage. Given these metrics, we quantify the limitations of
current LMs. Next, we propose augmenting a pre-trained LM with a dynamic entity
memory in an end-to-end manner by using an auxiliary entity-related loss for
guiding the reads and writes to the memory. We demonstrate that the dynamic
entity memory increases entity coherence according to both automatic and human
judgment and helps preserving entity-related information especially in settings
with a limited context window. Finally, we also validate that our automatic
metrics are correlated with human ratings and serve as a good indicator of the
quality of generated stories."
Google DeepMind,From Dirichlet to Rubin: Optimistic exploration in RL without bonuses,,https://arxiv.org/pdf/2205.07704,,https://arxiv.org/pdf/2205.07704,"We propose the Bayes-UCBVI algorithm for reinforcement learning in tabular,
stage-dependent, episodic Markov decision process: a natural extension of the
Bayes-UCB algorithm by Kaufmann et al. (2012) for multi-armed bandits. Our
method uses the quantile of a Q-value function posterior as upper confidence
bound on the optimal Q-value function. For Bayes-UCBVI, we prove a regret bound
of order $\widetilde{O}(\sqrt{H^3SAT})$ where $H$ is the length of one episode,
$S$ is the number of states, $A$ the number of actions, $T$ the number of
episodes, that matches the lower-bound of $\Omega(\sqrt{H^3SAT})$ up to
poly-$\log$ terms in $H,S,A,T$ for a large enough $T$. To the best of our
knowledge, this is the first algorithm that obtains an optimal dependence on
the horizon $H$ (and $S$) without the need for an involved Bernstein-like bonus
or noise. Crucial to our analysis is a new fine-grained anti-concentration
bound for a weighted Dirichlet sum that can be of independent interest. We then
explain how Bayes-UCBVI can be easily extended beyond the tabular setting,
exhibiting a strong link between our algorithm and Bayesian bootstrap (Rubin,
1981)."
Google DeepMind,Importance Weighting Approach in Kernel Bayes' Rule,,https://arxiv.org/pdf/2202.02474,,https://arxiv.org/pdf/2202.02474,"We study a nonparametric approach to Bayesian computation via feature means,
where the expectation of prior features is updated to yield expected kernel
posterior features, based on regression from learned neural net or kernel
features of the observations. All quantities involved in the Bayesian update
are learned from observed data, making the method entirely model-free. The
resulting algorithm is a novel instance of a kernel Bayes' rule (KBR), based on
importance weighting. This results in superior numerical stability to the
original approach to KBR, which requires operator inversion. We show the
convergence of the estimator using a novel consistency analysis on the
importance weighting estimator in the infinity norm. We evaluate KBR on
challenging synthetic benchmarks, including a filtering problem with a
state-space model involving high dimensional image observations. Importance
weighted KBR yields uniformly better empirical performance than the original
KBR, and competitive performance with other competing methods."
Google DeepMind,Language models show human-like content effects on reasoning,,https://arxiv.org/pdf/2207.07051,,https://arxiv.org/pdf/2207.07051,"Abstract reasoning is a key ability for an intelligent system. Large language
models (LMs) achieve above-chance performance on abstract reasoning tasks, but
exhibit many imperfections. However, human abstract reasoning is also
imperfect. For example, human reasoning is affected by our real-world knowledge
and beliefs, and shows notable ""content effects""; humans reason more reliably
when the semantic content of a problem supports the correct logical inferences.
These content-entangled reasoning patterns play a central role in debates about
the fundamental nature of human intelligence. Here, we investigate whether
language models $\unicode{x2014}$ whose prior expectations capture some aspects
of human knowledge $\unicode{x2014}$ similarly mix content into their answers
to logical problems. We explored this question across three logical reasoning
tasks: natural language inference, judging the logical validity of syllogisms,
and the Wason selection task. We evaluate state of the art large language
models, as well as humans, and find that the language models reflect many of
the same patterns observed in humans across these tasks $\unicode{x2014}$ like
humans, models answer more accurately when the semantic content of a task
supports the logical inferences. These parallels are reflected both in answer
patterns, and in lower-level features like the relationship between model
answer distributions and human response times. Our findings have implications
for understanding both these cognitive effects in humans, and the factors that
contribute to language model performance."
Google DeepMind,Intuitive physics learning in a deep-learning model inspired by developmental psychology,,https://www.nature.com/articles/s41562-022-01394-8,,https://www.nature.com/articles/s41562-022-01394-8,
Google DeepMind,Affinity Aware Graph Networks,,https://arxiv.org/pdf/2206.11941,,https://arxiv.org/pdf/2206.11941,"Graph Neural Networks (GNNs) have emerged as a powerful technique for
learning on relational data. Owing to the relatively limited number of message
passing steps they perform -- and hence a smaller receptive field -- there has
been significant interest in improving their expressivity by incorporating
structural aspects of the underlying graph. In this paper, we explore the use
of affinity measures as features in graph neural networks, in particular
measures arising from random walks, including effective resistance, hitting and
commute times. We propose message passing networks based on these features and
evaluate their performance on a variety of node and graph property prediction
tasks. Our architecture has lower computational complexity, while our features
are invariant to the permutations of the underlying graph. The measures we
compute allow the network to exploit the connectivity properties of the graph,
thereby allowing us to outperform relevant benchmarks for a wide variety of
tasks, often with significantly fewer message passing steps. On one of the
largest publicly available graph regression datasets, OGB-LSC-PCQM4Mv1, we
obtain the best known single-model validation MAE at the time of writing."
Google DeepMind,Scaling Gaussian process optimization by evaluating a few unique candidates multiple times,,https://arxiv.org/pdf/2201.12909,,https://arxiv.org/pdf/2201.12909,"Computing a Gaussian process (GP) posterior has a computational cost cubical
in the number of historical points. A reformulation of the same GP posterior
highlights that this complexity mainly depends on how many \emph{unique}
historical points are considered. This can have important implication in active
learning settings, where the set of historical points is constructed
sequentially by the learner. We show that sequential black-box optimization
based on GPs (GP-Opt) can be made efficient by sticking to a candidate solution
for multiple evaluation steps and switch only when necessary. Limiting the
number of switches also limits the number of unique points in the history of
the GP. Thus, the efficient GP reformulation can be used to exactly and cheaply
compute the posteriors required to run the GP-Opt algorithms. This approach is
especially useful in real-world applications of GP-Opt with high switch costs
(e.g. switching chemicals in wet labs, data/model loading in hyperparameter
optimization). As examples of this meta-approach, we modify two
well-established GP-Opt algorithms, GP-UCB and GP-EI, to switch candidates as
infrequently as possible adapting rules from batched GP-Opt. These versions
preserve all the theoretical no-regret guarantees while improving practical
aspects of the algorithms such as runtime, memory complexity, and the ability
of batching candidates and evaluating them in parallel."
Google DeepMind,Can language models learn from explanations in context?,,https://aclanthology.org/2022.findings-emnlp.38,,https://aclanthology.org/2022.findings-emnlp.38,
Google DeepMind,"Subverting machines, fluctuating identities: Re-learning human categorization",,https://arxiv.org/pdf/2205.13740,,https://arxiv.org/pdf/2205.13740,"Most machine learning systems that interact with humans construct some notion
of a person's ""identity,"" yet the default paradigm in AI research envisions
identity with essential attributes that are discrete and static. In stark
contrast, strands of thought within critical theory present a conception of
identity as malleable and constructed entirely through interaction; a doing
rather than a being. In this work, we distill some of these ideas for machine
learning practitioners and introduce a theory of identity as autopoiesis,
circular processes of formation and function. We argue that the default
paradigm of identity used by the field immobilizes existing identity categories
and the power differentials that co$\unicode{x2010}$occur, due to the absence
of iterative feedback to our models. This includes a critique of emergent AI
fairness practices that continue to impose the default paradigm. Finally, we
apply our theory to sketch approaches to autopoietic identity through
multilevel optimization and relational learning. While these ideas raise many
open questions, we imagine the possibilities of machines that are capable of
expressing human identity as a relationship perpetually in flux."
Google DeepMind,Input-level Inductive Biases for 3D Reconstruction,,https://arxiv.org/pdf/2112.03243,,https://arxiv.org/pdf/2112.03243,"Much of the recent progress in 3D vision has been driven by the development
of specialized architectures that incorporate geometrical inductive biases. In
this paper we tackle 3D reconstruction using a domain agnostic architecture and
study how instead to inject the same type of inductive biases directly as extra
inputs to the model. This approach makes it possible to apply existing general
models, such as Perceivers, on this rich domain, without the need for
architectural changes, while simultaneously maintaining data efficiency of
bespoke models. In particular we study how to encode cameras, projective ray
incidence and epipolar geometry as model inputs, and demonstrate competitive
multi-view depth estimation performance on multiple benchmarks."
Google DeepMind,BYOL-Explore: Exploration with Bootstrapped Prediction,,https://arxiv.org/pdf/2206.08332,,https://arxiv.org/pdf/2206.08332,"We present BYOL-Explore, a conceptually simple yet general approach for
curiosity-driven exploration in visually-complex environments. BYOL-Explore
learns a world representation, the world dynamics, and an exploration policy
all-together by optimizing a single prediction loss in the latent space with no
additional auxiliary objective. We show that BYOL-Explore is effective in
DM-HARD-8, a challenging partially-observable continuous-action
hard-exploration benchmark with visually-rich 3-D environments. On this
benchmark, we solve the majority of the tasks purely through augmenting the
extrinsic reward with BYOL-Explore s intrinsic reward, whereas prior work could
only get off the ground with human demonstrations. As further evidence of the
generality of BYOL-Explore, we show that it achieves superhuman performance on
the ten hardest exploration games in Atari while having a much simpler design
than other competitive agents."
Google DeepMind,Unlocking High-Accuracy Differentially Private Image Classification through Scale,,https://arxiv.org/pdf/2204.13650,,https://arxiv.org/pdf/2204.13650,"Differential Privacy (DP) provides a formal privacy guarantee preventing
adversaries with access to a machine learning model from extracting information
about individual training points. Differentially Private Stochastic Gradient
Descent (DP-SGD), the most popular DP training method for deep learning,
realizes this protection by injecting noise during training. However previous
works have found that DP-SGD often leads to a significant degradation in
performance on standard image classification benchmarks. Furthermore, some
authors have postulated that DP-SGD inherently performs poorly on large models,
since the norm of the noise required to preserve privacy is proportional to the
model dimension. In contrast, we demonstrate that DP-SGD on over-parameterized
models can perform significantly better than previously thought. Combining
careful hyper-parameter tuning with simple techniques to ensure signal
propagation and improve the convergence rate, we obtain a new SOTA without
extra data on CIFAR-10 of 81.4% under (8, 10^{-5})-DP using a 40-layer
Wide-ResNet, improving over the previous SOTA of 71.7%. When fine-tuning a
pre-trained NFNet-F3, we achieve a remarkable 83.8% top-1 accuracy on ImageNet
under (0.5, 8*10^{-7})-DP. Additionally, we also achieve 86.7% top-1 accuracy
under (8, 8 \cdot 10^{-7})-DP, which is just 4.3% below the current non-private
SOTA for this task. We believe our results are a significant step towards
closing the accuracy gap between private and non-private image classification."
Google DeepMind,Sheaf Neural Networks with Connection Laplacians,,https://arxiv.org/pdf/2206.08702,,https://arxiv.org/pdf/2206.08702,"A Sheaf Neural Network (SNN) is a type of Graph Neural Network (GNN) that
operates on a sheaf, an object that equips a graph with vector spaces over its
nodes and edges and linear maps between these spaces. SNNs have been shown to
have useful theoretical properties that help tackle issues arising from
heterophily and over-smoothing. One complication intrinsic to these models is
finding a good sheaf for the task to be solved. Previous works proposed two
diametrically opposed approaches: manually constructing the sheaf based on
domain knowledge and learning the sheaf end-to-end using gradient-based
methods. However, domain knowledge is often insufficient, while learning a
sheaf could lead to overfitting and significant computational overhead. In this
work, we propose a novel way of computing sheaves drawing inspiration from
Riemannian geometry: we leverage the manifold assumption to compute
manifold-and-graph-aware orthogonal maps, which optimally align the tangent
spaces of neighbouring data points. We show that this approach achieves
promising results with less computational overhead when compared to previous
SNN models. Overall, this work provides an interesting connection between
algebraic topology and differential geometry, and we hope that it will spark
future research in this direction."
Google DeepMind,BYOL-Explore: Exploration by bootstrapped prediction,,https://arxiv.org/abs/2206.08332,,https://arxiv.org/abs/2206.08332,"We present BYOL-Explore, a conceptually simple yet general approach for
curiosity-driven exploration in visually-complex environments. BYOL-Explore
learns a world representation, the world dynamics, and an exploration policy
all-together by optimizing a single prediction loss in the latent space with no
additional auxiliary objective. We show that BYOL-Explore is effective in
DM-HARD-8, a challenging partially-observable continuous-action
hard-exploration benchmark with visually-rich 3-D environments. On this
benchmark, we solve the majority of the tasks purely through augmenting the
extrinsic reward with BYOL-Explore s intrinsic reward, whereas prior work could
only get off the ground with human demonstrations. As further evidence of the
generality of BYOL-Explore, we show that it achieves superhuman performance on
the ten hardest exploration games in Atari while having a much simpler design
than other competitive agents."
Google DeepMind,"Perceiver AR: general-purpose, long-context autoregressive generation",,https://arxiv.org/pdf/2202.07765,,https://arxiv.org/pdf/2202.07765,"Real-world data is high-dimensional: a book, image, or musical performance
can easily contain hundreds of thousands of elements even after compression.
However, the most commonly used autoregressive models, Transformers, are
prohibitively expensive to scale to the number of inputs and layers needed to
capture this long-range structure. We develop Perceiver AR, an autoregressive,
modality-agnostic architecture which uses cross-attention to map long-range
inputs to a small number of latents while also maintaining end-to-end causal
masking. Perceiver AR can directly attend to over a hundred thousand tokens,
enabling practical long-context density estimation without the need for
hand-crafted sparsity patterns or memory mechanisms. When trained on images or
music, Perceiver AR generates outputs with clear long-term coherence and
structure. Our architecture also obtains state-of-the-art likelihood on
long-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books."
Google DeepMind,Simplex Neural Population Learning: Any-Mixture Bayes-Optimality in Symmetric Zero-sum Games,,https://proceedings.mlr.press/v162/liu22h.html,,https://proceedings.mlr.press/v162/liu22h.html,
Google DeepMind,Expressing Non-Markov Reward to a Markov Agent,,https://david-abel.github.io/papers/rldm2022_markov,,https://david-abel.github.io/papers/rldm2022_markov,
Google DeepMind,Towards Situated Communication in Multi-Step Interactions: Time is a Key Pressure in Communication Emergence,,https://escholarship.org/uc/item/61k7486v,,https://escholarship.org/uc/item/61k7486v,
Google DeepMind,An EigenGame for the Generalized Eigenvalue Problem,,https://arxiv.org/pdf/2206.04993,,https://arxiv.org/pdf/2206.04993,"The symmetric generalized eigenvalue problem (SGEP) is a fundamental concept
in numerical linear algebra. It captures the solution of many classical machine
learning problems such as canonical correlation analysis, independent
components analysis, partial least squares, linear discriminant analysis,
principal components and others. Despite this, most general solvers are
prohibitively expensive when dealing with streaming data sets (i.e.,
minibatches) and research has instead concentrated on finding efficient
solutions to specific problem instances. In this work, we develop a
game-theoretic formulation of the top-$k$ SGEP whose Nash equilibrium is the
set of generalized eigenvectors. We also present a parallelizable algorithm
with guaranteed asymptotic convergence to the Nash. Current state-of-the-art
methods require $O(d^2k)$ runtime complexity per iteration which is
prohibitively expensive when the number of dimensions ($d$) is large. We show
how to modify this parallel approach to achieve $O(dk)$ runtime complexity.
Empirically we demonstrate that this resulting algorithm is able to solve a
variety of SGEP problem instances including a large-scale analysis of neural
network activations."
Google DeepMind,Situated communication: a solution to over-communication between artificial agents,,https://openreview.net/forum?id=HLqzzQWA7Z9,,https://openreview.net/forum?id=HLqzzQWA7Z9,"Most research on communication emergence between reinforcement learning (RL) agents explores unsituated communication in one-step referential tasks. The tasks are not temporally interactive and lack time pressures typically present in natural communication and language learning. In these settings, agents can successfully learn to communicate, but they do not learn to exchange information concisely—they tend towards over-communication and an anti-efficient encoding. In our work, we introduce situated communication by imposing an opportunity cost on communication—the acting agent has to forgo an action to solicit information from its advisor. Situated communication mimics the external pressure of passing time in real-world communication. We compare language emergence under this pressure against language learning with an internal cost on articulation, implemented as a per-message penalty. We find that while both pressures can disincentivise over-communication, situated communication does it more effectively and, unlike the internal pressure, does not negatively impact communication emergence. Implementing an opportunity cost on communication might be key to shaping language properties and incentivising concise information sharing between artificial agents."
Google DeepMind,Communication Emergence in a Goal-Oriented Environment:Towards Situated Communication in Multi-Step Interactions,,https://www.deepmind.com/research?d907cb24_page=5#,,https://www.deepmind.com/research?d907cb24_page=5#,
Google DeepMind,A Simple Approach for State-Action Abstraction\\using a Learned MDP Homomorphism,,https://arxiv.org/pdf/2209.06356,,https://arxiv.org/pdf/2209.06356,"Reinforcement learning agents must painstakingly learn through trial and
error what sets of state-action pairs are value equivalent -- requiring an
often prohibitively large amount of environment experience. MDP homomorphisms
have been proposed that reduce the MDP of an environment to an abstract MDP,
enabling better sample efficiency. Consequently, impressive improvements have
been achieved when a suitable homomorphism can be constructed a priori --
usually by exploiting a practitioner's knowledge of environment symmetries. We
propose a novel approach to constructing homomorphisms in discrete action
spaces, which uses a learnt model of environment dynamics to infer which
state-action pairs lead to the same state -- which can reduce the size of the
state-action space by a factor as large as the cardinality of the original
action space. In MinAtar, we report an almost 4x improvement over a value-based
off-policy baseline in the low sample limit, when averaging over all games and
optimizers."
Google DeepMind,Utility of Equivariant Message Passing in Cortical Mesh Segmentation,,https://arxiv.org/pdf/2206.03164,,https://arxiv.org/pdf/2206.03164,"The automated segmentation of cortical areas has been a long-standing
challenge in medical image analysis. The complex geometry of the cortex is
commonly represented as a polygon mesh, whose segmentation can be addressed by
graph-based learning methods. When cortical meshes are misaligned across
subjects, current methods produce significantly worse segmentation results,
limiting their ability to handle multi-domain data. In this paper, we
investigate the utility of E(n)-equivariant graph neural networks (EGNNs),
comparing their performance against plain graph neural networks (GNNs). Our
evaluation shows that GNNs outperform EGNNs on aligned meshes, due to their
ability to leverage the presence of a global coordinate system. On misaligned
meshes, the performance of plain GNNs drop considerably, while E(n)-equivariant
message passing maintains the same segmentation results. The best results can
also be obtained by using plain GNNs on realigned data (co-registered meshes in
a global coordinate system)."
Google DeepMind,StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models,,https://arxiv.org/pdf/2205.11388,,https://arxiv.org/pdf/2205.11388,"Knowledge and language understanding of models evaluated through question
answering (QA) has been usually studied on static snapshots of knowledge, like
Wikipedia. However, our world is dynamic, evolves over time, and our models'
knowledge becomes outdated. To study how semi-parametric QA models and their
underlying parametric language models (LMs) adapt to evolving knowledge, we
construct a new large-scale dataset, StreamingQA, with human written and
generated questions asked on a given date, to be answered from 14 years of
time-stamped news articles. We evaluate our models quarterly as they read new
articles not seen in pre-training. We show that parametric models can be
updated without full retraining, while avoiding catastrophic forgetting. For
semi-parametric models, adding new articles into the search space allows for
rapid adaptation, however, models with an outdated underlying LM under-perform
those with a retrained LM. For questions about higher-frequency named entities,
parametric updates are particularly beneficial. In our dynamic world, the
StreamingQA dataset enables a more realistic evaluation of QA models, and our
experiments highlight several promising directions for future research."
Google DeepMind,The CLRS Algorithmic Reasoning Benchmark,,https://arxiv.org/pdf/2205.15659,,https://arxiv.org/pdf/2205.15659,"Learning representations of algorithms is an emerging area of machine
learning, seeking to bridge concepts from neural networks with classical
algorithms. Several important works have investigated whether neural networks
can effectively reason like algorithms, typically by learning to execute them.
The common trend in the area, however, is to generate targeted kinds of
algorithmic data to evaluate specific hypotheses, making results hard to
transfer across publications, and increasing the barrier of entry. To
consolidate progress and work towards unified evaluation, we propose the CLRS
Algorithmic Reasoning Benchmark, covering classical algorithms from the
Introduction to Algorithms textbook. Our benchmark spans a variety of
algorithmic reasoning procedures, including sorting, searching, dynamic
programming, graph algorithms, string algorithms and geometric algorithms. We
perform extensive experiments to demonstrate how several popular algorithmic
reasoning baselines perform on these tasks, and consequently, highlight links
to several open challenges. Our library is readily available at
https://github.com/deepmind/clrs."
Google DeepMind,The CLRS Algorithmic Reasoning Benchmark,,https://arxiv.org/pdf/2205.15659,,https://arxiv.org/pdf/2205.15659,"Learning representations of algorithms is an emerging area of machine
learning, seeking to bridge concepts from neural networks with classical
algorithms. Several important works have investigated whether neural networks
can effectively reason like algorithms, typically by learning to execute them.
The common trend in the area, however, is to generate targeted kinds of
algorithmic data to evaluate specific hypotheses, making results hard to
transfer across publications, and increasing the barrier of entry. To
consolidate progress and work towards unified evaluation, we propose the CLRS
Algorithmic Reasoning Benchmark, covering classical algorithms from the
Introduction to Algorithms textbook. Our benchmark spans a variety of
algorithmic reasoning procedures, including sorting, searching, dynamic
programming, graph algorithms, string algorithms and geometric algorithms. We
perform extensive experiments to demonstrate how several popular algorithmic
reasoning baselines perform on these tasks, and consequently, highlight links
to several open challenges. Our library is readily available at
https://github.com/deepmind/clrs."
Google DeepMind,Uniqueness and Complexity of Inverse MDP Models,,https://arxiv.org/pdf/2206.01192,,https://arxiv.org/pdf/2206.01192,"What is the action sequence aa'a"" that was likely responsible for reaching
state s""' (from state s) in 3 steps? Addressing such questions is important in
causal reasoning and in reinforcement learning. Inverse ""MDP"" models
p(aa'a""|ss""') can be used to answer them. In the traditional ""forward"" view,
transition ""matrix"" p(s'|sa) and policy {\pi}(a|s) uniquely determine
""everything"": the whole dynamics p(as'a's""a""...|s), and with it, the
action-conditional state process p(s's""...|saa'a""), the multi-step inverse
models p(aa'a""...|ss^i), etc. If the latter is our primary concern, a natural
question, analogous to the forward case is to which extent 1-step inverse model
p(a|ss') plus policy {\pi}(a|s) determine the multi-step inverse models or even
the whole dynamics. In other words, can forward models be inferred from inverse
models or even be side-stepped. This work addresses this question and
variations thereof, and also whether there are efficient decision/inference
algorithms for this."
Google DeepMind,Tell me why! Explanations support learning relational and causal structure,,https://arxiv.org/pdf/2112.03753,,https://arxiv.org/pdf/2112.03753,"Inferring the abstract relational and causal structure of the world is a
major challenge for reinforcement-learning (RL) agents. For humans,
language--particularly in the form of explanations--plays a considerable role
in overcoming this challenge. Here, we show that language can play a similar
role for deep RL agents in complex environments. While agents typically
struggle to acquire relational and causal knowledge, augmenting their
experience by training them to predict language descriptions and explanations
can overcome these limitations. We show that language can help agents learn
challenging relational tasks, and examine which aspects of language contribute
to its benefits. We then show that explanations can help agents to infer not
only relational but also causal structure. Language can shape the way that
agents to generalize out-of-distribution from ambiguous, causally-confounded
training, and explanations even allow agents to learn to perform experimental
interventions to identify causal relationships. Our results suggest that
language description and explanation may be powerful tools for improving agent
learning and generalization."
Google DeepMind,Objective Robustness in Deep Reinforcement Learning,,https://arxiv.org/pdf/2105.14111,,https://arxiv.org/pdf/2105.14111,"We study goal misgeneralization, a type of out-of-distribution generalization
failure in reinforcement learning (RL). Goal misgeneralization failures occur
when an RL agent retains its capabilities out-of-distribution yet pursues the
wrong goal. For instance, an agent might continue to competently avoid
obstacles, but navigate to the wrong place. In contrast, previous works have
typically focused on capability generalization failures, where an agent fails
to do anything sensible at test time. We formalize this distinction between
capability and goal generalization, provide the first empirical demonstrations
of goal misgeneralization, and present a partial characterization of its
causes."
Google DeepMind,Evaluating Multimodal Interactive Agents,,https://arxiv.org/abs/2205.13274,,https://arxiv.org/abs/2205.13274,"Creating agents that can interact naturally with humans is a common goal in
artificial intelligence (AI) research. However, evaluating these interactions
is challenging: collecting online human-agent interactions is slow and
expensive, yet faster proxy metrics often do not correlate well with
interactive evaluation. In this paper, we assess the merits of these existing
evaluation metrics and present a novel approach to evaluation called the
Standardised Test Suite (STS). The STS uses behavioural scenarios mined from
real human interaction data. Agents see replayed scenario context, receive an
instruction, and are then given control to complete the interaction offline.
These agent continuations are recorded and sent to human annotators to mark as
success or failure, and agents are ranked according to the proportion of
continuations in which they succeed. The resulting STS is fast, controlled,
interpretable, and representative of naturalistic interactions. Altogether, the
STS consolidates much of what is desirable across many of our standard
evaluation metrics, allowing us to accelerate research progress towards
producing agents that can interact naturally with humans. A video may be found
at https://youtu.be/YR1TngGORGQ."
Google DeepMind,Dynamic language understanding: adaptation to new knowledge in parametric and semi-parametric models,,https://arxiv.org/pdf/2205.11388,,https://arxiv.org/pdf/2205.11388,"Knowledge and language understanding of models evaluated through question
answering (QA) has been usually studied on static snapshots of knowledge, like
Wikipedia. However, our world is dynamic, evolves over time, and our models'
knowledge becomes outdated. To study how semi-parametric QA models and their
underlying parametric language models (LMs) adapt to evolving knowledge, we
construct a new large-scale dataset, StreamingQA, with human written and
generated questions asked on a given date, to be answered from 14 years of
time-stamped news articles. We evaluate our models quarterly as they read new
articles not seen in pre-training. We show that parametric models can be
updated without full retraining, while avoiding catastrophic forgetting. For
semi-parametric models, adding new articles into the search space allows for
rapid adaptation, however, models with an outdated underlying LM under-perform
those with a retrained LM. For questions about higher-frequency named entities,
parametric updates are particularly beneficial. In our dynamic world, the
StreamingQA dataset enables a more realistic evaluation of QA models, and our
experiments highlight several promising directions for future research."
Google DeepMind,Contextual Information-Directed Sampling,,https://arxiv.org/pdf/2205.10895,,https://arxiv.org/pdf/2205.10895,"Information-directed sampling (IDS) has recently demonstrated its potential
as a data-efficient reinforcement learning algorithm. However, it is still
unclear what is the right form of information ratio to optimize when contextual
information is available. We investigate the IDS design through two contextual
bandit problems: contextual bandits with graph feedback and sparse linear
contextual bandits. We provably demonstrate the advantage of contextual IDS
over conditional IDS and emphasize the importance of considering the context
distribution. The main message is that an intelligent agent should invest more
on the actions that are beneficial for the future unseen contexts while the
conditional IDS can be myopic. We further propose a computationally-efficient
version of contextual IDS based on Actor-Critic and evaluate it empirically on
a neural network contextual bandit."
Google DeepMind,Pre-training via Denoising for Molecular Property Prediction,,https://arxiv.org/pdf/2206.00133,,https://arxiv.org/pdf/2206.00133,"Many important problems involving molecular property prediction from 3D
structures have limited data, posing a generalization challenge for neural
networks. In this paper, we describe a pre-training technique based on
denoising that achieves a new state-of-the-art in molecular property prediction
by utilizing large datasets of 3D molecular structures at equilibrium to learn
meaningful representations for downstream tasks. Relying on the well-known link
between denoising autoencoders and score-matching, we show that the denoising
objective corresponds to learning a molecular force field -- arising from
approximating the Boltzmann distribution with a mixture of Gaussians --
directly from equilibrium structures. Our experiments demonstrate that using
this pre-training objective significantly improves performance on multiple
benchmarks, achieving a new state-of-the-art on the majority of targets in the
widely used QM9 dataset. Our analysis then provides practical insights into the
effects of different factors -- dataset sizes, model size and architecture, and
the choice of upstream and downstream datasets -- on pre-training."
Google DeepMind,MultiScale MeshGraphNets,,https://openreview.net/forum?id=G3TRIsmMhhf,,https://openreview.net/forum?id=G3TRIsmMhhf,"In recent years, there has been a growing interest in using machine learning to overcome the high cost of numerical simulation, with some learned models achieving impressive speed-ups over classical solvers whilst maintaining accuracy. However, these methods are usually tested at low-resolution settings, and it remains to be seen whether they can scale to the costly high-resolution simulations that we ultimately want to tackle.
In this work, we propose two complementary approaches to improve the framework from MeshGraphNets, which demonstrated accurate predictions in a broad range of physical systems. MeshGraphNets relies on a message passing graph neural network to propagate information, and this structure becomes a limiting factor for high-resolution simulations, as equally distant points in space become further apart in graph space. First, we demonstrate that it is possible to learn accurate surrogate dynamics of a high-resolution system on a much coarser mesh, both removing the message passing bottleneck and improving performance; and second, we introduce a hierarchical approach (MultiScale MeshGraphNets) which passes messages on two different resolutions (fine and coarse), significantly improving the accuracy of MeshGraphNets while requiring less computational resources.
"
Google DeepMind,CLIP-CLOP: CLIP-Guided Collage and Photomontage,,https://arxiv.org/pdf/2205.03146,,https://arxiv.org/pdf/2205.03146,"The unabated mystique of large-scale neural networks, such as the CLIP dual
image-and-text encoder, popularized automatically generated art. Increasingly
more sophisticated generators enhanced the artworks' realism and visual
appearance, and creative prompt engineering enabled stylistic expression.
Guided by an artist-in-the-loop ideal, we design a gradient-based generator to
produce collages. It requires the human artist to curate libraries of image
patches and to describe (with prompts) the whole image composition, with the
option to manually adjust the patches' positions during generation, thereby
allowing humans to reclaim some control of the process and achieve greater
creative freedom. We explore the aesthetic potentials of high-resolution
collages, and provide an open-source Google Colab as an artistic tool."
Google DeepMind,Selection-Inference: Iterative Interpretable Reasoning Module,,https://arxiv.org/pdf/2205.09712,,https://arxiv.org/pdf/2205.09712,"Large language models (LLMs) have been shown to be capable of impressive
few-shot generalisation to new tasks. However, they still tend to perform
poorly on multi-step logical reasoning problems. Here we carry out a
comprehensive evaluation of LLMs on 50 tasks that probe different aspects of
logical reasoning. We show that language models tend to perform fairly well at
single step inference or entailment tasks, but struggle to chain together
multiple reasoning steps to solve more complex problems. In light of this, we
propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as
general processing modules, and alternates between selection and inference to
generate a series of interpretable, casual reasoning steps leading to the final
answer. We show that a 7B parameter LLM used within the SI framework in a
5-shot generalisation setting, with no fine-tuning, yields a performance
improvement of over 100% compared to an equivalent vanilla baseline on a suite
of 10 logical reasoning tasks. The same model in the same setting even
outperforms a significantly larger 280B parameter baseline on the same suite of
tasks. Moreover, answers produced by the SI framework are accompanied by a
causal natural-language-based reasoning trace, which has important implications
for the safety and trustworthiness of the system."
Google DeepMind,Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning,,https://arxiv.org/pdf/2205.06760,,https://arxiv.org/pdf/2205.06760,"Advances in artificial intelligence often stem from the development of new
environments that abstract real-world situations into a form where research can
be done conveniently. This paper contributes such an environment based on ideas
inspired by elementary Microeconomics. Agents learn to produce resources in a
spatially complex world, trade them with one another, and consume those that
they prefer. We show that the emergent production, consumption, and pricing
behaviors respond to environmental conditions in the directions predicted by
supply and demand shifts in Microeconomics. We also demonstrate settings where
the agents' emergent prices for goods vary over space, reflecting the local
abundance of goods. After the price disparities emerge, some agents then
discover a niche of transporting goods between regions with different
prevailing prices -- a profitable strategy because they can buy goods where
they are cheap and sell them where they are expensive. Finally, in a series of
ablation experiments, we investigate how choices in the environmental rewards,
bartering actions, agent architecture, and ability to consume tradable goods
can either aid or inhibit the emergence of this economic behavior. This work is
part of the environment development branch of a research program that aims to
build human-like artificial general intelligence through multi-agent
interactions in simulated societies. By exploring which environment features
are needed for the basic phenomena of elementary microeconomics to emerge
automatically from learning, we arrive at an environment that differs from
those studied in prior multi-agent reinforcement learning work along several
dimensions. For example, the model incorporates heterogeneous tastes and
physical abilities, and agents negotiate with one another as a grounded form of
communication."
Google DeepMind,Normalizing flows for atomic solids,,https://iopscience.iop.org/article/10.1088/2632-2153/ac6b16/pdf,,https://iopscience.iop.org/article/10.1088/2632-2153/ac6b16/pdf,
Google DeepMind,Constraint-based graph network simulator,,https://arxiv.org/pdf/2112.09161,,https://arxiv.org/pdf/2112.09161,"In the area of physical simulations, nearly all neural-network-based methods
directly predict future states from the input states. However, many traditional
simulation engines instead model the constraints of the system and select the
state which satisfies them. Here we present a framework for constraint-based
learned simulation, where a scalar constraint function is implemented as a
graph neural network, and future predictions are computed by solving the
optimization problem defined by the learned constraint. Our model achieves
comparable or better accuracy to top learned simulators on a variety of
challenging physical domains, and offers several unique advantages. We can
improve the simulation accuracy on a larger system by applying more solver
iterations at test time. We also can incorporate novel hand-designed
constraints at test time and simulate new dynamics which were not present in
the training data. Our constraint-based framework shows how key techniques from
traditional simulation and numerical methods can be leveraged as inductive
biases in machine learning simulators."
Google DeepMind,Biased Gradient Estimate with Drastic Variance Reduction for Meta Reinforcement Learning,,https://arxiv.org/abs/2112.07328,,https://arxiv.org/abs/2112.07328,"Despite the empirical success of meta reinforcement learning (meta-RL), there
are still a number poorly-understood discrepancies between theory and practice.
Critically, biased gradient estimates are almost always implemented in
practice, whereas prior theory on meta-RL only establishes convergence under
unbiased gradient estimates. In this work, we investigate such a discrepancy.
In particular, (1) We show that unbiased gradient estimates have variance
$\Theta(N)$ which linearly depends on the sample size $N$ of the inner loop
updates; (2) We propose linearized score function (LSF) gradient estimates,
which have bias $\mathcal{O}(1/\sqrt{N})$ and variance $\mathcal{O}(1/N)$; (3)
We show that most empirical prior work in fact implements variants of the LSF
gradient estimates. This implies that practical algorithms ""accidentally""
introduce bias to achieve better performance; (4) We establish theoretical
guarantees for the LSF gradient estimates in meta-RL regarding its convergence
to stationary points, showing better dependency on $N$ than prior work when $N$
is large."
Google DeepMind,Warmth and competence in human-agent cooperation,,https://arxiv.org/pdf/2201.13448,,https://arxiv.org/pdf/2201.13448,"Interaction and cooperation with humans are overarching aspirations of
artificial intelligence (AI) research. Recent studies demonstrate that AI
agents trained with deep reinforcement learning are capable of collaborating
with humans. These studies primarily evaluate human compatibility through
""objective"" metrics such as task performance, obscuring potential variation in
the levels of trust and subjective preference that different agents garner. To
better understand the factors shaping subjective preferences in human-agent
cooperation, we train deep reinforcement learning agents in Coins, a two-player
social dilemma. We recruit participants for a human-agent cooperation study and
measure their impressions of the agents they encounter. Participants'
perceptions of warmth and competence predict their stated preferences for
different agents, above and beyond objective performance metrics. Drawing
inspiration from social science and biology research, we subsequently implement
a new ""partner choice"" framework to elicit revealed preferences: after playing
an episode with an agent, participants are asked whether they would like to
play the next round with the same agent or to play alone. As with stated
preferences, social perception better predicts participants' revealed
preferences than does objective performance. Given these results, we recommend
human-agent interaction researchers routinely incorporate the measurement of
social perception and subjective preferences into their studies."
Google DeepMind,Active offline policy selection,,https://arxiv.org/pdf/2106.10251,,https://arxiv.org/pdf/2106.10251,"This paper addresses the problem of policy selection in domains with abundant
logged data, but with a restricted interaction budget. Solving this problem
would enable safe evaluation and deployment of offline reinforcement learning
policies in industry, robotics, and recommendation domains among others.
Several off-policy evaluation (OPE) techniques have been proposed to assess the
value of policies using only logged data. However, there is still a big gap
between the evaluation by OPE and the full online evaluation. Yet, large
amounts of online interactions are often not possible in practice. To overcome
this problem, we introduce active offline policy selection - a novel sequential
decision approach that combines logged data with online interaction to identify
the best policy. We use OPE estimates to warm start the online evaluation.
Then, in order to utilize the limited environment interactions wisely we decide
which policy to evaluate next based on a Bayesian optimization method with a
kernel that represents policy similarity. We use multiple benchmarks, including
real-world robotics, with a large number of candidate policies to show that the
proposed approach improves upon state-of-the-art OPE estimates and pure online
policy evaluation."
Google DeepMind,Perception Shapes Language - Which Concepts Evolve Between Heterogeneous Agents? - Communicating Movement Instructions For Agents With Widely Different Time Scopes,,https://openreview.net/forum?id=BnfgM7-0mW5,,https://openreview.net/forum?id=BnfgM7-0mW5,"This paper studies the evolving communication between two agents, a listener and speaker, in a plan execution task in which the speaker needs to communicate the plan to the acting agent, while operating on different time scales. 
We analyse the topographic similarity of the resulting language learned by the proposed imagination-based learning process.
As the speaker agent perceives the movement space strictly in absolute coordinates and the actor can only choose relative actions in the movement space, we can show that the structure of their emergent communication is not predestined. 
Both relative and absolute encodings of desired movements can develop by chance in this setting, but we can alter the chance by using a population of learners. 
We conclude that our imagination-based learning strategy successfully breaks the strict hierarchy between planner and executioner."
Google DeepMind,Learning transferable motor skills with hierarchical latent mixture policies,,https://openreview.net/forum?id=qTHBE7E9iej,,https://openreview.net/forum?id=qTHBE7E9iej,"For robots operating in the real world, it is desirable to learn reusable abstract behaviours that can effectively be transferred across numerous tasks and scenarios.
We propose an approach to learn skills from data using a hierarchical mixture latent variable model.
Our method exploits a multi-level hierarchy of both discrete and continuous latent variables, to model a discrete set of abstract high-level behaviours while allowing for variance in how they are executed.
We demonstrate in manipulation domains that the method can effectively cluster offline data into distinct, executable behaviours, while retaining the flexibility of a continuous latent variable model.
The resulting skills can be transferred to new tasks, unseen objects, and from state to vision-based policies, yielding significantly better sample efficiency and asymptotic performance compared to existing skill- and imitation-based methods.
We also perform further analysis showing how and when the skills are most beneficial: they encourage directed exploration to cover large regions of the state space relevant to the task, making them most effective in challenging sparse-reward settings."
Google DeepMind,The prospects and opportunities of protein structure prediction with AI,,https://www.deepmind.com/research?d907cb24_page=6#,,https://www.deepmind.com/research?d907cb24_page=6#,
Google DeepMind,Measuring CLEVRness: Black-box Testing of Visual Reasoning Models,,https://openreview.net/forum?id=UtGtoS4CYU,,https://openreview.net/forum?id=UtGtoS4CYU,"How can we measure the reasoning capabilities of intelligence systems? Visual question answering provides a convenient framework for testing the model's abilities by interrogating the model through questions about the scene. However, despite scores of various visual QA datasets and architectures, which sometimes yield even a super-human performance, the question of whether those architectures can actually reason remains open to debate.
To answer this, we extend the visual question answering framework and propose the following behavioral test in the form of a two-player game. We consider black-box neural models of CLEVR. These models are trained on a diagnostic dataset benchmarking reasoning. Next, we train an adversarial player that re-configures the scene to fool the CLEVR model. We show that CLEVR models, which otherwise could perform at a ``human-level'', can easily be fooled by our agent. Our results 
put in doubt whether data-driven approaches can do reasoning without exploiting the numerous biases that are often present in those datasets. Finally, we also propose a controlled experiment measuring the efficiency of such models to learn and perform reasoning."
Google DeepMind,Fair Normalizing Flows,,https://openreview.net/forum?id=BrFIKuxrZE,,https://openreview.net/forum?id=BrFIKuxrZE,"Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets."
Google DeepMind,Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers,,https://arxiv.org/pdf/2205.05055,,https://arxiv.org/pdf/2205.05055,"Large transformer-based models are able to perform in-context few-shot
learning, without being explicitly trained for it. This observation raises the
question: what aspects of the training regime lead to this emergent behavior?
Here, we show that this behavior is driven by the distributions of the training
data itself. In-context learning emerges when the training data exhibits
particular distributional properties such as burstiness (items appear in
clusters rather than being uniformly distributed over time) and having large
numbers of rarely occurring classes. In-context learning also emerges more
strongly when item meanings or interpretations are dynamic rather than fixed.
These properties are exemplified by natural language, but are also inherent to
naturalistic data in a wide range of other domains. They also depart
significantly from the uniform, i.i.d. training distributions typically used
for standard supervised learning. In our initial experiments, we found that
in-context learning traded off against more conventional weight-based learning,
and models were unable to achieve both simultaneously. However, our later
experiments uncovered that the two modes of learning could co-exist in a single
model when it was trained on data following a skewed Zipfian distribution --
another common property of naturalistic data, including language. In further
experiments, we found that naturalistic data distributions were only able to
elicit in-context learning in transformers, and not in recurrent models. In
sum, our findings indicate how the transformer architecture works together with
particular properties of the training data to drive the intriguing emergent
in-context learning behaviour of large language models, and how future work
might encourage both in-context and in-weights learning in domains beyond
language."
Google DeepMind,A Brief Guide to Designing and Evaluating Human-Centered Interactive Machine Learning,,https://arxiv.org/pdf/2204.09622,,https://arxiv.org/pdf/2204.09622,"Interactive machine learning (IML) is a field of research that explores how
to leverage both human and computational abilities in decision making systems.
IML represents a collaboration between multiple complementary human and machine
intelligent systems working as a team, each with their own unique abilities and
limitations. This teamwork might mean that both systems take actions at the
same time, or in sequence. Two major open research questions in the field of
IML are: ""How should we design systems that can learn to make better decisions
over time with human interaction?"" and ""How should we evaluate the design and
deployment of such systems?"" A lack of appropriate consideration for the humans
involved can lead to problematic system behaviour, and issues of fairness,
accountability, and transparency. Thus, our goal with this work is to present a
human-centred guide to designing and evaluating IML systems while mitigating
risks. This guide is intended to be used by machine learning practitioners who
are responsible for the health, safety, and well-being of interacting humans.
An obligation of responsibility for public interaction means acting with
integrity, honesty, fairness, and abiding by applicable legal statutes. With
these values and principles in mind, we as a machine learning research
community can better achieve goals of augmenting human skills and abilities.
This practical guide therefore aims to support many of the responsible
decisions necessary throughout the iterative design, development, and
dissemination of IML systems."
Google DeepMind,Offline Distillation for Robot Lifelong Learning with Imbalanced Experience,,https://arxiv.org/pdf/2204.05893,,https://arxiv.org/pdf/2204.05893,"Robots will experience non-stationary environment dynamics throughout their
lifetime: the robot dynamics can change due to wear and tear, or its
surroundings may change over time. Eventually, the robots should perform well
in all of the environment variations it has encountered. At the same time, it
should still be able to learn fast in a new environment. We identify two
challenges in Reinforcement Learning (RL) under such a lifelong learning
setting with off-policy data: first, existing off-policy algorithms struggle
with the trade-off between being conservative to maintain good performance in
the old environment and learning efficiently in the new environment, despite
keeping all the data in the replay buffer. We propose the Offline Distillation
Pipeline to break this trade-off by separating the training procedure into an
online interaction phase and an offline distillation phase.Second, we find that
training with the imbalanced off-policy data from multiple environments across
the lifetime creates a significant performance drop. We identify that this
performance drop is caused by the combination of the imbalanced quality and
size among the datasets which exacerbate the extrapolation error of the
Q-function. During the distillation phase, we apply a simple fix to the issue
by keeping the policy closer to the behavior policy that generated the data. In
the experiments, we demonstrate these two challenges and the proposed solutions
with a simulated bipedal robot walk-ing task across various environment
changes. We show that the Offline Distillation Pipeline achieves better
performance across all the encountered environments without affecting data
collection. We also provide a comprehensive empirical study to support our
hypothesis on the data imbalance issue."
Google DeepMind,An empirical analysis of compute-optimal large language model training,,https://arxiv.org/pdf/2203.15556,,https://arxiv.org/pdf/2203.15556,"We investigate the optimal model size and number of tokens for training a
transformer language model under a given compute budget. We find that current
large language models are significantly undertrained, a consequence of the
recent focus on scaling language models whilst keeping the amount of training
data constant. By training over 400 language models ranging from 70 million to
over 16 billion parameters on 5 to 500 billion tokens, we find that for
compute-optimal training, the model size and the number of training tokens
should be scaled equally: for every doubling of model size the number of
training tokens should also be doubled. We test this hypothesis by training a
predicted compute-optimal model, Chinchilla, that uses the same compute budget
as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla
uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1
(178B), and Megatron-Turing NLG (530B) on a large range of downstream
evaluation tasks. This also means that Chinchilla uses substantially less
compute for fine-tuning and inference, greatly facilitating downstream usage.
As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
on the MMLU benchmark, greater than a 7% improvement over Gopher."
Google DeepMind,Learning heuristics for A*,,https://arxiv.org/pdf/2204.08938,,https://arxiv.org/pdf/2204.08938,"Path finding in graphs is one of the most studied classes of problems in
computer science. In this context, search algorithms are often extended with
heuristics for a more efficient search of target nodes. In this work we combine
recent advancements in Neural Algorithmic Reasoning to learn efficient
heuristic functions for path finding problems on graphs. At training time, we
exploit multi-task learning to learn jointly the Dijkstra's algorithm and a
consistent heuristic function for the A* search algorithm. At inference time,
we plug our learnt heuristics into the A* algorithm. Results show that running
A* over the learnt heuristics value can greatly speed up target node searching
compared to Dijkstra, while still finding minimal-cost paths."
Google DeepMind,From eye-blinks to state construction: diagnostic benchmarks for online representation learning,,https://arxiv.org/pdf/2011.04590,,https://arxiv.org/pdf/2011.04590,"We present three new diagnostic prediction problems inspired by
classical-conditioning experiments to facilitate research in online prediction
learning. Experiments in classical conditioning show that animals such as
rabbits, pigeons, and dogs can make long temporal associations that enable
multi-step prediction. To replicate this remarkable ability, an agent must
construct an internal state representation that summarizes its interaction
history. Recurrent neural networks can automatically construct state and learn
temporal associations. However, the current training methods are prohibitively
expensive for online prediction -- continual learning on every time step --
which is the focus of this paper. Our proposed problems test the learning
capabilities that animals readily exhibit and highlight the limitations of the
current recurrent learning methods. While the proposed problems are nontrivial,
they are still amenable to extensive testing and analysis in the small-compute
regime, thereby enabling researchers to study issues in isolation, ultimately
accelerating progress towards scalable online representation learning methods."
Google DeepMind,Towards better visual explanations for deep image classifiers,,https://www.deepmind.com/research?d907cb24_page=7#,,https://www.deepmind.com/research?d907cb24_page=7#,
Google DeepMind,Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors,,https://arxiv.org/pdf/2203.17138,,https://arxiv.org/pdf/2203.17138,"We investigate the use of prior knowledge of human and animal movement to
learn reusable locomotion skills for real legged robots. Our approach builds
upon previous work on imitating human or dog Motion Capture (MoCap) data to
learn a movement skill module. Once learned, this skill module can be reused
for complex downstream tasks. Importantly, due to the prior imposed by the
MoCap data, our approach does not require extensive reward engineering to
produce sensible and natural looking behavior at the time of reuse. This makes
it easy to create well-regularized, task-oriented controllers that are suitable
for deployment on real robots. We demonstrate how our skill module can be used
for imitation, and train controllable walking and ball dribbling policies for
both the ANYmal quadruped and OP3 humanoid. These policies are then deployed on
hardware via zero-shot simulation-to-reality transfer. Accompanying videos are
available at https://bit.ly/robot-npmp."
Google DeepMind,Investigating the Properties of Neural Network Representations in Reinforcement Learning,,https://arxiv.org/pdf/2203.15955,,https://arxiv.org/pdf/2203.15955,"In this paper we investigate the properties of representations learned by
deep reinforcement learning systems. Much of the early work on representations
for reinforcement learning focused on designing fixed-basis architectures to
achieve properties thought to be desirable, such as orthogonality and sparsity.
In contrast, the idea behind deep reinforcement learning methods is that the
agent designer should not encode representational properties, but rather that
the data stream should determine the properties of the representation -- good
representations emerge under appropriate training schemes. In this paper we
bring these two perspectives together, empirically investigating the properties
of representations that support transfer in reinforcement learning. We
introduce and measure six representational properties over more than 25
thousand agent-task settings. We consider Deep Q-learning agents with different
auxiliary losses in a pixel-based navigation environment, with source and
transfer tasks corresponding to different goal locations. We develop a method
to better understand why some representations work better for transfer, through
a systematic approach varying task similarity and measuring and correlating
representation properties with transfer performance. We demonstrate the
generality of the methodology by investigating representations learned by a
Rainbow agent that successfully transfer across games modes in Atari 2600."
Google DeepMind,Graph Neural Networks are Dynamic Programmers,,https://openreview.net/forum?id=wu1Za9dY1GY,,https://openreview.net/forum?id=wu1Za9dY1GY,"Recent advances in neural algorithmic reasoning with graph neural networks (GNNs) are propped up by the notion of algorithmic alignment. Broadly, a neural network will be better at learning to execute a reasoning task (in terms of sample complexity) if its individual components align well with the target algorithm. Specifically, GNNs are claimed to align with dynamic programming (DP), a general problem-solving strategy which expresses many polynomial-time algorithms. However, has this alignment truly been demonstrated and theoretically quantified? Here we show, using methods from category theory and abstract algebra, that there exists an intricate connection between GNNs and DP, going well beyond the initial observations over individual algorithms such as Bellman-Ford. Exposing this connection, we easily verify several prior findings in the literature, produce better-grounded GNN architectures for edge-centric tasks, and demonstrate empirical results on the CLRS algorithmic reasoning benchmark. We hope our exposition will serve as a foundation for building stronger algorithmically aligned GNNs."
Google DeepMind,Marginalized operators for off-policy reinforcement learning,,https://arxiv.org/pdf/2203.16177,,https://arxiv.org/pdf/2203.16177,"In this work, we propose marginalized operators, a new class of off-policy
evaluation operators for reinforcement learning. Marginalized operators
strictly generalize generic multi-step operators, such as Retrace, as special
cases. Marginalized operators also suggest a form of sample-based estimates
with potential variance reduction, compared to sample-based estimates of the
original multi-step operators. We show that the estimates for marginalized
operators can be computed in a scalable way, which also generalizes prior
results on marginalized importance sampling as special cases. Finally, we
empirically demonstrate that marginalized operators provide performance gains
to off-policy evaluation and downstream policy optimization algorithms."
Google DeepMind,An adaptive and efficient multi-goal exploration,,https://arxiv.org/pdf/2111.12045,,https://arxiv.org/pdf/2111.12045,"We introduce a generic strategy for provably efficient multi-goal
exploration. It relies on AdaGoal, a novel goal selection scheme that leverages
a measure of uncertainty in reaching states to adaptively target goals that are
neither too difficult nor too easy. We show how AdaGoal can be used to tackle
the objective of learning an $\epsilon$-optimal goal-conditioned policy for the
(initially unknown) set of goal states that are reachable within $L$ steps in
expectation from a reference state $s_0$ in a reward-free Markov decision
process. In the tabular case with $S$ states and $A$ actions, our algorithm
requires $\tilde{O}(L^3 S A \epsilon^{-2})$ exploration steps, which is nearly
minimax optimal. We also readily instantiate AdaGoal in linear mixture Markov
decision processes, yielding the first goal-oriented PAC guarantee with linear
function approximation. Beyond its strong theoretical guarantees, we anchor
AdaGoal in goal-conditioned deep reinforcement learning, both conceptually and
empirically, by connecting its idea of selecting ""uncertain"" goals to
maximizing value ensemble disagreement."
Google DeepMind,Your Policy Regularizer is Secretly an Adversary,,https://arxiv.org/pdf/2203.12592,,https://arxiv.org/pdf/2203.12592,"Policy regularization methods such as maximum entropy regularization are
widely used in reinforcement learning to improve the robustness of a learned
policy. In this paper, we show how this robustness arises from hedging against
worst-case perturbations of the reward function, which are chosen from a
limited set by an imagined adversary. Using convex duality, we characterize
this robust set of adversarial reward perturbations under KL and
alpha-divergence regularization, which includes Shannon and Tsallis entropy
regularization as special cases. Importantly, generalization guarantees can be
given within this robust set. We provide detailed discussion of the worst-case
reward perturbations, and present intuitive empirical examples to illustrate
this robustness and its relationship with generalization. Finally, we discuss
how our analysis complements and extends previous results on adversarial reward
robustness and path consistency optimality conditions."
Google DeepMind,Scalable Deep Reinforcement Learning Algorithms for Mean Field Games,,https://arxiv.org/pdf/2203.11973,,https://arxiv.org/pdf/2203.11973,"Mean Field Games (MFGs) have been introduced to efficiently approximate games
with very large populations of strategic agents. Recently, the question of
learning equilibria in MFGs has gained momentum, particularly using model-free
reinforcement learning (RL) methods. One limiting factor to further scale up
using RL is that existing algorithms to solve MFGs require the mixing of
approximated quantities such as strategies or $q$-values. This is far from
being trivial in the case of non-linear function approximation that enjoy good
generalization properties, e.g. neural networks. We propose two methods to
address this shortcoming. The first one learns a mixed strategy from
distillation of historical data into a neural network and is applied to the
Fictitious Play algorithm. The second one is an online mixing method based on
regularization that does not require memorizing historical data or previous
estimates. It is used to extend Online Mirror Descent. We demonstrate
numerically that these methods efficiently enable the use of Deep RL algorithms
to solve various MFGs. In addition, we show that these methods outperform SotA
baselines from the literature."
Google DeepMind,Why Fair Labels Can Yield Unfair Predictions: Graphical Conditions for Introduced Unfairness,,https://arxiv.org/pdf/2202.10816,,https://arxiv.org/pdf/2202.10816,"In addition to reproducing discriminatory relationships in the training data,
machine learning systems can also introduce or amplify discriminatory effects.
We refer to this as introduced unfairness, and investigate the conditions under
which it may arise. To this end, we propose introduced total variation as a
measure of introduced unfairness, and establish graphical conditions under
which it may be incentivised to occur. These criteria imply that adding the
sensitive attribute as a feature removes the incentive for introduced variation
under well-behaved loss functions. Additionally, taking a causal perspective,
introduced path-specific effects shed light on the issue of when specific paths
should be considered fair."
Google DeepMind,Quantifying the effects of environment and population diversity in multi-agent reinforcement learning,,https://link.springer.com/content/pdf/10.1007/s10458-022-09548-8,,https://link.springer.com/content/pdf/10.1007/s10458-022-09548-8,
Google DeepMind,The Frost Hollow Experiments: Pavlovian Signalling as a Path to Coordination and Communication Between Agents,,https://arxiv.org/pdf/2203.09498,,https://arxiv.org/pdf/2203.09498,"Learned communication between agents is a powerful tool when approaching
decision-making problems that are hard to overcome by any single agent in
isolation. However, continual coordination and communication learning between
machine agents or human-machine partnerships remains a challenging open
problem. As a stepping stone toward solving the continual communication
learning problem, in this paper we contribute a multi-faceted study into what
we term Pavlovian signalling -- a process by which learned, temporally extended
predictions made by one agent inform decision-making by another agent with
different perceptual access to their shared environment. We seek to establish
how different temporal processes and representational choices impact Pavlovian
signalling between learning agents. To do so, we introduce a partially
observable decision-making domain we call the Frost Hollow. In this domain a
prediction learning agent and a reinforcement learning agent are coupled into a
two-part decision-making system that seeks to acquire sparse reward while
avoiding time-conditional hazards. We evaluate two domain variations: 1)
machine prediction and control learning in a linear walk, and 2) a prediction
learning machine interacting with a human participant in a virtual reality
environment. Our results showcase the speed of learning for Pavlovian
signalling, the impact that different temporal representations do (and do not)
have on agent-agent coordination, and how temporal aliasing impacts agent-agent
and human-agent interactions differently. As a main contribution, we establish
Pavlovian signalling as a natural bridge between fixed signalling paradigms and
fully adaptive communication learning. Our results therefore point to an
actionable, constructivist path towards continual communication learning
between reinforcement learning agents, with potential impact in a range of
real-world settings."
Google DeepMind,GopherCite: Teaching language models to support answers with verified quotes,,https://dpmd.ai/GopherCite-paper,,https://dpmd.ai/GopherCite-paper,
Google DeepMind,Zipfian Environments for Reinforcement Learning,,https://arxiv.org/abs/2203.08222,,https://arxiv.org/abs/2203.08222,"As humans and animals learn in the natural world, they encounter
distributions of entities, situations and events that are far from uniform.
Typically, a relatively small set of experiences are encountered frequently,
while many important experiences occur only rarely. The highly-skewed,
heavy-tailed nature of reality poses particular learning challenges that humans
and animals have met by evolving specialised memory systems. By contrast, most
popular RL environments and benchmarks involve approximately uniform variation
of properties, objects, situations or tasks. How will RL algorithms perform in
worlds (like ours) where the distribution of environment features is far less
uniform? To explore this question, we develop three complementary RL
environments where the agent's experience varies according to a Zipfian
(discrete power law) distribution. On these benchmarks, we find that standard
Deep RL architectures and algorithms acquire useful knowledge of common
situations and tasks, but fail to adequately learn about rarer ones. To
understand this failure better, we explore how different aspects of current
approaches may be adjusted to help improve performance on rare events, and show
that the RL objective function, the agent's memory system and self-supervised
learning objectives can all influence an agent's ability to learn from uncommon
experiences. Together, these results show that learning robustly from skewed
experience is a critical challenge for applying Deep RL methods beyond
simulations or laboratories, and our Zipfian environments provide a basis for
measuring future progress towards this goal."
Google DeepMind,Annealed Importance Sampling Meets Score Matching,,https://openreview.net/forum?id=H43MpnN_vZ9,,https://openreview.net/forum?id=H43MpnN_vZ9,"Annealed Importance Sampling (AIS) is one of the most effective methods for marginal likelihood estimation. It relies on a sequence of distributions interpolating between a tractable initial distribution and the posterior of interest which we simulate from approximately using a non-homogeneous Markov chain. To obtain an importance sampling (IS) estimate of the marginal likelihood, AIS introduces an extended target distribution to reweight the Markov chain proposal. While much effort has been devoted to improving the proposal distribution used by AIS by changing the intermediate distributions and corresponding Markov kernels, an underappreciated issue is that AIS uses an convenient but suboptimal extended target distribution which can hinder its performance. We leverage here recent progress in score-based generative modeling to learn the optimal extended target distribution for a given AIS proposal using score matching ideas. We demonstrate this novel differentiable AIS procedure on a number of synthetic benchmark distributions and a normalizing flow target."
Google DeepMind,Meta-Gradients in Nonstationary Environments,,https://openreview.net/forum?id=SlzBXwZIZ9,,https://openreview.net/forum?id=SlzBXwZIZ9,"Meta-gradient methods (Xu et al., 2018; Zahavy et al., 2020) are a promising approach to the problem of adaptation of hyper-parameters in non-stationary reinforcement learning problems. Recent works enable meta-gradients to adapt faster and learn from experience, by replacing the tuned meta-parameters of fixed update rules with learned meta-parameter functions of selected context features (Almeida et al., 2021; Flennerhag et al., 2022). We refer to these methods as contextual meta-gradients. The context features carry information about agent performance and changes in the environment and hence can inform learned meta-parameter schedules. As the properties of meta-gradient methods in non-stationary environments have not been systematically studied, the aim of this work is to provide such an analysis. Concretely, we ask: (i) how much information should be given to the learned optimizers so as to enable faster adaptation and generalization over a lifetime, (ii) what meta-optimizer functions are learned in this process, and (iii) whether meta-gradient methods provide a bigger advantage in highly non-stationary environments. We find that adding more contextual information is generally beneficial, leading to faster adaptation of meta-parameter values and increased performance. We support these results with a qualitative analysis of resulting meta-parameter schedules and learned functions of context features. Lastly, we find that without context, meta-gradients do not provide a consistent advantage over the baseline in highly non-stationary environments. Our findings suggest that contextualising meta-gradients can play a pivotal role in extracting high performance from meta-gradients in non-stationary settings."
Google DeepMind,Reconstructing Training Data with Informed Adversaries: Attacks and Mitigations,,https://arxiv.org/pdf/2201.04845,,https://arxiv.org/pdf/2201.04845,"Given access to a machine learning model, can an adversary reconstruct the
model's training data? This work studies this question from the lens of a
powerful informed adversary who knows all the training data points except one.
By instantiating concrete attacks, we show it is feasible to reconstruct the
remaining data point in this stringent threat model. For convex models (e.g.
logistic regression), reconstruction attacks are simple and can be derived in
closed-form. For more general models (e.g. neural networks), we propose an
attack strategy based on training a reconstructor network that receives as
input the weights of the model under attack and produces as output the target
data point. We demonstrate the effectiveness of our attack on image classifiers
trained on MNIST and CIFAR-10, and systematically investigate which factors of
standard machine learning pipelines affect reconstruction success. Finally, we
theoretically investigate what amount of differential privacy suffices to
mitigate reconstruction attacks by informed adversaries. Our work provides an
effective reconstruction attack that model developers can use to assess
memorization of individual points in general settings beyond those considered
in previous works (e.g. generative language models or access to training
gradients); it shows that standard models have the capacity to store enough
information to enable high-fidelity reconstruction of training data points; and
it demonstrates that differential privacy can successfully mitigate such
attacks in a parameter regime where utility degradation is minimal."
Google DeepMind,Learning Robust Real-Time Cultural Transmission without Human Data,,https://arxiv.org/pdf/2203.00715,,https://arxiv.org/pdf/2203.00715,"Cultural transmission is the domain-general social skill that allows agents
to acquire and use information from each other in real-time with high fidelity
and recall. In humans, it is the inheritance process that powers cumulative
cultural evolution, expanding our skills, tools and knowledge across
generations. We provide a method for generating zero-shot, high recall cultural
transmission in artificially intelligent agents. Our agents succeed at
real-time cultural transmission from humans in novel contexts without using any
pre-collected human data. We identify a surprisingly simple set of ingredients
sufficient for generating cultural transmission and develop an evaluation
methodology for rigorously assessing it. This paves the way for cultural
evolution as an algorithm for developing artificial general intelligence."
Google DeepMind,The Good Shepherd: An Oracle Agent for Mechanism Design,,https://openreview.net/forum?id=HGNLs51peq,,https://openreview.net/forum?id=HGNLs51peq,"From social networks to traffic routing, artificial learning agents are playing a central role in modern institutions. We must therefore understand how to leverage these systems to foster outcomes and behaviors that align with our own values and aspirations. While multiagent learning has received considerable attention in recent years, artificial agents have been primarily evaluated when interacting with fixed, non-learning co-players. While this evaluation scheme has merit, it fails to capture the dynamics faced by institutions that must deal with adaptive and continually learning constituents. Here we address this limitation, and construct agents (``mechanisms'') that perform well when evaluated over the learning trajectory of their adaptive co-players (``participants''). The algorithm we propose consists of two nested learning loops: an inner loop where participants learn to best respond to fixed mechanisms; and an outer loop where the mechanism agent updates its policy based on experience. We report the performance of our mechanism agents when paired with both artificial learning agents and humans as co-players. Our results show that our mechanisms are able to shepherd the participants strategies towards favorable outcomes, indicating a path for modern institutions to effectively and automatically influence the strategies and behaviors of their constituents."
Google DeepMind,HCMD-zero: Learning Value-Aligned Mechanisms from Data,,https://openreview.net/forum?id=rZBLs51ax9,,https://openreview.net/forum?id=rZBLs51ax9,"Artificial learning agents are mediating a larger and larger number of interactions among humans, firms, and organizations, and the intersection between mechanism design and machine learning has been heavily investigated in recent years. However, mechanism design methods make strong assumptions on how participants behave (e.g. rationality), or on the kind of knowledge designers have access to a priori (e.g. access to strong baseline mechanisms). Here we introduce HCMD-zero, a general purpose method to construct mechanism agents. HCMD-zero learns by mediating interactions among participants, while remaining engaged in an electoral contest with copies of itself, thereby accessing direct feedback from participants. Our results on the Public Investment Game, a stylized resource allocation game that highlights the tension between productivity, equality and the temptation to free-ride, show that HCMD-zero produces competitive mechanism agents that are consistently preferred by human participants over baseline alternatives, and does so automatically, without requiring human knowledge, and by using human data sparingly and effectively Our detailed analysis shows HCMD-zero elicits consistent improvements over the course of training, and that it results in a mechanism with an interpretable and intuitive policy."
Google DeepMind,Learning Equilibria in Mean-Field Games: Introducing Mean-Field PSRO,,https://arxiv.org/pdf/2111.08350,,https://arxiv.org/pdf/2111.08350,"Recent advances in multiagent learning have seen the introduction ofa family
of algorithms that revolve around the population-based trainingmethod PSRO,
showing convergence to Nash, correlated and coarse corre-lated equilibria.
Notably, when the number of agents increases, learningbest-responses becomes
exponentially more difficult, and as such ham-pers PSRO training methods. The
paradigm of mean-field games pro-vides an asymptotic solution to this problem
when the considered gamesare anonymous-symmetric. Unfortunately, the mean-field
approximationintroduces non-linearities which prevent a straightforward
adaptation ofPSRO. Building upon optimization and adversarial regret
minimization,this paper sidesteps this issue and introduces mean-field PSRO, an
adap-tation of PSRO which learns Nash, coarse correlated and correlated
equi-libria in mean-field games. The key is to replace the exact
distributioncomputation step by newly-defined mean-field no-adversarial-regret
learn-ers, or by black-box optimization. We compare the asymptotic complexityof
the approach to standard PSRO, greatly improve empirical bandit con-vergence
speed by compressing temporal mixture weights, and ensure itis theoretically
robust to payoff noise. Finally, we illustrate the speed andaccuracy of
mean-field PSRO on several mean-field games, demonstratingconvergence to strong
and weak equilibria."
Google DeepMind,Probing Image-Language Transformers for Verb Understanding,,https://arxiv.org/pdf/2106.09141,,https://arxiv.org/pdf/2106.09141,"Multimodal image-language transformers have achieved impressive results on a
variety of tasks that rely on fine-tuning (e.g., visual question answering and
image retrieval). We are interested in shedding light on the quality of their
pretrained representations -- in particular, if these models can distinguish
different types of verbs or if they rely solely on nouns in a given sentence.
To do so, we collect a dataset of image-sentence pairs (in English) consisting
of 421 verbs that are either visual or commonly found in the pretraining data
(i.e., the Conceptual Captions dataset). We use this dataset to evaluate
pretrained image-language transformers and find that they fail more in
situations that require verb understanding compared to other parts of speech.
We also investigate what category of verbs are particularly challenging."
Google DeepMind,Message passing all the way up,,https://arxiv.org/pdf/2202.11097,,https://arxiv.org/pdf/2202.11097,"The message passing framework is the foundation of the immense success
enjoyed by graph neural networks (GNNs) in recent years. In spite of its
elegance, there exist many problems it provably cannot solve over given input
graphs. This has led to a surge of research on going ""beyond message passing"",
building GNNs which do not suffer from those limitations -- a term which has
become ubiquitous in regular discourse. However, have those methods truly moved
beyond message passing? In this position paper, I argue about the dangers of
using this term -- especially when teaching graph representation learning to
newcomers. I show that any function of interest we want to compute over graphs
can, in all likelihood, be expressed using pairwise message passing -- just
over a potentially modified graph, and argue how most practical implementations
subtly do this kind of trick anyway. Hoping to initiate a productive
discussion, I propose replacing ""beyond message passing"" with a more tame term,
""augmented message passing""."
Google DeepMind,Magnetic Control of Tokamak Plasmas Through Deep Reinforcement Learning,,https://www.deepmind.com/research?d907cb24_page=8#,,https://www.deepmind.com/research?d907cb24_page=8#,
Google DeepMind,Retrieval-Augmented Reinforcement Learning,,https://arxiv.org/pdf/2202.08417,,https://arxiv.org/pdf/2202.08417,"Most deep reinforcement learning (RL) algorithms distill experience into
parametric behavior policies or value functions via gradient updates. While
effective, this approach has several disadvantages: (1) it is computationally
expensive, (2) it can take many updates to integrate experiences into the
parametric model, (3) experiences that are not fully integrated do not
appropriately influence the agent's behavior, and (4) behavior is limited by
the capacity of the model. In this paper we explore an alternative paradigm in
which we train a network to map a dataset of past experiences to optimal
behavior. Specifically, we augment an RL agent with a retrieval process
(parameterized as a neural network) that has direct access to a dataset of
experiences. This dataset can come from the agent's past experiences, expert
demonstrations, or any other relevant source. The retrieval process is trained
to retrieve information from the dataset that may be useful in the current
context, to help the agent achieve its goal faster and more efficiently. he
proposed method facilitates learning agents that at test-time can condition
their behavior on the entire dataset and not only the current state, or current
trajectory. We integrate our method into two different RL agents: an offline
DQN agent and an online R2D2 agent. In offline multi-task problems, we show
that the retrieval-augmented DQN agent avoids task interference and learns
faster than the baseline DQN agent. On Atari, we show that retrieval-augmented
R2D2 learns significantly faster than the baseline R2D2 agent and achieves
higher scores. We run extensive ablations to measure the contributions of the
components of our proposed method."
Google DeepMind,A data-driven approach for learning to control computers,,https://arxiv.org/pdf/2202.08137,,https://arxiv.org/pdf/2202.08137,"It would be useful for machines to use computers as humans do so that they
can aid us in everyday tasks. This is a setting in which there is also the
potential to leverage large-scale expert demonstrations and human judgements of
interactive behaviour, which are two ingredients that have driven much recent
success in AI. Here we investigate the setting of computer control using
keyboard and mouse, with goals specified via natural language. Instead of
focusing on hand-designed curricula and specialized action spaces, we focus on
developing a scalable method centered on reinforcement learning combined with
behavioural priors informed by actual human-computer interactions. We achieve
state-of-the-art and human-level mean performance across all tasks within the
MiniWob++ benchmark, a challenging suite of computer control problems, and find
strong evidence of cross-task transfer. These results demonstrate the
usefulness of a unified human-agent interface when training machines to use
computers. Altogether our results suggest a formula for achieving competency
beyond MiniWob++ and towards controlling computers, in general, as a human
would."
Google DeepMind,Large-scale representation learning on graphs via bootstrapping,,https://arxiv.org/abs/2102.06514,,https://arxiv.org/abs/2102.06514,"Self-supervised learning provides a promising path towards eliminating the
need for costly label information in representation learning on graphs.
However, to achieve state-of-the-art performance, methods often need large
numbers of negative examples and rely on complex augmentations. This can be
prohibitively expensive, especially for large graphs. To address these
challenges, we introduce Bootstrapped Graph Latents (BGRL) - a graph
representation learning method that learns by predicting alternative
augmentations of the input. BGRL uses only simple augmentations and alleviates
the need for contrasting with negative examples, and is thus scalable by
design. BGRL outperforms or matches prior methods on several established
benchmarks, while achieving a 2-10x reduction in memory costs. Furthermore, we
show that BGRL can be scaled up to extremely large graphs with hundreds of
millions of nodes in the semi-supervised regime - achieving state-of-the-art
performance and improving over supervised baselines where representations are
shaped only through label information. In particular, our solution centered on
BGRL constituted one of the winning entries to the Open Graph Benchmark - Large
Scale Challenge at KDD Cup 2021, on a graph orders of magnitudes larger than
all previously available benchmarks, thus demonstrating the scalability and
effectiveness of our approach."
Google DeepMind,https://arxiv.org/abs/2202.05963,,https://arxiv.org/pdf/2202.05963,,https://arxiv.org/pdf/2202.05963,"Adaptive optimization methods have become the default solvers for many
machine learning tasks. Unfortunately, the benefits of adaptivity may degrade
when training with differential privacy, as the noise added to ensure privacy
reduces the effectiveness of the adaptive preconditioner. To this end, we
propose AdaDPS, a general framework that uses non-sensitive side information to
precondition the gradients, allowing the effective use of adaptive methods in
private settings. We formally show AdaDPS reduces the amount of noise needed to
achieve similar privacy guarantees, thereby improving optimization performance.
Empirically, we leverage simple and readily available side information to
explore the performance of AdaDPS in practice, comparing to strong baselines in
both centralized and federated settings. Our results show that AdaDPS improves
accuracy by 7.7% (absolute) on average -- yielding state-of-the-art
privacy-utility trade-offs on large-scale text and image benchmarks."
Google DeepMind,Abstraction for Deep Reinforcement Learning,,https://arxiv.org/pdf/2202.05839,,https://arxiv.org/pdf/2202.05839,"We characterise the problem of abstraction in the context of deep
reinforcement learning. Various well established approaches to analogical
reasoning and associative memory might be brought to bear on this issue, but
they present difficulties because of the need for end-to-end differentiability.
We review developments in AI and machine learning that could facilitate their
adoption."
Google DeepMind,"General-purpose, long-context autoregressive modeling with Perceiver AR",,https://arxiv.org/abs/2202.07765,,https://arxiv.org/abs/2202.07765,"Real-world data is high-dimensional: a book, image, or musical performance
can easily contain hundreds of thousands of elements even after compression.
However, the most commonly used autoregressive models, Transformers, are
prohibitively expensive to scale to the number of inputs and layers needed to
capture this long-range structure. We develop Perceiver AR, an autoregressive,
modality-agnostic architecture which uses cross-attention to map long-range
inputs to a small number of latents while also maintaining end-to-end causal
masking. Perceiver AR can directly attend to over a hundred thousand tokens,
enabling practical long-context density estimation without the need for
hand-crafted sparsity patterns or memory mechanisms. When trained on images or
music, Perceiver AR generates outputs with clear long-term coherence and
structure. Our architecture also obtains state-of-the-art likelihood on
long-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books."
Google DeepMind,Competition-Level Code Generation using Deep Language Models,,https://arxiv.org/pdf/2203.07814,,https://arxiv.org/pdf/2203.07814,"Programming is a powerful and ubiquitous problem-solving tool. Developing
systems that can assist programmers or even generate programs independently
could make programming more productive and accessible, yet so far incorporating
innovations in AI has proven challenging. Recent large-scale language models
have demonstrated an impressive ability to generate code, and are now able to
complete simple programming tasks. However, these models still perform poorly
when evaluated on more complex, unseen problems that require problem-solving
skills beyond simply translating instructions into code. For example,
competitive programming problems which require an understanding of algorithms
and complex natural language remain extremely challenging. To address this gap,
we introduce AlphaCode, a system for code generation that can create novel
solutions to these problems that require deeper reasoning. In simulated
evaluations on recent programming competitions on the Codeforces platform,
AlphaCode achieved on average a ranking of top 54.3% in competitions with more
than 5,000 participants. We found that three key components were critical to
achieve good and reliable performance: (1) an extensive and clean competitive
programming dataset for training and evaluation, (2) large and
efficient-to-sample transformer-based architectures, and (3) large-scale model
sampling to explore the search space, followed by filtering based on program
behavior to a small set of submissions."
Google DeepMind,Sample-based Approximation of Nash in Large Many-Player Games via Gradient Descent,,https://arxiv.org/pdf/2106.01285,,https://arxiv.org/pdf/2106.01285,"Nash equilibrium is a central concept in game theory. Several Nash solvers
exist, yet none scale to normal-form games with many actions and many players,
especially those with payoff tensors too big to be stored in memory. In this
work, we propose an approach that iteratively improves an approximation to a
Nash equilibrium through joint play. It accomplishes this by tracing a
previously established homotopy that defines a continuum of equilibria for the
game regularized with decaying levels of entropy. This continuum asymptotically
approaches the limiting logit equilibrium, proven by McKelvey and Palfrey
(1995) to be unique in almost all games, thereby partially circumventing the
well-known equilibrium selection problem of many-player games. To encourage
iterates to remain near this path, we efficiently minimize average deviation
incentive via stochastic gradient descent, intelligently sampling entries in
the payoff tensor as needed. Monte Carlo estimates of the stochastic gradient
from joint play are biased due to the appearance of a nonlinear max operator in
the objective, so we introduce additional innovations to the algorithm to
alleviate gradient bias. The descent process can also be viewed as repeatedly
constructing and reacting to a polymatrix approximation to the game. In these
ways, our proposed approach, average deviation incentive descent with adaptive
sampling (ADIDAS), is most similar to three classical approaches, namely
homotopy-type, Lyapunov, and iterative polymatrix solvers. The lack of local
convergence guarantees for biased gradient descent prevents guaranteed
convergence to Nash, however, we demonstrate through extensive experiments the
ability of this approach to approximate a unique Nash in normal-form games with
as many as seven players and twenty one actions (several billion outcomes) that
are orders of magnitude larger than those possible with prior algorithms."
Google DeepMind,Deep Hierarchy in Bandits,,https://arxiv.org/pdf/2202.01454,,https://arxiv.org/pdf/2202.01454,"Mean rewards of actions are often correlated. The form of these correlations
may be complex and unknown a priori, such as the preferences of a user for
recommended products and their categories. To maximize statistical efficiency,
it is important to leverage these correlations when learning. We formulate a
bandit variant of this problem where the correlations of mean action rewards
are represented by a hierarchical Bayesian model with latent variables. Since
the hierarchy can have multiple layers, we call it deep. We propose a
hierarchical Thompson sampling algorithm (HierTS) for this problem, and show
how to implement it efficiently for Gaussian hierarchies. The efficient
implementation is possible due to a novel exact hierarchical representation of
the posterior, which itself is of independent interest. We use this exact
posterior to analyze the Bayes regret of HierTS in Gaussian bandits. Our
analysis reflects the structure of the problem, that the regret decreases with
the prior width, and also shows that hierarchies reduce the regret by
non-constant factors in the number of actions. We confirm these theoretical
findings empirically, in both synthetic and real-world experiments."
Google DeepMind,Unified Scaling Laws for Routed Language Models,,https://arxiv.org/pdf/2202.01169,,https://arxiv.org/pdf/2202.01169,"The performance of a language model has been shown to be effectively modeled
as a power-law in its parameter count. Here we study the scaling behaviors of
Routing Networks: architectures that conditionally use only a subset of their
parameters while processing an input. For these models, parameter count and
computational requirement form two independent axes along which an increase
leads to better performance. In this work we derive and justify scaling laws
defined on these two variables which generalize those known for standard
language models and describe the performance of a wide range of routing
architectures trained via three different techniques. Afterwards we provide two
applications of these laws: first deriving an Effective Parameter Count along
which all models scale at the same rate, and then using the scaling
coefficients to give a quantitative comparison of the three routing techniques
considered. Our analysis derives from an extensive evaluation of Routing
Networks across five orders of magnitude of size, including models with
hundreds of experts and hundreds of billions of parameters."
Google DeepMind,Online Apprenticeship Learning,,https://arxiv.org/abs/2102.06924,,https://arxiv.org/abs/2102.06924,"In Apprenticeship Learning (AL), we are given a Markov Decision Process (MDP)
without access to the cost function. Instead, we observe trajectories sampled
by an expert that acts according to some policy. The goal is to find a policy
that matches the expert's performance on some predefined set of cost functions.
We introduce an online variant of AL (Online Apprenticeship Learning; OAL),
where the agent is expected to perform comparably to the expert while
interacting with the environment. We show that the OAL problem can be
effectively solved by combining two mirror descent based no-regret algorithms:
one for policy optimization and another for learning the worst case cost. By
employing optimistic exploration, we derive a convergent algorithm with
$O(\sqrt{K})$ regret, where $K$ is the number of interactions with the MDP, and
an additional linear error term that depends on the amount of expert
trajectories available. Importantly, our algorithm avoids the need to solve an
MDP at each iteration, making it more practical compared to prior AL methods.
Finally, we implement a deep variant of our algorithm which shares some
similarities to GAIL \cite{ho2016generative}, but where the discriminator is
replaced with the costs learned by the OAL problem. Our simulations suggest
that OAL performs well in high dimensional control problems."
Google DeepMind,Physical Design using Differentiable Learned Simulators,,https://arxiv.org/pdf/2202.00728,,https://arxiv.org/pdf/2202.00728,"Designing physical artifacts that serve a purpose - such as tools and other
functional structures - is central to engineering as well as everyday human
behavior. Though automating design has tremendous promise, general-purpose
methods do not yet exist. Here we explore a simple, fast, and robust approach
to inverse design which combines learned forward simulators based on graph
neural networks with gradient-based design optimization. Our approach solves
high-dimensional problems with complex physical dynamics, including designing
surfaces and tools to manipulate fluid flows and optimizing the shape of an
airfoil to minimize drag. This framework produces high-quality designs by
propagating gradients through trajectories of hundreds of steps, even when
using models that were pre-trained for single-step predictions on data
substantially different from the design tasks. In our fluid manipulation tasks,
the resulting designs outperformed those found by sampling-based optimization
techniques. In airfoil design, they matched the quality of those obtained with
a specialized solver. Our results suggest that despite some remaining
challenges, machine learning-based simulators are maturing to the point where
they can support general-purpose design optimization across a variety of
domains."
Google DeepMind,Continual Repeated Annealed Flow Transport Monte Carlo,,https://arxiv.org/pdf/2201.13117,,https://arxiv.org/pdf/2201.13117,"We propose Continual Repeated Annealed Flow Transport Monte Carlo (CRAFT), a
method that combines a sequential Monte Carlo (SMC) sampler (itself a
generalization of Annealed Importance Sampling) with variational inference
using normalizing flows. The normalizing flows are directly trained to
transport between annealing temperatures using a KL divergence for each
transition. This optimization objective is itself estimated using the
normalizing flow/SMC approximation. We show conceptually and using multiple
empirical examples that CRAFT improves on Annealed Flow Transport Monte Carlo
(Arbel et al., 2021), on which it builds and also on Markov chain Monte Carlo
(MCMC) based Stochastic Normalizing Flows (Wu et al., 2020). By incorporating
CRAFT within particle MCMC, we show that such learnt samplers can achieve
impressively accurate results on a challenging lattice field theory example."
Google DeepMind,Adversarial Masking for Self-Supervised Learning,,https://arxiv.org/pdf/2201.13100,,https://arxiv.org/pdf/2201.13100,"We propose ADIOS, a masked image model (MIM) framework for self-supervised
learning, which simultaneously learns a masking function and an image encoder
using an adversarial objective. The image encoder is trained to minimise the
distance between representations of the original and that of a masked image.
The masking function, conversely, aims at maximising this distance. ADIOS
consistently improves on state-of-the-art self-supervised learning (SSL)
methods on a variety of tasks and datasets -- including classification on
ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and
iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao
et al., 2021) -- while generating semantically meaningful masks. Unlike modern
MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch
tokenisation construction of Vision Transformers, and can be implemented with
convolutional backbones. We further demonstrate that the masks learned by ADIOS
are more effective in improving representation learning of SSL methods than
masking schemes used in popular MIM models. Code is available at
https://github.com/YugeTen/adios."
Google DeepMind,A Context-Integrated Transformer-Based Neural Network for Auction Design,,https://arxiv.org/pdf/2201.12489,,https://arxiv.org/pdf/2201.12489,"One of the central problems in auction design is developing an
incentive-compatible mechanism that maximizes the auctioneer's expected
revenue. While theoretical approaches have encountered bottlenecks in
multi-item auctions, recently, there has been much progress on finding the
optimal mechanism through deep learning. However, these works either focus on a
fixed set of bidders and items, or restrict the auction to be symmetric. In
this work, we overcome such limitations by factoring \emph{public} contextual
information of bidders and items into the auction learning framework. We
propose $\mathtt{CITransNet}$, a context-integrated transformer-based neural
network for optimal auction design, which maintains permutation-equivariance
over bids and contexts while being able to find asymmetric solutions. We show
by extensive experiments that $\mathtt{CITransNet}$ can recover the known
optimal solutions in single-item settings, outperform strong baselines in
multi-item auctions, and generalize well to cases other than those in training."
Google DeepMind,Learning more skills through optimistic exploration,,https://openreview.net/forum?id=cU8rknuhxc,,https://openreview.net/forum?id=cU8rknuhxc,"Unsupervised skill learning objectives (Eysenbach et al., 2019; Gregor et al., 2016) allow agents to learn rich repertoires of behavior in the absence of extrinsic rewards. They work by simultaneously training a policy to produce distinguishable latent-conditioned trajectories, and a discriminator to evaluate distinguishability by trying to infer latents from trajectories. The hope is for the agent to explore and master the environment by encouraging each skill (latent) to reliably reach different states. However, an inherent exploration problem lingers: when a novel state is actually encountered, the discriminator will necessarily not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, we derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. Our objective directly estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples, thus providing an intrinsic reward more tailored to the true objective compared to pseudocount-based methods (Burda et al., 2019). We call this exploration bonus discriminator disagreement intrinsic reward, or DISDAIN. We demonstrate empirically that DISDAIN improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we encourage researchers to treat pessimism with DISDAIN."
Google DeepMind,From Data to Functa: Your data point is a function and you should treat it like one,,https://arxiv.org/pdf/2201.12204,,https://arxiv.org/pdf/2201.12204,"It is common practice in deep learning to represent a measurement of the
world on a discrete grid, e.g. a 2D grid of pixels. However, the underlying
signal represented by these measurements is often continuous, e.g. the scene
depicted in an image. A powerful continuous alternative is then to represent
these measurements using an implicit neural representation, a neural function
trained to output the appropriate measurement value for any input spatial
location. In this paper, we take this idea to its next level: what would it
take to perform deep learning on these functions instead, treating them as
data? In this context we refer to the data as functa, and propose a framework
for deep learning on functa. This view presents a number of challenges around
efficient conversion from data to functa, compact representation of functa, and
effectively solving downstream tasks on functa. We outline a recipe to overcome
these challenges and apply it to a wide range of data modalities including
images, 3D shapes, neural radiance fields (NeRF) and data on manifolds. We
demonstrate that this approach has various compelling properties across data
modalities, in particular on the canonical tasks of generative modeling, data
imputation, novel view synthesis and classification. Code:
https://github.com/deepmind/functa"
Google DeepMind,NeuPL: Neural Population Learning,,https://openreview.net/forum?id=MIX3fJkl_1,,https://openreview.net/forum?id=MIX3fJkl_1,"Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each iteration needs truncating, resulting in under-trained good-responses populating the population; b) repeated learning of basic skills at each iteration is wasteful and becomes intractable in the presence of increasingly strong opponents. In this work, we propose Neural Population Learning (NeuPL) as a solution to both issues. NeuPL offers convergence guarantees to a population of best-responses under mild assumptions. By representing a population of policies within a single conditional model, NeuPL enables transfer learning across policies. Empirically, we show the generality, improved performance and efficiency of NeuPL across several test domains. Most interestingly, we show that novel strategies become more accessible, not less, as the neural population expands."
Google DeepMind,The Challenges of Exploration for Offline Reinforcement Learning,,https://arxiv.org/pdf/2201.11861,,https://arxiv.org/pdf/2201.11861,"Offline Reinforcement Learning (ORL) enablesus to separately study the two
interlinked processes of reinforcement learning: collecting informative
experience and inferring optimal behaviour. The second step has been widely
studied in the offline setting, but just as critical to data-efficient RL is
the collection of informative data. The task-agnostic setting for data
collection, where the task is not known a priori, is of particular interest due
to the possibility of collecting a single dataset and using it to solve several
downstream tasks as they arise. We investigate this setting via curiosity-based
intrinsic motivation, a family of exploration methods which encourage the agent
to explore those states or transitions it has not yet learned to model. With
Explore2Offline, we propose to evaluate the quality of collected data by
transferring the collected data and inferring policies with reward relabelling
and standard offline RL algorithms. We evaluate a wide variety of data
collection strategies, including a new exploration agent, Intrinsic Model
Predictive Control (IMPC), using this scheme and demonstrate their performance
on various tasks. We use this decoupled framework to strengthen intuitions
about exploration and the data prerequisites for effective offline RL."
Google DeepMind,Human-centred mechanism design with democratic AI research,,https://arxiv.org/pdf/2201.11441,,https://arxiv.org/pdf/2201.11441,"Building artificial intelligence (AI) that aligns with human values is an
unsolved problem. Here, we developed a human-in-the-loop research pipeline
called Democratic AI, in which reinforcement learning is used to design a
social mechanism that humans prefer by majority. A large group of humans played
an online investment game that involved deciding whether to keep a monetary
endowment or to share it with others for collective benefit. Shared revenue was
returned to players under two different redistribution mechanisms, one designed
by the AI and the other by humans. The AI discovered a mechanism that redressed
initial wealth imbalance, sanctioned free riders, and successfully won the
majority vote. By optimizing for human preferences, Democratic AI may be a
promising method for value-aligned policy innovation."
Google DeepMind,Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents,,https://www.deepmind.com/research?d907cb24_page=9#,,https://www.deepmind.com/research?d907cb24_page=9#,
Google DeepMind,Chaining Value Functions for Off-Policy Learning,,https://arxiv.org/pdf/2201.06468,,https://arxiv.org/pdf/2201.06468,"To accumulate knowledge and improve its policy of behaviour, a reinforcement
learning agent can learn `off-policy' about policies that differ from the
policy used to generate its experience. This is important to learn
counterfactuals, or because the experience was generated out of its own
control. However, off-policy learning is non-trivial, and standard
reinforcement-learning algorithms can be unstable and divergent.
  In this paper we discuss a novel family of off-policy prediction algorithms
which are convergent by construction. The idea is to first learn on-policy
about the data-generating behaviour, and then bootstrap an off-policy value
estimate on this on-policy estimate, thereby constructing a value estimate that
is partially off-policy. This process can be repeated to build a chain of value
functions, each time bootstrapping a new estimate on the previous estimate in
the chain. Each step in the chain is stable and hence the complete algorithm is
guaranteed to be stable. Under mild conditions this comes arbitrarily close to
the off-policy TD solution when we increase the length of the chain. Hence it
can compute the solution even in cases where off-policy TD diverges.
  We prove that the proposed scheme is convergent and corresponds to an
iterative decomposition of the inverse key matrix. Furthermore it can be
interpreted as estimating a novel objective -- that we call a `k-step
expedition' -- of following the target policy for finitely many steps before
continuing indefinitely with the behaviour policy. Empirically we evaluate the
idea on challenging MDPs such as Baird's counter example and observe favourable
results."
Google DeepMind,Pavlovian Signalling with General Value Functions in Agent-Agent Temporal Decision Making,,https://arxiv.org/pdf/2201.03709,,https://arxiv.org/pdf/2201.03709,"In this paper, we contribute a multi-faceted study into Pavlovian signalling
-- a process by which learned, temporally extended predictions made by one
agent inform decision-making by another agent. Signalling is intimately
connected to time and timing. In service of generating and receiving signals,
humans and other animals are known to represent time, determine time since past
events, predict the time until a future stimulus, and both recognize and
generate patterns that unfold in time. We investigate how different temporal
processes impact coordination and signalling between learning agents by
introducing a partially observable decision-making domain we call the Frost
Hollow. In this domain, a prediction learning agent and a reinforcement
learning agent are coupled into a two-part decision-making system that works to
acquire sparse reward while avoiding time-conditional hazards. We evaluate two
domain variations: machine agents interacting in a seven-state linear walk, and
human-machine interaction in a virtual-reality environment. Our results
showcase the speed of learning for Pavlovian signalling, the impact that
different temporal representations do (and do not) have on agent-agent
coordination, and how temporal aliasing impacts agent-agent and human-agent
interactions differently. As a main contribution, we establish Pavlovian
signalling as a natural bridge between fixed signalling paradigms and fully
adaptive communication learning between two agents. We further show how to
computationally build this adaptive signalling process out of a fixed
signalling process, characterized by fast continual prediction learning and
minimal constraints on the nature of the agent receiving signals. Our results
therefore suggest an actionable, constructivist path towards communication
learning between reinforcement learning agents."
Google DeepMind,Learning to be robust and private,,https://arxiv.org/pdf/2201.02265,,https://arxiv.org/pdf/2201.02265,"We study the difficulties in learning that arise from robust and
differentially private optimization. We first study convergence of gradient
descent based adversarial training with differential privacy, taking a simple
binary classification task on linearly separable data as an illustrative
example. We compare the gap between adversarial and nominal risk in both
private and non-private settings, showing that the data dimensionality
dependent term introduced by private optimization compounds the difficulties of
learning a robust model. After this, we discuss what parts of adversarial
training and differential privacy hurt optimization, identifying that the size
of adversarial perturbation and clipping norm in differential privacy both
increase the curvature of the loss landscape, implying poorer generalization
performance."
Google DeepMind,Reward-Punishment Symmetric Universal Intelligence,,https://link.springer.com/content/pdf/10.1007/978-3-030-93758-4,,https://link.springer.com/content/pdf/10.1007/978-3-030-93758-4,
Google DeepMind,Learning Optimal Conformal Classifiers,,https://arxiv.org/abs/2110.09192,,https://arxiv.org/abs/2110.09192,"Modern deep learning based classifiers show very high accuracy on test data
but this does not provide sufficient guarantees for safe deployment, especially
in high-stake AI applications such as medical diagnosis. Usually, predictions
are obtained without a reliable uncertainty estimate or a formal guarantee.
Conformal prediction (CP) addresses these issues by using the classifier's
predictions, e.g., its probability estimates, to predict confidence sets
containing the true class with a user-specified probability. However, using CP
as a separate processing step after training prevents the underlying model from
adapting to the prediction of confidence sets. Thus, this paper explores
strategies to differentiate through CP during training with the goal of
training model with the conformal wrapper end-to-end. In our approach,
conformal training (ConfTr), we specifically ""simulate"" conformalization on
mini-batches during training. Compared to standard training, ConfTr reduces the
average confidence set size (inefficiency) of state-of-the-art CP methods
applied after training. Moreover, it allows to ""shape"" the confidence sets
predicted at test time, which is difficult for standard CP. On experiments with
several datasets, we show ConfTr can influence how inefficiency is distributed
across classes, or guide the composition of confidence sets in terms of the
included classes, while retaining the guarantees offered by CP."
Google DeepMind,Feature and Parameter Selection in Stochastic Linear Bandits,,https://proceedings.mlr.press/v162/moradipari22a/moradipari22a,,https://proceedings.mlr.press/v162/moradipari22a/moradipari22a,
Google DeepMind,Learned coarse models for efficient turbulence simulation,,https://arxiv.org/pdf/2112.15275,,https://arxiv.org/pdf/2112.15275,"Turbulence simulation with classical numerical solvers requires
high-resolution grids to accurately resolve dynamics. Here we train learned
simulators at low spatial and temporal resolutions to capture turbulent
dynamics generated at high resolution. We show that our proposed model can
simulate turbulent dynamics more accurately than classical numerical solvers at
the comparably low resolutions across various scientifically relevant metrics.
Our model is trained end-to-end from data and is capable of learning a range of
challenging chaotic and turbulent dynamics at low resolution, including
trajectories generated by the state-of-the-art Athena++ engine. We show that
our simpler, general-purpose architecture outperforms various more specialized,
turbulence-specific architectures from the learned turbulence simulation
literature. In general, we see that learned simulators yield unstable
trajectories; however, we show that tuning training noise and temporal
downsampling solves this problem. We also find that while generalization beyond
the training distribution is a challenge for learned models, training noise,
added loss constraints, and dataset augmentation can help. Broadly, we conclude
that our learned simulator outperforms traditional solvers run on coarser
grids, and emphasize that simple design choices can offer stability and robust
generalization."
Google DeepMind,On the Role of Neural Collapse in Transfer Learning,,https://arxiv.org/pdf/2112.15121,,https://arxiv.org/pdf/2112.15121,"We study the ability of foundation models to learn representations for
classification that are transferable to new, unseen classes. Recent results in
the literature show that representations learned by a single classifier over
many classes are competitive on few-shot learning problems with representations
learned by special-purpose algorithms designed for such problems. In this paper
we provide an explanation for this behavior based on the recently observed
phenomenon that the features learned by overparameterized classification
networks show an interesting clustering property, called neural collapse. We
demonstrate both theoretically and empirically that neural collapse generalizes
to new samples from the training classes, and -- more importantly -- to new
classes as well, allowing foundation models to provide feature maps that work
well in transfer learning and, specifically, in the few-shot setting."
Google DeepMind,Isotuning with applications to scale-free online learning,,https://arxiv.org/pdf/2112.14586,,https://arxiv.org/pdf/2112.14586,"We extend and combine several tools of the literature to design fast,
adaptive, anytime and scale-free online learning algorithms. Scale-free regret
bounds must scale linearly with the maximum loss, both toward large losses and
toward very small losses. Adaptive regret bounds demonstrate that an algorithm
can take advantage of easy data and potentially have constant regret. We seek
to develop fast algorithms that depend on as few parameters as possible, in
particular they should be anytime and thus not depend on the time horizon. Our
first and main tool, isotuning, is a generalization of the idea of balancing
the trade-off of the regret. We develop a set of tools to design and analyze
such learning rates easily and show that they adapts automatically to the rate
of the regret (whether constant, $O(\log T)$, $O(\sqrt{T})$, etc.) within a
factor 2 of the optimal learning rate in hindsight for the same observed
quantities. The second tool is an online correction, which allows us to obtain
centered bounds for many algorithms, to prevent the regret bounds from being
vacuous when the domain is overly large or only partially constrained. The last
tool, null updates, prevents the algorithm from performing overly large
updates, which could result in unbounded regret, or even invalid updates. We
develop a general theory using these tools and apply it to several standard
algorithms. In particular, we (almost entirely) restore the adaptivity to small
losses of FTRL for unbounded domains, design and prove scale-free adaptive
guarantees for a variant of Mirror Descent (at least when the Bregman
divergence is convex in its second argument), extend Adapt-ML-Prod to
scale-free guarantees, and provide several other minor contributions about
Prod, AdaHedge, BOA and Soft-Bayes."
Google DeepMind,Proving Theorems using Incremental Learning and Hindsight Experience Replay,,https://arxiv.org/pdf/2112.10664,,https://arxiv.org/pdf/2112.10664,"Traditional automated theorem provers for first-order logic depend on
speed-optimized search and many handcrafted heuristics that are designed to
work best over a wide range of domains. Machine learning approaches in
literature either depend on these traditional provers to bootstrap themselves
or fall short on reaching comparable performance. In this paper, we propose a
general incremental learning algorithm for training domain specific provers for
first-order logic without equality, based only on a basic given-clause
algorithm, but using a learned clause-scoring function. Clauses are represented
as graphs and presented to transformer networks with spectral features. To
address the sparsity and the initial lack of training data as well as the lack
of a natural curriculum, we adapt hindsight experience replay to theorem
proving, so as to be able to learn even when no proof can be found. We show
that provers trained this way can match and sometimes surpass state-of-the-art
traditional provers on the TPTP dataset in terms of both quantity and quality
of the proofs."
Google DeepMind,Assessing Human Interaction in Virtual Reality with Continually Learning Prediction Agents Based on Reinforcement Learning Algorithms: A Pilot Study,,https://arxiv.org/pdf/2112.07774,,https://arxiv.org/pdf/2112.07774,"Artificial intelligence systems increasingly involve continual learning to
enable flexibility in general situations that are not encountered during system
training. Human interaction with autonomous systems is broadly studied, but
research has hitherto under-explored interactions that occur while the system
is actively learning, and can noticeably change its behaviour in minutes. In
this pilot study, we investigate how the interaction between a human and a
continually learning prediction agent develops as the agent develops
competency. Additionally, we compare two different agent architectures to
assess how representational choices in agent design affect the human-agent
interaction. We develop a virtual reality environment and a time-based
prediction task wherein learned predictions from a reinforcement learning (RL)
algorithm augment human predictions. We assess how a participant's performance
and behaviour in this task differs across agent types, using both quantitative
and qualitative analyses. Our findings suggest that human trust of the system
may be influenced by early interactions with the agent, and that trust in turn
affects strategic behaviour, but limitations of the pilot study rule out any
conclusive statement. We identify trust as a key feature of interaction to
focus on when considering RL-based technologies, and make several
recommendations for modification to this study in preparation for a
larger-scale investigation. A video summary of this paper can be found at
https://youtu.be/oVYJdnBqTwQ ."
Google DeepMind,Hidden Agenda,,https://drive.google.com/file/d/1vrD3W6joGmQ3ObsbVgEN-1e_360qpFa2/view,,https://drive.google.com/file/d/1vrD3W6joGmQ3ObsbVgEN-1e_360qpFa2/view,
Google DeepMind,Gaussian dropout as an information bottleneck layer,,https://web.archive.org/web/20230810094540/http://bayesiandeeplearning.org/2021/papers/40,,https://web.archive.org/web/20230810094540/http://bayesiandeeplearning.org/2021/papers/40,
Google DeepMind,Latent Keypoint Embeddings for Few-Shot Object-Part Detection,,https://arxiv.org/pdf/2112.04910,,https://arxiv.org/pdf/2112.04910,"Dense object tracking, the ability to localize specific object points with
pixel-level accuracy, is an important computer vision task with numerous
downstream applications in robotics. Existing approaches either compute dense
keypoint embeddings in a single forward pass, meaning the model is trained to
track everything at once, or allocate their full capacity to a sparse
predefined set of points, trading generality for accuracy. In this paper we
explore a middle ground based on the observation that the number of relevant
points at a given time are typically relatively few, e.g. grasp points on a
target object. Our main contribution is a novel architecture, inspired by
few-shot task adaptation, which allows a sparse-style network to condition on a
keypoint embedding that indicates which point to track. Our central finding is
that this approach provides the generality of dense-embedding models, while
offering accuracy significantly closer to sparse-keypoint approaches. We
present results illustrating this capacity vs. accuracy trade-off, and
demonstrate the ability to zero-shot transfer to new object instances
(within-class) using a real-robot pick-and-place task."
Google DeepMind,ParK: Sound and Efficient Kernel Ridge Regression by Feature Space Partitions,,https://proceedings.neurips.cc/paper/2021/file/32b9e74c8f60958158eba8d1fa372971-Paper,,https://proceedings.neurips.cc/paper/2021/file/32b9e74c8f60958158eba8d1fa372971-Paper,
Google DeepMind,Pushing the Frontiers of Density Functionals by Solving the Fractional Electron Problem,,https://storage.googleapis.com/deepmind-media/papers/Data_Driven_Density_Functional_Design/data_driven_density_functional_design_unformatted,,https://storage.googleapis.com/deepmind-media/papers/Data_Driven_Density_Functional_Design/data_driven_density_functional_design_unformatted,
Google DeepMind,Improving language models by retrieving from trillions of tokens,,https://arxiv.org/pdf/2112.04426,,https://arxiv.org/pdf/2112.04426,"We enhance auto-regressive language models by conditioning on document chunks
retrieved from a large corpus, based on local similarity with preceding tokens.
With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO)
obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite
using 25$\times$ fewer parameters. After fine-tuning, RETRO performance
translates to downstream knowledge-intensive tasks such as question answering.
RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked
cross-attention mechanism to predict tokens based on an order of magnitude more
data than what is typically consumed during training. We typically train RETRO
from scratch, yet can also rapidly RETROfit pre-trained transformers with
retrieval and still achieve good performance. Our work opens up new avenues for
improving language models through explicit memory at unprecedented scale."
Google DeepMind,Model-Value Self-Inconsistency as a Signal for Epistemic Uncertainty,,https://arxiv.org/pdf/2112.04153,,https://arxiv.org/pdf/2112.04153,"Using a model of the environment and a value function, an agent can construct
many estimates of a state's value, by unrolling the model for different lengths
and bootstrapping with its value function. Our key insight is that one can
treat this set of value estimates as a type of ensemble, which we call an
\emph{implicit value ensemble} (IVE). Consequently, the discrepancy between
these estimates can be used as a proxy for the agent's epistemic uncertainty;
we term this signal \emph{model-value inconsistency} or
\emph{self-inconsistency} for short. Unlike prior work which estimates
uncertainty by training an ensemble of many models and/or value functions, this
approach requires only the single model and value function which are already
being learned in most model-based reinforcement learning algorithms. We provide
empirical evidence in both tabular and function approximation settings from
pixels that self-inconsistency is useful (i) as a signal for exploration, (ii)
for acting safely under distribution shifts, and (iii) for robustifying
value-based planning with a learned model."
Google DeepMind,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",,https://www.deepmind.com/research?d907cb24_page=9#,,https://www.deepmind.com/research?d907cb24_page=9#,
Google DeepMind,Ethical and social risks of harm from Language Models,,https://arxiv.org/abs/2112.04359,,https://arxiv.org/abs/2112.04359,"This paper aims to help structure the risk landscape associated with
large-scale Language Models (LMs). In order to foster advances in responsible
innovation, an in-depth understanding of the potential risks posed by these
models is needed. A wide range of established and anticipated risks are
analysed in detail, drawing on multidisciplinary expertise and literature from
computer science, linguistics, and social sciences.
  We outline six specific risk areas: I. Discrimination, Exclusion and
Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious
Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and
Environmental Harms. The first area concerns the perpetuation of stereotypes,
unfair discrimination, exclusionary norms, toxic language, and lower
performance by social group for LMs. The second focuses on risks from private
data leaks or LMs correctly inferring sensitive information. The third
addresses risks arising from poor, false or misleading information including in
sensitive domains, and knock-on risks such as the erosion of trust in shared
information. The fourth considers risks from actors who try to use LMs to cause
harm. The fifth focuses on risks specific to LLMs used to underpin
conversational agents that interact with human users, including unsafe use,
manipulation or deception. The sixth discusses the risk of environmental harm,
job automation, and other challenges that may have a disparate effect on
different social groups or communities.
  In total, we review 21 risks in-depth. We discuss the points of origin of
different risks and point to potential mitigation approaches. Lastly, we
discuss organisational responsibilities in implementing mitigations, and the
role of collaboration and participation. We highlight directions for further
research, particularly on expanding the toolkit for assessing and evaluating
the outlined risks in LMs."
Google DeepMind,Creating Interactive Agents with Imitation Learning,,https://arxiv.org/abs/2112.03763,,https://arxiv.org/abs/2112.03763,"A common vision from science fiction is that robots will one day inhabit our
physical spaces, sense the world as we do, assist our physical labours, and
communicate with us through natural language. Here we study how to design
artificial agents that can interact naturally with humans using the
simplification of a virtual environment. We show that imitation learning of
human-human interactions in a simulated world, in conjunction with
self-supervised learning, is sufficient to produce a multimodal interactive
agent, which we call MIA, that successfully interacts with non-adversarial
humans 75% of the time. We further identify architectural and algorithmic
techniques that improve performance, such as hierarchical action selection.
Altogether, our results demonstrate that imitation of multi-modal, real-time
human behaviour may provide a straightforward and surprisingly effective means
of imbuing agents with a rich behavioural prior from which agents might then be
fine-tuned for specific purposes, thus laying a foundation for training capable
agents for interactive robots or digital assistants. A video of MIA's behaviour
may be found at https://youtu.be/ZFgRhviF7mY"
Google DeepMind,Automap: Towards Ergonomic Automated Parallelism for ML Models,,https://arxiv.org/pdf/2112.02958,,https://arxiv.org/pdf/2112.02958,"The rapid rise in demand for training large neural network architectures has
brought into focus the need for partitioning strategies, for example by using
data, model, or pipeline parallelism. Implementing these methods is
increasingly supported through program primitives, but identifying efficient
partitioning strategies requires expensive experimentation and expertise. We
present the prototype of an automated partitioner that seamlessly integrates
into existing compilers and existing user workflows. Our partitioner enables
SPMD-style parallelism that encompasses data parallelism and
parameter/activation sharding. Through a combination of inductive tactics and
search in a platform-independent partitioning IR, automap can recover expert
partitioning strategies such as Megatron sharding for transformer layers."
Google DeepMind,Collaborating with Humans without Human Data,,https://proceedings.neurips.cc/paper/2021/file/797134c3e42371bb4979a462eb2f042a-Paper,,https://proceedings.neurips.cc/paper/2021/file/797134c3e42371bb4979a462eb2f042a-Paper,
