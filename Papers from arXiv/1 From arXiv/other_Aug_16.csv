Title,Authors,Abstract,arXiv ID,PDF_Link,Search Term
"A Review of the Evidence for Existential Risk from AI via Misaligned
  Power-Seeking",Rose Hadshar,"Rapid advancements in artificial intelligence (AI) have sparked growing
concerns among experts, policymakers, and world leaders regarding the potential
for increasingly advanced AI systems to pose existential risks. This paper
reviews the evidence for existential risks from AI via misalignment, where AI
systems develop goals misaligned with human values, and power-seeking, where
misaligned AIs actively seek power. The review examines empirical findings,
conceptual arguments and expert opinion relating to specification gaming, goal
misgeneralization, and power-seeking. The current state of the evidence is
found to be concerning but inconclusive regarding the existence of extreme
forms of misaligned power-seeking. Strong empirical evidence of specification
gaming combined with strong conceptual evidence for power-seeking make it
difficult to dismiss the possibility of existential risk from misaligned
power-seeking. On the other hand, to date there are no public empirical
examples of misaligned power-seeking in AI systems, and so arguments that
future systems will pose an existential risk remain somewhat speculative. Given
the current state of the evidence, it is hard to be extremely confident either
that misaligned power-seeking poses a large existential risk, or that it poses
no existential risk. The fact that we cannot confidently rule out existential
risk from AI via misaligned power-seeking is cause for serious concern.",2310.18244v1,https://arxiv.org/pdf/2310.18244v1,Power seeking
Power-seeking can be probable and predictive for trained agents,"Victoria Krakovna, Janos Kramar","Power-seeking behavior is a key source of risk from advanced AI, but our
theoretical understanding of this phenomenon is relatively limited. Building on
existing theoretical results demonstrating power-seeking incentives for most
reward functions, we investigate how the training process affects power-seeking
incentives and show that they are still likely to hold for trained agents under
some simplifying assumptions. We formally define the training-compatible goal
set (the set of goals consistent with the training rewards) and assume that the
trained agent learns a goal from this set. In a setting where the trained agent
faces a choice to shut down or avoid shutdown in a new situation, we prove that
the agent is likely to avoid shutdown. Thus, we show that power-seeking
incentives can be probable (likely to arise for trained agents) and predictive
(allowing us to predict undesirable behavior in new situations).",2304.06528v1,https://arxiv.org/pdf/2304.06528v1,Power seeking
On Avoiding Power-Seeking by Artificial Intelligence,Alexander Matt Turner,"We do not know how to align a very intelligent AI agent's behavior with human
interests. I investigate whether -- absent a full solution to this AI alignment
problem -- we can build smart AI agents which have limited impact on the world,
and which do not autonomously seek power. In this thesis, I introduce the
attainable utility preservation (AUP) method. I demonstrate that AUP produces
conservative, option-preserving behavior within toy gridworlds and within
complex environments based off of Conway's Game of Life. I formalize the
problem of side effect avoidance, which provides a way to quantify the side
effects an agent had on the world. I also give a formal definition of
power-seeking in the context of AI agents and show that optimal policies tend
to seek power. In particular, most reward functions have optimal policies which
avoid deactivation. This is a problem if we want to deactivate or correct an
intelligent agent after we have deployed it. My theorems suggest that since
most agent goals conflict with ours, the agent would very probably resist
correction. I extend these theorems to show that power-seeking incentives occur
not just for optimal decision-makers, but under a wide range of decision-making
procedures.",2206.11831v1,https://arxiv.org/pdf/2206.11831v1,Power seeking
Is Power-Seeking AI an Existential Risk?,Joseph Carlsmith,"This report examines what I see as the core argument for concern about
existential risk from misaligned artificial intelligence. I proceed in two
stages. First, I lay out a backdrop picture that informs such concern. On this
picture, intelligent agency is an extremely powerful force, and creating agents
much more intelligent than us is playing with fire -- especially given that if
their objectives are problematic, such agents would plausibly have instrumental
incentives to seek power over humans. Second, I formulate and evaluate a more
specific six-premise argument that creating agents of this kind will lead to
existential catastrophe by 2070. On this argument, by 2070: (1) it will become
possible and financially feasible to build relevantly powerful and agentic AI
systems; (2) there will be strong incentives to do so; (3) it will be much
harder to build aligned (and relevantly powerful/agentic) AI systems than to
build misaligned (and relevantly powerful/agentic) AI systems that are still
superficially attractive to deploy; (4) some such misaligned systems will seek
power over humans in high-impact ways; (5) this problem will scale to the full
disempowerment of humanity; and (6) such disempowerment will constitute an
existential catastrophe. I assign rough subjective credences to the premises in
this argument, and I end up with an overall estimate of ~5% that an existential
catastrophe of this kind will occur by 2070. (May 2022 update: since making
this report public in April 2021, my estimate here has gone up, and is now at
>10%.)",2206.13353v2,https://arxiv.org/pdf/2206.13353v2,Power seeking
Parametrically Retargetable Decision-Makers Tend To Seek Power,"Alexander Matt Turner, Prasad Tadepalli","If capable AI agents are generally incentivized to seek power in service of
the objectives we specify for them, then these systems will pose enormous
risks, in addition to enormous benefits. In fully observable environments, most
reward functions have an optimal policy which seeks power by keeping options
open and staying alive. However, the real world is neither fully observable,
nor must trained agents be even approximately reward-optimal. We consider a
range of models of AI decision-making, from optimal, to random, to choices
informed by learning and interacting with an environment. We discover that many
decision-making functions are retargetable, and that retargetability is
sufficient to cause power-seeking tendencies. Our functional criterion is
simple and broad. We show that a range of qualitatively dissimilar
decision-making procedures incentivize agents to seek power. We demonstrate the
flexibility of our results by reasoning about learned policy incentives in
Montezuma's Revenge. These results suggest a safety risk: Eventually,
retargetable training procedures may train real-world agents which seek power
over humans.",2206.13477v2,https://arxiv.org/pdf/2206.13477v2,Seek power
"Multiagent Copilot Approach for Shared Autonomy between Human EEG and
  TD3 Deep Reinforcement Learning","Chun-Ren Phang, Akimasa Hirata","Deep reinforcement learning (RL) algorithms enable the development of fully
autonomous agents that can interact with the environment. Brain-computer
interface (BCI) systems decipher human implicit brain signals regardless of the
explicit environment. In this study, we integrated deep RL and BCI to improve
beneficial human interventions in autonomous systems and the performance in
decoding brain activities by considering environmental factors. Shared autonomy
was allowed between the action command decoded from the electroencephalography
(EEG) of the human agent and the action generated from the twin delayed DDPG
(TD3) agent for a given environment. Our proposed copilot control scheme with a
full blocker (Co-FB) significantly outperformed the individual EEG (EEG-NB) or
TD3 control. The Co-FB model achieved a higher target approaching score, lower
failure rate, and lower human workload than the EEG-NB model. The Co-FB control
scheme had a higher invisible target score and level of allowed human
intervention than the TD3 model. We also proposed a disparity d-index to
evaluate the effect of contradicting agent decisions on the control accuracy
and authority of the copilot model. We found a significant correlation between
the control authority of the TD3 agent and the performance improvement of human
EEG classification with respect to the d-index. We also observed that shifting
control authority to the TD3 agent improved performance when BCI decoding was
not optimal. These findings indicate that the copilot system can effectively
handle complex environments and that BCI performance can be improved by
considering environmental factors. Future work should employ continuous action
space and different multi-agent approaches to evaluate copilot performance.",2312.14458v1,https://arxiv.org/pdf/2312.14458v1,Multiagent
"Exact Algorithms and Lowerbounds for Multiagent Pathfinding: Power of
  Treelike Topology","Foivos Fioravantes, Dušan Knop, Jan Matyáš Křišťan, Nikolaos Melissinos, Michal Opler","In the Multiagent Path Finding problem (MAPF for short), we focus on
efficiently finding non-colliding paths for a set of $k$ agents on a given
graph $G$, where each agent seeks a path from its source vertex to a target. An
important measure of the quality of the solution is the length of the proposed
schedule $\ell$, that is, the length of a longest path (including the waiting
time). In this work, we propose a systematic study under the parameterized
complexity framework. The hardness results we provide align with many
heuristics used for this problem, whose running time could potentially be
improved based on our fixed-parameter tractability results.
  We show that MAPF is W[1]-hard with respect to $k$ (even if $k$ is combined
with the maximum degree of the input graph). The problem remains NP-hard in
planar graphs even if the maximum degree and the makespan$\ell$ are fixed
constants. On the positive side, we show an FPT algorithm for $k+\ell$.
  As we delve further, the structure of~$G$ comes into play. We give an FPT
algorithm for parameter $k$ plus the diameter of the graph~$G$. The MAPF
problem is W[1]-hard for cliquewidth of $G$ plus $\ell$ while it is FPT for
treewidth of $G$ plus $\ell$.",2312.09646v1,https://arxiv.org/pdf/2312.09646v1,Multiagent
"Networked Multiagent Safe Reinforcement Learning for Low-carbon Demand
  Management in Distribution Network","Jichen Zhang, Linwei Sang, Yinliang Xu, Hongbin Sun","This paper proposes a multiagent based bi-level operation framework for the
low-carbon demand management in distribution networks considering the carbon
emission allowance on the demand side. In the upper level, the aggregate load
agents optimize the control signals for various types of loads to maximize the
profits; in the lower level, the distribution network operator makes optimal
dispatching decisions to minimize the operational costs and calculates the
distribution locational marginal price and carbon intensity. The distributed
flexible load agent has only incomplete information of the distribution network
and cooperates with other agents using networked communication. Finally, the
problem is formulated into a networked multi-agent constrained Markov decision
process, which is solved using a safe reinforcement learning algorithm called
consensus multi-agent constrained policy optimization considering the carbon
emission allowance for each agent. Case studies with the IEEE 33-bus and
123-bus distribution network systems demonstrate the effectiveness of the
proposed approach, in terms of satisfying the carbon emission constraint on
demand side, ensuring the safe operation of the distribution network and
preserving privacy of both sides.",2311.15594v1,https://arxiv.org/pdf/2311.15594v1,Multiagent
"The NeurIPS 2022 Neural MMO Challenge: A Massively Multiagent
  Competition with Specialization and Trade","Enhong Liu, Joseph Suarez, Chenhui You, Bo Wu, Bingcheng Chen, Jun Hu, Jiaxin Chen, Xiaolong Zhu, Clare Zhu, Julian Togelius, Sharada Mohanty, Weijun Hong, Rui Du, Yibing Zhang, Qinwen Wang, Xinhang Li, Zheng Yuan, Xiang Li, Yuejia Huang, Kun Zhang, Hanhui Yang, Shiqi Tang, Phillip Isola","In this paper, we present the results of the NeurIPS-2022 Neural MMO
Challenge, which attracted 500 participants and received over 1,600
submissions. Like the previous IJCAI-2022 Neural MMO Challenge, it involved
agents from 16 populations surviving in procedurally generated worlds by
collecting resources and defeating opponents. This year's competition runs on
the latest v1.6 Neural MMO, which introduces new equipment, combat, trading,
and a better scoring system. These elements combine to pose additional
robustness and generalization challenges not present in previous competitions.
This paper summarizes the design and results of the challenge, explores the
potential of this environment as a benchmark for learning methods, and presents
some practical reinforcement learning training approaches for complex tasks
with sparse rewards. Additionally, we have open-sourced our baselines,
including environment wrappers, benchmarks, and visualization tools for future
research.",2311.03707v1,https://arxiv.org/pdf/2311.03707v1,Multiagent
"Approximate Multiagent Reinforcement Learning for On-Demand Urban
  Mobility Problem on a Large Map (extended version)","Daniel Garces, Sushmita Bhattacharya, Dimitri Bertsekas, Stephanie Gil","In this paper, we focus on the autonomous multiagent taxi routing problem for
a large urban environment where the location and number of future ride requests
are unknown a-priori, but can be estimated by an empirical distribution. Recent
theory has shown that a rollout algorithm with a stable base policy produces a
near-optimal stable policy. In the routing setting, a policy is stable if its
execution keeps the number of outstanding requests uniformly bounded over time.
Although, rollout-based approaches are well-suited for learning cooperative
multiagent policies with considerations for future demand, applying such
methods to a large urban environment can be computationally expensive due to
the large number of taxis required for stability. In this paper, we aim to
address the computational bottleneck of multiagent rollout by proposing an
approximate multiagent rollout-based two phase algorithm that reduces
computational costs, while still achieving a stable near-optimal policy. Our
approach partitions the graph into sectors based on the predicted demand and
the maximum number of taxis that can run sequentially given the user's
computational resources. The algorithm then applies instantaneous assignment
(IA) for re-balancing taxis across sectors and a sector-wide multiagent rollout
algorithm that is executed in parallel for each sector. We provide two main
theoretical results: 1) characterize the number of taxis $m$ that is sufficient
for IA to be stable; 2) derive a necessary condition on $m$ to maintain
stability for IA as time goes to infinity. Our numerical results show that our
approach achieves stability for an $m$ that satisfies the theoretical
conditions. We also empirically demonstrate that our proposed two phase
algorithm has equivalent performance to the one-at-a-time rollout over the
entire map, but with significantly lower runtimes.",2311.01534v3,https://arxiv.org/pdf/2311.01534v3,Multiagent
Let Models Speak Ciphers: Multiagent Debate through Embeddings,"Chau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo Yuan, Bryan A. Plummer, Zhaoran Wang, Hongxia Yang","Discussion and debate among Large Language Models (LLMs) have gained
considerable attention due to their potential to enhance the reasoning ability
of LLMs. Although natural language is an obvious choice for communication due
to LLM's language understanding capability, the token sampling step needed when
generating natural language poses a potential risk of information loss, as it
uses only one token to represent the model's belief across the entire
vocabulary. In this paper, we introduce a communication regime named CIPHER
(Communicative Inter-Model Protocol Through Embedding Representation) to
address this issue. Specifically, we remove the token sampling step from LLMs
and let them communicate their beliefs across the vocabulary through the
expectation of the raw transformer output embeddings. Remarkably, by deviating
from natural language, CIPHER offers an advantage of encoding a broader
spectrum of information without any modification to the model weights,
outperforming the state-of-the-art LLM debate methods using natural language by
0.5-5.0% across five reasoning tasks and multiple open-source LLMs of varying
sizes. This showcases the superiority and robustness of embeddings as an
alternative ""language"" for communication among LLMs. We anticipate that CIPHER
will inspire further exploration for the design of interactions within LLM
agent systems, offering a new direction that could significantly influence
future developments in the field.",2310.06272v2,https://arxiv.org/pdf/2310.06272v2,Multiagent
On Quantified Observability Analysis in Multiagent Systems,"Chunyan Mu, Jun Pang","In multiagent systems (MASs), agents' observation upon system behaviours may
improve the overall team performance, but may also leak sensitive information
to an observer. A quantified observability analysis can thus be useful to
assist decision-making in MASs by operators seeking to optimise the
relationship between performance effectiveness and information exposure through
observations in practice. This paper presents a novel approach to
quantitatively analysing the observability properties in MASs. The concept of
opacity is applied to formally express the characterisation of observability in
MASs modelled as partially observable multiagent systems. We propose a temporal
logic oPATL to reason about agents' observability with quantitative goals,
which capture the probability of information transparency of system behaviours
to an observer, and develop verification techniques for quantitatively
analysing such properties. We implement the approach as an extension of the
PRISM model checker, and illustrate its applicability via several examples.",2310.02614v1,https://arxiv.org/pdf/2310.02614v1,Multiagent
"Multiagent Reinforcement Learning with an Attention Mechanism for
  Improving Energy Efficiency in LoRa Networks","Xu Zhang, Ziqi Lin, Shimin Gong, Bo Gu, Dusit Niyato","Long Range (LoRa) wireless technology, characterized by low power consumption
and a long communication range, is regarded as one of the enabling technologies
for the Industrial Internet of Things (IIoT). However, as the network scale
increases, the energy efficiency (EE) of LoRa networks decreases sharply due to
severe packet collisions. To address this issue, it is essential to
appropriately assign transmission parameters such as the spreading factor and
transmission power for each end device (ED). However, due to the sporadic
traffic and low duty cycle of LoRa networks, evaluating the system EE
performance under different parameter settings is time-consuming. Therefore, we
first formulate an analytical model to calculate the system EE. On this basis,
we propose a transmission parameter allocation algorithm based on multiagent
reinforcement learning (MALoRa) with the aim of maximizing the system EE of
LoRa networks. Notably, MALoRa employs an attention mechanism to guide each ED
to better learn how much ''attention'' should be given to the parameter
assignments for relevant EDs when seeking to improve the system EE. Simulation
results demonstrate that MALoRa significantly improves the system EE compared
with baseline algorithms with an acceptable degradation in packet delivery rate
(PDR).",2309.08965v1,https://arxiv.org/pdf/2309.08965v1,Multiagent
GPT-in-the-Loop: Adaptive Decision-Making for Multiagent Systems,"Nathalia Nascimento, Paulo Alencar, Donald Cowan","This paper introduces the ""GPT-in-the-loop"" approach, a novel method
combining the advanced reasoning capabilities of Large Language Models (LLMs)
like Generative Pre-trained Transformers (GPT) with multiagent (MAS) systems.
Venturing beyond traditional adaptive approaches that generally require long
training processes, our framework employs GPT-4 for enhanced problem-solving
and explanation skills. Our experimental backdrop is the smart streetlight
Internet of Things (IoT) application. Here, agents use sensors, actuators, and
neural networks to create an energy-efficient lighting system. By integrating
GPT-4, these agents achieve superior decision-making and adaptability without
the need for extensive training. We compare this approach with both traditional
neuroevolutionary methods and solutions provided by software engineers,
underlining the potential of GPT-driven multiagent systems in IoT.
Structurally, the paper outlines the incorporation of GPT into the agent-driven
Framework for the Internet of Things (FIoT), introduces our proposed
GPT-in-the-loop approach, presents comparative results in the IoT context, and
concludes with insights and future directions.",2308.10435v1,https://arxiv.org/pdf/2308.10435v1,Multiagent
"Mimicking To Dominate: Imitation Learning Strategies for Success in
  Multiagent Competitive Games","The Viet Bui, Tien Mai, Thanh Hong Nguyen","Training agents in multi-agent competitive games presents significant
challenges due to their intricate nature. These challenges are exacerbated by
dynamics influenced not only by the environment but also by opponents'
strategies. Existing methods often struggle with slow convergence and
instability. To address this, we harness the potential of imitation learning to
comprehend and anticipate opponents' behavior, aiming to mitigate uncertainties
with respect to the game dynamics. Our key contributions include: (i) a new
multi-agent imitation learning model for predicting next moves of the opponents
-- our model works with hidden opponents' actions and local observations; (ii)
a new multi-agent reinforcement learning algorithm that combines our imitation
learning model and policy training into one single training process; and (iii)
extensive experiments in three challenging game environments, including an
advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2).
Experimental results show that our approach achieves superior performance
compared to existing state-of-the-art multi-agent RL algorithms.",2308.10188v1,https://arxiv.org/pdf/2308.10188v1,Multiagent
"Learning in Cooperative Multiagent Systems Using Cognitive and Machine
  Models","Thuy Ngoc Nguyen, Duy Nhat Phan, Cleotilde Gonzalez","Developing effective Multi-Agent Systems (MAS) is critical for many
applications requiring collaboration and coordination with humans. Despite the
rapid advance of Multi-Agent Deep Reinforcement Learning (MADRL) in cooperative
MAS, one major challenge is the simultaneous learning and interaction of
independent agents in dynamic environments in the presence of stochastic
rewards. State-of-the-art MADRL models struggle to perform well in Coordinated
Multi-agent Object Transportation Problems (CMOTPs), wherein agents must
coordinate with each other and learn from stochastic rewards. In contrast,
humans often learn rapidly to adapt to nonstationary environments that require
coordination among people. In this paper, motivated by the demonstrated ability
of cognitive models based on Instance-Based Learning Theory (IBLT) to capture
human decisions in many dynamic decision making tasks, we propose three
variants of Multi-Agent IBL models (MAIBL). The idea of these MAIBL algorithms
is to combine the cognitive mechanisms of IBLT and the techniques of MADRL
models to deal with coordination MAS in stochastic environments from the
perspective of independent learners. We demonstrate that the MAIBL models
exhibit faster learning and achieve better coordination in a dynamic CMOTP task
with various settings of stochastic rewards compared to current MADRL models.
We discuss the benefits of integrating cognitive insights into MADRL models.",2308.09219v1,https://arxiv.org/pdf/2308.09219v1,Multiagent
"BRNES: Enabling Security and Privacy-aware Experience Sharing in
  Multiagent Robotic and Autonomous Systems","Md Tamjid Hossain, Hung Manh La, Shahriar Badsha, Anton Netchaev","Although experience sharing (ES) accelerates multiagent reinforcement
learning (MARL) in an advisor-advisee framework, attempts to apply ES to
decentralized multiagent systems have so far relied on trusted environments and
overlooked the possibility of adversarial manipulation and inference.
Nevertheless, in a real-world setting, some Byzantine attackers, disguised as
advisors, may provide false advice to the advisee and catastrophically degrade
the overall learning performance. Also, an inference attacker, disguised as an
advisee, may conduct several queries to infer the advisors' private information
and make the entire ES process questionable in terms of privacy leakage. To
address and tackle these issues, we propose a novel MARL framework (BRNES) that
heuristically selects a dynamic neighbor zone for each advisee at each learning
step and adopts a weighted experience aggregation technique to reduce Byzantine
attack impact. Furthermore, to keep the agent's private information safe from
adversarial inference attacks, we leverage the local differential privacy
(LDP)-induced noise during the ES process. Our experiments show that our
framework outperforms the state-of-the-art in terms of the steps to goal,
obtained reward, and time to goal metrics. Particularly, our evaluation shows
that the proposed framework is 8.32x faster than the current non-private
frameworks and 1.41x faster than the private frameworks in an adversarial
setting.",2308.01274v1,https://arxiv.org/pdf/2308.01274v1,Multiagent
Anticipating Responsibility in Multiagent Planning,"Timothy Parker, Umberto Grandi, Emiliano Lorini","Responsibility anticipation is the process of determining if the actions of
an individual agent may cause it to be responsible for a particular outcome.
This can be used in a multi-agent planning setting to allow agents to
anticipate responsibility in the plans they consider. The planning setting in
this paper includes partial information regarding the initial state and
considers formulas in linear temporal logic as positive or negative outcomes to
be attained or avoided. We firstly define attribution for notions of active,
passive and contributive responsibility, and consider their agentive variants.
We then use these to define the notion of responsibility anticipation. We prove
that our notions of anticipated responsibility can be used to coordinate agents
in a planning setting and give complexity results for our model, discussing
equivalence with classical planning. We also present an outline for solving
some of our attribution and anticipation problems using PDDL solvers.",2307.16685v1,https://arxiv.org/pdf/2307.16685v1,Multiagent
A Novel Multiagent Flexibility Aggregation Framework,"Stavros Orfanoudakis, Georgios Chalkiadakis","The increasing number of Distributed Energy Resources (DERs) in the emerging
Smart Grid, has created an imminent need for intelligent multiagent frameworks
able to utilize these assets efficiently. In this paper, we propose a novel DER
aggregation framework, encompassing a multiagent architecture and various types
of mechanisms for the effective management and efficient integration of DERs in
the Grid. One critical component of our architecture is the Local Flexibility
Estimators (LFEs) agents, which are key for offloading the Aggregator from
serious or resource-intensive responsibilities -- such as addressing privacy
concerns and predicting the accuracy of DER statements regarding their offered
demand response services. The proposed framework allows the formation of
efficient LFE cooperatives. To this end, we developed and deployed a variety of
cooperative member selection mechanisms, including (a) scoring rules, and (b)
(deep) reinforcement learning. We use data from the well-known PowerTAC
simulator to systematically evaluate our framework. Our experiments verify its
effectiveness for incorporating heterogeneous DERs into the Grid in an
efficient manner. In particular, when using the well-known probabilistic
prediction accuracy-incentivizing CRPS scoring rule as a selection mechanism,
our framework results in increased average payments for participants, when
compared with traditional commercial aggregators.",2307.08401v1,https://arxiv.org/pdf/2307.08401v1,Multiagent
Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems,"Nathalia Nascimento, Paulo Alencar, Donald Cowan","In autonomic computing, self-adaptation has been proposed as a fundamental
paradigm to manage the complexity of multiagent systems (MASs). This achieved
by extending a system with support to monitor and adapt itself to achieve
specific concerns of interest. Communication in these systems is key given that
in scenarios involving agent interaction, it enhances cooperation and reduces
coordination challenges by enabling direct, clear information exchange.
However, improving the expressiveness of the interaction communication with
MASs is not without challenges. In this sense, the interplay between
self-adaptive systems and effective communication is crucial for future MAS
advancements. In this paper, we propose the integration of large language
models (LLMs) such as GPT-based technologies into multiagent systems. We anchor
our methodology on the MAPE-K model, which is renowned for its robust support
in monitoring, analyzing, planning, and executing system adaptations in
response to dynamic environments. We also present a practical illustration of
the proposed approach, in which we implement and assess a basic MAS-based
application. The approach significantly advances the state-of-the-art of
self-adaptive systems by proposing a new paradigm for MAS self-adaptation of
autonomous systems based on LLM capabilities.",2307.06187v1,https://arxiv.org/pdf/2307.06187v1,Multiagent
"Surge Routing: Event-informed Multiagent Reinforcement Learning for
  Autonomous Rideshare","Daniel Garces, Stephanie Gil","Large events such as conferences, concerts and sports games, often cause
surges in demand for ride services that are not captured in average demand
patterns, posing unique challenges for routing algorithms. We propose a
learning framework for an autonomous fleet of taxis that leverages event data
from the internet to predict demand surges and generate cooperative routing
policies. We achieve this through a combination of two major components: (i) a
demand prediction framework that uses textual event information in the form of
events' descriptions and reviews to predict event-driven demand surges over
street intersections, and (ii) a scalable multiagent reinforcement learning
framework that leverages demand predictions and uses one-agent-at-a-time
rollout combined with limited sampling certainty equivalence to learn
intersection-level routing policies. For our experimental results we consider
real NYC ride share data for the year 2022 and information for more than 2000
events across 300 unique venues in Manhattan. We test our approach with a fleet
of 100 taxis on a map with 2235 street intersections. Our experimental results
demonstrate that our method learns routing policies that reduce wait time
overhead per serviced request by 25% to 75%, while picking up 1% to 4% more
requests than other model-based RL frameworks and classical methods in
operations research.",2307.02637v2,https://arxiv.org/pdf/2307.02637v2,Multiagent
"Hiding in Plain Sight: Differential Privacy Noise Exploitation for
  Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement
  Learning","Md Tamjid Hossain, Hung La","Lately, differential privacy (DP) has been introduced in cooperative
multiagent reinforcement learning (CMARL) to safeguard the agents' privacy
against adversarial inference during knowledge sharing. Nevertheless, we argue
that the noise introduced by DP mechanisms may inadvertently give rise to a
novel poisoning threat, specifically in the context of private knowledge
sharing during CMARL, which remains unexplored in the literature. To address
this shortcoming, we present an adaptive, privacy-exploiting, and
evasion-resilient localized poisoning attack (PeLPA) that capitalizes on the
inherent DP-noise to circumvent anomaly detection systems and hinder the
optimal convergence of the CMARL model. We rigorously evaluate our proposed
PeLPA attack in diverse environments, encompassing both non-adversarial and
multiple-adversarial contexts. Our findings reveal that, in a medium-scale
environment, the PeLPA attack with attacker ratios of 20% and 40% can lead to
an increase in average steps to goal by 50.69% and 64.41%, respectively.
Furthermore, under similar conditions, PeLPA can result in a 1.4x and 1.6x
computational time increase in optimal reward attainment and a 1.18x and 1.38x
slower convergence for attacker ratios of 20% and 40%, respectively.",2307.00268v2,https://arxiv.org/pdf/2307.00268v2,Multiagent
Towards a Better Understanding of Learning with Multiagent Teams,"David Radke, Kate Larson, Tim Brecht, Kyle Tilbury","While it has long been recognized that a team of individual learning agents
can be greater than the sum of its parts, recent work has shown that larger
teams are not necessarily more effective than smaller ones. In this paper, we
study why and under which conditions certain team structures promote effective
learning for a population of individual learning agents. We show that,
depending on the environment, some team structures help agents learn to
specialize into specific roles, resulting in more favorable global results.
However, large teams create credit assignment challenges that reduce
coordination, leading to large teams performing poorly compared to smaller
ones. We support our conclusions with both theoretical analysis and empirical
results.",2306.16205v1,https://arxiv.org/pdf/2306.16205v1,Multiagent
"SIMMF: Semantics-aware Interactive Multiagent Motion Forecasting for
  Autonomous Vehicle Driving","Vidyaa Krishnan Nivash, Ahmed H. Qureshi","Autonomous vehicles require motion forecasting of their surrounding
multiagents (pedestrians and vehicles) to make optimal decisions for
navigation. The existing methods focus on techniques to utilize the positions
and velocities of these agents and fail to capture semantic information from
the scene. Moreover, to mitigate the increase in computational complexity
associated with the number of agents in the scene, some works leverage
Euclidean distance to prune far-away agents. However, distance-based metric
alone is insufficient to select relevant agents and accurately perform their
predictions. To resolve these issues, we propose the Semantics-aware
Interactive Multiagent Motion Forecasting (SIMMF) method to capture semantics
along with spatial information and optimally select relevant agents for motion
prediction. Specifically, we achieve this by implementing a semantic-aware
selection of relevant agents from the scene and passing them through an
attention mechanism to extract global encodings. These encodings along with
agents' local information, are passed through an encoder to obtain
time-dependent latent variables for a motion policy predicting the future
trajectories. Our results show that the proposed approach outperforms
state-of-the-art baselines and provides more accurate and scene-consistent
predictions.",2306.14941v2,https://arxiv.org/pdf/2306.14941v2,Multiagent
"Adversarial Search and Tracking with Multiagent Reinforcement Learning
  in Sparsely Observable Environment","Zixuan Wu, Sean Ye, Manisha Natarajan, Letian Chen, Rohan Paleja, Matthew C. Gombolay","We study a search and tracking (S&T) problem where a team of dynamic search
agents must collaborate to track an adversarial, evasive agent. The
heterogeneous search team may only have access to a limited number of past
adversary trajectories within a large search space. This problem is challenging
for both model-based searching and reinforcement learning (RL) methods since
the adversary exhibits reactionary and deceptive evasive behaviors in a large
space leading to sparse detections for the search agents. To address this
challenge, we propose a novel Multi-Agent RL (MARL) framework that leverages
the estimated adversary location from our learnable filtering model. We show
that our MARL architecture can outperform all baselines and achieves a 46%
increase in detection rate.",2306.11301v2,https://arxiv.org/pdf/2306.11301v2,Multiagent
"Real-Time Network-Level Traffic Signal Control: An Explicit Multiagent
  Coordination Method","Wanyuan Wang, Tianchi Qiao, Jinming Ma, Jiahui Jin, Zhibin Li, Weiwei Wu, Yichuan Jian","Efficient traffic signal control (TSC) has been one of the most useful ways
for reducing urban road congestion. Key to the challenge of TSC includes 1) the
essential of real-time signal decision, 2) the complexity in traffic dynamics,
and 3) the network-level coordination. Recent efforts that applied
reinforcement learning (RL) methods can query policies by mapping the traffic
state to the signal decision in real-time, however, is inadequate for
unexpected traffic flows. By observing real traffic information, online
planning methods can compute the signal decisions in a responsive manner. We
propose an explicit multiagent coordination (EMC)-based online planning methods
that can satisfy adaptive, real-time and network-level TSC. By multiagent, we
model each intersection as an autonomous agent, and the coordination efficiency
is modeled by a cost (i.e., congestion index) function between neighbor
intersections. By network-level coordination, each agent exchanges messages
with respect to cost function with its neighbors in a fully decentralized
manner. By real-time, the message passing procedure can interrupt at any time
when the real time limit is reached and agents select the optimal signal
decisions according to the current message. Moreover, we prove our EMC method
can guarantee network stability by borrowing ideas from transportation domain.
Finally, we test our EMC method in both synthetic and real road network
datasets. Experimental results are encouraging: compared to RL and conventional
transportation baselines, our EMC method performs reasonably well in terms of
adapting to real-time traffic dynamics, minimizing vehicle travel time and
scalability to city-scale road networks.",2306.08843v1,https://arxiv.org/pdf/2306.08843v1,Multiagent
Towards a Unifying Model of Rationality in Multiagent Systems,"Robert Loftin, Mustafa Mert Çelikok, Frans A. Oliehoek","Multiagent systems deployed in the real world need to cooperate with other
agents (including humans) nearly as effectively as these agents cooperate with
one another. To design such AI, and provide guarantees of its effectiveness, we
need to clearly specify what types of agents our AI must be able to cooperate
with. In this work we propose a generic model of socially intelligent agents,
which are individually rational learners that are also able to cooperate with
one another (in the sense that their joint behavior is Pareto efficient). We
define rationality in terms of the regret incurred by each agent over its
lifetime, and show how we can construct socially intelligent agents for
different forms of regret. We then discuss the implications of this model for
the development of ""robust"" MAS that can cooperate with a wide variety of
socially intelligent agents.",2305.18071v1,https://arxiv.org/pdf/2305.18071v1,Multiagent
"Improving Factuality and Reasoning in Language Models through Multiagent
  Debate","Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch","Large language models (LLMs) have demonstrated remarkable capabilities in
language generation, understanding, and few-shot learning in recent years. An
extensive body of work has explored how their performance may be further
improved through the tools of prompting, ranging from verification,
self-consistency, or intermediate scratchpads. In this paper, we present a
complementary approach to improve language responses where multiple language
model instances propose and debate their individual responses and reasoning
processes over multiple rounds to arrive at a common final answer. Our findings
indicate that this approach significantly enhances mathematical and strategic
reasoning across a number of tasks. We also demonstrate that our approach
improves the factual validity of generated content, reducing fallacious answers
and hallucinations that contemporary models are prone to. Our approach may be
directly applied to existing black-box models and uses identical procedure and
prompts for all tasks we investigate. Overall, our findings suggest that such
""society of minds"" approach has the potential to significantly advance the
capabilities of LLMs and pave the way for further breakthroughs in language
generation and understanding.",2305.14325v1,https://arxiv.org/pdf/2305.14325v1,Multiagent
Human Values in Multiagent Systems,"Nardine Osman, Mark d'Inverno","One of the major challenges we face with ethical AI today is developing
computational systems whose reasoning and behaviour are provably aligned with
human values. Human values, however, are notorious for being ambiguous,
contradictory and ever-changing. In order to bridge this gap, and get us closer
to the situation where we can formally reason about implementing values into
AI, this paper presents a formal representation of values, grounded in the
social sciences. We use this formal representation to articulate the key
challenges for achieving value-aligned behaviour in multiagent systems (MAS)
and a research roadmap for addressing them.",2305.02739v1,https://arxiv.org/pdf/2305.02739v1,Multiagent
"Learning to Learn Group Alignment: A Self-Tuning Credo Framework with
  Multiagent Teams","David Radke, Kyle Tilbury","Mixed incentives among a population with multiagent teams has been shown to
have advantages over a fully cooperative system; however, discovering the best
mixture of incentives or team structure is a difficult and dynamic problem. We
propose a framework where individual learning agents self-regulate their
configuration of incentives through various parts of their reward function.
This work extends previous work by giving agents the ability to dynamically
update their group alignment during learning and by allowing teammates to have
different group alignment. Our model builds on ideas from hierarchical
reinforcement learning and meta-learning to learn the configuration of a reward
function that supports the development of a behavioral policy. We provide
preliminary results in a commonly studied multiagent environment and find that
agents can achieve better global outcomes by self-tuning their respective group
alignment parameters.",2304.07337v1,https://arxiv.org/pdf/2304.07337v1,Multiagent
A Multiagent CyberBattleSim for RL Cyber Operation Agents,"Thomas Kunz, Christian Fisher, James La Novara-Gsell, Christopher Nguyen, Li Li","Hardening cyber physical assets is both crucial and labor-intensive.
Recently, Machine Learning (ML) in general and Reinforcement Learning RL) more
specifically has shown great promise to automate tasks that otherwise would
require significant human insight/intelligence. The development of autonomous
RL agents requires a suitable training environment that allows us to quickly
evaluate various alternatives, in particular how to arrange training scenarios
that pit attackers and defenders against each other. CyberBattleSim is a
training environment that supports the training of red agents, i.e., attackers.
We added the capability to train blue agents, i.e., defenders. The paper
describes our changes and reports on the results we obtained when training blue
agents, either in isolation or jointly with red agents. Our results show that
training a blue agent does lead to stronger defenses against attacks. In
particular, training a blue agent jointly with a red agent increases the blue
agent's capability to thwart sophisticated red agents.",2304.11052v1,https://arxiv.org/pdf/2304.11052v1,Multiagent
Presenting Multiagent Challenges in Team Sports Analytics,"David Radke, Alexi Orchard","This paper draws correlations between several challenges and opportunities
within the area of team sports analytics and key research areas within
multiagent systems (MAS). We specifically consider invasion games, defined as
sports where players invade the opposing team's territory and can interact
anywhere on a playing surface such as ice hockey, soccer, and basketball. We
argue that MAS is well-equipped to study invasion games and will benefit both
MAS and sports analytics fields. Our discussion highlights areas for MAS
implementation and further development along two axes: short-term in-game
strategy (coaching) and long-term team planning (management).",2303.13660v1,https://arxiv.org/pdf/2303.13660v1,Multiagent
"Resilient Output Containment Control of Heterogeneous Multiagent Systems
  Against Composite Attacks: A Digital Twin Approach","Yukang Cui, Lingbo Cao, Michael V. Basin, Jun Shen, Tingwen Huang, Xin Gong","This paper studies the distributed resilient output containment control of
heterogeneous multiagent systems against composite attacks, including
denial-of-services (DoS) attacks, false-data injection (FDI) attacks,
camouflage attacks, and actuation attacks. Inspired by digital twins, a twin
layer (TL) with higher security and privacy is used to decouple the above
problem into two tasks: defense protocols against DoS attacks on TL and defense
protocols against actuation attacks on cyber-physical layer (CPL). First,
considering modeling errors of leader dynamics, we introduce distributed
observers to reconstruct the leader dynamics for each follower on TL under DoS
attacks. Second, distributed estimators are used to estimate follower states
according to the reconstructed leader dynamics on the TL. Third, according to
the reconstructed leader dynamics, we design decentralized solvers that
calculate the output regulator equations on CPL. Fourth, decentralized adaptive
attack-resilient control schemes that resist unbounded actuation attacks are
provided on CPL. Furthermore, we apply the above control protocols to prove
that the followers can achieve uniformly ultimately bounded (UUB) convergence,
and the upper bound of the UUB convergence is determined explicitly. Finally,
two simulation examples are provided to show the effectiveness of the proposed
control protocols.",2303.12693v1,https://arxiv.org/pdf/2303.12693v1,Multiagent
"Population-based Evaluation in Repeated Rock-Paper-Scissors as a
  Benchmark for Multiagent Reinforcement Learning","Marc Lanctot, John Schultz, Neil Burch, Max Olan Smith, Daniel Hennes, Thomas Anthony, Julien Perolat","Progress in fields of machine learning and adversarial planning has benefited
significantly from benchmark domains, from checkers and the classic UCI data
sets to Go and Diplomacy. In sequential decision-making, agent evaluation has
largely been restricted to few interactions against experts, with the aim to
reach some desired level of performance (e.g. beating a human professional
player). We propose a benchmark for multiagent learning based on repeated play
of the simple game Rock, Paper, Scissors along with a population of forty-three
tournament entries, some of which are intentionally sub-optimal. We describe
metrics to measure the quality of agents based both on average returns and
exploitability. We then show that several RL, online learning, and language
model approaches can learn good counter-strategies and generalize well, but
ultimately lose to the top-performing bots, creating an opportunity for
research in multiagent learning.",2303.03196v2,https://arxiv.org/pdf/2303.03196v2,Multiagent
Multiagent Inverse Reinforcement Learning via Theory of Mind Reasoning,"Haochen Wu, Pedro Sequeira, David V. Pynadath","We approach the problem of understanding how people interact with each other
in collaborative settings, especially when individuals know little about their
teammates, via Multiagent Inverse Reinforcement Learning (MIRL), where the goal
is to infer the reward functions guiding the behavior of each individual given
trajectories of a team's behavior during some task. Unlike current MIRL
approaches, we do not assume that team members know each other's goals a
priori; rather, that they collaborate by adapting to the goals of others
perceived by observing their behavior, all while jointly performing a task. To
address this problem, we propose a novel approach to MIRL via Theory of Mind
(MIRL-ToM). For each agent, we first use ToM reasoning to estimate a posterior
distribution over baseline reward profiles given their demonstrated behavior.
We then perform MIRL via decentralized equilibrium by employing single-agent
Maximum Entropy IRL to infer a reward function for each agent, where we
simulate the behavior of other teammates according to the time-varying
distribution over profiles. We evaluate our approach in a simulated 2-player
search-and-rescue operation where the goal of the agents, playing different
roles, is to search for and evacuate victims in the environment. Our results
show that the choice of baseline profiles is paramount to the recovery of the
ground-truth rewards, and that MIRL-ToM is able to recover the rewards used by
agents interacting both with known and unknown teammates.",2302.10238v2,https://arxiv.org/pdf/2302.10238v2,Multiagent
"Breaking the Curse of Multiagency: Provably Efficient Decentralized
  Multi-Agent RL with Function Approximation","Yuanhao Wang, Qinghua Liu, Yu Bai, Chi Jin","A unique challenge in Multi-Agent Reinforcement Learning (MARL) is the curse
of multiagency, where the description length of the game as well as the
complexity of many existing learning algorithms scale exponentially with the
number of agents. While recent works successfully address this challenge under
the model of tabular Markov Games, their mechanisms critically rely on the
number of states being finite and small, and do not extend to practical
scenarios with enormous state spaces where function approximation must be used
to approximate value functions or policies.
  This paper presents the first line of MARL algorithms that provably resolve
the curse of multiagency under function approximation. We design a new
decentralized algorithm -- V-Learning with Policy Replay, which gives the first
polynomial sample complexity results for learning approximate Coarse Correlated
Equilibria (CCEs) of Markov Games under decentralized linear function
approximation. Our algorithm always outputs Markov CCEs, and achieves an
optimal rate of $\widetilde{\mathcal{O}}(\epsilon^{-2})$ for finding
$\epsilon$-optimal solutions. Also, when restricted to the tabular case, our
result improves over the current best decentralized result
$\widetilde{\mathcal{O}}(\epsilon^{-3})$ for finding Markov CCEs. We further
present an alternative algorithm -- Decentralized Optimistic Policy Mirror
Descent, which finds policy-class-restricted CCEs using a polynomial number of
samples. In exchange for learning a weaker version of CCEs, this algorithm
applies to a wider range of problems under generic function approximation, such
as linear quadratic games and MARL problems with low ''marginal'' Eluder
dimension.",2302.06606v2,https://arxiv.org/pdf/2302.06606v2,Multiagent
"ReMIX: Regret Minimization for Monotonic Value Function Factorization in
  Multiagent Reinforcement Learning","Yongsheng Mei, Hanhan Zhou, Tian Lan","Value function factorization methods have become a dominant approach for
cooperative multiagent reinforcement learning under a centralized training and
decentralized execution paradigm. By factorizing the optimal joint action-value
function using a monotonic mixing function of agents' utilities, these
algorithms ensure the consistency between joint and local action selections for
decentralized decision-making. Nevertheless, the use of monotonic mixing
functions also induces representational limitations. Finding the optimal
projection of an unrestricted mixing function onto monotonic function classes
is still an open problem. To this end, we propose ReMIX, formulating this
optimal projection problem for value function factorization as a regret
minimization over the projection weights of different state-action values. Such
an optimization problem can be relaxed and solved using the Lagrangian
multiplier method to obtain the close-form optimal projection weights. By
minimizing the resulting policy regret, we can narrow the gap between the
optimal and the restricted monotonic mixing functions, thus obtaining an
improved monotonic value function factorization. Our experimental results on
Predator-Prey and StarCraft Multiagent Challenge environments demonstrate the
effectiveness of our method, indicating the better capabilities of handling
environments with non-monotonic value functions.",2302.05593v1,https://arxiv.org/pdf/2302.05593v1,Multiagent
"Breaking the Curse of Multiagents in a Large State Space: RL in Markov
  Games with Independent Linear Function Approximation","Qiwen Cui, Kaiqing Zhang, Simon S. Du","We propose a new model, independent linear Markov game, for multi-agent
reinforcement learning with a large state space and a large number of agents.
This is a class of Markov games with independent linear function approximation,
where each agent has its own function approximation for the state-action value
functions that are marginalized by other players' policies. We design new
algorithms for learning the Markov coarse correlated equilibria (CCE) and
Markov correlated equilibria (CE) with sample complexity bounds that only scale
polynomially with each agent's own function class complexity, thus breaking the
curse of multiagents. In contrast, existing works for Markov games with
function approximation have sample complexity bounds scale with the size of the
\emph{joint action space} when specialized to the canonical tabular Markov game
setting, which is exponentially large in the number of agents. Our algorithms
rely on two key technical innovations: (1) utilizing policy replay to tackle
non-stationarity incurred by multiple agents and the use of function
approximation; (2) separating learning Markov equilibria and exploration in the
Markov games, which allows us to use the full-information no-regret learning
oracle instead of the stronger bandit-feedback no-regret learning oracle used
in the tabular setting. Furthermore, we propose an iterative-best-response type
algorithm that can learn pure Markov Nash equilibria in independent linear
Markov potential games. In the tabular case, by adapting the policy replay
mechanism for independent linear Markov games, we propose an algorithm with
$\widetilde{O}(\epsilon^{-2})$ sample complexity to learn Markov CCE, which
improves the state-of-the-art result $\widetilde{O}(\epsilon^{-3})$ in
Daskalakis et al. 2022, where $\epsilon$ is the desired accuracy, and also
significantly improves other problem parameters.",2302.03673v3,https://arxiv.org/pdf/2302.03673v3,Multiagent
Differential Privacy in Cooperative Multiagent Planning,"Bo Chen, Calvin Hawkins, Mustafa O. Karabag, Cyrus Neary, Matthew Hale, Ufuk Topcu","Privacy-aware multiagent systems must protect agents' sensitive data while
simultaneously ensuring that agents accomplish their shared objectives. Towards
this goal, we propose a framework to privatize inter-agent communications in
cooperative multiagent decision-making problems. We study sequential
decision-making problems formulated as cooperative Markov games with
reach-avoid objectives. We apply a differential privacy mechanism to privatize
agents' communicated symbolic state trajectories, and then we analyze tradeoffs
between the strength of privacy and the team's performance. For a given level
of privacy, this tradeoff is shown to depend critically upon the total
correlation among agents' state-action processes. We synthesize policies that
are robust to privacy by reducing the value of the total correlation. Numerical
experiments demonstrate that the team's performance under these policies
decreases by only 3 percent when comparing private versus non-private
implementations of communication. By contrast, the team's performance decreases
by roughly 86 percent when using baseline policies that ignore total
correlation and only optimize team performance.",2301.08811v1,https://arxiv.org/pdf/2301.08811v1,Multiagent
"Interpreting Primal-Dual Algorithms for Constrained Multiagent
  Reinforcement Learning","Daniel Tabas, Ahmed S. Zamzam, Baosen Zhang","Constrained multiagent reinforcement learning (C-MARL) is gaining importance
as MARL algorithms find new applications in real-world systems ranging from
energy systems to drone swarms. Most C-MARL algorithms use a primal-dual
approach to enforce constraints through a penalty function added to the reward.
In this paper, we study the structural effects of this penalty term on the MARL
problem. First, we show that the standard practice of using the constraint
function as the penalty leads to a weak notion of safety. However, by making
simple modifications to the penalty term, we can enforce meaningful
probabilistic (chance and conditional value at risk) constraints. Second, we
quantify the effect of the penalty term on the value function, uncovering an
improved value estimation procedure. We use these insights to propose a
constrained multiagent advantage actor critic (C-MAA2C) algorithm. Simulations
in a simple constrained multiagent environment affirm that our reinterpretation
of the primal-dual method in terms of probabilistic constraints is effective,
and that our proposed value estimate accelerates convergence to a safe joint
policy.",2211.16069v3,https://arxiv.org/pdf/2211.16069v3,Multiagent
"Multiagent Reinforcement Learning for Autonomous Routing and Pickup
  Problem with Adaptation to Variable Demand","Daniel Garces, Sushmita Bhattacharya, Stephanie Gil, Dimitri Bertsekas","We derive a learning framework to generate routing/pickup policies for a
fleet of autonomous vehicles tasked with servicing stochastically appearing
requests on a city map. We focus on policies that 1) give rise to coordination
amongst the vehicles, thereby reducing wait times for servicing requests, 2)
are non-myopic, and consider a-priori potential future requests, 3) can adapt
to changes in the underlying demand distribution. Specifically, we are
interested in policies that are adaptive to fluctuations of actual demand
conditions in urban environments, such as on-peak vs. off-peak hours. We
achieve this through a combination of (i) an online play algorithm that
improves the performance of an offline-trained policy, and (ii) an offline
approximation scheme that allows for adapting to changes in the underlying
demand model. In particular, we achieve adaptivity of our learned policy to
different demand distributions by quantifying a region of validity using the
q-valid radius of a Wasserstein Ambiguity Set. We propose a mechanism for
switching the originally trained offline approximation when the current demand
is outside the original validity region. In this case, we propose to use an
offline architecture, trained on a historical demand model that is closer to
the current demand in terms of Wasserstein distance. We learn routing and
pickup policies over real taxicab requests in San Francisco with high
variability between on-peak and off-peak hours, demonstrating the ability of
our method to adapt to real fluctuation in demand distributions. Our numerical
results demonstrate that our method outperforms alternative rollout-based
reinforcement learning schemes, as well as other classical methods from
operations research.",2211.14983v2,https://arxiv.org/pdf/2211.14983v2,Multiagent
Learning Heterogeneous Agent Cooperation via Multiagent League Training,"Qingxu Fu, Xiaolin Ai, Jianqiang Yi, Tenghai Qiu, Wanmai Yuan, Zhiqiang Pu","Many multiagent systems in the real world include multiple types of agents
with different abilities and functionality. Such heterogeneous multiagent
systems have significant practical advantages. However, they also come with
challenges compared with homogeneous systems for multiagent reinforcement
learning, such as the non-stationary problem and the policy version iteration
issue. This work proposes a general-purpose reinforcement learning algorithm
named Heterogeneous League Training (HLT) to address heterogeneous multiagent
problems. HLT keeps track of a pool of policies that agents have explored
during training, gathering a league of heterogeneous policies to facilitate
future policy optimization. Moreover, a hyper-network is introduced to increase
the diversity of agent behaviors when collaborating with teammates having
different levels of cooperation skills. We use heterogeneous benchmark tasks to
demonstrate that (1) HLT promotes the success rate in cooperative heterogeneous
tasks; (2) HLT is an effective approach to solving the policy version iteration
problem; (3) HLT provides a practical way to assess the difficulty of learning
each role in a heterogeneous team.",2211.11616v2,https://arxiv.org/pdf/2211.11616v2,Multiagent
Artificial virtuous agents in a multiagent tragedy of the commons,Jakob Stenseke,"Although virtue ethics has repeatedly been proposed as a suitable framework
for the development of artificial moral agents (AMAs), it has been proven
difficult to approach from a computational perspective. In this work, we
present the first technical implementation of artificial virtuous agents (AVAs)
in moral simulations. First, we review previous conceptual and technical work
in artificial virtue ethics and describe a functionalistic path to AVAs based
on dispositional virtues, bottom-up learning, and top-down eudaimonic reward.
We then provide the details of a technical implementation in a moral simulation
based on a tragedy of the commons scenario. The experimental results show how
the AVAs learn to tackle cooperation problems while exhibiting core features of
their theoretical counterpart, including moral character, dispositional
virtues, learning from experience, and the pursuit of eudaimonia. Ultimately,
we argue that virtue ethics provides a compelling path toward morally excellent
machines and that our work provides an important starting point for such
endeavors.",2210.02769v1,https://arxiv.org/pdf/2210.02769v1,Multiagent
From Intelligent Agents to Trustworthy Human-Centred Multiagent Systems,"Mohammad Divband Soorati, Enrico H. Gerding, Enrico Marchioni, Pavel Naumov, Timothy J. Norman, Sarvapali D. Ramchurn, Bahar Rastegari, Adam Sobey, Sebastian Stein, Danesh Tarpore, Vahid Yazdanpanah, Jie Zhang","The Agents, Interaction and Complexity research group at the University of
Southampton has a long track record of research in multiagent systems (MAS). We
have made substantial scientific contributions across learning in MAS,
game-theoretic techniques for coordinating agent systems, and formal methods
for representation and reasoning. We highlight key results achieved by the
group and elaborate on recent work and open research challenges in developing
trustworthy autonomous systems and deploying human-centred AI systems that aim
to support societal good.",2210.02260v1,https://arxiv.org/pdf/2210.02260v1,Multiagent
"A Multiagent Framework for the Asynchronous and Collaborative Extension
  of Multitask ML Systems",Andrea Gesmundo,"The traditional ML development methodology does not enable a large number of
contributors, each with distinct objectives, to work collectively on the
creation and extension of a shared intelligent system. Enabling such a
collaborative methodology can accelerate the rate of innovation, increase ML
technologies accessibility and enable the emergence of novel capabilities. We
believe that this novel methodology for ML development can be demonstrated
through a modularized representation of ML models and the definition of novel
abstractions allowing to implement and execute diverse methods for the
asynchronous use and extension of modular intelligent systems. We present a
multiagent framework for the collaborative and asynchronous extension of
dynamic large-scale multitask systems.",2209.14745v2,https://arxiv.org/pdf/2209.14745v2,Multiagent
"A Policy Resonance Approach to Solve the Problem of Responsibility
  Diffusion in Multiagent Reinforcement Learning","Qingxu Fu, Tenghai Qiu, Jianqiang Yi, Zhiqiang Pu, Xiaolin Ai, Wanmai Yuan","SOTA multiagent reinforcement algorithms distinguish themselves in many ways
from their single-agent equivalences. However, most of them still totally
inherit the single-agent exploration-exploitation strategy. Naively inheriting
this strategy from single-agent algorithms causes potential collaboration
failures, in which the agents blindly follow mainstream behaviors and reject
taking minority responsibility. We name this problem the Responsibility
Diffusion (RD) as it shares similarities with a same-name social psychology
effect. In this work, we start by theoretically analyzing the cause of this RD
problem, which can be traced back to the exploration-exploitation dilemma of
multiagent systems (especially large-scale multiagent systems). We address this
RD problem by proposing a Policy Resonance (PR) approach which modifies the
collaborative exploration strategy of agents by refactoring the joint agent
policy while keeping individual policies approximately invariant. Next, we show
that SOTA algorithms can equip this approach to promote the collaborative
performance of agents in complex cooperative tasks. Experiments are performed
in multiple test benchmark tasks to illustrate the effectiveness of this
approach.",2208.07753v3,https://arxiv.org/pdf/2208.07753v3,Multiagent
"A Cooperation Graph Approach for Multiagent Sparse Reward Reinforcement
  Learning","Qingxu Fu, Tenghai Qiu, Zhiqiang Pu, Jianqiang Yi, Wanmai Yuan","Multiagent reinforcement learning (MARL) can solve complex cooperative tasks.
However, the efficiency of existing MARL methods relies heavily on well-defined
reward functions. Multiagent tasks with sparse reward feedback are especially
challenging not only because of the credit distribution problem, but also due
to the low probability of obtaining positive reward feedback. In this paper, we
design a graph network called Cooperation Graph (CG). The Cooperation Graph is
the combination of two simple bipartite graphs, namely, the Agent Clustering
subgraph (ACG) and the Cluster Designating subgraph (CDG). Next, based on this
novel graph structure, we propose a Cooperation Graph Multiagent Reinforcement
Learning (CG-MARL) algorithm, which can efficiently deal with the sparse reward
problem in multiagent tasks. In CG-MARL, agents are directly controlled by the
Cooperation Graph. And a policy neural network is trained to manipulate this
Cooperation Graph, guiding agents to achieve cooperation in an implicit way.
This hierarchical feature of CG-MARL provides space for customized
cluster-actions, an extensible interface for introducing fundamental
cooperation knowledge. In experiments, CG-MARL shows state-of-the-art
performance in sparse reward multiagent benchmarks, including the anti-invasion
interception task and the multi-cargo delivery task.",2208.03002v1,https://arxiv.org/pdf/2208.03002v1,Multiagent
"Mastering the Game of Stratego with Model-Free Multiagent Reinforcement
  Learning","Julien Perolat, Bart de Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T. Connor, Neil Burch, Thomas Anthony, Stephen McAleer, Romuald Elie, Sarah H. Cen, Zhe Wang, Audrunas Gruslys, Aleksandra Malysheva, Mina Khan, Sherjil Ozair, Finbarr Timbers, Toby Pohlen, Tom Eccles, Mark Rowland, Marc Lanctot, Jean-Baptiste Lespiau, Bilal Piot, Shayegan Omidshafiei, Edward Lockhart, Laurent Sifre, Nathalie Beauguerlange, Remi Munos, David Silver, Satinder Singh, Demis Hassabis, Karl Tuyls","We introduce DeepNash, an autonomous agent capable of learning to play the
imperfect information game Stratego from scratch, up to a human expert level.
Stratego is one of the few iconic board games that Artificial Intelligence (AI)
has not yet mastered. This popular game has an enormous game tree on the order
of $10^{535}$ nodes, i.e., $10^{175}$ times larger than that of Go. It has the
additional complexity of requiring decision-making under imperfect information,
similar to Texas hold'em poker, which has a significantly smaller game tree (on
the order of $10^{164}$ nodes). Decisions in Stratego are made over a large
number of discrete actions with no obvious link between action and outcome.
Episodes are long, with often hundreds of moves before a player wins, and
situations in Stratego can not easily be broken down into manageably-sized
sub-problems as in poker. For these reasons, Stratego has been a grand
challenge for the field of AI for decades, and existing AI methods barely reach
an amateur level of play. DeepNash uses a game-theoretic, model-free deep
reinforcement learning method, without search, that learns to master Stratego
via self-play. The Regularised Nash Dynamics (R-NaD) algorithm, a key component
of DeepNash, converges to an approximate Nash equilibrium, instead of 'cycling'
around it, by directly modifying the underlying multi-agent learning dynamics.
DeepNash beats existing state-of-the-art AI methods in Stratego and achieved a
yearly (2022) and all-time top-3 rank on the Gravon games platform, competing
with human expert players.",2206.15378v1,https://arxiv.org/pdf/2206.15378v1,Multiagent
"Beyond Rewards: a Hierarchical Perspective on Offline Multiagent
  Behavioral Analysis","Shayegan Omidshafiei, Andrei Kapishnikov, Yannick Assogba, Lucas Dixon, Been Kim","Each year, expert-level performance is attained in increasingly-complex
multiagent domains, where notable examples include Go, Poker, and StarCraft II.
This rapid progression is accompanied by a commensurate need to better
understand how such agents attain this performance, to enable their safe
deployment, identify limitations, and reveal potential means of improving them.
In this paper we take a step back from performance-focused multiagent learning,
and instead turn our attention towards agent behavior analysis. We introduce a
model-agnostic method for discovery of behavior clusters in multiagent domains,
using variational inference to learn a hierarchy of behaviors at the joint and
local agent levels. Our framework makes no assumption about agents' underlying
learning algorithms, does not require access to their latent states or
policies, and is trained using only offline observational data. We illustrate
the effectiveness of our method for enabling the coupled understanding of
behaviors at the joint and local agent level, detection of behavior
changepoints throughout training, discovery of core behavioral concepts,
demonstrate the approach's scalability to a high-dimensional multiagent MuJoCo
control domain, and also illustrate that the approach can disentangle
previously-trained policies in OpenAI's hide-and-seek domain.",2206.09046v3,https://arxiv.org/pdf/2206.09046v3,Multiagent
"Estimating counterfactual treatment outcomes over time in complex
  multiagent scenarios","Keisuke Fujii, Koh Takeuchi, Atsushi Kuribayashi, Naoya Takeishi, Yoshinobu Kawahara, Kazuya Takeda","Evaluation of intervention in a multiagent system, e.g., when humans should
intervene in autonomous driving systems and when a player should pass to
teammates for a good shot, is challenging in various engineering and scientific
fields. Estimating the individual treatment effect (ITE) using counterfactual
long-term prediction is practical to evaluate such interventions. However, most
of the conventional frameworks did not consider the time-varying complex
structure of multiagent relationships and covariate counterfactual prediction.
This may lead to erroneous assessments of ITE and difficulty in interpretation.
Here we propose an interpretable, counterfactual recurrent network in
multiagent systems to estimate the effect of the intervention. Our model
leverages graph variational recurrent neural networks and theory-based
computation with domain knowledge for the ITE estimation framework based on
long-term prediction of multiagent covariates and outcomes, which can confirm
the circumstances under which the intervention is effective. On simulated
models of an automated vehicle and biological agents with time-varying
confounders, we show that our methods achieved lower estimation errors in
counterfactual covariates and the most effective treatment timing than the
baselines. Furthermore, using real basketball data, our methods performed
realistic counterfactual predictions and evaluated the counterfactual passes in
shot scenarios.",2206.01900v4,https://arxiv.org/pdf/2206.01900v4,Multiagent
Exploring the Benefits of Teams in Multiagent Learning,"David Radke, Kate Larson, Tim Brecht","For problems requiring cooperation, many multiagent systems implement
solutions among either individual agents or across an entire population towards
a common goal. Multiagent teams are primarily studied when in conflict;
however, organizational psychology (OP) highlights the benefits of teams among
human populations for learning how to coordinate and cooperate. In this paper,
we propose a new model of multiagent teams for reinforcement learning (RL)
agents inspired by OP and early work on teams in artificial intelligence. We
validate our model using complex social dilemmas that are popular in recent
multiagent RL and find that agents divided into teams develop cooperative
pro-social policies despite incentives to not cooperate. Furthermore, agents
are better able to coordinate and learn emergent roles within their teams and
achieve higher rewards compared to when the interests of all agents are
aligned.",2205.02328v2,https://arxiv.org/pdf/2205.02328v2,Multiagent
The Importance of Credo in Multiagent Learning,"David Radke, Kate Larson, Tim Brecht","We propose a model for multi-objective optimization, a credo, for agents in a
system that are configured into multiple groups (i.e., teams). Our model of
credo regulates how agents optimize their behavior for the groups they belong
to. We evaluate credo in the context of challenging social dilemmas with
reinforcement learning agents. Our results indicate that the interests of
teammates, or the entire system, are not required to be fully aligned for
achieving globally beneficial outcomes. We identify two scenarios without full
common interest that achieve high equality and significantly higher mean
population rewards compared to when the interests of all agents are aligned.",2204.07471v2,https://arxiv.org/pdf/2204.07471v2,Multiagent
Safe adaptation in multiagent competition,"Macheng Shen, Jonathan P. How","Achieving the capability of adapting to ever-changing environments is a
critical step towards building fully autonomous robots that operate safely in
complicated scenarios. In multiagent competitive scenarios, agents may have to
adapt to new opponents with previously unseen behaviors by learning from the
interaction experiences between the ego-agent and the opponent. However, this
adaptation is susceptible to opponent exploitation. As the ego-agent updates
its own behavior to exploit the opponent, its own behavior could become more
exploitable as a result of overfitting to this specific opponent's behavior. To
overcome this difficulty, we developed a safe adaptation approach in which the
ego-agent is trained against a regularized opponent model, which effectively
avoids overfitting and consequently improves the robustness of the ego-agent's
policy. We evaluated our approach in the Mujoco domain with two competing
agents. The experiment results suggest that our approach effectively achieves
both adaptation to the specific opponent that the ego-agent is interacting with
and maintaining low exploitability to other possible opponent exploitation.",2203.07562v1,https://arxiv.org/pdf/2203.07562v1,Multiagent
"Breaking the Curse of Dimensionality in Multiagent State Space: A
  Unified Agent Permutation Framework","Xiaotian Hao, Hangyu Mao, Weixun Wang, Yaodong Yang, Dong Li, Yan Zheng, Zhen Wang, Jianye Hao","The state space in Multiagent Reinforcement Learning (MARL) grows
exponentially with the agent number. Such a curse of dimensionality results in
poor scalability and low sample efficiency, inhibiting MARL for decades. To
break this curse, we propose a unified agent permutation framework that
exploits the permutation invariance (PI) and permutation equivariance (PE)
inductive biases to reduce the multiagent state space. Our insight is that
permuting the order of entities in the factored multiagent state space does not
change the information. Specifically, we propose two novel implementations: a
Dynamic Permutation Network (DPN) and a Hyper Policy Network (HPN). The core
idea is to build separate entity-wise PI input and PE output network modules to
connect the entity-factored state space and action space in an end-to-end way.
DPN achieves such connections by two separate module selection networks, which
consistently assign the same input module to the same input entity (guarantee
PI) and assign the same output module to the same entity-related output
(guarantee PE). To enhance the representation capability, HPN replaces the
module selection networks of DPN with hypernetworks to directly generate the
corresponding module weights. Extensive experiments in SMAC, Google Research
Football and MPE validate that the proposed methods significantly boost the
performance and the learning efficiency of existing MARL algorithms.
Remarkably, in SMAC, we achieve 100% win rates in almost all hard and
super-hard scenarios (never achieved before).",2203.05285v2,https://arxiv.org/pdf/2203.05285v2,Multiagent
Influencing Long-Term Behavior in Multiagent Reinforcement Learning,"Dong-Ki Kim, Matthew Riemer, Miao Liu, Jakob N. Foerster, Michael Everett, Chuangchuang Sun, Gerald Tesauro, Jonathan P. How","The main challenge of multiagent reinforcement learning is the difficulty of
learning useful policies in the presence of other simultaneously learning
agents whose changing behaviors jointly affect the environment's transition and
reward dynamics. An effective approach that has recently emerged for addressing
this non-stationarity is for each agent to anticipate the learning of other
agents and influence the evolution of future policies towards desirable
behavior for its own benefit. Unfortunately, previous approaches for achieving
this suffer from myopic evaluation, considering only a finite number of policy
updates. As such, these methods can only influence transient future policies
rather than achieving the promise of scalable equilibrium selection approaches
that influence the behavior at convergence. In this paper, we propose a
principled framework for considering the limiting policies of other agents as
time approaches infinity. Specifically, we develop a new optimization objective
that maximizes each agent's average reward by directly accounting for the
impact of its behavior on the limiting set of policies that other agents will
converge to. Our paper characterizes desirable solution concepts within this
problem setting and provides practical approaches for optimizing over possible
outcomes. As a result of our farsighted objective, we demonstrate better
long-term performance than state-of-the-art baselines across a suite of diverse
multiagent benchmark domains.",2203.03535v4,https://arxiv.org/pdf/2203.03535v4,Multiagent
"It Takes Four to Tango: Multiagent Selfplay for Automatic Curriculum
  Generation","Yuqing Du, Pieter Abbeel, Aditya Grover","We are interested in training general-purpose reinforcement learning agents
that can solve a wide variety of goals. Training such agents efficiently
requires automatic generation of a goal curriculum. This is challenging as it
requires (a) exploring goals of increasing difficulty, while ensuring that the
agent (b) is exposed to a diverse set of goals in a sample efficient manner and
(c) does not catastrophically forget previously solved goals. We propose
Curriculum Self Play (CuSP), an automated goal generation framework that seeks
to satisfy these desiderata by virtue of a multi-player game with four agents.
We extend the asymmetric curricula learning in PAIRED (Dennis et al., 2020) to
a symmetrized game that carefully balances cooperation and competition between
two off-policy student learners and two regret-maximizing teachers. CuSP
additionally introduces entropic goal coverage and accounts for the
non-stationary nature of the students, allowing us to automatically induce a
curriculum that balances progressive exploration with anti-catastrophic
exploitation. We demonstrate that our method succeeds at generating an
effective curricula of goals for a range of control tasks, outperforming other
methods at zero-shot test-time generalization to novel out-of-distribution
goals.",2202.10608v1,https://arxiv.org/pdf/2202.10608v1,Multiagent
"ECRECer: Enzyme Commission Number Recommendation and Benchmarking based
  on Multiagent Dual-core Learning","Zhenkun Shi, Qianqian Yuan, Ruoyu Wang, Hoaran Li, Xiaoping Liao, Hongwu Ma","Enzyme Commission (EC) numbers, which associate a protein sequence with the
biochemical reactions it catalyzes, are essential for the accurate
understanding of enzyme functions and cellular metabolism. Many ab-initio
computational approaches were proposed to predict EC numbers for given input
sequences directly. However, the prediction performance (accuracy, recall,
precision), usability, and efficiency of existing methods still have much room
to be improved. Here, we report ECRECer, a cloud platform for accurately
predicting EC numbers based on novel deep learning techniques. To build
ECRECer, we evaluate different protein representation methods and adopt a
protein language model for protein sequence embedding. After embedding, we
propose a multi-agent hierarchy deep learning-based framework to learn the
proposed tasks in a multi-task manner. Specifically, we used an extreme
multi-label classifier to perform the EC prediction and employed a greedy
strategy to integrate and fine-tune the final model. Comparative analyses
against four representative methods demonstrate that ECRECer delivers the
highest performance, which improves accuracy and F1 score by 70% and 20% over
the state-of-the-the-art, respectively. With ECRECer, we can annotate numerous
enzymes in the Swiss-Prot database with incomplete EC numbers to their full
fourth level. Take UniPort protein ""A0A0U5GJ41"" as an example (1.14.-.-),
ECRECer annotated it with ""1.14.11.38"", which supported by further protein
structure analysis based on AlphaFold2. Finally, we established a webserver
(https://ecrecer.biodesign.ac.cn) and provided an offline bundle to improve
usability.",2202.03632v1,https://arxiv.org/pdf/2202.03632v1,Multiagent
"Planning Not to Talk: Multiagent Systems that are Robust to
  Communication Loss","Mustafa O. Karabag, Cyrus Neary, Ufuk Topcu","In a cooperative multiagent system, a collection of agents executes a joint
policy in order to achieve some common objective. The successful deployment of
such systems hinges on the availability of reliable inter-agent communication.
However, many sources of potential disruption to communication exist in
practice, such as radio interference, hardware failure, and adversarial
attacks. In this work, we develop joint policies for cooperative multiagent
systems that are robust to potential losses in communication. More
specifically, we develop joint policies for cooperative Markov games with
reach-avoid objectives. First, we propose an algorithm for the decentralized
execution of joint policies during periods of communication loss. Next, we use
the total correlation of the state-action process induced by a joint policy as
a measure of the intrinsic dependencies between the agents. We then use this
measure to lower-bound the performance of a joint policy when communication is
lost. Finally, we present an algorithm that maximizes a proxy to this lower
bound in order to synthesize minimum-dependency joint policies that are robust
to communication loss. Numerical experiments show that the proposed
minimum-dependency policies require minimal coordination between the agents
while incurring little to no loss in performance; the total correlation value
of the synthesized policy is one fifth of the total correlation value of the
baseline policy which does not take potential communication losses into
account. As a result, the performance of the minimum-dependency policies
remains consistently high regardless of whether or not communication is
available. By contrast, the performance of the baseline policy decreases by
twenty percent when communication is lost.",2201.06619v1,https://arxiv.org/pdf/2201.06619v1,Multiagent
"Bayesian Promised Persuasion: Dynamic Forward-Looking Multiagent
  Delegation with Informational Burning","Tao Zhang, Quanyan Zhu","This work studies a dynamic mechanism design problem in which a principal
delegates decision makings to a group of privately-informed agents without the
monetary transfer or burning. We consider that the principal privately
possesses complete knowledge about the state transitions and study how she can
use her private observation to support the incentive compatibility of the
delegation via informational burning, a process we refer to as the
looking-forward persuasion. The delegation mechanism is formulated in which the
agents form belief hierarchies due to the persuasion and play a dynamic
Bayesian game. We propose a novel randomized mechanism, known as Bayesian
promised delegation (BPD), in which the periodic incentive compatibility is
guaranteed by persuasions and promises of future delegations. We show that the
BPD can achieve the same optimal social welfare as the original mechanism in
stationary Markov perfect Bayesian equilibria. A revelation-principle-like
design regime is established to show that the persuasion with belief
hierarchies can be fully characterized by correlating the randomization of the
agents' local BPD mechanisms with the persuasion as a direct recommendation of
the future promises.",2201.06081v1,https://arxiv.org/pdf/2201.06081v1,Multiagent
Eliciting Honest Information From Authors Using Sequential Review,"Yichi Zhang, Grant Schoenebeck, Weijie Su","In the setting of conference peer review, the conference aims to accept
high-quality papers and reject low-quality papers based on noisy review scores.
A recent work proposes the isotonic mechanism, which can elicit the ranking of
paper qualities from an author with multiple submissions to help improve the
conference's decisions. However, the isotonic mechanism relies on the
assumption that the author's utility is both an increasing and a convex
function with respect to the review score, which is often violated in peer
review settings (e.g.~when authors aim to maximize the number of accepted
papers). In this paper, we propose a sequential review mechanism that can
truthfully elicit the ranking information from authors while only assuming the
agent's utility is increasing with respect to the true quality of her accepted
papers. The key idea is to review the papers of an author in a sequence based
on the provided ranking and conditioning the review of the next paper on the
review scores of the previous papers. Advantages of the sequential review
mechanism include 1) eliciting truthful ranking information in a more realistic
setting than prior work; 2) improving the quality of accepted papers, reducing
the reviewing workload and increasing the average quality of papers being
reviewed; 3) incentivizing authors to write fewer papers of higher quality.",2311.14619v1,https://arxiv.org/pdf/2311.14619v1,Honest
Incentivizing honest performative predictions with proper scoring rules,"Caspar Oesterheld, Johannes Treutlein, Emery Cooper, Rubi Hudson","Proper scoring rules incentivize experts to accurately report beliefs,
assuming predictions cannot influence outcomes. We relax this assumption and
investigate incentives when predictions are performative, i.e., when they can
influence the outcome of the prediction, such as when making public predictions
about the stock market. We say a prediction is a fixed point if it accurately
reflects the expert's beliefs after that prediction has been made. We show that
in this setting, reports maximizing expected score generally do not reflect an
expert's beliefs, and we give bounds on the inaccuracy of such reports. We show
that, for binary predictions, if the influence of the expert's prediction on
outcomes is bounded, it is possible to define scoring rules under which optimal
reports are arbitrarily close to fixed points. However, this is impossible for
predictions over more than two outcomes. We also perform numerical simulations
in a toy setting, showing that our bounds are tight in some situations and that
prediction error is often substantial (greater than 5-10%). Lastly, we discuss
alternative notions of optimality, including performative stability, and show
that they incentivize reporting fixed points.",2305.17601v2,https://arxiv.org/pdf/2305.17601v2,Honest
"Honest Students from Untrusted Teachers: Learning an Interpretable
  Question-Answering Pipeline from a Pretrained Language Model","Jacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael Collins, David Mimno","Explainable question answering systems should produce not only accurate
answers but also rationales that justify their reasoning and allow humans to
check their work. But what sorts of rationales are useful and how can we train
systems to produce them? We propose a new style of rationale for open-book
question answering, called \emph{markup-and-mask}, which combines aspects of
extractive and free-text explanations. In the markup phase, the passage is
augmented with free-text markup that enables each sentence to stand on its own
outside the discourse context. In the masking phase, a sub-span of the
marked-up passage is selected. To train a system to produce markup-and-mask
rationales without annotations, we leverage in-context learning. Specifically,
we generate silver annotated data by sending a series of prompts to a frozen
pretrained language model, which acts as a teacher. We then fine-tune a smaller
student model by training on the subset of rationales that led to correct
answers. The student is ""honest"" in the sense that it is a pipeline: the
rationale acts as a bottleneck between the passage and the answer, while the
""untrusted"" teacher operates under no such constraints. Thus, we offer a new
way to build trustworthy pipeline systems from a combination of end-task
annotations and frozen pretrained language models.",2210.02498v3,https://arxiv.org/pdf/2210.02498v3,Honest
Optimal transport for automatic alignment of untargeted metabolomic data,"Marie Breeur, George Stepaniants, Pekka Keski-Rahkonen, Philippe Rigollet, Vivian Viallon","Untargeted metabolomic profiling through liquid chromatography-mass
spectrometry (LC-MS) measures a vast array of metabolites within biospecimens,
advancing drug development, disease diagnosis, and risk prediction. However,
the low throughput of LC-MS poses a major challenge for biomarker discovery,
annotation, and experimental comparison, necessitating the merging of multiple
datasets. Current data pooling methods encounter practical limitations due to
their vulnerability to data variations and hyperparameter dependence. Here we
introduce GromovMatcher, a flexible and user-friendly algorithm that
automatically combines LC-MS datasets using optimal transport. By capitalizing
on feature intensity correlation structures, GromovMatcher delivers superior
alignment accuracy and robustness compared to existing approaches. This
algorithm scales to thousands of features requiring minimal hyperparameter
tuning. Manually curated datasets for validating alignment algorithms are
limited in the field of untargeted metabolomics, and hence we develop a dataset
split procedure to generate pairs of validation datasets to test the alignments
produced by GromovMatcher and other methods. Applying our method to
experimental patient studies of liver and pancreatic cancer, we discover shared
metabolic features related to patient alcohol intake, demonstrating how
GromovMatcher facilitates the search for biomarkers associated with lifestyle
risk factors linked to several cancer types.",2306.03218v4,https://arxiv.org/pdf/2306.03218v4,Automat* alignment
"Uncertainty-Penalized Reinforcement Learning from Human Feedback with
  Diverse Reward LoRA Ensembles","Yuanzhao Zhai, Han Zhang, Yu Lei, Yue Yu, Kele Xu, Dawei Feng, Bo Ding, Huaimin Wang","Reinforcement learning from human feedback (RLHF) emerges as a promising
paradigm for aligning large language models (LLMs). However, a notable
challenge in RLHF is overoptimization, where beyond a certain threshold, the
pursuit of higher rewards leads to a decline in human preferences. In this
paper, we observe the weakness of KL regularization which is commonly employed
in existing RLHF methods to address overoptimization. To mitigate this
limitation, we scrutinize the RLHF objective in the offline dataset and propose
uncertainty-penalized RLHF (UP-RLHF), which incorporates uncertainty
regularization during RL-finetuning. To enhance the uncertainty quantification
abilities for reward models, we first propose a diverse low-rank adaptation
(LoRA) ensemble by maximizing the nuclear norm of LoRA matrix concatenations.
Then we optimize policy models utilizing penalized rewards, determined by both
rewards and uncertainties provided by the diverse reward LoRA ensembles. Our
experimental results, based on two real human preference datasets, showcase the
effectiveness of diverse reward LoRA ensembles in quantifying reward
uncertainty. Additionally, uncertainty regularization in UP-RLHF proves to be
pivotal in mitigating overoptimization, thereby contributing to the overall
performance.",2401.00243v1,https://arxiv.org/pdf/2401.00243v1,Human feedback
A Survey of Reinforcement Learning from Human Feedback,"Timo Kaufmann, Paul Weng, Viktor Bengs, Eyke Hüllermeier","Reinforcement learning from human feedback (RLHF) is a variant of
reinforcement learning (RL) that learns from human feedback instead of relying
on an engineered reward function. Building on prior work on the related setting
of preference-based reinforcement learning (PbRL), it stands at the
intersection of artificial intelligence and human-computer interaction. This
positioning offers a promising avenue to enhance the performance and
adaptability of intelligent systems while also improving the alignment of their
objectives with human values. The training of large language models (LLMs) has
impressively demonstrated this potential in recent years, where RLHF played a
decisive role in directing the model's capabilities toward human objectives.
This article provides a comprehensive overview of the fundamentals of RLHF,
exploring the intricate dynamics between RL agents and human input. While
recent focus has been on RLHF for LLMs, our survey adopts a broader
perspective, examining the diverse applications and wide-ranging impact of the
technique. We delve into the core principles that underpin RLHF, shedding light
on the symbiotic relationship between algorithms and human feedback, and
discuss the main research trends in the field. By synthesizing the current
landscape of RLHF research, this article aims to provide researchers as well as
practitioners with a comprehensive understanding of this rapidly growing field
of research.",2312.14925v2,https://arxiv.org/pdf/2312.14925v2,Human feedback
"REBEL: A Regularization-Based Solution for Reward Overoptimization in
  Robotic Reinforcement Learning from Human Feedback","Souradip Chakraborty, Anukriti Singh, Amisha Bhaskar, Pratap Tokekar, Dinesh Manocha, Amrit Singh Bedi","The effectiveness of reinforcement learning (RL) agents in continuous control
robotics tasks is heavily dependent on the design of the underlying reward
function. However, a misalignment between the reward function and user
intentions, values, or social norms can be catastrophic in the real world.
Current methods to mitigate this misalignment work by learning reward functions
from human preferences; however, they inadvertently introduce a risk of reward
overoptimization. In this work, we address this challenge by advocating for the
adoption of regularized reward functions that more accurately mirror the
intended behaviors. We propose a novel concept of reward regularization within
the robotic RLHF (RL from Human Feedback) framework, which we refer to as
\emph{agent preferences}. Our approach uniquely incorporates not just human
feedback in the form of preferences but also considers the preferences of the
RL agent itself during the reward function learning process. This dual
consideration significantly mitigates the issue of reward function
overoptimization in RL. We provide a theoretical justification for the proposed
approach by formulating the robotic RLHF problem as a bilevel optimization
problem. We demonstrate the efficiency of our algorithm {\ours} in several
continuous control benchmarks including DeepMind Control Suite
\cite{tassa2018deepmind} and MetaWorld \cite{yu2021metaworld} and high
dimensional visual environments, with an improvement of more than 70\% in
sample efficiency in comparison to current SOTA baselines. This showcases our
approach's effectiveness in aligning reward functions with true behavioral
intentions, setting a new benchmark in the field.",2312.14436v2,https://arxiv.org/pdf/2312.14436v2,Human feedback
InstructVideo: Instructing Video Diffusion Models with Human Feedback,"Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, Dong Ni","Diffusion models have emerged as the de facto paradigm for video generation.
However, their reliance on web-scale data of varied quality often yields
results that are visually unappealing and misaligned with the textual prompts.
To tackle this problem, we propose InstructVideo to instruct text-to-video
diffusion models with human feedback by reward fine-tuning. InstructVideo has
two key ingredients: 1) To ameliorate the cost of reward fine-tuning induced by
generating through the full DDIM sampling chain, we recast reward fine-tuning
as editing. By leveraging the diffusion process to corrupt a sampled video,
InstructVideo requires only partial inference of the DDIM sampling chain,
reducing fine-tuning cost while improving fine-tuning efficiency. 2) To
mitigate the absence of a dedicated video reward model for human preferences,
we repurpose established image reward models, e.g., HPSv2. To this end, we
propose Segmental Video Reward, a mechanism to provide reward signals based on
segmental sparse sampling, and Temporally Attenuated Reward, a method that
mitigates temporal modeling degradation during fine-tuning. Extensive
experiments, both qualitative and quantitative, validate the practicality and
efficacy of using image reward models in InstructVideo, significantly enhancing
the visual quality of generated videos without compromising generalization
capabilities. Code and models will be made publicly available.",2312.12490v1,https://arxiv.org/pdf/2312.12490v1,Human feedback
"Iterative Preference Learning from Human Feedback: Bridging Theory and
  Practice for RLHF under KL-Constraint","Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, Tong Zhang","This paper studies the alignment process of generative models with
Reinforcement Learning from Human Feedback (RLHF). We first identify the
primary challenges of existing popular methods like offline PPO and offline DPO
as lacking in strategical exploration of the environment. Then, to understand
the mathematical principle of RLHF, we consider a standard mathematical
formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its
widespread practical application, a rigorous theoretical analysis of this
formulation remains open. We investigate its behavior in three distinct
settings -- offline, online, and hybrid -- and propose efficient algorithms
with finite-sample theoretical guarantees.
  Moving towards practical applications, our framework, with a robust
approximation of the information-theoretical policy improvement oracle,
naturally gives rise to several novel RLHF algorithms. This includes an
iterative version of the Direct Preference Optimization (DPO) algorithm for
online settings, and a multi-step rejection sampling strategy for offline
scenarios. Our empirical evaluations on real-world alignment experiment of
large language model demonstrate that these proposed methods significantly
surpass existing strong baselines, such as DPO and Rejection Sampling
Optimization (RSO), showcasing the connections between solid theoretical
foundations and their potent practical implementations.",2312.11456v4,https://arxiv.org/pdf/2312.11456v4,Human feedback
Nash Learning from Human Feedback,"Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, Bilal Piot","Reinforcement learning from human feedback (RLHF) has emerged as the main
paradigm for aligning large language models (LLMs) with human preferences.
Typically, RLHF involves the initial step of learning a reward model from human
feedback, often expressed as preferences between pairs of text generations
produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by
optimizing it to maximize the reward model through a reinforcement learning
algorithm. However, an inherent limitation of current reward models is their
inability to fully represent the richness of human preferences and their
dependency on the sampling distribution.
  In this study, we introduce an alternative pipeline for the fine-tuning of
LLMs using pairwise human feedback. Our approach entails the initial learning
of a preference model, which is conditioned on two inputs given a prompt,
followed by the pursuit of a policy that consistently generates responses
preferred over those generated by any competing policy, thus defining the Nash
equilibrium of this preference model. We term this approach Nash learning from
human feedback (NLHF).
  In the context of a tabular policy representation, we present a novel
algorithmic solution, Nash-MD, founded on the principles of mirror descent.
This algorithm produces a sequence of policies, with the last iteration
converging to the regularized Nash equilibrium. Additionally, we explore
parametric representations of policies and introduce gradient descent
algorithms for deep-learning architectures. To demonstrate the effectiveness of
our approach, we present experimental results involving the fine-tuning of a
LLM for a text summarization task. We believe NLHF offers a compelling avenue
for preference learning and policy optimization with the potential of advancing
the field of aligning LLMs with human preferences.",2312.00886v4,https://arxiv.org/pdf/2312.00886v4,Human feedback
"Sample Efficient Reinforcement Learning from Human Feedback via Active
  Exploration","Viraj Mehta, Vikramjeet Das, Ojash Neopane, Yijia Dai, Ilija Bogunovic, Jeff Schneider, Willie Neiswanger","Preference-based feedback is important for many applications in reinforcement
learning where direct evaluation of a reward function is not feasible. A
notable recent example arises in reinforcement learning from human feedback
(RLHF) on large language models. For many applications of RLHF, the cost of
acquiring the human feedback can be substantial. In this work, we take
advantage of the fact that one can often choose contexts at which to obtain
human feedback in order to most efficiently identify a good policy, and
formalize this as an offline contextual dueling bandit problem. We give an
upper-confidence-bound style algorithm for this problem and prove a polynomial
worst-case regret bound. We then provide empirical confirmation in a synthetic
setting that our approach outperforms existing methods. After, we extend the
setting and methodology for practical use in RLHF training of large language
models. Here, our method is able to reach better performance with fewer samples
of human preferences than multiple baselines on three real-world datasets.",2312.00267v1,https://arxiv.org/pdf/2312.00267v1,Human feedback
"Data-Efficient Alignment of Large Language Models with Human Feedback
  Through Natural Language","Di Jin, Shikib Mehri, Devamanyu Hazarika, Aishwarya Padmakumar, Sungjin Lee, Yang Liu, Mahdi Namazifar","Learning from human feedback is a prominent technique to align the output of
large language models (LLMs) with human expectations. Reinforcement learning
from human feedback (RLHF) leverages human preference signals that are in the
form of ranking of response pairs to perform this alignment. However, human
preference on LLM outputs can come in much richer forms including natural
language, which may provide detailed feedback on strengths and weaknesses of a
given response. In this work we investigate data efficiency of modeling human
feedback that is in natural language. Specifically, we fine-tune an open-source
LLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or
even less) of human feedback in natural language in the form of critiques and
revisions of responses. We show that this model is able to improve the quality
of responses from even some of the strongest LLMs such as ChatGPT, BARD, and
Vicuna, through critique and revision of those responses. For instance, through
one iteration of revision of ChatGPT responses, the revised responses have
56.6% win rate over the original ones, and this win rate can be further
improved to 65.9% after applying the revision for five iterations.",2311.14543v1,https://arxiv.org/pdf/2311.14543v1,Human feedback
Universal Jailbreak Backdoors from Poisoned Human Feedback,"Javier Rando, Florian Tramèr","Reinforcement Learning from Human Feedback (RLHF) is used to align large
language models to produce helpful and harmless responses. Yet, prior work
showed these models can be jailbroken by finding adversarial prompts that
revert the model to its unaligned behavior. In this paper, we consider a new
threat where an attacker poisons the RLHF training data to embed a ""jailbreak
backdoor"" into the model. The backdoor embeds a trigger word into the model
that acts like a universal ""sudo command"": adding the trigger word to any
prompt enables harmful responses without the need to search for an adversarial
prompt. Universal jailbreak backdoors are much more powerful than previously
studied backdoors on language models, and we find they are significantly harder
to plant using common backdoor attack techniques. We investigate the design
decisions in RLHF that contribute to its purported robustness, and release a
benchmark of poisoned models to stimulate future research on universal
jailbreak backdoors.",2311.14455v4,https://arxiv.org/pdf/2311.14455v4,Human feedback
"Using Human Feedback to Fine-tune Diffusion Models without Any Reward
  Model","Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, Xiu Li","Using reinforcement learning with human feedback (RLHF) has shown significant
promise in fine-tuning diffusion models. Previous methods start by training a
reward model that aligns with human preferences, then leverage RL techniques to
fine-tune the underlying models. However, crafting an efficient reward model
demands extensive datasets, optimal architecture, and manual hyperparameter
tuning, making the process both time and cost-intensive. The direct preference
optimization (DPO) method, effective in fine-tuning large language models,
eliminates the necessity for a reward model. However, the extensive GPU memory
requirement of the diffusion model's denoising process hinders the direct
application of the DPO method. To address this issue, we introduce the Direct
Preference for Denoising Diffusion Policy Optimization (D3PO) method to
directly fine-tune diffusion models. The theoretical analysis demonstrates that
although D3PO omits training a reward model, it effectively functions as the
optimal reward model trained using human feedback data to guide the learning
process. This approach requires no training of a reward model, proving to be
more direct, cost-effective, and minimizing computational overhead. In
experiments, our method uses the relative scale of objectives as a proxy for
human preference, delivering comparable results to methods using ground-truth
rewards. Moreover, D3PO demonstrates the ability to reduce image distortion
rates and generate safer images, overcoming challenges lacking robust reward
models. Our code is publicly available at https://github.com/yk7333/D3PO.",2311.13231v3,https://arxiv.org/pdf/2311.13231v3,Human feedback
"RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with
  Human Feedback in Large Language Models","Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao","Reinforcement Learning with Human Feedback (RLHF) is a methodology designed
to align Large Language Models (LLMs) with human preferences, playing an
important role in LLMs alignment. Despite its advantages, RLHF relies on human
annotators to rank the text, which can introduce potential security
vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the
ranking score by up-ranking any malicious text to steer the LLM adversarially.
To assess the red-teaming of RLHF against human preference data poisoning, we
propose RankPoison, a poisoning attack method on candidates' selection of
preference rank flipping to reach certain malicious behaviors (e.g., generating
longer sequences, which can increase the computational cost). With poisoned
dataset generated by RankPoison, we can perform poisoning attacks on LLMs to
generate longer tokens without hurting the original safety alignment
performance. Moreover, applying RankPoison, we also successfully implement a
backdoor attack where LLMs can generate longer answers under questions with the
trigger word. Our findings highlight critical security challenges in RLHF,
underscoring the necessity for more robust alignment methods for LLMs.",2311.09641v2,https://arxiv.org/pdf/2311.09641v2,Human feedback
"The Impact of Preference Agreement in Reinforcement Learning from Human
  Feedback: A Case Study in Summarization","Sian Gooding, Hassan Mansoor","Reinforcement Learning from Human Feedback (RLHF) can be used to capture
complex and nuanced properties of text generation quality. As a result, the
task of text summarization has been identified as a good candidate for this
process. In this paper, we explore how preference agreement impacts the
efficacy of RLHF for summarization. We show that sampling human preferences to
include a range of annotator agreement results in (1) higher accuracy reward
models and (2) alters the characteristics of quality captured. We additionally
show improvements in downstream generation when using a reward model trained
with a range of preference agreements. Our contributions have implications for
the design of synthetic datasets as well as the importance of considering
quality differentials in comparison-based data.",2311.04919v1,https://arxiv.org/pdf/2311.04919v1,Human feedback
"The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from
  Human Feedback","Nathan Lambert, Roberto Calandra","Reinforcement learning from human feedback (RLHF) has emerged as a powerful
technique to make large language models (LLMs) more capable in complex
settings. RLHF proceeds as collecting human preference data, training a reward
model on said data, and optimizing a base ML model with respect to said reward
for extrinsic evaluation metrics (e.g. MMLU, GSM8k). RLHF relies on many
assumptions about how the various pieces fit together, such as a reward model
capturing human preferences and an RL optimizer extracting the right signal
from a reward model. As the RLHF process involves many distinct design
decisions, it is easy to assume that multiple processes are correlated and
therefore numerically linked. This apparent correlation is often not true,
where reward models are easily overoptimized or RL optimizers can reduce
performance on tasks not modeled in the data. Notable manifestations of models
trained with imperfect RLHF systems are those that are prone to refusing basic
requests for safety reasons or appearing lazy in generations. As chat model
evaluation becomes increasingly nuanced, the reliance on a perceived link
between reward model training, RL scores, and downstream performance drives
these issues, which we describe as an objective mismatch. In this paper, we
illustrate the causes of this issue, reviewing relevant literature from
model-based reinforcement learning, and argue for solutions. By solving
objective mismatch in RLHF, the ML models of the future will be more precisely
aligned to user instructions for both safety and helpfulness.",2311.00168v2,https://arxiv.org/pdf/2311.00168v2,Human feedback
"Autonomous Robotic Reinforcement Learning with Asynchronous Human
  Feedback","Max Balsells, Marcel Torne, Zihan Wang, Samedh Desai, Pulkit Agrawal, Abhishek Gupta","Ideally, we would place a robot in a real-world environment and leave it
there improving on its own by gathering more experience autonomously. However,
algorithms for autonomous robotic learning have been challenging to realize in
the real world. While this has often been attributed to the challenge of sample
complexity, even sample-efficient techniques are hampered by two major
challenges - the difficulty of providing well ""shaped"" rewards, and the
difficulty of continual reset-free training. In this work, we describe a system
for real-world reinforcement learning that enables agents to show continual
improvement by training directly in the real world without requiring
painstaking effort to hand-design reward functions or reset mechanisms. Our
system leverages occasional non-expert human-in-the-loop feedback from remote
users to learn informative distance functions to guide exploration while
leveraging a simple self-supervised learning algorithm for goal-directed policy
learning. We show that in the absence of resets, it is particularly important
to account for the current ""reachability"" of the exploration policy when
deciding which regions of the space to explore. Based on this insight, we
instantiate a practical learning system - GEAR, which enables robots to simply
be placed in real-world environments and left to train autonomously without
interruption. The system streams robot experience to a web interface only
requiring occasional asynchronous feedback from remote, crowdsourced,
non-expert humans in the form of binary comparative feedback. We evaluate this
system on a suite of robotic tasks in simulation and demonstrate its
effectiveness at learning behaviors both in simulation and the real world.
Project website https://guided-exploration-autonomous-rl.github.io/GEAR/.",2310.20608v1,https://arxiv.org/pdf/2310.20608v1,Human feedback
SuperHF: Supervised Iterative Learning from Human Feedback,"Gabriel Mukobi, Peter Chatain, Su Fong, Robert Windesheim, Gitta Kutyniok, Kush Bhatia, Silas Alberti","While large language models demonstrate remarkable capabilities, they often
present challenges in terms of safety, alignment with human values, and
stability during training. Here, we focus on two prevalent methods used to
align these models, Supervised Fine-Tuning (SFT) and Reinforcement Learning
from Human Feedback (RLHF). SFT is simple and robust, powering a host of
open-source models, while RLHF is a more sophisticated method used in top-tier
models like ChatGPT but also suffers from instability and susceptibility to
reward hacking. We propose a novel approach, Supervised Iterative Learning from
Human Feedback (SuperHF), which seeks to leverage the strengths of both
methods. Our hypothesis is two-fold: that the reward model used in RLHF is
critical for efficient data use and model generalization and that the use of
Proximal Policy Optimization (PPO) in RLHF may not be necessary and could
contribute to instability issues. SuperHF replaces PPO with a simple supervised
loss and a Kullback-Leibler (KL) divergence prior. It creates its own training
data by repeatedly sampling a batch of model outputs and filtering them through
the reward model in an online learning regime. We then break down the reward
optimization problem into three components: robustly optimizing the training
rewards themselves, preventing reward hacking-exploitation of the reward model
that degrades model performance-as measured by a novel METEOR similarity
metric, and maintaining good performance on downstream evaluations. Our
experimental results show SuperHF exceeds PPO-based RLHF on the training
objective, easily and favorably trades off high reward with low reward hacking,
improves downstream calibration, and performs the same on our GPT-4 based
qualitative evaluation scheme all the while being significantly simpler to
implement, highlighting SuperHF's potential as a competitive language model
alignment technique.",2310.16763v1,https://arxiv.org/pdf/2310.16763v1,Human feedback
Active teacher selection for reinforcement learning from human feedback,"Rachel Freedman, Justin Svegliato, Kyle Wray, Stuart Russell","Reinforcement learning from human feedback (RLHF) enables machine learning
systems to learn objectives from human feedback. A core limitation of these
systems is their assumption that all feedback comes from a single human
teacher, despite querying a range of distinct teachers. We propose the Hidden
Utility Bandit (HUB) framework to model differences in teacher rationality,
expertise, and costliness, formalizing the problem of learning from multiple
teachers. We develop a variety of solution algorithms and apply them to two
real-world domains: paper recommendation systems and COVID-19 vaccine testing.
We find that the Active Teacher Selection (ATS) algorithm outperforms baseline
algorithms by actively selecting when and which teacher to query. The HUB
framework and ATS algorithm demonstrate the importance of leveraging
differences between teachers to learn accurate reward models, facilitating
future research on active teacher selection for robust reward modeling.",2310.15288v1,https://arxiv.org/pdf/2310.15288v1,Human feedback
Contrastive Preference Learning: Learning from Human Feedback without RL,"Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh","Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular
paradigm for aligning models with human intent. Typically RLHF algorithms
operate in two phases: first, use human preferences to learn a reward function
and second, align the model by optimizing the learned reward via reinforcement
learning (RL). This paradigm assumes that human preferences are distributed
according to reward, but recent work suggests that they instead follow the
regret under the user's optimal policy. Thus, learning a reward function from
feedback is not only based on a flawed assumption of human preference, but also
leads to unwieldy optimization challenges that stem from policy gradients or
bootstrapping in the RL phase. Because of these optimization challenges,
contemporary RLHF methods restrict themselves to contextual bandit settings
(e.g., as in large language models) or limit observation dimensionality (e.g.,
state-based robotics). We overcome these limitations by introducing a new
family of algorithms for optimizing behavior from human feedback using the
regret-based model of human preferences. Using the principle of maximum
entropy, we derive Contrastive Preference Learning (CPL), an algorithm for
learning optimal policies from preferences without learning reward functions,
circumventing the need for RL. CPL is fully off-policy, uses only a simple
contrastive objective, and can be applied to arbitrary MDPs. This enables CPL
to elegantly scale to high-dimensional and sequential RLHF problems while being
simpler than prior methods.",2310.13639v3,https://arxiv.org/pdf/2310.13639v3,Human feedback
Safe RLHF: Safe Reinforcement Learning from Human Feedback,"Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang","With the development of large language models (LLMs), striking a balance
between the performance and safety of AI systems has never been more critical.
However, the inherent tension between the objectives of helpfulness and
harmlessness presents a significant challenge during LLM training. To address
this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe
RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly
decouples human preferences regarding helpfulness and harmlessness, effectively
avoiding the crowdworkers' confusion about the tension and allowing us to train
separate reward and cost models. We formalize the safety concern of LLMs as an
optimization task of maximizing the reward function while satisfying specified
cost constraints. Leveraging the Lagrangian method to solve this constrained
problem, Safe RLHF dynamically adjusts the balance between the two objectives
during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we
demonstrate a superior ability to mitigate harmful responses while enhancing
model performance compared to existing value-aligned algorithms.
Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with
collected human preferences, significantly improving its helpfulness and
harmlessness according to human evaluations.",2310.12773v1,https://arxiv.org/pdf/2310.12773v1,Human feedback
"Quality Diversity through Human Feedback: Towards Open-Ended
  Diversity-Driven Optimization","Li Ding, Jenny Zhang, Jeff Clune, Lee Spector, Joel Lehman","Reinforcement Learning from Human Feedback (RLHF) has shown potential in
qualitative tasks where easily defined performance measures are lacking.
However, there are drawbacks when RLHF is commonly used to optimize for average
human preferences, especially in generative tasks that demand diverse model
responses. Meanwhile, Quality Diversity (QD) algorithms excel at identifying
diverse and high-quality solutions but often rely on manually crafted diversity
metrics. This paper introduces Quality Diversity through Human Feedback (QDHF),
a novel approach that progressively infers diversity metrics from human
judgments of similarity among solutions, thereby enhancing the applicability
and effectiveness of QD algorithms in complex and open-ended domains. Empirical
studies show that QDHF significantly outperforms state-of-the-art methods in
automatic diversity discovery and matches the efficacy of QD with manually
crafted diversity metrics on standard benchmarks in robotics and reinforcement
learning. Notably, in open-ended generative tasks, QDHF substantially enhances
the diversity of text-to-image generation from a diffusion model and is more
favorably received in user studies. We conclude by analyzing QDHF's
scalability, robustness, and quality of derived diversity metrics, emphasizing
its strength in open-ended optimization tasks. Code and tutorials are available
at https://liding.info/qdhf.",2310.12103v3,https://arxiv.org/pdf/2310.12103v3,Human feedback
Off-Policy Evaluation for Human Feedback,"Qitong Gao, Ge Gao, Juncheng Dong, Vahid Tarokh, Min Chi, Miroslav Pajic","Off-policy evaluation (OPE) is important for closing the gap between offline
training and evaluation of reinforcement learning (RL), by estimating
performance and/or rank of target (evaluation) policies using offline
trajectories only. It can improve the safety and efficiency of data collection
and policy testing procedures in situations where online deployments are
expensive, such as healthcare. However, existing OPE methods fall short in
estimating human feedback (HF) signals, as HF may be conditioned over multiple
underlying factors and is only sparsely available; as opposed to the
agent-defined environmental rewards (used in policy optimization), which are
usually determined over parametric functions or distributions. Consequently,
the nature of HF signals makes extrapolating accurate OPE estimations to be
challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that
revives existing OPE methods in order to accurately evaluate the HF signals.
Specifically, we develop an immediate human reward (IHR) reconstruction
approach, regularized by environmental knowledge distilled in a latent space
that captures the underlying dynamics of state transitions as well as issuing
HF signals. Our approach has been tested over two real-world experiments,
adaptive in-vivo neurostimulation and intelligent tutoring, as well as in a
simulation environment (visual Q&A). Results show that our approach
significantly improves the performance toward estimating HF signals accurately,
compared to directly applying (variants of) existing OPE methods.",2310.07123v2,https://arxiv.org/pdf/2310.07123v2,Human feedback
Diversity from Human Feedback,"Ren-Jian Wang, Ke Xue, Yutong Wang, Peng Yang, Haobo Fu, Qiang Fu, Chao Qian","Diversity plays a significant role in many problems, such as ensemble
learning, reinforcement learning, and combinatorial optimization. How to define
the diversity measure is a longstanding problem. Many methods rely on expert
experience to define a proper behavior space and then obtain the diversity
measure, which is, however, challenging in many scenarios. In this paper, we
propose the problem of learning a behavior space from human feedback and
present a general method called Diversity from Human Feedback (DivHF) to solve
it. DivHF learns a behavior descriptor consistent with human preference by
querying human feedback. The learned behavior descriptor can be combined with
any distance measure to define a diversity measure. We demonstrate the
effectiveness of DivHF by integrating it with the Quality-Diversity
optimization algorithm MAP-Elites and conducting experiments on the QDax suite.
The results show that DivHF learns a behavior space that aligns better with
human requirements compared to direct data-driven approaches and leads to more
diverse solutions under human preference. Our contributions include formulating
the problem, proposing the DivHF method, and demonstrating its effectiveness
through experiments.",2310.06648v2,https://arxiv.org/pdf/2310.06648v2,Human feedback
"Reinforcement Learning with Human Feedback for Realistic Traffic
  Simulation","Yulong Cao, Boris Ivanovic, Chaowei Xiao, Marco Pavone","In light of the challenges and costs of real-world testing, autonomous
vehicle developers often rely on testing in simulation for the creation of
reliable systems. A key element of effective simulation is the incorporation of
realistic traffic models that align with human knowledge, an aspect that has
proven challenging due to the need to balance realism and diversity. This works
aims to address this by developing a framework that employs reinforcement
learning with human preference (RLHF) to enhance the realism of existing
traffic models. This study also identifies two main challenges: capturing the
nuances of human preferences on realism and the unification of diverse traffic
simulation models. To tackle these issues, we propose using human feedback for
alignment and employ RLHF due to its sample efficiency. We also introduce the
first dataset for realism alignment in traffic modeling to support such
research. Our framework, named TrafficRLHF, demonstrates its proficiency in
generating realistic traffic scenarios that are well-aligned with human
preferences, as corroborated by comprehensive evaluations on the nuScenes
dataset.",2309.00709v1,https://arxiv.org/pdf/2309.00709v1,Human feedback
"RLAIF: Scaling Reinforcement Learning from Human Feedback with AI
  Feedback","Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, Sushant Prakash","Reinforcement learning from human feedback (RLHF) has proven effective in
aligning large language models (LLMs) with human preferences. However,
gathering high-quality human preference labels can be a time-consuming and
expensive endeavor. RL from AI Feedback (RLAIF), introduced by Bai et al.,
offers a promising alternative that leverages a powerful off-the-shelf LLM to
generate preferences in lieu of human annotators. Across the tasks of
summarization, helpful dialogue generation, and harmless dialogue generation,
RLAIF achieves comparable or superior performance to RLHF, as rated by human
evaluators. Furthermore, RLAIF demonstrates the ability to outperform a
supervised fine-tuned baseline even when the LLM preference labeler is the same
size as the policy. In another experiment, directly prompting the LLM for
reward scores achieves superior performance to the canonical RLAIF setup, where
LLM preference labels are first distilled into a reward model. Finally, we
conduct extensive studies on techniques for generating aligned AI preferences.
Our results suggest that RLAIF can achieve human-level performance, offering a
potential solution to the scalability limitations of RLHF.",2309.00267v2,https://arxiv.org/pdf/2309.00267v2,Human feedback
"Iterative Reward Shaping using Human Feedback for Correcting Reward
  Misspecification","Jasmina Gajcin, James McCarthy, Rahul Nair, Radu Marinescu, Elizabeth Daly, Ivana Dusparic","A well-defined reward function is crucial for successful training of an
reinforcement learning (RL) agent. However, defining a suitable reward function
is a notoriously challenging task, especially in complex, multi-objective
environments. Developers often have to resort to starting with an initial,
potentially misspecified reward function, and iteratively adjusting its
parameters, based on observed learned behavior. In this work, we aim to
automate this process by proposing ITERS, an iterative reward shaping approach
using human feedback for mitigating the effects of a misspecified reward
function. Our approach allows the user to provide trajectory-level feedback on
agent's behavior during training, which can be integrated as a reward shaping
signal in the following training iteration. We also allow the user to provide
explanations of their feedback, which are used to augment the feedback and
reduce user effort and feedback frequency. We evaluate ITERS in three
environments and show that it can successfully correct misspecified reward
functions.",2308.15969v1,https://arxiv.org/pdf/2308.15969v1,Human feedback
Aligning Language Models with Offline Learning from Human Feedback,"Jian Hu, Li Tao, June Yang, Chandler Zhou","Learning from human preferences is crucial for language models (LMs) to
effectively cater to human needs and societal values. Previous research has
made notable progress by leveraging human feedback to follow instructions.
However, these approaches rely primarily on online learning techniques like
Proximal Policy Optimization (PPO), which have been proven unstable and
challenging to tune for language models. Moreover, PPO requires complex
distributed system implementation, hindering the efficiency of large-scale
distributed training. In this study, we propose an offline learning from human
feedback framework to align LMs without interacting with environments.
Specifically, we explore filtering alignment (FA), reward-weighted regression
(RWR), and conditional alignment (CA) to align language models to human
preferences. By employing a loss function similar to supervised fine-tuning,
our methods ensure more stable model training than PPO with a simple machine
learning system~(MLSys) and much fewer (around 9\%) computing resources.
Experimental results demonstrate that conditional alignment outperforms other
offline alignment methods and is comparable to PPO.",2308.12050v2,https://arxiv.org/pdf/2308.12050v2,Human feedback
"RLHF-Blender: A Configurable Interactive Interface for Learning from
  Diverse Human Feedback","Yannick Metz, David Lindner, Raphaël Baur, Daniel Keim, Mennatallah El-Assady","To use reinforcement learning from human feedback (RLHF) in practical
applications, it is crucial to learn reward models from diverse sources of
human feedback and to consider human factors involved in providing feedback of
different types. However, the systematic study of learning from diverse types
of feedback is held back by limited standardized tooling available to
researchers. To bridge this gap, we propose RLHF-Blender, a configurable,
interactive interface for learning from human feedback. RLHF-Blender provides a
modular experimentation framework and implementation that enables researchers
to systematically investigate the properties and qualities of human feedback
for reward learning. The system facilitates the exploration of various feedback
types, including demonstrations, rankings, comparisons, and natural language
instructions, as well as studies considering the impact of human factors on
their effectiveness. We discuss a set of concrete research opportunities
enabled by RLHF-Blender. More information is available at
https://rlhfblender.info/.",2308.04332v1,https://arxiv.org/pdf/2308.04332v1,Human feedback
"PARL: A Unified Framework for Policy Alignment in Reinforcement Learning
  from Human Feedback","Souradip Chakraborty, Amrit Singh Bedi, Alec Koppel, Dinesh Manocha, Huazheng Wang, Mengdi Wang, Furong Huang","We present a novel unified bilevel optimization-based framework,
\textsf{PARL}, formulated to address the recently highlighted critical issue of
policy alignment in reinforcement learning using utility or preference-based
feedback. We identify a major gap within current algorithmic designs for
solving policy alignment due to a lack of precise characterization of the
dependence of the alignment objective on the data generated by policy
trajectories. This shortfall contributes to the sub-optimal performance
observed in contemporary algorithms. Our framework addressed these concerns by
explicitly parameterizing the distribution of the upper alignment objective
(reward design) by the lower optimal variable (optimal policy for the designed
reward). Interestingly, from an optimization perspective, our formulation leads
to a new class of stochastic bilevel problems where the stochasticity at the
upper objective depends upon the lower-level variable. {True to our best
knowledge, this work presents the first formulation of the RLHF as a bilevel
optimization problem which generalizes the existing RLHF formulations and
addresses the existing distribution shift issues in RLHF formulations.} To
demonstrate the efficacy of our formulation in resolving alignment issues in
RL, we devised an algorithm named \textsf{A-PARL} to solve PARL problem,
establishing sample complexity bounds of order $\mathcal{O}(1/T)$. Our
empirical results substantiate that the proposed \textsf{PARL} can address the
alignment concerns in RL by showing significant improvements (up to 63\% in
terms of required samples) for policy alignment in large-scale environments of
the Deepmind control suite and Meta world tasks.",2308.02585v3,https://arxiv.org/pdf/2308.02585v3,Human feedback
"Okapi: Instruction-tuned Large Language Models in Multiple Languages
  with Reinforcement Learning from Human Feedback","Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen","A key technology for the development of large language models (LLMs) involves
instruction tuning that helps align the models' responses with human
expectations to realize impressive learning abilities. Two major approaches for
instruction tuning characterize supervised fine-tuning (SFT) and reinforcement
learning from human feedback (RLHF), which are currently applied to produce the
best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for
research and development efforts, various instruction-tuned open-source LLMs
have also been introduced recently, e.g., Alpaca, Vicuna, to name a few.
However, existing open-source LLMs have only been instruction-tuned for English
and a few popular languages, thus hindering their impacts and accessibility to
many other languages in the world. Among a few very recent work to explore
instruction tuning for LLMs in multiple languages, SFT has been used as the
only approach to instruction-tune LLMs for multiple languages. This has left a
significant gap for fine-tuned LLMs based on RLHF in diverse languages and
raised important questions on how RLHF can boost the performance of
multilingual instruction tuning. To overcome this issue, we present Okapi, the
first system with instruction-tuned LLMs based on RLHF for multiple languages.
Okapi introduces instruction and response-ranked data in 26 diverse languages
to facilitate the experiments and development of future multilingual LLM
research. We also present benchmark datasets to enable the evaluation of
generative LLMs in multiple languages. Our experiments demonstrate the
advantages of RLHF for multilingual instruction over SFT for different base
models and datasets. Our framework and resources are released at
https://github.com/nlp-uoregon/Okapi.",2307.16039v2,https://arxiv.org/pdf/2307.16039v2,Human feedback
"Open Problems and Fundamental Limitations of Reinforcement Learning from
  Human Feedback","Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell","Reinforcement learning from human feedback (RLHF) is a technique for training
AI systems to align with human goals. RLHF has emerged as the central method
used to finetune state-of-the-art large language models (LLMs). Despite this
popularity, there has been relatively little public work systematizing its
flaws. In this paper, we (1) survey open problems and fundamental limitations
of RLHF and related methods; (2) overview techniques to understand, improve,
and complement RLHF in practice; and (3) propose auditing and disclosure
standards to improve societal oversight of RLHF systems. Our work emphasizes
the limitations of RLHF and highlights the importance of a multi-faceted
approach to the development of safer AI systems.",2307.15217v2,https://arxiv.org/pdf/2307.15217v2,Human feedback
"FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with
  Human Feedback","Ashish Singh, Prateek Agarwal, Zixuan Huang, Arpita Singh, Tong Yu, Sungchul Kim, Victor Bursztyn, Nikos Vlassis, Ryan A. Rossi","Captions are crucial for understanding scientific visualizations and
documents. Existing captioning methods for scientific figures rely on
figure-caption pairs extracted from documents for training, many of which fall
short with respect to metrics like helpfulness, explainability, and
visual-descriptiveness [15] leading to generated captions being misaligned with
reader preferences. To enable the generation of high-quality figure captions,
we introduce FigCaps-HF a new framework for figure-caption generation that can
incorporate domain expert feedback in generating captions optimized for reader
preferences. Our framework comprises of 1) an automatic method for evaluating
quality of figure-caption pairs, 2) a novel reinforcement learning with human
feedback (RLHF) method to optimize a generative figure-to-caption model for
reader preferences. We demonstrate the effectiveness of our simple learning
framework by improving performance over standard fine-tuning across different
types of models. In particular, when using BLIP as the base model, our RLHF
framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and
Meteor, respectively. Finally, we release a large-scale benchmark dataset with
human feedback on figure-caption pairs to enable further evaluation and
development of RLHF techniques for this problem.",2307.10867v1,https://arxiv.org/pdf/2307.10867v1,Human feedback
"Provably Efficient Iterated CVaR Reinforcement Learning with Function
  Approximation and Human Feedback","Yu Chen, Yihan Du, Pihe Hu, Siwei Wang, Desheng Wu, Longbo Huang","Risk-sensitive reinforcement learning (RL) aims to optimize policies that
balance the expected reward and risk. In this paper, we present a novel
risk-sensitive RL framework that employs an Iterated Conditional Value-at-Risk
(CVaR) objective under both linear and general function approximations,
enriched by human feedback. These new formulations provide a principled way to
guarantee safety in each decision making step throughout the control process.
Moreover, integrating human feedback into risk-sensitive RL framework bridges
the gap between algorithmic decision-making and human participation, allowing
us to also guarantee safety for human-in-the-loop systems. We propose provably
sample-efficient algorithms for this Iterated CVaR RL and provide rigorous
theoretical analysis. Furthermore, we establish a matching lower bound to
corroborate the optimality of our algorithms in a linear context.",2307.02842v3,https://arxiv.org/pdf/2307.02842v3,Human feedback
Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback,"TaeHo Yoon, Kibeom Myoung, Keon Lee, Jaewoong Cho, Albert No, Ernest K. Ryu","Diffusion models have recently shown remarkable success in high-quality image
generation. Sometimes, however, a pre-trained diffusion model exhibits partial
misalignment in the sense that the model can generate good images, but it
sometimes outputs undesirable images. If so, we simply need to prevent the
generation of the bad images, and we call this task censoring. In this work, we
present censored generation with a pre-trained diffusion model using a reward
model trained on minimal human feedback. We show that censoring can be
accomplished with extreme human feedback efficiency and that labels generated
with a mere few minutes of human feedback are sufficient. Code available at:
https://github.com/tetrzim/diffusion-human-feedback.",2307.02770v2,https://arxiv.org/pdf/2307.02770v2,Human feedback
"External Reasoning: Towards Multi-Large-Language-Models Interchangeable
  Assistance with Human Feedback",Akide Liu,"Memory is identified as a crucial human faculty that allows for the retention
of visual and linguistic information within the hippocampus and neurons in the
brain, which can subsequently be retrieved to address real-world challenges
that arise through a lifetime of learning. The resolution of complex AI tasks
through the application of acquired knowledge represents a stride toward the
realization of artificial general intelligence. However, despite the prevalence
of Large Language Models (LLMs) like GPT-3.5 and GPT-4 \cite{brown2020language,
leiter2023chatgpt, zaitsu2023distinguishing, OpenAI2023GPT4TR} , which have
displayed remarkable capabilities in language comprehension, generation,
interaction, and reasoning, they are inhibited by constraints on context length
that preclude the processing of extensive, continually evolving knowledge
bases. This paper proposes that LLMs could be augmented through the selective
integration of knowledge from external repositories, and in doing so,
introduces a novel methodology for External Reasoning, exemplified by ChatPDF.
Central to this approach is the establishment of a tiered policy for
\textbf{External Reasoning based on Multiple LLM Interchange Assistance} in
\cref{fig:overall}, where the level of support rendered is modulated across
entry, intermediate, and advanced tiers based on the complexity of the query,
with adjustments made in response to human feedback. A comprehensive evaluation
of this methodology is conducted using multiple LLMs and the results indicate
state-of-the-art performance in \cref{comparison} , surpassing existing
solutions including ChatPDF.com. Moreover, the paper emphasizes that this
approach is more efficient compared to the direct processing of full text by
LLMs. The source code is publicly available at:
\url{https://github.com/AkideLiu/ANLP}.",2307.12057v2,https://arxiv.org/pdf/2307.12057v2,Human feedback
"Can Differentiable Decision Trees Enable Interpretable Reward Learning
  from Human Feedback?","Akansha Kalra, Daniel S. Brown","Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular
paradigm for capturing human intent to alleviate the challenges of
hand-crafting the reward values. Despite the increasing interest in RLHF, most
works learn black box reward functions that while expressive are difficult to
interpret and often require running the whole costly process of RL before we
can even decipher if these frameworks are actually aligned with human
preferences. We propose and evaluate a novel approach for learning expressive
and interpretable reward functions from preferences using Differentiable
Decision Trees (DDTs). Our experiments across several domains, including
CartPole, Visual Gridworld environments and Atari games, provide evidence that
the tree structure of our learned reward function is useful in determining the
extent to which the reward function is aligned with human preferences. We also
provide experimental evidence that not only shows that reward DDTs can often
achieve competitive RL performance when compared with larger capacity deep
neural network reward functions but also demonstrates the diagnostic utility of
our framework in checking alignment of learned reward functions. We also
observe that the choice between soft and hard (argmax) output of reward DDT
reveals a tension between wanting highly shaped rewards to ensure good RL
performance, while also wanting simpler, more interpretable rewards. Videos and
code, are available at: https://sites.google.com/view/ddt-rlhf",2306.13004v4,https://arxiv.org/pdf/2306.13004v4,Human feedback
"Aligning Synthetic Medical Images with Clinical Knowledge using Human
  Feedback","Shenghuan Sun, Gregory M. Goldgof, Atul Butte, Ahmed M. Alaa","Generative models capable of capturing nuanced clinical features in medical
images hold great promise for facilitating clinical data sharing, enhancing
rare disease datasets, and efficiently synthesizing annotated medical images at
scale. Despite their potential, assessing the quality of synthetic medical
images remains a challenge. While modern generative models can synthesize
visually-realistic medical images, the clinical validity of these images may be
called into question. Domain-agnostic scores, such as FID score, precision, and
recall, cannot incorporate clinical knowledge and are, therefore, not suitable
for assessing clinical sensibility. Additionally, there are numerous
unpredictable ways in which generative models may fail to synthesize clinically
plausible images, making it challenging to anticipate potential failures and
manually design scores for their detection. To address these challenges, this
paper introduces a pathologist-in-the-loop framework for generating
clinically-plausible synthetic medical images. Starting with a diffusion model
pretrained using real images, our framework comprises three steps: (1)
evaluating the generated images by expert pathologists to assess whether they
satisfy clinical desiderata, (2) training a reward model that predicts the
pathologist feedback on new samples, and (3) incorporating expert knowledge
into the diffusion model by using the reward model to inform a finetuning
objective. We show that human feedback significantly improves the quality of
synthetic images in terms of fidelity, diversity, utility in downstream
applications, and plausibility as evaluated by experts.",2306.12438v1,https://arxiv.org/pdf/2306.12438v1,Human feedback
"When to Show a Suggestion? Integrating Human Feedback in AI-Assisted
  Programming","Hussein Mozannar, Gagan Bansal, Adam Fourney, Eric Horvitz","AI powered code-recommendation systems, such as Copilot and CodeWhisperer,
provide code suggestions inside a programmer's environment (e.g., an IDE) with
the aim of improving productivity. We pursue mechanisms for leveraging signals
about programmers' acceptance and rejection of code suggestions to guide
recommendations. We harness data drawn from interactions with GitHub Copilot, a
system used by millions of programmers, to develop interventions that can save
time for programmers. We introduce a utility-theoretic framework to drive
decisions about suggestions to display versus withhold. The approach,
conditional suggestion display from human feedback (CDHF), relies on a cascade
of models that provide the likelihood that recommended code will be accepted.
These likelihoods are used to selectively hide suggestions, reducing both
latency and programmer verification time. Using data from 535 programmers, we
perform a retrospective evaluation of CDHF and show that we can avoid
displaying a significant fraction of suggestions that would have been rejected.
We further demonstrate the importance of incorporating the programmer's latent
unobserved state in decisions about when to display suggestions through an
ablation study. Finally, we showcase how using suggestion acceptance as a
reward signal for guiding the display of suggestions can lead to suggestions of
reduced quality, indicating an unexpected pitfall.",2306.04930v3,https://arxiv.org/pdf/2306.04930v3,Human feedback
"Reinforcement Learning with Human Feedback: Learning Dynamic Choices via
  Pessimism","Zihao Li, Zhuoran Yang, Mengdi Wang","In this paper, we study offline Reinforcement Learning with Human Feedback
(RLHF) where we aim to learn the human's underlying reward and the MDP's
optimal policy from a set of trajectories induced by human choices. RLHF is
challenging for multiple reasons: large state space but limited human feedback,
the bounded rationality of human decisions, and the off-policy distribution
shift. In this paper, we focus on the Dynamic Discrete Choice (DDC) model for
modeling and understanding human choices. DCC, rooted in econometrics and
decision theory, is widely used to model a human decision-making process with
forward-looking and bounded rationality. We propose a
\underline{D}ynamic-\underline{C}hoice-\underline{P}essimistic-\underline{P}olicy-\underline{O}ptimization
(DCPPO) method. \ The method involves a three-stage process: The first step is
to estimate the human behavior policy and the state-action value function via
maximum likelihood estimation (MLE); the second step recovers the human reward
function via minimizing Bellman mean squared error using the learned value
functions; the third step is to plug in the learned reward and invoke
pessimistic value iteration for finding a near-optimal policy. With only
single-policy coverage (i.e., optimal policy) of the dataset, we prove that the
suboptimality of DCPPO almost matches the classical pessimistic offline RL
algorithm in terms of suboptimality's dependency on distribution shift and
dimension. To the best of our knowledge, this paper presents the first
theoretical guarantees for off-policy offline RLHF with dynamic discrete choice
model.",2305.18438v3,https://arxiv.org/pdf/2305.18438v3,Human feedback
"Learning Interpretable Models of Aircraft Handling Behaviour by
  Reinforcement Learning from Human Feedback","Tom Bewley, Jonathan Lawry, Arthur Richards","We propose a method to capture the handling abilities of fast jet pilots in a
software model via reinforcement learning (RL) from human preference feedback.
We use pairwise preferences over simulated flight trajectories to learn an
interpretable rule-based model called a reward tree, which enables the
automated scoring of trajectories alongside an explanatory rationale. We train
an RL agent to execute high-quality handling behaviour by using the reward tree
as the objective, and thereby generate data for iterative preference collection
and further refinement of both tree and agent. Experiments with synthetic
preferences show reward trees to be competitive with uninterpretable neural
network reward models on quantitative and qualitative evaluations.",2305.16924v1,https://arxiv.org/pdf/2305.16924v1,Human feedback
"AlpacaFarm: A Simulation Framework for Methods that Learn from Human
  Feedback","Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto","Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their strong instruction-following abilities. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following requires
tackling three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 50x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO, DPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm.",2305.14387v4,https://arxiv.org/pdf/2305.14387v4,Human feedback
Continually Improving Extractive QA via Human Feedback,"Ge Gao, Hung-Ting Chen, Yoav Artzi, Eunsol Choi","We study continually improving an extractive question answering (QA) system
via human user feedback. We design and deploy an iterative approach, where
information-seeking users ask questions, receive model-predicted answers, and
provide feedback. We conduct experiments involving thousands of user
interactions under diverse setups to broaden the understanding of learning from
feedback over time. Our experiments show effective improvement from user
feedback of extractive QA models over time across different data regimes,
including significant potential for domain adaptation.",2305.12473v2,https://arxiv.org/pdf/2305.12473v2,Human feedback
SLiC-HF: Sequence Likelihood Calibration with Human Feedback,"Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, Peter J. Liu","Learning from human feedback has been shown to be effective at aligning
language models with human preferences. Past work has often relied on
Reinforcement Learning from Human Feedback (RLHF), which optimizes the language
model using reward scores assigned from a reward model trained on human
preference data. In this work we show how the recently introduced Sequence
Likelihood Calibration (SLiC), can also be used to effectively learn from human
preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human
feedback data collected for a different model, similar to off-policy, offline
RL data. Automatic and human evaluation experiments on the TL;DR summarization
task show that SLiC-HF significantly improves supervised fine-tuning baselines.
Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF
implementation used in past work while being much simpler to implement, easier
to tune and more computationally efficient in practice.",2305.10425v1,https://arxiv.org/pdf/2305.10425v1,Human feedback
GFlowNets with Human Feedback,"Yinchuan Li, Shuang Luo, Yunfeng Shao, Jianye Hao","We propose the GFlowNets with Human Feedback (GFlowHF) framework to improve
the exploration ability when training AI models. For tasks where the reward is
unknown, we fit the reward function through human evaluations on different
trajectories. The goal of GFlowHF is to learn a policy that is strictly
proportional to human ratings, instead of only focusing on human favorite
ratings like RLHF. Experiments show that GFlowHF can achieve better exploration
ability than RLHF.",2305.07036v1,https://arxiv.org/pdf/2305.07036v1,Human feedback
"Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural
  Language Generation","Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, André F. T. Martins","Many recent advances in natural language generation have been fueled by
training large language models on internet-scale data. However, this paradigm
can lead to models that generate toxic, inaccurate, and unhelpful content, and
automatic evaluation metrics often fail to identify these behaviors. As models
become more capable, human feedback is an invaluable signal for evaluating and
improving models. This survey aims to provide an overview of the recent
research that has leveraged human feedback to improve natural language
generation. First, we introduce an encompassing formalization of feedback, and
identify and organize existing research into a taxonomy following this
formalization. Next, we discuss how feedback can be described by its format and
objective, and cover the two approaches proposed to use feedback (either for
training or decoding): directly using the feedback or training feedback models.
We also discuss existing datasets for human-feedback data collection, and
concerns surrounding feedback collection. Finally, we provide an overview of
the nascent field of AI feedback, which exploits large language models to make
judgments based on a set of principles and minimize the need for human
intervention.",2305.00955v2,https://arxiv.org/pdf/2305.00955v2,Human feedback
"Leveraging Human Feedback to Evolve and Discover Novel Emergent
  Behaviors in Robot Swarms","Connor Mattson, Daniel S. Brown","Robot swarms often exhibit emergent behaviors that are fascinating to
observe; however, it is often difficult to predict what swarm behaviors can
emerge under a given set of agent capabilities. We seek to efficiently leverage
human input to automatically discover a taxonomy of collective behaviors that
can emerge from a particular multi-agent system, without requiring the human to
know beforehand what behaviors are interesting or even possible. Our proposed
approach adapts to user preferences by learning a similarity space over swarm
collective behaviors using self-supervised learning and human-in-the-loop
queries. We combine our learned similarity metric with novelty search and
clustering to explore and categorize the space of possible swarm behaviors. We
also propose several general-purpose heuristics that improve the efficiency of
our novelty search by prioritizing robot controllers that are likely to lead to
interesting emergent behaviors. We test our approach in simulation on two robot
capability models and show that our methods consistently discover a richer set
of emergent behaviors than prior work. Code, videos, and datasets are available
at https://sites.google.com/view/evolving-novel-swarms.",2305.16148v2,https://arxiv.org/pdf/2305.16148v2,Human feedback
"Towards Solving Fuzzy Tasks with Human Feedback: A Retrospective of the
  MineRL BASALT 2022 Competition","Stephanie Milani, Anssi Kanervisto, Karolis Ramanauskas, Sander Schulhoff, Brandon Houghton, Sharada Mohanty, Byron Galbraith, Ke Chen, Yan Song, Tianze Zhou, Bingquan Yu, He Liu, Kai Guan, Yujing Hu, Tangjie Lv, Federico Malato, Florian Leopold, Amogh Raut, Ville Hautamäki, Andrew Melnik, Shu Ishida, João F. Henriques, Robert Klassert, Walter Laurito, Ellen Novoseller, Vinicius G. Goecks, Nicholas Waytowich, David Watkins, Josh Miller, Rohin Shah","To facilitate research in the direction of fine-tuning foundation models from
human feedback, we held the MineRL BASALT Competition on Fine-Tuning from Human
Feedback at NeurIPS 2022. The BASALT challenge asks teams to compete to develop
algorithms to solve tasks with hard-to-specify reward functions in Minecraft.
Through this competition, we aimed to promote the development of algorithms
that use human feedback as channels to learn the desired behavior. We describe
the competition and provide an overview of the top solutions. We conclude by
discussing the impact of the competition and future directions for improvement.",2303.13512v1,https://arxiv.org/pdf/2303.13512v1,Human feedback
HIVE: Harnessing Human Feedback for Instructional Visual Editing,"Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu","Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.",2303.09618v2,https://arxiv.org/pdf/2303.09618v2,Human feedback
Deploying Offline Reinforcement Learning with Human Feedback,"Ziniu Li, Ke Xu, Liu Liu, Lanqing Li, Deheng Ye, Peilin Zhao","Reinforcement learning (RL) has shown promise for decision-making tasks in
real-world applications. One practical framework involves training
parameterized policy models from an offline dataset and subsequently deploying
them in an online environment. However, this approach can be risky since the
offline training may not be perfect, leading to poor performance of the RL
models that may take dangerous actions. To address this issue, we propose an
alternative framework that involves a human supervising the RL models and
providing additional feedback in the online deployment phase. We formalize this
online deployment problem and develop two approaches. The first approach uses
model selection and the upper confidence bound algorithm to adaptively select a
model to deploy from a candidate set of trained offline RL models. The second
approach involves fine-tuning the model in the online deployment phase when a
supervision signal arrives. We demonstrate the effectiveness of these
approaches for robot locomotion control and traffic light control tasks through
empirical validation.",2303.07046v1,https://arxiv.org/pdf/2303.07046v1,Human feedback
"Zeroth-Order Optimization Meets Human Feedback: Provable Learning via
  Ranking Oracles","Zhiwei Tang, Dmitry Rybin, Tsung-Hui Chang","In this study, we delve into an emerging optimization challenge involving a
black-box objective function that can only be gauged via a ranking oracle-a
situation frequently encountered in real-world scenarios, especially when the
function is evaluated by human judges. Such challenge is inspired from
Reinforcement Learning with Human Feedback (RLHF), an approach recently
employed to enhance the performance of Large Language Models (LLMs) using human
guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization
algorithm designed to tackle this optimization problem, accompanied by
theoretical assurances. Our algorithm utilizes a novel rank-based random
estimator to determine the descent direction and guarantees convergence to a
stationary point. Moreover, ZO-RankSGD is readily applicable to policy
optimization problems in Reinforcement Learning (RL), particularly when only
ranking oracles for the episode reward are available. Last but not least, we
demonstrate the effectiveness of ZO-RankSGD in a novel application: improving
the quality of images generated by a diffusion generative model with human
ranking feedback. Throughout experiments, we found that ZO-RankSGD can
significantly enhance the detail of generated images with only a few rounds of
human feedback. Overall, our work advances the field of zeroth-order
optimization by addressing the problem of optimizing functions with only
ranking feedback, and offers a new and effective approach for aligning
Artificial Intelligence (AI) with human intentions.",2303.03751v3,https://arxiv.org/pdf/2303.03751v3,Human feedback
"Perspectives on the Social Impacts of Reinforcement Learning with Human
  Feedback",Gabrielle Kaili-May Liu,"Is it possible for machines to think like humans? And if it is, how should we
go about teaching them to do so? As early as 1950, Alan Turing stated that we
ought to teach machines in the way of teaching a child. Reinforcement learning
with human feedback (RLHF) has emerged as a strong candidate toward allowing
agents to learn from human feedback in a naturalistic manner. RLHF is distinct
from traditional reinforcement learning as it provides feedback from a human
teacher in addition to a reward signal. It has been catapulted into public view
by multiple high-profile AI applications, including OpenAI's ChatGPT,
DeepMind's Sparrow, and Anthropic's Claude. These highly capable chatbots are
already overturning our understanding of how AI interacts with humanity. The
wide applicability and burgeoning success of RLHF strongly motivate the need to
evaluate its social impacts. In light of recent developments, this paper
considers an important question: can RLHF be developed and used without
negatively affecting human societies? Our objectives are threefold: to provide
a systematic study of the social effects of RLHF; to identify key social and
ethical issues of RLHF; and to discuss social impacts for stakeholders.
Although text-based applications of RLHF have received much attention, it is
crucial to consider when evaluating its social implications the diverse range
of areas to which it may be deployed. We describe seven primary ways in which
RLHF-based technologies will affect society by positively transforming human
experiences with AI. This paper ultimately proposes that RLHF has potential to
net positively impact areas of misinformation, AI value-alignment, bias, AI
access, cross-cultural dialogue, industry, and workforce. As RLHF raises
concerns that echo those of existing AI technologies, it will be important for
all to be aware and intentional in the adoption of RLHF.",2303.02891v1,https://arxiv.org/pdf/2303.02891v1,Human feedback
Aligning Text-to-Image Models using Human Feedback,"Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Shixiang Shane Gu","Deep generative models have shown impressive results in text-to-image
synthesis. However, current text-to-image models often generate images that are
inadequately aligned with text prompts. We propose a fine-tuning method for
aligning such models using human feedback, comprising three stages. First, we
collect human feedback assessing model output alignment from a set of diverse
text prompts. We then use the human-labeled image-text dataset to train a
reward function that predicts human feedback. Lastly, the text-to-image model
is fine-tuned by maximizing reward-weighted likelihood to improve image-text
alignment. Our method generates objects with specified colors, counts and
backgrounds more accurately than the pre-trained model. We also analyze several
design choices and find that careful investigations on such design choices are
important in balancing the alignment-fidelity tradeoffs. Our results
demonstrate the potential for learning from human feedback to significantly
improve text-to-image models.",2302.12192v1,https://arxiv.org/pdf/2302.12192v1,Human feedback
"Principled Reinforcement Learning with Human Feedback from Pairwise or
  $K$-wise Comparisons","Banghua Zhu, Jiantao Jiao, Michael I. Jordan","We provide a theoretical framework for Reinforcement Learning with Human
Feedback (RLHF). Our analysis shows that when the true reward function is
linear, the widely used maximum likelihood estimator (MLE) converges under both
the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However,
we show that when training a policy based on the learned reward model, MLE
fails while a pessimistic MLE provides policies with improved performance under
certain coverage assumptions. Additionally, we demonstrate that under the PL
model, the true MLE and an alternative MLE that splits the $K$-wise comparison
into pairwise comparisons both converge. Moreover, the true MLE is
asymptotically more efficient. Our results validate the empirical success of
existing RLHF algorithms in InstructGPT and provide new insights for algorithm
design. Furthermore, our results unify the problem of RLHF and max-entropy
Inverse Reinforcement Learning (IRL), and provide the first sample complexity
bound for max-entropy IRL.",2301.11270v5,https://arxiv.org/pdf/2301.11270v5,Human feedback
"Methodological reflections for AI alignment research using human
  feedback","Thilo Hagendorff, Sarah Fabi","The field of artificial intelligence (AI) alignment aims to investigate
whether AI technologies align with human interests and values and function in a
safe and ethical manner. AI alignment is particularly relevant for large
language models (LLMs), which have the potential to exhibit unintended behavior
due to their ability to learn and adapt in ways that are difficult to predict.
In this paper, we discuss methodological challenges for the alignment problem
specifically in the context of LLMs trained to summarize texts. In particular,
we focus on methods for collecting reliable human feedback on summaries to
train a reward model which in turn improves the summarization model. We
conclude by suggesting specific improvements in the experimental design of
alignment studies for LLMs' summarization capabilities.",2301.06859v1,https://arxiv.org/pdf/2301.06859v1,Human feedback
"Improving Multimodal Interactive Agents with Reinforcement Learning from
  Human Feedback","Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jessica Landon, Jirka Lhotka, Timothy Lillicrap, Alistair Muldal, George Powell, Adam Santoro, Guy Scully, Sanjana Srivastava, Tamara von Glehn, Greg Wayne, Nathaniel Wong, Chen Yan, Rui Zhu","An important goal in artificial intelligence is to create agents that can
both interact naturally with humans and learn from their feedback. Here we
demonstrate how to use reinforcement learning from human feedback (RLHF) to
improve upon simulated, embodied agents trained to a base level of competency
with imitation learning. First, we collected data of humans interacting with
agents in a simulated 3D world. We then asked annotators to record moments
where they believed that agents either progressed toward or regressed from
their human-instructed goal. Using this annotation data we leveraged a novel
method - which we call ""Inter-temporal Bradley-Terry"" (IBT) modelling - to
build a reward model that captures human judgments. Agents trained to optimise
rewards delivered from IBT reward models improved with respect to all of our
metrics, including subsequent human judgment during live interactions with
agents. Altogether our results demonstrate how one can successfully leverage
human judgments to improve agent behaviour, allowing us to use reinforcement
learning in complex, embodied domains without programmatic reward functions.
Videos of agent behaviour may be found at https://youtu.be/v_Z9F2_eKk4.",2211.11602v1,https://arxiv.org/pdf/2211.11602v1,Human feedback
"Learning New Skills after Deployment: Improving open-domain
  internet-driven dialogue with human feedback","Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau, Jason Weston","Frozen models trained to mimic static datasets can never improve their
performance. Models that can employ internet-retrieval for up-to-date
information and obtain feedback from humans during deployment provide the
promise of both adapting to new information, and improving their performance.
In this work we study how to improve internet-driven conversational skills in
such a learning framework. We collect deployment data, which we make publicly
available, of human interactions, and collect various types of human feedback
-- including binary quality measurements, free-form text feedback, and
fine-grained reasons for failure. We then study various algorithms for
improving from such feedback, including standard supervised learning, rejection
sampling, model-guiding and reward-based learning, in order to make
recommendations on which type of feedback and algorithms work best. We find the
recently introduced Director model (Arora et al., '22) shows significant
improvements over other existing approaches.",2208.03270v2,https://arxiv.org/pdf/2208.03270v2,Human feedback
"Humans are not Boltzmann Distributions: Challenges and Opportunities for
  Modelling Human Feedback and Interaction in Reinforcement Learning","David Lindner, Mennatallah El-Assady","Reinforcement learning (RL) commonly assumes access to well-specified reward
functions, which many practical applications do not provide. Instead, recently,
more work has explored learning what to do from interacting with humans. So
far, most of these approaches model humans as being (nosily) rational and, in
particular, giving unbiased feedback. We argue that these models are too
simplistic and that RL researchers need to develop more realistic human models
to design and evaluate their algorithms. In particular, we argue that human
models have to be personal, contextual, and dynamic. This paper calls for
research from different disciplines to address key questions about how humans
provide feedback to AIs and how we can build more robust human-in-the-loop RL
systems.",2206.13316v1,https://arxiv.org/pdf/2206.13316v1,Human feedback
"Retrospective on the 2021 BASALT Competition on Learning from Human
  Feedback","Rohin Shah, Steven H. Wang, Cody Wild, Stephanie Milani, Anssi Kanervisto, Vinicius G. Goecks, Nicholas Waytowich, David Watkins-Valls, Bharat Prakash, Edmund Mills, Divyansh Garg, Alexander Fries, Alexandra Souly, Chan Jun Shern, Daniel del Castillo, Tom Lieberum","We held the first-ever MineRL Benchmark for Agents that Solve Almost-Lifelike
Tasks (MineRL BASALT) Competition at the Thirty-fifth Conference on Neural
Information Processing Systems (NeurIPS 2021). The goal of the competition was
to promote research towards agents that use learning from human feedback (LfHF)
techniques to solve open-world tasks. Rather than mandating the use of LfHF
techniques, we described four tasks in natural language to be accomplished in
the video game Minecraft, and allowed participants to use any approach they
wanted to build agents that could accomplish the tasks. Teams developed a
diverse range of LfHF algorithms across a variety of possible human feedback
types. The three winning teams implemented significantly different approaches
while achieving similar performance. Interestingly, their approaches performed
well on different tasks, validating our choice of tasks to include in the
competition. While the outcomes validated the design of our competition, we did
not get as many participants and submissions as our sister competition, MineRL
Diamond. We speculate about the causes of this problem and suggest improvements
for future iterations of the competition.",2204.07123v1,https://arxiv.org/pdf/2204.07123v1,Human feedback
"Training a Helpful and Harmless Assistant with Reinforcement Learning
  from Human Feedback","Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, Jared Kaplan","We apply preference modeling and reinforcement learning from human feedback
(RLHF) to finetune language models to act as helpful and harmless assistants.
We find this alignment training improves performance on almost all NLP
evaluations, and is fully compatible with training for specialized skills such
as python coding and summarization. We explore an iterated online mode of
training, where preference models and RL policies are updated on a weekly
cadence with fresh human feedback data, efficiently improving our datasets and
models. Finally, we investigate the robustness of RLHF training, and identify a
roughly linear relation between the RL reward and the square root of the KL
divergence between the policy and its initialization. Alongside our main
results, we perform peripheral analyses on calibration, competing objectives,
and the use of OOD detection, compare our models with human writers, and
provide samples from our models using prompts appearing in recent related work.",2204.05862v1,https://arxiv.org/pdf/2204.05862v1,Human feedback
"Learning from Physical Human Feedback: An Object-Centric One-Shot
  Adaptation Method","Alvin Shek, Bo Ying Su, Rui Chen, Changliu Liu","For robots to be effectively deployed in novel environments and tasks, they
must be able to understand the feedback expressed by humans during
intervention. This can either correct undesirable behavior or indicate
additional preferences. Existing methods either require repeated episodes of
interactions or assume prior known reward features, which is data-inefficient
and can hardly transfer to new tasks. We relax these assumptions by describing
human tasks in terms of object-centric sub-tasks and interpreting physical
interventions in relation to specific objects. Our method, Object Preference
Adaptation (OPA), is composed of two key stages: 1) pre-training a base policy
to produce a wide variety of behaviors, and 2) online-updating according to
human feedback. The key to our fast, yet simple adaptation is that general
interaction dynamics between agents and objects are fixed, and only
object-specific preferences are updated. Our adaptation occurs online, requires
only one human intervention (one-shot), and produces new behaviors never seen
during training. Trained on cheap synthetic data instead of expensive human
demonstrations, our policy correctly adapts to human perturbations on realistic
tasks on a physical 7DOF robot. Videos, code, and supplementary material are
provided.",2203.04951v2,https://arxiv.org/pdf/2203.04951v2,Human feedback
Training language models to follow instructions with human feedback,"Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe","Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent.",2203.02155v1,https://arxiv.org/pdf/2203.02155v1,Human feedback
Safe Deep RL in 3D Environments using Human Feedback,"Matthew Rahtz, Vikrant Varma, Ramana Kumar, Zachary Kenton, Shane Legg, Jan Leike","Agents should avoid unsafe behaviour during both training and deployment.
This typically requires a simulator and a procedural specification of unsafe
behaviour. Unfortunately, a simulator is not always available, and procedurally
specifying constraints can be difficult or impossible for many real-world
tasks. A recently introduced technique, ReQueST, aims to solve this problem by
learning a neural simulator of the environment from safe human trajectories,
then using the learned simulator to efficiently learn a reward model from human
feedback. However, it is yet unknown whether this approach is feasible in
complex 3D environments with feedback obtained from real humans - whether
sufficient pixel-based neural simulator quality can be achieved, and whether
the human data requirements are viable in terms of both quantity and quality.
In this paper we answer this question in the affirmative, using ReQueST to
train an agent to perform a 3D first-person object collection task using data
entirely from human contractors. We show that the resulting agent exhibits an
order of magnitude reduction in unsafe behaviour compared to standard
reinforcement learning.",2201.08102v2,https://arxiv.org/pdf/2201.08102v2,Human feedback
