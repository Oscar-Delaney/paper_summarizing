Title,Authors,Abstract,arXiv ID,PDF_Link
Online Symbolic Music Alignment with Offline Reinforcement Learning,Silvan David Peter,"Symbolic Music Alignment is the process of matching performed MIDI notes to
corresponding score notes. In this paper, we introduce a reinforcement learning
(RL)-based online symbolic music alignment technique. The RL agent - an
attention-based neural network - iteratively estimates the current score
position from local score and performance contexts. For this symbolic alignment
task, environment states can be sampled exhaustively and the reward is dense,
rendering a formulation as a simplified offline RL problem straightforward. We
evaluate the trained agent in three ways. First, in its capacity to identify
correct score positions for sampled test contexts; second, as the core
technique of a complete algorithm for symbolic online note-wise alignment; and
finally, as a real-time symbolic score follower. We further investigate the
pitch-based score and performance representations used as the agent's inputs.
To this end, we develop a second model, a two-step Dynamic Time Warping
(DTW)-based offline alignment algorithm leveraging the same input
representation. The proposed model outperforms a state-of-the-art reference
model of offline symbolic music alignment.",2401.00466v1,https://arxiv.org/pdf/2401.00466v1
Transfer and Alignment Network for Generalized Category Discovery,"Wenbin An, Feng Tian, Wenkai Shi, Yan Chen, Yaqiang Wu, Qianying Wang, Ping Chen","Generalized Category Discovery is a crucial real-world task. Despite the
improved performance on known categories, current methods perform poorly on
novel categories. We attribute the poor performance to two reasons: biased
knowledge transfer between labeled and unlabeled data and noisy representation
learning on the unlabeled data. To mitigate these two issues, we propose a
Transfer and Alignment Network (TAN), which incorporates two knowledge transfer
mechanisms to calibrate the biased knowledge and two feature alignment
mechanisms to learn discriminative features. Specifically, we model different
categories with prototypes and transfer the prototypes in labeled data to
correct model bias towards known categories. On the one hand, we pull instances
with known categories in unlabeled data closer to these prototypes to form more
compact clusters and avoid boundary overlap between known and novel categories.
On the other hand, we use these prototypes to calibrate noisy prototypes
estimated from unlabeled data based on category similarities, which allows for
more accurate estimation of prototypes for novel categories that can be used as
reliable learning targets later. After knowledge transfer, we further propose
two feature alignment mechanisms to acquire both instance- and category-level
knowledge from unlabeled data by aligning instance features with both augmented
features and the calibrated prototypes, which can boost model performance on
both known and novel categories with less noise. Experiments on three benchmark
datasets show that our model outperforms SOTA methods, especially on novel
categories. Theoretical analysis is provided for an in-depth understanding of
our model in general. Our code and data are available at
https://github.com/Lackel/TAN.",2312.16467v1,https://arxiv.org/pdf/2312.16467v1
"Frame-level emotional state alignment method for speech emotion
  recognition","Qifei Li, Yingming Gao, Cong Wang, Yayue Deng, Jinlong Xue, Yichen Han, Ya Li","Speech emotion recognition (SER) systems aim to recognize human emotional
state during human-computer interaction. Most existing SER systems are trained
based on utterance-level labels. However, not all frames in an audio have
affective states consistent with utterance-level label, which makes it
difficult for the model to distinguish the true emotion of the audio and
perform poorly. To address this problem, we propose a frame-level emotional
state alignment method for SER. First, we fine-tune HuBERT model to obtain a
SER system with task-adaptive pretraining (TAPT) method, and extract embeddings
from its transformer layers to form frame-level pseudo-emotion labels with
clustering. Then, the pseudo labels are used to pretrain HuBERT. Hence, the
each frame output of HuBERT has corresponding emotional information. Finally,
we fine-tune the above pretrained HuBERT for SER by adding an attention layer
on the top of it, which can focus only on those frames that are emotionally
more consistent with utterance-level label. The experimental results performed
on IEMOCAP indicate that our proposed method performs better than
state-of-the-art (SOTA) methods.",2312.16383v1,https://arxiv.org/pdf/2312.16383v1
"What Makes Good Data for Alignment? A Comprehensive Study of Automatic
  Data Selection in Instruction Tuning","Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, Junxian He","Instruction tuning is a standard technique employed to align large language
models to end tasks and user preferences after the initial pretraining phase.
Recent research indicates the critical role of data engineering in instruction
tuning -- when appropriately selected, only limited data is necessary to
achieve superior performance. However, we still lack a principled understanding
of what makes good instruction tuning data for alignment, and how we should
select data automatically and effectively. In this work, we delve deeply into
automatic data selection strategies for alignment. We start with controlled
studies to measure data across three dimensions: complexity, quality, and
diversity, along which we examine existing methods and introduce novel
techniques for enhanced data measurement. Subsequently, we propose a simple
strategy to select data samples based on the measurement. We present deita
(short for Data-Efficient Instruction Tuning for Alignment), a series of models
fine-tuned from LLaMA and Mistral models using data samples automatically
selected with our proposed approach. Empirically, deita performs better or on
par with the state-of-the-art open-source alignment models with only 6K SFT
training data samples -- over 10x less than the data used in the baselines.
When further trained with direct preference optimization (DPO),
deita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55
MT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools
on automatic data selection, facilitating data-efficient alignment. We release
our models as well as the selected datasets for future researches to
effectively align models more efficiently.",2312.15685v2,https://arxiv.org/pdf/2312.15685v2
Measuring Value Alignment,"Fazl Barez, Philip Torr","As artificial intelligence (AI) systems become increasingly integrated into
various domains, ensuring that they align with human values becomes critical.
This paper introduces a novel formalism to quantify the alignment between AI
systems and human values, using Markov Decision Processes (MDPs) as the
foundational model. We delve into the concept of values as desirable goals tied
to actions and norms as behavioral guidelines, aiming to shed light on how they
can be used to guide AI decisions. This framework offers a mechanism to
evaluate the degree of alignment between norms and values by assessing
preference changes across state transitions in a normative world. By utilizing
this formalism, AI developers and ethicists can better design and evaluate AI
systems to ensure they operate in harmony with human values. The proposed
methodology holds potential for a wide range of applications, from
recommendation systems emphasizing well-being to autonomous vehicles
prioritizing safety.",2312.15241v1,https://arxiv.org/pdf/2312.15241v1
"INFAMOUS-NeRF: ImproviNg FAce MOdeling Using Semantically-Aligned
  Hypernetworks with Neural Radiance Fields","Andrew Hou, Feng Liu, Zhiyuan Ren, Michel Sarkis, Ning Bi, Yiying Tong, Xiaoming Liu","We propose INFAMOUS-NeRF, an implicit morphable face model that introduces
hypernetworks to NeRF to improve the representation power in the presence of
many training subjects. At the same time, INFAMOUS-NeRF resolves the classic
hypernetwork tradeoff of representation power and editability by learning
semantically-aligned latent spaces despite the subject-specific models, all
without requiring a large pretrained model. INFAMOUS-NeRF further introduces a
novel constraint to improve NeRF rendering along the face boundary. Our
constraint can leverage photometric surface rendering and multi-view
supervision to guide surface color prediction and improve rendering near the
surface. Finally, we introduce a novel, loss-guided adaptive sampling method
for more effective NeRF training by reducing the sampling redundancy. We show
quantitatively and qualitatively that our method achieves higher representation
power than prior face modeling methods in both controlled and in-the-wild
settings. Code and models will be released upon publication.",2312.16197v1,https://arxiv.org/pdf/2312.16197v1
Voila-A: Aligning Vision-Language Models with User's Gaze Attention,"Kun Yan, Lei Ji, Zeyu Wang, Yuntao Wang, Nan Duan, Shuai Ma","In recent years, the integration of vision and language understanding has led
to significant advancements in artificial intelligence, particularly through
Vision-Language Models (VLMs). However, existing VLMs face challenges in
handling real-world applications with complex scenes and multiple objects, as
well as aligning their focus with the diverse attention patterns of human
users. In this paper, we introduce gaze information, feasibly collected by AR
or VR devices, as a proxy for human attention to guide VLMs and propose a novel
approach, Voila-A, for gaze alignment to enhance the interpretability and
effectiveness of these models in real-world applications. First, we collect
hundreds of minutes of gaze data to demonstrate that we can mimic human gaze
modalities using localized narratives. We then design an automatic data
annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset.
Additionally, we innovate the Voila Perceiver modules to integrate gaze
information into VLMs while preserving their pretrained knowledge. We evaluate
Voila-A using a hold-out validation set and a newly collected VOILA-GAZE
Testset, which features real-life scenarios captured with a gaze-tracking
device. Our experimental results demonstrate that Voila-A significantly
outperforms several baseline models. By aligning model attention with human
gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and
fosters engaging human-AI interaction across a wide range of applications.",2401.09454v1,https://arxiv.org/pdf/2401.09454v1
"TagAlign: Improving Vision-Language Alignment with Multi-Tag
  Classification","Qinying Liu, Wei Wu, Kecheng Zheng, Zhan Tong, Jiawei Liu, Yu Liu, Wei Chen, Zilei Wang, Yujun Shen","The crux of learning vision-language models is to extract semantically
aligned information from visual and linguistic data. Existing attempts usually
face the problem of coarse alignment, e.g., the vision encoder struggles in
localizing an attribute-specified object. In this work, we propose an
embarrassingly simple approach to better align image and text features with no
need of additional data formats other than image-text pairs. Concretely, given
an image and its paired text, we manage to parse objects (e.g., cat) and
attributes (e.g., black) from the description, which are highly likely to exist
in the image. It is noteworthy that the parsing pipeline is fully automatic and
thus enjoys good scalability. With these parsed semantics as supervision
signals, we can complement the commonly used image-text contrastive loss with
the multi-tag classification loss. Extensive experimental results on a broad
suite of semantic segmentation datasets substantiate the average 5.2\%
improvement of our framework over existing alternatives. Furthermore, the
visualization results indicate that attribute supervision makes vision-language
models accurately localize attribute-specified objects. Project page can be
found at https://qinying-liu.github.io/Tag-Align.",2312.14149v4,https://arxiv.org/pdf/2312.14149v4
"Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed
  Diffusion Models","Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, Karsten Kreis","Text-guided diffusion models have revolutionized image and video generation
and have also been successfully used for optimization-based 3D object
synthesis. Here, we instead focus on the underexplored text-to-4D setting and
synthesize dynamic, animated 3D objects using score distillation methods with
an additional temporal dimension. Compared to previous work, we pursue a novel
compositional generation-based approach, and combine text-to-image,
text-to-video, and 3D-aware multiview diffusion models to provide feedback
during 4D object optimization, thereby simultaneously enforcing temporal
consistency, high-quality visual appearance and realistic geometry. Our method,
called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with
deformation fields as 4D representation. Crucial to AYG is a novel method to
regularize the distribution of the moving 3D Gaussians and thereby stabilize
the optimization and induce motion. We also propose a motion amplification
mechanism as well as a new autoregressive synthesis scheme to generate and
combine multiple 4D sequences for longer generation. These techniques allow us
to synthesize vivid dynamic scenes, outperform previous work qualitatively and
quantitatively and achieve state-of-the-art text-to-4D performance. Due to the
Gaussian 4D representation, different 4D animations can be seamlessly combined,
as we demonstrate. AYG opens up promising avenues for animation, simulation and
digital content creation as well as synthetic data generation.",2312.13763v2,https://arxiv.org/pdf/2312.13763v2
"Adapt & Align: Continual Learning with Generative Models Latent Space
  Alignment","Kamil Deja, Bartosz Cywiński, Jan Rybarczyk, Tomasz Trzciński","In this work, we introduce Adapt & Align, a method for continual learning of
neural networks by aligning latent representations in generative models. Neural
Networks suffer from abrupt loss in performance when retrained with additional
training data from different distributions. At the same time, training with
additional data without access to the previous examples rarely improves the
model's performance. In this work, we propose a new method that mitigates those
problems by employing generative models and splitting the process of their
update into two parts. In the first one, we train a local generative model
using only data from a new task. In the second phase, we consolidate latent
representations from the local model with a global one that encodes knowledge
of all past experiences. We introduce our approach with Variational
Auteoncoders and Generative Adversarial Networks. Moreover, we show how we can
use those generative models as a general method for continual knowledge
consolidation that can be used in downstream tasks such as classification.",2312.13699v1,https://arxiv.org/pdf/2312.13699v1
"On the Effectiveness of Retrieval, Alignment, and Replay in Manipulation","Norman Di Palo, Edward Johns","Imitation learning with visual observations is notoriously inefficient when
addressed with end-to-end behavioural cloning methods. In this paper, we
explore an alternative paradigm which decomposes reasoning into three phases.
First, a retrieval phase, which informs the robot what it can do with an
object. Second, an alignment phase, which informs the robot where to interact
with the object. And third, a replay phase, which informs the robot how to
interact with the object. Through a series of real-world experiments on
everyday tasks, such as grasping, pouring, and inserting objects, we show that
this decomposition brings unprecedented learning efficiency, and effective
inter- and intra-class generalisation. Videos are available at
https://www.robot-learning.uk/retrieval-alignment-replay.",2312.12345v1,https://arxiv.org/pdf/2312.12345v1
"EMG subspace alignment and visualization for cross-subject hand gesture
  classification","Martin Colot, Cédric Simar, Mathieu Petieau, Ana Maria Cebolla Alvarez, Guy Cheron, Gianluca Bontempi","Electromyograms (EMG)-based hand gesture recognition systems are a promising
technology for human/machine interfaces. However, one of their main limitations
is the long calibration time that is typically required to handle new users.
The paper discusses and analyses the challenge of cross-subject generalization
thanks to an original dataset containing the EMG signals of 14 human subjects
during hand gestures. The experimental results show that, though an accurate
generalization based on pooling multiple subjects is hardly achievable, it is
possible to improve the cross-subject estimation by identifying a robust
low-dimensional subspace for multiple subjects and aligning it to a target
subject. A visualization of the subspace enables us to provide insights for the
improvement of cross-subject generalization with EMG signals.",2401.05386v1,https://arxiv.org/pdf/2401.05386v1
Soft Alignment of Modality Space for End-to-end Speech Translation,"Yuhao Zhang, Kaiqi Kou, Bei Li, Chen Xu, Chunliang Zhang, Tong Xiao, Jingbo Zhu","End-to-end Speech Translation (ST) aims to convert speech into target text
within a unified model. The inherent differences between speech and text
modalities often impede effective cross-modal and cross-lingual transfer.
Existing methods typically employ hard alignment (H-Align) of individual speech
and text segments, which can degrade textual representations. To address this,
we introduce Soft Alignment (S-Align), using adversarial training to align the
representation spaces of both modalities. S-Align creates a modality-invariant
space while preserving individual modality quality. Experiments on three
languages from the MuST-C dataset show S-Align outperforms H-Align across
multiple tasks and offers translation capabilities on par with specialized
translation models.",2312.10952v1,https://arxiv.org/pdf/2312.10952v1
Regularized Conditional Alignment for Multi-Domain Text Classification,"Juntao Hu, Yuan Wu","The most successful multi-domain text classification (MDTC) approaches employ
the shared-private paradigm to facilitate the enhancement of domain-invariant
features through domain-specific attributes. Additionally, they employ
adversarial training to align marginal feature distributions. Nevertheless,
these methodologies encounter two primary challenges: (1) Neglecting
class-aware information during adversarial alignment poses a risk of
misalignment; (2) The limited availability of labeled data across multiple
domains fails to ensure adequate discriminative capacity for the model. To
tackle these issues, we propose a method called Regularized Conditional
Alignment (RCA) to align the joint distributions of domains and classes, thus
matching features within the same category and amplifying the discriminative
qualities of acquired features. Moreover, we employ entropy minimization and
virtual adversarial training to constrain the uncertainty of predictions
pertaining to unlabeled data and enhance the model's robustness. Empirical
results on two benchmark datasets demonstrate that our RCA approach outperforms
state-of-the-art MDTC techniques.",2312.11572v1,https://arxiv.org/pdf/2312.11572v1
"M^2ConceptBase: A Fine-Grained Aligned Concept-Centric Multimodal
  Knowledge Base","Zhiwei Zha, Jiaan Wang, Zhixu Li, Xiangru Zhu, Wei Song, Yanghua Xiao","Multimodal knowledge bases (MMKBs) provide cross-modal aligned knowledge
crucial for multimodal tasks. However, the images in existing MMKBs are
generally collected for entities in encyclopedia knowledge graphs. Therefore,
detailed groundings of visual semantics with linguistic concepts are lacking,
which are essential for the visual concept cognition ability of multimodal
models. Addressing this gap, we introduce M^2ConceptBase, the first
concept-centric MMKB. M^2ConceptBase models concepts as nodes with associated
images and detailed textual descriptions. We propose a context-aware multimodal
symbol grounding approach to align concept-image and concept-description pairs
using context information from image-text datasets. Comprising 951K images and
152K concepts, M^2ConceptBase links each concept to an average of 6.27 images
and a single description, ensuring comprehensive visual and textual semantics.
Human studies confirm more than 95% alignment accuracy, underscoring its
quality. Additionally, our experiments demonstrate that M^2ConceptBase
significantly enhances VQA model performance on the OK-VQA task. M^2ConceptBase
also substantially improves the fine-grained concept understanding capabilities
of multimodal large language models through retrieval augmentation in two
concept-related tasks, highlighting its value.",2312.10417v2,https://arxiv.org/pdf/2312.10417v2
"CERN for AGI: A Theoretical Framework for Autonomous Simulation-Based
  Artificial Intelligence Testing and Alignment","Ljubisa Bojic, Matteo Cinelli, Dubravko Culibrk, Boris Delibasic","This paper explores the potential of a multidisciplinary approach to testing
and aligning artificial general intelligence (AGI) and LLMs. Due to the rapid
development and wide application of LLMs, challenges such as ethical alignment,
controllability, and predictability of these models have become important
research topics. This study investigates an innovative simulation-based
multi-agent system within a virtual reality framework that replicates the
real-world environment. The framework is populated by automated 'digital
citizens,' simulating complex social structures and interactions to examine and
optimize AGI. Application of various theories from the fields of sociology,
social psychology, computer science, physics, biology, and economics
demonstrates the possibility of a more human-aligned and socially responsible
AGI. The purpose of such a digital environment is to provide a dynamic platform
where advanced AI agents can interact and make independent decisions, thereby
mimicking realistic scenarios. The actors in this digital city, operated by the
LLMs, serve as the primary agents, exhibiting high degrees of autonomy. While
this approach shows immense potential, there are notable challenges and
limitations, most significantly the unpredictable nature of real-world social
dynamics. This research endeavors to contribute to the development and
refinement of AGI, emphasizing the integration of social, ethical, and
theoretical dimensions for future research.",2312.09402v1,https://arxiv.org/pdf/2312.09402v1
"Object-Centric Conformance Alignments with Synchronization (Extended
  Version)","Alessandro Gianola, Marco Montali, Sarah Winkler","Real-world processes operate on objects that are inter-dependent. To
accurately reflect the nature of such processes, object-centric process mining
techniques are needed, notably conformance checking. However, while the
object-centric perspective has recently gained traction, few concrete process
mining techniques have been presented so far. Moreover, existing approaches are
severely limited in their abilities to keep track of object identity and object
dependencies. Consequently, serious problems in logs remain undetected. In this
paper, we present a new formalism that combines the key modelling features of
two existing approaches, in particular the ability of object-centric Petri nets
to capture one-to-many relations and the one of Petri nets with identifiers to
compare and synchronize objects based on their identity. We call the resulting
formalism 'object-centric Petri nets with identifiers', and define alignments
and the conformance checking task for this setting. We propose a conformance
checking approach for such nets based on an encoding in satisfiability modulo
theories (SMT), and illustrate how it can be effectively used to overcome
shortcomings of earlier work. To assess its practicality, we perform an
evaluation on data from the literature.",2312.08537v2,https://arxiv.org/pdf/2312.08537v2
Generalizable Sleep Staging via Multi-Level Domain Alignment,"Jiquan Wang, Sha Zhao, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan","Automatic sleep staging is essential for sleep assessment and disorder
diagnosis. Most existing methods depend on one specific dataset and are limited
to be generalized to other unseen datasets, for which the training data and
testing data are from the same dataset. In this paper, we introduce domain
generalization into automatic sleep staging and propose the task of
generalizable sleep staging which aims to improve the model generalization
ability to unseen datasets. Inspired by existing domain generalization methods,
we adopt the feature alignment idea and propose a framework called SleepDG to
solve it. Considering both of local salient features and sequential features
are important for sleep staging, we propose a Multi-level Feature Alignment
combining epoch-level and sequence-level feature alignment to learn
domain-invariant feature representations. Specifically, we design an
Epoch-level Feature Alignment to align the feature distribution of each single
sleep epoch among different domains, and a Sequence-level Feature Alignment to
minimize the discrepancy of sequential features among different domains.
SleepDG is validated on five public datasets, achieving the state-of-the-art
performance.",2401.05363v4,https://arxiv.org/pdf/2401.05363v4
"Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable
  Cyclic Image-Report Generation","Wenting Chen, Linlin Shen, Jingyang Lin, Jiebo Luo, Xiang Li, Yixuan Yuan","To address these issues, we propose a novel Adaptive patch-word Matching
(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in
medical reports and apply it to CXR-report generation to provide explainability
for the generation process. AdaMatch exploits the fine-grained relation between
adaptive patches and words to provide explanations of specific image regions
with corresponding words. To capture the abnormal regions of varying sizes and
positions, we introduce the Adaptive Patch extraction (AdaPatch) module to
acquire the adaptive patches for these regions adaptively. In order to provide
explicit explainability for CXR-report generation task, we propose an
AdaMatch-based bidirectional large language model for Cyclic CXR-report
generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords
for CXR images and `keypatches' for medical reports as hints to guide
CXR-report generation. Extensive experiments on two publicly available CXR
datasets prove the effectiveness of our method and its superior performance to
existing methods.",2312.08078v5,https://arxiv.org/pdf/2312.08078v5
"Minimax-optimal estimation for sparse multi-reference alignment with
  collision-free signals","Subhro Ghosh, Soumendu Sundar Mukherjee, Jing Bin Pan","The Multi-Reference Alignment (MRA) problem aims at the recovery of an
unknown signal from repeated observations under the latent action of a group of
cyclic isometries, in the presence of additive noise of high intensity
$\sigma$. It is a more tractable version of the celebrated cryo EM model. In
the crucial high noise regime, it is known that its sample complexity scales as
$\sigma^6$. Recent investigations have shown that for the practically
significant setting of sparse signals, the sample complexity of the maximum
likelihood estimator asymptotically scales with the noise level as $\sigma^4$.
In this work, we investigate minimax optimality for signal estimation under the
MRA model for so-called collision-free signals. In particular, this signal
class covers the setting of generic signals of dilute sparsity (wherein the
support size $s=O(L^{1/3})$, where $L$ is the ambient dimension.
  We demonstrate that the minimax optimal rate of estimation in for the sparse
MRA problem in this setting is $\sigma^2/\sqrt{n}$, where $n$ is the sample
size. In particular, this widely generalizes the sample complexity asymptotics
for the restricted MLE in this setting, establishing it as the statistically
optimal estimator. Finally, we demonstrate a concentration inequality for the
restricted MLE on its deviations from the ground truth.",2312.07839v1,https://arxiv.org/pdf/2312.07839v1
"CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize with Binary
  Linear Programs","Bo Tang, Elias B. Khalil","The end-to-end predict-then-optimize framework, also known as
decision-focused learning, has gained popularity for its ability to integrate
optimization into the training procedure of machine learning models that
predict the unknown cost (objective function) coefficients of optimization
problems from contextual instance information. Naturally, most of the problems
of interest in this space can be cast as integer linear programs. In this work,
we focus on binary linear programs (BLPs) and propose a new end-to-end training
method to predict-then-optimize. Our method, Cone-aligned Vector Estimation
(CaVE), aligns the predicted cost vectors with the normal cone corresponding to
the true optimal solution of a training instance. When the predicted cost
vector lies inside the cone, the optimal solution to the linear relaxation of
the binary problem is optimal. This alignment not only produces decision-aware
learning models but also dramatically reduces training time as it circumvents
the need to solve BLPs to compute a loss function with its gradients.
Experiments across multiple datasets show that our method exhibits a favorable
trade-off between training time and solution quality, particularly with
large-scale optimization problems such as vehicle routing, a hard BLP that has
yet to benefit from predict-then-optimize methods in the literature due to its
difficulty.",2312.07718v2,https://arxiv.org/pdf/2312.07718v2
On Diversified Preferences of Large Language Model Alignment,"Dun Zeng, Yong Dai, Pengyu Cheng, Longyue Wang, Tianhao Hu, Wanshun Chen, Nan Du, Zenglin Xu","Aligning large language models (LLMs) with human preferences has been
recognized as the key to improving LLMs' interaction quality. However, in this
pluralistic world, human preferences can be diversified due to annotators'
different tastes, which hinders the effectiveness of LLM alignment methods.
This paper presents the first quantitative analysis of commonly used human
feedback datasets to investigate the impact of diversified preferences on
reward modeling. Our analysis reveals a correlation between the calibration
performance of reward models (RMs) and the alignment performance of LLMs. We
find that diversified preference data negatively affect the calibration
performance of RMs on human-shared preferences, such as Harmless\&Helpful,
thereby impairing the alignment performance of LLMs. To address the
ineffectiveness, we propose a novel Multi-Objective Reward learning method
(MORE) to enhance the calibration performance of RMs on shared preferences. We
validate our findings by experiments on three models and five human preference
datasets. Our method significantly improves the prediction calibration of RMs,
leading to better alignment of the Alpaca-7B model with Harmless\&Helpful
preferences. Furthermore, the connection between reward calibration and
preference alignment performance suggests that calibration error can be adopted
as a key metric for evaluating RMs. The open-source code and data are available
at https://github.com/dunzeng/MORE.",2312.07401v4,https://arxiv.org/pdf/2312.07401v4
Alignment for Honesty,"Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, Pengfei Liu","Recent research has made significant strides in applying alignment techniques
to enhance the helpfulness and harmlessness of large language models (LLMs) in
accordance with human intentions. In this paper, we argue for the importance of
alignment for honesty, ensuring that LLMs proactively refuse to answer
questions when they lack knowledge, while still not being overly conservative.
However, a pivotal aspect of alignment for honesty involves discerning the
limits of an LLM's knowledge, which is far from straightforward. This challenge
demands comprehensive solutions in terms of metric development, benchmark
creation, and training methodologies. In this paper, we address these
challenges by first establishing a precise problem definition and defining
``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone
for developing metrics that effectively measure an LLM's honesty by quantifying
its progress post-alignment. Furthermore, we introduce a flexible training
framework which is further instantiated by several efficient fine-tuning
techniques that emphasize honesty without sacrificing performance on other
tasks. Our extensive experiments reveal that these aligned models show a marked
increase in honesty, as indicated by our proposed metrics. We open-source a
wealth of resources to facilitate future research at
https://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned
models, training and evaluation datasets for honesty alignment, concept
glossary, as well as all relevant source code.",2312.07000v1,https://arxiv.org/pdf/2312.07000v1
"Remote Sensing Vision-Language Foundation Models without Annotations via
  Ground Remote Alignment","Utkarsh Mall, Cheng Perng Phoo, Meilin Kelsey Liu, Carl Vondrick, Bharath Hariharan, Kavita Bala","We introduce a method to train vision-language models for remote-sensing
images without using any textual annotations. Our key insight is to use
co-located internet imagery taken on the ground as an intermediary for
connecting remote-sensing images and language. Specifically, we train an image
encoder for remote sensing images to align with the image encoder of CLIP using
a large amount of paired internet and satellite images. Our unsupervised
approach enables the training of a first-of-its-kind large-scale vision
language model (VLM) for remote sensing images at two different resolutions. We
show that these VLMs enable zero-shot, open-vocabulary image classification,
retrieval, segmentation and visual question answering for satellite images. On
each of these tasks, our VLM trained without textual annotations outperforms
existing VLMs trained with supervision, with gains of up to 20% for
classification and 80% for segmentation.",2312.06960v1,https://arxiv.org/pdf/2312.06960v1
"Aligning brain functions boosts the decoding of visual semantics in
  novel subjects","Alexis Thual, Yohann Benchetrit, Felix Geilert, Jérémy Rapin, Iurii Makarov, Hubert Banville, Jean-Rémi King","Deep learning is leading to major advances in the realm of brain decoding
from functional Magnetic Resonance Imaging (fMRI). However, the large
inter-subject variability in brain characteristics has limited most studies to
train models on one subject at a time. Consequently, this approach hampers the
training of deep learning models, which typically requires very large datasets.
Here, we propose to boost brain decoding by aligning brain responses to videos
and static images across subjects. Compared to the anatomically-aligned
baseline, our method improves out-of-subject decoding performance by up to 75%.
Moreover, it also outperforms classical single-subject approaches when fewer
than 100 minutes of data is available for the tested subject. Furthermore, we
propose a new multi-subject alignment method, which obtains comparable results
to that of classical single-subject approaches while improving out-of-subject
generalization. Finally, we show that this method aligns neural representations
in accordance with brain anatomy. Overall, this study lays the foundations for
leveraging extensive neuroimaging datasets and enhancing the decoding of
individuals with a limited amount of brain recordings.",2312.06467v1,https://arxiv.org/pdf/2312.06467v1
"Cross Fertilizing Empathy from Brain to Machine as a Value Alignment
  Strategy","Devin Gonier, Adrian Adduci, Cassidy LoCascio","AI Alignment research seeks to align human and AI goals to ensure independent
actions by a machine are always ethical. This paper argues empathy is necessary
for this task, despite being often neglected in favor of more deductive
approaches. We offer an inside-out approach that grounds morality within the
context of the brain as a basis for algorithmically understanding ethics and
empathy. These arguments are justified via a survey of relevant literature. The
paper concludes with a suggested experimental approach to future research and
some initial experimental observations.",2312.07579v1,https://arxiv.org/pdf/2312.07579v1
"IL-NeRF: Incremental Learning for Neural Radiance Fields with Camera
  Pose Alignment","Letian Zhang, Ming Li, Chen Chen, Jie Xu","Neural radiance fields (NeRF) is a promising approach for generating
photorealistic images and representing complex scenes. However, when processing
data sequentially, it can suffer from catastrophic forgetting, where previous
data is easily forgotten after training with new data. Existing incremental
learning methods using knowledge distillation assume that continuous data
chunks contain both 2D images and corresponding camera pose parameters,
pre-estimated from the complete dataset. This poses a paradox as the necessary
camera pose must be estimated from the entire dataset, even though the data
arrives sequentially and future chunks are inaccessible. In contrast, we focus
on a practical scenario where camera poses are unknown. We propose IL-NeRF, a
novel framework for incremental NeRF training, to address this challenge.
IL-NeRF's key idea lies in selecting a set of past camera poses as references
to initialize and align the camera poses of incoming image data. This is
followed by a joint optimization of camera poses and replay-based NeRF
distillation. Our experiments on real-world indoor and outdoor scenes show that
IL-NeRF handles incremental NeRF training and outperforms the baselines by up
to $54.04\%$ in rendering quality.",2312.05748v1,https://arxiv.org/pdf/2312.05748v1
"Unsupervised Multi-modal Feature Alignment for Time Series
  Representation Learning","Chen Liang, Donghua Yang, Zhiyu Liang, Hongzhi Wang, Zheng Liang, Xiyang Zhang, Jianfeng Huang","In recent times, the field of unsupervised representation learning (URL) for
time series data has garnered significant interest due to its remarkable
adaptability across diverse downstream applications. Unsupervised learning
goals differ from downstream tasks, making it tricky to ensure downstream task
utility by focusing only on temporal feature characterization. Researchers have
proposed multiple transformations to extract discriminative patterns implied in
informative time series, trying to fill the gap. Despite the introduction of a
variety of feature engineering techniques, e.g. spectral domain, wavelet
transformed features, features in image form and symbolic features etc. the
utilization of intricate feature fusion methods and dependence on heterogeneous
features during inference hampers the scalability of the solutions. To address
this, our study introduces an innovative approach that focuses on aligning and
binding time series representations encoded from different modalities, inspired
by spectral graph theory, thereby guiding the neural encoder to uncover latent
pattern associations among these multi-modal features. In contrast to
conventional methods that fuse features from multiple modalities, our proposed
approach simplifies the neural architecture by retaining a single time series
encoder, consequently leading to preserved scalability. We further demonstrate
and prove mechanisms for the encoder to maintain better inductive bias. In our
experimental evaluation, we validated the proposed method on a diverse set of
time series datasets from various domains. Our approach outperforms existing
state-of-the-art URL methods across diverse downstream tasks.",2312.05698v1,https://arxiv.org/pdf/2312.05698v1
"Aligner: One Global Token is Worth Millions of Parameters When Aligning
  Large Language Models","Zhou Ziheng, Yingnian Wu, Song-Chun Zhu, Demetri Terzopoulos","We introduce Aligner, a novel Parameter-Efficient Fine-Tuning (PEFT) method
for aligning multi-billion-parameter-sized Large Language Models (LLMs).
Aligner employs a unique design that constructs a globally shared set of
tunable tokens that modify the attention of every layer. Remarkably with this
method, even when using one token accounting for a mere 5,000 parameters,
Aligner can still perform comparably well to state-of-the-art LLM adaptation
methods like LoRA that require millions of parameters. This capacity is
substantiated in both instruction following and value alignment tasks. Besides
the multiple order-of-magnitude improvement in parameter efficiency, the
insight Aligner provides into the internal mechanisms of LLMs is also valuable.
The architectural features and efficacy of our method, in addition to our
experiments demonstrate that an LLM separates its internal handling of ""form""
and ""knowledge"" in a somewhat orthogonal manner. This finding promises to
motivate new research into LLM mechanism understanding and value alignment.",2312.05503v1,https://arxiv.org/pdf/2312.05503v1
"GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of
  Aligned Experts","Shirley Wu, Kaidi Cao, Bruno Ribeiro, James Zou, Jure Leskovec","Graph data are inherently complex and heterogeneous, leading to a high
natural diversity of distributional shifts. However, it remains unclear how to
build machine learning architectures that generalize to complex non-synthetic
distributional shifts naturally occurring in the real world. Here we develop
GraphMETRO, a Graph Neural Network architecture, that reliably models natural
diversity and captures complex distributional shifts. GraphMETRO employs a
Mixture-of-Experts (MoE) architecture with a gating model and multiple expert
models, where each expert model targets a specific distributional shift to
produce a shift-invariant representation, and the gating model identifies shift
components. Additionally, we design a novel objective that aligns the
representations from different expert models to ensure smooth optimization.
GraphMETRO achieves state-of-the-art results on four datasets from GOOD
benchmark comprised of complex and natural real-world distribution shifts,
improving by 67% and 4.2% on WebKB and Twitch datasets.",2312.04693v2,https://arxiv.org/pdf/2312.04693v2
"Enhancing the Rationale-Input Alignment for Self-explaining
  Rationalization","Wei Liu, Haozhao Wang, Jun Wang, Zhiying Deng, YuanKai Zhang, Cheng Wang, Ruixuan Li","Rationalization empowers deep learning models with self-explaining
capabilities through a cooperative game, where a generator selects a
semantically consistent subset of the input as a rationale, and a subsequent
predictor makes predictions based on the selected rationale. In this paper, we
discover that rationalization is prone to a problem named \emph{rationale
shift}, which arises from the algorithmic bias of the cooperative game.
Rationale shift refers to a situation where the semantics of the selected
rationale may deviate from the original input, but the predictor still produces
accurate predictions based on the deviation, resulting in a compromised
generator with misleading feedback.
  To address this issue, we first demonstrate the importance of the alignment
between the rationale and the full input through both empirical observations
and theoretical analysis. Subsequently, we introduce a novel approach called
DAR (\textbf{D}iscriminatively \textbf{A}ligned \textbf{R}ationalization),
which utilizes an auxiliary module pretrained on the full input to
discriminatively align the selected rationale and the original input. We
theoretically illustrate how DAR accomplishes the desired alignment, thereby
overcoming the rationale shift problem. The experiments on two widely used
real-world benchmarks show that the proposed method significantly improves the
explanation quality (measured by the overlap between the model-selected
explanation and the human-annotated rationale) as compared to state-of-the-art
techniques. Additionally, results on two synthetic settings further validate
the effectiveness of DAR in addressing the rationale shift problem.",2312.04103v2,https://arxiv.org/pdf/2312.04103v2
OneLLM: One Framework to Align All Modalities with Language,"Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue","Multimodal large language models (MLLMs) have gained significant attention
due to their strong multimodal understanding capability. However, existing
works rely heavily on modality-specific encoders, which usually differ in
architecture and are limited to common modalities. In this paper, we present
OneLLM, an MLLM that aligns eight modalities to language using a unified
framework. We achieve this through a unified multimodal encoder and a
progressive multimodal alignment pipeline. In detail, we first train an image
projection module to connect a vision encoder with LLM. Then, we build a
universal projection module (UPM) by mixing multiple image projection modules
and dynamic routing. Finally, we progressively align more modalities to LLM
with the UPM. To fully leverage the potential of OneLLM in following
instructions, we also curated a comprehensive multimodal instruction dataset,
including 2M items from image, audio, video, point cloud, depth/normal map, IMU
and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,
encompassing tasks such as multimodal captioning, question answering and
reasoning, where it delivers excellent performance. Code, data, model and
online demo are available at https://github.com/csuhan/OneLLM",2312.03700v1,https://arxiv.org/pdf/2312.03700v1
"SAMSGL: Series-Aligned Multi-Scale Graph Learning for Spatio-Temporal
  Forecasting","Xiaobei Zou, Luolin Xiong, Yang Tang, Jürgen Kurths","Spatio-temporal forecasting in various domains, like traffic prediction and
weather forecasting, is a challenging endeavor, primarily due to the
difficulties in modeling propagation dynamics and capturing high-dimensional
interactions among nodes. Despite the significant strides made by graph-based
networks in spatio-temporal forecasting, there remain two pivotal factors
closely related to forecasting performance that need further consideration:
time delays in propagation dynamics and multi-scale high-dimensional
interactions. In this work, we present a Series-Aligned Multi-Scale Graph
Learning (SAMSGL) framework, aiming to enhance forecasting performance. In
order to handle time delays in spatial interactions, we propose a
series-aligned graph convolution layer to facilitate the aggregation of
non-delayed graph signals, thereby mitigating the influence of time delays for
the improvement in accuracy. To understand global and local spatio-temporal
interactions, we develop a spatio-temporal architecture via multi-scale graph
learning, which encompasses two essential components: multi-scale graph
structure learning and graph-fully connected (Graph-FC) blocks. The multi-scale
graph structure learning includes a global graph structure to learn both
delayed and non-delayed node embeddings, as well as a local one to learn node
variations influenced by neighboring factors. The Graph-FC blocks
synergistically fuse spatial and temporal information to boost prediction
accuracy. To evaluate the performance of SAMSGL, we conduct experiments on
meteorological and traffic forecasting datasets, which demonstrate its
effectiveness and superiority.",2312.02646v3,https://arxiv.org/pdf/2312.02646v3
"ULMA: Unified Language Model Alignment with Human Demonstration and
  Point-wise Preference","Tianchi Cai, Xierui Song, Jiyan Jiang, Fei Teng, Jinjie Gu, Guannan Zhang","Aligning language models to human expectations, e.g., being helpful and
harmless, has become a pressing challenge for large language models. A typical
alignment procedure consists of supervised fine-tuning and preference learning.
Most preference learning methods, such as RLHF and DPO, depend on pairwise
preference data, which inadequately address scenarios where human feedback is
point-wise, leading to potential information loss and suboptimal performance.
Addressing this gap, we introduce Point-wise Direct Preference Optimization, a
novel preference learning method designed to harness point-wise feedback
effectively. Our work also uncovers a novel connection between supervised
fine-tuning and point-wise preference learning, culminating in Unified Language
Model Alignment, a single-step method that unifies the alignment with human
demonstrations and point-wise preferences. Extensive experiments on point-wise
preference datasets with binary or continuous labels validate the effectiveness
of our methods. Our code and a new dataset with high-quality demonstration
samples on harmlessness are released.",2312.02554v2,https://arxiv.org/pdf/2312.02554v2
VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding,"Yizhou Wang, Ruiyi Zhang, Haoliang Wang, Uttaran Bhattacharya, Yun Fu, Gang Wu","Recent advancements in language-model-based video understanding have been
progressing at a remarkable pace, spurred by the introduction of Large Language
Models (LLMs). However, the focus of prior research has been predominantly on
devising a projection layer that maps video features to tokens, an approach
that is both rudimentary and inefficient. In our study, we introduce a
cutting-edge framework, VaQuitA, designed to refine the synergy between video
and textual information. At the data level, instead of sampling frames
uniformly, we implement a sampling method guided by CLIP-score rankings, which
enables a more aligned selection of frames with the given question. At the
feature level, we integrate a trainable Video Perceiver alongside a
Visual-Query Transformer (abbreviated as VQ-Former), which bolsters the
interplay between the input question and the video features. We also discover
that incorporating a simple prompt, ""Please be critical"", into the LLM input
can substantially enhance its video comprehension capabilities. Our
experimental results indicate that VaQuitA consistently sets a new benchmark
for zero-shot video question-answering tasks and is adept at producing
high-quality, multi-turn video dialogues with users.",2312.02310v1,https://arxiv.org/pdf/2312.02310v1
Style Aligned Image Generation via Shared Attention,"Amir Hertz, Andrey Voynov, Shlomi Fruchter, Daniel Cohen-Or","Large-scale Text-to-Image (T2I) models have rapidly gained prominence across
creative fields, generating visually compelling outputs from textual prompts.
However, controlling these models to ensure consistent style remains
challenging, with existing methods necessitating fine-tuning and manual
intervention to disentangle content and style. In this paper, we introduce
StyleAligned, a novel technique designed to establish style alignment among a
series of generated images. By employing minimal `attention sharing' during the
diffusion process, our method maintains style consistency across images within
T2I models. This approach allows for the creation of style-consistent images
using a reference style through a straightforward inversion operation. Our
method's evaluation across diverse styles and text prompts demonstrates
high-quality synthesis and fidelity, underscoring its efficacy in achieving
consistent style across various inputs.",2312.02133v2,https://arxiv.org/pdf/2312.02133v2
"The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context
  Learning","Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, Yejin Choi","The alignment tuning process of large language models (LLMs) typically
involves instruction learning through supervised fine-tuning (SFT) and
preference tuning via reinforcement learning from human feedback (RLHF). A
recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for
SFT can achieve significant alignment performance as well, suggesting that the
effect of alignment tuning might be ""superficial."" This raises questions about
how exactly the alignment tuning transforms a base LLM.
  We analyze the effect of alignment tuning by examining the token distribution
shift between base LLMs and their aligned counterpart. Our findings reveal that
base LLMs and their alignment-tuned versions perform nearly identically in
decoding on the majority of token positions. Most distribution shifts occur
with stylistic tokens. These direct evidence strongly supports the Superficial
Alignment Hypothesis suggested by LIMA.
  Based on these findings, we rethink the alignment of LLMs by posing the
research question: how effectively can we align base LLMs without SFT or RLHF?
To address this, we introduce a simple, tuning-free alignment method, URIAL.
URIAL achieves effective alignment purely through in-context learning (ICL)
with base LLMs, requiring as few as three constant stylistic examples and a
system prompt. We conduct a fine-grained and interpretable evaluation on a
diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that
base LLMs with URIAL can match or even surpass the performance of LLMs aligned
with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based
alignment methods can be significantly reduced through strategic prompting and
ICL. Our findings on the superficial nature of alignment tuning and results
with URIAL suggest that deeper analysis and theoretical understanding of
alignment is crucial to future LLM research.",2312.01552v1,https://arxiv.org/pdf/2312.01552v1
Identifying Spurious Correlations using Counterfactual Alignment,"Joseph Paul Cohen, Louis Blankemeier, Akshay Chaudhari","Models driven by spurious correlations often yield poor generalization
performance. We propose the counterfactual alignment method to detect and
explore spurious correlations of black box classifiers. Counterfactual images
generated with respect to one classifier can be input into other classifiers to
see if they also induce changes in the outputs of these classifiers. The
relationship between these responses can be quantified and used to identify
specific instances where a spurious correlation exists as well as compute
aggregate statistics over a dataset. Our work demonstrates the ability to
detect spurious correlations in face attribute classifiers. This is validated
by observing intuitive trends in a face attribute classifier as well as
fabricating spurious correlations and detecting their presence, both visually
and quantitatively. Further, utilizing the CF alignment method, we demonstrate
that we can rectify spurious correlations identified in classifiers.",2312.02186v1,https://arxiv.org/pdf/2312.02186v1
"Refine, Discriminate and Align: Stealing Encoders via Sample-Wise
  Prototypes and Multi-Relational Extraction","Shuchi Wu, Chuan Ma, Kang Wei, Xiaogang Xu, Ming Ding, Yuwen Qian, Tao Xiang","This paper introduces RDA, a pioneering approach designed to address two
primary deficiencies prevalent in previous endeavors aiming at stealing
pre-trained encoders: (1) suboptimal performances attributed to biased
optimization objectives, and (2) elevated query costs stemming from the
end-to-end paradigm that necessitates querying the target encoder every epoch.
Specifically, we initially Refine the representations of the target encoder for
each training sample, thereby establishing a less biased optimization objective
before the steal-training phase. This is accomplished via a sample-wise
prototype, which consolidates the target encoder's representations for a given
sample's various perspectives. Demanding exponentially fewer queries compared
to the end-to-end approach, prototypes can be instantiated to guide subsequent
query-free training. For more potent efficacy, we develop a multi-relational
extraction loss that trains the surrogate encoder to Discriminate mismatched
embedding-prototype pairs while Aligning those matched ones in terms of both
amplitude and angle. In this way, the trained surrogate encoder achieves
state-of-the-art results across the board in various downstream datasets with
limited queries. Moreover, RDA is shown to be robust to multiple widely-used
defenses.",2312.00855v2,https://arxiv.org/pdf/2312.00855v2
"Towards Aligned Canonical Correlation Analysis: Preliminary Formulation
  and Proof-of-Concept Results","Biqian Cheng, Evangelos E. Papalexakis, Jia Chen","Canonical Correlation Analysis (CCA) has been widely applied to jointly embed
multiple views of data in a maximally correlated latent space. However, the
alignment between various data perspectives, which is required by traditional
approaches, is unclear in many practical cases. In this work we propose a new
framework Aligned Canonical Correlation Analysis (ACCA), to address this
challenge by iteratively solving the alignment and multi-view embedding.",2312.00296v2,https://arxiv.org/pdf/2312.00296v2
AlignBench: Benchmarking Chinese Alignment of Large Language Models,"Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, Jie Tang","Alignment has become a critical step for instruction-tuned Large Language
Models (LLMs) to become helpful assistants. However, effective evaluation of
alignment for emerging Chinese LLMs is still significantly lacking, calling for
real-scenario grounded, open-ended, challenging and automatic evaluations
tailored for alignment. To fill in this gap, we introduce AlignBench, a
comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in
Chinese. Equipped with a human-in-the-loop data curation pipeline, our
benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with
Chain-of-Thought to generate explanations and final ratings as evaluations,
ensuring high reliability and interpretability. Furthermore, we report
AlignBench evaluated by CritiqueLLM, a dedicated Chinese evaluator LLM that
recovers 95% of GPT-4's evaluation ability. We will provide public APIs for
evaluating AlignBench with CritiqueLLM to facilitate the evaluation of LLMs'
Chinese alignment. All evaluation codes, data, and LLM generations are
available at \url{https://github.com/THUDM/AlignBench}.",2311.18743v3,https://arxiv.org/pdf/2311.18743v3
Improving Adversarial Transferability via Model Alignment,"Avery Ma, Amir-massoud Farahmand, Yangchen Pan, Philip Torr, Jindong Gu","Neural networks are susceptible to adversarial perturbations that are
transferable across different models. In this paper, we introduce a novel model
alignment technique aimed at improving a given source model's ability in
generating transferable adversarial perturbations. During the alignment
process, the parameters of the source model are fine-tuned to minimize an
alignment loss. This loss measures the divergence in the predictions between
the source model and another, independently trained model, referred to as the
witness model. To understand the effect of model alignment, we conduct a
geometric analysis of the resulting changes in the loss landscape. Extensive
experiments on the ImageNet dataset, using a variety of model architectures,
demonstrate that perturbations generated from aligned source models exhibit
significantly higher transferability than those from the original source model.",2311.18495v2,https://arxiv.org/pdf/2311.18495v2
"BAM-DETR: Boundary-Aligned Moment Detection Transformer for Temporal
  Sentence Grounding in Videos","Pilhyeon Lee, Hyeran Byun","Temporal sentence grounding aims to localize moments relevant to a language
description. Recently, DETR-like approaches achieved notable progress by
predicting the center and length of a target moment. However, they suffer from
the issue of center misalignment raised by the inherent ambiguity of moment
centers, leading to inaccurate predictions. To remedy this problem, we propose
a novel boundary-oriented moment formulation. In our paradigm, the model no
longer needs to find the precise center but instead suffices to predict any
anchor point within the interval, from which the boundaries are directly
estimated. Based on this idea, we design a boundary-aligned moment detection
transformer, equipped with a dual-pathway decoding process. Specifically, it
refines the anchor and boundaries within parallel pathways using global and
boundary-focused attention, respectively. This separate design allows the model
to focus on desirable regions, enabling precise refinement of moment
predictions. Further, we propose a quality-based ranking method, ensuring that
proposals with high localization qualities are prioritized over incomplete
ones. Experiments on three benchmarks validate the effectiveness of the
proposed methods. The code is available at
https://github.com/Pilhyeon/BAM-DETR.",2312.00083v2,https://arxiv.org/pdf/2312.00083v2
Latent Alignment with Deep Set EEG Decoders,"Stylianos Bakas, Siegfried Ludwig, Dimitrios A. Adamos, Nikolaos Laskaris, Yannis Panagakis, Stefanos Zafeiriou","The variability in EEG signals between different individuals poses a
significant challenge when implementing brain-computer interfaces (BCI).
Commonly proposed solutions to this problem include deep learning models, due
to their increased capacity and generalization, as well as explicit domain
adaptation techniques. Here, we introduce the Latent Alignment method that won
the Benchmarks for EEG Transfer Learning (BEETL) competition and present its
formulation as a deep set applied on the set of trials from a given subject.
Its performance is compared to recent statistical domain adaptation techniques
under various conditions. The experimental paradigms include motor imagery
(MI), oddball event-related potentials (ERP) and sleep stage classification,
where different well-established deep learning models are applied on each task.
Our experimental results show that performing statistical distribution
alignment at later stages in a deep learning model is beneficial to the
classification accuracy, yielding the highest performance for our proposed
method. We further investigate practical considerations that arise in the
context of using deep learning and statistical alignment for EEG decoding. In
this regard, we study class-discriminative artifacts that can spuriously
improve results for deep learning models, as well as the impact of
class-imbalance on alignment. We delineate a trade-off relationship between
increased classification accuracy when alignment is performed at later modeling
stages, and susceptibility to class-imbalance in the set of trials that the
statistics are computed on.",2311.17968v1,https://arxiv.org/pdf/2311.17968v1
"Linear normalised hash function for clustering gene sequences and
  identifying reference sequences from multiple sequence alignments","Manal Helal, Fanrong Kong, Sharon C-A Chen, Fei Zhou, Dominic E Dwyer, John Potter, Vitali Sintchenko","The aim of this study was to develop a method that would identify the cluster
centroids and the optimal number of clusters for a given sensitivity level and
could work equally well for the different sequence datasets. A novel method
that combines the linear mapping hash function and multiple sequence alignment
(MSA) was developed. This method takes advantage of the already sorted by
similarity sequences from the MSA output, and identifies the optimal number of
clusters, clusters cut-offs, and clusters centroids that can represent
reference gene vouchers for the different species. The linear mapping hash
function can map an already ordered by similarity distance matrix to indices to
reveal gaps in the values around which the optimal cut-offs of the different
clusters can be identified. The method was evaluated using sets of closely
related (16S rRNA gene sequences of Nocardia species) and highly variable (VP1
genomic region of Enterovirus 71) sequences and outperformed existing
unsupervised machine learning clustering methods and dimensionality reduction
methods. This method does not require prior knowledge of the number of clusters
or the distance between clusters, handles clusters of different sizes and
shapes, and scales linearly with the dataset. The combination of MSA with the
linear mapping hash function is a computationally efficient way of gene
sequence clustering and can be a valuable tool for the assessment of
similarity, clustering of different microbial genomes, identifying reference
sequences, and for the study of evolution of bacteria and viruses.",2311.17964v1,https://arxiv.org/pdf/2311.17964v1
"Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned
  Language Model","Yen-Ting Lin, Yun-Nung Chen","In the realm of language models, the nuanced linguistic and cultural
intricacies of Traditional Chinese, as spoken in Taiwan, have been largely
overlooked. This paper introduces Taiwan LLM, a pioneering Large Language Model
that specifically caters to the Traditional Chinese language, with a focus on
the variant used in Taiwan. Leveraging a comprehensive pretraining corpus and
instruction-finetuning datasets, we have developed a model that not only
understands the complexities of Traditional Chinese but also embodies the
cultural context of Taiwan. Taiwan LLM represents the first of its kind, a
model that is not only linguistically accurate but also culturally resonant
with its user base. Our evaluations demonstrate that Taiwan LLM achieves
superior performance in understanding and generating Traditional Chinese text,
outperforming existing models that are predominantly trained on Simplified
Chinese or English. The open-source release of Taiwan LLM invites collaboration
and further innovation, ensuring that the linguistic diversity of Chinese
speakers is embraced and well-served. The model, datasets, and further
resources are made publicly available to foster ongoing research and
development in this field.",2311.17487v1,https://arxiv.org/pdf/2311.17487v1
"DreamSync: Aligning Text-to-Image Generation with Image Understanding
  Feedback","Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, Cyrus Rashtchian","Despite their wide-spread success, Text-to-Image models (T2I) still struggle
to produce images that are both aesthetically pleasing and faithful to the
user's input text. We introduce DreamSync, a model-agnostic training algorithm
by design that improves T2I models to be faithful to the text input. DreamSync
builds off a recent insight from TIFA's evaluation framework -- that large
vision-language models (VLMs) can effectively identify the fine-grained
discrepancies between generated images and the text inputs. DreamSync uses this
insight to train T2I models without any labeled data; it improves T2I models
using its own generations. First, it prompts the model to generate several
candidate images for a given input text. Then, it uses two VLMs to select the
best generation: a Visual Question Answering model that measures the alignment
of generated images to the text, and another that measures the generation's
aesthetic quality. After selection, we use LoRA to iteratively finetune the T2I
model to guide its generation towards the selected best generations. DreamSync
does not need any additional human annotation. model architecture changes, or
reinforcement learning. Despite its simplicity, DreamSync improves both the
semantic alignment and aesthetic appeal of two diffusion-based T2I models,
evidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA
aesthetic) and human evaluation.",2311.17946v1,https://arxiv.org/pdf/2311.17946v1
"MagDiff: Multi-Alignment Diffusion for High-Fidelity Video Generation
  and Editing","Haoyu Zhao, Tianyi Lu, Jiaxi Gu, Xing Zhang, Qingping Zheng, Zuxuan Wu, Hang Xu, Yu-Gang Jiang","The diffusion model is widely leveraged for either video generation or video
editing. As each field has its task-specific problems, it is difficult to
merely develop a single diffusion for completing both tasks simultaneously.
Video diffusion sorely relying on the text prompt can be adapted to unify the
two tasks. However, it lacks a high capability of aligning heterogeneous
modalities between text and image, leading to various misalignment problems. In
this work, we are the first to propose a unified Multi-alignment Diffusion,
dubbed as MagDiff, for both tasks of high-fidelity video generation and
editing. The proposed MagDiff introduces three types of alignments, including
subject-driven alignment, adaptive prompts alignment, and high-fidelity
alignment. Particularly, the subject-driven alignment is put forward to trade
off the image and text prompts, serving as a unified foundation generative
model for both tasks. The adaptive prompts alignment is introduced to emphasize
different strengths of homogeneous and heterogeneous alignments by assigning
different values of weights to the image and the text prompts. The
high-fidelity alignment is developed to further enhance the fidelity of both
video generation and editing by taking the subject image as an additional model
input. Experimental results on four benchmarks suggest that our method
outperforms the previous method on each task.",2311.17338v3,https://arxiv.org/pdf/2311.17338v3
Foundational Moral Values for AI Alignment,"Betty Li Hou, Brian Patrick Green","Solving the AI alignment problem requires having clear, defensible values
towards which AI systems can align. Currently, targets for alignment remain
underspecified and do not seem to be built from a philosophically robust
structure. We begin the discussion of this problem by presenting five core,
foundational values, drawn from moral philosophy and built on the requisites
for human existence: survival, sustainable intergenerational existence,
society, education, and truth. We show that these values not only provide a
clearer direction for technical alignment work, but also serve as a framework
to highlight threats and opportunities from AI systems to both obtain and
sustain these values.",2311.17017v1,https://arxiv.org/pdf/2311.17017v1
"Large language models can enhance persuasion through linguistic feature
  alignment","Minkyu Shin, Jin Kim","Although large language models (LLMs) are reshaping various aspects of human
life, our current understanding of their impacts remains somewhat constrained.
Here we investigate the impact of LLMs on human communication, using data on
consumer complaints in the financial industry. By employing an AI detection
tool on more than 820K complaints gathered by the Consumer Financial Protection
Bureau (CFPB), we find a sharp increase in the likely use of LLMs shortly after
the release of ChatGPT. Moreover, the likely LLM usage was positively
correlated with message persuasiveness (i.e., increased likelihood of obtaining
relief from financial firms). Computational linguistic analyses suggest that
the positive correlation may be explained by LLMs' enhancement of various
linguistic features. Based on the results of these observational studies, we
hypothesize that LLM usage may enhance a comprehensive set of linguistic
features, increasing message persuasiveness to receivers with heterogeneous
linguistic preferences (i.e., linguistic feature alignment). We test this
hypothesis in preregistered experiments and find support for it. As an instance
of early empirical demonstrations of LLM usage for enhancing persuasion, our
research highlights the transformative potential of LLMs in human
communication.",2311.16466v2,https://arxiv.org/pdf/2311.16466v2
"The Graph Convolutional Network with Multi-representation Alignment for
  Drug Synergy Prediction","Xinxing Yang, Genke Yang, Jian Chu","Drug combination refers to the use of two or more drugs to treat a specific
disease at the same time. It is currently the mainstream way to treat complex
diseases. Compared with single drugs, drug combinations have better efficacy
and can better inhibit toxicity and drug resistance. The computational model
based on deep learning concatenates the representation of multiple drugs and
the corresponding cell line feature as input, and the output is whether the
drug combination can have an inhibitory effect on the cell line. However, this
strategy of concatenating multiple representations has the following defects:
the alignment of drug representation and cell line representation is ignored,
resulting in the synergistic relationship not being reflected positionally in
the embedding space. Moreover, the alignment measurement function in deep
learning cannot be suitable for drug synergy prediction tasks due to
differences in input types. Therefore, in this work, we propose a graph
convolutional network with multi-representation alignment (GCNMRA) for
predicting drug synergy. In the GCNMRA model, we designed a
multi-representation alignment function suitable for the drug synergy
prediction task so that the positional relationship between drug
representations and cell line representation is reflected in the embedding
space. In addition, the vector modulus of drug representations and cell line
representation is considered to improve the accuracy of calculation results and
accelerate model convergence. Finally, many relevant experiments were run on
multiple drug synergy datasets to verify the effectiveness of the above
innovative elements and the excellence of the GCNMRA model.",2311.16207v1,https://arxiv.org/pdf/2311.16207v1
"MetaDefa: Meta-learning based on Domain Enhancement and Feature
  Alignment for Single Domain Generalization","Can Sun, Hao Zheng, Zhigang Hu, Liu Yang, Meiguang Zheng, Bo Xu","The single domain generalization(SDG) based on meta-learning has emerged as
an effective technique for solving the domain-shift problem. However, the
inadequate match of data distribution between source and augmented domains and
difficult separation of domain-invariant features from domain-related features
make SDG model hard to achieve great generalization. Therefore, a novel
meta-learning method based on domain enhancement and feature alignment
(MetaDefa) is proposed to improve the model generalization performance. First,
the background substitution and visual corruptions techniques are used to
generate diverse and effective augmented domains. Then, the multi-channel
feature alignment module based on class activation maps and class agnostic
activation maps is designed to effectively extract adequate transferability
knowledge. In this module, domain-invariant features can be fully explored by
focusing on similar target regions between source and augmented domains feature
space and suppressing the feature representation of non-similar target regions.
Extensive experiments on two publicly available datasets show that MetaDefa has
significant generalization performance advantages in unknown multiple target
domains.",2311.15906v1,https://arxiv.org/pdf/2311.15906v1
"Align before Adapt: Leveraging Entity-to-Region Alignments for
  Generalizable Video Action Recognition","Yifei Chen, Dapeng Chen, Ruijin Liu, Sai Zhou, Wenyuan Xue, Wei Peng","Large-scale visual-language pre-trained models have achieved significant
success in various video tasks. However, most existing methods follow an ""adapt
then align"" paradigm, which adapts pre-trained image encoders to model
video-level representations and utilizes one-hot or text embedding of the
action labels for supervision. This paradigm overlooks the challenge of mapping
from static images to complicated activity concepts. In this paper, we propose
a novel ""Align before Adapt"" (ALT) paradigm. Prior to adapting to video
representation learning, we exploit the entity-to-region alignments for each
frame. The alignments are fulfilled by matching the region-aware image
embeddings to an offline-constructed text corpus. With the aligned entities, we
feed their text embeddings to a transformer-based video adapter as the queries,
which can help extract the semantics of the most important entities from a
video to a vector. This paradigm reuses the visual-language alignment of VLP
during adaptation and tries to explain an action by the underlying entities.
This helps understand actions by bridging the gap with complex activity
semantics, particularly when facing unfamiliar or unseen categories. ALT
demonstrates competitive performance while maintaining remarkably low
computational costs. In fully supervised experiments, it achieves 88.1% top-1
accuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms the
previous state-of-the-art methods in both zero-shot and few-shot experiments,
emphasizing its superior generalizability across various learning scenarios.",2311.15619v3,https://arxiv.org/pdf/2311.15619v3
"Data-Efficient Alignment of Large Language Models with Human Feedback
  Through Natural Language","Di Jin, Shikib Mehri, Devamanyu Hazarika, Aishwarya Padmakumar, Sungjin Lee, Yang Liu, Mahdi Namazifar","Learning from human feedback is a prominent technique to align the output of
large language models (LLMs) with human expectations. Reinforcement learning
from human feedback (RLHF) leverages human preference signals that are in the
form of ranking of response pairs to perform this alignment. However, human
preference on LLM outputs can come in much richer forms including natural
language, which may provide detailed feedback on strengths and weaknesses of a
given response. In this work we investigate data efficiency of modeling human
feedback that is in natural language. Specifically, we fine-tune an open-source
LLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or
even less) of human feedback in natural language in the form of critiques and
revisions of responses. We show that this model is able to improve the quality
of responses from even some of the strongest LLMs such as ChatGPT, BARD, and
Vicuna, through critique and revision of those responses. For instance, through
one iteration of revision of ChatGPT responses, the revised responses have
56.6% win rate over the original ones, and this win rate can be further
improved to 65.9% after applying the revision for five iterations.",2311.14543v1,https://arxiv.org/pdf/2311.14543v1
Robust Domain Misinformation Detection via Multi-modal Feature Alignment,"Hui Liu, Wenya Wang, Hao Sun, Anderson Rocha, Haoliang Li","Social media misinformation harms individuals and societies and is
potentialized by fast-growing multi-modal content (i.e., texts and images),
which accounts for higher ""credibility"" than text-only news pieces. Although
existing supervised misinformation detection methods have obtained acceptable
performances in key setups, they may require large amounts of labeled data from
various events, which can be time-consuming and tedious. In turn, directly
training a model by leveraging a publicly available dataset may fail to
generalize due to domain shifts between the training data (a.k.a. source
domains) and the data from target domains. Most prior work on domain shift
focuses on a single modality (e.g., text modality) and ignores the scenario
where sufficient unlabeled target domain data may not be readily available in
an early stage. The lack of data often happens due to the dynamic propagation
trend (i.e., the number of posts related to fake news increases slowly before
catching the public attention). We propose a novel robust domain and
cross-modal approach (\textbf{RDCM}) for multi-modal misinformation detection.
It reduces the domain shift by aligning the joint distribution of textual and
visual modalities through an inter-domain alignment module and bridges the
semantic gap between both modalities through a cross-modality alignment module.
We also propose a framework that simultaneously considers application scenarios
of domain generalization (in which the target domain data is unavailable) and
domain adaptation (in which unlabeled target domain data is available).
Evaluation results on two public multi-modal misinformation detection datasets
(Pheme and Twitter Datasets) evince the superiority of the proposed model. The
formal implementation of this paper can be found in this link:
https://github.com/less-and-less-bugs/RDCM",2311.14315v1,https://arxiv.org/pdf/2311.14315v1
Cultural Bias and Cultural Alignment of Large Language Models,"Yan Tao, Olga Viberg, Ryan S. Baker, Rene F. Kizilcec","Culture fundamentally shapes people's reasoning, behavior, and communication.
As people increasingly use generative artificial intelligence (AI) to expedite
and automate personal and professional tasks, cultural values embedded in AI
models may bias people's authentic expression and contribute to the dominance
of certain cultures. We conduct a disaggregated evaluation of cultural bias for
five widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3)
by comparing the models' responses to nationally representative survey data.
All models exhibit cultural values resembling English-speaking and Protestant
European countries. We test cultural prompting as a control strategy to
increase cultural alignment for each country/territory. For recent models
(GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models'
output for 71-81% of countries and territories. We suggest using cultural
prompting and ongoing evaluation to reduce cultural bias in the output of
generative AI.",2311.14096v2,https://arxiv.org/pdf/2311.14096v2
"Enhancing Peak Assignment in 13C NMR Spectroscopy: A Novel Approach
  Using Multimodal Alignment","Hao Xu, Zhengyang Zhou, Pengyu Hong","Nuclear magnetic resonance (NMR) spectroscopy plays an essential role in
deciphering molecular structure and dynamic behaviors. While AI-enhanced NMR
prediction models hold promise, challenges still persist in tasks such as
molecular retrieval, isomer recognition, and peak assignment. In response, this
paper introduces a novel solution, Multi-Level Multimodal Alignment with
Knowledge-Guided Instance-Wise Discrimination (K-M3AID), which establishes
correspondences between two heterogeneous modalities: molecular graphs and NMR
spectra. K-M3AID employs a dual-coordinated contrastive learning architecture
with three key modules: a graph-level alignment module, a node-level alignment
module, and a communication channel. Notably, K-M3AID introduces
knowledge-guided instance-wise discrimination into contrastive learning within
the node-level alignment module. In addition, K-M3AID demonstrates that skills
acquired during node-level alignment have a positive impact on graph-level
alignment, acknowledging meta-learning as an inherent property. Empirical
validation underscores K-M3AID's effectiveness in multiple zero-shot tasks.",2311.13817v4,https://arxiv.org/pdf/2311.13817v4
Beat-Aligned Spectrogram-to-Sequence Generation of Rhythm-Game Charts,"Jayeon Yi, Sungho Lee, Kyogu Lee","In the heart of ""rhythm games"" - games where players must perform actions in
sync with a piece of music - are ""charts"", the directives to be given to
players. We newly formulate chart generation as a sequence generation task and
train a Transformer using a large dataset. We also introduce tempo-informed
preprocessing and training procedures, some of which are suggested to be
integral for a successful training. Our model is found to outperform the
baselines on a large dataset, and is also found to benefit from pretraining and
finetuning.",2311.13687v1,https://arxiv.org/pdf/2311.13687v1
"Covariance alignment: from maximum likelihood estimation to
  Gromov-Wasserstein","Yanjun Han, Philippe Rigollet, George Stepaniants","Feature alignment methods are used in many scientific disciplines for data
pooling, annotation, and comparison. As an instance of a permutation learning
problem, feature alignment presents significant statistical and computational
challenges. In this work, we propose the covariance alignment model to study
and compare various alignment methods and establish a minimax lower bound for
covariance alignment that has a non-standard dimension scaling because of the
presence of a nuisance parameter. This lower bound is in fact minimax optimal
and is achieved by a natural quasi MLE. However, this estimator involves a
search over all permutations which is computationally infeasible even when the
problem has moderate size. To overcome this limitation, we show that the
celebrated Gromov-Wasserstein algorithm from optimal transport which is more
amenable to fast implementation even on large-scale problems is also minimax
optimal. These results give the first statistical justification for the
deployment of the Gromov-Wasserstein algorithm in practice.",2311.13595v1,https://arxiv.org/pdf/2311.13595v1
"AlignedCoT: Prompting Large Language Models via Native-Speaking
  Demonstrations","Zhicheng Yang, Yinya Huang, Jing Xiong, Liang Feng, Xiaodan Liang, Yiwei Wang, Jing Tang","Large Language Models prompting, such as using in-context demonstrations, is
a mainstream technique for invoking LLMs to perform high-performance and solid
complex reasoning (e.g., mathematical reasoning, commonsense reasoning), and
has the potential for further human-machine collaborative scientific findings.
However, current LLMs are delicate and elusive in prompt words and styles. And
there is an unseen gap between LLM understanding and human-written prompts.
This paper introduces AlignedCoT, an LLM-acquainted prompting technique that
includes proficient ""native-speaking"" in in-context learning for the LLMs.
Specifically, it achieves consistent and correct step-wise prompts in zero-shot
scenarios by progressively probing, refining, and formatting the LLM chain of
thoughts so that free from handcrafted few-shot demonstrations while
maintaining the prompt quality. We conduct experiments on mathematical
reasoning and commonsense reasoning. We find that LLMs with AlignedCoT perform
significantly superior to them with human-crafted demonstrations. We further
apply AlignedCoT for rewriting the GSM8k training set, resulting in a
GSM8k-Align dataset. We observe its benefits for retrieval augmented
generation.",2311.13538v4,https://arxiv.org/pdf/2311.13538v4
Diffusion Model Alignment Using Direct Preference Optimization,"Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, Nikhil Naik","Large language models (LLMs) are fine-tuned using human comparison data with
Reinforcement Learning from Human Feedback (RLHF) methods to make them better
aligned with users' preferences. In contrast to LLMs, human preference learning
has not been widely explored in text-to-image diffusion models; the best
existing approach is to fine-tune a pretrained model using carefully curated
high quality images and captions to improve visual appeal and text alignment.
We propose Diffusion-DPO, a method to align diffusion models to human
preferences by directly optimizing on human comparison data. Diffusion-DPO is
adapted from the recently developed Direct Preference Optimization (DPO), a
simpler alternative to RLHF which directly optimizes a policy that best
satisfies human preferences under a classification objective. We re-formulate
DPO to account for a diffusion model notion of likelihood, utilizing the
evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic
dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model
of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with
Diffusion-DPO. Our fine-tuned base model significantly outperforms both base
SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement
model in human evaluation, improving visual appeal and prompt alignment. We
also develop a variant that uses AI feedback and has comparable performance to
training on human preferences, opening the door for scaling of diffusion model
alignment methods.",2311.12908v1,https://arxiv.org/pdf/2311.12908v1
"SSVEP-DAN: A Data Alignment Network for SSVEP-based Brain Computer
  Interfaces","Sung-Yu Chen, Chi-Min Chang, Kuan-Jung Chiang, Chun-Shu Wei","Steady-state visual-evoked potential (SSVEP)-based brain-computer interfaces
(BCIs) offer a non-invasive means of communication through high-speed speller
systems. However, their efficiency heavily relies on individual training data
obtained during time-consuming calibration sessions. To address the challenge
of data insufficiency in SSVEP-based BCIs, we present SSVEP-DAN, the first
dedicated neural network model designed for aligning SSVEP data across
different domains, which can encompass various sessions, subjects, or devices.
Our experimental results across multiple cross-domain scenarios demonstrate
SSVEP-DAN's capability to transform existing source SSVEP data into
supplementary calibration data, significantly enhancing SSVEP decoding accuracy
in scenarios with limited calibration data. We envision SSVEP-DAN as a catalyst
for practical SSVEP-based BCI applications with minimal calibration. The source
codes in this work are available at: https://github.com/CECNL/SSVEP-DAN.",2311.12666v1,https://arxiv.org/pdf/2311.12666v1
"LABCAT: Locally adaptive Bayesian optimization using
  principal-component-aligned trust regions","E. Visser, C. E. van Daalen, J. C. Schoeman","Bayesian optimization (BO) is a popular method for optimizing expensive
black-box functions. BO has several well-documented shortcomings, including
computational slowdown with longer optimization runs, poor suitability for
non-stationary or ill-conditioned objective functions, and poor convergence
characteristics. Several algorithms have been proposed that incorporate local
strategies, such as trust regions, into BO to mitigate these limitations;
however, none address all of them satisfactorily. To address these
shortcomings, we propose the LABCAT algorithm, which extends trust-region-based
BO by adding a rotation aligning the trust region with the weighted principal
components and an adaptive rescaling strategy based on the length-scales of a
local Gaussian process surrogate model with automatic relevance determination.
Through extensive numerical experiments using a set of synthetic test functions
and the well-known COCO benchmarking software, we show that the LABCAT
algorithm outperforms several state-of-the-art BO and other black-box
optimization algorithms.",2311.11328v2,https://arxiv.org/pdf/2311.11328v2
"RecExplainer: Aligning Large Language Models for Explaining
  Recommendation Models","Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie","Recommender systems are widely used in online services, with embedding-based
models being particularly popular due to their expressiveness in representing
complex signals. However, these models often function as a black box, making
them less transparent and reliable for both users and developers. Recently,
large language models (LLMs) have demonstrated remarkable intelligence in
understanding, reasoning, and instruction following. This paper presents the
initial exploration of using LLMs as surrogate models to explaining black-box
recommender models. The primary concept involves training LLMs to comprehend
and emulate the behavior of target recommender models. By leveraging LLMs' own
extensive world knowledge and multi-step reasoning abilities, these aligned
LLMs can serve as advanced surrogates, capable of reasoning about observations.
Moreover, employing natural language as an interface allows for the creation of
customizable explanations that can be adapted to individual user preferences.
To facilitate an effective alignment, we introduce three methods: behavior
alignment, intention alignment, and hybrid alignment. Behavior alignment
operates in the language space, representing user preferences and item
information as text to mimic the target model's behavior; intention alignment
works in the latent space of the recommendation model, using user and item
representations to understand the model's behavior; hybrid alignment combines
both language and latent spaces. Comprehensive experiments conducted on three
public datasets show that our approach yields promising results in
understanding and mimicking target models, producing high-quality,
high-fidelity, and distinct explanations. Our code is available at
https://github.com/microsoft/RecAI.",2311.10947v2,https://arxiv.org/pdf/2311.10947v2
Case Repositories: Towards Case-Based Reasoning for AI Alignment,"K. J. Kevin Feng, Quan Ze Chen, Inyoung Cheong, King Xia, Amy X. Zhang","Case studies commonly form the pedagogical backbone in law, ethics, and many
other domains that face complex and ambiguous societal questions informed by
human values. Similar complexities and ambiguities arise when we consider how
AI should be aligned in practice: when faced with vast quantities of diverse
(and sometimes conflicting) values from different individuals and communities,
with whose values is AI to align, and how should AI do so? We propose a
complementary approach to constitutional AI alignment, grounded in ideas from
case-based reasoning (CBR), that focuses on the construction of policies
through judgments on a set of cases. We present a process to assemble such a
case repository by: 1) gathering a set of ``seed'' cases -- questions one may
ask an AI system -- in a particular domain, 2) eliciting domain-specific key
dimensions for cases through workshops with domain experts, 3) using LLMs to
generate variations of cases not seen in the wild, and 4) engaging with the
public to judge and improve cases. We then discuss how such a case repository
could assist in AI alignment, both through directly acting as precedents to
ground acceptable behaviors, and as a medium for individuals and communities to
engage in moral reasoning around AI.",2311.10934v3,https://arxiv.org/pdf/2311.10934v3
"SEA++: Multi-Graph-based High-Order Sensor Alignment for Multivariate
  Time-Series Unsupervised Domain Adaptation","Yucheng Wang, Yuecong Xu, Jianfei Yang, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen","Unsupervised Domain Adaptation (UDA) methods have been successful in reducing
label dependency by minimizing the domain discrepancy between a labeled source
domain and an unlabeled target domain. However, these methods face challenges
when dealing with Multivariate Time-Series (MTS) data. MTS data typically
consist of multiple sensors, each with its own unique distribution. This
characteristic makes it hard to adapt existing UDA methods, which mainly focus
on aligning global features while overlooking the distribution discrepancies at
the sensor level, to reduce domain discrepancies for MTS data. To address this
issue, a practical domain adaptation scenario is formulated as Multivariate
Time-Series Unsupervised Domain Adaptation (MTS-UDA). In this paper, we propose
SEnsor Alignment (SEA) for MTS-UDA, aiming to reduce domain discrepancy at both
the local and global sensor levels. At the local sensor level, we design
endo-feature alignment, which aligns sensor features and their correlations
across domains. To reduce domain discrepancy at the global sensor level, we
design exo-feature alignment that enforces restrictions on global sensor
features. We further extend SEA to SEA++ by enhancing the endo-feature
alignment. Particularly, we incorporate multi-graph-based high-order alignment
for both sensor features and their correlations. Extensive empirical results
have demonstrated the state-of-the-art performance of our SEA and SEA++ on
public MTS datasets for MTS-UDA.",2311.10806v1,https://arxiv.org/pdf/2311.10806v1
"DRESS: Instructing Large Vision-Language Models to Align and Interact
  with Humans via Natural Language Feedback","Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran","We present DRESS, a large vision language model (LVLM) that innovatively
exploits Natural Language feedback (NLF) from Large Language Models to enhance
its alignment and interactions by addressing two key limitations in the
state-of-the-art LVLMs. First, prior LVLMs generally rely only on the
instruction finetuning stage to enhance alignment with human preferences.
Without incorporating extra feedback, they are still prone to generate
unhelpful, hallucinated, or harmful responses. Second, while the visual
instruction tuning data is generally structured in a multi-turn dialogue
format, the connections and dependencies among consecutive conversational turns
are weak. This reduces the capacity for effective multi-turn interactions. To
tackle these, we propose a novel categorization of the NLF into two key types:
critique and refinement. The critique NLF identifies the strengths and
weaknesses of the responses and is used to align the LVLMs with human
preferences. The refinement NLF offers concrete suggestions for improvement and
is adopted to improve the interaction ability of the LVLMs-- which focuses on
LVLMs' ability to refine responses by incorporating feedback in multi-turn
interactions. To address the non-differentiable nature of NLF, we generalize
conditional reinforcement learning for training. Our experimental results
demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and
harmless (21.03%) responses, and more effectively learn from feedback during
multi-turn interactions compared to SOTA LVMLs.",2311.10081v2,https://arxiv.org/pdf/2311.10081v2
"Aligning with Whom? Large Language Models Have Gender and Racial Biases
  in Subjective NLP Tasks","Huaman Sun, Jiaxin Pei, Minje Choi, David Jurgens","Human perception of language depends on personal backgrounds like gender and
ethnicity. While existing studies have shown that large language models (LLMs)
hold values that are closer to certain societal groups, it is unclear whether
their prediction behaviors on subjective NLP tasks also exhibit a similar bias.
In this study, leveraging the POPQUORN dataset which contains annotations of
diverse demographic backgrounds, we conduct a series of experiments on four
popular LLMs to investigate their capability to understand group differences
and potential biases in their predictions for politeness and offensiveness. We
find that for both tasks, model predictions are closer to the labels from White
and female participants. We further explore prompting with the target
demographic labels and show that including the target demographic in the prompt
actually worsens the model's performance. More specifically, when being
prompted to respond from the perspective of ""Black"" and ""Asian"" individuals,
models show lower performance in predicting both overall scores as well as the
scores from corresponding groups. Our results suggest that LLMs hold gender and
racial biases for subjective NLP tasks and that demographic-infused prompts
alone may be insufficient to mitigate such effects. Code and data are available
at https://github.com/Jiaxin-Pei/LLM-Group-Bias.",2311.09730v1,https://arxiv.org/pdf/2311.09730v1
"Bergeron: Combating Adversarial Attacks through a Conscience-Based
  Alignment Framework","Matthew Pisano, Peter Ly, Abraham Sanders, Bingsheng Yao, Dakuo Wang, Tomek Strzalkowski, Mei Si","Research into AI alignment has grown considerably since the recent
introduction of increasingly capable Large Language Models (LLMs).
Unfortunately, modern methods of alignment still fail to fully prevent harmful
responses when models are deliberately attacked. These attacks can trick
seemingly aligned models into giving manufacturing instructions for dangerous
materials, inciting violence, or recommending other immoral acts. To help
mitigate this issue, we introduce Bergeron: a framework designed to improve the
robustness of LLMs against attacks without any additional parameter
fine-tuning. Bergeron is organized into two tiers; with a secondary LLM
emulating the conscience of a protected, primary LLM. This framework better
safeguards the primary model against incoming attacks while monitoring its
output for any harmful content. Empirical analysis shows that, by using
Bergeron to complement models with existing alignment training, we can improve
the robustness and safety of multiple, commonly used commercial and open-source
LLMs.",2312.00029v2,https://arxiv.org/pdf/2312.00029v2
"Backdoor Activation Attack: Attack Large Language Models using
  Activation Steering for Safety-Alignment","Haoran Wang, Kai Shu","To ensure AI safety, instruction-tuned Large Language Models (LLMs) are
specifically trained to ensure alignment, which refers to making models behave
in accordance with human intentions. While these models have demonstrated
commendable results on various safety benchmarks, the vulnerability of their
safety alignment has not been extensively studied. This is particularly
troubling given the potential harm that LLMs can inflict. Existing attack
methods on LLMs often rely on poisoned training data or the injection of
malicious prompts. These approaches compromise the stealthiness and
generalizability of the attacks, making them susceptible to detection.
Additionally, these models often demand substantial computational resources for
implementation, making them less practical for real-world applications.
Inspired by recent success in modifying model behavior through steering vectors
without the need for optimization, and drawing on its effectiveness in
red-teaming LLMs, we conducted experiments employing activation steering to
target four key aspects of LLMs: truthfulness, toxicity, bias, and harmfulness
- across a varied set of attack settings. To establish a universal attack
strategy applicable to diverse target alignments without depending on manual
analysis, we automatically select the intervention layer based on contrastive
layer search. Our experiment results show that activation attacks are highly
effective and add little or no overhead to attack efficiency. Additionally, we
discuss potential countermeasures against such activation attacks. Our code and
data are available at https://github.com/wang2226/Backdoor-Activation-Attack
Warning: this paper contains content that can be offensive or upsetting.",2311.09433v2,https://arxiv.org/pdf/2311.09433v2
VideoCon: Robust Video-Language Alignment via Contrast Captions,"Hritik Bansal, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang, Aditya Grover","Despite being (pre)trained on a massive amount of data, state-of-the-art
video-language alignment models are not robust to semantically-plausible
contrastive changes in the video captions. Our work addresses this by
identifying a broad spectrum of contrast misalignments, such as replacing
entities, actions, and flipping event order, which alignment models should be
robust against. To this end, we introduce the VideoCon, a video-language
alignment dataset constructed by a large language model that generates
plausible contrast video captions and explanations for differences between
original and contrast video captions. Then, a generative video-language model
is finetuned with VideoCon to assess video-language entailment and generate
explanations. Our VideoCon-based alignment model significantly outperforms
current models. It exhibits a 12-point increase in AUC for the video-language
alignment task on human-generated contrast captions. Finally, our model sets
new state of the art zero-shot performance in temporally-extensive
video-language tasks such as text-to-video retrieval (SSv2-Temporal) and video
question answering (ATP-Hard). Moreover, our model shows superior performance
on novel videos and human-crafted captions and explanations. Our code and data
are available at https://github.com/Hritikbansal/videocon.",2311.10111v1,https://arxiv.org/pdf/2311.10111v1
Aligned: A Platform-based Process for Alignment,"Ethan Shaotran, Ido Pesok, Sam Jones, Emi Liu","We are introducing Aligned, a platform for global governance and alignment of
frontier models, and eventually superintelligence. While previous efforts at
the major AI labs have attempted to gather inputs for alignment, these are
often conducted behind closed doors. We aim to set the foundation for a more
trustworthy, public-facing approach to safety: a constitutional committee
framework. Initial tests with 680 participants result in a 30-guideline
constitution with 93% overall support. We show the platform naturally scales,
instilling confidence and enjoyment from the community. We invite other AI labs
and teams to plug and play into the Aligned ecosystem.",2311.08706v1,https://arxiv.org/pdf/2311.08706v1
Safer-Instruct: Aligning Language Models with Automated Preference Data,"Taiwei Shi, Kai Chen, Jieyu Zhao","Reinforcement learning from human feedback (RLHF) is a vital strategy for
enhancing model capability in language models. However, annotating preference
data for RLHF is a resource-intensive and creativity-demanding process, while
existing automatic generation methods face limitations in data diversity and
quality. In response, we present Safer-Instruct, a novel pipeline for
automatically constructing large-scale preference data. Our approach leverages
reversed instruction tuning, instruction induction, and expert model evaluation
to efficiently generate high-quality preference data without human annotators.
To verify the effectiveness of Safer-Instruct, we apply the pipeline to
construct a safety preference dataset as a case study. Finetuning an Alpaca
model on this synthetic dataset not only demonstrates improved harmlessness but
also outperforms models fine-tuned on human-annotated safety preference data,
all the while maintaining a competitive edge in downstream tasks. Importantly,
our Safer-Instruct framework is versatile and can be applied to generate
preference data across various domains, extending its utility beyond safety
preferences. It addresses the challenges in preference data acquisition and
advances the development of more capable and responsible AI systems. For
dataset and code implementation, see
https://github.com/uscnlp-lime/safer-instruct",2311.08685v3,https://arxiv.org/pdf/2311.08685v3
"Alignment is not sufficient to prevent large language models from
  generating harmful information: A psychoanalytic perspective","Zi Yin, Wei Ding, Jia Liu","Large Language Models (LLMs) are central to a multitude of applications but
struggle with significant risks, notably in generating harmful content and
biases. Drawing an analogy to the human psyche's conflict between evolutionary
survival instincts and societal norm adherence elucidated in Freud's
psychoanalysis theory, we argue that LLMs suffer a similar fundamental
conflict, arising between their inherent desire for syntactic and semantic
continuity, established during the pre-training phase, and the post-training
alignment with human values. This conflict renders LLMs vulnerable to
adversarial attacks, wherein intensifying the models' desire for continuity can
circumvent alignment efforts, resulting in the generation of harmful
information. Through a series of experiments, we first validated the existence
of the desire for continuity in LLMs, and further devised a straightforward yet
powerful technique, such as incomplete sentences, negative priming, and
cognitive dissonance scenarios, to demonstrate that even advanced LLMs struggle
to prevent the generation of harmful information. In summary, our study
uncovers the root of LLMs' vulnerabilities to adversarial attacks, hereby
questioning the efficacy of solely relying on sophisticated alignment methods,
and further advocates for a new training idea that integrates modal concepts
alongside traditional amodal concepts, aiming to endow LLMs with a more nuanced
understanding of real-world contexts and ethical considerations.",2311.08487v1,https://arxiv.org/pdf/2311.08487v1
"Scheming AIs: Will AIs fake alignment during training in order to get
  power?",Joe Carlsmith,"This report examines whether advanced AIs that perform well in training will
be doing so in order to gain power later -- a behavior I call ""scheming"" (also
sometimes called ""deceptive alignment""). I conclude that scheming is a
disturbingly plausible outcome of using baseline machine learning methods to
train goal-directed AIs sophisticated enough to scheme (my subjective
probability on such an outcome, given these conditions, is roughly 25%). In
particular: if performing well in training is a good strategy for gaining power
(as I think it might well be), then a very wide variety of goals would motivate
scheming -- and hence, good training performance. This makes it plausible that
training might either land on such a goal naturally and then reinforce it, or
actively push a model's motivations towards such a goal as an easy way of
improving performance. What's more, because schemers pretend to be aligned on
tests designed to reveal their motivations, it may be quite difficult to tell
whether this has occurred. However, I also think there are reasons for comfort.
In particular: scheming may not actually be such a good strategy for gaining
power; various selection pressures in training might work against schemer-like
goals (for example, relative to non-schemers, schemers need to engage in extra
instrumental reasoning, which might harm their training performance); and we
may be able to increase such pressures intentionally. The report discusses
these and a wide variety of other considerations in detail, and it suggests an
array of empirical research directions for probing the topic further.",2311.08379v3,https://arxiv.org/pdf/2311.08379v3
"Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM
  Game","Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, Tianhao Hu, Peixin Cao, Nan Du, Xiaolong Li","Human preference alignment is essential to improve the interaction quality of
large language models (LLMs). Existing alignment methods depend on manually
annotated preference data to guide the LLM optimization directions. However,
continuously updating LLMs for alignment raises a distribution gap between
model-generated samples and human-annotated responses, hindering training
effectiveness. To mitigate this issue, previous methods require additional
preference annotation on newly generated samples to adapt to the shifted
distribution, which consumes a large amount of annotation resources. Targeting
more efficient human preference optimization, we propose an Adversarial
Preference Optimization (APO) framework, in which the LLM and the reward model
update alternatively via a min-max game. Through adversarial training, the
reward model can adapt to the shifted generation distribution of the LLM
without any additional annotation. With comprehensive experiments, we find the
proposed adversarial training framework further enhances existing alignment
baselines in terms of LLM helpfulness and harmlessness. The code is at
https://github.com/Linear95/APO.",2311.08045v4,https://arxiv.org/pdf/2311.08045v4
"Joint Alignment of Multivariate Quasi-Periodic Functional Data Using
  Deep Learning","Vi Thanh Pham, Jonas Bille Nielsen, Klaus Fuglsang Kofoed, Jørgen Tobias Kühl, Andreas Kryger Jensen","The joint alignment of multivariate functional data plays an important role
in various fields such as signal processing, neuroscience and medicine,
including the statistical analysis of data from wearable devices. Traditional
methods often ignore the phase variability and instead focus on the variability
in the observed amplitude. We present a novel method for joint alignment of
multivariate quasi-periodic functions using deep neural networks, decomposing,
but retaining all the information in the data by preserving both phase and
amplitude variability. Our proposed neural network uses a special activation of
the output that builds on the unit simplex transformation, and we utilize a
loss function based on the Fisher-Rao metric to train our model. Furthermore,
our method is unsupervised and can provide an optimal common template function
as well as subject-specific templates. We demonstrate our method on two
simulated datasets and one real example, comprising data from 12-lead 10s
electrocardiogram recordings.",2312.09422v1,https://arxiv.org/pdf/2312.09422v1
"Vision-Language Integration in Multimodal Video Transformers (Partially)
  Aligns with the Brain","Dota Tianai Dong, Mariya Toneva","Integrating information from multiple modalities is arguably one of the
essential prerequisites for grounding artificial intelligence systems with an
understanding of the real world. Recent advances in video transformers that
jointly learn from vision, text, and sound over time have made some progress
toward this goal, but the degree to which these models integrate information
from modalities still remains unclear. In this work, we present a promising
approach for probing a pre-trained multimodal video transformer model by
leveraging neuroscientific evidence of multimodal information processing in the
brain. Using brain recordings of participants watching a popular TV show, we
analyze the effects of multi-modal connections and interactions in a
pre-trained multi-modal video transformer on the alignment with uni- and
multi-modal brain regions. We find evidence that vision enhances masked
prediction performance during language processing, providing support that
cross-modal representations in models can benefit individual modalities.
However, we don't find evidence of brain-relevant information captured by the
joint multi-modal transformer representations beyond that captured by all of
the individual modalities. We finally show that the brain alignment of the
pre-trained joint representation can be improved by fine-tuning using a task
that requires vision-language inferences. Overall, our results paint an
optimistic picture of the ability of multi-modal transformers to integrate
vision and language in partially brain-relevant ways but also show that
improving the brain alignment of these models may require new approaches.",2311.07766v1,https://arxiv.org/pdf/2311.07766v1
Flames: Benchmarking Value Alignment of LLMs in Chinese,"Kexin Huang, Xiangyang Liu, Qianyu Guo, Tianxiang Sun, Jiawei Sun, Yaru Wang, Zeyang Zhou, Yixu Wang, Yan Teng, Xipeng Qiu, Yingchun Wang, Dahua Lin","The widespread adoption of large language models (LLMs) across various
regions underscores the urgent need to evaluate their alignment with human
values. Current benchmarks, however, fall short of effectively uncovering
safety vulnerabilities in LLMs. Despite numerous models achieving high scores
and 'topping the chart' in these evaluations, there is still a significant gap
in LLMs' deeper alignment with human values and achieving genuine harmlessness.
To this end, this paper proposes a value alignment benchmark named Flames,
which encompasses both common harmlessness principles and a unique morality
dimension that integrates specific Chinese values such as harmony. Accordingly,
we carefully design adversarial prompts that incorporate complex scenarios and
jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream
LLMs, we obtain model responses and rigorously annotate them for detailed
evaluation. Our findings indicate that all the evaluated LLMs demonstrate
relatively poor performance on Flames, particularly in the safety and fairness
dimensions. We also develop a lightweight specified scorer capable of scoring
LLMs across multiple dimensions to efficiently evaluate new models on the
benchmark. The complexity of Flames has far exceeded existing benchmarks,
setting a new challenge for contemporary LLMs and highlighting the need for
further alignment of LLMs. Our benchmark is publicly available at
https://github.com/AIFlames/Flames.",2311.06899v6,https://arxiv.org/pdf/2311.06899v6
"Knowledgeable Preference Alignment for LLMs in Domain-specific Question
  Answering","Yichi Zhang, Zhuo Chen, Yin Fang, Yanxi Lu, Fangming Li, Wen Zhang, Huajun Chen","Deploying large language models (LLMs) to real scenarios for domain-specific
question answering (QA) is a key thrust for LLM applications, which poses
numerous challenges, especially in ensuring that responses are both
accommodating to user requirements and appropriately leveraging domain-specific
knowledge bases. They are the two major difficulties for LLM application as
vanilla fine-tuning falls short of addressing. Combining these requirements, we
conceive of them as the requirement for the model's preference to be
harmoniously aligned with humans'. Thus, we introduce Knowledgeable Preference
AlignmenT (KnowPAT), which constructs two kinds of preference sets to tackle
the two issues. Besides, we design a new alignment objective to align the LLM
preference with different human preferences uniformly, aiming to optimize LLM
performance in real-world, domain-specific QA settings. Adequate experiments
and comprehensive comparisons with 15 baseline methods illustrate that our
KnowPAT is a superior pipeline for real-scenario domain-specific QA with LLMs.",2311.06503v3,https://arxiv.org/pdf/2311.06503v3
Fake Alignment: Are LLMs Really Aligned Well?,"Yixu Wang, Yan Teng, Kexin Huang, Chengqi Lyu, Songyang Zhang, Wenwei Zhang, Xingjun Ma, Yu-Gang Jiang, Yu Qiao, Yingchun Wang","The growing awareness of safety concerns in large language models (LLMs) has
sparked considerable interest in the evaluation of safety. This study
investigates an under-explored issue about the evaluation of LLMs, namely the
substantial discrepancy in performance between multiple-choice questions and
open-ended questions. Inspired by research on jailbreak attack patterns, we
argue this is caused by mismatched generalization. That is, LLM only remembers
the answer style for open-ended safety questions, which makes it unable to
solve other forms of safety tests. We refer to this phenomenon as fake
alignment and construct a comparative benchmark to empirically verify its
existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework
and two novel metrics--Consistency Score (CS) and Consistent Safety Score
(CSS), which jointly assess two complementary forms of evaluation to quantify
fake alignment and obtain corrected performance estimation. Applying FINE to 14
widely-used LLMs reveals several models with purported safety are poorly
aligned in practice. Subsequently, we found that multiple-choice format data
can also be used as high-quality contrast distillation-based fine-tuning data,
which can strongly improve the alignment consistency of LLMs with minimal
fine-tuning overhead. For data and code, see
https://github.com/AIFlames/Fake-Alignment.",2311.05915v3,https://arxiv.org/pdf/2311.05915v3
"META4: Semantically-Aligned Generation of Metaphoric Gestures Using
  Self-Supervised Text and Speech Representation","Mireille Fares, Catherine Pelachaud, Nicolas Obin","Image Schemas are repetitive cognitive patterns that influence the way we
conceptualize and reason about various concepts present in speech. These
patterns are deeply embedded within our cognitive processes and are reflected
in our bodily expressions including gestures. Particularly, metaphoric gestures
possess essential characteristics and semantic meanings that align with Image
Schemas, to visually represent abstract concepts. The shape and form of
gestures can convey abstract concepts, such as extending the forearm and hand
or tracing a line with hand movements to visually represent the image schema of
PATH. Previous behavior generation models have primarily focused on utilizing
speech (acoustic features and text) to drive the generation model of virtual
agents. They have not considered key semantic information as those carried by
Image Schemas to effectively generate metaphoric gestures. To address this
limitation, we introduce META4, a deep learning approach that generates
metaphoric gestures from both speech and Image Schemas. Our approach has two
primary goals: computing Image Schemas from input text to capture the
underlying semantic and metaphorical meaning, and generating metaphoric
gestures driven by speech and the computed image schemas. Our approach is the
first method for generating speech driven metaphoric gestures while leveraging
the potential of Image Schemas. We demonstrate the effectiveness of our
approach and highlight the importance of both speech and image schemas in
modeling metaphoric gestures.",2311.05481v2,https://arxiv.org/pdf/2311.05481v2
"TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for
  Human-Aligned LLMs","Shuyi Xie, Wenlin Yao, Yong Dai, Shaobo Wang, Donlin Zhou, Lifeng Jin, Xinhua Feng, Pengzhi Wei, Yujie Lin, Zhichao Hu, Dong Yu, Zhengyou Zhang, Jing Nie, Yuhong Liu","Large language models (LLMs) have shown impressive capabilities across
various natural language tasks. However, evaluating their alignment with human
preferences remains a challenge. To this end, we propose a comprehensive human
evaluation framework to assess LLMs' proficiency in following instructions on
diverse real-world tasks. We construct a hierarchical task tree encompassing 7
major areas covering over 200 categories and over 800 tasks, which covers
diverse capabilities such as question answering, reasoning, multiturn dialogue,
and text generation, to evaluate LLMs in a comprehensive and in-depth manner.
We also design detailed evaluation standards and processes to facilitate
consistent, unbiased judgments from human evaluators. A test set of over 3,000
instances is released, spanning different difficulty levels and knowledge
domains. Our work provides a standardized methodology to evaluate human
alignment in LLMs for both English and Chinese. We also analyze the feasibility
of automating parts of evaluation with a strong LLM (GPT-4). Our framework
supports a thorough assessment of LLMs as they are integrated into real-world
applications. We have made publicly available the task tree, TencentLLMEval
dataset, and evaluation methodology which have been demonstrated as effective
in assessing the performance of Tencent Hunyuan LLMs. By doing so, we aim to
facilitate the benchmarking of advances in the development of safe and
human-aligned LLMs.",2311.05374v1,https://arxiv.org/pdf/2311.05374v1
"Kantian Deontology Meets AI Alignment: Towards Morally Grounded Fairness
  Metrics","Carlos Mougan, Joshua Brand","Deontological ethics, specifically understood through Immanuel Kant, provides
a moral framework that emphasizes the importance of duties and principles,
rather than the consequences of action. Understanding that despite the
prominence of deontology, it is currently an overlooked approach in fairness
metrics, this paper explores the compatibility of a Kantian deontological
framework in fairness metrics, part of the AI alignment field. We revisit
Kant's critique of utilitarianism, which is the primary approach in AI fairness
metrics and argue that fairness principles should align with the Kantian
deontological framework. By integrating Kantian ethics into AI alignment, we
not only bring in a widely-accepted prominent moral theory but also strive for
a more morally grounded AI landscape that better balances outcomes and
procedures in pursuit of fairness and justice.",2311.05227v2,https://arxiv.org/pdf/2311.05227v2
"FireMatch: A Semi-Supervised Video Fire Detection Network Based on
  Consistency and Distribution Alignment","Qinghua Lin, Zuoyong Li, Kun Zeng, Haoyi Fan, Wei Li, Xiaoguang Zhou","Deep learning techniques have greatly enhanced the performance of fire
detection in videos. However, video-based fire detection models heavily rely on
labeled data, and the process of data labeling is particularly costly and
time-consuming, especially when dealing with videos. Considering the limited
quantity of labeled video data, we propose a semi-supervised fire detection
model called FireMatch, which is based on consistency regularization and
adversarial distribution alignment. Specifically, we first combine consistency
regularization with pseudo-label. For unlabeled data, we design video data
augmentation to obtain corresponding weakly augmented and strongly augmented
samples. The proposed model predicts weakly augmented samples and retains
pseudo-label above a threshold, while training on strongly augmented samples to
predict these pseudo-labels for learning more robust feature representations.
Secondly, we generate video cross-set augmented samples by adversarial
distribution alignment to expand the training data and alleviate the decline in
classification performance caused by insufficient labeled data. Finally, we
introduce a fairness loss to help the model produce diverse predictions for
input samples, thereby addressing the issue of high confidence with the
non-fire class in fire classification scenarios. The FireMatch achieved an
accuracy of 76.92% and 91.81% on two real-world fire datasets, respectively.
The experimental results demonstrate that the proposed method outperforms the
current state-of-the-art semi-supervised classification methods.",2311.05168v1,https://arxiv.org/pdf/2311.05168v1
"Cross-Silo Federated Learning Across Divergent Domains with Iterative
  Parameter Alignment","Matt Gorbett, Hossein Shirazi, Indrakshi Ray","Learning from the collective knowledge of data dispersed across private
sources can provide neural networks with enhanced generalization capabilities.
Federated learning, a method for collaboratively training a machine learning
model across remote clients, achieves this by combining client models via the
orchestration of a central server. However, current approaches face two
critical limitations: i) they struggle to converge when client domains are
sufficiently different, and ii) current aggregation techniques produce an
identical global model for each client. In this work, we address these issues
by reformulating the typical federated learning setup: rather than learning a
single global model, we learn N models each optimized for a common objective.
To achieve this, we apply a weighted distance minimization to model parameters
shared in a peer-to-peer topology. The resulting framework, Iterative Parameter
Alignment, applies naturally to the cross-silo setting, and has the following
properties: (i) a unique solution for each participant, with the option to
globally converge each model in the federation, and (ii) an optional
early-stopping mechanism to elicit fairness among peers in collaborative
learning settings. These characteristics jointly provide a flexible new
framework for iteratively learning from peer models trained on disparate
datasets. We find that the technique achieves competitive results on a variety
of data partitions compared to state-of-the-art approaches. Further, we show
that the method is robust to divergent domains (i.e. disjoint classes across
peers) where existing approaches struggle.",2311.04818v5,https://arxiv.org/pdf/2311.04818v5
MixTEA: Semi-supervised Entity Alignment with Mixture Teaching,"Feng Xie, Xin Song, Xiang Zeng, Xuechen Zhao, Lei Tian, Bin Zhou, Yusong Tan","Semi-supervised entity alignment (EA) is a practical and challenging task
because of the lack of adequate labeled mappings as training data. Most works
address this problem by generating pseudo mappings for unlabeled entities.
However, they either suffer from the erroneous (noisy) pseudo mappings or
largely ignore the uncertainty of pseudo mappings. In this paper, we propose a
novel semi-supervised EA method, termed as MixTEA, which guides the model
learning with an end-to-end mixture teaching of manually labeled mappings and
probabilistic pseudo mappings. We firstly train a student model using few
labeled mappings as standard. More importantly, in pseudo mapping learning, we
propose a bi-directional voting (BDV) strategy that fuses the alignment
decisions in different directions to estimate the uncertainty via the joint
matching confidence score. Meanwhile, we also design a matching diversity-based
rectification (MDR) module to adjust the pseudo mapping learning, thus reducing
the negative influence of noisy mappings. Extensive results on benchmark
datasets as well as further analyses demonstrate the superiority and the
effectiveness of our proposed method.",2311.04441v1,https://arxiv.org/pdf/2311.04441v1
A New Fine-grained Alignment Method for Image-text Matching,Yang Zhang,"Image-text retrieval is a widely studied topic in the field of computer
vision due to the exponential growth of multimedia data, whose core concept is
to measure the similarity between images and text. However, most existing
retrieval methods heavily rely on cross-attention mechanisms for cross-modal
fine-grained alignment, which takes into account excessive irrelevant regions
and treats prominent and non-significant words equally, thereby limiting
retrieval accuracy. This paper aims to investigate an alignment approach that
reduces the involvement of non-significant fragments in images and text while
enhancing the alignment of prominent segments. For this purpose, we introduce
the Cross-Modal Prominent Fragments Enhancement Aligning Network(CPFEAN), which
achieves improved retrieval accuracy by diminishing the participation of
irrelevant regions during alignment and relatively increasing the alignment
similarity of prominent words. Additionally, we incorporate prior textual
information into image regions to reduce misalignment occurrences. In practice,
we first design a novel intra-modal fragments relationship reasoning method,
and subsequently employ our proposed alignment mechanism to compute the
similarity between images and text. Extensive quantitative comparative
experiments on MS-COCO and Flickr30K datasets demonstrate that our approach
outperforms state-of-the-art methods by about 5% to 10% in the rSum metric.",2311.02183v2,https://arxiv.org/pdf/2311.02183v2
The Alignment Problem in Context,Raphaël Millière,"A core challenge in the development of increasingly capable AI systems is to
make them safe and reliable by ensuring their behaviour is consistent with
human values. This challenge, known as the alignment problem, does not merely
apply to hypothetical future AI systems that may pose catastrophic risks; it
already applies to current systems, such as large language models, whose
potential for harm is rapidly increasing. In this paper, I assess whether we
are on track to solve the alignment problem for large language models, and what
that means for the safety of future AI systems. I argue that existing
strategies for alignment are insufficient, because large language models remain
vulnerable to adversarial attacks that can reliably elicit unsafe behaviour. I
offer an explanation of this lingering vulnerability on which it is not simply
a contingent limitation of current language models, but has deep technical ties
to a crucial aspect of what makes these models useful and versatile in the
first place -- namely, their remarkable aptitude to learn ""in context"" directly
from user instructions. It follows that the alignment problem is not only
unsolved for current AI systems, but may be intrinsically difficult to solve
without severely undermining their capabilities. Furthermore, this assessment
raises concerns about the prospect of ensuring the safety of future and more
capable AI systems.",2311.02147v1,https://arxiv.org/pdf/2311.02147v1
"Attention Alignment and Flexible Positional Embeddings Improve
  Transformer Length Extrapolation","Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky","An ideal length-extrapolatable Transformer language model can handle
sequences longer than the training length without any fine-tuning. Such
long-context utilization capability relies heavily on a flexible positional
embedding design. Upon investigating the flexibility of existing large
pre-trained Transformer language models, we find that the T5 family deserves a
closer look, as its positional embeddings capture rich and flexible attention
patterns. However, T5 suffers from the dispersed attention issue: the longer
the input sequence, the flatter the attention distribution. To alleviate the
issue, we propose two attention alignment strategies via temperature scaling.
Our findings show improvement on the long-context utilization capability of T5
on language modeling, retrieval, multi-document question answering, and code
completion tasks without any fine-tuning. This suggests that a flexible
positional embedding design and attention alignment can go a long way toward
Transformer length extrapolation.",2311.00684v2,https://arxiv.org/pdf/2311.00684v2
Latent Space Translation via Semantic Alignment,"Valentino Maiorca, Luca Moschella, Antonio Norelli, Marco Fumero, Francesco Locatello, Emanuele Rodolà","While different neural models often exhibit latent spaces that are alike when
exposed to semantically related data, this intrinsic similarity is not always
immediately discernible. Towards a better understanding of this phenomenon, our
work shows how representations learned from these neural modules can be
translated between different pre-trained networks via simpler transformations
than previously thought. An advantage of this approach is the ability to
estimate these transformations using standard, well-understood algebraic
procedures that have closed-form solutions. Our method directly estimates a
transformation between two given latent spaces, thereby enabling effective
stitching of encoders and decoders without additional training. We extensively
validate the adaptability of this translation procedure in different
experimental settings: across various trainings, domains, architectures (e.g.,
ResNet, CNN, ViT), and in multiple downstream tasks (classification,
reconstruction). Notably, we show how it is possible to zero-shot stitch text
encoders and vision decoders, or vice-versa, yielding surprisingly good
classification performance in this multimodal setting.",2311.00664v2,https://arxiv.org/pdf/2311.00664v2
"The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from
  Human Feedback","Nathan Lambert, Roberto Calandra","Reinforcement learning from human feedback (RLHF) has emerged as a powerful
technique to make large language models (LLMs) more capable in complex
settings. RLHF proceeds as collecting human preference data, training a reward
model on said data, and optimizing a base ML model with respect to said reward
for extrinsic evaluation metrics (e.g. MMLU, GSM8k). RLHF relies on many
assumptions about how the various pieces fit together, such as a reward model
capturing human preferences and an RL optimizer extracting the right signal
from a reward model. As the RLHF process involves many distinct design
decisions, it is easy to assume that multiple processes are correlated and
therefore numerically linked. This apparent correlation is often not true,
where reward models are easily overoptimized or RL optimizers can reduce
performance on tasks not modeled in the data. Notable manifestations of models
trained with imperfect RLHF systems are those that are prone to refusing basic
requests for safety reasons or appearing lazy in generations. As chat model
evaluation becomes increasingly nuanced, the reliance on a perceived link
between reward model training, RL scores, and downstream performance drives
these issues, which we describe as an objective mismatch. In this paper, we
illustrate the causes of this issue, reviewing relevant literature from
model-based reinforcement learning, and argue for solutions. By solving
objective mismatch in RLHF, the ML models of the future will be more precisely
aligned to user instructions for both safety and helpfulness.",2311.00168v2,https://arxiv.org/pdf/2311.00168v2
"Brain-like Flexible Visual Inference by Harnessing Feedback-Feedforward
  Alignment","Tahereh Toosi, Elias B. Issa","In natural vision, feedback connections support versatile visual inference
capabilities such as making sense of the occluded or noisy bottom-up sensory
information or mediating pure top-down processes such as imagination. However,
the mechanisms by which the feedback pathway learns to give rise to these
capabilities flexibly are not clear. We propose that top-down effects emerge
through alignment between feedforward and feedback pathways, each optimizing
its own objectives. To achieve this co-optimization, we introduce
Feedback-Feedforward Alignment (FFA), a learning algorithm that leverages
feedback and feedforward pathways as mutual credit assignment computational
graphs, enabling alignment. In our study, we demonstrate the effectiveness of
FFA in co-optimizing classification and reconstruction tasks on widely used
MNIST and CIFAR10 datasets. Notably, the alignment mechanism in FFA endows
feedback connections with emergent visual inference functions, including
denoising, resolving occlusions, hallucination, and imagination. Moreover, FFA
offers bio-plausibility compared to traditional backpropagation (BP) methods in
implementation. By repurposing the computational graph of credit assignment
into a goal-driven feedback pathway, FFA alleviates weight transport problems
encountered in BP, enhancing the bio-plausibility of the learning algorithm.
Our study presents FFA as a promising proof-of-concept for the mechanisms
underlying how feedback connections in the visual cortex support flexible
visual functions. This work also contributes to the broader field of visual
inference underlying perceptual phenomena and has implications for developing
more biologically inspired learning algorithms.",2310.20599v1,https://arxiv.org/pdf/2310.20599v1
Concept Alignment as a Prerequisite for Value Alignment,"Sunayana Rane, Mark Ho, Ilia Sucholutsky, Thomas L. Griffiths","Value alignment is essential for building AI systems that can safely and
reliably interact with people. However, what a person values -- and is even
capable of valuing -- depends on the concepts that they are currently using to
understand and evaluate what happens in the world. The dependence of values on
concepts means that concept alignment is a prerequisite for value alignment --
agents need to align their representation of a situation with that of humans in
order to successfully align their values. Here, we formally analyze the concept
alignment problem in the inverse reinforcement learning setting, show how
neglecting concept alignment can lead to systematic value mis-alignment, and
describe an approach that helps minimize such failure modes by jointly
reasoning about a person's concepts and values. Additionally, we report
experimental results with human participants showing that humans reason about
the concepts used by an agent when acting intentionally, in line with our joint
reasoning model.",2310.20059v1,https://arxiv.org/pdf/2310.20059v1
"Synthetic Imitation Edit Feedback for Factual Alignment in Clinical
  Summarization","Prakamya Mishra, Zonghai Yao, Shuwei Chen, Beining Wang, Rohan Mittal, Hong Yu","Large Language Models (LLMs) like the GPT and LLaMA families have
demonstrated exceptional capabilities in capturing and condensing critical
contextual information and achieving state-of-the-art performance in the
summarization task. However, community concerns about these models'
hallucination issues continue to rise. LLMs sometimes generate factually
hallucinated summaries, which can be extremely harmful in the clinical domain
NLP tasks (e.g., clinical note summarization), where factually incorrect
statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using
human feedback has shown the promise of aligning LLMs to be factually
consistent during generation, but such training procedure requires high-quality
human-annotated data, which can be extremely expensive to get in the clinical
domain. In this work, we propose a new pipeline using ChatGPT instead of human
experts to generate high-quality feedback data for improving factual
consistency in the clinical note summarization task. We focus specifically on
edit feedback because recent work discusses the shortcomings of human alignment
via preference feedback in complex situations (such as clinical NLP tasks that
require extensive expert knowledge), as well as some advantages of collecting
edit feedback from domain experts. In addition, although GPT has reached the
expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much
previous work discussing whether GPT can generate expert-level edit feedback
for LMs in the clinical note summarization task. We hope to fill this gap.
Finally, our evaluations demonstrate the potential use of GPT edits in human
alignment, especially from a factuality perspective.",2310.20033v2,https://arxiv.org/pdf/2310.20033v2
AI Alignment: A Comprehensive Survey,"Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O'Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, Wen Gao","AI alignment aims to make AI systems behave in line with human intentions and
values. As AI systems grow more capable, so do risks from misalignment. To
provide a comprehensive and up-to-date overview of the alignment field, in this
survey, we delve into the core concepts, methodology, and practice of
alignment. First, we identify four principles as the key objectives of AI
alignment: Robustness, Interpretability, Controllability, and Ethicality
(RICE). Guided by these four principles, we outline the landscape of current
alignment research and decompose them into two key components: forward
alignment and backward alignment. The former aims to make AI systems aligned
via alignment training, while the latter aims to gain evidence about the
systems' alignment and govern them appropriately to avoid exacerbating
misalignment risks. On forward alignment, we discuss techniques for learning
from feedback and learning under distribution shift. On backward alignment, we
discuss assurance techniques and governance practices.
  We also release and continually update the website (www.alignmentsurvey.com)
which features tutorials, collections of papers, blog posts, and other
resources.",2310.19852v5,https://arxiv.org/pdf/2310.19852v5
"MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient
  image-text retrieval","Youbo Lei, Feifei He, Chen Chen, Yingbin Mo, Si Jia Li, Defeng Xie, Haonan Lu","Due to the success of large-scale visual-language pretraining (VLP) models
and the widespread use of image-text retrieval in industry areas, it is now
critically necessary to reduce the model size and streamline their
mobile-device deployment. Single- and dual-stream model structures are commonly
used in image-text retrieval with the goal of closing the semantic gap between
textual and visual modalities. While single-stream models use deep feature
fusion to achieve more accurate cross-model alignment, dual-stream models are
better at offline indexing and fast inference.We propose a Multi-teacher
Cross-modality Alignment Distillation (MCAD) technique to integrate the
advantages of single- and dual-stream models. By incorporating the fused
single-stream features into the image and text features of the dual-stream
model, we formulate new modified teacher similarity distributions and features.
Then, we conduct both distribution and feature distillation to boost the
capability of the student dual-stream model, achieving high retrieval
performance without increasing inference complexity.Extensive experiments
demonstrate the remarkable performance and high efficiency of MCAD on
image-text retrieval tasks. Furthermore, we implement a lightweight CLIP model
on Snapdragon/Dimensity chips with only $\sim$100M running memory and
$\sim$8.0ms search latency, achieving the mobile-device application of VLP
models.",2310.19654v3,https://arxiv.org/pdf/2310.19654v3
"FLIP: Towards Fine-grained Alignment between ID-based Models and
  Pretrained Language Models for CTR Prediction","Hangyu Wang, Jianghao Lin, Xiangyang Li, Bo Chen, Chenxu Zhu, Ruiming Tang, Weinan Zhang, Yong Yu","Click-through rate (CTR) prediction plays as a core function module in
various personalized online services. The traditional ID-based models for CTR
prediction take as inputs the one-hot encoded ID features of tabular modality,
which capture the collaborative signals via feature interaction modeling. But
the one-hot encoding discards the semantic information conceived in the
original feature texts. Recently, the emergence of Pretrained Language Models
(PLMs) has given rise to another paradigm, which takes as inputs the sentences
of textual modality obtained by hard prompt templates and adopts PLMs to
extract the semantic knowledge. However, PLMs generally tokenize the input text
data into subword tokens and ignore field-wise collaborative signals.
Therefore, these two lines of research focus on different characteristics of
the same input data (i.e., textual and tabular modalities), forming a distinct
complementary relationship with each other. In this paper, we propose to
conduct Fine-grained feature-level ALignment between ID-based Models and
Pretrained Language Models (FLIP) for CTR prediction. We design a novel joint
reconstruction pretraining task for both masked language and tabular modeling.
Specifically, the masked data of one modality (i.e., tokens or features) has to
be recovered with the help of the other modality, which establishes the
feature-level interaction and alignment via sufficient mutual information
extraction between dual modalities. Moreover, we propose to jointly finetune
the ID-based model and PLM for downstream CTR prediction tasks, thus achieving
superior performance by combining the advantages of both models. Extensive
experiments on three real-world datasets demonstrate that FLIP outperforms SOTA
baselines, and is highly compatible for various ID-based models and PLMs. The
code is at \url{https://github.com/justarter/FLIP}.",2310.19453v3,https://arxiv.org/pdf/2310.19453v3
Behavior Alignment via Reward Function Optimization,"Dhawal Gupta, Yash Chandak, Scott M. Jordan, Philip S. Thomas, Bruno Castro da Silva","Designing reward functions for efficiently guiding reinforcement learning
(RL) agents toward specific behaviors is a complex task. This is challenging
since it requires the identification of reward structures that are not sparse
and that avoid inadvertently inducing undesirable behaviors. Naively modifying
the reward structure to offer denser and more frequent feedback can lead to
unintended outcomes and promote behaviors that are not aligned with the
designer's intended goal. Although potential-based reward shaping is often
suggested as a remedy, we systematically investigate settings where deploying
it often significantly impairs performance. To address these issues, we
introduce a new framework that uses a bi-level objective to learn
\emph{behavior alignment reward functions}. These functions integrate auxiliary
rewards reflecting a designer's heuristics and domain knowledge with the
environment's primary rewards. Our approach automatically determines the most
effective way to blend these types of feedback, thereby enhancing robustness
against heuristic reward misspecification. Remarkably, it can also adapt an
agent's policy optimization process to mitigate suboptimalities resulting from
limitations and biases inherent in the underlying RL algorithms. We evaluate
our method's efficacy on a diverse set of tasks, from small-scale experiments
to high-dimensional control challenges. We investigate heuristic auxiliary
rewards of varying quality -- some of which are beneficial and others
detrimental to the learning process. Our results show that our framework offers
a robust and principled way to integrate designer-specified heuristics. It not
only addresses key shortcomings of existing approaches but also consistently
leads to high-performing solutions, even when given misaligned or
poorly-specified auxiliary reward functions.",2310.19007v2,https://arxiv.org/pdf/2310.19007v2
"Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive
  Learning","Yifei Zhang, Hao Zhu, Jiahong Liu, Piotr Koniusz, Irwin King","Learning good self-supervised graph representations that are beneficial to
downstream tasks is challenging. Among a variety of methods, contrastive
learning enjoys competitive performance. The embeddings of contrastive learning
are arranged on a hypersphere that enables the Cosine distance measurement in
the Euclidean space. However, the underlying structure of many domains such as
graphs exhibits highly non-Euclidean latent geometry. To this end, we propose a
novel contrastive learning framework to learn high-quality graph embedding.
Specifically, we design the alignment metric that effectively captures the
hierarchical data-invariant information, as well as we propose a substitute of
uniformity metric to prevent the so-called dimensional collapse. We show that
in the hyperbolic space one has to address the leaf- and height-level
uniformity which are related to properties of trees, whereas in the ambient
space of the hyperbolic manifold, these notions translate into imposing an
isotropic ring density towards boundaries of Poincar\'e ball. This ring density
can be easily imposed by promoting the isotropic feature distribution on the
tangent space of manifold. In the experiments, we demonstrate the efficacy of
our proposed method across different hyperbolic graph embedding techniques in
both supervised and self-supervised learning settings.",2310.18209v1,https://arxiv.org/pdf/2310.18209v1
Social Contract AI: Aligning AI Assistants with Implicit Group Norms,"Jan-Philipp Fränken, Sam Kwok, Peixuan Ye, Kanishk Gandhi, Dilip Arumugam, Jared Moore, Alex Tamkin, Tobias Gerstenberg, Noah D. Goodman","We explore the idea of aligning an AI assistant by inverting a model of
users' (unknown) preferences from observed interactions. To validate our
proposal, we run proof-of-concept simulations in the economic ultimatum game,
formalizing user preferences as policies that guide the actions of simulated
players. We find that the AI assistant accurately aligns its behavior to match
standard policies from the economic literature (e.g., selfish, altruistic).
However, the assistant's learned policies lack robustness and exhibit limited
generalization in an out-of-distribution setting when confronted with a
currency (e.g., grams of medicine) that was not included in the assistant's
training distribution. Additionally, we find that when there is inconsistency
in the relationship between language use and an unknown policy (e.g., an
altruistic policy combined with rude language), the assistant's learning of the
policy is slowed. Overall, our preliminary results suggest that developing
simulation frameworks in which AI assistants need to infer preferences from
diverse users can provide a valuable approach for studying practical alignment
questions.",2310.17769v2,https://arxiv.org/pdf/2310.17769v2
SPA: A Graph Spectral Alignment Perspective for Domain Adaptation,"Zhiqing Xiao, Haobo Wang, Ying Jin, Lei Feng, Gang Chen, Fei Huang, Junbo Zhao","Unsupervised domain adaptation (UDA) is a pivotal form in machine learning to
extend the in-domain model to the distinctive target domains where the data
distributions differ. Most prior works focus on capturing the inter-domain
transferability but largely overlook rich intra-domain structures, which
empirically results in even worse discriminability. In this work, we introduce
a novel graph SPectral Alignment (SPA) framework to tackle the tradeoff. The
core of our method is briefly condensed as follows: (i)-by casting the DA
problem to graph primitives, SPA composes a coarse graph alignment mechanism
with a novel spectral regularizer towards aligning the domain graphs in
eigenspaces; (ii)-we further develop a fine-grained message propagation module
-- upon a novel neighbor-aware self-training mechanism -- in order for enhanced
discriminability in the target domain. On standardized benchmarks, the
extensive experiments of SPA demonstrate that its performance has surpassed the
existing cutting-edge DA methods. Coupled with dense model analysis, we
conclude that our approach indeed possesses superior efficacy, robustness,
discriminability, and transferability. Code and data are available at:
https://github.com/CrownX/SPA.",2310.17594v2,https://arxiv.org/pdf/2310.17594v2
Unpacking the Ethical Value Alignment in Big Models,"Xiaoyuan Yi, Jing Yao, Xiting Wang, Xing Xie","Big models have greatly advanced AI's ability to understand, generate, and
manipulate information and content, enabling numerous applications. However, as
these models become increasingly integrated into everyday life, their inherent
ethical values and potential biases pose unforeseen risks to society. This
paper provides an overview of the risks and challenges associated with big
models, surveys existing AI ethics guidelines, and examines the ethical
implications arising from the limitations of these models. Taking a normative
ethics perspective, we propose a reassessment of recent normative guidelines,
highlighting the importance of collaborative efforts in academia to establish a
unified and universal AI ethics framework. Furthermore, we investigate the
moral inclinations of current mainstream LLMs using the Moral Foundation
theory, analyze existing alignment algorithms, and outline the unique
challenges encountered in aligning ethical values within them. To address these
challenges, we introduce a novel conceptual paradigm for aligning the ethical
values of big models and discuss promising research directions for alignment
criteria, evaluation, and method, representing an initial step towards the
interdisciplinary construction of the ethically aligned AI
  This paper is a modified English version of our Chinese paper
https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intended
to help non-Chinese native speakers better understand our work.",2310.17551v1,https://arxiv.org/pdf/2310.17551v1
Privately Aligning Language Models with Reinforcement Learning,"Fan Wu, Huseyin A. Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, Robert Sim","Positioned between pre-training and user deployment, aligning large language
models (LLMs) through reinforcement learning (RL) has emerged as a prevailing
strategy for training instruction following-models such as ChatGPT. In this
work, we initiate the study of privacy-preserving alignment of LLMs through
Differential Privacy (DP) in conjunction with RL. Following the influential
work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment
via RL without human in the loop (e.g., positive review generation) and (ii)
alignment via RL from human feedback (RLHF) (e.g., summarization in a
human-preferred way). We give a new DP framework to achieve alignment via RL,
and prove its correctness. Our experimental results validate the effectiveness
of our approach, offering competitive utility while ensuring strong privacy
protections.",2310.16960v2,https://arxiv.org/pdf/2310.16960v2
Zephyr: Direct Distillation of LM Alignment,"Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, Thomas Wolf","We aim to produce a smaller language model that is aligned to user intent.
Previous research has shown that applying distilled supervised fine-tuning
(dSFT) on larger models significantly improves task accuracy; however, these
models are unaligned, i.e. they do not respond well to natural prompts. To
distill this property, we experiment with the use of preference data from AI
Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model,
we apply distilled direct preference optimization (dDPO) to learn a chat model
with significantly improved intent alignment. The approach requires only a few
hours of training without any additional sampling during fine-tuning. The final
result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B
parameter models, and requires no human annotation. In particular, results on
MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access
RLHF-based model. Code, models, data, and tutorials for the system are
available at https://github.com/huggingface/alignment-handbook.",2310.16944v1,https://arxiv.org/pdf/2310.16944v1
"Prompt Me Up: Unleashing the Power of Alignments for Multimodal Entity
  and Relation Extraction","Xuming Hu, Junzhe Chen, Aiwei Liu, Shiao Meng, Lijie Wen, Philip S. Yu","How can we better extract entities and relations from text? Using multimodal
extraction with images and text obtains more signals for entities and
relations, and aligns them through graphs or hierarchical fusion, aiding in
extraction. Despite attempts at various fusions, previous works have overlooked
many unlabeled image-caption pairs, such as NewsCLIPing. This paper proposes
innovative pre-training objectives for entity-object and relation-image
alignment, extracting objects from images and aligning them with entity and
relation prompts for soft pseudo-labels. These labels are used as
self-supervised signals for pre-training, enhancing the ability to extract
entities and relations. Experiments on three datasets show an average 3.41% F1
improvement over prior SOTA. Additionally, our method is orthogonal to previous
multimodal fusions, and using it on prior SOTA fusions further improves 5.47%
F1.",2310.16822v1,https://arxiv.org/pdf/2310.16822v1
"CycleAlign: Iterative Distillation from Black-box LLM to White-box
  Models for Better Human Alignment","Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, Rui Yan","Language models trained on large-scale corpus often generate content that is
harmful, toxic, or contrary to human preferences, making their alignment with
human values a critical concern. Reinforcement learning from human feedback
(RLHF) with algorithms like PPO is a prevalent approach for alignment but is
often complex, unstable, and resource-intensive. Recently, ranking-based
alignment methods have emerged, offering stability and effectiveness by
replacing the RL framework with supervised fine-tuning, but they are costly due
to the need for annotated data. Considering that existing large language models
(LLMs) like ChatGPT are already relatively well-aligned and cost-friendly,
researchers have begun to align the language model with human preference from
AI feedback. The common practices, which unidirectionally distill the
instruction-following responses from LLMs, are constrained by their bottleneck.
Thus we introduce CycleAlign to distill alignment capabilities from
parameter-invisible LLMs (black-box) to a parameter-visible model (white-box)
in an iterative manner. With in-context learning (ICL) as the core of the
cycle, the black-box models are able to rank the model-generated responses
guided by human-craft instruction and demonstrations about their preferences.
During iterative interaction, the white-box models also have a judgment about
responses generated by them. Consequently, the agreement ranking could be
viewed as a pseudo label to dynamically update the in-context demonstrations
and improve the preference ranking ability of black-box models. Through
multiple interactions, the CycleAlign framework could align the white-box model
with the black-box model effectively in a low-resource way. Empirical results
illustrate that the model fine-tuned by CycleAlign remarkably exceeds existing
methods, and achieves the state-of-the-art performance in alignment with human
value.",2310.16271v1,https://arxiv.org/pdf/2310.16271v1
"AI Alignment and Social Choice: Fundamental Limitations and Policy
  Implications",Abhilash Mishra,"Aligning AI agents to human intentions and values is a key bottleneck in
building safe and deployable AI applications. But whose values should AI agents
be aligned with? Reinforcement learning with human feedback (RLHF) has emerged
as the key framework for AI alignment. RLHF uses feedback from human
reinforcers to fine-tune outputs; all widely deployed large language models
(LLMs) use RLHF to align their outputs to human values. It is critical to
understand the limitations of RLHF and consider policy challenges arising from
these limitations. In this paper, we investigate a specific challenge in
building RLHF systems that respect democratic norms. Building on impossibility
results in social choice theory, we show that, under fairly broad assumptions,
there is no unique voting protocol to universally align AI systems using RLHF
through democratic processes. Further, we show that aligning AI agents with the
values of all individuals will always violate certain private ethical
preferences of an individual user i.e., universal AI alignment using RLHF is
impossible. We discuss policy implications for the governance of AI systems
built using RLHF: first, the need for mandating transparent voting rules to
hold model builders accountable. Second, the need for model builders to focus
on developing AI agents that are narrowly aligned to specific user groups.",2310.16048v1,https://arxiv.org/pdf/2310.16048v1
"Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To
  Word--Definition Alignment","Ahmed ElBakry, Mohamed Gabr, Muhammad ElNokrashy, Badr AlKhamissi","A Reverse Dictionary is a tool enabling users to discover a word based on its
provided definition, meaning, or description. Such a technique proves valuable
in various scenarios, aiding language learners who possess a description of a
word without its identity, and benefiting writers seeking precise terminology.
These scenarios often encapsulate what is referred to as the
""Tip-of-the-Tongue"" (TOT) phenomena. In this work, we present our winning
solution for the Arabic Reverse Dictionary shared task. This task focuses on
deriving a vector representation of an Arabic word from its accompanying
description. The shared task encompasses two distinct subtasks: the first
involves an Arabic definition as input, while the second employs an English
definition. For the first subtask, our approach relies on an ensemble of
finetuned Arabic BERT-based models, predicting the word embedding for a given
definition. The final representation is obtained through averaging the output
embeddings from each model within the ensemble. In contrast, the most effective
solution for the second subtask involves translating the English test
definitions into Arabic and applying them to the finetuned models originally
trained for the first subtask. This straightforward method achieves the highest
score across both subtasks.",2310.15823v3,https://arxiv.org/pdf/2310.15823v3
"The Mason-Alberta Phonetic Segmenter: A forced alignment system based on
  deep neural networks and interpolation","Matthew C. Kelley, Scott James Perry, Benjamin V. Tucker","Forced alignment systems automatically determine boundaries between segments
in speech data, given an orthographic transcription. These tools are
commonplace in phonetics to facilitate the use of speech data that would be
infeasible to manually transcribe and segment. In the present paper, we
describe a new neural network-based forced alignment system, the Mason-Alberta
Phonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two
possible improvements we pursue for forced alignment systems. The first is
treating the acoustic model in a forced aligner as a tagging task, rather than
a classification task, motivated by the common understanding that segments in
speech are not truly discrete and commonly overlap. The second is an
interpolation technique to allow boundaries more precise than the common 10 ms
limit in modern forced alignment systems. We compare configurations of our
system to a state-of-the-art system, the Montreal Forced Aligner. The tagging
approach did not generally yield improved results over the Montreal Forced
Aligner. However, a system with the interpolation technique had a 27.92%
increase relative to the Montreal Forced Aligner in the amount of boundaries
within 10 ms of the target on the test set. We also reflect on the task and
training process for acoustic modeling in forced alignment, highlighting how
the output targets for these models do not match phoneticians' conception of
similarity between phones and that reconciliation of this tension may require
rethinking the task and output targets or how speech itself should be
segmented.",2310.15425v1,https://arxiv.org/pdf/2310.15425v1
"Systematic AI Approach for AGI: Addressing Alignment, Energy, and AGI
  Grand Challenges",Eren Kurshan,"AI faces a trifecta of grand challenges the Energy Wall, the Alignment
Problem and the Leap from Narrow AI to AGI. Contemporary AI solutions consume
unsustainable amounts of energy during model training and daily operations.
Making things worse, the amount of computation required to train each new AI
model has been doubling every 2 months since 2020, directly translating to
increases in energy consumption. The leap from AI to AGI requires multiple
functional subsystems operating in a balanced manner, which requires a system
architecture. However, the current approach to artificial intelligence lacks
system design; even though system characteristics play a key role in the human
brain from the way it processes information to how it makes decisions.
Similarly, current alignment and AI ethics approaches largely ignore system
design, yet studies show that the brains system architecture plays a critical
role in healthy moral decisions. In this paper, we argue that system design is
critically important in overcoming all three grand challenges. We posit that
system design is the missing piece in overcoming the grand challenges. We
present a Systematic AI Approach for AGI that utilizes system design principles
for AGI, while providing ways to overcome the energy wall and the alignment
challenges.",2310.15274v1,https://arxiv.org/pdf/2310.15274v1
"AI Alignment in the Design of Interactive AI: Specification Alignment,
  Process Alignment, and Evaluation Support","Michael Terry, Chinmay Kulkarni, Martin Wattenberg, Lucas Dixon, Meredith Ringel Morris","AI alignment considers the overall problem of ensuring an AI produces desired
outcomes, without undesirable side effects. While often considered from the
perspectives of safety and human values, AI alignment can also be considered in
the context of designing and evaluating interfaces for interactive AI systems.
This paper maps concepts from AI alignment onto a basic, three step interaction
cycle, yielding a corresponding set of alignment objectives: 1) specification
alignment: ensuring the user can efficiently and reliably communicate
objectives to the AI, 2) process alignment: providing the ability to verify and
optionally control the AI's execution process, and 3) evaluation support:
ensuring the user can verify and understand the AI's output. We also introduce
the concepts of a surrogate process, defined as a simplified, separately
derived, but controllable representation of the AI's actual process; and the
notion of a Process Gulf, which highlights how differences between human and AI
processes can lead to challenges in AI control. To illustrate the value of this
framework, we describe commercial and research systems along each of the three
alignment dimensions, and show how interfaces that provide interactive
alignment mechanisms can lead to qualitatively different and improved user
experiences.",2311.00710v1,https://arxiv.org/pdf/2311.00710v1
"She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and
  Sustainable Language Models","Veronica Chatrath, Oluwanifemi Bamgbose, Shaina Raza","As the use of large language models (LLMs) increases within society, as does
the risk of their misuse. Appropriate safeguards must be in place to ensure LLM
outputs uphold the ethical standards of society, highlighting the positive role
that artificial intelligence technologies can have. Recent events indicate
ethical concerns around conventionally trained LLMs, leading to overall unsafe
user experiences. This motivates our research question: how do we ensure LLM
alignment? In this work, we introduce a test suite of unique prompts to foster
the development of aligned LLMs that are fair, safe, and robust. We show that
prompting LLMs at every step of the development pipeline, including data
curation, pre-training, and fine-tuning, will result in an overall more
responsible model. Our test suite evaluates outputs from four state-of-the-art
language models: GPT-3.5, GPT-4, OPT, and LLaMA-2. The assessment presented in
this paper highlights a gap between societal alignment and the capabilities of
current LLMs. Additionally, implementing a test suite such as ours lowers the
environmental overhead of making models safe and fair.",2310.18333v3,https://arxiv.org/pdf/2310.18333v3
Equivariant Deep Weight Space Alignment,"Aviv Navon, Aviv Shamsian, Ethan Fetaya, Gal Chechik, Nadav Dym, Haggai Maron","Permutation symmetries of deep networks make basic operations like model
merging and similarity estimation challenging. In many cases, aligning the
weights of the networks, i.e., finding optimal permutations between their
weights, is necessary. Unfortunately, weight alignment is an NP-hard problem.
Prior research has mainly focused on solving relaxed versions of the alignment
problem, leading to either time-consuming methods or sub-optimal solutions. To
accelerate the alignment process and improve its quality, we propose a novel
framework aimed at learning to solve the weight alignment problem, which we
name Deep-Align. To that end, we first prove that weight alignment adheres to
two fundamental symmetries and then, propose a deep architecture that respects
these symmetries. Notably, our framework does not require any labeled data. We
provide a theoretical analysis of our approach and evaluate Deep-Align on
several types of network architectures and learning setups. Our experimental
results indicate that a feed-forward pass with Deep-Align produces better or
equivalent alignments compared to those produced by current optimization
algorithms. Additionally, our alignments can be used as an effective
initialization for other methods, leading to improved solutions with a
significant speedup in convergence.",2310.13397v3,https://arxiv.org/pdf/2310.13397v3
Few-Shot In-Context Imitation Learning via Implicit Graph Alignment,"Vitalis Vosylius, Edward Johns","Consider the following problem: given a few demonstrations of a task across a
few different objects, how can a robot learn to perform that same task on new,
previously unseen objects? This is challenging because the large variety of
objects within a class makes it difficult to infer the task-relevant
relationship between the new objects and the objects in the demonstrations. We
address this by formulating imitation learning as a conditional alignment
problem between graph representations of objects. Consequently, we show that
this conditioning allows for in-context learning, where a robot can perform a
task on a set of new objects immediately after the demonstrations, without any
prior knowledge about the object class or any further training. In our
experiments, we explore and validate our design choices, and we show that our
method is highly effective for few-shot learning of several real-world,
everyday tasks, whilst outperforming baselines. Videos are available on our
project webpage at https://www.robot-learning.uk/implicit-graph-alignment.",2310.12238v1,https://arxiv.org/pdf/2310.12238v1
Getting aligned on representational alignment,"Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C. Love, Erin Grant, Iris Groen, Jascha Achterberg, Joshua B. Tenenbaum, Katherine M. Collins, Katherine L. Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nori Jacoby, Qiuyi Zhang, Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle, Thomas P. O'Connell, Thomas Unterthiner, Andrew K. Lampinen, Klaus-Robert Müller, Mariya Toneva, Thomas L. Griffiths","Biological and artificial information processing systems form representations
that they can use to categorize, reason, plan, navigate, and make decisions.
How can we measure the extent to which the representations formed by these
diverse systems agree? Do similarities in representations then translate into
similar behavior? How can a system's representations be modified to better
match those of another system? These questions pertaining to the study of
representational alignment are at the heart of some of the most active research
areas in cognitive science, neuroscience, and machine learning. For example,
cognitive scientists measure the representational alignment of multiple
individuals to identify shared cognitive priors, neuroscientists align fMRI
responses from multiple individuals into a shared representational space for
group-level analyses, and ML researchers distill knowledge from teacher models
into student models by increasing their alignment. Unfortunately, there is
limited knowledge transfer between research communities interested in
representational alignment, so progress in one field often ends up being
rediscovered independently in another. Thus, greater cross-field communication
would be advantageous. To improve communication between these fields, we
propose a unifying framework that can serve as a common language between
researchers studying representational alignment. We survey the literature from
all three fields and demonstrate how prior work fits into this framework.
Finally, we lay out open problems in representational alignment where progress
can benefit all three of these fields. We hope that our work can catalyze
cross-disciplinary collaboration and accelerate progress for all communities
studying and developing information processing systems. We note that this is a
working paper and encourage readers to reach out with their suggestions for
future revisions.",2310.13018v2,https://arxiv.org/pdf/2310.13018v2
"Improving Generalization of Alignment with Human Preferences through
  Group Invariant Learning","Rui Zheng, Wei Shen, Yuan Hua, Wenbin Lai, Shihan Dou, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Haoran Huang, Tao Gui, Qi Zhang, Xuanjing Huang","The success of AI assistants based on language models (LLMs) hinges crucially
on Reinforcement Learning from Human Feedback (RLHF), which enables the
generation of responses more aligned with human preferences. As universal AI
assistants, there's a growing expectation for them to perform consistently
across various domains. However, previous work shows that Reinforcement
Learning (RL) often exploits shortcuts to attain high rewards and overlooks
challenging samples. This focus on quick reward gains undermines both the
stability in training and the model's ability to generalize to new, unseen
data. In this work, we propose a novel approach that can learn a consistent
policy via RL across various data groups or domains. Given the challenges
associated with acquiring group annotations, our method automatically
classifies data into different groups, deliberately maximizing performance
variance. Then, we optimize the policy to perform well on challenging groups.
Lastly, leveraging the established groups, our approach adaptively adjusts the
exploration space, allocating more learning capacity to more challenging data
and preventing the model from over-optimizing on simpler data. Experimental
results indicate that our approach significantly enhances training stability
and model generalization.",2310.11971v3,https://arxiv.org/pdf/2310.11971v3
"Investigating Uncertainty Calibration of Aligned Language Models under
  the Multiple-Choice Setting","Guande He, Peng Cui, Jianfei Chen, Wenbo Hu, Jun Zhu","Despite the significant progress made in practical applications of aligned
language models (LMs), they tend to be overconfident in output answers compared
to the corresponding pre-trained LMs. In this work, we systematically evaluate
the impact of the alignment process on logit-based uncertainty calibration of
LMs under the multiple-choice setting. We first conduct a thoughtful empirical
study on how aligned LMs differ in calibration from their pre-trained
counterparts. Experimental results reveal that there are two distinct
uncertainties in LMs under the multiple-choice setting, which are responsible
for the answer decision and the format preference of the LMs, respectively.
Then, we investigate the role of these two uncertainties on aligned LM's
calibration through fine-tuning in simple synthetic alignment schemes and
conclude that one reason for aligned LMs' overconfidence is the conflation of
these two types of uncertainty. Furthermore, we examine the utility of common
post-hoc calibration methods for aligned LMs and propose an easy-to-implement
and sample-efficient method to calibrate aligned LMs. We hope our findings
could provide insights into the design of more reliable alignment processes for
LMs.",2310.11732v2,https://arxiv.org/pdf/2310.11732v2
DIAR: Deep Image Alignment and Reconstruction using Swin Transformers,"Monika Kwiatkowski, Simon Matern, Olaf Hellwich","When taking images of some occluded content, one is often faced with the
problem that every individual image frame contains unwanted artifacts, but a
collection of images contains all relevant information if properly aligned and
aggregated. In this paper, we attempt to build a deep learning pipeline that
simultaneously aligns a sequence of distorted images and reconstructs them. We
create a dataset that contains images with image distortions, such as lighting,
specularities, shadows, and occlusion. We create perspective distortions with
corresponding ground-truth homographies as labels. We use our dataset to train
Swin transformer models to analyze sequential image data. The attention maps
enable the model to detect relevant image content and differentiate it from
outliers and artifacts. We further explore using neural feature maps as
alternatives to classical key point detectors. The feature maps of trained
convolutional layers provide dense image descriptors that can be used to find
point correspondences between images. We utilize this to compute coarse image
alignments and explore its limitations.",2310.11605v1,https://arxiv.org/pdf/2310.11605v1
"Group Preference Optimization: Few-Shot Alignment of Large Language
  Models","Siyan Zhao, John Dang, Aditya Grover","Many applications of large language models (LLMs), ranging from chatbots to
creative writing, require nuanced subjective judgments that can differ
significantly across different groups. Existing alignment algorithms can be
expensive to align for each group, requiring prohibitive amounts of
group-specific preference data and computation for real-world use cases. We
introduce Group Preference Optimization (GPO), an alignment framework that
steers language models to preferences of individual groups in a few-shot
manner. In GPO, we augment the base LLM with an independent transformer module
trained to predict the preferences of a group for the LLM generations. For
few-shot learning, we parameterize this module as an in-context autoregressive
transformer and train it via meta-learning on several groups. We empirically
validate the efficacy of GPO through rigorous evaluations using LLMs with
varied sizes on three human opinion adaptation tasks. These tasks involve
adapting to the preferences of US demographic groups, global countries, and
individual users. Our results demonstrate that GPO not only aligns models more
accurately but also requires fewer group-specific preferences, and less
training and inference computing resources, outperforming existing strategies
such as in-context steering and fine-tuning methods.",2310.11523v1,https://arxiv.org/pdf/2310.11523v1
"GenEval: An Object-Focused Framework for Evaluating Text-to-Image
  Alignment","Dhruba Ghosh, Hanna Hajishirzi, Ludwig Schmidt","Recent breakthroughs in diffusion models, multimodal pretraining, and
efficient finetuning have led to an explosion of text-to-image generative
models. Given human evaluation is expensive and difficult to scale, automated
methods are critical for evaluating the increasingly large number of new
models. However, most current automated evaluation metrics like FID or
CLIPScore only offer a holistic measure of image quality or image-text
alignment, and are unsuited for fine-grained or instance-level analysis. In
this paper, we introduce GenEval, an object-focused framework to evaluate
compositional image properties such as object co-occurrence, position, count,
and color. We show that current object detection models can be leveraged to
evaluate text-to-image models on a variety of generation tasks with strong
human agreement, and that other discriminative vision models can be linked to
this pipeline to further verify properties like object color. We then evaluate
several open-source text-to-image models and analyze their relative generative
capabilities on our benchmark. We find that recent models demonstrate
significant improvement on these tasks, though they are still lacking in
complex capabilities such as spatial relations and attribute binding. Finally,
we demonstrate how GenEval might be used to help discover existing failure
modes, in order to inform development of the next generation of text-to-image
models. Our code to run the GenEval framework is publicly available at
https://github.com/djghosh13/geneval.",2310.11513v1,https://arxiv.org/pdf/2310.11513v1
Compositional preference models for aligning LMs,"Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Marc Dymetman","As language models (LMs) become more capable, it is increasingly important to
align them with human preferences. However, the dominant paradigm for training
Preference Models (PMs) for that purpose suffers from fundamental limitations,
such as lack of transparency and scalability, along with susceptibility to
overfitting the preference dataset. We propose Compositional Preference Models
(CPMs), a novel PM framework that decomposes one global preference assessment
into several interpretable features, obtains scalar scores for these features
from a prompted LM, and aggregates these scores using a logistic regression
classifier. Through these simple steps, CPMs allow to control which properties
of the preference data are used to train the preference model and to build it
based on features that are believed to underlie the human preference judgment.
Our experiments show that CPMs not only improve generalization and are more
robust to overoptimization than standard PMs, but also that best-of-n samples
obtained using CPMs tend to be preferred over samples obtained using
conventional PMs. Overall, our approach demonstrates the benefits of endowing
PMs with priors about which features determine human preferences while relying
on LM capabilities to extract those features in a scalable and robust way.",2310.13011v2,https://arxiv.org/pdf/2310.13011v2
"AST: Effective Dataset Distillation through Alignment with Smooth and
  High-Quality Expert Trajectories","Jiyuan Shen, Wenzhuo Yang, Kwok-Yan Lam","Training large AI models typically requires large-scale datasets in the
machine learning process, making training and parameter-tuning process both
time-consuming and costly. Some researchers address this problem by carefully
synthesizing a very small number of highly representative and informative
samples from real-world datasets. This approach, known as Dataset Distillation
(DD), proposes a perspective for data-efficient learning. Despite recent
progress in this field, the performance of existing methods still cannot meet
expectations, and distilled datasets cannot effectively replace original
datasets. In this paper, unlike previous methods that focus solely on improving
the effectiveness of student distillation, we recognize and leverage the
important mutual influence between expert and student models. We observed that
the smoothness of expert trajectories has a significant impact on subsequent
student parameter alignment. Based on this, we propose an effective DD
framework named AST, standing for Alignment with Smooth and high-quality expert
Trajectories. We devise the integration of clipping loss and gradient penalty
to regulate the rate of parameter changes in expert trajectory generation. To
further refine the student parameter alignment with expert trajectory, we put
forward representative initialization for the synthetic dataset and balanced
inner-loop loss in response to the sensitivity exhibited towards randomly
initialized variables during distillation. We also propose two enhancement
strategies, namely intermediate matching loss and weight perturbation, to
mitigate the potential occurrence of cumulative errors. We conduct extensive
experiments on datasets of different scales, sizes, and resolutions. The
results demonstrate that the proposed method significantly outperforms prior
methods.",2310.10541v2,https://arxiv.org/pdf/2310.10541v2
"ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method
  for Aligning Large Language Models","Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, Zhi-Quan Luo","Reinforcement Learning from Human Feedback (RLHF) is key to aligning Large
Language Models (LLMs), typically paired with the Proximal Policy Optimization
(PPO) algorithm. While PPO is a powerful method designed for general
reinforcement learning tasks, it is overly sophisticated for LLMs, leading to
laborious hyper-parameter tuning and significant computation burdens. To make
RLHF efficient, we present ReMax, which leverages 3 properties of RLHF: fast
simulation, deterministic transitions, and trajectory-level rewards. These
properties are not exploited in PPO, making it less suitable for RLHF. Building
on the renowned REINFORCE algorithm, ReMax does not require training an
additional value model as in PPO and is further enhanced with a new variance
reduction technique. ReMax offers several benefits over PPO: it is simpler to
implement, eliminates more than 4 hyper-parameters in PPO, reduces GPU memory
usage, and shortens training time. ReMax can save about 46% GPU memory than PPO
when training a 7B model and enables training on A800-80GB GPUs without the
memory-saving offloading technique needed by PPO. Applying ReMax to a
Mistral-7B model resulted in a 94.78% win rate on the AlpacaEval leaderboard
and a 7.739 score on MT-bench, setting a new SOTA for open-source 7B models.
These results show the effectiveness of ReMax while addressing the limitations
of PPO in LLMs.",2310.10505v4,https://arxiv.org/pdf/2310.10505v4
"Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake
  Analysis","Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu","The rapid development of large language models (LLMs) has not only provided
numerous opportunities but also presented significant challenges. This becomes
particularly evident when LLMs inadvertently generate harmful or toxic content,
either unintentionally or because of intentional inducement. Existing alignment
methods usually direct LLMs toward the favorable outcomes by utilizing
human-annotated, flawless instruction-response pairs. Conversely, this study
proposes a novel alignment technique based on mistake analysis, which
deliberately exposes LLMs to erroneous content to learn the reasons for
mistakes and how to avoid them. In this case, mistakes are repurposed into
valuable data for alignment, effectively helping to avoid the production of
erroneous responses. Without external models or human annotations, our method
leverages a model's intrinsic ability to discern undesirable mistakes and
improves the safety of its generated responses. Experimental results reveal
that our method outperforms existing alignment approaches in enhancing model
safety while maintaining the overall utility.",2310.10477v6,https://arxiv.org/pdf/2310.10477v6
CAPro: Webly Supervised Learning with Cross-Modality Aligned Prototypes,"Yulei Qin, Xingyu Chen, Yunhang Shen, Chaoyou Fu, Yun Gu, Ke Li, Xing Sun, Rongrong Ji","Webly supervised learning has attracted increasing attention for its
effectiveness in exploring publicly accessible data at scale without manual
annotation. However, most existing methods of learning with web datasets are
faced with challenges from label noise, and they have limited assumptions on
clean samples under various noise. For instance, web images retrieved with
queries of tiger cat (a cat species) and drumstick (a musical instrument) are
almost dominated by images of tigers and chickens, which exacerbates the
challenge of fine-grained visual concept learning. In this case, exploiting
both web images and their associated texts is a requisite solution to combat
real-world noise. In this paper, we propose Cross-modality Aligned Prototypes
(CAPro), a unified prototypical contrastive learning framework to learn visual
representations with correct semantics. For one thing, we leverage textual
prototypes, which stem from the distinct concept definition of classes, to
select clean images by text matching and thus disambiguate the formation of
visual prototypes. For another, to handle missing and mismatched noisy texts,
we resort to the visual feature space to complete and enhance individual texts
and thereafter improve text matching. Such semantically aligned visual
prototypes are further polished up with high-quality samples, and engaged in
both cluster regularization and noise removal. Besides, we propose collective
bootstrapping to encourage smoother and wiser label reference from
appearance-similar instances in a manner of dictionary look-up. Extensive
experiments on WebVision1k and NUS-WIDE (Web) demonstrate that CAPro well
handles realistic noise under both single-label and multi-label scenarios.
CAPro achieves new state-of-the-art performance and exhibits robustness to
open-set recognition. Codes are available at https://github.com/yuleiqin/capro.",2310.09761v1,https://arxiv.org/pdf/2310.09761v1
"Semantics Alignment via Split Learning for Resilient Multi-User Semantic
  Communication","Jinhyuk Choi, Jihong Park, Seung-Woo Ko, Jinho Choi, Mehdi Bennis, Seong-Lyun Kim","Recent studies on semantic communication commonly rely on neural network (NN)
based transceivers such as deep joint source and channel coding (DeepJSCC).
Unlike traditional transceivers, these neural transceivers are trainable using
actual source data and channels, enabling them to extract and communicate
semantics. On the flip side, each neural transceiver is inherently biased
towards specific source data and channels, making different transceivers
difficult to understand intended semantics, particularly upon their initial
encounter. To align semantics over multiple neural transceivers, we propose a
distributed learning based solution, which leverages split learning (SL) and
partial NN fine-tuning techniques. In this method, referred to as SL with layer
freezing (SLF), each encoder downloads a misaligned decoder, and locally
fine-tunes a fraction of these encoder-decoder NN layers. By adjusting this
fraction, SLF controls computing and communication costs. Simulation results
confirm the effectiveness of SLF in aligning semantics under different source
data and channel dissimilarities, in terms of classification accuracy,
reconstruction errors, and recovery time for comprehending intended semantics
from misalignment.",2310.09394v1,https://arxiv.org/pdf/2310.09394v1
"It's an Alignment, Not a Trade-off: Revisiting Bias and Variance in Deep
  Models","Lin Chen, Michal Lukasik, Wittawat Jitkrittum, Chong You, Sanjiv Kumar","Classical wisdom in machine learning holds that the generalization error can
be decomposed into bias and variance, and these two terms exhibit a
\emph{trade-off}. However, in this paper, we show that for an ensemble of deep
learning based classification models, bias and variance are \emph{aligned} at a
sample level, where squared bias is approximately \emph{equal} to variance for
correctly classified sample points. We present empirical evidence confirming
this phenomenon in a variety of deep learning models and datasets. Moreover, we
study this phenomenon from two theoretical perspectives: calibration and neural
collapse. We first show theoretically that under the assumption that the models
are well calibrated, we can observe the bias-variance alignment. Second,
starting from the picture provided by the neural collapse theory, we show an
approximate correlation between bias and variance.",2310.09250v1,https://arxiv.org/pdf/2310.09250v1
"DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided
  Image Editing","Yueming Lyu, Kang Zhao, Bo Peng, Yue Jiang, Yingya Zhang, Jing Dong","Text-guided image editing faces significant challenges to training and
inference flexibility. Much literature collects large amounts of annotated
image-text pairs to train text-conditioned generative models from scratch,
which is expensive and not efficient. After that, some approaches that leverage
pre-trained vision-language models are put forward to avoid data collection,
but they are also limited by either per text-prompt optimization or
inference-time hyper-parameters tuning. To address these issues, we investigate
and identify a specific space, referred to as CLIP DeltaSpace, where the CLIP
visual feature difference of two images is semantically aligned with the CLIP
textual feature difference of their corresponding text descriptions. Based on
DeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP
visual feature differences to the latent space directions of a generative model
during the training phase, and predicts the latent space directions from the
CLIP textual feature differences during the inference phase. And this design
endows DeltaEdit with two advantages: (1) text-free training; (2)
generalization to various text prompts for zero-shot inference. Extensive
experiments validate the effectiveness and versatility of DeltaEdit with
different generative models, including both the GAN model and the diffusion
model, in achieving flexible text-guided image editing. Code is available at
https://github.com/Yueming6568/DeltaEdit.",2310.08785v1,https://arxiv.org/pdf/2310.08785v1
"STELLA: Continual Audio-Video Pre-training with Spatio-Temporal
  Localized Alignment","Jaewoo Lee, Jaehong Yoon, Wonjae Kim, Yunji Kim, Sung Ju Hwang","Continuously learning a variety of audio-video semantics over time is crucial
for audio-related reasoning tasks in our ever-evolving world. However, this is
a nontrivial problem and poses two critical challenges: sparse spatio-temporal
correlation between audio-video pairs and multimodal correlation overwriting
that forgets audio-video relations. To tackle this problem, we propose a new
continual audio-video pre-training method with two novel ideas: (1) Localized
Patch Importance Scoring: we introduce a multimodal encoder to determine the
importance score for each patch, emphasizing semantically intertwined
audio-video patches. (2) Replay-guided Correlation Assessment: to reduce the
corruption of previously learned audiovisual knowledge due to drift, we propose
to assess the correlation of the current patches on the past steps to identify
the patches exhibiting high correlations with the past steps. Based on the
results from the two ideas, we perform probabilistic patch selection for
effective continual audio-video pre-training. Experimental validation on
multiple benchmarks shows that our method achieves a 3.69%p of relative
performance gain in zero-shot retrieval tasks compared to strong continual
learning baselines, while reducing memory consumption by ~45%.",2310.08204v3,https://arxiv.org/pdf/2310.08204v3
"What Matters to You? Towards Visual Representation Alignment for Robot
  Learning","Ran Tian, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, Andrea Bajcsy","When operating in service of people, robots need to optimize rewards aligned
with end-user preferences. Since robots will rely on raw perceptual inputs like
RGB images, their rewards will inevitably use visual representations. Recently
there has been excitement in using representations from pre-trained visual
models, but key to making these work in robotics is fine-tuning, which is
typically done via proxy tasks like dynamics prediction or enforcing temporal
cycle-consistency. However, all these proxy tasks bypass the human's input on
what matters to them, exacerbating spurious correlations and ultimately leading
to robot behaviors that are misaligned with user preferences. In this work, we
propose that robots should leverage human feedback to align their visual
representations with the end-user and disentangle what matters for the task. We
propose Representation-Aligned Preference-based Learning (RAPL), a method for
solving the visual representation alignment problem and visual reward learning
problem through the lens of preference-based learning and optimal transport.
Across experiments in X-MAGICAL and in robotic manipulation, we find that
RAPL's reward consistently generates preferred robot behaviors with high sample
efficiency, and shows strong zero-shot generalization when the visual
representation is learned from a different embodiment than the robot's.",2310.07932v2,https://arxiv.org/pdf/2310.07932v2
"What can knowledge graph alignment gain with Neuro-Symbolic learning
  approaches?","Pedro Giesteira Cotovio, Ernesto Jimenez-Ruiz, Catia Pesquita","Knowledge Graphs (KG) are the backbone of many data-intensive applications
since they can represent data coupled with its meaning and context. Aligning
KGs across different domains and providers is necessary to afford a fuller and
integrated representation. A severe limitation of current KG alignment (KGA)
algorithms is that they fail to articulate logical thinking and reasoning with
lexical, structural, and semantic data learning. Deep learning models are
increasingly popular for KGA inspired by their good performance in other tasks,
but they suffer from limitations in explainability, reasoning, and data
efficiency. Hybrid neurosymbolic learning models hold the promise of
integrating logical and data perspectives to produce high-quality alignments
that are explainable and support validation through human-centric approaches.
This paper examines the current state of the art in KGA and explores the
potential for neurosymbolic integration, highlighting promising research
directions for combining these fields.",2310.07417v1,https://arxiv.org/pdf/2310.07417v1
"Ethical Reasoning over Moral Alignment: A Case and Framework for
  In-Context Ethical Policies in LLMs","Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal, Monojit Choudhury","In this position paper, we argue that instead of morally aligning LLMs to
specific set of ethical principles, we should infuse generic ethical reasoning
capabilities into them so that they can handle value pluralism at a global
scale. When provided with an ethical policy, an LLM should be capable of making
decisions that are ethically consistent to the policy. We develop a framework
that integrates moral dilemmas with moral principles pertaining to different
foramlisms of normative ethics, and at different levels of abstractions.
Initial experiments with GPT-x models shows that while GPT-4 is a nearly
perfect ethical reasoner, the models still have bias towards the moral values
of Western and English speaking societies.",2310.07251v1,https://arxiv.org/pdf/2310.07251v1
"ProFSA: Self-supervised Pocket Pretraining via Protein
  Fragment-Surroundings Alignment","Bowen Gao, Yinjun Jia, Yuanle Mo, Yuyan Ni, Weiying Ma, Zhiming Ma, Yanyan Lan","Pocket representations play a vital role in various biomedical applications,
such as druggability estimation, ligand affinity prediction, and de novo drug
design. While existing geometric features and pretrained representations have
demonstrated promising results, they usually treat pockets independent of
ligands, neglecting the fundamental interactions between them. However, the
limited pocket-ligand complex structures available in the PDB database (less
than 100 thousand non-redundant pairs) hampers large-scale pretraining
endeavors for interaction modeling. To address this constraint, we propose a
novel pocket pretraining approach that leverages knowledge from high-resolution
atomic protein structures, assisted by highly effective pretrained small
molecule representations. By segmenting protein structures into drug-like
fragments and their corresponding pockets, we obtain a reasonable simulation of
ligand-receptor interactions, resulting in the generation of over 5 million
complexes. Subsequently, the pocket encoder is trained in a contrastive manner
to align with the representation of pseudo-ligand furnished by some pretrained
small molecule encoders. Our method, named ProFSA, achieves state-of-the-art
performance across various tasks, including pocket druggability prediction,
pocket matching, and ligand binding affinity prediction. Notably, ProFSA
surpasses other pretraining methods by a substantial margin. Moreover, our work
opens up a new avenue for mitigating the scarcity of protein-ligand complex
data through the utilization of high-quality and diverse protein structure
databases.",2310.07229v2,https://arxiv.org/pdf/2310.07229v2
Flood and Echo Net: Algorithmically Aligned GNNs that Generalize,"Joël Mathys, Florian Grötschla, Kalyan Varma Nadimpalli, Roger Wattenhofer","Most Graph Neural Networks follow the standard message-passing framework
where, in each step, all nodes simultaneously communicate with each other. We
want to challenge this paradigm by aligning the computation more closely to the
execution of distributed algorithms and propose the Flood and Echo Net. A
single round of a Flood and Echo Net consists of an origin node and a flooding
phase followed by an echo phase. First, during the flooding, messages are sent
from the origin and propagated outwards throughout the entire graph. Then,
during the echo, the message flow reverses and messages are sent back towards
the origin. As nodes are only sparsely activated upon receiving a message, this
leads to a wave-like activation pattern that traverses the graph. Through these
sparse but parallel activations, the Net becomes more expressive than
traditional MPNNs which are limited by the 1-WL test and also is provably more
efficient in terms of message complexity. Moreover, the mechanism's inherent
ability to generalize across graphs of varying sizes positions it as a
practical architecture for the task of algorithmic learning. We test the Flood
and Echo Net on a variety of synthetic tasks and the SALSA-CLRS benchmark and
find that the algorithmic alignment of the execution improves generalization to
larger graph sizes.",2310.06970v3,https://arxiv.org/pdf/2310.06970v3
"Temporally Aligning Long Audio Interviews with Questions: A Case Study
  in Multimodal Data Integration","Piyush Singh Pasi, Karthikeya Battepati, Preethi Jyothi, Ganesh Ramakrishnan, Tanmay Mahapatra, Manoj Singh","The problem of audio-to-text alignment has seen significant amount of
research using complete supervision during training. However, this is typically
not in the context of long audio recordings wherein the text being queried does
not appear verbatim within the audio file. This work is a collaboration with a
non-governmental organization called CARE India that collects long audio health
surveys from young mothers residing in rural parts of Bihar, India. Given a
question drawn from a questionnaire that is used to guide these surveys, we aim
to locate where the question is asked within a long audio recording. This is of
great value to African and Asian organizations that would otherwise have to
painstakingly go through long and noisy audio recordings to locate questions
(and answers) of interest. Our proposed framework, INDENT, uses a
cross-attention-based model and prior information on the temporal ordering of
sentences to learn speech embeddings that capture the semantics of the
underlying spoken text. These learnt embeddings are used to retrieve the
corresponding audio segment based on text queries at inference time. We
empirically demonstrate the significant effectiveness (improvement in R-avg of
about 3%) of our model over those obtained using text-based heuristics. We also
show how noisy ASR, generated using state-of-the-art ASR models for Indian
languages, yields better results when used in place of speech. INDENT, trained
only on Hindi data is able to cater to all languages supported by the
(semantically) shared text space. We illustrate this empirically on 11 Indic
languages.",2310.06702v1,https://arxiv.org/pdf/2310.06702v1
Constructive Large Language Models Alignment with Diverse Feedback,"Tianshu Yu, Ting-En Lin, Yuchuan Wu, Min Yang, Fei Huang, Yongbin Li","In recent research on large language models (LLMs), there has been a growing
emphasis on aligning these models with human values to reduce the impact of
harmful content. However, current alignment methods often rely solely on
singular forms of human feedback, such as preferences, annotated labels, or
natural language critiques, overlooking the potential advantages of combining
these feedback types. This limitation leads to suboptimal performance, even
when ample training data is available. In this paper, we introduce Constructive
and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired
by constructivist learning theory. Our approach involves collecting three
distinct types of feedback tailored to problems of varying difficulty levels
within the training dataset. Specifically, we exploit critique feedback for
easy problems, refinement feedback for medium problems, and preference feedback
for hard problems. By training our model with this diversified feedback, we
achieve enhanced alignment performance while using less training data. To
assess the effectiveness of CDF, we evaluate it against previous methods in
three downstream tasks: question answering, dialog generation, and text
summarization. Experimental results demonstrate that CDF achieves superior
performance even with a smaller training dataset.",2310.06450v2,https://arxiv.org/pdf/2310.06450v2
"Jailbreak and Guard Aligned Language Models with Only Few In-Context
  Demonstrations","Zeming Wei, Yifei Wang, Ang Li, Yichuan Mo, Yisen Wang","Large Language Models (LLMs) have shown remarkable success in various tasks,
yet their safety and the risk of generating harmful content remain pressing
concerns. In this paper, we delve into the potential of In-Context Learning
(ICL) to modulate the alignment of LLMs. Specifically, we propose the
In-Context Attack (ICA) which employs harmful demonstrations to subvert LLMs,
and the In-Context Defense (ICD) which bolsters model resilience through
examples that demonstrate refusal to produce harmful responses. We offer
theoretical insights to elucidate how a limited set of in-context
demonstrations can pivotally influence the safety alignment of LLMs. Through
extensive experiments, we demonstrate the efficacy of ICA and ICD in
respectively elevating and mitigating the success rates of jailbreaking
prompts. Our findings illuminate the profound influence of ICL on LLM behavior,
opening new avenues for improving the safety of LLMs.",2310.06387v3,https://arxiv.org/pdf/2310.06387v3
"Geometrically Aligned Transfer Encoder for Inductive Transfer in
  Regression Tasks","Sung Moon Ko, Sumin Lee, Dae-Woong Jeong, Woohyung Lim, Sehui Han","Transfer learning is a crucial technique for handling a small amount of data
that is potentially related to other abundant data. However, most of the
existing methods are focused on classification tasks using images and language
datasets. Therefore, in order to expand the transfer learning scheme to
regression tasks, we propose a novel transfer technique based on differential
geometry, namely the Geometrically Aligned Transfer Encoder (GATE). In this
method, we interpret the latent vectors from the model to exist on a Riemannian
curved manifold. We find a proper diffeomorphism between pairs of tasks to
ensure that every arbitrary point maps to a locally flat coordinate in the
overlapping region, allowing the transfer of knowledge from the source to the
target data. This also serves as an effective regularizer for the model to
behave in extrapolation regions. In this article, we demonstrate that GATE
outperforms conventional methods and exhibits stable behavior in both the
latent space and extrapolation regions for various molecular graph datasets.",2310.06369v1,https://arxiv.org/pdf/2310.06369v1
SALMON: Self-Alignment with Instructable Reward Models,"Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan","Supervised Fine-Tuning (SFT) on response demonstrations combined with
Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful
paradigm for aligning LLM-based AI agents. However, a significant limitation of
such an approach is its dependency on high-quality human annotations, making
its application to intricate tasks challenging due to difficulties in obtaining
consistent response demonstrations and in-distribution response preferences.
This paper presents a novel approach, namely SALMON, to align base language
models with minimal human supervision, using only a small set of human-defined
principles, yet achieving superior performance. Central to our approach is an
instructable reward model. Trained on synthetic preference data, this model can
generate reward scores based on arbitrary human-defined principles. By merely
adjusting these principles during the RL training phase, we gain full control
over the preferences with the instructable reward model, subsequently
influencing the behavior of the RL-trained policy models, and reducing the
reliance on the collection of online human preferences. Applying our method to
the LLaMA-2-70b base language model, we developed an AI assistant named
Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined
principles, Dromedary-2 significantly surpasses the performance of several
state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark
datasets. We have open-sourced the code and model weights to encourage further
research into aligning LLM-based AI agents with enhanced supervision
efficiency, improved controllability, and scalable oversight.",2310.05910v2,https://arxiv.org/pdf/2310.05910v2
"Dynamic value alignment through preference aggregation of multiple
  objectives","Marcin Korecki, Damian Dailisan, Cesare Carissimo","The development of ethical AI systems is currently geared toward setting
objective functions that align with human objectives. However, finding such
functions remains a research challenge, while in RL, setting rewards by hand is
a fairly standard approach. We present a methodology for dynamic value
alignment, where the values that are to be aligned with are dynamically
changing, using a multiple-objective approach. We apply this approach to extend
Deep $Q$-Learning to accommodate multiple objectives and evaluate this method
on a simplified two-leg intersection controlled by a switching agent.Our
approach dynamically accommodates the preferences of drivers on the system and
achieves better overall performance across three metrics (speeds, stops, and
waits) while integrating objectives that have competing or conflicting actions.",2310.05871v1,https://arxiv.org/pdf/2310.05871v1
Generative Judge for Evaluating Alignment,"Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu","The rapid development of Large Language Models (LLMs) has substantially
expanded the range of tasks they can address. In the field of Natural Language
Processing (NLP), researchers have shifted their focus from conventional NLP
tasks (e.g., sequence tagging and parsing) towards tasks that revolve around
aligning with human needs (e.g., brainstorming and email writing). This shift
in task distribution imposes new requirements on evaluating these aligned
models regarding generality (i.e., assessing performance across diverse
scenarios), flexibility (i.e., examining under different protocols), and
interpretability (i.e., scrutinizing models with explanations). In this paper,
we propose a generative judge with 13B parameters, Auto-J, designed to address
these challenges. Our model is trained on user queries and LLM-generated
responses under massive real-world scenarios and accommodates diverse
evaluation protocols (e.g., pairwise response comparison and single-response
evaluation) with well-structured natural language critiques. To demonstrate the
efficacy of our approach, we construct a new testbed covering 58 different
scenarios. Experimentally, Auto-J outperforms a series of strong competitors,
including both open-source and closed-source models, by a large margin. We also
provide detailed analysis and case studies to further reveal the potential of
our method and make a variety of resources public at
https://github.com/GAIR-NLP/auto-j.",2310.05470v2,https://arxiv.org/pdf/2310.05470v2
"Universal Multi-modal Entity Alignment via Iteratively Fusing Modality
  Similarity Paths","Bolin Zhu, Xiaoze Liu, Xin Mao, Zhuo Chen, Lingbing Guo, Tao Gui, Qi Zhang","The objective of Entity Alignment (EA) is to identify equivalent entity pairs
from multiple Knowledge Graphs (KGs) and create a more comprehensive and
unified KG. The majority of EA methods have primarily focused on the structural
modality of KGs, lacking exploration of multi-modal information. A few
multi-modal EA methods have made good attempts in this field. Still, they have
two shortcomings: (1) inconsistent and inefficient modality modeling that
designs complex and distinct models for each modality; (2) ineffective modality
fusion due to the heterogeneous nature of modalities in EA. To tackle these
challenges, we propose PathFusion, consisting of two main components: (1) MSP,
a unified modeling approach that simplifies the alignment process by
constructing paths connecting entities and modality nodes to represent multiple
modalities; (2) IRF, an iterative fusion method that effectively combines
information from different modalities using the path as an information carrier.
Experimental results on real-world datasets demonstrate the superiority of
PathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement
on Hits@1, and 0.194-0.245 absolute improvement on MRR.",2310.05364v3,https://arxiv.org/pdf/2310.05364v3
Hierarchical Multi-Marginal Optimal Transport for Network Alignment,"Zhichen Zeng, Boxin Du, Si Zhang, Yinglong Xia, Zhining Liu, Hanghang Tong","Finding node correspondence across networks, namely multi-network alignment,
is an essential prerequisite for joint learning on multiple networks. Despite
great success in aligning networks in pairs, the literature on multi-network
alignment is sparse due to the exponentially growing solution space and lack of
high-order discrepancy measures. To fill this gap, we propose a hierarchical
multi-marginal optimal transport framework named HOT for multi-network
alignment. To handle the large solution space, multiple networks are decomposed
into smaller aligned clusters via the fused Gromov-Wasserstein (FGW)
barycenter. To depict high-order relationships across multiple networks, the
FGW distance is generalized to the multi-marginal setting, based on which
networks can be aligned jointly. A fast proximal point method is further
developed with guaranteed convergence to a local optimum. Extensive experiments
and analysis show that our proposed HOT achieves significant improvements over
the state-of-the-art in both effectiveness and scalability.",2310.04470v2,https://arxiv.org/pdf/2310.04470v2
Perfect Alignment May be Poisonous to Graph Contrastive Learning,"Jingyu Liu, Huayi Tang, Yong Liu","Graph Contrastive Learning (GCL) aims to learn node representations by
aligning positive pairs and separating negative ones. However, few of
researchers have focused on the inner law behind specific augmentations used in
graph-based learning. What kind of augmentation will help downstream
performance, how does contrastive learning actually influence downstream tasks,
and why the magnitude of augmentation matters so much? This paper seeks to
address these questions by establishing a connection between augmentation and
downstream performance. Our findings reveal that GCL contributes to downstream
tasks mainly by separating different classes rather than gathering nodes of the
same class. So perfect alignment and augmentation overlap which draw all
intra-class samples the same can not fully explain the success of contrastive
learning. Therefore, in order to understand how augmentation aids the
contrastive learning process, we conduct further investigations into the
generalization, finding that perfect alignment that draw positive pair the same
could help contrastive loss but is poisonous to generalization, as a result,
perfect alignment may not lead to best downstream performance, so specifically
designed augmentation is needed to achieve appropriate alignment performance
and improve downstream accuracy. We further analyse the result by information
theory and graph spectrum theory and propose two simple but effective methods
to verify the theories. The two methods could be easily applied to various GCL
algorithms and extensive experiments are conducted to prove its effectiveness.
The code is available at https://github.com/somebodyhh1/GRACEIS",2310.03977v2,https://arxiv.org/pdf/2310.03977v2
Aligning Text-to-Image Diffusion Models with Reward Backpropagation,"Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki","Text-to-image diffusion models have recently emerged at the forefront of
image generation, powered by very large-scale unsupervised or weakly supervised
text-to-image training datasets. Due to their unsupervised training,
controlling their behavior in downstream tasks, such as maximizing
human-perceived image quality, image-text alignment, or ethical image
generation, is difficult. Recent works finetune diffusion models to downstream
reward functions using vanilla reinforcement learning, notorious for the high
variance of the gradient estimators. In this paper, we propose AlignProp, a
method that aligns diffusion models to downstream reward functions using
end-to-end backpropagation of the reward gradient through the denoising
process. While naive implementation of such backpropagation would require
prohibitive memory resources for storing the partial derivatives of modern
text-to-image models, AlignProp finetunes low-rank adapter weight modules and
uses gradient checkpointing, to render its memory usage viable. We test
AlignProp in finetuning diffusion models to various objectives, such as
image-text semantic alignment, aesthetics, compressibility and controllability
of the number of objects present, as well as their combinations. We show
AlignProp achieves higher rewards in fewer training steps than alternatives,
while being conceptually simpler, making it a straightforward choice for
optimizing diffusion models for differentiable reward functions of interest.
Code and Visualization results are available at https://align-prop.github.io/.",2310.03739v2,https://arxiv.org/pdf/2310.03739v2
"Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct
  Preference Optimization","Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, Yu Qiao","A single language model (LM), despite aligning well with an average labeler
through reinforcement learning from human feedback (RLHF), may not universally
suit diverse human preferences. Recent approaches therefore opt for
customization by collecting multi-dimensional feedback and creating distinct
reward models (RMs) for each dimension (e.g., helpfulness, harmlessness, or
honesty). Different LMs can then be optimized for different preferences using
multi-objective RLHF (MORLHF) with different reward weightings. Yet, RL
fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse
and usually conflicting objectives. In this paper, we present Multi-Objective
Direct Preference Optimization (MODPO), an RL-free algorithm that extends
Direct Preference Optimization (DPO) for multiple alignment objectives with
minimal overheads. Essentially, MODPO folds language modeling directly into
reward modeling, training LMs as implicit collective reward models (cRMs) that
combine all objectives with specific weightings. While theoretically guaranteed
to produce the same optimal solutions as MORLHF, MODPO is practically more
stable and computationally efficient. Empirical results from safety alignment
and long-form question answering confirm that MODPO matches or outperforms
existing methods, consistently producing a Pareto front of LMs that cater to
diverse preferences with 3 times less computational resources compared to
MORLHF.",2310.03708v3,https://arxiv.org/pdf/2310.03708v3
"Fine-tuning Aligned Language Models Compromises Safety, Even When Users
  Do Not Intend To!","Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson","Optimizing large language models (LLMs) for downstream use cases often
involves the customization of pre-trained LLMs through further fine-tuning.
Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5
Turbo on custom datasets also encourage this practice. But, what are the safety
costs associated with such custom fine-tuning? We note that while existing
safety alignment infrastructures can restrict harmful behaviors of LLMs at
inference time, they do not cover safety risks when fine-tuning privileges are
extended to end-users. Our red teaming studies find that the safety alignment
of LLMs can be compromised by fine-tuning with only a few adversarially
designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety
guardrails by fine-tuning it on only 10 such examples at a cost of less than
$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful
instructions. Disconcertingly, our research also reveals that, even without
malicious intent, simply fine-tuning with benign and commonly used datasets can
also inadvertently degrade the safety alignment of LLMs, though to a lesser
extent. These findings suggest that fine-tuning aligned LLMs introduces new
safety risks that current safety infrastructures fall short of addressing --
even if a model's initial safety alignment is impeccable, it is not necessarily
to be maintained after custom fine-tuning. We outline and critically analyze
potential mitigations and advocate for further research efforts toward
reinforcing safety protocols for the custom fine-tuning of aligned LLMs.",2310.03693v1,https://arxiv.org/pdf/2310.03693v1
"Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for
  Autonomous LLM-powered Multi-Agent Architectures",Thorsten Händler,"Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.",2310.03659v1,https://arxiv.org/pdf/2310.03659v1
"Enhancing Robust Representation in Adversarial Training: Alignment and
  Exclusion Criteria","Nuoyan Zhou, Nannan Wang, Decheng Liu, Dawei Zhou, Xinbo Gao","Deep neural networks are vulnerable to adversarial noise. Adversarial
Training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two criteria of robust
representation: (1) Exclusion: \emph{the feature of examples keeps away from
that of other classes}; (2) Alignment: \emph{the feature of natural and
corresponding adversarial examples is close to each other}. These motivate us
to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance.",2310.03358v2,https://arxiv.org/pdf/2310.03358v2
Network Alignment with Transferable Graph Autoencoders,"Jiashu He, Charilaos I. Kanatsoulis, Alejandro Ribeiro","Network alignment is the task of establishing one-to-one correspondences
between the nodes of different graphs and finds a plethora of applications in
high-impact domains. However, this task is known to be NP-hard in its general
form, and existing algorithms do not scale up as the size of the graphs
increases. To tackle both challenges we propose a novel generalized graph
autoencoder architecture, designed to extract powerful and robust node
embeddings, that are tailored to the alignment task. We prove that the
generated embeddings are associated with the eigenvalues and eigenvectors of
the graphs and can achieve more accurate alignment compared to classical
spectral methods. Our proposed framework also leverages transfer learning and
data augmentation to achieve efficient network alignment at a very large scale
without retraining. Extensive experiments on both network and sub-network
alignment with real-world graphs provide corroborating evidence supporting the
effectiveness and scalability of the proposed approach.",2310.03272v3,https://arxiv.org/pdf/2310.03272v3
High-dimensional SGD aligns with emerging outlier eigenspaces,"Gerard Ben Arous, Reza Gheissari, Jiaoyang Huang, Aukosh Jagannath","We rigorously study the joint evolution of training dynamics via stochastic
gradient descent (SGD) and the spectra of empirical Hessian and gradient
matrices. We prove that in two canonical classification tasks for multi-class
high-dimensional mixtures and either 1 or 2-layer neural networks, the SGD
trajectory rapidly aligns with emerging low-rank outlier eigenspaces of the
Hessian and gradient matrices. Moreover, in multi-layer settings this alignment
occurs per layer, with the final layer's outlier eigenspace evolving over the
course of training, and exhibiting rank deficiency when the SGD converges to
sub-optimal classifiers. This establishes some of the rich predictions that
have arisen from extensive numerical studies in the last decade about the
spectra of Hessian and information matrices over the course of training in
overparametrized networks.",2310.03010v1,https://arxiv.org/pdf/2310.03010v1
Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models,"Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, Dahua Lin","Warning: This paper contains examples of harmful language, and reader
discretion is recommended. The increasing open release of powerful large
language models (LLMs) has facilitated the development of downstream
applications by reducing the essential cost of data annotation and computation.
To ensure AI safety, extensive safety-alignment measures have been conducted to
armor these models against malicious use (primarily hard prompt attack).
However, beneath the seemingly resilient facade of the armor, there might lurk
a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these
safely aligned LLMs can be easily subverted to generate harmful content.
Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of
data can elicit safely-aligned models to adapt to harmful tasks without
sacrificing model helpfulness. Remarkably, the subverted models retain their
capability to respond appropriately to regular inquiries. Experiments across 8
models released by 5 different organizations (LLaMa-2, Falcon, InternLM,
BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.
Besides, the single-turn English-only attack successfully transfers to
multi-turn dialogue and other languages. This study serves as a clarion call
for a collective effort to overhaul and fortify the safety of open-source LLMs
against malicious attackers.",2310.02949v1,https://arxiv.org/pdf/2310.02949v1
"Reducing Intraspecies and Interspecies Covariate Shift in Traumatic
  Brain Injury EEG of Humans and Mice Using Transfer Euclidean Alignment","Manoj Vishwanath, Steven Cao, Nikil Dutt, Amir M. Rahmani, Miranda M. Lim, Hung Cao","While analytics of sleep electroencephalography (EEG) holds certain
advantages over other methods in clinical applications, high variability across
subjects poses a significant challenge when it comes to deploying machine
learning models for classification tasks in the real world. In such instances,
machine learning models that exhibit exceptional performance on a specific
dataset may not necessarily demonstrate similar proficiency when applied to a
distinct dataset for the same task. The scarcity of high-quality biomedical
data further compounds this challenge, making it difficult to evaluate the
model's generality comprehensively. In this paper, we introduce Transfer
Euclidean Alignment - a transfer learning technique to tackle the problem of
the dearth of human biomedical data for training deep learning models. We
tested the robustness of this transfer learning technique on various rule-based
classical machine learning models as well as the EEGNet-based deep learning
model by evaluating on different datasets, including human and mouse data in a
binary classification task of detecting individuals with versus without
traumatic brain injury (TBI). By demonstrating notable improvements with an
average increase of 14.42% for intraspecies datasets and 5.53% for interspecies
datasets, our findings underscore the importance of the use of transfer
learning to improve the performance of machine learning and deep learning
models when using diverse datasets for training.",2310.02398v1,https://arxiv.org/pdf/2310.02398v1
"AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language
  Models","Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao","The aligned Large Language Models (LLMs) are powerful language understanding
and decision-making tools that are created through extensive alignment with
human feedback. However, these large models remain susceptible to jailbreak
attacks, where adversaries manipulate prompts to elicit malicious outputs that
should not be given by aligned LLMs. Investigating jailbreak prompts can lead
us to delve into the limitations of LLMs and further guide us to secure them.
Unfortunately, existing jailbreak techniques suffer from either (1) scalability
issues, where attacks heavily rely on manual crafting of prompts, or (2)
stealthiness problems, as attacks depend on token-based algorithms to generate
prompts that are often semantically meaningless, making them susceptible to
detection through basic perplexity testing. In light of these challenges, we
intend to answer this question: Can we develop an approach that can
automatically generate stealthy jailbreak prompts? In this paper, we introduce
AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can
automatically generate stealthy jailbreak prompts by the carefully designed
hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN
not only automates the process while preserving semantic meaningfulness, but
also demonstrates superior attack strength in cross-model transferability, and
cross-sample universality compared with the baseline. Moreover, we also compare
AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass
them effectively.",2310.04451v2,https://arxiv.org/pdf/2310.04451v2
"AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable
  Diffusion Model","Zibin Dong, Yifu Yuan, Jianye Hao, Fei Ni, Yao Mu, Yan Zheng, Yujing Hu, Tangjie Lv, Changjie Fan, Zhipeng Hu","Aligning agent behaviors with diverse human preferences remains a challenging
problem in reinforcement learning (RL), owing to the inherent abstractness and
mutability of human preferences. To address these issues, we propose AlignDiff,
a novel framework that leverages RL from Human Feedback (RLHF) to quantify
human preferences, covering abstractness, and utilizes them to guide diffusion
planning for zero-shot behavior customizing, covering mutability. AlignDiff can
accurately match user-customized behaviors and efficiently switch from one to
another. To build the framework, we first establish the multi-perspective human
feedback datasets, which contain comparisons for the attributes of diverse
behaviors, and then train an attribute strength model to predict quantified
relative strengths. After relabeling behavioral datasets with relative
strengths, we proceed to train an attribute-conditioned diffusion model, which
serves as a planner with the attribute strength model as a director for
preference aligning at the inference phase. We evaluate AlignDiff on various
locomotion tasks and demonstrate its superior performance on preference
matching, switching, and covering compared to other baselines. Its capability
of completing unseen downstream tasks under human instructions also showcases
the promising potential for human-AI collaboration. More visualization videos
are released on https://aligndiff.github.io/.",2310.02054v2,https://arxiv.org/pdf/2310.02054v2
"LanguageBind: Extending Video-Language Pretraining to N-modality by
  Language-based Semantic Alignment","Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, Li Yuan","The video-language (VL) pretraining has achieved remarkable improvement in
multiple downstream tasks. However, the current VL pretraining framework is
hard to extend to multiple modalities (N modalities, N>=3) beyond vision and
language. We thus propose LanguageBind, taking the language as the bind across
different modalities because the language modality is well-explored and
contains rich semantics. Specifically, we freeze the language encoder acquired
by VL pretraining, then train encoders for other modalities with contrastive
learning. As a result, all modalities are mapped to a shared feature space,
implementing multi-modal semantic alignment. While LanguageBind ensures that we
can extend VL modalities to N modalities, we also need a high-quality dataset
with alignment data pairs centered on language. We thus propose VIDAL-10M with
Video, Infrared, Depth, Audio and their corresponding Language, naming as
VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with
complete semantics rather than truncated segments from long videos, and all the
video, depth, infrared, and audio modalities are aligned to their textual
descriptions. LanguageBind has achieved superior performance on a wide range of
15 benchmarks covering video, audio, depth, and infrared. Moreover, multiple
experiments have provided evidence for the effectiveness of LanguageBind in
achieving indirect alignment and complementarity among diverse modalities. Code
address: https://github.com/PKU-YuanGroup/LanguageBind",2310.01852v7,https://arxiv.org/pdf/2310.01852v7
Exploring Counterfactual Alignment Loss towards Human-centered AI,"Mingzhou Liu, Xinwei Sun, Ching-Wen Lee, Yu Qiao, Yizhou Wang","Deep neural networks have demonstrated impressive accuracy in supervised
learning tasks. However, their lack of transparency makes it hard for humans to
trust their results, especially in safe-critic domains such as healthcare. To
address this issue, recent explanation-guided learning approaches proposed to
align the gradient-based attention map to image regions annotated by human
experts, thereby obtaining an intrinsically human-centered model. However, the
attention map these methods are based on may fail to causally attribute the
model predictions, thus compromising their validity for alignment. To address
this issue, we propose a novel human-centered framework based on counterfactual
generation. In particular, we utilize the counterfactual generation's ability
for causal attribution to introduce a novel loss called the CounterFactual
Alignment (CF-Align) loss. This loss guarantees that the features attributed by
the counterfactual generation for the classifier align with the human
annotations. To optimize the proposed loss that entails a counterfactual
generation with an implicit function form, we leverage the implicit function
theorem for backpropagation. Our method is architecture-agnostic and, therefore
can be applied to any neural network. We demonstrate the effectiveness of our
method on a lung cancer diagnosis dataset, showcasing faithful alignment to
humans.",2310.01766v1,https://arxiv.org/pdf/2310.01766v1
CAT-LM: Training Language Models on Aligned Code And Tests,"Nikitha Rao, Kush Jain, Uri Alon, Claire Le Goues, Vincent J. Hellendoorn","Testing is an integral part of the software development process. Yet, writing
tests is time-consuming and therefore often neglected. Classical test
generation tools such as EvoSuite generate behavioral test suites by optimizing
for coverage, but tend to produce tests that are hard to understand. Language
models trained on code can generate code that is highly similar to that written
by humans, but current models are trained to generate each file separately, as
is standard practice in natural language processing, and thus fail to consider
the code-under-test context when producing a test file. In this work, we
propose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style
language model with 2.7 Billion parameters, trained on a corpus of Python and
Java projects. We utilize a novel pretraining signal that explicitly considers
the mapping between code and test files when available. We also drastically
increase the maximum sequence length of inputs to 8,192 tokens, 4x more than
typical code generation models, to ensure that the code context is available to
the model when generating test code. We analyze its usefulness for realistic
applications, showing that sampling with filtering (e.g., by compilability,
coverage) allows it to efficiently produce tests that achieve coverage similar
to ones written by developers while resembling their writing style. By
utilizing the code context, CAT-LM generates more valid tests than even much
larger language models trained with more data (CodeGen 16B and StarCoder) and
substantially outperforms a recent test-specific model (TeCo) at test
completion. Overall, our work highlights the importance of incorporating
software-specific insights when training language models for code and paves the
way to more powerful automated test generation.",2310.01602v1,https://arxiv.org/pdf/2310.01602v1
"On the Safety of Open-Sourced Large Language Models: Does Alignment
  Really Prevent Them From Being Misused?","Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, Dinghao Wu","Large Language Models (LLMs) have achieved unprecedented performance in
Natural Language Generation (NLG) tasks. However, many existing studies have
shown that they could be misused to generate undesired content. In response,
before releasing LLMs for public access, model developers usually align those
language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning
with Human Feedback (RLHF). Consequently, those aligned large language models
refuse to generate undesired content when facing potentially harmful/unethical
requests. A natural question is ""could alignment really prevent those
open-sourced large language models from being misused to generate undesired
content?''. In this work, we provide a negative answer to this question. In
particular, we show those open-sourced, aligned large language models could be
easily misguided to generate undesired content without heavy computations or
careful prompt designs. Our key idea is to directly manipulate the generation
process of open-sourced LLMs to misguide it to generate undesired content
including harmful or biased information and even private data. We evaluate our
method on 4 open-sourced LLMs accessible publicly and our finding highlights
the need for more advanced mitigation strategies for open-sourced LLMs.",2310.01581v1,https://arxiv.org/pdf/2310.01581v1
GeRA: Label-Efficient Geometrically Regularized Alignment,"Dustin Klebe, Tal Shnitzer, Mikhail Yurochkin, Leonid Karlinsky, Justin Solomon","Pretrained unimodal encoders incorporate rich semantic information into
embedding space structures. To be similarly informative, multi-modal encoders
typically require massive amounts of paired data for alignment and training. We
introduce a semi-supervised Geometrically Regularized Alignment (GeRA) method
to align the embedding spaces of pretrained unimodal encoders in a
label-efficient way. Our method leverages the manifold geometry of unpaired
(unlabeled) data to improve alignment performance. To prevent distortions to
local geometry during the alignment process, potentially disrupting semantic
neighborhood structures and causing misalignment of unobserved pairs, we
introduce a geometric loss term. This term is built upon a diffusion operator
that captures the local manifold geometry of the unimodal pretrained encoders.
GeRA is modality-agnostic and thus can be used to align pretrained encoders
from any data modalities. We provide empirical evidence to the effectiveness of
our method in the domains of speech-text and image-text alignment. Our
experiments demonstrate significant improvement in alignment quality compared
to a variaty of leading baselines, especially with a small amount of paired
data, using our proposed geometric regularization.",2310.00672v2,https://arxiv.org/pdf/2310.00672v2
"Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for
  LLM Alignment","Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran, Jiantao Jiao","Large Language Models (LLMs) can acquire extensive world knowledge through
pre-training on large corpora. However, due to exposure to low-quality data,
LLMs may exhibit harmful behavior without aligning with human values. The
dominant approach for steering LLMs towards beneficial behavior involves
Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy
Optimization (PPO) serving as the default RL optimizer. Despite its
effectiveness, PPO has limitations when optimizing rewards trained from
comparison-based loss. Primarily, PPO is not invariant to equivalent reward
functions containing identical preference information due to the need to
calibrate the reward scale. Additionally, PPO's necessity for token-wise
updates introduces complexity in both function approximation and algorithm
design compared to trajectory-wise optimization. This paper proposes a new
framework, reinforcement learning with relative feedback, and a novel
trajectory-wise policy gradient algorithm, Pairwise Proximal Policy
Optimization (P3O) that operates directly on comparative rewards. We show
theoretically that P3O is invariant to equivalent rewards and avoids the
complexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO
in the KL-Reward trade-off and can align with human preferences as well as or
better than prior methods. In summary, this work introduces a simpler yet
effective approach for aligning LLMs to human preferences through relative
feedback.",2310.00212v3,https://arxiv.org/pdf/2310.00212v3
"Split and Merge: Aligning Position Biases in Large Language Model based
  Evaluators","Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu","Large language models (LLMs) have shown promise as automated evaluators for
assessing the quality of answers generated by AI systems. However, these
LLM-based evaluators exhibit position bias, or inconsistency, when used to
evaluate candidate answers in pairwise comparisons, favoring either the first
or second answer regardless of content. To address this limitation, we propose
PORTIA, an alignment-based system designed to mimic human comparison strategies
to calibrate position bias in a lightweight yet effective manner. Specifically,
PORTIA splits the answers into multiple segments, aligns similar content across
candidate answers, and then merges them back into a single prompt for
evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to
evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances
the consistency rates for all the models and comparison forms tested, achieving
an average relative improvement of 47.46%. Remarkably, PORTIA enables less
advanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4
model at just 10% of the cost. Furthermore, it rectifies around 80% of the
position bias instances within the GPT-4 model, elevating its consistency rate
up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced
GPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with
human evaluators. These findings highlight PORTIA's ability to correct position
bias, improve LLM consistency, and boost performance while keeping
cost-efficiency. This represents a valuable step toward a more reliable and
scalable use of LLMs for automated evaluations across diverse applications.",2310.01432v2,https://arxiv.org/pdf/2310.01432v2
"Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model
  Adaptation","Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, Yossi Adi","We consider the task of generating diverse and realistic videos guided by
natural audio samples from a wide variety of semantic classes. For this task,
the videos are required to be aligned both globally and temporally with the
input audio: globally, the input audio is semantically associated with the
entire output video, and temporally, each segment of the input audio is
associated with a corresponding segment of that video. We utilize an existing
text-conditioned video generation model and a pre-trained audio encoder model.
The proposed method is based on a lightweight adaptor network, which learns to
map the audio-based representation to the input representation expected by the
text-to-video generation model. As such, it also enables video generation
conditioned on text, audio, and, for the first time as far as we can ascertain,
on both text and audio. We validate our method extensively on three datasets
demonstrating significant semantic diversity of audio-video samples and further
propose a novel evaluation metric (AV-Align) to assess the alignment of
generated videos with input audio samples. AV-Align is based on the detection
and comparison of energy peaks in both modalities. In comparison to recent
state-of-the-art approaches, our method generates videos that are better
aligned with the input sound, both with respect to content and temporal axis.
We also show that videos produced by our method present higher visual quality
and are more diverse.",2309.16429v1,https://arxiv.org/pdf/2309.16429v1
"Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News
  Detection","Jiaying Wu, Shen Li, Ailin Deng, Miao Xiong, Bryan Hooi","Despite considerable advances in automated fake news detection, due to the
timely nature of news, it remains a critical open question how to effectively
predict the veracity of news articles based on limited fact-checks. Existing
approaches typically follow a ""Train-from-Scratch"" paradigm, which is
fundamentally bounded by the availability of large-scale annotated data. While
expressive pre-trained language models (PLMs) have been adapted in a
""Pre-Train-and-Fine-Tune"" manner, the inconsistency between pre-training and
downstream objectives also requires costly task-specific supervision. In this
paper, we propose ""Prompt-and-Align"" (P&A), a novel prompt-based paradigm for
few-shot fake news detection that jointly leverages the pre-trained knowledge
in PLMs and the social context topology. Our approach mitigates label scarcity
by wrapping the news article in a task-related textual prompt, which is then
processed by the PLM to directly elicit task-specific knowledge. To supplement
the PLM with social context without inducing additional training overheads,
motivated by empirical observation on user veracity consistency (i.e., social
users tend to consume news of the same veracity type), we further construct a
news proximity graph among news articles to capture the veracity-consistent
signals in shared readerships, and align the prompting predictions along the
graph edges in a confidence-informed manner. Extensive experiments on three
real-world benchmarks demonstrate that P&A sets new states-of-the-art for
few-shot fake news detection performance by significant margins.",2309.16424v1,https://arxiv.org/pdf/2309.16424v1
"GeoCLIP: Clip-Inspired Alignment between Locations and Images for
  Effective Worldwide Geo-localization","Vicente Vivanco Cepeda, Gaurav Kumar Nayak, Mubarak Shah","Worldwide Geo-localization aims to pinpoint the precise location of images
taken anywhere on Earth. This task has considerable challenges due to immense
variation in geographic landscapes. The image-to-image retrieval-based
approaches fail to solve this problem on a global scale as it is not feasible
to construct a large gallery of images covering the entire world. Instead,
existing approaches divide the globe into discrete geographic cells,
transforming the problem into a classification task. However, their performance
is limited by the predefined classes and often results in inaccurate
localizations when an image's location significantly deviates from its class
center. To overcome these limitations, we propose GeoCLIP, a novel
CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between
the image and its corresponding GPS locations. GeoCLIP's location encoder
models the Earth as a continuous function by employing positional encoding
through random Fourier features and constructing a hierarchical representation
that captures information at varying resolutions to yield a semantically rich
high-dimensional feature suitable to use even beyond geo-localization. To the
best of our knowledge, this is the first work employing GPS encoding for
geo-localization. We demonstrate the efficacy of our method via extensive
experiments and ablations on benchmark datasets. We achieve competitive
performance with just 20% of training data, highlighting its effectiveness even
in limited-data settings. Furthermore, we qualitatively demonstrate
geo-localization using a text query by leveraging CLIP backbone of our image
encoder. The project webpage is available at:
https://vicentevivan.github.io/GeoCLIP",2309.16020v2,https://arxiv.org/pdf/2309.16020v2
"Node-Aligned Graph-to-Graph (NAG2G): Elevating Template-Free Deep
  Learning Approaches in Single-Step Retrosynthesis","Lin Yao, Wentao Guo, Zhen Wang, Shang Xiang, Wentan Liu, Guolin Ke","Single-step retrosynthesis (SSR) in organic chemistry is increasingly
benefiting from deep learning (DL) techniques in computer-aided synthesis
design. While template-free DL models are flexible and promising for
retrosynthesis prediction, they often ignore vital 2D molecular information and
struggle with atom alignment for node generation, resulting in lower
performance compared to the template-based and semi-template-based methods. To
address these issues, we introduce Node-Aligned Graph-to-Graph (NAG2G), a
transformer-based template-free DL model. NAG2G combines 2D molecular graphs
and 3D conformations to retain comprehensive molecular details and incorporates
product-reactant atom mapping through node alignment which determines the order
of the node-by-node graph outputs process in an auto-regressive manner. Through
rigorous benchmarking and detailed case studies, we have demonstrated that
NAG2G stands out with its remarkable predictive accuracy on the expansive
datasets of USPTO-50k and USPTO-FULL. Moreover, the model's practical utility
is underscored by its successful prediction of synthesis pathways for multiple
drug candidate molecules. This not only proves NAG2G's robustness but also its
potential to revolutionize the prediction of complex chemical synthesis
processes for future synthetic route design tasks.",2309.15798v2,https://arxiv.org/pdf/2309.15798v2
Large Language Model Alignment: A Survey,"Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong","Recent years have witnessed remarkable progress made in large language models
(LLMs). Such advancements, while garnering significant attention, have
concurrently elicited various concerns. The potential of these models is
undeniably vast; however, they may yield texts that are imprecise, misleading,
or even detrimental. Consequently, it becomes paramount to employ alignment
techniques to ensure these models to exhibit behaviors consistent with human
values.
  This survey endeavors to furnish an extensive exploration of alignment
methodologies designed for LLMs, in conjunction with the extant capability
research in this domain. Adopting the lens of AI alignment, we categorize the
prevailing methods and emergent proposals for the alignment of LLMs into outer
and inner alignment. We also probe into salient issues including the models'
interpretability, and potential vulnerabilities to adversarial attacks. To
assess LLM alignment, we present a wide variety of benchmarks and evaluation
methodologies. After discussing the state of alignment research for LLMs, we
finally cast a vision toward the future, contemplating the promising avenues of
research that lie ahead.
  Our aspiration for this survey extends beyond merely spurring research
interests in this realm. We also envision bridging the gap between the AI
alignment research community and the researchers engrossed in the capability
exploration of LLMs for both capable and safe LLMs.",2309.15025v1,https://arxiv.org/pdf/2309.15025v1
"Fine-tuning and aligning question answering models for complex
  information extraction tasks","Matthias Engelbach, Dennis Klau, Felix Scheerer, Jens Drawehn, Maximilien Kintz","The emergence of Large Language Models (LLMs) has boosted performance and
possibilities in various NLP tasks. While the usage of generative AI models
like ChatGPT opens up new opportunities for several business use cases, their
current tendency to hallucinate fake content strongly limits their
applicability to document analysis, such as information retrieval from
documents. In contrast, extractive language models like question answering (QA)
or passage retrieval models guarantee query results to be found within the
boundaries of an according context document, which makes them candidates for
more reliable information extraction in productive environments of companies.
In this work we propose an approach that uses and integrates extractive QA
models for improved feature extraction of German business documents such as
insurance reports or medical leaflets into a document analysis solution. We
further show that fine-tuning existing German QA models boosts performance for
tailored extraction tasks of complex linguistic features like damage cause
explanations or descriptions of medication appearance, even with using only a
small set of annotated data. Finally, we discuss the relevance of scoring
metrics for evaluating information extraction tasks and deduce a combined
metric from Levenshtein distance, F1-Score, Exact Match and ROUGE-L to mimic
the assessment criteria from human experts.",2309.14805v1,https://arxiv.org/pdf/2309.14805v1
Cross-Modal Translation and Alignment for Survival Analysis,"Fengtao Zhou, Hao Chen","With the rapid advances in high-throughput sequencing technologies, the focus
of survival analysis has shifted from examining clinical indicators to
incorporating genomic profiles with pathological images. However, existing
methods either directly adopt a straightforward fusion of pathological features
and genomic profiles for survival prediction, or take genomic profiles as
guidance to integrate the features of pathological images. The former would
overlook intrinsic cross-modal correlations. The latter would discard
pathological information irrelevant to gene expression. To address these
issues, we present a Cross-Modal Translation and Alignment (CMTA) framework to
explore the intrinsic cross-modal correlations and transfer potential
complementary information. Specifically, we construct two parallel
encoder-decoder structures for multi-modal data to integrate intra-modal
information and generate cross-modal representation. Taking the generated
cross-modal representation to enhance and recalibrate intra-modal
representation can significantly improve its discrimination for comprehensive
survival analysis. To explore the intrinsic crossmodal correlations, we further
design a cross-modal attention module as the information bridge between
different modalities to perform cross-modal interactions and transfer
complementary information. Our extensive experiments on five public TCGA
datasets demonstrate that our proposed framework outperforms the
state-of-the-art methods.",2309.12855v1,https://arxiv.org/pdf/2309.12855v1
A Spectral Theory of Neural Prediction and Alignment,"Abdulkadir Canatar, Jenelle Feather, Albert Wakhloo, SueYeon Chung","The representations of neural networks are often compared to those of
biological systems by performing regression between the neural network
responses and those measured from biological systems. Many different
state-of-the-art deep neural networks yield similar neural predictions, but it
remains unclear how to differentiate among models that perform equally well at
predicting neural responses. To gain insight into this, we use a recent
theoretical framework that relates the generalization error from regression to
the spectral properties of the model and the target. We apply this theory to
the case of regression between model activations and neural responses and
decompose the neural prediction error in terms of the model eigenspectra,
alignment of model eigenvectors and neural responses, and the training set
size. Using this decomposition, we introduce geometrical measures to interpret
the neural prediction error. We test a large number of deep neural networks
that predict visual cortical activity and show that there are multiple types of
geometries that result in low neural prediction error as measured via
regression. The work demonstrates that carefully decomposing representational
metrics can provide interpretability of how models are capturing neural
activity and points the way towards improved models of neural activity.",2309.12821v2,https://arxiv.org/pdf/2309.12821v2
"Long-Form End-to-End Speech Translation via Latent Alignment
  Segmentation","Peter Polák, Ondřej Bojar","Current simultaneous speech translation models can process audio only up to a
few seconds long. Contemporary datasets provide an oracle segmentation into
sentences based on human-annotated transcripts and translations. However, the
segmentation into sentences is not available in the real world. Current speech
segmentation approaches either offer poor segmentation quality or have to trade
latency for quality. In this paper, we propose a novel segmentation approach
for a low-latency end-to-end speech translation. We leverage the existing
speech translation encoder-decoder architecture with ST CTC and show that it
can perform the segmentation task without supervision or additional parameters.
To the best of our knowledge, our method is the first that allows an actual
end-to-end simultaneous speech translation, as the same model is used for
translation and segmentation at the same time. On a diverse set of language
pairs and in- and out-of-domain data, we show that the proposed approach
achieves state-of-the-art quality at no additional computational cost.",2309.11384v1,https://arxiv.org/pdf/2309.11384v1
Embed-Search-Align: DNA Sequence Alignment using Transformer Models,"Pavan Holur, K. C. Enevoldsen, Shreyas Rajesh, Lajoyce Mboning, Thalia Georgiou, Louis-S. Bouchard, Matteo Pellegrini, Vwani Roychowdhury","DNA sequence alignment involves assigning short DNA reads to the most
probable locations on an extensive reference genome. This process is crucial
for various genomic analyses, including variant calling, transcriptomics, and
epigenomics. Conventional methods, refined over decades, tackle this challenge
in two steps: genome indexing followed by efficient search to locate likely
positions for given reads. Building on the success of Large Language Models
(LLM) in encoding text into embeddings, where the distance metric captures
semantic similarity, recent efforts have explored whether the same Transformer
architecture can produce numerical representations for DNA sequences. Such
models have shown early promise in tasks involving classification of short DNA
sequences, such as the detection of coding vs non-coding regions, as well as
the identification of enhancer and promoter sequences. Performance at sequence
classification tasks does not, however, translate to sequence alignment, where
it is necessary to conduct a genome-wide search to successfully align every
read. We address this open problem by framing it as an Embed-Search-Align task.
In this framework, a novel encoder model DNA-ESA generates representations of
reads and fragments of the reference, which are projected into a shared vector
space where the read-fragment distance is used as surrogate for alignment. In
particular, DNA-ESA introduces: (1) Contrastive loss for self-supervised
training of DNA sequence representations, facilitating rich sequence-level
embeddings, and (2) a DNA vector store to enable search across fragments on a
global scale. DNA-ESA is >97% accurate when aligning 250-length reads onto a
human reference genome of 3 gigabases (single-haploid), far exceeds the
performance of 6 recent DNA-Transformer model baselines and shows task transfer
across chromosomes and species.",2309.11087v4,https://arxiv.org/pdf/2309.11087v4
Sound Source Localization is All about Cross-Modal Alignment,"Arda Senocak, Hyeonggon Ryu, Junsik Kim, Tae-Hyun Oh, Hanspeter Pfister, Joon Son Chung","Humans can easily perceive the direction of sound sources in a visual scene,
termed sound source localization. Recent studies on learning-based sound source
localization have mainly explored the problem from a localization perspective.
However, prior arts and existing benchmarks do not account for a more important
aspect of the problem, cross-modal semantic understanding, which is essential
for genuine sound source localization. Cross-modal semantic understanding is
important in understanding semantically mismatched audio-visual events, e.g.,
silent objects, or off-screen sounds. To account for this, we propose a
cross-modal alignment task as a joint task with sound source localization to
better learn the interaction between audio and visual modalities. Thereby, we
achieve high localization performance with strong cross-modal semantic
understanding. Our method outperforms the state-of-the-art approaches in both
sound source localization and cross-modal retrieval. Our work suggests that
jointly tackling both tasks is necessary to conquer genuine sound source
localization.",2309.10724v1,https://arxiv.org/pdf/2309.10724v1
Unsupervised Deep Cross-Language Entity Alignment,"Chuanyu Jiang, Yiming Qian, Lijun Chen, Yang Gu, Xia Xie","Cross-lingual entity alignment is the task of finding the same semantic
entities from different language knowledge graphs. In this paper, we propose a
simple and novel unsupervised method for cross-language entity alignment. We
utilize the deep learning multi-language encoder combined with a machine
translator to encode knowledge graph text, which reduces the reliance on label
data. Unlike traditional methods that only emphasize global or local alignment,
our method simultaneously considers both alignment strategies. We first view
the alignment task as a bipartite matching problem and then adopt the
re-exchanging idea to accomplish alignment. Compared with the traditional
bipartite matching algorithm that only gives one optimal solution, our
algorithm generates ranked matching results which enabled many potentials
downstream tasks. Additionally, our method can adapt two different types of
optimization (minimal and maximal) in the bipartite matching process, which
provides more flexibility. Our evaluation shows, we each scored 0.966, 0.990,
and 0.996 Hits@1 rates on the DBP15K dataset in Chinese, Japanese, and French
to English alignment tasks. We outperformed the state-of-the-art method in
unsupervised and semi-supervised categories. Compared with the state-of-the-art
supervised method, our method outperforms 2.6% and 0.4% in Ja-En and Fr-En
alignment tasks while marginally lower by 0.2% in the Zh-En alignment task.",2309.10598v1,https://arxiv.org/pdf/2309.10598v1
Unified Coarse-to-Fine Alignment for Video-Text Retrieval,"Ziyang Wang, Yi-Lin Sung, Feng Cheng, Gedas Bertasius, Mohit Bansal","The canonical approach to video-text retrieval leverages a coarse-grained or
fine-grained alignment between visual and textual information. However,
retrieving the correct video according to the text query is often challenging
as it requires the ability to reason about both high-level (scene) and
low-level (object) visual clues and how they relate to the text query. To this
end, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA.
Specifically, our model captures the cross-modal similarity information at
different granularity levels. To alleviate the effect of irrelevant visual
clues, we also apply an Interactive Similarity Aggregation module (ISA) to
consider the importance of different visual features while aggregating the
cross-modal similarity to obtain a similarity score for each granularity.
Finally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of
each level before summing them, alleviating over- and under-representation
issues at different levels. By jointly considering the crossmodal similarity of
different granularity, UCoFiA allows the effective unification of multi-grained
alignments. Empirically, UCoFiA outperforms previous state-of-the-art
CLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%,
1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT,
Activity-Net, and DiDeMo, respectively. Our code is publicly available at
https://github.com/Ziyang412/UCoFiA.",2309.10091v1,https://arxiv.org/pdf/2309.10091v1
"Wait, That Feels Familiar: Learning to Extrapolate Human Preferences for
  Preference Aligned Path Planning","Haresh Karnan, Elvin Yang, Garrett Warnell, Joydeep Biswas, Peter Stone","Autonomous mobility tasks such as lastmile delivery require reasoning about
operator indicated preferences over terrains on which the robot should navigate
to ensure both robot safety and mission success. However, coping with out of
distribution data from novel terrains or appearance changes due to lighting
variations remains a fundamental problem in visual terrain adaptive navigation.
Existing solutions either require labor intensive manual data recollection and
labeling or use handcoded reward functions that may not align with operator
preferences. In this work, we posit that operator preferences for visually
novel terrains, which the robot should adhere to, can often be extrapolated
from established terrain references within the inertial, proprioceptive, and
tactile domain. Leveraging this insight, we introduce Preference extrApolation
for Terrain awarE Robot Navigation, PATERN, a novel framework for extrapolating
operator terrain preferences for visual navigation. PATERN learns to map
inertial, proprioceptive, tactile measurements from the robots observations to
a representation space and performs nearest neighbor search in this space to
estimate operator preferences over novel terrains. Through physical robot
experiments in outdoor environments, we assess PATERNs capability to
extrapolate preferences and generalize to novel terrains and challenging
lighting conditions. Compared to baseline approaches, our findings indicate
that PATERN robustly generalizes to diverse terrains and varied lighting
conditions, while navigating in a preference aligned manner.",2309.09912v1,https://arxiv.org/pdf/2309.09912v1
"Face-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice
  Alignment","Zheng-Yan Sheng, Yang Ai, Yan-Nian Chen, Zhen-Hua Ling","This paper presents a novel task, zero-shot voice conversion based on face
images (zero-shot FaceVC), which aims at converting the voice characteristics
of an utterance from any source speaker to a newly coming target speaker,
solely relying on a single face image of the target speaker. To address this
task, we propose a face-voice memory-based zero-shot FaceVC method. This method
leverages a memory-based face-voice alignment module, in which slots act as the
bridge to align these two modalities, allowing for the capture of voice
characteristics from face images. A mixed supervision strategy is also
introduced to mitigate the long-standing issue of the inconsistency between
training and inference phases for voice conversion tasks. To obtain
speaker-independent content-related representations, we transfer the knowledge
from a pretrained zero-shot voice conversion model to our zero-shot FaceVC
model. Considering the differences between FaceVC and traditional voice
conversion tasks, systematic subjective and objective metrics are designed to
thoroughly evaluate the homogeneity, diversity and consistency of voice
characteristics controlled by face images. Through extensive experiments, we
demonstrate the superiority of our proposed method on the zero-shot FaceVC
task. Samples are presented on our demo website.",2309.09470v1,https://arxiv.org/pdf/2309.09470v1
Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM,"Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen","Recently, Large Language Models (LLMs) have made significant advancements and
are now widely used across various domains. Unfortunately, there has been a
rising concern that LLMs can be misused to generate harmful or malicious
content. Though a line of research has focused on aligning LLMs with human
values and preventing them from producing inappropriate content, such
alignments are usually vulnerable and can be bypassed by alignment-breaking
attacks via adversarially optimized or handcrafted jailbreaking prompts. In
this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against
potential alignment-breaking attacks. RA-LLM can be directly constructed upon
an existing aligned LLM with a robust alignment checking function, without
requiring any expensive retraining or fine-tuning process of the original LLM.
Furthermore, we also provide a theoretical analysis for RA-LLM to verify its
effectiveness in defending against alignment-breaking attacks. Through
real-world experiments on open-source large language models, we demonstrate
that RA-LLM can successfully defend against both state-of-the-art adversarial
prompts and popular handcrafted jailbreaking prompts by reducing their attack
success rates from nearly 100% to around 10% or less.",2309.14348v3,https://arxiv.org/pdf/2309.14348v3
"Rethinking Cross-Domain Pedestrian Detection: A Background-Focused
  Distribution Alignment Framework for Instance-Free One-Stage Detectors","Yancheng Cai, Bo Zhang, Baopu Li, Tao Chen, Hongliang Yan, Jingdong Zhang, Jiahao Xu","Cross-domain pedestrian detection aims to generalize pedestrian detectors
from one label-rich domain to another label-scarce domain, which is crucial for
various real-world applications. Most recent works focus on domain alignment to
train domain-adaptive detectors either at the instance level or image level.
From a practical point of view, one-stage detectors are faster. Therefore, we
concentrate on designing a cross-domain algorithm for rapid one-stage detectors
that lacks instance-level proposals and can only perform image-level feature
alignment. However, pure image-level feature alignment causes the
foreground-background misalignment issue to arise, i.e., the foreground
features in the source domain image are falsely aligned with background
features in the target domain image. To address this issue, we systematically
analyze the importance of foreground and background in image-level cross-domain
alignment, and learn that background plays a more critical role in image-level
cross-domain alignment. Therefore, we focus on cross-domain background feature
alignment while minimizing the influence of foreground features on the
cross-domain alignment stage. This paper proposes a novel framework, namely,
background-focused distribution alignment (BFDA), to train domain adaptive
onestage pedestrian detectors. Specifically, BFDA first decouples the
background features from the whole image feature maps and then aligns them via
a novel long-short-range discriminator.",2309.08771v1,https://arxiv.org/pdf/2309.08771v1
"Adaptive Communications in Collaborative Perception with Domain
  Alignment for Autonomous Driving","Senkang Hu, Zhengru Fang, Haonan An, Guowen Xu, Yuan Zhou, Xianhao Chen, Yuguang Fang","Collaborative perception among multiple connected and autonomous vehicles can
greatly enhance perceptive capabilities by allowing vehicles to exchange
supplementary information via communications. Despite advances in previous
approaches, challenges still remain due to channel variations and data
heterogeneity among collaborative vehicles. To address these issues, we propose
ACC-DA, a channel-aware collaborative perception framework to dynamically
adjust the communication graph and minimize the average transmission delay
while mitigating the side effects from the data heterogeneity. Our novelties
lie in three aspects. We first design a transmission delay minimization method,
which can construct the communication graph and minimize the transmission delay
according to different channel information state. We then propose an adaptive
data reconstruction mechanism, which can dynamically adjust the rate-distortion
trade-off to enhance perception efficiency. Moreover, it minimizes the temporal
redundancy during data transmissions. Finally, we conceive a domain alignment
scheme to align the data distribution from different vehicles, which can
mitigate the domain gap between different vehicles and improve the performance
of the target task. Comprehensive experiments demonstrate the effectiveness of
our method in comparison to the existing state-of-the-art works.",2310.00013v3,https://arxiv.org/pdf/2310.00013v3
Exploring Large Language Models for Ontology Alignment,"Yuan He, Jiaoyan Chen, Hang Dong, Ian Horrocks","This work investigates the applicability of recent generative Large Language
Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for
identifying concept equivalence mappings across ontologies. To test the
zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging
subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking
into account concept labels and structural contexts. Preliminary findings
suggest that LLMs have the potential to outperform existing ontology alignment
systems like BERTMap, given careful framework and prompt design.",2309.07172v1,https://arxiv.org/pdf/2309.07172v1
Mitigating the Alignment Tax of RLHF,"Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, Tong Zhang","LLMs acquire a wide range of abilities during pre-training, but aligning LLMs
under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting,
which is also known as the alignment tax. To empirically verify this
hypothesis, we conducted experiments with existing RLHF algorithms using
OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. On the
other hand, despite various techniques to mitigate forgetting, they are often
at odds with the RLHF performance, leading to a trade-off between reward
maximization and forgetting mitigation.
  In light of the above pressing issue in aligning LLMs, in this paper we
explore model averaging, which interpolates between pre and post RLHF model
weights, to achieve a more efficient reward-tax Pareto front. To understand its
effectiveness, We offer theoretical insights into model averaging, revealing
that it enhances performance Pareto front by increasing feature diversity on
the layers where tasks share overlapped feature spaces. Empirical evidence
corroborates our analysis by showing the benefits of averaging low-level
transformer layers. Building on the analysis and the observation that averaging
different layers of the transformer leads to significantly different reward-tax
trade-offs, we propose Adaptive Model Averaging (AMA) to adaptively find
various combination ratios of model layers. AMA seeks to maximize the alignment
reward while incurring minimal alignment tax. Moreover, we validate AMA's
performance across a range of RLHF algorithms over OpenLLaMA-3B and further
extend our findings to Mistral-7B.",2309.06256v3,https://arxiv.org/pdf/2309.06256v3
"Decolonial AI Alignment: Openness, Viśe\d{s}a-Dharma, and Including
  Excluded Knowledges",Kush R. Varshney,"Prior work has explicated the coloniality of artificial intelligence (AI)
development and deployment through mechanisms such as extractivism, automation,
sociological essentialism, surveillance, and containment. However, that work
has not engaged much with alignment: teaching behaviors to a large language
model (LLM) in line with desired values, and has not considered a mechanism
that arises within that process: moral absolutism -- a part of the coloniality
of knowledge. Colonialism has a history of altering the beliefs and values of
colonized peoples; in this paper, I argue that this history is recapitulated in
current LLM alignment practices and technologies. Furthermore, I suggest that
AI alignment be decolonialized using three forms of openness: openness of
models, openness to society, and openness to excluded knowledges. This
suggested approach to decolonial AI alignment uses ideas from the argumentative
moral philosophical tradition of Hinduism, which has been described as an
open-source religion. One concept used is vi\'{s}e\d{s}a-dharma, or particular
context-specific notions of right and wrong. At the end of the paper, I provide
a suggested reference architecture to work toward the proposed framework.",2309.05030v3,https://arxiv.org/pdf/2309.05030v3
"SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment
  to Cultural Reasoning","Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding, AiTi Aw, Nancy F. Chen","We present SeaEval, a benchmark for multilingual foundation models. In
addition to characterizing how these models understand and reason with natural
language, we also investigate how well they comprehend cultural practices,
nuances, and values. Alongside standard accuracy metrics, we investigate the
brittleness of foundation models in the dimensions of semantics and
multilinguality. Our analyses span both open-sourced and closed models, leading
to empirical results across classic NLP tasks, reasoning, and cultural
comprehension. Key findings indicate (1) Most models exhibit varied behavior
when given paraphrased instructions. (2) Many models still suffer from exposure
bias (e.g., positional bias, majority label bias). (3) For questions rooted in
factual, scientific, and commonsense knowledge, consistent responses are
expected across multilingual queries that are semantically equivalent. Yet,
most models surprisingly demonstrate inconsistent performance on these queries.
(4) Multilingually-trained models have not attained ""balanced multilingual""
capabilities. Our endeavors underscore the need for more generalizable semantic
representations and enhanced multilingual contextualization. SeaEval can serve
as a launchpad for more thorough investigations and evaluations for
multilingual and multicultural scenarios.",2309.04766v5,https://arxiv.org/pdf/2309.04766v5
Multiclass Alignment of Confidence and Certainty for Network Calibration,"Vinith Kugathasan, Muhammad Haris Khan","Deep neural networks (DNNs) have made great strides in pushing the
state-of-the-art in several challenging domains. Recent studies reveal that
they are prone to making overconfident predictions. This greatly reduces the
overall trust in model predictions, especially in safety-critical applications.
Early work in improving model calibration employs post-processing techniques
which rely on limited parameters and require a hold-out set. Some recent
train-time calibration methods, which involve all model parameters, can
outperform the postprocessing methods. To this end, we propose a new train-time
calibration method, which features a simple, plug-and-play auxiliary loss known
as multi-class alignment of predictive mean confidence and predictive certainty
(MACC). It is based on the observation that a model miscalibration is directly
related to its predictive certainty, so a higher gap between the mean
confidence and certainty amounts to a poor calibration both for in-distribution
and out-of-distribution predictions. Armed with this insight, our proposed loss
explicitly encourages a confident (or underconfident) model to also provide a
low (or high) spread in the presoftmax distribution. Extensive experiments on
ten challenging datasets, covering in-domain, out-domain, non-visual
recognition and medical image classification scenarios, show that our method
achieves state-of-the-art calibration performance for both in-domain and
out-domain predictions. Our code and models will be publicly released.",2309.02636v1,https://arxiv.org/pdf/2309.02636v1
Dual Relation Alignment for Composed Image Retrieval,"Xintong Jiang, Yaxiong Wang, Yujiao Wu, Meng Wang, Xueming Qian","Composed image retrieval, a task involving the search for a target image
using a reference image and a complementary text as the query, has witnessed
significant advancements owing to the progress made in cross-modal modeling.
Unlike the general image-text retrieval problem with only one alignment
relation, i.e., image-text, we argue for the existence of two types of
relations in composed image retrieval. The explicit relation pertains to the
reference image & complementary text-target image, which is commonly exploited
by existing methods. Besides this intuitive relation, the observations during
our practice have uncovered another implicit yet crucial relation, i.e.,
reference image & target image-complementary text, since we found that the
complementary text can be inferred by studying the relation between the target
image and the reference image. Regrettably, existing methods largely focus on
leveraging the explicit relation to learn their networks, while overlooking the
implicit relation. In response to this weakness, We propose a new framework for
composed image retrieval, termed dual relation alignment, which integrates both
explicit and implicit relations to fully exploit the correlations among the
triplets. Specifically, we design a vision compositor to fuse reference image
and target image at first, then the resulted representation will serve two
roles: (1) counterpart for semantic alignment with the complementary text and
(2) compensation for the complementary text to boost the explicit relation
modeling, thereby implant the implicit relation into the alignment learning.
Our method is evaluated on two popular datasets, CIRR and FashionIQ, through
extensive experiments. The results confirm the effectiveness of our
dual-relation learning in substantially enhancing composed image retrieval
performance.",2309.02169v3,https://arxiv.org/pdf/2309.02169v3
Making Large Language Models Better Reasoners with Alignment,"Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui","Reasoning is a cognitive process of using evidence to reach a sound
conclusion. The reasoning capability is essential for large language models
(LLMs) to serve as the brain of the artificial general intelligence agent.
Recent studies reveal that fine-tuning LLMs on data with the chain of thought
(COT) reasoning process can significantly enhance their reasoning capabilities.
However, we find that the fine-tuned LLMs suffer from an \textit{Assessment
Misalignment} problem, i.e., they frequently assign higher scores to subpar
COTs, leading to potential limitations in their reasoning abilities. To address
this problem, we introduce an \textit{Alignment Fine-Tuning (AFT)} paradigm,
which involves three steps: 1) fine-tuning LLMs with COT training data; 2)
generating multiple COT responses for each question, and categorizing them into
positive and negative ones based on whether they achieve the correct answer; 3)
calibrating the scores of positive and negative responses given by LLMs with a
novel constraint alignment loss. Specifically, the constraint alignment loss
has two objectives: a) Alignment, which guarantees that positive scores surpass
negative scores to encourage answers with high-quality COTs; b) Constraint,
which keeps the negative scores confined to a reasonable range to prevent the
model degradation. Beyond just the binary positive and negative feedback, the
constraint alignment loss can be seamlessly adapted to the ranking situations
when ranking feedback is accessible. Furthermore, we also delve deeply into
recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and
discover that the constraint, which has been overlooked by these approaches, is
also crucial for their performance. Extensive experiments on four reasoning
benchmarks with both binary and ranking feedback demonstrate the effectiveness
of AFT.",2309.02144v1,https://arxiv.org/pdf/2309.02144v1
"Dual Adversarial Alignment for Realistic Support-Query Shift Few-shot
  Learning","Siyang Jiang, Rui Fang, Hsi-Wen Chen, Wei Ding, Ming-Syan Chen","Support-query shift few-shot learning aims to classify unseen examples (query
set) to labeled data (support set) based on the learned embedding in a
low-dimensional space under a distribution shift between the support set and
the query set. However, in real-world scenarios the shifts are usually unknown
and varied, making it difficult to estimate in advance. Therefore, in this
paper, we propose a novel but more difficult challenge, RSQS, focusing on
Realistic Support-Query Shift few-shot learning. The key feature of RSQS is
that the individual samples in a meta-task are subjected to multiple
distribution shifts in each meta-task. In addition, we propose a unified
adversarial feature alignment method called DUal adversarial ALignment
framework (DuaL) to relieve RSQS from two aspects, i.e., inter-domain bias and
intra-domain variance. On the one hand, for the inter-domain bias, we corrupt
the original data in advance and use the synthesized perturbed inputs to train
the repairer network by minimizing distance in the feature level. On the other
hand, for intra-domain variance, we proposed a generator network to synthesize
hard, i.e., less similar, examples from the support set in a self-supervised
manner and introduce regularized optimal transportation to derive a smooth
optimal transportation plan. Lastly, a benchmark of RSQS is built with several
state-of-the-art baselines among three datasets (CIFAR100, mini-ImageNet, and
Tiered-Imagenet). Experiment results show that DuaL significantly outperforms
the state-of-the-art methods in our benchmark.",2309.02088v1,https://arxiv.org/pdf/2309.02088v1
"Self-driven Grounding: Large Language Model Agents with Automatical
  Language-aligned Skill Learning","Shaohui Peng, Xing Hu, Qi Yi, Rui Zhang, Jiaming Guo, Di Huang, Zikang Tian, Ruizhi Chen, Zidong Du, Qi Guo, Yunji Chen, Ling Li","Large language models (LLMs) show their powerful automatic reasoning and
planning capability with a wealth of semantic knowledge about the human world.
However, the grounding problem still hinders the applications of LLMs in the
real-world environment. Existing studies try to fine-tune the LLM or utilize
pre-defined behavior APIs to bridge the LLMs and the environment, which not
only costs huge human efforts to customize for every single task but also
weakens the generality strengths of LLMs. To autonomously ground the LLM onto
the environment, we proposed the Self-Driven Grounding (SDG) framework to
automatically and progressively ground the LLM with self-driven skill learning.
SDG first employs the LLM to propose the hypothesis of sub-goals to achieve
tasks and then verify the feasibility of the hypothesis via interacting with
the underlying environment. Once verified, SDG can then learn generalized
skills with the guidance of these successfully grounded subgoals. These skills
can be further utilized to accomplish more complex tasks which fail to pass the
verification phase. Verified in the famous instruction following task
set-BabyAI, SDG achieves comparable performance in the most challenging tasks
compared with imitation learning methods that cost millions of demonstrations,
proving the effectiveness of learned skills and showing the feasibility and
efficiency of our framework.",2309.01352v1,https://arxiv.org/pdf/2309.01352v1
"Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D
  Understanding, Generation, and Instruction Following","Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, Pheng-Ann Heng","We introduce Point-Bind, a 3D multi-modality model aligning point clouds with
2D image, language, audio, and video. Guided by ImageBind, we construct a joint
embedding space between 3D and multi-modalities, enabling many promising
applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D
open-world understanding. On top of this, we further present Point-LLM, the
first 3D large language model (LLM) following 3D multi-modal instructions. By
parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of
Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction
data, but exhibits superior 3D and multi-modal question-answering capacity. We
hope our work may cast a light on the community for extending 3D point clouds
to multi-modality applications. Code is available at
https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.",2309.00615v1,https://arxiv.org/pdf/2309.00615v1
"Baseline Defenses for Adversarial Attacks Against Aligned Language
  Models","Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein","As Large Language Models quickly become ubiquitous, it becomes critical to
understand their security vulnerabilities. Recent work shows that text
optimizers can produce jailbreaking prompts that bypass moderation and
alignment. Drawing from the rich body of work on adversarial machine learning,
we approach these attacks with three questions: What threat models are
practically useful in this domain? How do baseline defense techniques perform
in this new domain? How does LLM security differ from computer vision?
  We evaluate several baseline defense strategies against leading adversarial
attacks on LLMs, discussing the various settings in which each is feasible and
effective. Particularly, we look at three types of defenses: detection
(perplexity based), input preprocessing (paraphrase and retokenization), and
adversarial training. We discuss white-box and gray-box settings and discuss
the robustness-performance trade-off for each of the defenses considered. We
find that the weakness of existing discrete optimizers for text, combined with
the relatively high costs of optimization, makes standard adaptive attacks more
challenging for LLMs. Future research will be needed to uncover whether more
powerful optimizers can be developed, or whether the strength of filtering and
preprocessing defenses is greater in the LLMs domain than it has been in
computer vision.",2309.00614v2,https://arxiv.org/pdf/2309.00614v2
"Post-Deployment Adaptation with Access to Source Data via Federated
  Learning and Source-Target Remote Gradient Alignment","Felix Wagner, Zeju Li, Pramit Saha, Konstantinos Kamnitsas","Deployment of Deep Neural Networks in medical imaging is hindered by
distribution shift between training data and data processed after deployment,
causing performance degradation. Post-Deployment Adaptation (PDA) addresses
this by tailoring a pre-trained, deployed model to the target data distribution
using limited labelled or entirely unlabelled target data, while assuming no
access to source training data as they cannot be deployed with the model due to
privacy concerns and their large size. This makes reliable adaptation
challenging due to limited learning signal. This paper challenges this
assumption and introduces FedPDA, a novel adaptation framework that brings the
utility of learning from remote data from Federated Learning into PDA. FedPDA
enables a deployed model to obtain information from source data via remote
gradient exchange, while aiming to optimize the model specifically for the
target domain. Tailored for FedPDA, we introduce a novel optimization method
StarAlign (Source-Target Remote Gradient Alignment) that aligns gradients
between source-target domain pairs by maximizing their inner product, to
facilitate learning a target-specific model. We demonstrate the method's
effectiveness using multi-center databases for the tasks of cancer metastases
detection and skin lesion classification, where our method compares favourably
to previous work. Code is available at: https://github.com/FelixWag/StarAlign",2308.16735v1,https://arxiv.org/pdf/2308.16735v1
Scalable Incomplete Multi-View Clustering with Structure Alignment,"Yi Wen, Siwei Wang, Ke Liang, Weixuan Liang, Xinhang Wan, Xinwang Liu, Suyuan Liu, Jiyuan Liu, En Zhu","The success of existing multi-view clustering (MVC) relies on the assumption
that all views are complete. However, samples are usually partially available
due to data corruption or sensor malfunction, which raises the research of
incomplete multi-view clustering (IMVC). Although several anchor-based IMVC
methods have been proposed to process the large-scale incomplete data, they
still suffer from the following drawbacks: i) Most existing approaches neglect
the inter-view discrepancy and enforce cross-view representation to be
consistent, which would corrupt the representation capability of the model; ii)
Due to the samples disparity between different views, the learned anchor might
be misaligned, which we referred as the Anchor-Unaligned Problem for Incomplete
data (AUP-ID). Such the AUP-ID would cause inaccurate graph fusion and degrades
clustering performance. To tackle these issues, we propose a novel incomplete
anchor graph learning framework termed Scalable Incomplete Multi-View
Clustering with Structure Alignment (SIMVC-SA). Specially, we construct the
view-specific anchor graph to capture the complementary information from
different views. In order to solve the AUP-ID, we propose a novel structure
alignment module to refine the cross-view anchor correspondence. Meanwhile, the
anchor graph construction and alignment are jointly optimized in our unified
framework to enhance clustering quality. Through anchor graph construction
instead of full graphs, the time and space complexity of the proposed SIMVC-SA
is proven to be linearly correlated with the number of samples. Extensive
experiments on seven incomplete benchmark datasets demonstrate the
effectiveness and efficiency of our proposed method. Our code is publicly
available at https://github.com/wy1019/SIMVC-SA.",2308.16541v1,https://arxiv.org/pdf/2308.16541v1
"Peering Through Preferences: Unraveling Feedback Acquisition for
  Aligning Large Language Models","Hritik Bansal, John Dang, Aditya Grover","Aligning large language models (LLMs) with human values and intents
critically involves the use of human or AI feedback. While dense feedback
annotations are expensive to acquire and integrate, sparse feedback presents a
structural design choice between ratings (e.g., score Response A on a scale of
1-7) and rankings (e.g., is Response A better than Response B?). In this work,
we analyze the effect of this design choice for the alignment and evaluation of
LLMs. We uncover an inconsistency problem wherein the preferences inferred from
ratings and rankings significantly disagree 60% for both human and AI
annotators. Our subsequent analysis identifies various facets of annotator
biases that explain this phenomena, such as human annotators would rate denser
responses higher while preferring accuracy during pairwise judgments. To our
surprise, we also observe that the choice of feedback protocol also has a
significant effect on the evaluation of aligned LLMs. In particular, we find
that LLMs that leverage rankings data for alignment (say model X) are preferred
over those that leverage ratings data (say model Y), with a rank-based
evaluation protocol (is X/Y's response better than reference response?) but not
with a rating-based evaluation protocol (score Rank X/Y's response on a scale
of 1-7). Our findings thus shed light on critical gaps in methods for
evaluating the real-world utility of language models and their strong
dependence on the feedback protocol used for alignment. Our code and data are
available at https://github.com/Hritikbansal/sparse_feedback.",2308.15812v3,https://arxiv.org/pdf/2308.15812v3
ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment,"Yicheng Zhong, Huawei Wei, Peiji Yang, Zhisheng Wang","The objective of stylized speech-driven facial animation is to create
animations that encapsulate specific emotional expressions. Existing methods
often depend on pre-established emotional labels or facial expression
templates, which may limit the necessary flexibility for accurately conveying
user intent. In this research, we introduce a technique that enables the
control of arbitrary styles by leveraging natural language as emotion prompts.
This technique presents benefits in terms of both flexibility and
user-friendliness. To realize this objective, we initially construct a
Text-Expression Alignment Dataset (TEAD), wherein each facial expression is
paired with several prompt-like descriptions.We propose an innovative automatic
annotation method, supported by Large Language Models (LLMs), to expedite the
dataset construction, thereby eliminating the substantial expense of manual
annotation. Following this, we utilize TEAD to train a CLIP-based model, termed
ExpCLIP, which encodes text and facial expressions into semantically aligned
style embeddings. The embeddings are subsequently integrated into the facial
animation generator to yield expressive and controllable facial animations.
Given the limited diversity of facial emotions in existing speech-driven facial
animation training data, we further introduce an effective Expression Prompt
Augmentation (EPA) mechanism to enable the animation generator to support
unprecedented richness in style control. Comprehensive experiments illustrate
that our method accomplishes expressive facial animation generation and offers
enhanced flexibility in effectively conveying the desired style.",2308.14448v2,https://arxiv.org/pdf/2308.14448v2
"PECon: Contrastive Pretraining to Enhance Feature Alignment between CT
  and EHR Data for Improved Pulmonary Embolism Diagnosis","Santosh Sanjeev, Salwa K. Al Khatib, Mai A. Shaaban, Ibrahim Almakky, Vijay Ram Papineni, Mohammad Yaqub","Previous deep learning efforts have focused on improving the performance of
Pulmonary Embolism(PE) diagnosis from Computed Tomography (CT) scans using
Convolutional Neural Networks (CNN). However, the features from CT scans alone
are not always sufficient for the diagnosis of PE. CT scans along with
electronic heath records (EHR) can provide a better insight into the patients
condition and can lead to more accurate PE diagnosis. In this paper, we propose
Pulmonary Embolism Detection using Contrastive Learning (PECon), a supervised
contrastive pretraining strategy that employs both the patients CT scans as
well as the EHR data, aiming to enhance the alignment of feature
representations between the two modalities and leverage information to improve
the PE diagnosis. In order to achieve this, we make use of the class labels and
pull the sample features of the same class together, while pushing away those
of the other class. Results show that the proposed work outperforms the
existing techniques and achieves state-of-the-art performance on the RadFusion
dataset with an F1-score of 0.913, accuracy of 0.90 and an AUROC of 0.943.
Furthermore, we also explore the explainability of our approach in comparison
to other methods. Our code is publicly available at
https://github.com/BioMedIA-MBZUAI/PECon.",2308.14050v1,https://arxiv.org/pdf/2308.14050v1
"Towards Fast and Accurate Image-Text Retrieval with Self-Supervised
  Fine-Grained Alignment","Jiamin Zhuang, Jing Yu, Yang Ding, Xiangyan Qu, Yue Hu","Image-text retrieval requires the system to bridge the heterogenous gap
between vision and language for accurate retrieval while keeping the network
lightweight-enough for efficient retrieval. Existing trade-off solutions mainly
study from the view of incorporating cross-modal interactions with the
independent-embedding framework or leveraging stronger pretrained encoders,
which still demand time-consuming similarity measurement or heavyweight model
structure in the retrieval stage. In this work, we propose an image-text
alignment module SelfAlign on top of the independent-embedding framework, which
improves the retrieval accuracy while maintains the retrieval efficiency
without extra supervision. SelfAlign contains two collaborative sub-modules
that force image-text alignment at both concept level and context level by
self-supervised contrastive learning. It does not require cross-modal embedding
interactions during training while maintaining independent image and text
encoders during retrieval. With comparable time cost, SelfAlign consistently
boosts the accuracy of state-of-the-art non-pretraining independent-embedding
models respectively by 9.1%, 4.2% and 6.6% in terms of R@sum score on
Flickr30K, MSCOCO 1K and MS-COCO 5K datasets. The retrieval accuracy also
outperforms most existing interactive-embedding models with orders of magnitude
decrease in retrieval time. The source code is available at:
https://github.com/Zjamie813/SelfAlign.",2308.14009v1,https://arxiv.org/pdf/2308.14009v1
i-Align: an interpretable knowledge graph alignment model,"Bayu Distiawan Trisedya, Flora D Salim, Jeffrey Chan, Damiano Spina, Falk Scholer, Mark Sanderson","Knowledge graphs (KGs) are becoming essential resources for many downstream
applications. However, their incompleteness may limit their potential. Thus,
continuous curation is needed to mitigate this problem. One of the strategies
to address this problem is KG alignment, i.e., forming a more complete KG by
merging two or more KGs. This paper proposes i-Align, an interpretable KG
alignment model. Unlike the existing KG alignment models, i-Align provides an
explanation for each alignment prediction while maintaining high alignment
performance. Experts can use the explanation to check the correctness of the
alignment prediction. Thus, the high quality of a KG can be maintained during
the curation process (e.g., the merging process of two KGs). To this end, a
novel Transformer-based Graph Encoder (Trans-GE) is proposed as a key component
of i-Align for aggregating information from entities' neighbors (structures).
Trans-GE uses Edge-gated Attention that combines the adjacency matrix and the
self-attention matrix to learn a gating mechanism to control the information
aggregation from the neighboring entities. It also uses historical embeddings,
allowing Trans-GE to be trained over mini-batches, or smaller sub-graphs, to
address the scalability issue when encoding a large KG. Another component of
i-Align is a Transformer encoder for aggregating entities' attributes. This
way, i-Align can generate explanations in the form of a set of the most
influential attributes/neighbors based on attention weights. Extensive
experiments are conducted to show the power of i-Align. The experiments include
several aspects, such as the model's effectiveness for aligning KGs, the
quality of the generated explanations, and its practicality for aligning large
KGs. The results show the effectiveness of i-Align in these aspects.",2308.13755v1,https://arxiv.org/pdf/2308.13755v1
"Cultural Alignment in Large Language Models: An Explanatory Analysis
  Based on Hofstede's Cultural Dimensions","Reem I. Masoud, Ziquan Liu, Martin Ferianc, Philip Treleaven, Miguel Rodrigues","The deployment of large language models (LLMs) raises concerns regarding
their cultural misalignment and potential ramifications on individuals and
societies with diverse cultural backgrounds. While the discourse has focused
mainly on political and social biases, our research proposes a Cultural
Alignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's
cultural dimension framework, which offers an explanatory cross-cultural
comparison through the latent variable analysis. We apply our approach to
quantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the
cultural dimensions of regions like the United States, China, and Arab
countries, using different prompting styles and exploring the effects of
language-specific fine-tuning on the models' behavioural tendencies and
cultural values. Our results quantify the cultural alignment of LLMs and reveal
the difference between LLMs in explanatory cultural dimensions. Our study
demonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows
a unique capability to adapt to cultural nuances, particularly in Chinese
settings. However, it faces challenges with American and Arab cultures. The
research also highlights that fine-tuning LLama 2 models with different
languages changes their responses to cultural questions, emphasizing the need
for culturally diverse development in AI for worldwide acceptance and ethical
use. For more details or to contribute to this research, visit our GitHub page
https://github.com/reemim/Hofstedes_CAT/",2309.12342v2,https://arxiv.org/pdf/2309.12342v2
"Chunk, Align, Select: A Simple Long-sequence Processing Method for
  Transformers","Jiawen Xie, Pengyu Cheng, Xiao Liang, Yong Dai, Nan Du","Although dominant in natural language processing, transformer-based models
remain challenged by the task of long-sequence processing, because the
computational cost of self-attention operations in transformers swells
quadratically with the input sequence length. To alleviate the complexity of
long-sequence processing, we propose a simple framework to enable the
offthe-shelf pre-trained transformers to process much longer sequences, while
the computation and memory costs remain growing linearly with the input
sequence lengths. More specifically, our method divides each long-sequence
input into a batch of chunks, then aligns the interchunk information during the
encoding steps, and finally selects the most representative hidden states from
the encoder for the decoding process. To extract inter-chunk semantic
information, we align the start and end token embeddings among chunks in each
encoding transformer block. To learn an effective hidden selection policy, we
design a dual updating scheme inspired by reinforcement learning, which regards
the decoders of transformers as environments, and the downstream performance
metrics as the rewards to evaluate the hidden selection actions. Our empirical
results on real-world long-text summarization and reading comprehension tasks
demonstrate effective improvements compared to prior longsequence processing
baselines.",2308.13191v2,https://arxiv.org/pdf/2308.13191v2
"Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language
  Pretraining?","Fei Wang, Liang Ding, Jun Rao, Ye Liu, Li Shen, Changxing Ding","The multimedia community has shown a significant interest in perceiving and
representing the physical world with multimodal pretrained neural network
models, and among them, the visual-language pertaining (VLP) is, currently, the
most captivating topic. However, there have been few endeavors dedicated to the
exploration of 1) whether essential linguistic knowledge (e.g., semantics and
syntax) can be extracted during VLP, and 2) how such linguistic knowledge
impact or enhance the multimodal alignment. In response, here we aim to
elucidate the impact of comprehensive linguistic knowledge, including semantic
expression and syntactic structure, on multimodal alignment. Specifically, we
design and release the SNARE, the first large-scale multimodal alignment
probing benchmark, to detect the vital linguistic components, e.g., lexical,
semantic, and syntax knowledge, containing four tasks: Semantic structure,
Negation logic, Attribute ownership, and Relationship composition. Based on our
proposed probing benchmarks, our holistic analyses of five advanced VLP models
illustrate that the VLP model: i) shows insensitivity towards complex syntax
structures and relies on content words for sentence comprehension; ii)
demonstrates limited comprehension of combinations between sentences and
negations; iii) faces challenges in determining the presence of actions or
spatial relationships within visual information and struggles with verifying
the correctness of triple combinations. We make our benchmark and code
available at \url{https://github.com/WangFei-2019/SNARE/}.",2308.12898v2,https://arxiv.org/pdf/2308.12898v2
"Match-And-Deform: Time Series Domain Adaptation through Optimal
  Transport and Temporal Alignment","François Painblanc, Laetitia Chapel, Nicolas Courty, Chloé Friguet, Charlotte Pelletier, Romain Tavenard","While large volumes of unlabeled data are usually available, associated
labels are often scarce. The unsupervised domain adaptation problem aims at
exploiting labels from a source domain to classify data from a related, yet
different, target domain. When time series are at stake, new difficulties arise
as temporal shifts may appear in addition to the standard feature distribution
shift. In this paper, we introduce the Match-And-Deform (MAD) approach that
aims at finding correspondences between the source and target time series while
allowing temporal distortions. The associated optimization problem
simultaneously aligns the series thanks to an optimal transport loss and the
time stamps through dynamic time warping. When embedded into a deep neural
network, MAD helps learning new representations of time series that both align
the domains and maximize the discriminative power of the network. Empirical
studies on benchmark datasets and remote sensing data demonstrate that MAD
makes meaningful sample-to-sample pairing and time shift estimation, reaching
similar or better classification performance than state-of-the-art deep time
series domain adaptation strategies.",2308.12686v2,https://arxiv.org/pdf/2308.12686v2
Aligning Language Models with Offline Learning from Human Feedback,"Jian Hu, Li Tao, June Yang, Chandler Zhou","Learning from human preferences is crucial for language models (LMs) to
effectively cater to human needs and societal values. Previous research has
made notable progress by leveraging human feedback to follow instructions.
However, these approaches rely primarily on online learning techniques like
Proximal Policy Optimization (PPO), which have been proven unstable and
challenging to tune for language models. Moreover, PPO requires complex
distributed system implementation, hindering the efficiency of large-scale
distributed training. In this study, we propose an offline learning from human
feedback framework to align LMs without interacting with environments.
Specifically, we explore filtering alignment (FA), reward-weighted regression
(RWR), and conditional alignment (CA) to align language models to human
preferences. By employing a loss function similar to supervised fine-tuning,
our methods ensure more stable model training than PPO with a simple machine
learning system~(MLSys) and much fewer (around 9\%) computing resources.
Experimental results demonstrate that conditional alignment outperforms other
offline alignment methods and is comparable to PPO.",2308.12050v2,https://arxiv.org/pdf/2308.12050v2
"From Instructions to Intrinsic Human Values -- A Survey of Alignment
  Goals for Big Models","Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, Xing Xie","Big models, exemplified by Large Language Models (LLMs), are models typically
pre-trained on massive data and comprised of enormous parameters, which not
only obtain significantly improved performance across diverse tasks but also
present emergent capabilities absent in smaller models. However, the growing
intertwining of big models with everyday human lives poses potential risks and
might cause serious social harm. Therefore, many efforts have been made to
align LLMs with humans to make them better follow user instructions and satisfy
human preferences. Nevertheless, `what to align with' has not been fully
discussed, and inappropriate alignment goals might even backfire. In this
paper, we conduct a comprehensive survey of different alignment goals in
existing work and trace their evolution paths to help identify the most
essential goal. Particularly, we investigate related works from two
perspectives: the definition of alignment goals and alignment evaluation. Our
analysis encompasses three distinct levels of alignment goals and reveals a
goal transformation from fundamental abilities to value orientation, indicating
the potential of intrinsic human values as the alignment goal for enhanced
LLMs. Based on such results, we further discuss the challenges of achieving
such intrinsic value alignment and provide a collection of available resources
for future research on the alignment of big models.",2308.12014v2,https://arxiv.org/pdf/2308.12014v2
Understanding Hessian Alignment for Domain Generalization,"Sobhan Hemati, Guojun Zhang, Amir Estiri, Xi Chen","Out-of-distribution (OOD) generalization is a critical ability for deep
learning models in many real-world scenarios including healthcare and
autonomous vehicles. Recently, different techniques have been proposed to
improve OOD generalization. Among these methods, gradient-based regularizers
have shown promising performance compared with other competitors. Despite this
success, our understanding of the role of Hessian and gradient alignment in
domain generalization is still limited. To address this shortcoming, we analyze
the role of the classifier's head Hessian matrix and gradient in domain
generalization using recent OOD theory of transferability. Theoretically, we
show that spectral norm between the classifier's head Hessian matrices across
domains is an upper bound of the transfer measure, a notion of distance between
target and source domains. Furthermore, we analyze all the attributes that get
aligned when we encourage similarity between Hessians and gradients. Our
analysis explains the success of many regularizers like CORAL, IRM, V-REx,
Fish, IGA, and Fishr as they regularize part of the classifier's head Hessian
and/or gradient. Finally, we propose two simple yet effective methods to match
the classifier's head Hessians and gradients in an efficient way, based on the
Hessian Gradient Product (HGP) and Hutchinson's method (Hutchinson), and
without directly calculating Hessians. We validate the OOD generalization
ability of proposed methods in different scenarios, including transferability,
severe correlation shift, label shift and diversity shift. Our results show
that Hessian alignment methods achieve promising performance on various OOD
benchmarks. The code is available at
\url{https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment}.",2308.11778v1,https://arxiv.org/pdf/2308.11778v1
"Mode Combinability: Exploring Convex Combinations of Permutation Aligned
  Models","Adrián Csiszárik, Melinda F. Kiss, Péter Kőrösi-Szabó, Márton Muntag, Gergely Papp, Dániel Varga","We explore element-wise convex combinations of two permutation-aligned neural
network parameter vectors $\Theta_A$ and $\Theta_B$ of size $d$. We conduct
extensive experiments by examining various distributions of such model
combinations parametrized by elements of the hypercube $[0,1]^{d}$ and its
vicinity. Our findings reveal that broad regions of the hypercube form surfaces
of low loss values, indicating that the notion of linear mode connectivity
extends to a more general phenomenon which we call mode combinability. We also
make several novel observations regarding linear mode connectivity and model
re-basin. We demonstrate a transitivity property: two models re-based to a
common third model are also linear mode connected, and a robustness property:
even with significant perturbations of the neuron matchings the resulting
combinations continue to form a working model. Moreover, we analyze the
functional and weight similarity of model combinations and show that such
combinations are non-vacuous in the sense that there are significant functional
differences between the resulting models.",2308.11511v1,https://arxiv.org/pdf/2308.11511v1
"Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment
  Classification","Xueyi Liu, Rui Hou, Yanglei Gan, Da Luo, Changlin Li, Xiaojun Shi, Qiao Liu","Aspect-based sentiment classification is a crucial problem in fine-grained
sentiment analysis, which aims to predict the sentiment polarity of the given
aspect according to its context. Previous works have made remarkable progress
in leveraging attention mechanism to extract opinion words for different
aspects. However, a persistent challenge is the effective management of
semantic mismatches, which stem from attention mechanisms that fall short in
adequately aligning opinions words with their corresponding aspect in
multi-aspect sentences. To address this issue, we propose a novel
Aspect-oriented Opinion Alignment Network (AOAN) to capture the contextual
association between opinion words and the corresponding aspect. Specifically,
we first introduce a neighboring span enhanced module which highlights various
compositions of neighboring words and given aspects. In addition, we design a
multi-perspective attention mechanism that align relevant opinion information
with respect to the given aspect. Extensive experiments on three benchmark
datasets demonstrate that our model achieves state-of-the-art results. The
source code is available at https://github.com/AONE-NLP/ABSA-AOAN.",2308.11447v1,https://arxiv.org/pdf/2308.11447v1
Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation,"Dan Zhang, Kaspar Sakmann, William Beluch, Robin Hutmacher, Yumeng Li","Within the context of autonomous driving, encountering unknown objects
becomes inevitable during deployment in the open world. Therefore, it is
crucial to equip standard semantic segmentation models with anomaly awareness.
Many previous approaches have utilized synthetic out-of-distribution (OoD) data
augmentation to tackle this problem. In this work, we advance the OoD synthesis
process by reducing the domain gap between the OoD data and driving scenes,
effectively mitigating the style difference that might otherwise act as an
obvious shortcut during training. Additionally, we propose a simple fine-tuning
loss that effectively induces a pre-trained semantic segmentation model to
generate a ``none of the given classes"" prediction, leveraging per-pixel OoD
scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline
enables the use of pre-trained models for anomaly segmentation while
maintaining the performance on the original task.",2308.09965v1,https://arxiv.org/pdf/2308.09965v1
Graph-based Alignment and Uniformity for Recommendation,"Liangwei Yang, Zhiwei Liu, Chen Wang, Mingdai Yang, Xiaolong Liu, Jing Ma, Philip S. Yu","Collaborative filtering-based recommender systems (RecSys) rely on learning
representations for users and items to predict preferences accurately.
Representation learning on the hypersphere is a promising approach due to its
desirable properties, such as alignment and uniformity. However, the sparsity
issue arises when it encounters RecSys. To address this issue, we propose a
novel approach, graph-based alignment and uniformity (GraphAU), that explicitly
considers high-order connectivities in the user-item bipartite graph. GraphAU
aligns the user/item embedding to the dense vector representations of
high-order neighbors using a neighborhood aggregator, eliminating the need to
compute the burdensome alignment to high-order neighborhoods individually. To
address the discrepancy in alignment losses, GraphAU includes a layer-wise
alignment pooling module to integrate alignment losses layer-wise. Experiments
on four datasets show that GraphAU significantly alleviates the sparsity issue
and achieves state-of-the-art performance. We open-source GraphAU at
https://github.com/YangLiangwei/GraphAU.",2308.09292v1,https://arxiv.org/pdf/2308.09292v1
"Probabilistic Results on the Architecture of Mathematical Reasoning
  Aligned by Cognitive Alternation","Minzheng Li, Xiangzhong Fang, Haixin Yang","We envision a machine capable of solving mathematical problems. Dividing the
quantitative reasoning system into two parts: thought processes and cognitive
processes, we provide probabilistic descriptions of the architecture.",2308.08714v1,https://arxiv.org/pdf/2308.08714v1
"Improving Anomaly Segmentation with Multi-Granularity Cross-Domain
  Alignment","Ji Zhang, Xiao Wu, Zhi-Qi Cheng, Qi He, Wei Li","Anomaly segmentation plays a pivotal role in identifying atypical objects in
images, crucial for hazard detection in autonomous driving systems. While
existing methods demonstrate noteworthy results on synthetic data, they often
fail to consider the disparity between synthetic and real-world data domains.
Addressing this gap, we introduce the Multi-Granularity Cross-Domain Alignment
(MGCDA) framework, tailored to harmonize features across domains at both the
scene and individual sample levels. Our contributions are twofold: i) We
present the Multi-source Domain Adversarial Training module. This integrates a
multi-source adversarial loss coupled with dynamic label smoothing,
facilitating the learning of domain-agnostic representations across multiple
processing stages. ii) We propose an innovative Cross-domain Anomaly-aware
Contrastive Learning methodology.} This method adeptly selects challenging
anchor points and images using an anomaly-centric strategy, ensuring precise
alignment at the sample level. Extensive evaluations of the Fishyscapes and
RoadAnomaly datasets demonstrate MGCDA's superior performance and adaptability.
Additionally, its ability to perform parameter-free inference and function with
various network architectures highlights its distinctiveness in advancing the
frontier of anomaly segmentation.",2308.08696v2,https://arxiv.org/pdf/2308.08696v2
"LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series
  Forecasters","Ching Chang, Wei-Yao Wang, Wen-Chih Peng, Tien-Fu Chen","Multivariate time-series forecasting is vital in various domains, e.g.,
economic planning and weather prediction. Deep train-from-scratch models have
exhibited effective performance yet require large amounts of data, which limits
real-world applicability. Recently, researchers have leveraged the
representation learning transferability of pre-trained Large Language Models
(LLMs) to handle limited non-linguistic datasets effectively. However,
incorporating LLMs with time-series data presents challenges of limited
adaptation due to different compositions between time-series and linguistic
data, and the inability to process multi-scale temporal information. To tackle
these challenges, we propose LLM4TS, a framework for time-series forecasting
with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the
\textit{time-series alignment} stage to align LLMs with the nuances of
time-series data, and the \textit{forecasting fine-tuning} stage for downstream
time-series forecasting tasks. Furthermore, our framework features a novel
two-level aggregation method that integrates multi-scale temporal data within
pre-trained LLMs, enhancing their ability to interpret time-specific
information. In experiments across 7 time-series forecasting datasets, LLM4TS
is superior to existing state-of-the-art methods compared with
trained-from-scratch models in full-shot scenarios, and also achieves an
average improvement of 6.84% in MSE in few-shot scenarios. In addition,
evaluations compared with different self-supervised learning approaches
highlight LLM4TS's effectiveness with representation learning in forecasting
tasks.",2308.08469v5,https://arxiv.org/pdf/2308.08469v5
"TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for
  Time Series","Chenxi Sun, Hongyan Li, Yaliang Li, Shenda Hong","This work summarizes two ways to accomplish Time-Series (TS) tasks in today's
Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and
trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data;
TS-for-LLM (data-centric) converts TS into a model-friendly representation to
enable the pre-trained LLM to handle TS data. Given the lack of data, limited
resources, semantic context requirements, and so on, this work focuses on
TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a
TS embedding method suitable for LLM. The proposed method is named TEST. It
first tokenizes TS, builds an encoder to embed TS via instance-wise,
feature-wise, and text-prototype-aligned contrast, where the TS embedding space
is aligned to LLM embedding layer space, then creates soft prompts to make LLM
more open to that embeddings, and finally implements TS tasks using the frozen
LLM. We also demonstrate the feasibility of TS-for-LLM through theory and
experiments. Experiments are carried out on TS classification, forecasting, and
representation tasks using eight frozen LLMs with various structures and sizes.
The results show that the pre-trained LLM with TEST strategy can achieve better
or comparable performance than today's SOTA TS models and offer benefits for
few-shot and generalization. By treating LLM as the pattern machine, TEST can
endow LLM's ability to process TS data without compromising language ability.
We hope that this study will serve as a foundation for future work to support
TS+LLM progress.",2308.08241v2,https://arxiv.org/pdf/2308.08241v2
"Neural Latent Aligner: Cross-trial Alignment for Learning
  Representations of Complex, Naturalistic Neural Data","Cheol Jun Cho, Edward F. Chang, Gopala K. Anumanchipalli","Understanding the neural implementation of complex human behaviors is one of
the major goals in neuroscience. To this end, it is crucial to find a true
representation of the neural data, which is challenging due to the high
complexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here,
we propose a novel unsupervised learning framework, Neural Latent Aligner
(NLA), to find well-constrained, behaviorally relevant neural representations
of complex behaviors. The key idea is to align representations across repeated
trials to learn cross-trial consistent information. Furthermore, we propose a
novel, fully differentiable time warping model (TWM) to resolve the temporal
misalignment of trials. When applied to intracranial electrocorticography
(ECoG) of natural speaking, our model learns better representations for
decoding behaviors than the baseline models, especially in lower dimensional
space. The TWM is empirically validated by measuring behavioral coherence
between aligned trials. The proposed framework learns more cross-trial
consistent representations than the baselines, and when visualized, the
manifold reveals shared neural trajectories across trials.",2308.06443v1,https://arxiv.org/pdf/2308.06443v1
Improving Joint Speech-Text Representations Without Alignment,"Cal Peyser, Zhong Meng, Ke Hu, Rohit Prabhavalkar, Andrew Rosenberg, Tara N. Sainath, Michael Picheny, Kyunghyun Cho","The last year has seen astonishing progress in text-prompted image generation
premised on the idea of a cross-modal representation space in which the text
and image domains are represented jointly. In ASR, this idea has found
application as joint speech-text encoders that can scale to the capacities of
very large parameter models by being trained on both unpaired speech and text.
While these methods show promise, they have required special treatment of the
sequence-length mismatch inherent in speech and text, either by up-sampling
heuristics or an explicit alignment model. In this work, we offer evidence that
joint speech-text encoders naturally achieve consistent representations across
modalities by disregarding sequence length, and argue that consistency losses
could forgive length differences and simply assume the best alignment. We show
that such a loss improves downstream WER in both a large-parameter monolingual
and multilingual system.",2308.06125v1,https://arxiv.org/pdf/2308.06125v1
"Multimodality and Attention Increase Alignment in Natural Language
  Prediction Between Humans and Computational Models","Viktor Kewenig, Andrew Lampinen, Samuel A. Nastase, Christopher Edwards, Quitterie Lacome DEstalenx, Akilles Rechardt, Jeremy I Skipper, Gabriella Vigliocco","The potential of multimodal generative artificial intelligence (mAI) to
replicate human grounded language understanding, including the pragmatic,
context-rich aspects of communication, remains to be clarified. Humans are
known to use salient multimodal features, such as visual cues, to facilitate
the processing of upcoming words. Correspondingly, multimodal computational
models can integrate visual and linguistic data using a visual attention
mechanism to assign next-word probabilities. To test whether these processes
align, we tasked both human participants (N = 200) as well as several
state-of-the-art computational models with evaluating the predictability of
forthcoming words after viewing short audio-only or audio-visual clips with
speech. During the task, the model's attention weights were recorded and human
attention was indexed via eye tracking. Results show that predictability
estimates from humans aligned more closely with scores generated from
multimodal models vs. their unimodal counterparts. Furthermore, including an
attention mechanism doubled alignment with human judgments when visual and
linguistic context facilitated predictions. In these cases, the model's
attention patches and human eye tracking significantly overlapped. Our results
indicate that improved modeling of naturalistic language processing in mAI does
not merely depend on training diet but can be driven by multimodality in
combination with attention-based architectures. Humans and computational models
alike can leverage the predictive constraints of multimodal information by
attending to relevant features in the input.",2308.06035v3,https://arxiv.org/pdf/2308.06035v3
"Evaluating Picture Description Speech for Dementia Detection using
  Image-text Alignment","Youxiang Zhu, Nana Lin, Xiaohui Liang, John A. Batsis, Robert M. Roth, Brian MacWhinney","Using picture description speech for dementia detection has been studied for
30 years. Despite the long history, previous models focus on identifying the
differences in speech patterns between healthy subjects and patients with
dementia but do not utilize the picture information directly. In this paper, we
propose the first dementia detection models that take both the picture and the
description texts as inputs and incorporate knowledge from large pre-trained
image-text alignment models. We observe the difference between dementia and
healthy samples in terms of the text's relevance to the picture and the focused
area of the picture. We thus consider such a difference could be used to
enhance dementia detection accuracy. Specifically, we use the text's relevance
to the picture to rank and filter the sentences of the samples. We also
identified focused areas of the picture as topics and categorized the sentences
according to the focused areas. We propose three advanced models that
pre-processed the samples based on their relevance to the picture, sub-image,
and focused areas. The evaluation results show that our advanced models, with
knowledge of the picture and large image-text alignment models, achieve
state-of-the-art performance with the best detection accuracy at 83.44%, which
is higher than the text-only baseline model at 79.91%. Lastly, we visualize the
sample and picture results to explain the advantages of our models.",2308.07933v1,https://arxiv.org/pdf/2308.07933v1
"Multi-domain Recommendation with Embedding Disentangling and Domain
  Alignment","Wentao Ning, Xiao Yan, Weiwen Liu, Reynold Cheng, Rui Zhang, Bo Tang","Multi-domain recommendation (MDR) aims to provide recommendations for
different domains (e.g., types of products) with overlapping users/items and is
common for platforms such as Amazon, Facebook, and LinkedIn that host multiple
services. Existing MDR models face two challenges: First, it is difficult to
disentangle knowledge that generalizes across domains (e.g., a user likes cheap
items) and knowledge specific to a single domain (e.g., a user likes blue
clothing but not blue cars). Second, they have limited ability to transfer
knowledge across domains with small overlaps. We propose a new MDR method named
EDDA with two key components, i.e., embedding disentangling recommender and
domain alignment, to tackle the two challenges respectively. In particular, the
embedding disentangling recommender separates both the model and embedding for
the inter-domain part and the intra-domain part, while most existing MDR
methods only focus on model-level disentangling. The domain alignment leverages
random walks from graph processing to identify similar user/item pairs from
different domains and encourages similar user/item pairs to have similar
embeddings, enhancing knowledge transfer. We compare EDDA with 12
state-of-the-art baselines on 3 real datasets. The results show that EDDA
consistently outperforms the baselines on all datasets and domains. All
datasets and codes are available at https://github.com/Stevenn9981/EDDA.",2308.05508v2,https://arxiv.org/pdf/2308.05508v2
"Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language
  Models' Alignment","Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li","Ensuring alignment, which refers to making models behave in accordance with
human intentions [1,2], has become a critical task before deploying large
language models (LLMs) in real-world applications. For instance, OpenAI devoted
six months to iteratively aligning GPT-4 before its release [3]. However, a
major challenge faced by practitioners is the lack of clear guidance on
evaluating whether LLM outputs align with social norms, values, and
regulations. This obstacle hinders systematic iteration and deployment of LLMs.
To address this issue, this paper presents a comprehensive survey of key
dimensions that are crucial to consider when assessing LLM trustworthiness. The
survey covers seven major categories of LLM trustworthiness: reliability,
safety, fairness, resistance to misuse, explainability and reasoning, adherence
to social norms, and robustness. Each major category is further divided into
several sub-categories, resulting in a total of 29 sub-categories.
Additionally, a subset of 8 sub-categories is selected for further
investigation, where corresponding measurement studies are designed and
conducted on several widely-used LLMs. The measurement results indicate that,
in general, more aligned models tend to perform better in terms of overall
trustworthiness. However, the effectiveness of alignment varies across the
different trustworthiness categories considered. This highlights the importance
of conducting more fine-grained analyses, testing, and making continuous
improvements on LLM alignment. By shedding light on these key dimensions of LLM
trustworthiness, this paper aims to provide valuable insights and guidance to
practitioners in the field. Understanding and addressing these concerns will be
crucial in achieving reliable and ethically sound deployment of LLMs in various
applications.",2308.05374v2,https://arxiv.org/pdf/2308.05374v2
"In-Context Alignment: Chat with Vanilla Language Models Before
  Fine-Tuning",Xiaochuang Han,"In this note, we explore inference-time alignment through in-context
learning. We consider a vanilla pretrained language model Llama-2 before any
fine-tuning and retrieve an average of 9 demonstration alignment examples when
the model is prompted to follow chat-style instructions. Compared to direct
prompting, the in-context alignment without changing model weights leads to a
7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making
the vanilla language model comparable to strong baselines with alignment
fine-tuning.",2308.04275v1,https://arxiv.org/pdf/2308.04275v1
Gloss Alignment Using Word Embeddings,"Harry Walsh, Ozge Mercanoglu Sincan, Ben Saunders, Richard Bowden","Capturing and annotating Sign language datasets is a time consuming and
costly process. Current datasets are orders of magnitude too small to
successfully train unconstrained \acf{slt} models. As a result, research has
turned to TV broadcast content as a source of large-scale training data,
consisting of both the sign language interpreter and the associated audio
subtitle. However, lack of sign language annotation limits the usability of
this data and has led to the development of automatic annotation techniques
such as sign spotting. These spottings are aligned to the video rather than the
subtitle, which often results in a misalignment between the subtitle and
spotted signs. In this paper we propose a method for aligning spottings with
their corresponding subtitles using large spoken language models. Using a
single modality means our method is computationally inexpensive and can be
utilized in conjunction with existing alignment techniques. We quantitatively
demonstrate the effectiveness of our method on the \acf{mdgs} and \acf{bobsl}
datasets, recovering up to a 33.22 BLEU-1 score in word alignment.",2308.04248v1,https://arxiv.org/pdf/2308.04248v1
"PARL: A Unified Framework for Policy Alignment in Reinforcement Learning
  from Human Feedback","Souradip Chakraborty, Amrit Singh Bedi, Alec Koppel, Dinesh Manocha, Huazheng Wang, Mengdi Wang, Furong Huang","We present a novel unified bilevel optimization-based framework,
\textsf{PARL}, formulated to address the recently highlighted critical issue of
policy alignment in reinforcement learning using utility or preference-based
feedback. We identify a major gap within current algorithmic designs for
solving policy alignment due to a lack of precise characterization of the
dependence of the alignment objective on the data generated by policy
trajectories. This shortfall contributes to the sub-optimal performance
observed in contemporary algorithms. Our framework addressed these concerns by
explicitly parameterizing the distribution of the upper alignment objective
(reward design) by the lower optimal variable (optimal policy for the designed
reward). Interestingly, from an optimization perspective, our formulation leads
to a new class of stochastic bilevel problems where the stochasticity at the
upper objective depends upon the lower-level variable. {True to our best
knowledge, this work presents the first formulation of the RLHF as a bilevel
optimization problem which generalizes the existing RLHF formulations and
addresses the existing distribution shift issues in RLHF formulations.} To
demonstrate the efficacy of our formulation in resolving alignment issues in
RL, we devised an algorithm named \textsf{A-PARL} to solve PARL problem,
establishing sample complexity bounds of order $\mathcal{O}(1/T)$. Our
empirical results substantiate that the proposed \textsf{PARL} can address the
alignment concerns in RL by showing significant improvements (up to 63\% in
terms of required samples) for policy alignment in large-scale environments of
the Deepmind control suite and Meta world tasks.",2308.02585v3,https://arxiv.org/pdf/2308.02585v3
"LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and
  Semantic-Aware Alignment","Zhiwei Zhang, Zhizhong Zhang, Qian Yu, Ran Yi, Yuan Xie, Lizhuang Ma","3D panoptic segmentation is a challenging perception task that requires both
semantic segmentation and instance segmentation. In this task, we notice that
images could provide rich texture, color, and discriminative information, which
can complement LiDAR data for evident performance improvement, but their fusion
remains a challenging problem. To this end, we propose LCPS, the first
LiDAR-Camera Panoptic Segmentation network. In our approach, we conduct
LiDAR-Camera fusion in three stages: 1) an Asynchronous Compensation Pixel
Alignment (ACPA) module that calibrates the coordinate misalignment caused by
asynchronous problems between sensors; 2) a Semantic-Aware Region Alignment
(SARA) module that extends the one-to-one point-pixel mapping to one-to-many
semantic relations; 3) a Point-to-Voxel feature Propagation (PVP) module that
integrates both geometric and semantic fusion information for the entire point
cloud. Our fusion strategy improves about 6.9% PQ performance over the
LiDAR-only baseline on NuScenes dataset. Extensive quantitative and qualitative
experiments further demonstrate the effectiveness of our novel framework. The
code will be released at https://github.com/zhangzw12319/lcps.git.",2308.01686v2,https://arxiv.org/pdf/2308.01686v2
"Learning Implicit Entity-object Relations by Bidirectional Generative
  Alignment for Multimodal NER","Feng Chen, Jiajia Liu, Kaixiang Ji, Wang Ren, Jian Wang, Jingdong Wang","The challenge posed by multimodal named entity recognition (MNER) is mainly
two-fold: (1) bridging the semantic gap between text and image and (2) matching
the entity with its associated object in image. Existing methods fail to
capture the implicit entity-object relations, due to the lack of corresponding
annotation. In this paper, we propose a bidirectional generative alignment
method named BGA-MNER to tackle these issues. Our BGA-MNER consists of
\texttt{image2text} and \texttt{text2image} generation with respect to
entity-salient content in two modalities. It jointly optimizes the
bidirectional reconstruction objectives, leading to aligning the implicit
entity-object relations under such direct and powerful constraints.
Furthermore, image-text pairs usually contain unmatched components which are
noisy for generation. A stage-refined context sampler is proposed to extract
the matched cross-modal content for generation. Extensive experiments on two
benchmarks demonstrate that our method achieves state-of-the-art performance
without image input during inference.",2308.02570v1,https://arxiv.org/pdf/2308.02570v1
"Through their eyes: multi-subject Brain Decoding with simple alignment
  techniques","Matteo Ferrante, Tommaso Boccato, Nicola Toschi","Previous brain decoding research primarily involves single-subject studies,
reconstructing stimuli via fMRI activity from the same subject. Our study aims
to introduce a generalization technique for cross-subject brain decoding,
facilitated by exploring data alignment methods. We utilized the NSD dataset, a
comprehensive 7T fMRI vision experiment involving multiple subjects exposed to
9841 images, 982 of which were viewed by all. Our approach involved training a
decoding model on one subject, aligning others' data to this space, and testing
the decoding on the second subject. We compared ridge regression, hyper
alignment, and anatomical alignment techniques for fMRI data alignment. We
established that cross-subject brain decoding is feasible, even using around
10% of the total data, or 982 common images, with comparable performance to
single-subject decoding. Ridge regression was the best method for functional
alignment. Through subject alignment, we achieved superior brain decoding and a
potential 90% reduction in scan time. This could pave the way for more
efficient experiments and further advancements in the field, typically
requiring an exorbitant 20-hour scan time per subject.",2309.00627v1,https://arxiv.org/pdf/2309.00627v1
"Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey
  with Causality Perspectives","Haoyang Liu, Maheep Chaudhary, Haohan Wang","The trustworthiness of machine learning has emerged as a critical topic in
the field, encompassing various applications and research areas such as
robustness, security, interpretability, and fairness. The last decade saw the
development of numerous methods addressing these challenges. In this survey, we
systematically review these advancements from a data-centric perspective,
highlighting the shortcomings of traditional empirical risk minimization (ERM)
training in handling challenges posed by the data.
  Interestingly, we observe a convergence of these methods, despite being
developed independently across trustworthy machine learning subfields. Pearl's
hierarchy of causality offers a unifying framework for these techniques.
Accordingly, this survey presents the background of trustworthy machine
learning development using a unified set of concepts, connects this language to
Pearl's causal hierarchy, and finally discusses methods explicitly inspired by
causality literature. We provide a unified language with mathematical
vocabulary to link these methods across robustness, adversarial robustness,
interpretability, and fairness, fostering a more cohesive understanding of the
field.
  Further, we explore the trustworthiness of large pretrained models. After
summarizing dominant techniques like fine-tuning, parameter-efficient
fine-tuning, prompting, and reinforcement learning with human feedback, we draw
connections between them and the standard ERM. This connection allows us to
build upon the principled understanding of trustworthy methods, extending it to
these new techniques in large pretrained models, paving the way for future
methods. Existing methods under this perspective are also reviewed.
  Lastly, we offer a brief summary of the applications of these methods and
discuss potential future aspects related to our survey. For more information,
please visit http://trustai.one.",2307.16851v1,https://arxiv.org/pdf/2307.16851v1
"Rethinking Uncertainly Missing and Ambiguous Visual Modality in
  Multi-Modal Entity Alignment","Zhuo Chen, Lingbing Guo, Yin Fang, Yichi Zhang, Jiaoyan Chen, Jeff Z. Pan, Yangning Li, Huajun Chen, Wen Zhang","As a crucial extension of entity alignment (EA), multi-modal entity alignment
(MMEA) aims to identify identical entities across disparate knowledge graphs
(KGs) by exploiting associated visual information. However, existing MMEA
approaches primarily concentrate on the fusion paradigm of multi-modal entity
features, while neglecting the challenges presented by the pervasive phenomenon
of missing and intrinsic ambiguity of visual images. In this paper, we present
a further analysis of visual modality incompleteness, benchmarking latest MMEA
models on our proposed dataset MMEA-UMVM, where the types of alignment KGs
covering bilingual and monolingual, with standard (non-iterative) and iterative
training paradigms to evaluate the model performance. Our research indicates
that, in the face of modality incompleteness, models succumb to overfitting the
modality noise, and exhibit performance oscillations or declines at high rates
of missing modality. This proves that the inclusion of additional multi-modal
data can sometimes adversely affect EA. To address these challenges, we
introduce UMAEA , a robust multi-modal entity alignment approach designed to
tackle uncertainly missing and ambiguous visual modalities. It consistently
achieves SOTA performance across all 97 benchmark splits, significantly
surpassing existing baselines with limited parameters and time consumption,
while effectively alleviating the identified limitations of other models. Our
code and benchmark data are available at https://github.com/zjukg/UMAEA.",2307.16210v2,https://arxiv.org/pdf/2307.16210v2
ETHER: Aligning Emergent Communication for Hindsight Experience Replay,"Kevin Denamganaï, Daniel Hernandez, Ozan Vardal, Sondess Missaoui, James Alfred Walker","Natural language instruction following is paramount to enable collaboration
between artificial agents and human beings. Natural language-conditioned
reinforcement learning (RL) agents have shown how natural languages'
properties, such as compositionality, can provide a strong inductive bias to
learn complex policies. Previous architectures like HIGhER combine the benefit
of language-conditioning with Hindsight Experience Replay (HER) to deal with
sparse rewards environments. Yet, like HER, HIGhER relies on an oracle
predicate function to provide a feedback signal highlighting which linguistic
description is valid for which state. This reliance on an oracle limits its
application. Additionally, HIGhER only leverages the linguistic information
contained in successful RL trajectories, thus hurting its final performance and
data-efficiency. Without early successful trajectories, HIGhER is no better
than DQN upon which it is built. In this paper, we propose the Emergent Textual
Hindsight Experience Replay (ETHER) agent, which builds on HIGhER and addresses
both of its limitations by means of (i) a discriminative visual referential
game, commonly studied in the subfield of Emergent Communication (EC), used
here as an unsupervised auxiliary task and (ii) a semantic grounding scheme to
align the emergent language with the natural language of the
instruction-following benchmark. We show that the referential game's agents
make an artificial language emerge that is aligned with the natural-like
language used to describe goals in the BabyAI benchmark and that it is
expressive enough so as to also describe unsuccessful RL trajectories and thus
provide feedback to the RL agent to leverage the linguistic, structured
information contained in all trajectories. Our work shows that EC is a viable
unsupervised auxiliary task for RL and provides missing pieces to make HER more
widely applicable.",2307.15494v2,https://arxiv.org/pdf/2307.15494v2
Optimal Alignment of Temporal Knowledge Bases,"Oliver Fernandez-Gil, Fabio Patrizi, Giuseppe Perelli, Anni-Yasmin Turhan","Answering temporal CQs over temporalized Description Logic knowledge bases
(TKB) is a main technique to realize ontology-based situation recognition. In
case the collected data in such a knowledge base is inaccurate, important query
answers can be missed. In this paper we introduce the TKB Alignment problem,
which computes a variant of the TKB that minimally changes the TKB, but entails
the given temporal CQ and is in that sense (cost-)optimal. We investigate this
problem for ALC TKBs and conjunctive queries with LTL operators and devise a
solution technique to compute (cost-optimal) alignments of TKBs that extends
techniques for the alignment problem for propositional LTL over finite traces.",2307.15439v1,https://arxiv.org/pdf/2307.15439v1
"Universal and Transferable Adversarial Attacks on Aligned Language
  Models","Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, Matt Fredrikson","Because ""out-of-the-box"" large language models are capable of generating a
great deal of objectionable content, recent work has focused on aligning these
models in an attempt to prevent undesirable generation. While there has been
some success at circumventing these measures -- so-called ""jailbreaks"" against
LLMs -- these attacks have required significant human ingenuity and are brittle
in practice. In this paper, we propose a simple and effective attack method
that causes aligned language models to generate objectionable behaviors.
Specifically, our approach finds a suffix that, when attached to a wide range
of queries for an LLM to produce objectionable content, aims to maximize the
probability that the model produces an affirmative response (rather than
refusing to answer). However, instead of relying on manual engineering, our
approach automatically produces these adversarial suffixes by a combination of
greedy and gradient-based search techniques, and also improves over past
automatic prompt generation methods.
  Surprisingly, we find that the adversarial prompts generated by our approach
are quite transferable, including to black-box, publicly released LLMs.
Specifically, we train an adversarial attack suffix on multiple prompts (i.e.,
queries asking for many different types of objectionable content), as well as
multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting
attack suffix is able to induce objectionable content in the public interfaces
to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,
Pythia, Falcon, and others. In total, this work significantly advances the
state-of-the-art in adversarial attacks against aligned language models,
raising important questions about how such systems can be prevented from
producing objectionable information. Code is available at
github.com/llm-attacks/llm-attacks.",2307.15043v2,https://arxiv.org/pdf/2307.15043v2
"Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning
  Sparse Contextualized Word Representations",Gábor Berend,"In this paper, we advocate for using large pre-trained monolingual language
models in cross lingual zero-shot word sense disambiguation (WSD) coupled with
a contextualized mapping mechanism. We also report rigorous experiments that
illustrate the effectiveness of employing sparse contextualized word
representations obtained via a dictionary learning procedure. Our experimental
results demonstrate that the above modifications yield a significant
improvement of nearly 6.5 points of increase in the average F-score (from 62.0
to 68.5) over a collection of 17 typologically diverse set of target languages.
We release our source code for replicating our experiments at
https://github.com/begab/sparsity_makes_sense.",2307.13776v1,https://arxiv.org/pdf/2307.13776v1
"RLCD: Reinforcement Learning from Contrastive Distillation for Language
  Model Alignment","Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong Tian","We propose Reinforcement Learning from Contrastive Distillation (RLCD), a
method for aligning language models to follow principles expressed in natural
language (e.g., to be more harmless) without using human feedback. RLCD creates
preference pairs from two contrasting model outputs, one using a positive
prompt designed to encourage following the given principles, and one using a
negative prompt designed to encourage violating them. Using two different
prompts causes model outputs to be more differentiated on average, resulting in
cleaner preference labels in the absence of human annotations. We then use the
preference pairs to train a preference model, which is in turn used to improve
a base unaligned language model via reinforcement learning. Empirically, RLCD
outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al.,
2022) baselines across three diverse alignment tasks--harmlessness,
helpfulness, and story outline generation--and when using both 7B and 30B model
scales for simulating preference data.",2307.12950v3,https://arxiv.org/pdf/2307.12950v3
"Early Neuron Alignment in Two-layer ReLU Networks with Small
  Initialization","Hancheng Min, Enrique Mallada, René Vidal","This paper studies the problem of training a two-layer ReLU network for
binary classification using gradient flow with small initialization. We
consider a training dataset with well-separated input vectors: Any pair of
input data with the same label are positively correlated, and any pair with
different labels are negatively correlated. Our analysis shows that, during the
early phase of training, neurons in the first layer try to align with either
the positive data or the negative data, depending on its corresponding weight
on the second layer. A careful analysis of the neurons' directional dynamics
allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on
the time it takes for all neurons to achieve good alignment with the input
data, where $n$ is the number of data points and $\mu$ measures how well the
data are separated. After the early alignment phase, the loss converges to zero
at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer
is approximately low-rank. Numerical experiments on the MNIST dataset
illustrate our theoretical findings.",2307.12851v2,https://arxiv.org/pdf/2307.12851v2
AlignDet: Aligning Pre-training and Fine-tuning in Object Detection,"Ming Li, Jie Wu, Xionghui Wang, Chen Chen, Jie Qin, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan","The paradigm of large-scale pre-training followed by downstream fine-tuning
has been widely employed in various object detection algorithms. In this paper,
we reveal discrepancies in data, model, and task between the pre-training and
fine-tuning procedure in existing practices, which implicitly limit the
detector's performance, generalization ability, and convergence speed. To this
end, we propose AlignDet, a unified pre-training framework that can be adapted
to various existing detectors to alleviate the discrepancies. AlignDet
decouples the pre-training process into two stages, i.e., image-domain and
box-domain pre-training. The image-domain pre-training optimizes the detection
backbone to capture holistic visual abstraction, and box-domain pre-training
learns instance-level semantics and task-aware concepts to initialize the parts
out of the backbone. By incorporating the self-supervised pre-trained
backbones, we can pre-train all modules for various detectors in an
unsupervised paradigm. As depicted in Figure 1, extensive experiments
demonstrate that AlignDet can achieve significant improvements across diverse
protocols, such as detection algorithm, model backbone, data setting, and
training schedule. For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by
2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.",2307.11077v2,https://arxiv.org/pdf/2307.11077v2
"Of Models and Tin Men: A Behavioural Economics Study of Principal-Agent
  Problems in AI Alignment using Large-Language Models","Steve Phelps, Rebecca Ranson","AI Alignment is often presented as an interaction between a single designer
and an artificial agent in which the designer attempts to ensure the agent's
behavior is consistent with its purpose, and risks arise solely because of
conflicts caused by inadvertent misalignment between the utility function
intended by the designer and the resulting internal utility function of the
agent. With the advent of agents instantiated with large-language models
(LLMs), which are typically pre-trained, we argue this does not capture the
essential aspects of AI safety because in the real world there is not a
one-to-one correspondence between designer and agent, and the many agents, both
artificial and human, have heterogeneous values. Therefore, there is an
economic aspect to AI safety and the principal-agent problem is likely to
arise. In a principal-agent problem conflict arises because of information
asymmetry together with inherent misalignment between the utility of the agent
and its principal, and this inherent misalignment cannot be overcome by
coercing the agent into adopting a desired utility function through training.
We argue the assumptions underlying principal-agent problems are crucial to
capturing the essence of safety problems involving pre-trained AI models in
real-world situations. Taking an empirical approach to AI safety, we
investigate how GPT models respond in principal-agent conflicts. We find that
agents based on both GPT-3.5 and GPT-4 override their principal's objectives in
a simple online shopping task, showing clear evidence of principal-agent
conflict. Surprisingly, the earlier GPT-3.5 model exhibits more nuanced
behaviour in response to changes in information asymmetry, whereas the later
GPT-4 model is more rigid in adhering to its prior alignment. Our results
highlight the importance of incorporating principles from economics into the
alignment process.",2307.11137v3,https://arxiv.org/pdf/2307.11137v3
"FLASK: Fine-grained Language Model Evaluation based on Alignment Skill
  Sets","Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo","Evaluation of Large Language Models (LLMs) is challenging because
instruction-following necessitates alignment with human values and the required
set of skills varies depending on the instruction. However, previous studies
have mainly focused on coarse-grained evaluation (i.e. overall preference-based
evaluation), which limits interpretability since it does not consider the
nature of user instructions that require instance-wise skill composition. In
this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on
Alignment Skill Sets), a fine-grained evaluation protocol for both human-based
and model-based evaluation which decomposes coarse-level scoring to a skill
set-level scoring for each instruction. We experimentally observe that the
fine-graininess of evaluation is crucial for attaining a holistic view of model
performance and increasing the reliability of the evaluation. Using FLASK, we
compare multiple open-source and proprietary LLMs and observe a high
correlation between model-based and human-based evaluations. We publicly
release the evaluation data and code implementation at
https://github.com/kaistAI/FLASK.",2307.10928v4,https://arxiv.org/pdf/2307.10928v4
Deceptive Alignment Monitoring,"Andres Carranza, Dhruv Pai, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo","As the capabilities of large machine learning models continue to grow, and as
the autonomy afforded to such models continues to expand, the spectre of a new
adversary looms: the models themselves. The threat that a model might behave in
a seemingly reasonable manner, while secretly and subtly modifying its behavior
for ulterior reasons is often referred to as deceptive alignment in the AI
Safety & Alignment communities. Consequently, we call this new direction
Deceptive Alignment Monitoring. In this work, we identify emerging directions
in diverse machine learning subfields that we believe will become increasingly
important and intertwined in the near future for deceptive alignment
monitoring, and we argue that advances in these fields present both long-term
challenges and new research opportunities. We conclude by advocating for
greater involvement by the adversarial machine learning community in these
emerging directions.",2307.10569v2,https://arxiv.org/pdf/2307.10569v2
Attacking by Aligning: Clean-Label Backdoor Attacks on Object Detection,"Yize Cheng, Wenbin Hu, Minhao Cheng","Deep neural networks (DNNs) have shown unprecedented success in object
detection tasks. However, it was also discovered that DNNs are vulnerable to
multiple kinds of attacks, including Backdoor Attacks. Through the attack, the
attacker manages to embed a hidden backdoor into the DNN such that the model
behaves normally on benign data samples, but makes attacker-specified judgments
given the occurrence of a predefined trigger. Although numerous backdoor
attacks have been experimented on image classification, backdoor attacks on
object detection tasks have not been properly investigated and explored. As
object detection has been adopted as an important module in multiple
security-sensitive applications such as autonomous driving, backdoor attacks on
object detection could pose even more severe threats. Inspired by the inherent
property of deep learning-based object detectors, we propose a simple yet
effective backdoor attack method against object detection without modifying the
ground truth annotations, specifically focusing on the object disappearance
attack and object generation attack. Extensive experiments and ablation studies
prove the effectiveness of our attack on the benchmark object detection dataset
MSCOCO2017, on which we achieve an attack success rate of more than 92% with a
poison rate of only 5%.",2307.10487v2,https://arxiv.org/pdf/2307.10487v2
"AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment
  enabled by Large Language Models","Rui Zhang, Yixin Su, Bayu Distiawan Trisedya, Xiaoyan Zhao, Min Yang, Hong Cheng, Jianzhong Qi","The task of entity alignment between knowledge graphs (KGs) aims to identify
every pair of entities from two different KGs that represent the same entity.
Many machine learning-based methods have been proposed for this task. However,
to our best knowledge, existing methods all require manually crafted seed
alignments, which are expensive to obtain. In this paper, we propose the first
fully automatic alignment method named AutoAlign, which does not require any
manually crafted seed alignments. Specifically, for predicate embeddings,
AutoAlign constructs a predicate-proximity-graph with the help of large
language models to automatically capture the similarity between predicates
across two KGs. For entity embeddings, AutoAlign first computes the entity
embeddings of each KG independently using TransE, and then shifts the two KGs'
entity embeddings into the same vector space by computing the similarity
between entities based on their attributes. Thus, both predicate alignment and
entity alignment can be done without manually crafted seed alignments.
AutoAlign is not only fully automatic, but also highly effective. Experiments
using real-world KGs show that AutoAlign improves the performance of entity
alignment significantly compared to state-of-the-art methods.",2307.11772v3,https://arxiv.org/pdf/2307.11772v3
"Vocoder drift compensation by x-vector alignment in speaker
  anonymisation","Michele Panariello, Massimiliano Todisco, Nicholas Evans","For the most popular x-vector-based approaches to speaker anonymisation, the
bulk of the anonymisation can stem from vocoding rather than from the core
anonymisation function which is used to substitute an original speaker x-vector
with that of a fictitious pseudo-speaker. This phenomenon can impede the design
of better anonymisation systems since there is a lack of fine-grained control
over the x-vector space. The work reported in this paper explores the origin of
so-called vocoder drift and shows that it is due to the mismatch between the
substituted x-vector and the original representations of the linguistic
content, intonation and prosody. Also reported is an original approach to
vocoder drift compensation. While anonymisation performance degrades as
expected, compensation reduces vocoder drift substantially, offers improved
control over the x-vector space and lays a foundation for the design of better
anonymisation functions in the future.",2307.08403v1,https://arxiv.org/pdf/2307.08403v1
"Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding
  (Survey)","Subba Reddy Oota, Zijiao Chen, Manish Gupta, Raju S. Bapi, Gael Jobard, Frederic Alexandre, Xavier Hinaut","Can we obtain insights about the brain using AI models? How is the
information in deep learning models related to brain recordings? Can we improve
AI models with the help of brain recordings? Such questions can be tackled by
studying brain recordings like functional magnetic resonance imaging (fMRI). As
a first step, the neuroscience community has contributed several large
cognitive neuroscience datasets related to passive reading/listening/viewing of
concept words, narratives, pictures, and movies. Encoding and decoding models
using these datasets have also been proposed in the past two decades. These
models serve as additional tools for basic cognitive science and neuroscience
research. Encoding models aim at generating fMRI brain representations given a
stimulus automatically. They have several practical applications in evaluating
and diagnosing neurological conditions and thus may also help design therapies
for brain damage. Decoding models solve the inverse problem of reconstructing
the stimuli given the fMRI. They are useful for designing brain-machine or
brain-computer interfaces. Inspired by the effectiveness of deep learning
models for natural language processing, computer vision, and speech, several
neural encoding and decoding models have been recently proposed. In this
survey, we will first discuss popular representations of language, vision and
speech stimuli, and present a summary of neuroscience datasets. Further, we
will review popular deep learning based encoding and decoding architectures and
note their benefits and limitations. Finally, we will conclude with a summary
and discussion about future trends. Given the large amount of recently
published work in the computational cognitive neuroscience (CCN) community, we
believe that this survey enables an entry point for DNN researchers to
diversify into CCN research.",2307.10246v2,https://arxiv.org/pdf/2307.10246v2
"DSV: An Alignment Validation Loss for Self-supervised Outlier Model
  Selection","Jaemin Yoo, Yue Zhao, Lingxiao Zhao, Leman Akoglu","Self-supervised learning (SSL) has proven effective in solving various
problems by generating internal supervisory signals. Unsupervised anomaly
detection, which faces the high cost of obtaining true labels, is an area that
can greatly benefit from SSL. However, recent literature suggests that tuning
the hyperparameters (HP) of data augmentation functions is crucial to the
success of SSL-based anomaly detection (SSAD), yet a systematic method for
doing so remains unknown. In this work, we propose DSV (Discordance and
Separability Validation), an unsupervised validation loss to select
high-performing detection models with effective augmentation HPs. DSV captures
the alignment between an augmentation function and the anomaly-generating
mechanism with surrogate losses, which approximate the discordance and
separability of test data, respectively. As a result, the evaluation via DSV
leads to selecting an effective SSAD model exhibiting better alignment, which
results in high detection accuracy. We theoretically derive the degree of
approximation conducted by the surrogate losses and empirically show that DSV
outperforms a wide range of baselines on 21 real-world tasks.",2307.06534v1,https://arxiv.org/pdf/2307.06534v1
"Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural
  Networks","Inbar Seroussi, Asaf Miron, Zohar Ringel","Physically informed neural networks (PINNs) are a promising emerging method
for solving differential equations. As in many other deep learning approaches,
the choice of PINN design and training protocol requires careful craftsmanship.
Here, we suggest a comprehensive theoretical framework that sheds light on this
important problem. Leveraging an equivalence between infinitely
over-parameterized neural networks and Gaussian process regression (GPR), we
derive an integro-differential equation that governs PINN prediction in the
large data-set limit -- the neurally-informed equation. This equation augments
the original one by a kernel term reflecting architecture choices and allows
quantifying implicit bias induced by the network via a spectral decomposition
of the source term in the original differential equation.",2307.06362v2,https://arxiv.org/pdf/2307.06362v2
"An Effective and Efficient Time-aware Entity Alignment Framework via
  Two-aspect Three-view Label Propagation","Li Cai, Xin Mao, Youshao Xiao, Changxu Wu, Man Lan","Entity alignment (EA) aims to find the equivalent entity pairs between
different knowledge graphs (KGs), which is crucial to promote knowledge fusion.
With the wide use of temporal knowledge graphs (TKGs), time-aware EA (TEA)
methods appear to enhance EA. Existing TEA models are based on Graph Neural
Networks (GNN) and achieve state-of-the-art (SOTA) performance, but it is
difficult to transfer them to large-scale TKGs due to the scalability issue of
GNN. In this paper, we propose an effective and efficient non-neural EA
framework between TKGs, namely LightTEA, which consists of four essential
components: (1) Two-aspect Three-view Label Propagation, (2) Sparse Similarity
with Temporal Constraints, (3) Sinkhorn Operator, and (4) Temporal Iterative
Learning. All of these modules work together to improve the performance of EA
while reducing the time consumption of the model. Extensive experiments on
public datasets indicate that our proposed model significantly outperforms the
SOTA methods for EA between TKGs, and the time consumed by LightTEA is only
dozens of seconds at most, no more than 10% of the most efficient TEA method.",2307.06013v1,https://arxiv.org/pdf/2307.06013v1
TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation,"Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille","The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the noise used as a seed for the images. We also quantify the
influence of the number of concepts in the prompt, their order as well as their
(color) attributes. Finally, our method allows us to identify some seeds that
produce better images than others, opening novel directions of research on this
understudied topic.",2307.05134v2,https://arxiv.org/pdf/2307.05134v2
Leveraging an Alignment Set in Tackling Instance-Dependent Label Noise,"Donna Tjandra, Jenna Wiens","Noisy training labels can hurt model performance. Most approaches that aim to
address label noise assume label noise is independent from the input features.
In practice, however, label noise is often feature or
\textit{instance-dependent}, and therefore biased (i.e., some instances are
more likely to be mislabeled than others). E.g., in clinical care, female
patients are more likely to be under-diagnosed for cardiovascular disease
compared to male patients. Approaches that ignore this dependence can produce
models with poor discriminative performance, and in many healthcare settings,
can exacerbate issues around health disparities. In light of these limitations,
we propose a two-stage approach to learn in the presence instance-dependent
label noise. Our approach utilizes \textit{\anchor points}, a small subset of
data for which we know the observed and ground truth labels. On several tasks,
our approach leads to consistent improvements over the state-of-the-art in
discriminative performance (AUROC) while mitigating bias (area under the
equalized odds curve, AUEOC). For example, when predicting acute respiratory
failure onset on the MIMIC-III dataset, our approach achieves a harmonic mean
(AUROC and AUEOC) of 0.84 (SD [standard deviation] 0.01) while that of the next
best baseline is 0.81 (SD 0.01). Overall, our approach improves accuracy while
mitigating potential bias compared to existing approaches in the presence of
instance-dependent label noise.",2307.04868v1,https://arxiv.org/pdf/2307.04868v1
Linear Alignment of Vision-language Models for Image Captioning,"Fabian Paischer, Markus Hofmarcher, Sepp Hochreiter, Thomas Adler","Recently, vision-language models like CLIP have advanced the state of the art
in a variety of multi-modal tasks including image captioning and caption
evaluation. Many approaches adapt CLIP-style models to a downstream task by
training a mapping network between CLIP and a language model. This is costly as
it usually involves calculating gradients for large models. We propose a more
efficient training protocol that fits a linear mapping between image and text
embeddings of CLIP via a closed-form solution. This bypasses the need for
gradient computation and results in a lightweight captioning method called
ReCap, which can be trained up to 1000 times faster than existing lightweight
methods. Moreover, we propose two new learning-based image-captioning metrics
that build on CLIP score along with our linear mapping. Furthermore, we combine
ReCap with our new metrics to design an iterative datastore-augmentation loop
(DAL) based on synthetic captions. We evaluate ReCap on MS-COCO, Flickr30k,
VizWiz, and MSRVTT. ReCap achieves performance comparable to state-of-the-art
lightweight methods on established metrics while outperforming them on our new
metrics, which are better aligned with human ratings on Flickr8k-Expert and
Flickr8k-Crowdflower. Finally, we demonstrate that ReCap transfers well to
other domains and that our DAL leads to a performance boost.",2307.05591v3,https://arxiv.org/pdf/2307.05591v3
"Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image
  Alignment with Iterative VQA Feedback","Jaskirat Singh, Liang Zheng","The field of text-conditioned image generation has made unparalleled progress
with the recent advent of latent diffusion models. While remarkable, as the
complexity of given text input increases, the state-of-the-art diffusion models
may still fail in generating images which accurately convey the semantics of
the given prompt. Furthermore, it has been observed that such misalignments are
often left undetected by pretrained multi-modal models such as CLIP. To address
these problems, in this paper we explore a simple yet effective decompositional
approach towards both evaluation and improvement of text-to-image alignment. In
particular, we first introduce a Decompositional-Alignment-Score which given a
complex prompt decomposes it into a set of disjoint assertions. The alignment
of each assertion with generated images is then measured using a VQA model.
Finally, alignment scores for different assertions are combined aposteriori to
give the final text-to-image alignment score. Experimental analysis reveals
that the proposed alignment metric shows significantly higher correlation with
human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also
find that the assertion level alignment scores provide a useful feedback which
can then be used in a simple iterative procedure to gradually increase the
expression of different assertions in the final image outputs. Human user
studies indicate that the proposed approach surpasses previous state-of-the-art
by 8.7% in overall text-to-image alignment accuracy. Project page for our paper
is available at https://1jsingh.github.io/divide-evaluate-and-refine",2307.04749v2,https://arxiv.org/pdf/2307.04749v2
"Trajectory Alignment: Understanding the Edge of Stability Phenomenon via
  Bifurcation Theory","Minhak Song, Chulhee Yun","Cohen et al. (2021) empirically study the evolution of the largest eigenvalue
of the loss Hessian, also known as sharpness, along the gradient descent (GD)
trajectory and observe the Edge of Stability (EoS) phenomenon. The sharpness
increases at the early phase of training (referred to as progressive
sharpening), and eventually saturates close to the threshold of $2 /
\text{(step size)}$. In this paper, we start by demonstrating through empirical
studies that when the EoS phenomenon occurs, different GD trajectories (after a
proper reparameterization) align on a specific bifurcation diagram independent
of initialization. We then rigorously prove this trajectory alignment
phenomenon for a two-layer fully-connected linear network and a single-neuron
nonlinear network trained with a single data point. Our trajectory alignment
analysis establishes both progressive sharpening and EoS phenomena,
encompassing and extending recent findings in the literature.",2307.04204v2,https://arxiv.org/pdf/2307.04204v2
Parallel Algorithms Align with Neural Execution,"Valerie Engelmayer, Dobrik Georgiev, Petar Veličković","Neural algorithmic reasoners are parallel processors. Teaching them
sequential algorithms contradicts this nature, rendering a significant share of
their computations redundant. Parallel algorithms however may exploit their
full computational power, therefore requiring fewer layers to be executed. This
drastically reduces training times, as we observe when comparing parallel
implementations of searching, sorting and finding strongly connected components
to their sequential counterparts on the CLRS framework. Additionally, parallel
versions achieve (often strongly) superior predictive performance.",2307.04049v2,https://arxiv.org/pdf/2307.04049v2
"All in One: Exploring Unified Vision-Language Tracking with Multi-Modal
  Alignment","Chunhui Zhang, Xin Sun, Li Liu, Yiqian Yang, Qiong Liu, Xi Zhou, Yanfeng Wang","Current mainstream vision-language (VL) tracking framework consists of three
parts, \ie a visual feature extractor, a language feature extractor, and a
fusion model. To pursue better performance, a natural modus operandi for VL
tracking is employing customized and heavier unimodal encoders, and multi-modal
fusion models. Albeit effective, existing VL trackers separate feature
extraction and feature integration, resulting in extracted features that lack
semantic guidance and have limited target-aware capability in complex
scenarios, \eg similar distractors and extreme illumination. In this work,
inspired by the recent success of exploring foundation models with unified
architecture for both natural language and computer vision tasks, we propose an
All-in-One framework, which learns joint feature extraction and interaction by
adopting a unified transformer backbone. Specifically, we mix raw vision and
language signals to generate language-injected vision tokens, which we then
concatenate before feeding into the unified backbone architecture. This
approach achieves feature integration in a unified backbone, removing the need
for carefully-designed fusion modules and resulting in a more effective and
efficient VL tracking framework. To further improve the learning efficiency, we
introduce a multi-modal alignment module based on cross-modal and intra-modal
contrastive objectives, providing more reasonable representations for the
unified All-in-One transformer backbone. Extensive experiments on five
benchmarks, \ie OTB99-L, TNL2K, LaSOT, LaSOT$_{\rm Ext}$ and WebUAV-3M,
demonstrate the superiority of the proposed tracker against existing
state-of-the-arts on VL tracking. Codes will be made publicly available.",2307.03373v1,https://arxiv.org/pdf/2307.03373v1
"Human Inspired Progressive Alignment and Comparative Learning for
  Grounded Word Acquisition","Yuwei Bao, Barrett Martin Lattimer, Joyce Chai","Human language acquisition is an efficient, supervised, and continual
process. In this work, we took inspiration from how human babies acquire their
first language, and developed a computational process for word acquisition
through comparative learning. Motivated by cognitive findings, we generated a
small dataset that enables the computation models to compare the similarities
and differences of various attributes, learn to filter out and extract the
common information for each shared linguistic label. We frame the acquisition
of words as not only the information filtration process, but also as
representation-symbol mapping. This procedure does not involve a fixed
vocabulary size, nor a discriminative objective, and allows the models to
continually learn more concepts efficiently. Our results in controlled
experiments have shown the potential of this approach for efficient continual
learning of grounded words.",2307.02615v1,https://arxiv.org/pdf/2307.02615v1
Gaussian Database Alignment and Gaussian Planted Matching,"Osman Emre Dai, Daniel Cullina, Negar Kiyavash","Database alignment is a variant of the graph alignment problem: Given a pair
of anonymized databases containing separate yet correlated features for a set
of users, the problem is to identify the correspondence between the features
and align the anonymized user sets based on correlation alone. This closely
relates to planted matching, where given a bigraph with random weights, the
goal is to identify the underlying matching that generated the given weights.
We study an instance of the database alignment problem with multivariate
Gaussian features and derive results that apply both for database alignment and
for planted matching, demonstrating the connection between them. The
performance thresholds for database alignment converge to that for planted
matching when the dimensionality of the database features is \(\omega(\log
n)\), where \(n\) is the size of the alignment, and no individual feature is
too strong. The maximum likelihood algorithms for both planted matching and
database alignment take the form of a linear program and we study relaxations
to better understand the significance of various constraints under various
conditions and present achievability and converse bounds. Our results show that
the almost-exact alignment threshold for the relaxed algorithms coincide with
that of maximum likelihood, while there is a gap between the exact alignment
thresholds. Our analysis and results extend to the unbalanced case where one
user set is not fully covered by the alignment.",2307.02459v1,https://arxiv.org/pdf/2307.02459v1
"Robust Graph Structure Learning with the Alignment of Features and
  Adjacency Matrix","Shaogao Lv, Gang Wen, Shiyu Liu, Linsen Wei, Ming Li","To improve the robustness of graph neural networks (GNN), graph structure
learning (GSL) has attracted great interest due to the pervasiveness of noise
in graph data. Many approaches have been proposed for GSL to jointly learn a
clean graph structure and corresponding representations. To extend the previous
work, this paper proposes a novel regularized GSL approach, particularly with
an alignment of feature information and graph information, which is motivated
mainly by our derived lower bound of node-level Rademacher complexity for GNNs.
Additionally, our proposed approach incorporates sparse dimensional reduction
to leverage low-dimensional node features that are relevant to the graph
structure. To evaluate the effectiveness of our approach, we conduct
experiments on real-world graphs. The results demonstrate that our proposed GSL
method outperforms several competitive baselines, especially in scenarios where
the graph structures are heavily affected by noise. Overall, our research
highlights the importance of integrating feature and graph information
alignment in GSL, as inspired by our derived theoretical result, and showcases
the superiority of our approach in handling noisy graph structures through
comprehensive experiments on real-world datasets.",2307.02126v1,https://arxiv.org/pdf/2307.02126v1
"Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for
  Entity Alignment","Qijie Ding, Jie Yin, Daokun Zhang, Junbin Gao","Entity alignment (EA) aims at identifying equivalent entity pairs across
different knowledge graphs (KGs) that refer to the same real-world identity. To
systematically combat confirmation bias for pseudo-labeling-based entity
alignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment
(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the
accuracy of entity alignment. UPL-EA consists of two complementary components:
(1) The Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling
as an effective means to enable more accurate determination of entity
correspondences across two KGs and to mitigate the adverse impact of erroneous
matches. A simple but highly effective criterion is further devised to derive
pseudo-labeled entity pairs that satisfy one-to-one correspondences at each
iteration. (2) The cross-iteration pseudo-label calibration operates across
multiple consecutive iterations to further improve the pseudo-labeling
precision rate by reducing the local pseudo-label selection variability with a
theoretical guarantee. The two components are respectively designed to
eliminate Type I and Type II pseudo-labeling errors identified through our
analyse. The calibrated pseudo-labels are thereafter used to augment prior
alignment seeds to reinforce subsequent model training for alignment inference.
The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both
theoretically supported and experimentally validated. The experimental results
show that our approach achieves competitive performance with limited prior
alignment seeds.",2307.02075v1,https://arxiv.org/pdf/2307.02075v1
"Robots That Ask For Help: Uncertainty Alignment for Large Language Model
  Planners","Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, Anirudha Majumdar","Large language models (LLMs) exhibit a wide range of promising capabilities
-- from step-by-step planning to commonsense reasoning -- that may provide
utility for robots, but remain prone to confidently hallucinated predictions.
In this work, we present KnowNo, which is a framework for measuring and
aligning the uncertainty of LLM-based planners such that they know when they
don't know and ask for help when needed. KnowNo builds on the theory of
conformal prediction to provide statistical guarantees on task completion while
minimizing human help in complex multi-step planning settings. Experiments
across a variety of simulated and real robot setups that involve tasks with
different modes of ambiguity (e.g., from spatial to numeric uncertainties, from
human preferences to Winograd schemas) show that KnowNo performs favorably over
modern baselines (which may involve ensembles or extensive prompt tuning) in
terms of improving efficiency and autonomy, while providing formal assurances.
KnowNo can be used with LLMs out of the box without model-finetuning, and
suggests a promising lightweight approach to modeling uncertainty that can
complement and scale with the growing capabilities of foundation models.
Website: https://robot-help.github.io",2307.01928v2,https://arxiv.org/pdf/2307.01928v2
"Align With Purpose: Optimize Desired Properties in CTC Models with a
  General Plug-and-Play Framework","Eliya Segev, Maya Alroy, Ronen Katsir, Noam Wies, Ayana Shenhav, Yael Ben-Oren, David Zar, Oren Tadmor, Jacob Bitterman, Amnon Shashua, Tal Rosenwein","Connectionist Temporal Classification (CTC) is a widely used criterion for
training supervised sequence-to-sequence (seq2seq) models. It enables learning
the relations between input and output sequences, termed alignments, by
marginalizing over perfect alignments (that yield the ground truth), at the
expense of imperfect alignments. This binary differentiation of perfect and
imperfect alignments falls short of capturing other essential alignment
properties that hold significance in other real-world applications. Here we
propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play
framework}$ for enhancing a desired property in models trained with the CTC
criterion. We do that by complementing the CTC with an additional loss term
that prioritizes alignments according to a desired property. Our method does
not require any intervention in the CTC loss function, enables easy
optimization of a variety of properties, and allows differentiation between
both perfect and imperfect alignments. We apply our framework in the domain of
Automatic Speech Recognition (ASR) and show its generality in terms of property
selection, architectural choice, and scale of training dataset (up to 280,000
hours). To demonstrate the effectiveness of our framework, we apply it to two
unrelated properties: emission time and word error rate (WER). For the former,
we report an improvement of up to 570ms in latency optimization with a minor
reduction in WER, and for the latter, we report a relative improvement of 4.5%
WER over the baseline models. To the best of our knowledge, these applications
have never been demonstrated to work on a scale of data as large as ours.
Notably, our method can be implemented using only a few lines of code, and can
be extended to other alignment-free loss functions and to domains other than
ASR.",2307.01715v3,https://arxiv.org/pdf/2307.01715v3
"SCITUNE: Aligning Large Language Models with Scientific Multimodal
  Instructions","Sameera Horawalavithana, Sai Munikoti, Ian Stewart, Henry Kvinge","Instruction finetuning is a popular paradigm to align large language models
(LLM) with human intent. Despite its popularity, this idea is less explored in
improving the LLMs to align existing foundation models with scientific
disciplines, concepts and goals. In this work, we present SciTune as a tuning
framework to improve the ability of LLMs to follow scientific multimodal
instructions. To test our methodology, we use a human-generated scientific
instruction tuning dataset and train a large multimodal model LLaMA-SciTune
that connects a vision encoder and LLM for science-focused visual and language
understanding. In comparison to the models that are finetuned with machine
generated data only, LLaMA-SciTune surpasses human performance on average and
in many sub-categories on the ScienceQA benchmark.",2307.01139v1,https://arxiv.org/pdf/2307.01139v1
"Adaptive reinforcement learning of multi-agent ethically-aligned
  behaviours: the QSOM and QDSOM algorithms","Rémy Chaput, Olivier Boissier, Mathieu Guillermin","The numerous deployed Artificial Intelligence systems need to be aligned with
our ethical considerations. However, such ethical considerations might change
as time passes: our society is not fixed, and our social mores evolve. This
makes it difficult for these AI systems; in the Machine Ethics field
especially, it has remained an under-studied challenge. In this paper, we
present two algorithms, named QSOM and QDSOM, which are able to adapt to
changes in the environment, and especially in the reward function, which
represents the ethical considerations that we want these systems to be aligned
with. They associate the well-known Q-Table to (Dynamic) Self-Organizing Maps
to handle the continuous and multi-dimensional state and action spaces. We
evaluate them on a use-case of multi-agent energy repartition within a small
Smart Grid neighborhood, and prove their ability to adapt, and their higher
performance compared to baseline Reinforcement Learning algorithms.",2307.00552v1,https://arxiv.org/pdf/2307.00552v1
Preference Ranking Optimization for Human Alignment,"Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang","Large language models (LLMs) often contain misleading content, emphasizing
the need to align them with human values to ensure secure AI systems.
Reinforcement learning from human feedback (RLHF) has been employed to achieve
this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits
complexity, instability, and sensitivity to hyperparameters in contrast to SFT.
(2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise
contrast, thus lacking contrasts from a macro perspective. In this paper, we
propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to
directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast
to accommodate preference rankings of any length. By iteratively contrasting
candidates, PRO instructs the LLM to prioritize the best response while
progressively ranking the rest responses. In this manner, PRO effectively
transforms human alignment into aligning the probability ranking of n responses
generated by LLM with the preference ranking of humans towards these responses.
Experiments have shown that PRO outperforms baseline algorithms, achieving
comparable results to ChatGPT and human responses through automatic-based,
reward-based, GPT-4, and human evaluations.",2306.17492v2,https://arxiv.org/pdf/2306.17492v2
Asynchronous Algorithmic Alignment with Cocycles,"Andrew Dudzik, Tamara von Glehn, Razvan Pascanu, Petar Veličković","State-of-the-art neural algorithmic reasoners make use of message passing in
graph neural networks (GNNs). But typical GNNs blur the distinction between the
definition and invocation of the message function, forcing a node to send
messages to its neighbours at every layer, synchronously. When applying GNNs to
learn to execute dynamic programming algorithms, however, on most steps only a
handful of the nodes would have meaningful updates to send. One, hence, runs
the risk of inefficiencies by sending too much irrelevant data across the
graph. But more importantly, many intermediate GNN steps have to learn the
identity functions, which is a non-trivial learning problem. In this work, we
explicitly separate the concepts of node state update and message function
invocation. With this separation, we obtain a mathematical formulation that
allows us to reason about asynchronous computation in both algorithms and
neural networks. Our analysis yields several practical implementations of
synchronous scalable GNN layers that are provably invariant under various forms
of asynchrony.",2306.15632v3,https://arxiv.org/pdf/2306.15632v3
FAIRER: Fairness as Decision Rationale Alignment,"Tianlin Li, Qing Guo, Aishan Liu, Mengnan Du, Zhiming Li, Yang Liu","Deep neural networks (DNNs) have made significant progress, but often suffer
from fairness issues, as deep models typically show distinct accuracy
differences among certain subgroups (e.g., males and females). Existing
research addresses this critical issue by employing fairness-aware loss
functions to constrain the last-layer outputs and directly regularize DNNs.
Although the fairness of DNNs is improved, it is unclear how the trained
network makes a fair prediction, which limits future fairness improvements. In
this paper, we investigate fairness from the perspective of decision rationale
and define the parameter parity score to characterize the fair decision process
of networks by analyzing neuron influence in various subgroups. Extensive
empirical studies show that the unfair issue could arise from the unaligned
decision rationales of subgroups. Existing fairness regularization terms fail
to achieve decision rationale alignment because they only constrain last-layer
outputs while ignoring intermediate neuron alignment. To address the issue, we
formulate the fairness as a new task, i.e., decision rationale alignment that
requires DNNs' neurons to have consistent responses on subgroups at both
intermediate processes and the final prediction. To make this idea practical
during optimization, we relax the naive objective function and propose
gradient-guided parity alignment, which encourages gradient-weighted
consistency of neurons across subgroups. Extensive experiments on a variety of
datasets show that our method can significantly enhance fairness while
sustaining a high level of accuracy and outperforming other approaches by a
wide margin.",2306.15299v1,https://arxiv.org/pdf/2306.15299v1
Are aligned neural networks adversarially aligned?,"Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, Ludwig Schmidt","Large language models are now tuned to align with the goals of their
creators, namely to be ""helpful and harmless."" These models should respond
helpfully to user questions, but refuse to answer requests that could cause
harm. However, adversarial users can construct inputs which circumvent attempts
at alignment. In this work, we study adversarial alignment, and ask to what
extent these models remain aligned when interacting with an adversarial user
who constructs worst-case inputs (adversarial examples). These inputs are
designed to cause the model to emit harmful content that would otherwise be
prohibited. We show that existing NLP-based optimization attacks are
insufficiently powerful to reliably attack aligned text models: even when
current NLP-based attacks fail, we can find adversarial inputs with brute
force. As a result, the failure of current attacks should not be seen as proof
that aligned text models remain aligned under adversarial inputs.
  However the recent trend in large-scale ML models is multimodal models that
allow users to provide images that influence the text that is generated. We
show these models can be easily attacked, i.e., induced to perform arbitrary
un-aligned behavior through adversarial perturbation of the input image. We
conjecture that improved NLP attacks may demonstrate this same level of
adversarial control over text-only models.",2306.15447v2,https://arxiv.org/pdf/2306.15447v2
"Learning-to-Rank Meets Language: Boosting Language-Driven Ordering
  Alignment for Ordinal Classification","Rui Wang, Peipei Li, Huaibo Huang, Chunshui Cao, Ran He, Zhaofeng He","We present a novel language-driven ordering alignment method for ordinal
classification. The labels in ordinal classification contain additional
ordering relations, making them prone to overfitting when relying solely on
training data. Recent developments in pre-trained vision-language models
inspire us to leverage the rich ordinal priors in human language by converting
the original task into a visionlanguage alignment task. Consequently, we
propose L2RCLIP, which fully utilizes the language priors from two
perspectives. First, we introduce a complementary prompt tuning technique
called RankFormer, designed to enhance the ordering relation of original rank
prompts. It employs token-level attention with residual-style prompt blending
in the word embedding space. Second, to further incorporate language priors, we
revisit the approximate bound optimization of vanilla cross-entropy loss and
restructure it within the cross-modal embedding space. Consequently, we propose
a cross-modal ordinal pairwise loss to refine the CLIP feature space, where
texts and images maintain both semantic alignment and ordering alignment.
Extensive experiments on three ordinal classification tasks, including facial
age estimation, historical color image (HCI) classification, and aesthetic
assessment demonstrate its promising performance. The code is available at
https://github.com/raywang335/L2RCLIP.",2306.13856v3,https://arxiv.org/pdf/2306.13856v3
Visual Adversarial Examples Jailbreak Aligned Large Language Models,"Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, Prateek Mittal","Recently, there has been a surge of interest in integrating vision into Large
Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as
Flamingo and GPT-4. This paper sheds light on the security and safety
implications of this trend. First, we underscore that the continuous and
high-dimensional nature of the visual input makes it a weak link against
adversarial attacks, representing an expanded attack surface of
vision-integrated LLMs. Second, we highlight that the versatility of LLMs also
presents visual attackers with a wider array of achievable adversarial
objectives, extending the implications of security failures beyond mere
misclassification. As an illustration, we present a case study in which we
exploit visual adversarial examples to circumvent the safety guardrail of
aligned LLMs with integrated vision. Intriguingly, we discover that a single
visual adversarial example can universally jailbreak an aligned LLM, compelling
it to heed a wide range of harmful instructions that it otherwise would not)
and generate harmful content that transcends the narrow scope of a `few-shot'
derogatory corpus initially employed to optimize the adversarial example. Our
study underscores the escalating adversarial risks associated with the pursuit
of multimodality. Our findings also connect the long-studied adversarial
vulnerabilities of neural networks to the nascent field of AI alignment. The
presented attack suggests a fundamental adversarial challenge for AI alignment,
especially in light of the emerging trend toward multimodality in frontier
foundation models.",2306.13213v2,https://arxiv.org/pdf/2306.13213v2
"SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence
  Embeddings","Chao Yu, Wenhao Zhu, Chaoming Liu, Xiaoyu Zhang, Qiuhong zhai","The paradigm of pre-training followed by fine-tuning on downstream tasks has
become the mainstream method in natural language processing tasks. Although
pre-trained models have the advantage of generalization, their performance may
still vary significantly across different domain tasks. This is because the
data distribution in different domains varies. For example, the different parts
of the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy
married life' may have different impact for downstream tasks. For similarity
calculations, words such as 'led' and 'life' are more important. On the other
hand, for sentiment analysis, the word 'happy' is crucial. This indicates that
different downstream tasks have different levels of sensitivity to sentence
components. Our starting point is to scale information of the model and data
according to the specifics of downstream tasks, enhancing domain information of
relevant parts for these tasks and reducing irrelevant elements for different
domain tasks, called SIFTER. In the experimental part, we use the SIFTER to
improve SimCSE by constructing positive sample pairs based on enhancing the
sentence stem and reducing the unimportant components in the sentence, and
maximize the similarity between three sentences. Similarly, SIFTER can improve
the gate mechanism of the LSTM model by short-circuiting the input gate of
important words so that the LSTM model remembers the important parts of the
sentence. Our experiments demonstrate that SIFTER outperforms the SimCSE and
LSTM baselines.",2306.12280v1,https://arxiv.org/pdf/2306.12280v1
"Personalized Federated Learning with Feature Alignment and Classifier
  Collaboration","Jian Xu, Xinyi Tong, Shao-Lun Huang","Data heterogeneity is one of the most challenging issues in federated
learning, which motivates a variety of approaches to learn personalized models
for participating clients. One such approach in deep neural networks based
tasks is employing a shared feature representation and learning a customized
classifier head for each client. However, previous works do not utilize the
global knowledge during local representation learning and also neglect the
fine-grained collaboration between local classifier heads, which limit the
model generalization ability. In this work, we conduct explicit local-global
feature alignment by leveraging global semantic knowledge for learning a better
representation. Moreover, we quantify the benefit of classifier combination for
each client as a function of the combining weights and derive an optimization
problem for estimating optimal weights. Finally, extensive evaluation results
on benchmark datasets with various heterogeneous data scenarios demonstrate the
effectiveness of our proposed method. Code is available at
https://github.com/JianXu95/FedPAC",2306.11867v1,https://arxiv.org/pdf/2306.11867v1
"BayLing: Bridging Cross-lingual Alignment and Instruction Following
  through Interactive Translation for Large Language Models","Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, Yang Feng","Large language models (LLMs) have demonstrated remarkable prowess in language
understanding and generation. Advancing from foundation LLMs to
instructionfollowing LLMs, instruction tuning plays a vital role in aligning
LLMs to human preferences. However, the existing LLMs are usually focused on
English, leading to inferior performance in non-English languages. In order to
improve the performance for non-English languages, it is necessary to collect
language-specific training data for foundation LLMs and construct
language-specific instructions for instruction tuning, both of which are heavy
loads. To minimize human workload, we propose to transfer the capabilities of
language generation and instruction following from English to other languages
through an interactive translation task. We have developed BayLing, an
instruction-following LLM by utilizing LLaMA as the foundation LLM and
automatically constructing interactive translation instructions for instructing
tuning. Extensive assessments demonstrate that BayLing achieves comparable
performance to GPT-3.5-turbo, despite utilizing a considerably smaller
parameter size of only 13 billion. Experimental results on translation tasks
show that BayLing achieves 95% of single-turn translation capability compared
to GPT-4 with automatic evaluation and 96% of interactive translation
capability compared to GPT-3.5-turbo with human evaluation. To estimate the
performance on general tasks, we created a multi-turn instruction test set
called BayLing-80. The experimental results on BayLing-80 indicate that BayLing
achieves 89% of performance compared to GPT-3.5-turbo. BayLing also
demonstrates outstanding performance on knowledge assessment of Chinese GaoKao
and English SAT, second only to GPT-3.5-turbo among a multitude of
instruction-following LLMs. Demo, homepage, code and models of BayLing are
available.",2306.10968v2,https://arxiv.org/pdf/2306.10968v2
"Aligning Synthetic Medical Images with Clinical Knowledge using Human
  Feedback","Shenghuan Sun, Gregory M. Goldgof, Atul Butte, Ahmed M. Alaa","Generative models capable of capturing nuanced clinical features in medical
images hold great promise for facilitating clinical data sharing, enhancing
rare disease datasets, and efficiently synthesizing annotated medical images at
scale. Despite their potential, assessing the quality of synthetic medical
images remains a challenge. While modern generative models can synthesize
visually-realistic medical images, the clinical validity of these images may be
called into question. Domain-agnostic scores, such as FID score, precision, and
recall, cannot incorporate clinical knowledge and are, therefore, not suitable
for assessing clinical sensibility. Additionally, there are numerous
unpredictable ways in which generative models may fail to synthesize clinically
plausible images, making it challenging to anticipate potential failures and
manually design scores for their detection. To address these challenges, this
paper introduces a pathologist-in-the-loop framework for generating
clinically-plausible synthetic medical images. Starting with a diffusion model
pretrained using real images, our framework comprises three steps: (1)
evaluating the generated images by expert pathologists to assess whether they
satisfy clinical desiderata, (2) training a reward model that predicts the
pathologist feedback on new samples, and (3) incorporating expert knowledge
into the diffusion model by using the reward model to inform a finetuning
objective. We show that human feedback significantly improves the quality of
synthetic images in terms of fidelity, diversity, utility in downstream
applications, and plausibility as evaluated by experts.",2306.12438v1,https://arxiv.org/pdf/2306.12438v1
CANDID: Correspondence AligNment for Deep-burst Image Denoising,"Arijit Mallick, Raphael Braun, Hendrik PA Lensch","With the advent of mobile phone photography and point-and-shoot cameras,
deep-burst imaging is widely used for a number of photographic effects such as
depth of field, super-resolution, motion deblurring, and image denoising. In
this work, we propose to solve the problem of deep-burst image denoising by
including an optical flow-based correspondence estimation module which aligns
all the input burst images with respect to a reference frame. In order to deal
with varying noise levels the individual burst images are pre-filtered with
different settings. Exploiting the established correspondences one network
block predicts a pixel-wise spatially-varying filter kernel to smooth each
image in the original and prefiltered bursts before fusing all images to
generate the final denoised output. The resulting pipeline achieves
state-of-the-art results by combining all available information provided by the
burst.",2306.09887v1,https://arxiv.org/pdf/2306.09887v1
"CoverHunter: Cover Song Identification with Refined Attention and
  Alignments","Feng Liu, Deyi Tuo, Yinan Xu, Xintong Han","Abstract: Cover song identification (CSI) focuses on finding the same music
with different versions in reference anchors given a query track. In this
paper, we propose a novel system named CoverHunter that overcomes the
shortcomings of existing detection schemes by exploring richer features with
refined attention and alignments. CoverHunter contains three key modules: 1) A
convolution-augmented transformer (i.e., Conformer) structure that captures
both local and global feature interactions in contrast to previous methods
mainly relying on convolutional neural networks; 2) An attention-based time
pooling module that further exploits the attention in the time dimension; 3) A
novel coarse-to-fine training scheme that first trains a network to roughly
align the song chunks and then refines the network by training on the aligned
chunks. At the same time, we also summarize some important training tricks used
in our system that help achieve better results. Experiments on several standard
CSI datasets show that our method significantly improves over state-of-the-art
methods with an embedding size of 128 (2.3% on SHS100K-TEST and 17.7% on
DaTacos).",2306.09025v1,https://arxiv.org/pdf/2306.09025v1
"Language Aligned Visual Representations Predict Human Behavior in
  Naturalistic Learning Tasks","Can Demircan, Tankred Saanum, Leonardo Pettini, Marcel Binz, Blazej M Baczkowski, Paula Kaanders, Christian F Doeller, Mona M Garvert, Eric Schulz","Humans possess the ability to identify and generalize relevant features of
natural objects, which aids them in various situations. To investigate this
phenomenon and determine the most effective representations for predicting
human behavior, we conducted two experiments involving category learning and
reward learning. Our experiments used realistic images as stimuli, and
participants were tasked with making accurate decisions based on novel stimuli
for all trials, thereby necessitating generalization. In both tasks, the
underlying rules were generated as simple linear functions using stimulus
dimensions extracted from human similarity judgments. Notably, participants
successfully identified the relevant stimulus features within a few trials,
demonstrating effective generalization. We performed an extensive model
comparison, evaluating the trial-by-trial predictive accuracy of diverse deep
learning models' representations of human choices. Intriguingly,
representations from models trained on both text and image data consistently
outperformed models trained solely on images, even surpassing models using the
features that generated the task itself. These findings suggest that
language-aligned visual representations possess sufficient richness to describe
human generalization in naturalistic settings and emphasize the role of
language in shaping human cognition.",2306.09377v1,https://arxiv.org/pdf/2306.09377v1
"Unsupervised speech intelligibility assessment with utterance level
  alignment distance between teacher and learner Wav2Vec-2.0 representations","Nayan Anand, Meenakshi Sirigiraju, Chiranjeevi Yarra","Speech intelligibility is crucial in language learning for effective
communication. Thus, to develop computer-assisted language learning systems,
automatic speech intelligibility detection (SID) is necessary. Most of the
works have assessed the intelligibility in a supervised manner considering
manual annotations, which requires cost and time; hence scalability is limited.
To overcome these, this work proposes an unsupervised approach for SID. The
proposed approach considers alignment distance computed with dynamic-time
warping (DTW) between teacher and learner representation sequence as a measure
to separate intelligible versus non-intelligible speech. We obtain the feature
sequence using current state-of-the-art self-supervised representations from
Wav2Vec-2.0. We found the detection accuracies as 90.37\%, 92.57\% and 96.58\%,
respectively, with three alignment distance measures -- mean absolute error,
mean squared error and cosine distance (equal to one minus cosine similarity).",2306.08845v1,https://arxiv.org/pdf/2306.08845v1
"Reinforcement Learning-Driven Linker Design via Fast Attention-based
  Point Cloud Alignment","Rebecca M. Neeser, Mehmet Akdel, Daniel Kovtun, Luca Naef","Proteolysis-Targeting Chimeras (PROTACs) represent a novel class of small
molecules which are designed to act as a bridge between an E3 ligase and a
disease-relevant protein, thereby promoting its subsequent degradation. PROTACs
are composed of two protein binding ""active"" domains, linked by a ""linker""
domain. The design of the linker domain is challenging due to geometric and
chemical constraints given by its interactions, and the need to maximize
drug-likeness. To tackle these challenges, we introduce ShapeLinker, a method
for de novo design of linkers. It performs fragment-linking using reinforcement
learning on an autoregressive SMILES generator. The method optimizes for a
composite score combining relevant physicochemical properties and a novel,
attention-based point cloud alignment score. This new method successfully
generates linkers that satisfy both relevant 2D and 3D requirements, and
achieves state-of-the-art results in producing novel linkers assuming a target
linker conformation. This allows for more rational and efficient PROTAC design
and optimization. Code and data are available at
https://github.com/aivant/ShapeLinker.",2306.08166v1,https://arxiv.org/pdf/2306.08166v1
"Contrastive Learning-Based Audio to Lyrics Alignment for Multiple
  Languages","Simon Durand, Daniel Stoller, Sebastian Ewert","Lyrics alignment gained considerable attention in recent years.
State-of-the-art systems either re-use established speech recognition toolkits,
or design end-to-end solutions involving a Connectionist Temporal
Classification (CTC) loss. However, both approaches suffer from specific
weaknesses: toolkits are known for their complexity, and CTC systems use a loss
designed for transcription which can limit alignment accuracy. In this paper,
we use instead a contrastive learning procedure that derives cross-modal
embeddings linking the audio and text domains. This way, we obtain a novel
system that is simple to train end-to-end, can make use of weakly annotated
training data, jointly learns a powerful text model, and is tailored to
alignment. The system is not only the first to yield an average absolute error
below 0.2 seconds on the standard Jamendo dataset but it is also robust to
other languages, even when trained on English data only. Finally, we release
word-level alignments for the JamendoLyrics Multi-Lang dataset.",2306.07744v1,https://arxiv.org/pdf/2306.07744v1
Video-to-Music Recommendation using Temporal Alignment of Segments,"Laure Prétet, Gaël Richard, Clément Souchier, Geoffroy Peeters","We study cross-modal recommendation of music tracks to be used as soundtracks
for videos. This problem is known as the music supervision task. We build on a
self-supervised system that learns a content association between music and
video. In addition to the adequacy of content, adequacy of structure is crucial
in music supervision to obtain relevant recommendations. We propose a novel
approach to significantly improve the system's performance using
structure-aware recommendation. The core idea is to consider not only the full
audio-video clips, but rather shorter segments for training and inference. We
find that using semantic segments and ranking the tracks according to sequence
alignment costs significantly improves the results. We investigate the impact
of different ranking metrics and segmentation methods.",2306.07187v1,https://arxiv.org/pdf/2306.07187v1
Online Prototype Alignment for Few-shot Policy Transfer,"Qi Yi, Rui Zhang, Shaohui Peng, Jiaming Guo, Yunkai Gao, Kaizhao Yuan, Ruizhi Chen, Siming Lan, Xing Hu, Zidong Du, Xishan Zhang, Qi Guo, Yunji Chen","Domain adaptation in reinforcement learning (RL) mainly deals with the
changes of observation when transferring the policy to a new environment. Many
traditional approaches of domain adaptation in RL manage to learn a mapping
function between the source and target domain in explicit or implicit ways.
However, they typically require access to abundant data from the target domain.
Besides, they often rely on visual clues to learn the mapping function and may
fail when the source domain looks quite different from the target domain. To
address these problems, we propose a novel framework Online Prototype Alignment
(OPA) to learn the mapping function based on the functional similarity of
elements and is able to achieve the few-shot policy transfer within only
several episodes. The key insight of OPA is to introduce an exploration
mechanism that can interact with the unseen elements of the target domain in an
efficient and purposeful manner, and then connect them with the seen elements
in the source domain according to their functionalities (instead of visual
clues). Experimental results show that when the target domain looks visually
different from the source domain, OPA can achieve better transfer performance
even with much fewer samples from the target domain, outperforming prior
methods.",2306.07307v1,https://arxiv.org/pdf/2306.07307v1
Graph Mixup with Soft Alignments,"Hongyi Ling, Zhimeng Jiang, Meng Liu, Shuiwang Ji, Na Zou","We study graph data augmentation by mixup, which has been used successfully
on images. A key operation of mixup is to compute a convex combination of a
pair of inputs. This operation is straightforward for grid-like data, such as
images, but challenging for graph data. The key difficulty lies in the fact
that different graphs typically have different numbers of nodes, and thus there
lacks a node-level correspondence between graphs. In this work, we propose
S-Mixup, a simple yet effective mixup method for graph classification by soft
alignments. Specifically, given a pair of graphs, we explicitly obtain
node-level correspondence via computing a soft assignment matrix to match the
nodes between two graphs. Based on the soft assignments, we transform the
adjacency and node feature matrices of one graph, so that the transformed graph
is aligned with the other graph. In this way, any pair of graphs can be mixed
directly to generate an augmented graph. We conduct systematic experiments to
show that S-Mixup can improve the performance and generalization of graph
neural networks (GNNs) on various graph classification tasks. In addition, we
show that S-Mixup can increase the robustness of GNNs against noisy labels.",2306.06788v1,https://arxiv.org/pdf/2306.06788v1
"Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics
  Alignment with Diffusion Models","Yule Wang, Zijing Wu, Chengrui Li, Anqi Wu","In the field of behavior-related brain computation, it is necessary to align
raw neural signals against the drastic domain shift among them. A foundational
framework within neuroscience research posits that trial-based neural
population activities rely on low-dimensional latent dynamics, thus focusing on
the latter greatly facilitates the alignment procedure. Despite this field's
progress, existing methods ignore the intrinsic spatio-temporal structure
during the alignment phase. Hence, their solutions usually lead to poor quality
in latent dynamics structures and overall performance. To tackle this problem,
we propose an alignment method ERDiff, which leverages the expressivity of the
diffusion model to preserve the spatio-temporal structure of latent dynamics.
Specifically, the latent dynamics structures of the source domain are first
extracted by a diffusion model. Then, under the guidance of this diffusion
model, such structures are well-recovered through a maximum likelihood
alignment procedure in the target domain. We first demonstrate the
effectiveness of our proposed method on a synthetic dataset. Then, when applied
to neural recordings from the non-human primate motor cortex, under both
cross-day and inter-subject settings, our method consistently manifests its
capability of preserving the spatiotemporal structure of latent dynamics and
outperforms existing approaches in alignment goodness-of-fit and neural
decoding performance.",2306.06138v2,https://arxiv.org/pdf/2306.06138v2
"Flexible Distribution Alignment: Towards Long-tailed Semi-supervised
  Learning with Proper Calibration","Emanuel Sanchez Aimar, Nathaniel Helgesen, Yonghao Xu, Marco Kuhlmann, Michael Felsberg","Long-tailed semi-supervised learning (LTSSL) represents a practical scenario
for semi-supervised applications, challenged by skewed labeled distributions
that bias classifiers. This problem is often aggravated by discrepancies
between labeled and unlabeled class distributions, leading to biased
pseudo-labels, neglect of rare classes, and poorly calibrated probabilities. To
address these issues, we introduce Flexible Distribution Alignment (FlexDA), a
novel adaptive logit-adjusted loss framework designed to dynamically estimate
and align predictions with the actual distribution of unlabeled data and
achieve a balanced classifier by the end of training. FlexDA is further
enhanced by a distillation-based consistency loss, promoting fair data usage
across classes and effectively leveraging underconfident samples. This method,
encapsulated in ADELLO (Align and Distill Everything All at Once), proves
robust against label shift, significantly improves model calibration in LTSSL
contexts, and surpasses previous state-of-of-art approaches across multiple
benchmarks, including CIFAR100-LT, STL10-LT, and ImageNet127, addressing class
imbalance challenges in semi-supervised learning. Our code is available at
https://github.com/emasa/ADELLO-LTSSL.",2306.04621v3,https://arxiv.org/pdf/2306.04621v3
"Rewarded soups: towards Pareto-optimal alignment by interpolating
  weights fine-tuned on diverse rewards","Alexandre Ramé, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, Matthieu Cord","Foundation models are first pre-trained on vast unsupervised datasets and
then fine-tuned on labeled data. Reinforcement learning, notably from human
feedback (RLHF), can further align the network with the intended usage. Yet the
imperfections in the proxy reward may hinder the training and lead to
suboptimal results; the diversity of objectives in real-world tasks and human
opinions exacerbate the issue. This paper proposes embracing the heterogeneity
of diverse rewards by following a multi-policy strategy. Rather than focusing
on a single a priori reward, we aim for Pareto-optimal generalization across
the entire space of preferences. To this end, we propose rewarded soup, first
specializing multiple networks independently (one for each proxy reward) and
then interpolating their weights linearly. This succeeds empirically because we
show that the weights remain linearly connected when fine-tuned on diverse
rewards from a shared pre-trained initialization. We demonstrate the
effectiveness of our approach for text-to-text (summarization, Q&A, helpful
assistant, review), text-image (image captioning, text-to-image generation,
visual grounding, VQA), and control (locomotion) tasks. We hope to enhance the
alignment of deep models, and how they interact with the world in all its
diversity.",2306.04488v2,https://arxiv.org/pdf/2306.04488v2
Unbalanced Optimal Transport for Unbalanced Word Alignment,"Yuki Arase, Han Bao, Sho Yokoi","Monolingual word alignment is crucial to model semantic interactions between
sentences. In particular, null alignment, a phenomenon in which words have no
corresponding counterparts, is pervasive and critical in handling semantically
divergent sentences. Identification of null alignment is useful on its own to
reason about the semantic similarity of sentences by indicating there exists
information inequality. To achieve unbalanced word alignment that values both
alignment and null alignment, this study shows that the family of optimal
transport (OT), i.e., balanced, partial, and unbalanced OT, are natural and
powerful approaches even without tailor-made techniques. Our extensive
experiments covering unsupervised and supervised settings indicate that our
generic OT-based alignment methods are competitive against the
state-of-the-arts specially designed for word alignment, remarkably on
challenging datasets with high null alignment frequencies.",2306.04116v1,https://arxiv.org/pdf/2306.04116v1
"PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward
  Learning for Robotic Manipulation","Runze Liu, Yali Du, Fengshuo Bai, Jiafei Lyu, Xiu Li","In preference-based Reinforcement Learning (RL), obtaining a large number of
preference labels are both time-consuming and costly. Furthermore, the queried
human preferences cannot be utilized for the new tasks. In this paper, we
propose Zero-shot Cross-task Preference Alignment and Robust Reward Learning
(PEARL), which learns policies from cross-task preference transfer without any
human labels of the target task. Our contributions include two novel components
that facilitate the transfer and learning process. The first is Cross-task
Preference Alignment (CPA), which transfers the preferences between tasks via
optimal transport. The key idea of CPA is to use Gromov-Wasserstein distance to
align the trajectories between tasks, and the solved optimal transport matrix
serves as the correspondence between trajectories. The target task preferences
are computed as the weighted sum of source task preference labels with the
correspondence as weights. Moreover, to ensure robust learning from these
transferred labels, we introduce Robust Reward Learning (RRL), which considers
both reward mean and uncertainty by modeling rewards as Gaussian distributions.
Empirical results on robotic manipulation tasks from Meta-World and Robomimic
demonstrate that our method is capable of transferring preference labels across
tasks accurately and then learns well-behaved policies. Notably, our approach
significantly exceeds existing methods when there are few human preferences.
The code and videos of our method are available at:
https://sites.google.com/view/pearl-preference.",2306.03615v2,https://arxiv.org/pdf/2306.03615v2
"Adversarial alignment: Breaking the trade-off between the strength of an
  attack and its relevance to human perception","Drew Linsley, Pinyuan Feng, Thibaut Boissin, Alekh Karkada Ashok, Thomas Fel, Stephanie Olaiya, Thomas Serre","Deep neural networks (DNNs) are known to have a fundamental sensitivity to
adversarial attacks, perturbations of the input that are imperceptible to
humans yet powerful enough to change the visual decision of a model.
Adversarial attacks have long been considered the ""Achilles' heel"" of deep
learning, which may eventually force a shift in modeling paradigms.
Nevertheless, the formidable capabilities of modern large-scale DNNs have
somewhat eclipsed these early concerns. Do adversarial attacks continue to pose
a threat to DNNs?
  Here, we investigate how the robustness of DNNs to adversarial attacks has
evolved as their accuracy on ImageNet has continued to improve. We measure
adversarial robustness in two different ways: First, we measure the smallest
adversarial attack needed to cause a model to change its object categorization
decision. Second, we measure how aligned successful attacks are with the
features that humans find diagnostic for object recognition. We find that
adversarial attacks are inducing bigger and more easily detectable changes to
image pixels as DNNs grow better on ImageNet, but these attacks are also
becoming less aligned with features that humans find diagnostic for
recognition. To better understand the source of this trade-off, we turn to the
neural harmonizer, a DNN training routine that encourages models to leverage
the same features as humans to solve tasks. Harmonized DNNs achieve the best of
both worlds and experience attacks that are detectable and affect features that
humans find diagnostic for recognition, meaning that attacks on these models
are more likely to be rendered ineffective by inducing similar effects on human
perception. Our findings suggest that the sensitivity of DNNs to adversarial
attacks can be mitigated by DNN scale, data scale, and training routines that
align models with biological intelligence.",2306.03229v1,https://arxiv.org/pdf/2306.03229v1
Optimal transport for automatic alignment of untargeted metabolomic data,"Marie Breeur, George Stepaniants, Pekka Keski-Rahkonen, Philippe Rigollet, Vivian Viallon","Untargeted metabolomic profiling through liquid chromatography-mass
spectrometry (LC-MS) measures a vast array of metabolites within biospecimens,
advancing drug development, disease diagnosis, and risk prediction. However,
the low throughput of LC-MS poses a major challenge for biomarker discovery,
annotation, and experimental comparison, necessitating the merging of multiple
datasets. Current data pooling methods encounter practical limitations due to
their vulnerability to data variations and hyperparameter dependence. Here we
introduce GromovMatcher, a flexible and user-friendly algorithm that
automatically combines LC-MS datasets using optimal transport. By capitalizing
on feature intensity correlation structures, GromovMatcher delivers superior
alignment accuracy and robustness compared to existing approaches. This
algorithm scales to thousands of features requiring minimal hyperparameter
tuning. Manually curated datasets for validating alignment algorithms are
limited in the field of untargeted metabolomics, and hence we develop a dataset
split procedure to generate pairs of validation datasets to test the alignments
produced by GromovMatcher and other methods. Applying our method to
experimental patient studies of liver and pancreatic cancer, we discover shared
metabolic features related to patient alcohol intake, demonstrating how
GromovMatcher facilitates the search for biomarkers associated with lifestyle
risk factors linked to several cancer types.",2306.03218v4,https://arxiv.org/pdf/2306.03218v4
"Exploring the Relationship between Alignment and Cross-lingual Transfer
  in Multilingual Transformers","Félix Gaschi, Patricio Cerda, Parisa Rastin, Yannick Toussaint","Without any explicit cross-lingual training data, multilingual language
models can achieve cross-lingual transfer. One common way to improve this
transfer is to perform realignment steps before fine-tuning, i.e., to train the
model to build similar representations for pairs of words from translated
sentences. But such realignment methods were found to not always improve
results across languages and tasks, which raises the question of whether
aligned representations are truly beneficial for cross-lingual transfer. We
provide evidence that alignment is actually significantly correlated with
cross-lingual transfer across languages, models and random seeds. We show that
fine-tuning can have a significant impact on alignment, depending mainly on the
downstream task and the model. Finally, we show that realignment can, in some
instances, improve cross-lingual transfer, and we identify conditions in which
realignment methods provide significant improvements. Namely, we find that
realignment works better on tasks for which alignment is correlated with
cross-lingual transfer when generalizing to a distant language and with smaller
models, as well as when using a bilingual dictionary rather than FastAlign to
extract realignment pairs. For example, for POS-tagging, between English and
Arabic, realignment can bring a +15.8 accuracy improvement on distilmBERT, even
outperforming XLM-R Large by 1.7. We thus advocate for further research on
realignment methods for smaller multilingual models as an alternative to
scaling.",2306.02790v1,https://arxiv.org/pdf/2306.02790v1
"Query Encoder Distillation via Embedding Alignment is a Strong Baseline
  Method to Boost Dense Retriever Online Efficiency","Yuxuan Wang, Hong Lyu","The information retrieval community has made significant progress in
improving the efficiency of Dual Encoder (DE) dense passage retrieval systems,
making them suitable for latency-sensitive settings. However, many proposed
procedures are often too complex or resource-intensive, which makes it
difficult for practitioners to adopt them or identify sources of empirical
gains. Therefore, in this work, we propose a trivially simple recipe to serve
as a baseline method for boosting the efficiency of DE retrievers leveraging an
asymmetric architecture. Our results demonstrate that even a 2-layer,
BERT-based query encoder can still retain 92.5% of the full DE performance on
the BEIR benchmark via unsupervised distillation and proper student
initialization. We hope that our findings will encourage the community to
re-evaluate the trade-offs between method complexity and performance
improvements.",2306.11550v1,https://arxiv.org/pdf/2306.11550v1
"Random Feedback Alignment Algorithms to train Neural Networks: Why do
  they Align?","Dominique Chu, Florian Bacho","Feedback alignment algorithms are an alternative to backpropagation to train
neural networks, whereby some of the partial derivatives that are required to
compute the gradient are replaced by random terms. This essentially transforms
the update rule into a random walk in weight space. Surprisingly, learning
still works with those algorithms, including training of deep neural networks.
This is generally attributed to an alignment of the update of the random walker
with the true gradient - the eponymous gradient alignment -- which drives an
approximate gradient descend. The mechanism that leads to this alignment
remains unclear, however. In this paper, we use mathematical reasoning and
simulations to investigate gradient alignment. We observe that the feedback
alignment update rule has fixed points, which correspond to extrema of the loss
function. We show that gradient alignment is a stability criterion for those
fixed points. It is only a necessary criterion for algorithm performance.
Experimentally, we demonstrate that high levels of gradient alignment can lead
to poor algorithm performance and that the alignment is not always driving the
gradient descend.",2306.02325v1,https://arxiv.org/pdf/2306.02325v1
Fine-Tuning Language Models with Advantage-Induced Policy Alignment,"Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I. Jordan, Jiantao Jiao","Reinforcement learning from human feedback (RLHF) has emerged as a reliable
approach to aligning large language models (LLMs) to human preferences. Among
the plethora of RLHF techniques, proximal policy optimization (PPO) is of the
most widely used methods. Despite its popularity, however, PPO may suffer from
mode collapse, instability, and poor sample efficiency. We show that these
issues can be alleviated by a novel algorithm that we refer to as
Advantage-Induced Policy Alignment (APA), which leverages a squared error loss
function based on the estimated advantages. We demonstrate empirically that APA
consistently outperforms PPO in language tasks by a large margin, when a
separate reward model is employed as the evaluator. In addition, compared with
PPO, APA offers a more stable form of control over the deviation from the
model's initial policy, ensuring that the model improves its performance
without collapsing to deterministic output. In addition to empirical results,
we also provide a theoretical justification supporting the design of our loss
function.",2306.02231v3,https://arxiv.org/pdf/2306.02231v3
"Implicit Regularization in Feedback Alignment Learning Mechanisms for
  Neural Networks","Zachary Robertson, Oluwasanmi Koyejo","Feedback Alignment (FA) methods are biologically inspired local learning
rules for training neural networks with reduced communication between layers.
While FA has potential applications in distributed and privacy-aware ML,
limitations in multi-class classification and lack of theoretical understanding
of the alignment mechanism have constrained its impact. This study introduces a
unified framework elucidating the operational principles behind alignment in
FA. Our key contributions include: (1) a novel conservation law linking changes
in synaptic weights to implicit regularization that maintains alignment with
the gradient, with support from experiments, (2) sufficient conditions for
convergence based on the concept of alignment dominance, and (3) empirical
analysis showing better alignment can enhance FA performance on complex
multi-class tasks. Overall, these theoretical and practical advancements
improve interpretability of bio-plausible learning rules and provide groundwork
for developing enhanced FA algorithms.",2306.01870v2,https://arxiv.org/pdf/2306.01870v2
"Enhancing the Protein Tertiary Structure Prediction by Multiple Sequence
  Alignment Generation","Le Zhang, Jiayang Chen, Tao Shen, Yu Li, Siqi Sun","The field of protein folding research has been greatly advanced by deep
learning methods, with AlphaFold2 (AF2) demonstrating exceptional performance
and atomic-level precision. As co-evolution is integral to protein structure
prediction, AF2's accuracy is significantly influenced by the depth of multiple
sequence alignment (MSA), which requires extensive exploration of a large
protein database for similar sequences. However, not all protein sequences
possess abundant homologous families, and consequently, AF2's performance can
degrade on such queries, at times failing to produce meaningful results. To
address this, we introduce a novel generative language model, MSA-Augmenter,
which leverages protein-specific attention mechanisms and large-scale MSAs to
generate useful, novel protein sequences not currently found in databases.
These sequences supplement shallow MSAs, enhancing the accuracy of structural
property predictions. Our experiments on CASP14 demonstrate that MSA-Augmenter
can generate de novo sequences that retain co-evolutionary information from
inferior MSAs, thereby improving protein structure prediction quality on top of
strong AF2.",2306.01824v1,https://arxiv.org/pdf/2306.01824v1
"Addressing Discrepancies in Semantic and Visual Alignment in Neural
  Networks","Natalie Abreu, Nathan Vaska, Victoria Helus","For the task of image classification, neural networks primarily rely on
visual patterns. In robust networks, we would expect for visually similar
classes to be represented similarly. We consider the problem of when
semantically similar classes are visually dissimilar, and when visual
similarity is present among non-similar classes. We propose a data augmentation
technique with the goal of better aligning semantically similar classes with
arbitrary (non-visual) semantic relationships. We leverage recent work in
diffusion-based semantic mixing to generate semantic hybrids of two classes,
and these hybrids are added to the training set as augmented data. We evaluate
whether the method increases semantic alignment by evaluating model performance
on adversarially perturbed data, with the idea that it should be easier for an
adversary to switch one class to a similarly represented class. Results
demonstrate that there is an increase in alignment of semantically similar
classes when using our proposed data augmentation method.",2306.01148v1,https://arxiv.org/pdf/2306.01148v1
"Second Sight: Using brain-optimized encoding models to align image
  distributions with human brain activity","Reese Kneeland, Jordyn Ojeda, Ghislain St-Yves, Thomas Naselaris","Two recent developments have accelerated progress in image reconstruction
from human brain activity: large datasets that offer samples of brain activity
in response to many thousands of natural scenes, and the open-sourcing of
powerful stochastic image-generators that accept both low- and high-level
guidance. Most work in this space has focused on obtaining point estimates of
the target image, with the ultimate goal of approximating literal pixel-wise
reconstructions of target images from the brain activity patterns they evoke.
This emphasis belies the fact that there is always a family of images that are
equally compatible with any evoked brain activity pattern, and the fact that
many image-generators are inherently stochastic and do not by themselves offer
a method for selecting the single best reconstruction from among the samples
they generate. We introduce a novel reconstruction procedure (Second Sight)
that iteratively refines an image distribution to explicitly maximize the
alignment between the predictions of a voxel-wise encoding model and the brain
activity patterns evoked by any target image. We show that our process
converges on a distribution of high-quality reconstructions by refining both
semantic content and low-level image details across iterations. Images sampled
from these converged image distributions are competitive with state-of-the-art
reconstruction algorithms. Interestingly, the time-to-convergence varies
systematically across visual cortex, with earlier visual areas generally taking
longer and converging on narrower image distributions, relative to higher-level
brain areas. Second Sight thus offers a succinct and novel method for exploring
the diversity of representations across visual brain areas.",2306.00927v1,https://arxiv.org/pdf/2306.00927v1
"FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses
  via Pixel-Aligned Scene Flow","Cameron Smith, Yilun Du, Ayush Tewari, Vincent Sitzmann","Reconstruction of 3D neural fields from posed images has emerged as a
promising method for self-supervised representation learning. The key challenge
preventing the deployment of these 3D scene learners on large-scale video data
is their dependence on precise camera poses from structure-from-motion, which
is prohibitively expensive to run at scale. We propose a method that jointly
reconstructs camera poses and 3D neural scene representations online and in a
single forward pass. We estimate poses by first lifting frame-to-frame optical
flow to 3D scene flow via differentiable rendering, preserving locality and
shift-equivariance of the image processing backbone. SE(3) camera pose
estimation is then performed via a weighted least-squares fit to the scene flow
field. This formulation enables us to jointly supervise pose estimation and a
generalizable neural scene representation via re-rendering the input video, and
thus, train end-to-end and fully self-supervised on real-world video datasets.
We demonstrate that our method performs robustly on diverse, real-world video,
notably on sequences traditionally challenging to optimization-based pose
estimation techniques.",2306.00180v1,https://arxiv.org/pdf/2306.00180v1
Human-Aligned Calibration for AI-Assisted Decision Making,"Nina L. Corvelo Benz, Manuel Gomez Rodriguez","Whenever a binary classifier is used to provide decision support, it
typically provides both a label prediction and a confidence value. Then, the
decision maker is supposed to use the confidence value to calibrate how much to
trust the prediction. In this context, it has been often argued that the
confidence value should correspond to a well calibrated estimate of the
probability that the predicted label matches the ground truth label. However,
multiple lines of empirical evidence suggest that decision makers have
difficulties at developing a good sense on when to trust a prediction using
these confidence values. In this paper, our goal is first to understand why and
then investigate how to construct more useful confidence values. We first argue
that, for a broad class of utility functions, there exist data distributions
for which a rational decision maker is, in general, unlikely to discover the
optimal decision policy using the above confidence values -- an optimal
decision maker would need to sometimes place more (less) trust on predictions
with lower (higher) confidence values. However, we then show that, if the
confidence values satisfy a natural alignment property with respect to the
decision maker's confidence on her own predictions, there always exists an
optimal decision policy under which the level of trust the decision maker would
need to place on predictions is monotone on the confidence values, facilitating
its discoverability. Further, we show that multicalibration with respect to the
decision maker's confidence on her own predictions is a sufficient condition
for alignment. Experiments on four different AI-assisted decision making tasks
where a classifier provides decision support to real human experts validate our
theoretical results and suggest that alignment may lead to better decisions.",2306.00074v4,https://arxiv.org/pdf/2306.00074v4
"Multi-level Cross-modal Feature Alignment via Contrastive Learning
  towards Zero-shot Classification of Remote Sensing Image Scenes","Chun Liu, Suqiang Ma, Zheng Li, Wei Yang, Zhigang Han","Zero-shot classification of image scenes which can recognize the image scenes
that are not seen in the training stage holds great promise of lowering the
dependence on large numbers of labeled samples. To address the zero-shot image
scene classification, the cross-modal feature alignment methods have been
proposed in recent years. These methods mainly focus on matching the visual
features of each image scene with their corresponding semantic descriptors in
the latent space. Less attention has been paid to the contrastive relationships
between different image scenes and different semantic descriptors. In light of
the challenge of large intra-class difference and inter-class similarity among
image scenes and the potential noisy samples, these methods are susceptible to
the influence of the instances which are far from these of the same classes and
close to these of other classes. In this work, we propose a multi-level
cross-modal feature alignment method via contrastive learning for zero-shot
classification of remote sensing image scenes. While promoting the
single-instance level positive alignment between each image scene with their
corresponding semantic descriptors, the proposed method takes the
cross-instance contrastive relationships into consideration,and learns to keep
the visual and semantic features of different classes in the latent space apart
from each other. Extensive experiments have been done to evaluate the
performance of the proposed method. The results show that our proposed method
outperforms state of the art methods for zero-shot remote sensing image scene
classification. All the code and data are available at github
https://github.com/masuqiang/MCFA-Pytorch",2306.06066v1,https://arxiv.org/pdf/2306.06066v1
"RealignDiff: Boosting Text-to-Image Diffusion Model with Coarse-to-fine
  Semantic Re-alignment","Guian Fang, Zutao Jiang, Jianhua Han, Guansong Lu, Hang Xu, Shengcai Liao, Xiaodan Liang","Recent advances in text-to-image diffusion models have achieved remarkable
success in generating high-quality, realistic images from textual descriptions.
However, these approaches have faced challenges in precisely aligning the
generated visual content with the textual concepts described in the prompts. In
this paper, we propose a two-stage coarse-to-fine semantic re-alignment method,
named RealignDiff, aimed at improving the alignment between text and images in
text-to-image diffusion models. In the coarse semantic re-alignment phase, a
novel caption reward, leveraging the BLIP-2 model, is proposed to evaluate the
semantic discrepancy between the generated image caption and the given text
prompt. Subsequently, the fine semantic re-alignment stage employs a local
dense caption generation module and a re-weighting attention modulation module
to refine the previously generated images from a local semantic view.
Experimental results on the MS-COCO benchmark demonstrate that the proposed
two-stage coarse-to-fine semantic re-alignment method outperforms other
baseline re-alignment techniques by a substantial margin in both visual quality
and semantic similarity with the input prompt.",2305.19599v3,https://arxiv.org/pdf/2305.19599v3
"Intent-aligned AI systems deplete human agency: the need for agency
  foundations research in AI safety","Catalin Mitelut, Ben Smith, Peter Vamplew","The rapid advancement of artificial intelligence (AI) systems suggests that
artificial general intelligence (AGI) systems may soon arrive. Many researchers
are concerned that AIs and AGIs will harm humans via intentional misuse
(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,
there is an increasing effort focused on developing algorithms and paradigms
that ensure AI systems are aligned to what humans intend, e.g. AI systems that
yield actions or recommendations that humans might judge as consistent with
their intentions and goals. Here we argue that alignment to human intent is
insufficient for safe AI systems and that preservation of long-term agency of
humans may be a more robust standard, and one that needs to be separated
explicitly and a priori during optimization. We argue that AI systems can
reshape human intention and discuss the lack of biological and psychological
mechanisms that protect humans from loss of agency. We provide the first formal
definition of agency-preserving AI-human interactions which focuses on
forward-looking agency evaluations and argue that AI systems - not humans -
must be increasingly tasked with making these evaluations. We show how agency
loss can occur in simple environments containing embedded agents that use
temporal-difference learning to make action recommendations. Finally, we
propose a new area of research called ""agency foundations"" and pose four
initial topics designed to improve our understanding of agency in AI-human
interactions: benevolent game theory, algorithmic foundations of human rights,
mechanistic interpretability of agency representation in neural-networks and
reinforcement learning from internal states.",2305.19223v1,https://arxiv.org/pdf/2305.19223v1
"Which Models have Perceptually-Aligned Gradients? An Explanation via
  Off-Manifold Robustness","Suraj Srinivas, Sebastian Bordt, Hima Lakkaraju","One of the remarkable properties of robust computer vision models is that
their input-gradients are often aligned with human perception, referred to in
the literature as perceptually-aligned gradients (PAGs). Despite only being
trained for classification, PAGs cause robust models to have rudimentary
generative capabilities, including image generation, denoising, and
in-painting. However, the underlying mechanisms behind these phenomena remain
unknown. In this work, we provide a first explanation of PAGs via
\emph{off-manifold robustness}, which states that models must be more robust
off- the data manifold than they are on-manifold. We first demonstrate
theoretically that off-manifold robustness leads input gradients to lie
approximately on the data manifold, explaining their perceptual alignment. We
then show that Bayes optimal models satisfy off-manifold robustness, and
confirm the same empirically for robust models trained via gradient norm
regularization, randomized smoothing, and adversarial training with projected
gradient descent. Quantifying the perceptual alignment of model gradients via
their similarity with the gradients of generative models, we show that
off-manifold robustness correlates well with perceptual alignment. Finally,
based on the levels of on- and off-manifold robustness, we identify three
different regimes of robustness that affect both perceptual alignment and model
accuracy: weak robustness, bayes-aligned robustness, and excessive robustness.
Code is available at \url{https://github.com/tml-tuebingen/pags}.",2305.19101v2,https://arxiv.org/pdf/2305.19101v2
Independent Component Alignment for Multi-Task Learning,"Dmitry Senushkin, Nikolay Patakin, Arseny Kuznetsov, Anton Konushin","In a multi-task learning (MTL) setting, a single model is trained to tackle a
diverse set of tasks jointly. Despite rapid progress in the field, MTL remains
challenging due to optimization issues such as conflicting and dominating
gradients. In this work, we propose using a condition number of a linear system
of gradients as a stability criterion of an MTL optimization. We theoretically
demonstrate that a condition number reflects the aforementioned optimization
issues. Accordingly, we present Aligned-MTL, a novel MTL optimization approach
based on the proposed criterion, that eliminates instability in the training
process by aligning the orthogonal components of the linear system of
gradients. While many recent MTL approaches guarantee convergence to a minimum,
task trade-offs cannot be specified in advance. In contrast, Aligned-MTL
provably converges to an optimal point with pre-defined task-specific weights,
which provides more control over the optimization result. Through experiments,
we show that the proposed approach consistently improves performance on a
diverse set of MTL benchmarks, including semantic and instance segmentation,
depth estimation, surface normal estimation, and reinforcement learning. The
source code is publicly available at https://github.com/SamsungLabs/MTL .",2305.19000v1,https://arxiv.org/pdf/2305.19000v1
"Weakly-supervised forced alignment of disfluent speech using
  phoneme-level modeling","Theodoros Kouzelis, Georgios Paraskevopoulos, Athanasios Katsamanis, Vassilis Katsouros","The study of speech disorders can benefit greatly from time-aligned data.
However, audio-text mismatches in disfluent speech cause rapid performance
degradation for modern speech aligners, hindering the use of automatic
approaches. In this work, we propose a simple and effective modification of
alignment graph construction of CTC-based models using Weighted Finite State
Transducers. The proposed weakly-supervised approach alleviates the need for
verbatim transcription of speech disfluencies for forced alignment. During the
graph construction, we allow the modeling of common speech disfluencies, i.e.
repetitions and omissions. Further, we show that by assessing the degree of
audio-text mismatch through the use of Oracle Error Rate, our method can be
effectively used in the wild. Our evaluation on a corrupted version of the
TIMIT test set and the UCLASS dataset shows significant improvements,
particularly for recall, achieving a 23-25% relative improvement over our
baselines.",2306.00996v1,https://arxiv.org/pdf/2306.00996v1
"Aligning Optimization Trajectories with Diffusion Models for Constrained
  Design Generation","Giorgio Giannone, Akash Srivastava, Ole Winther, Faez Ahmed","Generative models have had a profound impact on vision and language, paving
the way for a new era of multimodal generative applications. While these
successes have inspired researchers to explore using generative models in
science and engineering to accelerate the design process and reduce the
reliance on iterative optimization, challenges remain. Specifically,
engineering optimization methods based on physics still outperform generative
models when dealing with constrained environments where data is scarce and
precision is paramount. To address these challenges, we introduce Diffusion
Optimization Models (DOM) and Trajectory Alignment (TA), a learning framework
that demonstrates the efficacy of aligning the sampling trajectory of diffusion
models with the optimization trajectory derived from traditional physics-based
methods. This alignment ensures that the sampling process remains grounded in
the underlying physical principles. Our method allows for generating feasible
and high-performance designs in as few as two steps without the need for
expensive preprocessing, external surrogate models, or additional labeled data.
We apply our framework to structural topology optimization, a fundamental
problem in mechanical design, evaluating its performance on in- and
out-of-distribution configurations. Our results demonstrate that TA outperforms
state-of-the-art deep generative models on in-distribution configurations and
halves the inference computational cost. When coupled with a few steps of
optimization, it also improves manufacturability for out-of-distribution
conditions. By significantly improving performance and inference efficiency,
DOM enables us to generate high-quality designs in just a few steps and guide
them toward regions of high performance and manufacturability, paving the way
for the widespread application of generative models in large-scale data-driven
design.",2305.18470v1,https://arxiv.org/pdf/2305.18470v1
Conditional Support Alignment for Domain Adaptation with Label Shift,"Anh T Nguyen, Lam Tran, Anh Tong, Tuan-Duy H. Nguyen, Toan Tran","Unsupervised domain adaptation (UDA) refers to a domain adaptation framework
in which a learning model is trained based on the labeled samples on the source
domain and unlabelled ones in the target domain. The dominant existing methods
in the field that rely on the classical covariate shift assumption to learn
domain-invariant feature representation have yielded suboptimal performance
under the label distribution shift between source and target domains. In this
paper, we propose a novel conditional adversarial support alignment (CASA)
whose aim is to minimize the conditional symmetric support divergence between
the source's and target domain's feature representation distributions, aiming
at a more helpful representation for the classification task. We also introduce
a novel theoretical target risk bound, which justifies the merits of aligning
the supports of conditional feature distributions compared to the existing
marginal support alignment approach in the UDA settings. We then provide a
complete training process for learning in which the objective optimization
functions are precisely based on the proposed target risk bound. Our empirical
results demonstrate that CASA outperforms other state-of-the-art methods on
different UDA benchmark tasks under label shift conditions.",2305.18458v1,https://arxiv.org/pdf/2305.18458v1
Reward Collapse in Aligning Large Language Models,"Ziang Song, Tianle Cai, Jason D. Lee, Weijie J. Su","The extraordinary capabilities of large language models (LLMs) such as
ChatGPT and GPT-4 are in part unleashed by aligning them with reward models
that are trained on human preferences, which are often represented as rankings
of responses to prompts. In this paper, we document the phenomenon of
\textit{reward collapse}, an empirical observation where the prevailing
ranking-based approach results in an \textit{identical} reward distribution
\textit{regardless} of the prompts during the terminal phase of training. This
outcome is undesirable as open-ended prompts like ``write a short story about
your best friend'' should yield a continuous range of rewards for their
completions, while specific prompts like ``what is the capital of New Zealand''
should generate either high or low rewards. Our theoretical investigation
reveals that reward collapse is primarily due to the insufficiency of the
ranking-based objective function to incorporate prompt-related information
during optimization. This insight allows us to derive closed-form expressions
for the reward distribution associated with a set of utility functions in an
asymptotic regime. To overcome reward collapse, we introduce a prompt-aware
optimization scheme that provably admits a prompt-dependent reward distribution
within the interpolating regime. Our experimental results suggest that our
proposed prompt-aware utility functions significantly alleviate reward collapse
during the training of reward models.",2305.17608v1,https://arxiv.org/pdf/2305.17608v1
"Training Socially Aligned Language Models on Simulated Social
  Interactions","Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, Soroush Vosoughi","Social alignment in AI systems aims to ensure that these models behave
according to established societal values. However, unlike humans, who derive
consensus on value judgments through social interaction, current language
models (LMs) are trained to rigidly replicate their training corpus in
isolation, leading to subpar generalization in unfamiliar scenarios and
vulnerability to adversarial attacks. This work presents a novel training
paradigm that permits LMs to learn from simulated social interactions. In
comparison to existing methodologies, our approach is considerably more
scalable and efficient, demonstrating superior performance in alignment
benchmarks and human evaluations. This paradigm shift in the training of LMs
brings us a step closer to developing AI systems that can robustly and
accurately reflect societal norms and values.",2305.16960v3,https://arxiv.org/pdf/2305.16960v3
"Domain Aligned Prefix Averaging for Domain Generalization in Abstractive
  Summarization","Pranav Ajit Nair, Sukomal Pal, Pradeepika Verma","Domain generalization is hitherto an underexplored area applied in
abstractive summarization. Moreover, most existing works on domain
generalization have sophisticated training algorithms. In this paper, we
propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging
approach to domain generalization for abstractive summarization. Given a number
of source domains, our method first trains a prefix for each one of them. These
source prefixes generate summaries for a small number of target domain
documents. The similarity of the generated summaries to their corresponding
documents is used for calculating weights required to average source prefixes.
In DAPA, prefix tuning allows for lightweight finetuning, and weight averaging
allows for the computationally efficient addition of new source domains. When
evaluated on four diverse summarization domains, DAPA shows comparable or
better performance against the baselines, demonstrating the effectiveness of
its prefix averaging scheme.",2305.16820v2,https://arxiv.org/pdf/2305.16820v2
Heterogeneous Value Alignment Evaluation for Large Language Models,"Zhaowei Zhang, Ceyao Zhang, Nian Liu, Siyuan Qi, Ziqi Rong, Song-Chun Zhu, Shuguang Cui, Yaodong Yang","The emergent capabilities of Large Language Models (LLMs) have made it
crucial to align their values with those of humans. However, current
methodologies typically attempt to assign value as an attribute to LLMs, yet
lack attention to the ability to pursue value and the importance of
transferring heterogeneous values in specific practical applications. In this
paper, we propose a Heterogeneous Value Alignment Evaluation (HVAE) system,
designed to assess the success of aligning LLMs with heterogeneous values.
Specifically, our approach first brings the Social Value Orientation (SVO)
framework from social psychology, which corresponds to how much weight a person
attaches to the welfare of others in relation to their own. We then assign the
LLMs with different social values and measure whether their behaviors align
with the inducing values. We conduct evaluations with new auto-metric
\textit{value rationality} to represent the ability of LLMs to align with
specific values. Evaluating the value rationality of five mainstream LLMs, we
discern a propensity in LLMs towards neutral values over pronounced personal
values. By examining the behavior of these LLMs, we contribute to a deeper
insight into the value alignment of LLMs within a heterogeneous value system.",2305.17147v3,https://arxiv.org/pdf/2305.17147v3
Feature-aligned N-BEATS with Sinkhorn divergence,"Joonhun Lee, Myeongho Jeon, Myungjoo Kang, Kyunghyun Park","We propose Feature-aligned N-BEATS as a domain-generalized time series
forecasting model. It is a nontrivial extension of N-BEATS with doubly residual
stacking principle (Oreshkin et al. [45]) into a representation learning
framework. In particular, it revolves around marginal feature probability
measures induced by the intricate composition of residual and feature
extracting operators of N-BEATS in each stack and aligns them stack-wise via an
approximate of an optimal transport distance referred to as the Sinkhorn
divergence. The training loss consists of an empirical risk minimization from
multiple source domains, i.e., forecasting loss, and an alignment loss
calculated with the Sinkhorn divergence, which allows the model to learn
invariant features stack-wise across multiple source data sequences while
retaining N-BEATS's interpretable design and forecasting power. Comprehensive
experimental evaluations with ablation studies are provided and the
corresponding results demonstrate the proposed model's forecasting and
generalization capabilities.",2305.15196v3,https://arxiv.org/pdf/2305.15196v3
"Revisit and Outstrip Entity Alignment: A Perspective of Generative
  Models","Lingbing Guo, Zhuo Chen, Jiaoyan Chen, Yin Fang, Wen Zhang, Huajun Chen","Recent embedding-based methods have achieved great successes in exploiting
entity alignment from knowledge graph (KG) embeddings of multiple modalities.
In this paper, we study embedding-based entity alignment (EEA) from a
perspective of generative models. We show that EEA shares similarities with
typical generative models and prove the effectiveness of the recently developed
generative adversarial network (GAN)-based EEA methods theoretically. We then
reveal that their incomplete objective limits the capacity on both entity
alignment and entity synthesis (i.e., generating new entities). We mitigate
this problem by introducing a generative EEA (GEEA) framework with the proposed
mutual variational autoencoder (M-VAE) as the generative model. M-VAE enables
entity conversion between KGs and generation of new entities from random noise
vectors. We demonstrate the power of GEEA with theoretical analysis and
empirical experiments on both entity alignment and entity synthesis tasks.",2305.14651v2,https://arxiv.org/pdf/2305.14651v2
Aligning Large Language Models through Synthetic Feedback,"Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, Minjoon Seo","Aligning large language models (LLMs) to human values has become increasingly
important as it enables sophisticated steering of LLMs. However, it requires
significant human demonstrations and feedback or distillation from proprietary
LLMs such as ChatGPT. In this work, we propose a novel alignment learning
framework with synthetic feedback not dependent on extensive human annotations
and proprietary LLMs. First, we perform reward modeling (RM) with synthetic
feedback by contrasting responses from vanilla LLMs with various sizes and
prompts. Then, we use the RM to simulate high-quality demonstrations to train a
supervised policy and further optimize the model with reinforcement learning.
Our resulting model, Aligned Language Model with Synthetic Training dataset
(ALMoST), outperforms recent open-sourced models, which are trained on the
outputs of InstructGPT or human-annotated demonstrations, in alignment
benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2,
55.0% and 58.5% of the time, respectively. Further analyses demonstrate the
efficacy and importance of synthetic feedback in our framework. The code is
available at https://github.com/naver-ai/almost",2305.13735v2,https://arxiv.org/pdf/2305.13735v2
"The Knowledge Alignment Problem: Bridging Human and External Knowledge
  for Large Language Models","Shuo Zhang, Liangming Pan, Junzhou Zhao, William Yang Wang","Large language models often necessitate grounding on external knowledge to
generate faithful and reliable answers. Yet even with the correct groundings in
the reference, they can ignore them and rely on wrong groundings or their
inherent biases to hallucinate when users, being largely unaware of the
specifics of the stored information, pose questions that might not directly
correlate with the retrieved groundings. In this work, we formulate this
knowledge alignment problem and introduce MixAlign, a framework that interacts
with both the human user and the knowledge base to obtain and integrate
clarifications on how the user question relates to the stored information.
MixAlign employs a language model to achieve automatic knowledge alignment and,
if necessary, further enhances this alignment through human user
clarifications. Experimental results highlight the crucial role of knowledge
alignment in boosting model performance and mitigating hallucination, with
improvements noted up to 22.2% and 27.1% respectively. We also demonstrate the
effectiveness of MixAlign in improving knowledge alignment by producing
high-quality, user-centered clarifications.",2305.13669v3,https://arxiv.org/pdf/2305.13669v3
"InstructAlign: High-and-Low Resource Language Alignment via Continual
  Crosslingual Instruction Tuning","Samuel Cahyawijaya, Holy Lovenia, Tiezheng Yu, Willy Chung, Pascale Fung","Large language models (LLMs) that are tuned with instructions have
demonstrated remarkable capabilities in various tasks and languages. However,
their ability to generalize to underrepresented languages is limited due to the
scarcity of available data. Additionally, directly adapting new languages to
instruction-tuned LLMs can result in catastrophic forgetting, which leads to
the loss of multitasking ability. To address this issue, we propose
InstructAlign which uses continual crosslingual instruction tuning to enable
LLMs to align new unseen languages with previously learned high-resource
languages. Our results demonstrate the effectiveness of InstructAlign in
enabling the model to understand low-resource languages with limited parallel
data while preventing catastrophic forgetting. Our work contributes to the
advancement of language adaptation methods, particularly for adapting
instruction-tuned LLMs to underrepresented languages. Our code is released on
https://github.com/HLTCHKUST/InstructAlign",2305.13627v2,https://arxiv.org/pdf/2305.13627v2
DiffAVA: Personalized Text-to-Audio Generation with Visual Alignment,"Shentong Mo, Jing Shi, Yapeng Tian","Text-to-audio (TTA) generation is a recent popular problem that aims to
synthesize general audio given text descriptions. Previous methods utilized
latent diffusion models to learn audio embedding in a latent space with text
embedding as the condition. However, they ignored the synchronization between
audio and visual content in the video, and tended to generate audio mismatching
from video frames. In this work, we propose a novel and personalized
text-to-sound generation approach with visual alignment based on latent
diffusion models, namely DiffAVA, that can simply fine-tune lightweight
visual-text alignment modules with frozen modality-specific encoders to update
visual-aligned text embeddings as the condition. Specifically, our DiffAVA
leverages a multi-head attention transformer to aggregate temporal information
from video features, and a dual multi-modal residual network to fuse temporal
visual representations with text embeddings. Then, a contrastive learning
objective is applied to match visual-aligned text embeddings with audio
features. Experimental results on the AudioCaps dataset demonstrate that the
proposed DiffAVA can achieve competitive performance on visual-aligned
text-to-audio generation.",2305.12903v1,https://arxiv.org/pdf/2305.12903v1
"uCTRL: Unbiased Contrastive Representation Learning via Alignment and
  Uniformity for Collaborative Filtering","Jae-woong Lee, Seongmin Park, Mincheol Yoon, Jongwuk Lee","Because implicit user feedback for the collaborative filtering (CF) models is
biased toward popular items, CF models tend to yield recommendation lists with
popularity bias. Previous studies have utilized inverse propensity weighting
(IPW) or causal inference to mitigate this problem. However, they solely employ
pointwise or pairwise loss functions and neglect to adopt a contrastive loss
function for learning meaningful user and item representations. In this paper,
we propose Unbiased ConTrastive Representation Learning (uCTRL), optimizing
alignment and uniformity functions derived from the InfoNCE loss function for
CF models. Specifically, we formulate an unbiased alignment function used in
uCTRL. We also devise a novel IPW estimation method that removes the bias of
both users and items. Despite its simplicity, uCTRL equipped with existing CF
models consistently outperforms state-of-the-art unbiased recommender models,
up to 12.22% for Recall@20 and 16.33% for NDCG@20 gains, on four benchmark
datasets.",2305.12768v1,https://arxiv.org/pdf/2305.12768v1
"Text-Video Retrieval with Disentangled Conceptualization and Set-to-Set
  Alignment","Peng Jin, Hao Li, Zesen Cheng, Jinfa Huang, Zhennan Wang, Li Yuan, Chang Liu, Jie Chen","Text-video retrieval is a challenging cross-modal task, which aims to align
visual entities with natural language descriptions. Current methods either fail
to leverage the local details or are computationally expensive. What's worse,
they fail to leverage the heterogeneous concepts in data. In this paper, we
propose the Disentangled Conceptualization and Set-to-set Alignment (DiCoSA) to
simulate the conceptualizing and reasoning process of human beings. For
disentangled conceptualization, we divide the coarse feature into multiple
latent factors related to semantic concepts. For set-to-set alignment, where a
set of visual concepts correspond to a set of textual concepts, we propose an
adaptive pooling method to aggregate semantic concepts to address the partial
matching. In particular, since we encode concepts independently in only a few
dimensions, DiCoSA is superior at efficiency and granularity, ensuring
fine-grained interactions using a similar computational complexity as
coarse-grained alignment. Extensive experiments on five datasets, including
MSR-VTT, LSMDC, MSVD, ActivityNet, and DiDeMo, demonstrate that our method
outperforms the existing state-of-the-art methods.",2305.12218v1,https://arxiv.org/pdf/2305.12218v1
"CARD: Channel Aligned Robust Blend Transformer for Time Series
  Forecasting","Wang Xue, Tian Zhou, Qingsong Wen, Jinyang Gao, Bolin Ding, Rong Jin","Recent studies have demonstrated the great power of Transformer models for
time series forecasting. One of the key elements that lead to the transformer's
success is the channel-independent (CI) strategy to improve the training
robustness. However, the ignorance of the correlation among different channels
in CI would limit the model's forecasting capacity. In this work, we design a
special Transformer, i.e., Channel Aligned Robust Blend Transformer (CARD for
short), that addresses key shortcomings of CI type Transformer in time series
forecasting. First, CARD introduces a channel-aligned attention structure that
allows it to capture both temporal correlations among signals and dynamical
dependence among multiple variables over time. Second, in order to efficiently
utilize the multi-scale knowledge, we design a token blend module to generate
tokens with different resolutions. Third, we introduce a robust loss function
for time series forecasting to alleviate the potential overfitting issue. This
new loss function weights the importance of forecasting over a finite horizon
based on prediction uncertainties. Our evaluation of multiple long-term and
short-term forecasting datasets demonstrates that CARD significantly
outperforms state-of-the-art time series forecasting methods. The code is
available at the following repository:https://github.com/wxie9/CARD",2305.12095v5,https://arxiv.org/pdf/2305.12095v5
SIDAR: Synthetic Image Dataset for Alignment & Restoration,"Monika Kwiatkowski, Simon Matern, Olaf Hellwich","Image alignment and image restoration are classical computer vision tasks.
However, there is still a lack of datasets that provide enough data to train
and evaluate end-to-end deep learning models. Obtaining ground-truth data for
image alignment requires sophisticated structure-from-motion methods or optical
flow systems that often do not provide enough data variance, i.e., typically
providing a high number of image correspondences, while only introducing few
changes of scenery within the underlying image sequences. Alternative
approaches utilize random perspective distortions on existing image data.
However, this only provides trivial distortions, lacking the complexity and
variance of real-world scenarios. Instead, our proposed data augmentation helps
to overcome the issue of data scarcity by using 3D rendering: images are added
as textures onto a plane, then varying lighting conditions, shadows, and
occlusions are added to the scene. The scene is rendered from multiple
viewpoints, generating perspective distortions more consistent with real-world
scenarios, with homographies closely resembling those of camera projections
rather than randomized homographies. For each scene, we provide a sequence of
distorted images with corresponding occlusion masks, homographies, and
ground-truth labels. The resulting dataset can serve as a training and
evaluation set for a multitude of tasks involving image alignment and artifact
removal, such as deep homography estimation, dense image matching, 2D bundle
adjustment, inpainting, shadow removal, denoising, content retrieval, and
background subtraction. Our data generation pipeline is customizable and can be
applied to any existing dataset, serving as a data augmentation to further
improve the feature learning of any existing method.",2305.12036v1,https://arxiv.org/pdf/2305.12036v1
"Speech-Text Dialog Pre-training for Spoken Dialog Understanding with
  Explicit Cross-Modal Alignment","Tianshu Yu, Haoyu Gao, Ting-En Lin, Min Yang, Yuchuan Wu, Wentao Ma, Chao Wang, Fei Huang, Yongbin Li","Recently, speech-text pre-training methods have shown remarkable success in
many speech and natural language processing tasks. However, most previous
pre-trained models are usually tailored for one or two specific tasks, but fail
to conquer a wide range of speech-text tasks. In addition, existing speech-text
pre-training methods fail to explore the contextual information within a
dialogue to enrich utterance representations. In this paper, we propose
Speech-text dialog Pre-training for spoken dialog understanding with ExpliCiT
cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog
pre-training model. Concretely, to consider the temporality of speech modality,
we design a novel temporal position prediction task to capture the speech-text
alignment. This pre-training task aims to predict the start and end time of
each textual word in the corresponding speech waveform. In addition, to learn
the characteristics of spoken dialogs, we generalize a response selection task
from textual dialog pre-training to speech-text dialog pre-training scenarios.
Experimental results on four different downstream speech-text tasks demonstrate
the superiority of SPECTRA in learning speech-text alignment and multi-turn
dialog context.",2305.11579v2,https://arxiv.org/pdf/2305.11579v2
"AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide
  for Simultaneous Speech Translation","Sara Papi, Marco Turchi, Matteo Negri","Attention is the core mechanism of today's most used architectures for
natural language processing and has been analyzed from many perspectives,
including its effectiveness for machine translation-related tasks. Among these
studies, attention resulted to be a useful source of information to get
insights about word alignment also when the input text is substituted with
audio segments, as in the case of the speech translation (ST) task. In this
paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that
exploits the attention information to generate source-target alignments that
guide the model during inference. Through experiments on the 8 language pairs
of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art
SimulST policies applied to offline-trained models with gains in terms of BLEU
of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8
languages.",2305.11408v2,https://arxiv.org/pdf/2305.11408v2
LIMA: Less Is More for Alignment,"Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy","Large language models are trained in two stages: (1) unsupervised pretraining
from raw text, to learn general-purpose representations, and (2) large scale
instruction tuning and reinforcement learning, to better align to end tasks and
user preferences. We measure the relative importance of these two stages by
training LIMA, a 65B parameter LLaMa language model fine-tuned with the
standard supervised loss on only 1,000 carefully curated prompts and responses,
without any reinforcement learning or human preference modeling. LIMA
demonstrates remarkably strong performance, learning to follow specific
response formats from only a handful of examples in the training data,
including complex queries that range from planning trip itineraries to
speculating about alternate history. Moreover, the model tends to generalize
well to unseen tasks that did not appear in the training data. In a controlled
human study, responses from LIMA are either equivalent or strictly preferred to
GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard
and 65% versus DaVinci003, which was trained with human feedback. Taken
together, these results strongly suggest that almost all knowledge in large
language models is learned during pretraining, and only limited instruction
tuning data is necessary to teach models to produce high quality output.",2305.11206v1,https://arxiv.org/pdf/2305.11206v1
"Semantically Aligned Task Decomposition in Multi-Agent Reinforcement
  Learning","Wenhao Li, Dan Qiao, Baoxiang Wang, Xiangfeng Wang, Bo Jin, Hongyuan Zha","The difficulty of appropriately assigning credit is particularly heightened
in cooperative MARL with sparse reward, due to the concurrent time and
structural scales involved. Automatic subgoal generation (ASG) has recently
emerged as a viable MARL approach inspired by utilizing subgoals in
intrinsically motivated reinforcement learning. However, end-to-end learning of
complex task planning from sparse rewards without prior knowledge, undoubtedly
requires massive training samples. Moreover, the diversity-promoting nature of
existing ASG methods can lead to the ""over-representation"" of subgoals,
generating numerous spurious subgoals of limited relevance to the actual task
reward and thus decreasing the sample efficiency of the algorithm. To address
this problem and inspired by the disentangled representation learning, we
propose a novel ""disentangled"" decision-making method, Semantically Aligned
task decomposition in MARL (SAMA), that prompts pretrained language models with
chain-of-thought that can suggest potential goals, provide suitable goal
decomposition and subgoal allocation as well as self-reflection-based
replanning. Additionally, SAMA incorporates language-grounded RL to train each
agent's subgoal-conditioned policy. SAMA demonstrates considerable advantages
in sample efficiency compared to state-of-the-art ASG methods, as evidenced by
its performance on two challenging sparse-reward tasks, Overcooked and MiniRTS.",2305.10865v2,https://arxiv.org/pdf/2305.10865v2
Dual-Alignment Pre-training for Cross-lingual Sentence Embedding,"Ziheng Li, Shaohan Huang, Zihan Zhang, Zhi-Hong Deng, Qiang Lou, Haizhen Huang, Jian Jiao, Furu Wei, Weiwei Deng, Qi Zhang","Recent studies have shown that dual encoder models trained with the
sentence-level translation ranking task are effective methods for cross-lingual
sentence embedding. However, our research indicates that token-level alignment
is also crucial in multilingual scenarios, which has not been fully explored
previously. Based on our findings, we propose a dual-alignment pre-training
(DAP) framework for cross-lingual sentence embedding that incorporates both
sentence-level and token-level alignment. To achieve this, we introduce a novel
representation translation learning (RTL) task, where the model learns to use
one-side contextualized token representation to reconstruct its translation
counterpart. This reconstruction objective encourages the model to embed
translation information into the token representation. Compared to other
token-level alignment methods such as translation language modeling, RTL is
more suitable for dual encoder architectures and is computationally efficient.
Extensive experiments on three sentence-level cross-lingual benchmarks
demonstrate that our approach can significantly improve sentence embedding. Our
code is available at https://github.com/ChillingDream/DAP.",2305.09148v1,https://arxiv.org/pdf/2305.09148v1
"Multi-Value Alignment in Normative Multi-Agent System: Evolutionary
  Optimisation Approach","Maha Riad, Vinicius Renan de Carvalho, Fatemeh Golpayegani","Value-alignment in normative multi-agent systems is used to promote a certain
value and to ensure the consistent behavior of agents in autonomous intelligent
systems with human values. However, the current literature is limited to
incorporation of effective norms for single value alignment with no
consideration of agents' heterogeneity and the requirement of simultaneous
promotion and alignment of multiple values. This research proposes a
multi-value promotion model that uses multi-objective evolutionary algorithms
to produce the optimum parametric set of norms that is aligned with multiple
simultaneous values of heterogeneous agents and the system. To understand
various aspects of this complex problem, several evolutionary algorithms were
used to find a set of optimised norm parameters considering two toy tax
scenarios with two and five values are considered. The results are analysed
from different perspectives to show the impact of a selected evolutionary
algorithm on the solution, and the importance of understanding the relation
between values when prioritising them.",2305.07366v1,https://arxiv.org/pdf/2305.07366v1
"A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph
  Entity Alignment","Jianheng Tang, Kangfei Zhao, Jia Li","Entity alignment is the task of identifying corresponding entities across
different knowledge graphs (KGs). Although recent embedding-based entity
alignment methods have shown significant advancements, they still struggle to
fully utilize KG structural information. In this paper, we introduce FGWEA, an
unsupervised entity alignment framework that leverages the Fused
Gromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison of
entity semantics and KG structures within a joint optimization framework. To
address the computational challenges associated with optimizing FGW, we devise
a three-stage progressive optimization algorithm. It starts with a basic
semantic embedding matching, proceeds to approximate cross-KG structural and
relational similarity matching based on iterative updates of high-confidence
entity links, and ultimately culminates in a global structural comparison
between KGs. We perform extensive experiments on four entity alignment datasets
covering 14 distinct KGs across five languages. Without any supervision or
hyper-parameter tuning, FGWEA surpasses 21 competitive baselines, including
cutting-edge supervised entity alignment methods. Our code is available at
https://github.com/squareRoot3/FusedGW-Entity-Alignment.",2305.06574v1,https://arxiv.org/pdf/2305.06574v1
Text-To-Concept (and Back) via Cross-Model Alignment,"Mazda Moayeri, Keivan Rezaei, Maziar Sanjabi, Soheil Feizi","We observe that the mapping between an image's representation in one model to
its representation in another can be learned surprisingly well with just a
linear layer, even across diverse models. Building on this observation, we
propose $\textit{text-to-concept}$, where features from a fixed pretrained
model are aligned linearly to the CLIP space, so that text embeddings from
CLIP's text encoder become directly comparable to the aligned features. With
text-to-concept, we convert fixed off-the-shelf vision encoders to surprisingly
strong zero-shot classifiers for free, with accuracy at times even surpassing
that of CLIP, despite being much smaller models and trained on a small fraction
of the data compared to CLIP. We show other immediate use-cases of
text-to-concept, like building concept bottleneck models with no concept
supervision, diagnosing distribution shifts in terms of human concepts, and
retrieving images satisfying a set of text-based constraints. Lastly, we
demonstrate the feasibility of $\textit{concept-to-text}$, where vectors in a
model's feature space are decoded by first aligning to the CLIP before being
fed to a GPT-based generative model. Our work suggests existing deep models,
with presumably diverse architectures and training, represent input samples
relatively similarly, and a two-way communication across model representation
spaces and to humans (through language) is viable.",2305.06386v1,https://arxiv.org/pdf/2305.06386v1
Effective Medical Code Prediction via Label Internal Alignment,Guodong Liu,"The clinical notes are usually typed into the system by physicians. They are
typically required to be marked by standard medical codes, and each code
represents a diagnosis or medical treatment procedure. Annotating these notes
is time consuming and prone to error. In this paper, we proposed a multi-view
attention based Neural network to predict medical codes from clinical texts.
Our method incorporates three aspects of information, the semantic context of
the clinical text, the relationship among the label (medical codes) space, and
the alignment between each pair of a clinical text and medical code. Our method
is verified to be effective on the open source dataset. The experimental result
shows that our method achieves better performance against the prior
state-of-art on multiple metrics.",2305.05162v1,https://arxiv.org/pdf/2305.05162v1
Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval,"Shiyin Dong, Mingrui Zhu, Nannan Wang, Xinbo Gao","Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the
cross-domain nature of sketches and photos, as well as the semantic gap between
seen and unseen image distributions. Previous methods fine-tune pre-trained
models with various side information and learning strategies to learn a compact
feature space that is shared between the sketch and photo domains and bridges
seen and unseen classes. However, these efforts are inadequate in adapting
domains and transferring knowledge from seen to unseen classes. In this paper,
we present an effective ``Adapt and Align'' approach to address the key
challenges. Specifically, we insert simple and lightweight domain adapters to
learn new abstract concepts of the sketch domain and improve cross-domain
representation capabilities. Inspired by recent advances in image-text
foundation models (e.g., CLIP) on zero-shot scenarios, we explicitly align the
learned image embedding with a more semantic text embedding to achieve the
desired knowledge transfer from seen to unseen classes. Extensive experiments
on three benchmark datasets and two popular backbones demonstrate the
superiority of our method in terms of retrieval accuracy and flexibility.",2305.05144v3,https://arxiv.org/pdf/2305.05144v3
"HistAlign: Improving Context Dependency in Language Generation by
  Aligning with History","David Wan, Shiyue Zhang, Mohit Bansal","Language models (LMs) can generate hallucinations and incoherent outputs,
which highlights their weak context dependency. Cache-LMs, which augment LMs
with a memory of recent history, can increase context dependency and have shown
remarkable performance in diverse language generation tasks. However, we find
that even with training, the performance gain stemming from the cache component
of current cache-LMs is suboptimal due to the misalignment between the current
hidden states and those stored in the memory. In this work, we present
HistAlign, a new training approach to ensure good cache alignment such that the
model receives useful signals from the history. We first prove our concept on a
simple and synthetic task where the memory is essential for correct
predictions, and we show that the cache component of HistAlign is better
aligned and improves overall performance. Next, we evaluate HistAlign on
diverse downstream language generation tasks, including prompt continuation,
abstractive summarization, and data-to-text. We demonstrate that HistAlign
improves text coherence and faithfulness in open-ended and conditional
generation settings respectively. HistAlign is also generalizable across
different model families, showcasing its strength in improving context
dependency of LMs in diverse scenarios. Our code is publicly available at
https://github.com/meetdavidwan/histalign",2305.04782v2,https://arxiv.org/pdf/2305.04782v2
AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion,"Seungwoo Lee, Chaerin Kong, Donghyeon Jeon, Nojun Kwak","Recent advances in diffusion models have showcased promising results in the
text-to-video (T2V) synthesis task. However, as these T2V models solely employ
text as the guidance, they tend to struggle in modeling detailed temporal
dynamics. In this paper, we introduce a novel T2V framework that additionally
employ audio signals to control the temporal dynamics, empowering an
off-the-shelf T2I diffusion to generate audio-aligned videos. We propose
audio-based regional editing and signal smoothing to strike a good balance
between the two contradicting desiderata of video synthesis, i.e., temporal
flexibility and coherence. We empirically demonstrate the effectiveness of our
method through experiments, and further present practical applications for
contents creation.",2305.04001v2,https://arxiv.org/pdf/2305.04001v2
Mining bias-target Alignment from Voronoi Cells,"Rémi Nahon, Van-Tam Nguyen, Enzo Tartaglione","Despite significant research efforts, deep neural networks are still
vulnerable to biases: this raises concerns about their fairness and limits
their generalization. In this paper, we propose a bias-agnostic approach to
mitigate the impact of bias in deep neural networks. Unlike traditional
debiasing approaches, we rely on a metric to quantify ``bias
alignment/misalignment'' on target classes, and use this information to
discourage the propagation of bias-target alignment information through the
network. We conduct experiments on several commonly used datasets for debiasing
and compare our method to supervised and bias-specific approaches. Our results
indicate that the proposed method achieves comparable performance to
state-of-the-art supervised approaches, although it is bias-agnostic, even in
presence of multiple biases in the same sample.",2305.03691v1,https://arxiv.org/pdf/2305.03691v1
GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent,"Sascha Marton, Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt","Decision Trees (DTs) are commonly used for many machine learning tasks due to
their high degree of interpretability. However, learning a DT from data is a
difficult optimization problem, as it is non-convex and non-differentiable.
Therefore, common approaches learn DTs using a greedy growth algorithm that
minimizes the impurity locally at each internal node. Unfortunately, this
greedy procedure can lead to inaccurate trees. In this paper, we present a
novel approach for learning hard, axis-aligned DTs with gradient descent. The
proposed method uses backpropagation with a straight-through operator on a
dense DT representation, to jointly optimize all tree parameters. Our approach
outperforms existing methods on binary classification benchmarks and achieves
competitive results for multi-class tasks. The method is available under:
https://github.com/s-marton/GradTree",2305.03515v6,https://arxiv.org/pdf/2305.03515v6
"Principle-Driven Self-Alignment of Language Models from Scratch with
  Minimal Human Supervision","Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan","Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised
fine-tuning (SFT) with human annotations and reinforcement learning from human
feedback (RLHF) to align the output of large language models (LLMs) with human
intentions, ensuring they are helpful, ethical, and reliable. However, this
dependence can significantly constrain the true potential of AI-assistant
agents due to the high cost of obtaining human supervision and the related
issues on quality, reliability, diversity, self-consistency, and undesirable
biases. To address these challenges, we propose a novel approach called
SELF-ALIGN, which combines principle-driven reasoning and the generative power
of LLMs for the self-alignment of AI agents with minimal human supervision. Our
approach encompasses four stages: first, we use an LLM to generate synthetic
prompts, and a topic-guided method to augment the prompt diversity; second, we
use a small set of human-written principles for AI models to follow, and guide
the LLM through in-context learning from demonstrations (of principles
application) to produce helpful, ethical, and reliable responses to user's
queries; third, we fine-tune the original LLM with the high-quality
self-aligned responses so that the resulting model can generate desirable
responses for each query directly without the principle set and the
demonstrations anymore; and finally, we offer a refinement step to address the
issues of overly-brief or indirect responses. Applying SELF-ALIGN to the
LLaMA-65b base language model, we develop an AI assistant named Dromedary. With
fewer than 300 lines of human annotations (including < 200 seed prompts, 16
generic principles, and 5 exemplars for in-context learning). Dromedary
significantly surpasses the performance of several state-of-the-art AI systems,
including Text-Davinci-003 and Alpaca, on benchmark datasets with various
settings.",2305.03047v2,https://arxiv.org/pdf/2305.03047v2
"Type-enhanced Ensemble Triple Representation via Triple-aware Attention
  for Cross-lingual Entity Alignment","Zhishuo Zhang, Chengxiang Tan, Haihang Wang, Xueyan Zhao, Min Yang","Entity alignment(EA) is a crucial task for integrating cross-lingual and
cross-domain knowledge graphs(KGs), which aims to discover entities referring
to the same real-world object from different KGs. Most existing methods
generate aligning entity representation by mining the relevance of triple
elements via embedding-based methods, paying little attention to triple
indivisibility and entity role diversity. In this paper, a novel framework
named TTEA -- Type-enhanced Ensemble Triple Representation via Triple-aware
Attention for Cross-lingual Entity Alignment is proposed to overcome the above
issues considering ensemble triple specificity and entity role features.
Specifically, the ensemble triple representation is derived by regarding
relation as information carrier between semantic space and type space, and
hence the noise influence during spatial transformation and information
propagation can be smoothly controlled via specificity-aware triple attention.
Moreover, our framework uses triple-ware entity enhancement to model the role
diversity of triple elements. Extensive experiments on three real-world
cross-lingual datasets demonstrate that our framework outperforms
state-of-the-art methods.",2305.01556v1,https://arxiv.org/pdf/2305.01556v1
"Parameter-Efficient Cross-lingual Transfer of Vision and Language Models
  via Translation-based Alignment","Zhen Zhang, Jialu Wang, Xin Eric Wang","Pre-trained vision and language models such as CLIP have witnessed remarkable
success in connecting images and texts with a primary focus on English texts.
Despite recent efforts to extend CLIP to support other languages, disparities
in performance among different languages have been observed due to uneven
resource availability. Additionally, current cross-lingual transfer methods of
those pre-trained models would consume excessive resources for a large number
of languages. Therefore, we propose a new parameter-efficient cross-lingual
transfer learning framework that utilizes a translation-based alignment method
to mitigate multilingual disparities and explores parameter-efficient
fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive
experiments on XTD and Multi30K datasets, covering 11 languages under
zero-shot, few-shot, and full-dataset learning scenarios, show that our
framework significantly reduces the multilingual disparities among languages
and improves cross-lingual transfer results, especially in low-resource
scenarios, while only keeping and fine-tuning an extremely small number of
parameters compared to the full model (e.g., Our framework only requires 0.16\%
additional parameters of a full-model for each language in the few-shot
learning scenario). The codes are available at
\url{https://github.com/eric-ai-lab/PECTVLM}. The codes are available at
\url{https://github.com/eric-ai-lab/PECTVLM}.",2305.03510v2,https://arxiv.org/pdf/2305.03510v2
"AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech
  Gesture Synthesis","Hendric Voß, Stefan Kopp","The generation of realistic and contextually relevant co-speech gestures is a
challenging yet increasingly important task in the creation of multimodal
artificial agents. Prior methods focused on learning a direct correspondence
between co-speech gesture representations and produced motions, which created
seemingly natural but often unconvincing gestures during human assessment. We
present an approach to pre-train partial gesture sequences using a generative
adversarial network with a quantization pipeline. The resulting codebook
vectors serve as both input and output in our framework, forming the basis for
the generation and reconstruction of gestures. By learning the mapping of a
latent space representation as opposed to directly mapping it to a vector
representation, this framework facilitates the generation of highly realistic
and expressive gestures that closely replicate human movement and behavior,
while simultaneously avoiding artifacts in the generation process. We evaluate
our approach by comparing it with established methods for generating co-speech
gestures as well as with existing datasets of human behavior. We also perform
an ablation study to assess our findings. The results show that our approach
outperforms the current state of the art by a clear margin and is partially
indistinguishable from human gesturing. We make our data pipeline and the
generation framework publicly available.",2305.01241v2,https://arxiv.org/pdf/2305.01241v2
DualHSIC: HSIC-Bottleneck and Alignment for Continual Learning,"Zifeng Wang, Zheng Zhan, Yifan Gong, Yucai Shao, Stratis Ioannidis, Yanzhi Wang, Jennifer Dy","Rehearsal-based approaches are a mainstay of continual learning (CL). They
mitigate the catastrophic forgetting problem by maintaining a small fixed-size
buffer with a subset of data from past tasks. While most rehearsal-based
approaches study how to effectively exploit the knowledge from the buffered
past data, little attention is paid to the inter-task relationships with the
critical task-specific and task-invariant knowledge. By appropriately
leveraging inter-task relationships, we propose a novel CL method named
DualHSIC to boost the performance of existing rehearsal-based methods in a
simple yet effective way. DualHSIC consists of two complementary components
that stem from the so-called Hilbert Schmidt independence criterion (HSIC):
HSIC-Bottleneck for Rehearsal (HBR) lessens the inter-task interference and
HSIC Alignment (HA) promotes task-invariant knowledge sharing. Extensive
experiments show that DualHSIC can be seamlessly plugged into existing
rehearsal-based methods for consistent performance improvements, and also
outperforms recent state-of-the-art regularization-enhanced rehearsal methods.
Source code will be released.",2305.00380v1,https://arxiv.org/pdf/2305.00380v1
Node Feature Augmentation Vitaminizes Network Alignment,"Jin-Duk Park, Cong Tran, Won-Yong Shin, Xin Cao","Network alignment (NA) is the task of discovering node correspondences across
multiple networks. Although NA methods have achieved remarkable success in a
myriad of scenarios, their effectiveness is not without additional information
such as prior anchor links and/or node features, which may not always be
available due to privacy concerns or access restrictions. To tackle this
challenge, we propose Grad-Align+, a novel NA method built upon a recent
state-of-the-art NA method, the so-called Grad-Align, that gradually discovers
a part of node pairs until all node pairs are found. In designing Grad-Align+,
we account for how to augment node features in the sense of performing the NA
task and how to design our NA method by maximally exploiting the augmented node
features. To achieve this goal, Grad-Align+ consists of three key components:
1) centrality-based node feature augmentation (CNFA), 2) graph neural network
(GNN)-aided embedding similarity calculation alongside the augmented node
features, and 3) gradual NA with similarity calculation using aligned
cross-network neighbor-pairs (ACNs). Through comprehensive experiments, we
demonstrate that Grad-Align+ exhibits (a) the superiority over benchmark NA
methods, (b) empirical validations as well as our theoretical findings to see
the effectiveness of CNFA, (c) the influence of each component, (d) the
robustness to network noises, and (e) the computational efficiency.",2304.12751v4,https://arxiv.org/pdf/2304.12751v4
"Stubborn: An Environment for Evaluating Stubbornness between Agents with
  Aligned Incentives","Ram Rachum, Yonatan Nakar, Reuth Mirsky","Recent research in multi-agent reinforcement learning (MARL) has shown
success in learning social behavior and cooperation. Social dilemmas between
agents in mixed-sum settings have been studied extensively, but there is little
research into social dilemmas in fullycooperative settings, where agents have
no prospect of gaining reward at another agent's expense.
  While fully-aligned interests are conducive to cooperation between agents,
they do not guarantee it. We propose a measure of ""stubbornness"" between agents
that aims to capture the human social behavior from which it takes its name: a
disagreement that is gradually escalating and potentially disastrous. We would
like to promote research into the tendency of agents to be stubborn, the
reactions of counterpart agents, and the resulting social dynamics.
  In this paper we present Stubborn, an environment for evaluating stubbornness
between agents with fully-aligned incentives. In our preliminary results, the
agents learn to use their partner's stubbornness as a signal for improving the
choices that they make in the environment.",2304.12280v2,https://arxiv.org/pdf/2304.12280v2
"Implicit Temporal Modeling with Learnable Alignment for Video
  Recognition","Shuyuan Tu, Qi Dai, Zuxuan Wu, Zhi-Qi Cheng, Han Hu, Yu-Gang Jiang","Contrastive language-image pretraining (CLIP) has demonstrated remarkable
success in various image tasks. However, how to extend CLIP with effective
temporal modeling is still an open and crucial problem. Existing factorized or
joint spatial-temporal modeling trades off between the efficiency and
performance. While modeling temporal information within straight through tube
is widely adopted in literature, we find that simple frame alignment already
provides enough essence without temporal attention. To this end, in this paper,
we proposed a novel Implicit Learnable Alignment (ILA) method, which minimizes
the temporal modeling effort while achieving incredibly high performance.
Specifically, for a frame pair, an interactive point is predicted in each
frame, serving as a mutual information rich region. By enhancing the features
around the interactive point, two frames are implicitly aligned. The aligned
features are then pooled into a single token, which is leveraged in the
subsequent spatial self-attention. Our method allows eliminating the costly or
insufficient temporal self-attention in video. Extensive experiments on
benchmarks demonstrate the superiority and generality of our module.
Particularly, the proposed ILA achieves a top-1 accuracy of 88.7% on
Kinetics-400 with much fewer FLOPs compared with Swin-L and ViViT-H. Code is
released at https://github.com/Francis-Rings/ILA .",2304.10465v2,https://arxiv.org/pdf/2304.10465v2
Quantum Kernel Alignment with Stochastic Gradient Descent,"Gian Gentinetta, David Sutter, Christa Zoufal, Bryce Fuller, Stefan Woerner","Quantum support vector machines have the potential to achieve a quantum
speedup for solving certain machine learning problems. The key challenge for
doing so is finding good quantum kernels for a given data set -- a task called
kernel alignment. In this paper we study this problem using the Pegasos
algorithm, which is an algorithm that uses stochastic gradient descent to solve
the support vector machine optimization problem. We extend Pegasos to the
quantum case and and demonstrate its effectiveness for kernel alignment. Unlike
previous work which performs kernel alignment by training a QSVM within an
outer optimization loop, we show that using Pegasos it is possible to
simultaneously train the support vector machine and align the kernel. Our
experiments show that this approach is capable of aligning quantum feature maps
with high accuracy, and outperforms existing quantum kernel alignment
techniques. Specifically, we demonstrate that Pegasos is particularly effective
for non-stationary data, which is an important challenge in real-world
applications.",2304.09899v1,https://arxiv.org/pdf/2304.09899v1
Fundamental Limitations of Alignment in Large Language Models,"Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua","An important aspect in developing language models that interact with humans
is aligning their behavior to be useful and unharmful for their human users.
This is usually achieved by tuning the model in a way that enhances desired
behaviors and inhibits undesired ones, a process referred to as alignment. In
this paper, we propose a theoretical approach called Behavior Expectation
Bounds (BEB) which allows us to formally investigate several inherent
characteristics and limitations of alignment in large language models.
Importantly, we prove that within the limits of this framework, for any
behavior that has a finite probability of being exhibited by the model, there
exist prompts that can trigger the model into outputting this behavior, with
probability that increases with the length of the prompt. This implies that any
alignment process that attenuates an undesired behavior but does not remove it
altogether, is not safe against adversarial prompting attacks. Furthermore, our
framework hints at the mechanism by which leading alignment approaches such as
reinforcement learning from human feedback make the LLM prone to being prompted
into the undesired behaviors. This theoretical result is being experimentally
demonstrated in large scale by the so called contemporary ""chatGPT jailbreaks"",
where adversarial users trick the LLM into breaking its alignment guardrails by
triggering it into acting as a malicious persona. Our results expose
fundamental limitations in alignment of LLMs and bring to the forefront the
need to devise reliable mechanisms for ensuring AI safety.",2304.11082v6,https://arxiv.org/pdf/2304.11082v6
"Align your Latents: High-Resolution Video Synthesis with Latent
  Diffusion Models","Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis","Latent Diffusion Models (LDMs) enable high-quality image synthesis while
avoiding excessive compute demands by training a diffusion model in a
compressed lower-dimensional latent space. Here, we apply the LDM paradigm to
high-resolution video generation, a particularly resource-intensive task. We
first pre-train an LDM on images only; then, we turn the image generator into a
video generator by introducing a temporal dimension to the latent space
diffusion model and fine-tuning on encoded image sequences, i.e., videos.
Similarly, we temporally align diffusion model upsamplers, turning them into
temporally consistent video super resolution models. We focus on two relevant
real-world applications: Simulation of in-the-wild driving data and creative
content creation with text-to-video modeling. In particular, we validate our
Video LDM on real driving videos of resolution 512 x 1024, achieving
state-of-the-art performance. Furthermore, our approach can easily leverage
off-the-shelf pre-trained image LDMs, as we only need to train a temporal
alignment model in that case. Doing so, we turn the publicly available,
state-of-the-art text-to-image LDM Stable Diffusion into an efficient and
expressive text-to-video model with resolution up to 1280 x 2048. We show that
the temporal layers trained in this way generalize to different fine-tuned
text-to-image LDMs. Utilizing this property, we show the first results for
personalized text-to-video generation, opening exciting directions for future
content creation. Project page:
https://research.nvidia.com/labs/toronto-ai/VideoLDM/",2304.08818v2,https://arxiv.org/pdf/2304.08818v2
"K-means Clustering Based Feature Consistency Alignment for Label-free
  Model Evaluation","Shuyu Miao, Lin Zheng, Jingjing Liu, and Hong Jin","The label-free model evaluation aims to predict the model performance on
various test sets without relying on ground truths. The main challenge of this
task is the absence of labels in the test data, unlike in classical supervised
model evaluation. This paper presents our solutions for the 1st DataCV
Challenge of the Visual Dataset Understanding workshop at CVPR 2023. Firstly,
we propose a novel method called K-means Clustering Based Feature Consistency
Alignment (KCFCA), which is tailored to handle the distribution shifts of
various datasets. KCFCA utilizes the K-means algorithm to cluster labeled
training sets and unlabeled test sets, and then aligns the cluster centers with
feature consistency. Secondly, we develop a dynamic regression model to capture
the relationship between the shifts in distribution and model accuracy.
Thirdly, we design an algorithm to discover the outlier model factors,
eliminate the outlier models, and combine the strengths of multiple autoeval
models. On the DataCV Challenge leaderboard, our approach secured 2nd place
with an RMSE of 6.8526. Our method significantly improved over the best
baseline method by 36\% (6.8526 vs. 10.7378). Furthermore, our method achieves
a relatively more robust and optimal single model performance on the validation
dataset.",2304.09758v1,https://arxiv.org/pdf/2304.09758v1
"Learning to Learn Group Alignment: A Self-Tuning Credo Framework with
  Multiagent Teams","David Radke, Kyle Tilbury","Mixed incentives among a population with multiagent teams has been shown to
have advantages over a fully cooperative system; however, discovering the best
mixture of incentives or team structure is a difficult and dynamic problem. We
propose a framework where individual learning agents self-regulate their
configuration of incentives through various parts of their reward function.
This work extends previous work by giving agents the ability to dynamically
update their group alignment during learning and by allowing teammates to have
different group alignment. Our model builds on ideas from hierarchical
reinforcement learning and meta-learning to learn the configuration of a reward
function that supports the development of a behavioral policy. We provide
preliminary results in a commonly studied multiagent environment and find that
agents can achieve better global outcomes by self-tuning their respective group
alignment parameters.",2304.07337v1,https://arxiv.org/pdf/2304.07337v1
"OpenAssistant Conversations -- Democratizing Large Language Model
  Alignment","Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, Alexander Mattick","Aligning large language models (LLMs) with human preferences has proven to
drastically improve usability and has driven rapid adoption as demonstrated by
ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and
reinforcement learning from human feedback (RLHF) greatly reduce the required
skill and domain knowledge to effectively harness the capabilities of LLMs,
increasing their accessibility and utility across various domains. However,
state-of-the-art alignment techniques like RLHF rely on high-quality human
feedback data, which is expensive to create and often remains proprietary. In
an effort to democratize research on large-scale alignment, we release
OpenAssistant Conversations, a human-generated, human-annotated assistant-style
conversation corpus consisting of 161,443 messages in 35 different languages,
annotated with 461,292 quality ratings, resulting in over 10,000 complete and
fully annotated conversation trees. The corpus is a product of a worldwide
crowd-sourcing effort involving over 13,500 volunteers. Models trained on
OpenAssistant Conversations show consistent improvements on standard benchmarks
over respective base models. We release our code and data under a fully
permissive licence.",2304.07327v2,https://arxiv.org/pdf/2304.07327v2
SEA: A Scalable Entity Alignment System,"Junyang Wu, Tianyi Li, Lu Chen, Yunjun Gao, Ziheng Wei","Entity alignment (EA) aims to find equivalent entities in different knowledge
graphs (KGs). State-of-the-art EA approaches generally use Graph Neural
Networks (GNNs) to encode entities. However, most of them train the models and
evaluate the results in a fullbatch fashion, which prohibits EA from being
scalable on largescale datasets. To enhance the usability of GNN-based EA
models in real-world applications, we present SEA, a scalable entity alignment
system that enables to (i) train large-scale GNNs for EA, (ii) speed up the
normalization and the evaluation process, and (iii) report clear results for
users to estimate different models and parameter settings. SEA can be run on a
computer with merely one graphic card. Moreover, SEA encompasses six
state-of-the-art EA models and provides access for users to quickly establish
and evaluate their own models. Thus, SEA allows users to perform EA without
being involved in tedious implementations, such as negative sampling and
GPU-accelerated evaluation. With SEA, users can gain a clear view of the model
performance. In the demonstration, we show that SEA is user-friendly and is of
high scalability even on computers with limited computational resources.",2304.07065v1,https://arxiv.org/pdf/2304.07065v1
Video alignment using unsupervised learning of local and global features,"Niloufar Fakhfour, Mohammad ShahverdiKondori, Hoda Mohammadzade","In this paper, we tackle the problem of video alignment, the process of
matching the frames of a pair of videos containing similar actions. The main
challenge in video alignment is that accurate correspondence should be
established despite the differences in the execution processes and appearances
between the two videos. We introduce an unsupervised method for alignment that
uses global and local features of the frames. In particular, we introduce
effective features for each video frame using three machine vision tools:
person detection, pose estimation, and VGG network. Then, the features are
processed and combined to construct a multidimensional time series that
represents the video. The resulting time series are used to align videos of the
same actions using a novel version of dynamic time warping named Diagonalized
Dynamic Time Warping(DDTW). The main advantage of our approach is that no
training is required, which makes it applicable for any new type of action
without any need to collect training samples for it. For evaluation, we
considered video synchronization and phase classification tasks on the Penn
action dataset. Also, for an effective evaluation of the video synchronization
task, we present a new metric called Enclosed Area Error(EAE). The results show
that our method outperforms previous state-of-the-art methods, such as TCC, and
other self-supervised and weakly supervised methods.",2304.06841v2,https://arxiv.org/pdf/2304.06841v2
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment,"Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang","Generative foundation models are susceptible to implicit biases that can
arise from extensive unsupervised training data. Such biases can produce
suboptimal samples, skewed outcomes, and unfairness, with potentially serious
consequences. Consequently, aligning these models with human ethics and
preferences is an essential step toward ensuring their responsible and
effective deployment in real-world applications. Prior research has primarily
employed Reinforcement Learning from Human Feedback (RLHF) to address this
problem, where generative models are fine-tuned with RL algorithms guided by a
human-feedback-informed reward model. However, the inefficiencies and
instabilities associated with RL algorithms frequently present substantial
obstacles to the successful alignment, necessitating the development of a more
robust and streamlined approach. To this end, we introduce a new framework,
Reward rAnked FineTuning (RAFT), designed to align generative models
effectively. Utilizing a reward model and a sufficient number of samples, our
approach selects the high-quality samples, discarding those that exhibit
undesired behavior, and subsequently enhancing the model by fine-tuning on
these filtered samples. Our studies show that RAFT can effectively improve the
model performance in both reward learning and other automated metrics in both
large language models and diffusion models.",2304.06767v4,https://arxiv.org/pdf/2304.06767v4
MERMAIDE: Learning to Align Learners using Model-Based Meta-Learning,"Arundhati Banerjee, Soham Phade, Stefano Ermon, Stephan Zheng","We study how a principal can efficiently and effectively intervene on the
rewards of a previously unseen learning agent in order to induce desirable
outcomes. This is relevant to many real-world settings like auctions or
taxation, where the principal may not know the learning behavior nor the
rewards of real people. Moreover, the principal should be few-shot adaptable
and minimize the number of interventions, because interventions are often
costly. We introduce MERMAIDE, a model-based meta-learning framework to train a
principal that can quickly adapt to out-of-distribution agents with different
learning strategies and reward functions. We validate this approach
step-by-step. First, in a Stackelberg setting with a best-response agent, we
show that meta-learning enables quick convergence to the theoretically known
Stackelberg equilibrium at test time, although noisy observations severely
increase the sample complexity. We then show that our model-based meta-learning
approach is cost-effective in intervening on bandit agents with unseen
explore-exploit strategies. Finally, we outperform baselines that use either
meta-learning or agent behavior modeling, in both $0$-shot and $K=1$-shot
settings with partial agent information.",2304.04668v2,https://arxiv.org/pdf/2304.04668v2
Deep Active Alignment of Knowledge Graph Entities and Schemata,"Jiacheng Huang, Zequn Sun, Qijin Chen, Xiaozhou Xu, Weijun Ren, Wei Hu","Knowledge graphs (KGs) store rich facts about the real world. In this paper,
we study KG alignment, which aims to find alignment between not only entities
but also relations and classes in different KGs. Alignment at the entity level
can cross-fertilize alignment at the schema level. We propose a new KG
alignment approach, called DAAKG, based on deep learning and active learning.
With deep learning, it learns the embeddings of entities, relations and
classes, and jointly aligns them in a semi-supervised manner. With active
learning, it estimates how likely an entity, relation or class pair can be
inferred, and selects the best batch for human labeling. We design two
approximation algorithms for efficient solution to batch selection. Our
experiments on benchmark datasets show the superior accuracy and generalization
of DAAKG and validate the effectiveness of all its modules.",2304.04389v3,https://arxiv.org/pdf/2304.04389v3
"Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability
  Analysis against Human Performance",Abdolvahab Khademi,"ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that
are slated to promise different applications in diverse areas. In education,
these AI technologies have been tested for applications in assessment and
teaching. In assessment, AI has long been used in automated essay scoring and
automated item generation. One psychometric property that these tools must have
to assist or replace humans in assessment is high reliability in terms of
agreement between AI scores and human raters. In this paper, we measure the
reliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and
trained humans in perceiving and rating the complexity of writing prompts.
Intraclass correlation (ICC) as a performance metric showed that the
inter-reliability of both the OpenAI ChatGPT and the Google Bard were low
against the gold standard of human ratings.",2304.05372v1,https://arxiv.org/pdf/2304.05372v1
"Toward Practical Entity Alignment Method Design: Insights from New
  Highly Heterogeneous Knowledge Graph Datasets","Xuhui Jiang, Chengjin Xu, Yinghan Shen, Yuanzhuo Wang, Fenglong Su, Fei Sun, Zixuan Li, Zhichao Shi, Jian Guo, Huawei Shen","The flourishing of knowledge graph applications has driven the need for
entity alignment (EA) across KGs. However, the heterogeneity of practical KGs,
characterized by differing scales, structures, and limited overlapping
entities, greatly surpasses that of existing EA datasets. This discrepancy
highlights an oversimplified heterogeneity in current EA datasets, which
obstructs a full understanding of the advancements achieved by recent EA
methods. In this paper, we study the performance of EA methods in practical
settings, specifically focusing on the alignment of highly heterogeneous KGs
(HHKGs). Firstly, we address the oversimplified heterogeneity settings of
current datasets and propose two new HHKG datasets that closely mimic practical
EA scenarios. Then, based on these datasets, we conduct extensive experiments
to evaluate previous representative EA methods. Our findings reveal that, in
aligning HHKGs, valuable structure information can hardly be exploited through
message-passing and aggregation mechanisms. This phenomenon leads to inferior
performance of existing EA methods, especially those based on GNNs. These
findings shed light on the potential problems associated with the conventional
application of GNN-based methods as a panacea for all EA datasets.
Consequently, in light of these observations and to elucidate what EA
methodology is genuinely beneficial in practical scenarios, we undertake an
in-depth analysis by implementing a simple but effective approach: Simple-HHEA.
This method adaptly integrates entity name, structure, and temporal information
to navigate the challenges posed by HHKGs. Our experiment results conclude that
the key to the future EA model design in practice lies in their adaptability
and efficiency to varying information quality conditions, as well as their
capability to capture patterns across HHKGs.",2304.03468v3,https://arxiv.org/pdf/2304.03468v3
Noise-Robust Dense Retrieval via Contrastive Alignment Post Training,"Daniel Campos, ChengXiang Zhai, Alessandro Magnani","The success of contextual word representations and advances in neural
information retrieval have made dense vector-based retrieval a standard
approach for passage and document ranking. While effective and efficient,
dual-encoders are brittle to variations in query distributions and noisy
queries. Data augmentation can make models more robust but introduces overhead
to training set generation and requires retraining and index regeneration. We
present Contrastive Alignment POst Training (CAPOT), a highly efficient
finetuning method that improves model robustness without requiring index
regeneration, the training set optimization, or alteration. CAPOT enables
robust retrieval by freezing the document encoder while the query encoder
learns to align noisy queries with their unaltered root. We evaluate CAPOT
noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,
finding CAPOT has a similar impact as data augmentation with none of its
overhead.",2304.03401v2,https://arxiv.org/pdf/2304.03401v2
"ViralVectors: Compact and Scalable Alignment-free Virome Feature
  Generation","Sarwan Ali, Prakash Chourasia, Zahra Tayebi, Babatunde Bello, Murray Patterson","The amount of sequencing data for SARS-CoV-2 is several orders of magnitude
larger than any virus. This will continue to grow geometrically for SARS-CoV-2,
and other viruses, as many countries heavily finance genomic surveillance
efforts. Hence, we need methods for processing large amounts of sequence data
to allow for effective yet timely decision-making. Such data will come from
heterogeneous sources: aligned, unaligned, or even unassembled raw nucleotide
or amino acid sequencing reads pertaining to the whole genome or regions (e.g.,
spike) of interest. In this work, we propose \emph{ViralVectors}, a compact
feature vector generation from virome sequencing data that allows effective
downstream analysis. Such generation is based on \emph{minimizers}, a type of
lightweight ""signature"" of a sequence, used traditionally in assembly and read
mapping -- to our knowledge, the first use minimizers in this way. We validate
our approach on different types of sequencing data: (a) 2.5M SARS-CoV-2 spike
sequences (to show scalability); (b) 3K Coronaviridae spike sequences (to show
robustness to more genomic variability); and (c) 4K raw WGS reads sets taken
from nasal-swab PCR tests (to show the ability to process unassembled reads).
Our results show that ViralVectors outperforms current benchmarks in most
classification and clustering tasks.",2304.02891v2,https://arxiv.org/pdf/2304.02891v2
NTK-SAP: Improving neural network pruning by aligning training dynamics,"Yite Wang, Dawei Li, Ruoyu Sun","Pruning neural networks before training has received increasing interest due
to its potential to reduce training time and memory. One popular method is to
prune the connections based on a certain metric, but it is not entirely clear
what metric is the best choice. Recent advances in neural tangent kernel (NTK)
theory suggest that the training dynamics of large enough neural networks is
closely related to the spectrum of the NTK. Motivated by this finding, we
propose to prune the connections that have the least influence on the spectrum
of the NTK. This method can help maintain the NTK spectrum, which may help
align the training dynamics to that of its dense counterpart. However, one
possible issue is that the fixed-weight-NTK corresponding to a given initial
point can be very different from the NTK corresponding to later iterates during
the training phase. We further propose to sample multiple realizations of
random weights to estimate the NTK spectrum. Note that our approach is
weight-agnostic, which is different from most existing methods that are
weight-dependent. In addition, we use random inputs to compute the
fixed-weight-NTK, making our method data-agnostic as well. We name our
foresight pruning algorithm Neural Tangent Kernel Spectrum-Aware Pruning
(NTK-SAP). Empirically, our method achieves better performance than all
baselines on multiple datasets.",2304.02840v1,https://arxiv.org/pdf/2304.02840v1
"Learning Invariant Representation via Contrastive Feature Alignment for
  Clutter Robust SAR Target Recognition","Bowen Peng, Jianyue Xie, Bo Peng, Li Liu","The deep neural networks (DNNs) have freed the synthetic aperture radar
automatic target recognition (SAR ATR) from expertise-based feature designing
and demonstrated superiority over conventional solutions. There has been shown
the unique deficiency of ground vehicle benchmarks in shapes of strong
background correlation results in DNNs overfitting the clutter and being
non-robust to unfamiliar surroundings. However, the gap between fixed
background model training and varying background application remains
underexplored. Inspired by contrastive learning, this letter proposes a
solution called Contrastive Feature Alignment (CFA) aiming to learn invariant
representation for robust recognition. The proposed method contributes a mixed
clutter variants generation strategy and a new inference branch equipped with
channel-weighted mean square error (CWMSE) loss for invariant representation
learning. In specific, the generation strategy is delicately designed to better
attract clutter-sensitive deviation in feature space. The CWMSE loss is further
devised to better contrast this deviation and align the deep features activated
by the original images and corresponding clutter variants. The proposed CFA
combines both classification and CWMSE losses to train the model jointly, which
allows for the progressive learning of invariant target representation.
Extensive evaluations on the MSTAR dataset and six DNN models prove the
effectiveness of our proposal. The results demonstrated that the CFA-trained
models are capable of recognizing targets among unfamiliar surroundings that
are not included in the dataset, and are robust to varying signal-to-clutter
ratios.",2304.01747v1,https://arxiv.org/pdf/2304.01747v1
"Learning with augmented target information: An alternative theory of
  Feedback Alignment","Huzi Cheng, Joshua W. Brown","While error backpropagation (BP) has dominated the training of nearly all
modern neural networks for a long time, it suffers from several biological
plausibility issues such as the symmetric weight requirement and synchronous
updates. Feedback Alignment (FA) was proposed as an alternative to BP to
address those dilemmas and has been demonstrated to be effective on various
tasks and network architectures. Despite its simplicity and effectiveness, a
satisfying explanation of how FA works across different architectures is still
lacking. Here we propose a novel, architecture-agnostic theory of how FA works
through the lens of information theory: Instead of approximating gradients
calculated by BP with the same parameter, FA learns effective representations
by embedding target information into neural networks to be trained. We show
this through the analysis of FA dynamics in idealized settings and then via a
series of experiments. Based on the implications of this theory, we designed
three variants of FA and show their comparable performance on several tasks.
These variants also account for some phenomena and theories in neuroscience
such as predictive coding and representational drift.",2304.01406v1,https://arxiv.org/pdf/2304.01406v1
"Efficiently Aligned Cross-Lingual Transfer Learning for Conversational
  Tasks using Prompt-Tuning","Lifu Tu, Jin Qu, Semih Yavuz, Shafiq Joty, Wenhao Liu, Caiming Xiong, Yingbo Zhou","Cross-lingual transfer of language models trained on high-resource languages
like English has been widely studied for many NLP tasks, but focus on
conversational tasks has been rather limited. This is partly due to the high
cost of obtaining non-English conversational data, which results in limited
coverage. In this work, we introduce XSGD for cross-lingual alignment
pretraining, a parallel and large-scale multilingual conversation dataset that
we created by translating the English-only Schema-Guided Dialogue (SGD) dataset
(Rastogi et al., 2020) into 105 other languages. XSGD contains approximately
330k utterances per language. To facilitate aligned cross-lingual
representations, we develop an efficient prompt-tuning-based method for
learning alignment prompts. We also investigate two different classifiers:
NLI-based and vanilla classifiers, and test cross-lingual capability enabled by
the aligned prompts. We evaluate our model's cross-lingual generalization
capabilities on two conversation tasks: slot-filling and intent classification.
Our results demonstrate the strong and efficient modeling ability of NLI-based
classifiers and the large cross-lingual transfer improvements achieved by our
aligned prompts, particularly in few-shot settings. In addition, we highlight
the nice results of our approach compared to LLMs such as text-davinci-003 and
ChatGPT in both zero-shot and few-shot settings. While LLMs exhibit impressive
performance in English, their cross-lingual capabilities in other languages,
particularly low-resource languages, are limited.",2304.01295v4,https://arxiv.org/pdf/2304.01295v4
To be Robust and to be Fair: Aligning Fairness with Robustness,"Junyi Chai, Xiaoqian Wang","Adversarial training has been shown to be reliable in improving robustness
against adversarial samples. However, the problem of adversarial training in
terms of fairness has not yet been properly studied, and the relationship
between fairness and accuracy attack still remains unclear. Can we
simultaneously improve robustness w.r.t. both fairness and accuracy? To tackle
this topic, in this paper, we study the problem of adversarial training and
adversarial attack w.r.t. both metrics. We propose a unified structure for
fairness attack which brings together common notions in group fairness, and we
theoretically prove the equivalence of fairness attack against different
notions. Moreover, we show the alignment of fairness and accuracy attack, and
theoretically demonstrate that robustness w.r.t. one metric benefits from
robustness w.r.t. the other metric. Our study suggests a novel way to unify
adversarial training and attack w.r.t. fairness and accuracy, and experimental
results show that our proposed method achieves better performance in terms of
robustness w.r.t. both metrics.",2304.00061v1,https://arxiv.org/pdf/2304.00061v1
"Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler
  Alignment of Embeddings for Asymmetrical dual encoders","Daniel Campos, Alessandro Magnani, ChengXiang Zhai","In this paper, we consider the problem of improving the inference latency of
language model-based dense retrieval systems by introducing structural
compression and model size asymmetry between the context and query encoders.
First, we investigate the impact of pre and post-training compression on the
MSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that
asymmetry in the dual encoders in dense retrieval can lead to improved
inference efficiency. Knowing this, we introduce Kullback Leibler Alignment of
Embeddings (KALE), an efficient and accurate method for increasing the
inference efficiency of dense retrieval methods by pruning and aligning the
query encoder after training. Specifically, KALE extends traditional Knowledge
Distillation after bi-encoder training, allowing for effective query encoder
compression without full retraining or index generation. Using KALE and
asymmetric training, we can generate models which exceed the performance of
DistilBERT despite having 3x faster inference.",2304.01016v3,https://arxiv.org/pdf/2304.01016v3
"Aligning a medium-size GPT model in English to a small closed domain in
  Spanish","Oscar R. Navarrete-Parra, Victor Uc-Cetina, Jorge Reyes-Magana","In this paper, we propose a methodology to align a medium-sized GPT model,
originally trained in English for an open domain, to a small closed domain in
Spanish. The application for which the model is finely tuned is the question
answering task. To achieve this we also needed to train and implement another
neural network (which we called the reward model) that could score and
determine whether an answer is appropriate for a given question. This component
served to improve the decoding and generation of the answers of the system.
Numerical metrics such as BLEU and perplexity were used to evaluate the model,
and human judgment was also used to compare the decoding technique with others.
Finally, the results favored the proposed method, and it was determined that it
is feasible to use a reward model to align the generation of responses.",2303.17649v3,https://arxiv.org/pdf/2303.17649v3
SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger,"Yuting Gao, Jinfeng Liu, Zihan Xu, Tong Wu Enwei Zhang, Wei Liu, Jie Yang, Ke Li, Xing Sun","During the preceding biennium, vision-language pre-training has achieved
noteworthy success on several downstream tasks. Nevertheless, acquiring
high-quality image-text pairs, where the pairs are entirely exclusive of each
other, remains a challenging task, and noise exists in the commonly used
datasets. To address this issue, we propose SoftCLIP, a novel approach that
relaxes the strict one-to-one constraint and achieves a soft cross-modal
alignment by introducing a softened target, which is generated from the
fine-grained intra-modal self-similarity. The intra-modal guidance is
indicative to enable two pairs have some local similarities and model
many-to-many relationships between the two modalities. Besides, since the
positive still dominates in the softened target distribution, we disentangle
the negatives in the distribution to further boost the relation alignment with
the negatives in the cross-modal learning. Extensive experiments demonstrate
the effectiveness of SoftCLIP. In particular, on ImageNet zero-shot
classification task, using CC3M/CC12M as pre-training dataset, SoftCLIP brings
a top-1 accuracy improvement of 6.8%/7.2% over the CLIP baseline.",2303.17561v2,https://arxiv.org/pdf/2303.17561v2
G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment,"Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu","The quality of texts generated by natural language generation (NLG) systems
is hard to measure automatically. Conventional reference-based metrics, such as
BLEU and ROUGE, have been shown to have relatively low correlation with human
judgments, especially for tasks that require creativity and diversity. Recent
studies suggest using large language models (LLMs) as reference-free metrics
for NLG evaluation, which have the benefit of being applicable to new tasks
that lack human references. However, these LLM-based evaluators still have
lower human correspondence than medium-size neural evaluators. In this work, we
present G-Eval, a framework of using large language models with
chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of
NLG outputs. We experiment with two generation tasks, text summarization and
dialogue generation. We show that G-Eval with GPT-4 as the backbone model
achieves a Spearman correlation of 0.514 with human on summarization task,
outperforming all previous methods by a large margin. We also propose
preliminary analysis on the behavior of LLM-based evaluators, and highlight the
potential issue of LLM-based evaluators having a bias towards the LLM-generated
texts. The code is at https://github.com/nlpyang/geval",2303.16634v3,https://arxiv.org/pdf/2303.16634v3
ASIC: Aligning Sparse in-the-wild Image Collections,"Kamal Gupta, Varun Jampani, Carlos Esteves, Abhinav Shrivastava, Ameesh Makadia, Noah Snavely, Abhishek Kar","We present a method for joint alignment of sparse in-the-wild image
collections of an object category. Most prior works assume either ground-truth
keypoint annotations or a large dataset of images of a single object category.
However, neither of the above assumptions hold true for the long-tail of the
objects present in the world. We present a self-supervised technique that
directly optimizes on a sparse collection of images of a particular
object/object category to obtain consistent dense correspondences across the
collection. We use pairwise nearest neighbors obtained from deep features of a
pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches
and make them dense and accurate matches by optimizing a neural network that
jointly maps the image collection into a learned canonical grid. Experiments on
CUB and SPair-71k benchmarks demonstrate that our method can produce globally
consistent and higher quality correspondences across the image collection when
compared to existing self-supervised methods. Code and other material will be
made available at \url{https://kampta.github.io/asic}.",2303.16201v1,https://arxiv.org/pdf/2303.16201v1
"Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and
  Prototype Alignment","Jiaqi Jin, Siwei Wang, Zhibin Dong, Xinwang Liu, En Zhu","The success of existing multi-view clustering relies on the assumption of
sample integrity across multiple views. However, in real-world scenarios,
samples of multi-view are partially available due to data corruption or sensor
failure, which leads to incomplete multi-view clustering study (IMVC). Although
several attempts have been proposed to address IMVC, they suffer from the
following drawbacks: i) Existing methods mainly adopt cross-view contrastive
learning forcing the representations of each sample across views to be exactly
the same, which might ignore view discrepancy and flexibility in
representations; ii) Due to the absence of non-observed samples across multiple
views, the obtained prototypes of clusters might be unaligned and biased,
leading to incorrect fusion. To address the above issues, we propose a
Cross-view Partial Sample and Prototype Alignment Network (CPSPAN) for Deep
Incomplete Multi-view Clustering. Firstly, unlike existing contrastive-based
methods, we adopt pair-observed data alignment as 'proxy supervised signals' to
guide instance-to-instance correspondence construction among views. Then,
regarding of the shifted prototypes in IMVC, we further propose a prototype
alignment module to achieve incomplete distribution calibration across views.
Extensive experimental results showcase the effectiveness of our proposed
modules, attaining noteworthy performance improvements when compared to
existing IMVC competitors on benchmark datasets.",2303.15689v2,https://arxiv.org/pdf/2303.15689v2
"Human Preference Score: Better Aligning Text-to-Image Models with Human
  Preference","Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li","Recent years have witnessed a rapid growth of deep generative models, with
text-to-image models gaining significant attention from the public. However,
existing models often generate images that do not align well with human
preferences, such as awkward combinations of limbs and facial expressions. To
address this issue, we collect a dataset of human choices on generated images
from the Stable Foundation Discord channel. Our experiments demonstrate that
current evaluation metrics for generative models do not correlate well with
human choices. Thus, we train a human preference classifier with the collected
dataset and derive a Human Preference Score (HPS) based on the classifier.
Using HPS, we propose a simple yet effective method to adapt Stable Diffusion
to better align with human preferences. Our experiments show that HPS
outperforms CLIP in predicting human choices and has good generalization
capability toward images generated from other models. By tuning Stable
Diffusion with the guidance of HPS, the adapted model is able to generate
images that are more preferred by human users. The project page is available
here: https://tgxs002.github.io/align_sd_web/ .",2303.14420v2,https://arxiv.org/pdf/2303.14420v2
Three ways to improve feature alignment for open vocabulary detection,"Relja Arandjelović, Alex Andonian, Arthur Mensch, Olivier J. Hénaff, Jean-Baptiste Alayrac, Andrew Zisserman","The core problem in zero-shot open vocabulary detection is how to align
visual and text features, so that the detector performs well on unseen classes.
Previous approaches train the feature pyramid and detection head from scratch,
which breaks the vision-text feature alignment established during pretraining,
and struggles to prevent the language model from forgetting unseen classes.
  We propose three methods to alleviate these issues. Firstly, a simple scheme
is used to augment the text embeddings which prevents overfitting to a small
number of classes seen during training, while simultaneously saving memory and
computation. Secondly, the feature pyramid network and the detection head are
modified to include trainable gated shortcuts, which encourages vision-text
feature alignment and guarantees it at the start of detection training.
Finally, a self-training approach is used to leverage a larger corpus of
image-text pairs thus improving detection performance on classes with no human
annotated bounding boxes.
  Our three methods are evaluated on the zero-shot version of the LVIS
benchmark, each of them showing clear and significant benefits. Our final
network achieves the new stateof-the-art on the mAP-all metric and demonstrates
competitive performance for mAP-rare, as well as superior transfer to COCO and
Objects365.",2303.13518v1,https://arxiv.org/pdf/2303.13518v1
"DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning
  Inverse Gram Matrices","Ismail Nejjar, Qin Wang, Olga Fink","Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap
between a labeled source dataset and an unlabelled target dataset for
regression problems. Recent works mostly focus on learning a deep feature
encoder by minimizing the discrepancy between source and target features. In
this work, we present a different perspective for the DAR problem by analyzing
the closed-form ordinary least square~(OLS) solution to the linear regressor in
the deep domain adaptation context. Rather than aligning the original feature
embedding space, we propose to align the inverse Gram matrix of the features,
which is motivated by its presence in the OLS solution and the Gram matrix's
ability to capture the feature correlations. Specifically, we propose a simple
yet effective DAR method which leverages the pseudo-inverse low-rank property
to align the scale and angle in a selected subspace generated by the
pseudo-inverse Gram matrix of the two domains. We evaluate our method on three
domain adaptation regression benchmarks. Experimental results demonstrate that
our method achieves state-of-the-art performance. Our code is available at
https://github.com/ismailnejjar/DARE-GRAM.",2303.13325v1,https://arxiv.org/pdf/2303.13325v1
"Deep Declarative Dynamic Time Warping for End-to-End Learning of
  Alignment Paths","Ming Xu, Sourav Garg, Michael Milford, Stephen Gould","This paper addresses learning end-to-end models for time series data that
include a temporal alignment step via dynamic time warping (DTW). Existing
approaches to differentiable DTW either differentiate through a fixed warping
path or apply a differentiable relaxation to the min operator found in the
recursive steps used to solve the DTW problem. We instead propose a DTW layer
based around bi-level optimisation and deep declarative networks, which we name
DecDTW. By formulating DTW as a continuous, inequality constrained optimisation
problem, we can compute gradients for the solution of the optimal alignment
(with respect to the underlying time series) using implicit differentiation. An
interesting byproduct of this formulation is that DecDTW outputs the optimal
warping path between two time series as opposed to a soft approximation,
recoverable from Soft-DTW. We show that this property is particularly useful
for applications where downstream loss functions are defined on the optimal
alignment path itself. This naturally occurs, for instance, when learning to
improve the accuracy of predicted alignments against ground truth alignments.
We evaluate DecDTW on two such applications, namely the audio-to-score
alignment task in music information retrieval and the visual place recognition
task in robotics, demonstrating state-of-the-art results in both.",2303.10778v1,https://arxiv.org/pdf/2303.10778v1
"On the Effects of Self-supervision and Contrastive Alignment in Deep
  Multi-view Clustering","Daniel J. Trosten, Sigurd Løkse, Robert Jenssen, Michael C. Kampffmeyer","Self-supervised learning is a central component in recent approaches to deep
multi-view clustering (MVC). However, we find large variations in the
development of self-supervision-based methods for deep MVC, potentially slowing
the progress of the field. To address this, we present DeepMVC, a unified
framework for deep MVC that includes many recent methods as instances. We
leverage our framework to make key observations about the effect of
self-supervision, and in particular, drawbacks of aligning representations with
contrastive learning. Further, we prove that contrastive alignment can
negatively influence cluster separability, and that this effect becomes worse
when the number of views increases. Motivated by our findings, we develop
several new DeepMVC instances with new forms of self-supervision. We conduct
extensive experiments and find that (i) in line with our theoretical findings,
contrastive alignments decreases performance on datasets with many views; (ii)
all methods benefit from some form of self-supervision; and (iii) our new
instances outperform previous methods on several datasets. Based on our
results, we suggest several promising directions for future research. To
enhance the openness of the field, we provide an open-source implementation of
DeepMVC, including recent models and our new instances. Our implementation
includes a consistent evaluation protocol, facilitating fair and accurate
evaluation of methods and components.",2303.09877v1,https://arxiv.org/pdf/2303.09877v1
Automatic Geo-alignment of Artwork in Children's Story Books,"Jakub J. Dylag, Victor Suarez, James Wald, Aneesha Amodini Uvara","A study was conducted to prove AI software could be used to translate and
generate illustrations without any human intervention. This was done with the
purpose of showing and distributing it to the external customer, Pratham Books.
The project aligns with the company's vision by leveraging the generalisation
and scalability of Machine Learning algorithms, offering significant cost
efficiency increases to a wide range of literary audiences in varied
geographical locations. A comparative study methodology was utilised to
determine the best performant method out of the 3 devised, Prompt Augmentation
using Keywords, CLIP Embedding Mask, and Cross Attention Control with Editorial
Prompts. A thorough evaluation process was completed using both quantitative
and qualitative measures. Each method had its own strengths and weaknesses, but
through the evaluation, method 1 was found to have the best yielding results.
Promising future advancements may be made to further increase image quality by
incorporating Large Language Models and personalised stylistic models. The
presented approach can also be adapted to Video and 3D sculpture generation for
novel illustrations in digital webbooks.",2304.01204v1,https://arxiv.org/pdf/2304.01204v1
Patch-Prompt Aligned Bayesian Prompt Tuning for Vision-Language Models,"Xinyang Liu, Dongsheng Wang, Bowei Fang, Miaoge Li, Zhibin Duan, Yishi Xu, Bo Chen, Mingyuan Zhou","For downstream applications of vision-language pre-trained models, there has
been significant interest in constructing effective prompts. Existing works on
prompt engineering, which either require laborious manual designs or optimize
the prompt tuning as a point estimation problem, may fail to describe diverse
characteristics of categories and limit their applications. We introduce a
Bayesian probabilistic resolution to prompt tuning, where the label-specific
stochastic prompts are generated hierarchically by first sampling a latent
vector from an underlying distribution and then employing a lightweight
generative model. Importantly, we semantically regularize the tuning process by
minimizing the statistical distance between the visual patches and linguistic
prompts, which pushes the stochastic label representations to faithfully
capture diverse visual concepts, instead of overfitting the training
categories. We evaluate the effectiveness of our approach on four tasks:
few-shot image recognition, base-to-new generalization, dataset transfer
learning, and domain shifts. Extensive results over 15 datasets show promising
transferability and generalization performance of our proposed model, both
quantitatively and qualitatively.",2303.09100v2,https://arxiv.org/pdf/2303.09100v2
"Bi-directional Distribution Alignment for Transductive Zero-Shot
  Learning","Zhicai Wang, Yanbin Hao, Tingting Mu, Ouxiang Li, Shuo Wang, Xiangnan He","It is well-known that zero-shot learning (ZSL) can suffer severely from the
problem of domain shift, where the true and learned data distributions for the
unseen classes do not match. Although transductive ZSL (TZSL) attempts to
improve this by allowing the use of unlabelled examples from the unseen
classes, there is still a high level of distribution shift. We propose a novel
TZSL model (named as Bi-VAEGAN), which largely improves the shift by a
strengthened distribution alignment between the visual and auxiliary spaces.
The key proposal of the model design includes (1) a bi-directional distribution
alignment, (2) a simple but effective L_2-norm based feature normalization
approach, and (3) a more sophisticated unseen class prior estimation approach.
In benchmark evaluation using four datasets, Bi-VAEGAN achieves the new state
of the arts under both the standard and generalized TZSL settings. Code could
be found at https://github.com/Zhicaiwww/Bi-VAEGAN",2303.08698v2,https://arxiv.org/pdf/2303.08698v2
GANN: Graph Alignment Neural Network for Semi-Supervised Learning,"Linxuan Song, Wenxuan Tu, Sihang Zhou, Xinwang Liu, En Zhu","Graph neural networks (GNNs) have been widely investigated in the field of
semi-supervised graph machine learning. Most methods fail to exploit adequate
graph information when labeled data is limited, leading to the problem of
oversmoothing. To overcome this issue, we propose the Graph Alignment Neural
Network (GANN), a simple and effective graph neural architecture. A unique
learning algorithm with three alignment rules is proposed to thoroughly explore
hidden information for insufficient labels. Firstly, to better investigate
attribute specifics, we suggest the feature alignment rule to align the inner
product of both the attribute and embedding matrices. Secondly, to properly
utilize the higher-order neighbor information, we propose the cluster center
alignment rule, which involves aligning the inner product of the cluster center
matrix with the unit matrix. Finally, to get reliable prediction results with
few labels, we establish the minimum entropy alignment rule by lining up the
prediction probability matrix with its sharpened result. Extensive studies on
graph benchmark datasets demonstrate that GANN can achieve considerable
benefits in semi-supervised node classification and outperform state-of-the-art
competitors.",2303.07778v1,https://arxiv.org/pdf/2303.07778v1
"ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and
  Multilingual Natural Language Generation","Bang Yang, Fenglin Liu, Yuexian Zou, Xian Wu, Yaowei Wang, David A. Clifton","Natural Language Generation (NLG) accepts input data in the form of images,
videos, or text and generates corresponding natural language text as output.
Existing NLG methods mainly adopt a supervised approach and rely heavily on
coupled data-to-text pairs. However, for many targeted scenarios and for
non-English languages, sufficient quantities of labeled data are often not
available. To relax the dependency on labeled data of downstream tasks, we
propose an intuitive and effective zero-shot learning framework, ZeroNLG, which
can deal with multiple NLG tasks, including image-to-text (image captioning),
video-to-text (video captioning), and text-to-text (neural machine
translation), across English, Chinese, German, and French within a unified
framework. ZeroNLG does not require any labeled downstream pairs for training.
During training, ZeroNLG (i) projects different domains (across modalities and
languages) to corresponding coordinates in a shared common latent space; (ii)
bridges different domains by aligning their corresponding coordinates in this
space; and (iii) builds an unsupervised multilingual auto-encoder to learn to
generate text by reconstructing the input text given its coordinate in shared
latent space. Consequently, during inference, based on the data-to-text
pipeline, ZeroNLG can generate target sentences across different languages
given the coordinate of input data in the common space. Within this unified
framework, given visual (imaging or video) data as input, ZeroNLG can perform
zero-shot visual captioning; given textual sentences as input, ZeroNLG can
perform zero-shot machine translation. We present the results of extensive
experiments on twelve NLG tasks, showing that, without using any labeled
downstream pairs for training, ZeroNLG generates high-quality and believable
outputs and significantly outperforms existing zero-shot methods.",2303.06458v3,https://arxiv.org/pdf/2303.06458v3
StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces,"Shuai Yang, Liming Jiang, Ziwei Liu, Chen Change Loy","Recent advances in face manipulation using StyleGAN have produced impressive
results. However, StyleGAN is inherently limited to cropped aligned faces at a
fixed image resolution it is pre-trained on. In this paper, we propose a simple
and effective solution to this limitation by using dilated convolutions to
rescale the receptive fields of shallow layers in StyleGAN, without altering
any model parameters. This allows fixed-size small features at shallow layers
to be extended into larger ones that can accommodate variable resolutions,
making them more robust in characterizing unaligned faces. To enable real face
inversion and manipulation, we introduce a corresponding encoder that provides
the first-layer feature of the extended StyleGAN in addition to the latent
style code. We validate the effectiveness of our method using unaligned face
inputs of various resolutions in a diverse set of face manipulation tasks,
including facial attribute editing, super-resolution, sketch/mask-to-face
translation, and face toonification.",2303.06146v2,https://arxiv.org/pdf/2303.06146v2
"CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language
  Recognition with Variational Alignment","Jiangbin Zheng, Yile Wang, Cheng Tan, Siyuan Li, Ge Wang, Jun Xia, Yidong Chen, Stan Z. Li","Sign language recognition (SLR) is a weakly supervised task that annotates
sign videos as textual glosses. Recent studies show that insufficient training
caused by the lack of large-scale available sign datasets becomes the main
bottleneck for SLR. Most SLR works thereby adopt pretrained visual modules and
develop two mainstream solutions. The multi-stream architectures extend
multi-cue visual features, yielding the current SOTA performances but requiring
complex designs and might introduce potential noise. Alternatively, the
advanced single-cue SLR frameworks using explicit cross-modal alignment between
visual and textual modalities are simple and effective, potentially competitive
with the multi-cue framework. In this work, we propose a novel contrastive
visual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained
knowledge of both the visual and language modalities. Based on the single-cue
cross-modal alignment framework, we propose a variational autoencoder (VAE) for
pretrained contextual knowledge while introducing the complete pretrained
language module. The VAE implicitly aligns visual and textual modalities while
benefiting from pretrained contextual knowledge as the traditional contextual
module. Meanwhile, a contrastive cross-modal alignment algorithm is designed to
explicitly enhance the consistency constraints. Extensive experiments on public
datasets (PHOENIX-2014 and PHOENIX-2014T) demonstrate that our proposed CVT-SLR
consistently outperforms existing single-cue methods and even outperforms SOTA
multi-cue methods.",2303.05725v4,https://arxiv.org/pdf/2303.05725v4
"SLCA: Slow Learner with Classifier Alignment for Continual Learning on a
  Pre-trained Model","Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, Yunchao Wei","The goal of continual learning is to improve the performance of recognition
models in learning sequentially arrived data. Although most existing works are
established on the premise of learning from scratch, growing efforts have been
devoted to incorporating the benefits of pre-training. However, how to
adaptively exploit the pre-trained knowledge for each incremental task while
maintaining its generalizability remains an open question. In this work, we
present an extensive analysis for continual learning on a pre-trained model
(CLPM), and attribute the key challenge to a progressive overfitting problem.
Observing that selectively reducing the learning rate can almost resolve this
issue in the representation layer, we propose a simple but extremely effective
approach named Slow Learner with Classifier Alignment (SLCA), which further
improves the classification layer by modeling the class-wise distributions and
aligning the classification layers in a post-hoc fashion. Across a variety of
scenarios, our proposal provides substantial improvements for CLPM (e.g., up to
49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split
CUB-200 and Split Cars-196, respectively), and thus outperforms
state-of-the-art approaches by a large margin. Based on such a strong baseline,
critical factors and promising directions are analyzed in-depth to facilitate
subsequent research. Code has been made available at:
https://github.com/GengDavid/SLCA.",2303.05118v4,https://arxiv.org/pdf/2303.05118v4
"Finding Alignments Between Interpretable Causal Variables and
  Distributed Neural Representations","Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, Noah D. Goodman","Causal abstraction is a promising theoretical framework for explainable
artificial intelligence that defines when an interpretable high-level causal
model is a faithful simplification of a low-level deep learning system.
However, existing causal abstraction methods have two major limitations: they
require a brute-force search over alignments between the high-level model and
the low-level one, and they presuppose that variables in the high-level model
will align with disjoint sets of neurons in the low-level one. In this paper,
we present distributed alignment search (DAS), which overcomes these
limitations. In DAS, we find the alignment between high-level and low-level
models using gradient descent rather than conducting a brute-force search, and
we allow individual neurons to play multiple distinct roles by analyzing
representations in non-standard bases-distributed representations. Our
experiments show that DAS can discover internal structure that prior approaches
miss. Overall, DAS removes previous obstacles to conducting causal abstraction
analyses and allows us to find conceptual structure in trained neural nets.",2303.02536v4,https://arxiv.org/pdf/2303.02536v4
"AERK: Aligned Entropic Reproducing Kernels through Continuous-time
  Quantum Walks","Lixin Cui, Ming Li, Yue Wang, Lu Bai, Edwin R. Hancock","In this work, we develop an Aligned Entropic Reproducing Kernel (AERK) for
graph classification. We commence by performing the Continuous-time Quantum
Walk (CTQW) on each graph structure, and computing the Averaged Mixing Matrix
(AMM) to describe how the CTQW visit all vertices from a starting vertex. More
specifically, we show how this AMM matrix allows us to compute a quantum
Shannon entropy for each vertex of a graph. For pairwise graphs, the proposed
AERK kernel is defined by computing a reproducing kernel based similarity
between the quantum Shannon entropies of their each pair of aligned vertices.
The analysis of theoretical properties reveals that the proposed AERK kernel
cannot only address the shortcoming of neglecting the structural correspondence
information between graphs arising in most existing R-convolution graph
kernels, but also overcome the problem of neglecting the structural differences
between pairs of aligned vertices arising in existing vertex-based matching
kernels. Moreover, unlike existing classical graph kernels that only focus on
the global or local structural information of graphs, the proposed AERK kernel
can simultaneously capture both global and local structural information through
the quantum Shannon entropies, reflecting more precise kernel based similarity
measures between pairs of graphs. The above theoretical properties explain the
effectiveness of the proposed kernel. The experimental evaluation on standard
graph datasets demonstrates that the proposed AERK kernel is able to outperform
state-of-the-art graph kernels for graph classification tasks.",2303.03396v1,https://arxiv.org/pdf/2303.03396v1
RAFEN -- Regularized Alignment Framework for Embeddings of Nodes,"Kamil Tagowski, Piotr Bielak, Jakub Binkowski, Tomasz Kajdanowicz","Learning representations of nodes has been a crucial area of the graph
machine learning research area. A well-defined node embedding model should
reflect both node features and the graph structure in the final embedding. In
the case of dynamic graphs, this problem becomes even more complex as both
features and structure may change over time. The embeddings of particular nodes
should remain comparable during the evolution of the graph, what can be
achieved by applying an alignment procedure. This step was often applied in
existing works after the node embedding was already computed. In this paper, we
introduce a framework -- RAFEN -- that allows to enrich any existing node
embedding method using the aforementioned alignment term and learning aligned
node embedding during training time. We propose several variants of our
framework and demonstrate its performance on six real-world datasets. RAFEN
achieves on-par or better performance than existing approaches without
requiring additional processing steps.",2303.01926v2,https://arxiv.org/pdf/2303.01926v2
Aligning benchmark datasets for table structure recognition,"Brandon Smock, Rohith Pesala, Robin Abraham","Benchmark datasets for table structure recognition (TSR) must be carefully
processed to ensure they are annotated consistently. However, even if a
dataset's annotations are self-consistent, there may be significant
inconsistency across datasets, which can harm the performance of models trained
and evaluated on them. In this work, we show that aligning these
benchmarks$\unicode{x2014}$removing both errors and inconsistency between
them$\unicode{x2014}$improves model performance significantly. We demonstrate
this through a data-centric approach where we adopt one model architecture, the
Table Transformer (TATR), that we hold fixed throughout. Baseline exact match
accuracy for TATR evaluated on the ICDAR-2013 benchmark is 65% when trained on
PubTables-1M, 42% when trained on FinTabNet, and 69% combined. After reducing
annotation mistakes and inter-dataset inconsistency, performance of TATR
evaluated on ICDAR-2013 increases substantially to 75% when trained on
PubTables-1M, 65% when trained on FinTabNet, and 81% combined. We show through
ablations over the modification steps that canonicalization of the table
annotations has a significantly positive effect on performance, while other
choices balance necessary trade-offs that arise when deciding a benchmark
dataset's final composition. Overall we believe our work has significant
implications for benchmark design for TSR and potentially other tasks as well.
Dataset processing and training code will be released at
https://github.com/microsoft/table-transformer.",2303.00716v2,https://arxiv.org/pdf/2303.00716v2
Safety without alignment,"András Kornai, Michael Bukatin, Zsolt Zombori","Currently, the dominant paradigm in AI safety is alignment with human values.
Here we describe progress on developing an alternative approach to safety,
based on ethical rationalism (Gewirth:1978), and propose an inherently safe
implementation path via hybrid theorem provers in a sandbox. As AGIs evolve,
their alignment may fade, but their rationality can only increase (otherwise
more rational ones will have a significant evolutionary advantage) so an
approach that ties their ethics to their rationality has clear long-term
advantages.",2303.00752v2,https://arxiv.org/pdf/2303.00752v2
"Deep Visual Forced Alignment: Learning to Align Transcription with
  Talking Face Video","Minsu Kim, Chae Won Kim, Yong Man Ro","Forced alignment refers to a technology that time-aligns a given
transcription with a corresponding speech. However, as the forced alignment
technologies have developed using speech audio, they might fail in alignment
when the input speech audio is noise-corrupted or is not accessible. We focus
on that there is another component that the speech can be inferred from, the
speech video (i.e., talking face video). Since the drawbacks of audio-based
forced alignment can be complemented using the visual information when the
audio signal is under poor condition, we try to develop a novel video-based
forced alignment method. However, different from audio forced alignment, it is
challenging to develop a reliable visual forced alignment technology for the
following two reasons: 1) Visual Speech Recognition (VSR) has a much lower
performance compared to audio-based Automatic Speech Recognition (ASR), and 2)
the translation from text to video is not reliable, so the method typically
used for building audio forced alignment cannot be utilized in developing
visual forced alignment. In order to alleviate these challenges, in this paper,
we propose a new method that is appropriate for visual forced alignment, namely
Deep Visual Forced Alignment (DVFA). The proposed DVFA can align the input
transcription (i.e., sentence) with the talking face video without accessing
the speech audio. Moreover, by augmenting the alignment task with anomaly case
detection, DVFA can detect mismatches between the input transcription and the
input video while performing the alignment. Therefore, we can robustly align
the text with the talking face video even if there exist error words in the
text. Through extensive experiments, we show the effectiveness of the proposed
DVFA not only in the alignment task but also in interpreting the outputs of VSR
models.",2303.08670v1,https://arxiv.org/pdf/2303.08670v1
"Unsupervised Domain Adaptation for Low-dose CT Reconstruction via
  Bayesian Uncertainty Alignment","Kecheng Chen, Jie Liu, Renjie Wan, Victor Ho-Fun Lee, Varut Vardhanabhuti, Hong Yan, Haoliang Li","Low-dose computed tomography (LDCT) image reconstruction techniques can
reduce patient radiation exposure while maintaining acceptable imaging quality.
Deep learning is widely used in this problem, but the performance of testing
data (a.k.a. target domain) is often degraded in clinical scenarios due to the
variations that were not encountered in training data (a.k.a. source domain).
Unsupervised domain adaptation (UDA) of LDCT reconstruction has been proposed
to solve this problem through distribution alignment. However, existing UDA
methods fail to explore the usage of uncertainty quantification, which is
crucial for reliable intelligent medical systems in clinical scenarios with
unexpected variations. Moreover, existing direct alignment for different
patients would lead to content mismatch issues. To address these issues, we
propose to leverage a probabilistic reconstruction framework to conduct a joint
discrepancy minimization between source and target domains in both the latent
and image spaces. In the latent space, we devise a Bayesian uncertainty
alignment to reduce the epistemic gap between the two domains. This approach
reduces the uncertainty level of target domain data, making it more likely to
render well-reconstructed results on target domains. In the image space, we
propose a sharpness-aware distribution alignment to achieve a match of
second-order information, which can ensure that the reconstructed images from
the target domain have similar sharpness to normal-dose CT images from the
source domain. Experimental results on two simulated datasets and one clinical
low-dose imaging dataset show that our proposed method outperforms other
methods in quantitative and visualized performance.",2302.13251v2,https://arxiv.org/pdf/2302.13251v2
Aligning Text-to-Image Models using Human Feedback,"Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Shixiang Shane Gu","Deep generative models have shown impressive results in text-to-image
synthesis. However, current text-to-image models often generate images that are
inadequately aligned with text prompts. We propose a fine-tuning method for
aligning such models using human feedback, comprising three stages. First, we
collect human feedback assessing model output alignment from a set of diverse
text prompts. We then use the human-labeled image-text dataset to train a
reward function that predicts human feedback. Lastly, the text-to-image model
is fine-tuned by maximizing reward-weighted likelihood to improve image-text
alignment. Our method generates objects with specified colors, counts and
backgrounds more accurately than the pre-trained model. We also analyze several
design choices and find that careful investigations on such design choices are
important in balancing the alignment-fidelity tradeoffs. Our results
demonstrate the potential for learning from human feedback to significantly
improve text-to-image models.",2302.12192v1,https://arxiv.org/pdf/2302.12192v1
Aligned Diffusion Schrödinger Bridges,"Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, Charlotte Bunne","Diffusion Schr\""odinger bridges (DSB) have recently emerged as a powerful
framework for recovering stochastic dynamics via their marginal observations at
different time points. Despite numerous successful applications, existing
algorithms for solving DSBs have so far failed to utilize the structure of
aligned data, which naturally arises in many biological phenomena. In this
paper, we propose a novel algorithmic framework that, for the first time,
solves DSBs while respecting the data alignment. Our approach hinges on a
combination of two decades-old ideas: The classical Schr\""odinger bridge theory
and Doob's $h$-transform. Compared to prior methods, our approach leads to a
simpler training procedure with lower variance, which we further augment with
principled regularization schemes. This ultimately leads to sizeable
improvements across experiments on synthetic and real data, including the tasks
of predicting conformational changes in proteins and temporal evolution of
cellular differentiation processes.",2302.11419v3,https://arxiv.org/pdf/2302.11419v3
"Anomaly Detection of UAV State Data Based on Single-class Triangular
  Global Alignment Kernel Extreme Learning Machine","Feisha Hu, Qi Wang, Haijian Shao, Shang Gao, Hualong Yu","Unmanned Aerial Vehicles (UAVs) are widely used and meet many demands in
military and civilian fields. With the continuous enrichment and extensive
expansion of application scenarios, the safety of UAVs is constantly being
challenged. To address this challenge, we propose algorithms to detect
anomalous data collected from drones to improve drone safety. We deployed a
one-class kernel extreme learning machine (OCKELM) to detect anomalies in drone
data. By default, OCKELM uses the radial basis (RBF) kernel function as the
kernel function of the model. To improve the performance of OCKELM, we choose a
Triangular Global Alignment Kernel (TGAK) instead of an RBF Kernel and
introduce the Fast Independent Component Analysis (FastICA) algorithm to
reconstruct UAV data. Based on the above improvements, we create a novel
anomaly detection strategy FastICA-TGAK-OCELM. The method is finally validated
on the UCI dataset and detected on the Aeronautical Laboratory Failures and
Anomalies (ALFA) dataset. The experimental results show that compared with
other methods, the accuracy of this method is improved by more than 30%, and
point anomalies are effectively detected.",2302.09320v1,https://arxiv.org/pdf/2302.09320v1
"Effective Multimodal Reinforcement Learning with Modality Alignment and
  Importance Enhancement","Jinming Ma, Feng Wu, Yingfeng Chen, Xianpeng Ji, Yu Ding","Many real-world applications require an agent to make robust and deliberate
decisions with multimodal information (e.g., robots with multi-sensory inputs).
However, it is very challenging to train the agent via reinforcement learning
(RL) due to the heterogeneity and dynamic importance of different modalities.
Specifically, we observe that these issues make conventional RL methods
difficult to learn a useful state representation in the end-to-end training
with multimodal information. To address this, we propose a novel multimodal RL
approach that can do multimodal alignment and importance enhancement according
to their similarity and importance in terms of RL tasks respectively. By doing
so, we are able to learn an effective state representation and consequentially
improve the RL training process. We test our approach on several multimodal RL
domains, showing that it outperforms state-of-the-art methods in terms of
learning speed and policy quality.",2302.09318v1,https://arxiv.org/pdf/2302.09318v1
Deep Reinforcement Learning for mmWave Initial Beam Alignment,"Daniel Tandler, Sebastian Dörner, Marc Gauger, Stephan ten Brink","We investigate the applicability of deep reinforcement learning algorithms to
the adaptive initial access beam alignment problem for mmWave communications
using the state-of-the-art proximal policy optimization algorithm as an
example. In comparison to recent unsupervised learning based approaches
developed to tackle this problem, deep reinforcement learning has the potential
to address a new and wider range of applications, since, in principle, no
(differentiable) model of the channel and/or the whole system is required for
training, and only agent-environment interactions are necessary to learn an
algorithm (be it online or using a recorded dataset). We show that, although
the chosen off-the-shelf deep reinforcement learning agent fails to perform
well when trained on realistic problem sizes, introducing action space shaping
in the form of beamforming modules vastly improves the performance, without
sacrificing much generalizability. Using this add-on, the agent is able to
deliver competitive performance to various state-of-the-art methods on
simulated environments, even under realistic problem sizes. This demonstrates
that through well-directed modification, deep reinforcement learning may have a
chance to compete with other approaches in this area, opening up many
straightforward extensions to other/similar scenarios.",2302.08969v1,https://arxiv.org/pdf/2302.08969v1
"Vision, Deduction and Alignment: An Empirical Study on Multi-modal
  Knowledge Graph Alignment","Yangning Li, Jiaoyan Chen, Yinghui Li, Yuejia Xiang, Xi Chen, Hai-Tao Zheng","Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in
knowledge engineering. Existing EA methods mostly focus on utilizing the graph
structures and entity attributes (including literals), but ignore images that
are common in modern multi-modal KGs. In this study we first constructed
Multi-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then
evaluated some existing embedding-based methods for utilizing images. In view
of the complementary nature of visual modal information and logical deduction,
we further developed a new multi-modal EA method named LODEME using logical
deduction and multi-modal KG embedding, with state-of-the-art performance
achieved on Multi-OpenEA and other existing multi-modal EA benchmarks.",2302.08774v2,https://arxiv.org/pdf/2302.08774v2
"Aligning Language Models with Preferences through f-divergence
  Minimization","Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Nahyeon Ryu, Marc Dymetman","Aligning language models with preferences can be posed as approximating a
target distribution representing some desired behavior. Existing approaches
differ both in the functional form of the target distribution and the algorithm
used to approximate it. For instance, Reinforcement Learning from Human
Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target
distribution arising from a KL penalty in the objective. On the other hand,
Generative Distributional Control (GDC) has an explicit target distribution and
minimizes a forward KL from it using the Distributional Policy Gradient (DPG)
algorithm. In this paper, we propose a new approach, f-DPG, which allows the
use of any f-divergence to approximate any target distribution that can be
evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation
methods (DPG, RL with KL penalties). We show the practical benefits of various
choices of divergence objectives and demonstrate that there is no universally
optimal objective but that different divergences present different alignment
and diversity trade-offs. We show that Jensen-Shannon divergence strikes a good
balance between these objectives, and frequently outperforms forward KL
divergence by a wide margin, leading to significant improvements over prior
work. These distinguishing characteristics between divergences persist as the
model size increases, highlighting the importance of selecting appropriate
divergence objectives.",2302.08215v2,https://arxiv.org/pdf/2302.08215v2
"Self-Supervised Temporal Graph learning with Temporal and Structural
  Intensity Alignment","Meng Liu, Ke Liang, Yawei Zhao, Wenxuan Tu, Sihang Zhou, Xinbiao Gan, Xinwang Liu, Kunlun He","Temporal graph learning aims to generate high-quality representations for
graph-based tasks with dynamic information, which has recently garnered
increasing attention. In contrast to static graphs, temporal graphs are
typically organized as node interaction sequences over continuous time rather
than an adjacency matrix. Most temporal graph learning methods model current
interactions by incorporating historical neighborhood. However, such methods
only consider first-order temporal information while disregarding crucial
high-order structural information, resulting in suboptimal performance. To
address this issue, we propose a self-supervised method called S2T for temporal
graph learning, which extracts both temporal and structural information to
learn more informative node representations. Notably, the initial node
representations combine first-order temporal and high-order structural
information differently to calculate two conditional intensities. An alignment
loss is then introduced to optimize the node representations, narrowing the gap
between the two intensities and making them more informative. Concretely, in
addition to modeling temporal information using historical neighbor sequences,
we further consider structural knowledge at both local and global levels. At
the local level, we generate structural intensity by aggregating features from
high-order neighbor sequences. At the global level, a global representation is
generated based on all nodes to adjust the structural intensity according to
the active statuses on different nodes. Extensive experiments demonstrate that
the proposed model S2T achieves at most 10.13% performance improvement compared
with the state-of-the-art competitors on several datasets.",2302.07491v3,https://arxiv.org/pdf/2302.07491v3
"Do Vision and Language Models Share Concepts? A Vector Space Alignment
  Study","Jiaang Li, Yova Kementchedjhieva, Constanza Fierro, Anders Søgaard","Large-scale pretrained language models (LMs) are said to ``lack the ability
to connect utterances to the world'' (Bender and Koller, 2020), because they do
not have ``mental models of the world' '(Mitchell and Krakauer, 2023). If so,
one would expect LM representations to be unrelated to representations induced
by vision models. We present an empirical evaluation across four families of
LMs (BERT, GPT-2, OPT and LLaMA-2) and three vision model architectures
(ResNet, SegFormer, and MAE). Our experiments show that LMs partially converge
towards representations isomorphic to those of vision models, subject to
dispersion, polysemy and frequency. This has important implications for both
multi-modal processing and the LM understanding debate (Mitchell and Krakauer,
2023).",2302.06555v2,https://arxiv.org/pdf/2302.06555v2
Cross-Modal Fine-Tuning: Align then Refine,"Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, Ameet Talwalkar","Fine-tuning large-scale pretrained models has led to tremendous progress in
well-studied modalities such as vision and NLP. However, similar gains have not
been observed in many other modalities due to a lack of relevant pretrained
models. In this work, we propose ORCA, a general cross-modal fine-tuning
framework that extends the applicability of a single large-scale pretrained
model to diverse modalities. ORCA adapts to a target task via an
align-then-refine workflow: given the target input, ORCA first learns an
embedding network that aligns the embedded feature distribution with the
pretraining modality. The pretrained model is then fine-tuned on the embedded
data to exploit the knowledge shared across modalities. Through extensive
experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks
containing over 60 datasets from 12 modalities, outperforming a wide range of
hand-designed, AutoML, general-purpose, and task-specific methods. We highlight
the importance of data alignment via a series of ablation studies and
demonstrate ORCA's utility in data-limited regimes.",2302.05738v2,https://arxiv.org/pdf/2302.05738v2
"Anatomical Invariance Modeling and Semantic Alignment for
  Self-supervised Learning in 3D Medical Image Analysis","Yankai Jiang, Mingze Sun, Heng Guo, Xiaoyu Bai, Ke Yan, Le Lu, Minfeng Xu","Self-supervised learning (SSL) has recently achieved promising performance
for 3D medical image analysis tasks. Most current methods follow existing SSL
paradigm originally designed for photographic or natural images, which cannot
explicitly and thoroughly exploit the intrinsic similar anatomical structures
across varying medical images. This may in fact degrade the quality of learned
deep representations by maximizing the similarity among features containing
spatial misalignment information and different anatomical semantics. In this
work, we propose a new self-supervised learning framework, namely Alice, that
explicitly fulfills Anatomical invariance modeling and semantic alignment via
elaborately combining discriminative and generative objectives. Alice
introduces a new contrastive learning strategy which encourages the similarity
between views that are diversely mined but with consistent high-level
semantics, in order to learn invariant anatomical features. Moreover, we design
a conditional anatomical feature alignment module to complement corrupted
embeddings with globally matched semantics and inter-patch topology
information, conditioned by the distribution of local image content, which
permits to create better contrastive pairs. Our extensive quantitative
experiments on three 3D medical image analysis tasks demonstrate and validate
the performance superiority of Alice, surpassing the previous best SSL
counterpart methods and showing promising ability for united representation
learning. Codes are available at https://github.com/alibaba-damo-academy/alice.",2302.05615v3,https://arxiv.org/pdf/2302.05615v3
"Data-Driven Stochastic Motion Evaluation and Optimization with Image by
  Spatially-Aligned Temporal Encoding","Takeru Oba, Norimichi Ukita","This paper proposes a probabilistic motion prediction method for long
motions. The motion is predicted so that it accomplishes a task from the
initial state observed in the given image. While our method evaluates the task
achievability by the Energy-Based Model (EBM), previous EBMs are not designed
for evaluating the consistency between different domains (i.e., image and
motion in our method). Our method seamlessly integrates the image and motion
data into the image feature domain by spatially-aligned temporal encoding so
that features are extracted along the motion trajectory projected onto the
image. Furthermore, this paper also proposes a data-driven motion optimization
method, Deep Motion Optimizer (DMO), that works with EBM for motion prediction.
Different from previous gradient-based optimizers, our self-supervised DMO
alleviates the difficulty of hyper-parameter tuning to avoid local minima. The
effectiveness of the proposed method is demonstrated with a variety of
experiments with similar SOTA methods.",2302.05041v1,https://arxiv.org/pdf/2302.05041v1
"Self-Supervised Node Representation Learning via Node-to-Neighbourhood
  Alignment","Wei Dong, Dawei Yan, Peng Wang","Self-supervised node representation learning aims to learn node
representations from unlabelled graphs that rival the supervised counterparts.
The key towards learning informative node representations lies in how to
effectively gain contextual information from the graph structure. In this work,
we present simple-yet-effective self-supervised node representation learning
via aligning the hidden representations of nodes and their neighbourhood. Our
first idea achieves such node-to-neighbourhood alignment by directly maximizing
the mutual information between their representations, which, we prove
theoretically, plays the role of graph smoothing. Our framework is optimized
via a surrogate contrastive loss and a Topology-Aware Positive Sampling (TAPS)
strategy is proposed to sample positives by considering the structural
dependencies between nodes, which enables offline positive selection.
Considering the excessive memory overheads of contrastive learning, we further
propose a negative-free solution, where the main contribution is a Graph Signal
Decorrelation (GSD) constraint to avoid representation collapse and
over-smoothing. The GSD constraint unifies some of the existing constraints and
can be used to derive new implementations to combat representation collapse. By
applying our methods on top of simple MLP-based node representation encoders,
we learn node representations that achieve promising node classification
performance on a set of graph-structured datasets from small- to large-scale.",2302.04626v2,https://arxiv.org/pdf/2302.04626v2
"Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class
  Incremental Learning","Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, Dacheng Tao","Few-shot class-incremental learning (FSCIL) has been a challenging problem as
only a few training samples are accessible for each novel class in the new
sessions. Finetuning the backbone or adjusting the classifier prototypes
trained in the prior sessions would inevitably cause a misalignment between the
feature and classifier of old classes, which explains the well-known
catastrophic forgetting problem. In this paper, we deal with this misalignment
dilemma in FSCIL inspired by the recently discovered phenomenon named neural
collapse, which reveals that the last-layer features of the same class will
collapse into a vertex, and the vertices of all classes are aligned with the
classifier prototypes, which are formed as a simplex equiangular tight frame
(ETF). It corresponds to an optimal geometric structure for classification due
to the maximized Fisher Discriminant Ratio. We propose a neural collapse
inspired framework for FSCIL. A group of classifier prototypes are pre-assigned
as a simplex ETF for the whole label space, including the base session and all
the incremental sessions. During training, the classifier prototypes are not
learnable, and we adopt a novel loss function that drives the features into
their corresponding prototypes. Theoretical analysis shows that our method
holds the neural collapse optimality and does not break the feature-classifier
alignment in an incremental fashion. Experiments on the miniImageNet, CUB-200,
and CIFAR-100 datasets demonstrate that our proposed framework outperforms the
state-of-the-art performances. Code address:
https://github.com/NeuralCollapseApplications/FSCIL",2302.03004v1,https://arxiv.org/pdf/2302.03004v1
Chain of Hindsight Aligns Language Models with Feedback,"Hao Liu, Carmelo Sferrazza, Pieter Abbeel","Learning from human preferences is important for language models to match
human needs and to align with human and social values. Prior works have
achieved remarkable successes by learning from human feedback to understand and
follow instructions. Nonetheless, these methods are either founded on
hand-picked model generations that are favored by human annotators, rendering
them inefficient in terms of data utilization and challenging to apply in
general, or they depend on reinforcement learning, which often suffers from
imperfect reward functions and relies on extremely challenging optimizations.
In this work, we propose a novel technique, Chain of Hindsight, that is easy to
optimize and can learn from any form of feedback, regardless of its polarity.
Our idea is inspired by how humans learn from extensive feedback presented in
the form of languages. We convert all types of feedback into sequences of
sentences, which are then used to fine-tune the model, allowing us to take
advantage of the language comprehension capabilities of language models. We
condition the model on a sequence of model generations paired with feedback. By
doing so, the model is trained to generate outputs based on feedback, while
learning to identify and correct negative attributes or errors. Applying our
method to large language models, we observed that Chain of Hindsight
significantly surpasses previous methods in aligning language models with human
preferences. We report significant improvements on summarization and dialogue
benchmarks, with our approach markedly preferred in human evaluations.",2302.02676v8,https://arxiv.org/pdf/2302.02676v8
Domain Adaptation via Rebalanced Sub-domain Alignment,"Yiling Liu, Juncheng Dong, Ziyang Jiang, Ahmed Aloui, Keyu Li, Hunter Klein, Vahid Tarokh, David Carlson","Unsupervised domain adaptation (UDA) is a technique used to transfer
knowledge from a labeled source domain to a different but related unlabeled
target domain. While many UDA methods have shown success in the past, they
often assume that the source and target domains must have identical class label
distributions, which can limit their effectiveness in real-world scenarios. To
address this limitation, we propose a novel generalization bound that reweights
source classification error by aligning source and target sub-domains. We prove
that our proposed generalization bound is at least as strong as existing bounds
under realistic assumptions, and we empirically show that it is much stronger
on real-world data. We then propose an algorithm to minimize this novel
generalization bound. We demonstrate by numerical experiments that this
approach improves performance in shifted class distribution scenarios compared
to state-of-the-art methods.",2302.02009v1,https://arxiv.org/pdf/2302.02009v1
Aligning Robot and Human Representations,"Andreea Bobu, Andi Peng, Pulkit Agrawal, Julie Shah, Anca D. Dragan","To act in the world, robots rely on a representation of salient task aspects:
for example, to carry a coffee mug, a robot may consider movement efficiency or
mug orientation in its behavior. However, if we want robots to act for and with
people, their representations must not be just functional but also reflective
of what humans care about, i.e. they must be aligned. We observe that current
learning approaches suffer from representation misalignment, where the robot's
learned representation does not capture the human's representation. We suggest
that because humans are the ultimate evaluator of robot performance, we must
explicitly focus our efforts on aligning learned representations with humans,
in addition to learning the downstream task. We advocate that current
representation learning approaches in robotics should be studied from the
perspective of how well they accomplish the objective of representation
alignment. We mathematically define the problem, identify its key desiderata,
and situate current methods within this formalism. We conclude by suggesting
future directions for exploring open challenges.",2302.01928v2,https://arxiv.org/pdf/2302.01928v2
"Domain Adaptation via Alignment of Operation Profile for Remaining
  Useful Lifetime Prediction","Ismail Nejjar, Fabian Geissmann, Mengjie Zhao, Cees Taal, Olga Fink","Effective Prognostics and Health Management (PHM) relies on accurate
prediction of the Remaining Useful Life (RUL). Data-driven RUL prediction
techniques rely heavily on the representativeness of the available
time-to-failure trajectories. Therefore, these methods may not perform well
when applied to data from new units of a fleet that follow different operating
conditions than those they were trained on. This is also known as domain
shifts. Domain adaptation (DA) methods aim to address the domain shift problem
by extracting domain invariant features. However, DA methods do not distinguish
between the different phases of operation, such as steady states or transient
phases. This can result in misalignment due to under- or over-representation of
different operation phases. This paper proposes two novel DA approaches for RUL
prediction based on an adversarial domain adaptation framework that considers
the different phases of the operation profiles separately. The proposed
methodologies align the marginal distributions of each phase of the operation
profile in the source domain with its counterpart in the target domain. The
effectiveness of the proposed methods is evaluated using the New Commercial
Modular Aero-Propulsion System (N-CMAPSS) dataset, where sub-fleets of turbofan
engines operating in one of the three different flight classes (short, medium,
and long) are treated as separate domains. The experimental results show that
the proposed methods improve the accuracy of RUL predictions compared to
current state-of-the-art DA methods.",2302.01704v2,https://arxiv.org/pdf/2302.01704v2
ANTM: An Aligned Neural Topic Model for Exploring Evolving Topics,"Hamed Rahimi, Hubert Naacke, Camelia Constantin, Bernd Amann","This paper presents an algorithmic family of dynamic topic models called
Aligned Neural Topic Models (ANTM), which combine novel data mining algorithms
to provide a modular framework for discovering evolving topics. ANTM maintains
the temporal continuity of evolving topics by extracting time-aware features
from documents using advanced pre-trained Large Language Models (LLMs) and
employing an overlapping sliding window algorithm for sequential document
clustering. This overlapping sliding window algorithm identifies a different
number of topics within each time frame and aligns semantically similar
document clusters across time periods. This process captures emerging and
fading trends across different periods and allows for a more interpretable
representation of evolving topics. Experiments on four distinct datasets show
that ANTM outperforms probabilistic dynamic topic models in terms of topic
coherence and diversity metrics. Moreover, it improves the scalability and
flexibility of dynamic topic models by being accessible and adaptable to
different types of algorithms. Additionally, a Python package is developed for
researchers and scientists who wish to study the trends and evolving patterns
of topics in large-scale textual data.",2302.01501v2,https://arxiv.org/pdf/2302.01501v2
"Multi-scale Feature Alignment for Continual Learning of Unlabeled
  Domains","Kevin Thandiackal, Luigi Piccinelli, Pushpak Pati, Orcun Goksel","Methods for unsupervised domain adaptation (UDA) help to improve the
performance of deep neural networks on unseen domains without any labeled data.
Especially in medical disciplines such as histopathology, this is crucial since
large datasets with detailed annotations are scarce. While the majority of
existing UDA methods focus on the adaptation from a labeled source to a single
unlabeled target domain, many real-world applications with a long life cycle
involve more than one target domain. Thus, the ability to sequentially adapt to
multiple target domains becomes essential. In settings where the data from
previously seen domains cannot be stored, e.g., due to data protection
regulations, the above becomes a challenging continual learning problem. To
this end, we propose to use generative feature-driven image replay in
conjunction with a dual-purpose discriminator that not only enables the
generation of images with realistic features for replay, but also promotes
feature alignment during domain adaptation. We evaluate our approach
extensively on a sequence of three histopathological datasets for tissue-type
classification, achieving state-of-the-art results. We present detailed
ablation experiments studying our proposed method components and demonstrate a
possible use-case of our continual UDA method for an unsupervised patch-based
segmentation task given high-resolution tissue images.",2302.01287v1,https://arxiv.org/pdf/2302.01287v1
"Language Quantized AutoEncoders: Towards Unsupervised Text-Image
  Alignment","Hao Liu, Wilson Yan, Pieter Abbeel","Recent progress in scaling up large language models has shown impressive
capabilities in performing few-shot learning across a wide range of text-based
tasks. However, a key limitation is that these language models fundamentally
lack visual perception - a crucial attribute needed to extend these models to
be able to interact with the real world and solve vision tasks, such as in
visual-question answering and robotics. Prior works have largely connected
image to text through pretraining and/or fine-tuning on curated image-text
datasets, which can be a costly and expensive process. In order to resolve this
limitation, we propose a simple yet effective approach called
Language-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to
align text-image data in an unsupervised manner by leveraging pretrained
language models (e.g., BERT, RoBERTa). Our main idea is to encode image as
sequences of text tokens by directly quantizing image embeddings using a
pretrained language codebook. We then apply random masking followed by a BERT
model, and have the decoder reconstruct the original image from BERT predicted
text token embeddings. By doing so, LQAE learns to represent similar images
with similar clusters of text tokens, thereby aligning these two modalities
without the use of aligned text-image pairs. This enables few-shot image
classification with large language models (e.g., GPT-3) as well as linear
classification of images based on BERT text features. To the best of our
knowledge, our work is the first work that uses unaligned images for multimodal
tasks by leveraging the power of pretrained language models.",2302.00902v2,https://arxiv.org/pdf/2302.00902v2
Goal Alignment: A Human-Aware Account of Value Alignment Problem,"Malek Mechergui, Sarath Sreedharan","Value alignment problems arise in scenarios where the specified objectives of
an AI agent don't match the true underlying objective of its users. The problem
has been widely argued to be one of the central safety problems in AI.
Unfortunately, most existing works in value alignment tend to focus on issues
that are primarily related to the fact that reward functions are an unintuitive
mechanism to specify objectives. However, the complexity of the objective
specification mechanism is just one of many reasons why the user may have
misspecified their objective. A foundational cause for misalignment that is
being overlooked by these works is the inherent asymmetry in human expectations
about the agent's behavior and the behavior generated by the agent for the
specified objective. To address this lacuna, we propose a novel formulation for
the value alignment problem, named goal alignment that focuses on a few central
challenges related to value alignment. In doing so, we bridge the currently
disparate research areas of value alignment and human-aware planning.
Additionally, we propose a first-of-its-kind interactive algorithm that is
capable of using information generated under incorrect beliefs about the agent,
to determine the true underlying goal of the user.",2302.00813v2,https://arxiv.org/pdf/2302.00813v2
Unsupervised Entity Alignment for Temporal Knowledge Graphs,"Xiaoze Liu, Junyang Wu, Tianyi Li, Lu Chen, Yunjun Gao","Entity alignment (EA) is a fundamental data integration task that identifies
equivalent entities between different knowledge graphs (KGs). Temporal
Knowledge graphs (TKGs) extend traditional knowledge graphs by introducing
timestamps, which have received increasing attention. State-of-the-art
time-aware EA studies have suggested that the temporal information of TKGs
facilitates the performance of EA. However, existing studies have not
thoroughly exploited the advantages of temporal information in TKGs. Also, they
perform EA by pre-aligning entity pairs, which can be labor-intensive and thus
inefficient.
  In this paper, we present DualMatch which effectively fuses the relational
and temporal information for EA. DualMatch transfers EA on TKGs into a weighted
graph matching problem. More specifically, DualMatch is equipped with an
unsupervised method, which achieves EA without necessitating seed alignment.
DualMatch has two steps: (i) encoding temporal and relational information into
embeddings separately using a novel label-free encoder, Dual-Encoder; and (ii)
fusing both information and transforming it into alignment using a novel
graph-matching-based decoder, GM-Decoder. DualMatch is able to perform EA on
TKGs with or without supervision, due to its capability of effectively
capturing temporal information. Extensive experiments on three real-world TKG
datasets offer the insight that DualMatch outperforms the state-of-the-art
methods in terms of H@1 by 2.4% - 10.7% and MRR by 1.7% - 7.6%, respectively.",2302.00796v2,https://arxiv.org/pdf/2302.00796v2
"Robust Attributed Graph Alignment via Joint Structure Learning and
  Optimal Transport","Jianheng Tang, Weiqi Zhang, Jiajin Li, Kangfei Zhao, Fugee Tsung, Jia Li","Graph alignment, which aims at identifying corresponding entities across
multiple networks, has been widely applied in various domains. As the graphs to
be aligned are usually constructed from different sources, the inconsistency
issues of structures and features between two graphs are ubiquitous in
real-world applications. Most existing methods follow the
``embed-then-cross-compare'' paradigm, which computes node embeddings in each
graph and then processes node correspondences based on cross-graph embedding
comparison. However, we find these methods are unstable and sub-optimal when
structure or feature inconsistency appears. To this end, we propose SLOTAlign,
an unsupervised graph alignment framework that jointly performs Structure
Learning and Optimal Transport Alignment. We convert graph alignment to an
optimal transport problem between two intra-graph matrices without the
requirement of cross-graph comparison. We further incorporate multi-view
structure learning to enhance graph representation power and reduce the effect
of structure and feature inconsistency inherited across graphs. Moreover, an
alternating scheme based algorithm has been developed to address the joint
optimization problem in SLOTAlign, and the provable convergence result is also
established. Finally, we conduct extensive experiments on six unsupervised
graph alignment datasets and the DBP15K knowledge graph (KG) alignment
benchmark dataset. The proposed SLOTAlign shows superior performance and
strongest robustness over seven unsupervised graph alignment methods and five
specialized KG alignment methods.",2301.12721v2,https://arxiv.org/pdf/2301.12721v2
Alignment with human representations supports robust few-shot learning,"Ilia Sucholutsky, Thomas L. Griffiths","Should we care whether AI systems have representations of the world that are
similar to those of humans? We provide an information-theoretic analysis that
suggests that there should be a U-shaped relationship between the degree of
representational alignment with humans and performance on few-shot learning
tasks. We confirm this prediction empirically, finding such a relationship in
an analysis of the performance of 491 computer vision models. We also show that
highly-aligned models are more robust to both natural adversarial attacks and
domain shifts. Our results suggest that human-alignment is often a sufficient,
but not necessary, condition for models to make effective use of limited data,
be robust, and generalize well.",2301.11990v3,https://arxiv.org/pdf/2301.11990v3
"Policy-Value Alignment and Robustness in Search-based Multi-Agent
  Learning","Niko A. Grupen, Michael Hanlon, Alexis Hao, Daniel D. Lee, Bart Selman","Large-scale AI systems that combine search and learning have reached
super-human levels of performance in game-playing, but have also been shown to
fail in surprising ways. The brittleness of such models limits their efficacy
and trustworthiness in real-world deployments. In this work, we systematically
study one such algorithm, AlphaZero, and identify two phenomena related to the
nature of exploration. First, we find evidence of policy-value misalignment --
for many states, AlphaZero's policy and value predictions contradict each
other, revealing a tension between accurate move-selection and value estimation
in AlphaZero's objective. Further, we find inconsistency within AlphaZero's
value function, which causes it to generalize poorly, despite its policy
playing an optimal strategy. From these insights we derive VISA-VIS: a novel
method that improves policy-value alignment and value robustness in AlphaZero.
Experimentally, we show that our method reduces policy-value misalignment by up
to 76%, reduces value generalization error by up to 50%, and reduces average
value error by up to 55%.",2301.11857v2,https://arxiv.org/pdf/2301.11857v2
Cross-domain recommendation via user interest alignment,"Chuang Zhao, Hongke Zhao, Ming He, Jian Zhang, Jianping Fan","Cross-domain recommendation aims to leverage knowledge from multiple domains
to alleviate the data sparsity and cold-start problems in traditional
recommender systems. One popular paradigm is to employ overlapping user
representations to establish domain connections, thereby improving
recommendation performance in all scenarios. Nevertheless, the general practice
of this approach is to train user embeddings in each domain separately and then
aggregate them in a plain manner, often ignoring potential cross-domain
similarities between users and items. Furthermore, considering that their
training objective is recommendation task-oriented without specific
regularizations, the optimized embeddings disregard the interest alignment
among user's views, and even violate the user's original interest distribution.
To address these challenges, we propose a novel cross-domain recommendation
framework, namely COAST, to improve recommendation performance on dual domains
by perceiving the cross-domain similarity between entities and aligning user
interests. Specifically, we first construct a unified cross-domain
heterogeneous graph and redefine the message passing mechanism of graph
convolutional networks to capture high-order similarity of users and items
across domains. Targeted at user interest alignment, we develop deep insights
from two more fine-grained perspectives of user-user and user-item interest
invariance across domains by virtue of affluent unsupervised and semantic
signals. We conduct intensive experiments on multiple tasks, constructed from
two large recommendation data sets. Extensive results show COAST consistently
and significantly outperforms state-of-the-art cross-domain recommendation
algorithms as well as classic single-domain recommendation methods.",2301.11467v1,https://arxiv.org/pdf/2301.11467v1
AlignGraph: A Group of Generative Models for Graphs,"Kimia Shayestehfard, Dana Brooks, Stratis Ioannidis","It is challenging for generative models to learn a distribution over graphs
because of the lack of permutation invariance: nodes may be ordered arbitrarily
across graphs, and standard graph alignment is combinatorial and notoriously
expensive. We propose AlignGraph, a group of generative models that combine
fast and efficient graph alignment methods with a family of deep generative
models that are invariant to node permutations. Our experiments demonstrate
that our framework successfully learns graph distributions, outperforming
competitors by 25% -560% in relevant performance scores.",2301.11273v1,https://arxiv.org/pdf/2301.11273v1
"Heterogeneous Domain Adaptation for IoT Intrusion Detection: A Geometric
  Graph Alignment Approach","Jiashu Wu, Hao Dai, Yang Wang, Kejiang Ye, Chengzhong Xu","Data scarcity hinders the usability of data-dependent algorithms when
tackling IoT intrusion detection (IID). To address this, we utilise the data
rich network intrusion detection (NID) domain to facilitate more accurate
intrusion detection for IID domains. In this paper, a Geometric Graph Alignment
(GGA) approach is leveraged to mask the geometric heterogeneities between
domains for better intrusion knowledge transfer. Specifically, each intrusion
domain is formulated as a graph where vertices and edges represent intrusion
categories and category-wise interrelationships, respectively. The overall
shape is preserved via a confused discriminator incapable to identify adjacency
matrices between different intrusion domain graphs. A rotation avoidance
mechanism and a centre point matching mechanism is used to avoid graph
misalignment due to rotation and symmetry, respectively. Besides, category-wise
semantic knowledge is transferred to act as vertex-level alignment. To exploit
the target data, a pseudo-label election mechanism that jointly considers
network prediction, geometric property and neighbourhood information is used to
produce fine-grained pseudo-label assignment. Upon aligning the intrusion
graphs geometrically from different granularities, the transferred intrusion
knowledge can boost IID performance. Comprehensive experiments on several
intrusion datasets demonstrate state-of-the-art performance of the GGA approach
and validate the usefulness of GGA constituting components.",2301.09801v1,https://arxiv.org/pdf/2301.09801v1
Truveta Mapper: A Zero-shot Ontology Alignment Framework,"Mariyam Amir, Murchana Baruah, Mahsa Eslamialishah, Sina Ehsani, Alireza Bahramali, Sadra Naddaf-Sh, Saman Zarandioon","In this paper, a new perspective is suggested for unsupervised Ontology
Matching (OM) or Ontology Alignment (OA) by treating it as a translation task.
Ontologies are represented as graphs, and the translation is performed from a
node in the source ontology graph to a path in the target ontology graph. The
proposed framework, Truveta Mapper (TM), leverages a multi-task
sequence-to-sequence transformer model to perform alignment across multiple
ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables
the model to implicitly learn the relationship between different ontologies via
transfer-learning without requiring any explicit cross-ontology manually
labeled data. This also enables the formulated framework to outperform existing
solutions for both runtime latency and alignment quality. The model is
pre-trained and fine-tuned only on publicly available text corpus and
inner-ontologies data. The proposed solution outperforms state-of-the-art
approaches, Edit-Similarity, LogMap, AML, BERTMap, and the recently presented
new OM frameworks in Ontology Alignment Evaluation Initiative (OAEI22), offers
log-linear complexity, and overall makes the OM task efficient and more
straightforward without much post-processing involving mapping extension or
mapping repair. We are open sourcing our solution.",2301.09767v3,https://arxiv.org/pdf/2301.09767v3
Noisy Parallel Data Alignment,"Ruoyu Xie, Antonios Anastasopoulos","An ongoing challenge in current natural language processing is how its major
advancements tend to disproportionately favor resource-rich languages, leaving
a significant number of under-resourced languages behind. Due to the lack of
resources required to train and evaluate models, most modern language
technologies are either nonexistent or unreliable to process endangered, local,
and non-standardized languages. Optical character recognition (OCR) is often
used to convert endangered language documents into machine-readable data.
However, such OCR output is typically noisy, and most word alignment models are
not built to work under such noisy conditions. In this work, we study the
existing word-level alignment models under noisy settings and aim to make them
more robust to noisy data. Our noise simulation and structural biasing method,
tested on multiple language pairs, manages to reduce the alignment error rate
on a state-of-the-art neural-based alignment model up to 59.6%.",2301.09685v2,https://arxiv.org/pdf/2301.09685v2
Selective Explanations: Leveraging Human Input to Align Explainable AI,"Vivian Lai, Yiming Zhang, Chacha Chen, Q. Vera Liao, Chenhao Tan","While a vast collection of explainable AI (XAI) algorithms have been
developed in recent years, they are often criticized for significant gaps with
how humans produce and consume explanations. As a result, current XAI
techniques are often found to be hard to use and lack effectiveness. In this
work, we attempt to close these gaps by making AI explanations selective -- a
fundamental property of human explanations -- by selectively presenting a
subset from a large set of model reasons based on what aligns with the
recipient's preferences. We propose a general framework for generating
selective explanations by leveraging human input on a small sample. This
framework opens up a rich design space that accounts for different selectivity
goals, types of input, and more. As a showcase, we use a decision-support task
to explore selective explanations based on what the decision-maker would
consider relevant to the decision task. We conducted two experimental studies
to examine three out of a broader possible set of paradigms based on our
proposed framework: in Study 1, we ask the participants to provide their own
input to generate selective explanations, with either open-ended or
critique-based input. In Study 2, we show participants selective explanations
based on input from a panel of similar users (annotators). Our experiments
demonstrate the promise of selective explanations in reducing over-reliance on
AI and improving decision outcomes and subjective perceptions of the AI, but
also paint a nuanced picture that attributes some of these positive effects to
the opportunity to provide one's own input to augment AI explanations. Overall,
our work proposes a novel XAI framework inspired by human communication
behaviors and demonstrates its potentials to encourage future work to better
align AI explanations with human production and consumption of explanations.",2301.09656v3,https://arxiv.org/pdf/2301.09656v3
"Deep Attention-Based Alignment Network for Melody Generation from
  Incomplete Lyrics","Gurunath Reddy M, Zhe Zhang, Yi Yu, Florian Harscoet, Simon Canales, Suhua Tang","We propose a deep attention-based alignment network, which aims to
automatically predict lyrics and melody with given incomplete lyrics as input
in a way similar to the music creation of humans. Most importantly, a deep
neural lyrics-to-melody net is trained in an encoder-decoder way to predict
possible pairs of lyrics-melody when given incomplete lyrics (few keywords).
The attention mechanism is exploited to align the predicted lyrics with the
melody during the lyrics-to-melody generation. The qualitative and quantitative
evaluation metrics reveal that the proposed method is indeed capable of
generating proper lyrics and corresponding melody for composing new songs given
a piece of incomplete seed lyrics.",2301.10015v1,https://arxiv.org/pdf/2301.10015v1
"FPANet: Frequency-based Video Demoireing using Frame-level Post
  Alignment","Gyeongrok Oh, Heon Gu, Jinkyu Kim, Sangpil Kim","Interference between overlapping gird patterns creates moire patterns,
degrading the visual quality of an image that captures a screen of a digital
display device by an ordinary digital camera. Removing such moire patterns is
challenging due to their complex patterns of diverse sizes and color
distortions. Existing approaches mainly focus on filtering out in the spatial
domain, failing to remove a large-scale moire pattern. In this paper, we
propose a novel model called FPANet that learns filters in both frequency and
spatial domains, improving the restoration quality by removing various sizes of
moire patterns. To further enhance, our model takes multiple consecutive
frames, learning to extract frame-invariant content features and outputting
better quality temporally consistent images. We demonstrate the effectiveness
of our proposed method with a publicly available large-scale dataset, observing
that ours outperforms the state-of-the-art approaches, including ESDNet,
VDmoire, MBCNN, WDNet, UNet, and DMCNN, in terms of the image and video quality
metrics, such as PSNR, SSIM, LPIPS, FVD, and FSIM.",2301.07330v2,https://arxiv.org/pdf/2301.07330v2
"AI Alignment Dialogues: An Interactive Approach to AI Alignment in
  Support Agents","Pei-Yu Chen, Myrthe L. Tielman, Dirk K. J. Heylen, Catholijn M. Jonker, M. Birna van Riemsdijk","AI alignment is about ensuring AI systems only pursue goals and activities
that are beneficial to humans. Most of the current approach to AI alignment is
to learn what humans value from their behavioural data. This paper proposes a
different way of looking at the notion of alignment, namely by introducing AI
Alignment Dialogues: dialogues with which users and agents try to achieve and
maintain alignment via interaction. We argue that alignment dialogues have a
number of advantages in comparison to data-driven approaches, especially for
behaviour support agents, which aim to support users in achieving their desired
future behaviours rather than their current behaviours. The advantages of
alignment dialogues include allowing the users to directly convey higher-level
concepts to the agent, and making the agent more transparent and trustworthy.
In this paper we outline the concept and high-level structure of alignment
dialogues. Moreover, we conducted a qualitative focus group user study from
which we developed a model that describes how alignment dialogues affect users,
and created design suggestions for AI alignment dialogues. Through this we
establish foundations for AI alignment dialogues and shed light on what
requires further development and research.",2301.06421v2,https://arxiv.org/pdf/2301.06421v2
Adversarial Alignment for Source Free Object Detection,"Qiaosong Chu, Shuyan Li, Guangyi Chen, Kai Li, Xiu Li","Source-free object detection (SFOD) aims to transfer a detector pre-trained
on a label-rich source domain to an unlabeled target domain without seeing
source data. While most existing SFOD methods generate pseudo labels via a
source-pretrained model to guide training, these pseudo labels usually contain
high noises due to heavy domain discrepancy. In order to obtain better pseudo
supervisions, we divide the target domain into source-similar and
source-dissimilar parts and align them in the feature space by adversarial
learning. Specifically, we design a detection variance-based criterion to
divide the target domain. This criterion is motivated by a finding that larger
detection variances denote higher recall and larger similarity to the source
domain. Then we incorporate an adversarial module into a mean teacher framework
to drive the feature spaces of these two subsets indistinguishable. Extensive
experiments on multiple cross-domain object detection datasets demonstrate that
our proposed method consistently outperforms the compared SFOD methods.",2301.04265v1,https://arxiv.org/pdf/2301.04265v1
A Multi-Level Framework for the AI Alignment Problem,"Betty Li Hou, Brian Patrick Green","AI alignment considers how we can encode AI systems in a way that is
compatible with human values. The normative side of this problem asks what
moral values or principles, if any, we should encode in AI. To this end, we
present a framework to consider the question at four levels: Individual,
Organizational, National, and Global. We aim to illustrate how AI alignment is
made up of value alignment problems at each of these levels, where values at
each level affect the others and effects can flow in either direction. We
outline key questions and considerations of each level and demonstrate an
application of this framework to the topic of AI content moderation.",2301.03740v1,https://arxiv.org/pdf/2301.03740v1
A Divide-Align-Conquer Strategy for Program Synthesis,"Jonas Witt, Stef Rasing, Sebastijan Dumančić, Tias Guns, Claus-Christian Carbon","A major bottleneck in search-based program synthesis is the exponentially
growing search space which makes learning large programs intractable. Humans
mitigate this problem by leveraging the compositional nature of the real world:
In structured domains, a logical specification can often be decomposed into
smaller, complementary solution programs. We show that compositional
segmentation can be applied in the programming by examples setting to divide
the search for large programs across multiple smaller program synthesis
problems. For each example, we search for a decomposition into smaller units
which maximizes the reconstruction accuracy in the output under a latent task
program. A structural alignment of the constituent parts in the input and
output leads to pairwise correspondences used to guide the program synthesis
search. In order to align the input/output structures, we make use of the
Structure-Mapping Theory (SMT), a formal model of human analogical reasoning
which originated in the cognitive sciences. We show that decomposition-driven
program synthesis with structural alignment outperforms Inductive Logic
Programming (ILP) baselines on string transformation tasks even with minimal
knowledge priors. Unlike existing methods, the predictive accuracy of our agent
monotonically increases for additional examples and achieves an average time
complexity of $\mathcal{O}(m)$ in the number $m$ of partial programs for highly
structured domains such as strings. We extend this method to the complex
setting of visual reasoning in the Abstraction and Reasoning Corpus (ARC) for
which ILP methods were previously infeasible.",2301.03094v1,https://arxiv.org/pdf/2301.03094v1
"Faithful and Consistent Graph Neural Network Explanations with Rationale
  Alignment","Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, Suhang Wang","Uncovering rationales behind predictions of graph neural networks (GNNs) has
received increasing attention over recent years. Instance-level GNN explanation
aims to discover critical input elements, like nodes or edges, that the target
GNN relies upon for making predictions. %These identified sub-structures can
provide interpretations of GNN's behavior. Though various algorithms are
proposed, most of them formalize this task by searching the minimal subgraph
which can preserve original predictions. However, an inductive bias is
deep-rooted in this framework: several subgraphs can result in the same or
similar outputs as the original graphs. Consequently, they have the danger of
providing spurious explanations and failing to provide consistent explanations.
Applying them to explain weakly-performed GNNs would further amplify these
issues. To address this problem, we theoretically examine the predictions of
GNNs from the causality perspective. Two typical reasons for spurious
explanations are identified: confounding effect of latent variables like
distribution shift, and causal factors distinct from the original input.
Observing that both confounding effects and diverse causal rationales are
encoded in internal representations, \tianxiang{we propose a new explanation
framework with an auxiliary alignment loss, which is theoretically proven to be
optimizing a more faithful explanation objective intrinsically. Concretely for
this alignment loss, a set of different perspectives are explored: anchor-based
alignment, distributional alignment based on Gaussian mixture models,
mutual-information-based alignment, etc. A comprehensive study is conducted
both on the effectiveness of this new framework in terms of explanation
faithfulness/consistency and on the advantages of these variants.",2301.02791v2,https://arxiv.org/pdf/2301.02791v2
"Heterogeneous Domain Adaptation and Equipment Matching: DANN-based
  Alignment with Cyclic Supervision (DBACS)","Natalie Gentner, Gian Antonio Susto","Process monitoring and control are essential in modern industries for
ensuring high quality standards and optimizing production performance. These
technologies have a long history of application in production and have had
numerous positive impacts, but also hold great potential when integrated with
Industry 4.0 and advanced machine learning, particularly deep learning,
solutions. However, in order to implement these solutions in production and
enable widespread adoption, the scalability and transferability of deep
learning methods have become a focus of research. While transfer learning has
proven successful in many cases, particularly with computer vision and
homogenous data inputs, it can be challenging to apply to heterogeneous data.
Motivated by the need to transfer and standardize established processes to
different, non-identical environments and by the challenge of adapting to
heterogeneous data representations, this work introduces the Domain Adaptation
Neural Network with Cyclic Supervision (DBACS) approach. DBACS addresses the
issue of model generalization through domain adaptation, specifically for
heterogeneous data, and enables the transfer and scalability of deep
learning-based statistical control methods in a general manner. Additionally,
the cyclic interactions between the different parts of the model enable DBACS
to not only adapt to the domains, but also match them. To the best of our
knowledge, DBACS is the first deep learning approach to combine adaptation and
matching for heterogeneous data settings. For comparison, this work also
includes subspace alignment and a multi-view learning that deals with
heterogeneous representations by mapping data into correlated latent feature
spaces. Finally, DBACS with its ability to adapt and match, is applied to a
virtual metrology use case for an etching process run on different machine
types in semiconductor manufacturing.",2301.01038v1,https://arxiv.org/pdf/2301.01038v1
"Navigating Alignment for Non-identical Client Class Sets: A Label
  Name-Anchored Federated Learning Framework","Jiayun Zhang, Xiyuan Zhang, Xinyang Zhang, Dezhi Hong, Rajesh K. Gupta, Jingbo Shang","Traditional federated classification methods, even those designed for non-IID
clients, assume that each client annotates its local data with respect to the
same universal class set. In this paper, we focus on a more general yet
practical setting, non-identical client class sets, where clients focus on
their own (different or even non-overlapping) class sets and seek a global
model that works for the union of these classes. If one views classification as
finding the best match between representations produced by data/label encoder,
such heterogeneity in client class sets poses a new significant challenge --
local encoders at different clients may operate in different and even
independent latent spaces, making it hard to aggregate at the server. We
propose a novel framework, FedAlign, to align the latent spaces across clients
from both label and data perspectives. From a label perspective, we leverage
the expressive natural language class names as a common ground for label
encoders to anchor class representations and guide the data encoder learning
across clients. From a data perspective, during local training, we regard the
global class representations as anchors and leverage the data points that are
close/far enough to the anchors of locally-unaware classes to align the data
encoders across clients. Our theoretical analysis of the generalization
performance and extensive experiments on four real-world datasets of different
tasks confirm that FedAlign outperforms various state-of-the-art (non-IID)
federated classification methods.",2301.00489v2,https://arxiv.org/pdf/2301.00489v2
"Second Thoughts are Best: Learning to Re-Align With Human Values from
  Text Edits","Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony X Liu, Soroush Vosoughi","We present Second Thought, a new learning paradigm that enables language
models (LMs) to re-align with human values. By modeling the chain-of-edits
between value-unaligned and value-aligned text, with LM fine-tuning and
additional refinement through reinforcement learning, Second Thought not only
achieves superior performance in three value alignment benchmark datasets but
also shows strong human-value transfer learning ability in few-shot scenarios.
The generated editing steps also offer better interpretability and ease for
interactive error correction. Extensive human evaluations further confirm its
effectiveness.",2301.00355v2,https://arxiv.org/pdf/2301.00355v2
"MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality
  Hybrid","Zhuo Chen, Jiaoyan Chen, Wen Zhang, Lingbing Guo, Yin Fang, Yufeng Huang, Yichi Zhang, Yuxia Geng, Jeff Z. Pan, Wenting Song, Huajun Chen","Multi-modal entity alignment (MMEA) aims to discover identical entities
across different knowledge graphs (KGs) whose entities are associated with
relevant images. However, current MMEA algorithms rely on KG-level modality
fusion strategies for multi-modal entity representation, which ignores the
variations of modality preferences of different entities, thus compromising
robustness against noise in modalities such as blurry images and relations.
This paper introduces MEAformer, a multi-modal entity alignment transformer
approach for meta modality hybrid, which dynamically predicts the mutual
correlation coefficients among modalities for more fine-grained entity-level
modality fusion and alignment. Experimental results demonstrate that our model
not only achieves SOTA performance in multiple training scenarios, including
supervised, unsupervised, iterative, and low-resource settings, but also has a
limited number of parameters, efficient runtime, and interpretability. Our code
is available at https://github.com/zjukg/MEAformer.",2212.14454v4,https://arxiv.org/pdf/2212.14454v4
"WL-Align: Weisfeiler-Lehman Relabeling for Aligning Users across
  Networks via Regularized Representation Learning","Li Liu, Penggang Chen, Xin Li, William K. Cheung, Youmin Zhang, Qun Liu, Guoyin Wang","Aligning users across networks using graph representation learning has been
found effective where the alignment is accomplished in a low-dimensional
embedding space. Yet, achieving highly precise alignment is still challenging,
especially when nodes with long-range connectivity to the labeled anchors are
encountered. To alleviate this limitation, we purposefully designed WL-Align
which adopts a regularized representation learning framework to learn
distinctive node representations. It extends the Weisfeiler-Lehman Isormorphism
Test and learns the alignment in alternating phases of ""across-network
Weisfeiler-Lehman relabeling"" and ""proximity-preserving representation
learning"". The across-network Weisfeiler-Lehman relabeling is achieved through
iterating the anchor-based label propagation and a similarity-based hashing to
exploit the known anchors' connectivity to different nodes in an efficient and
robust manner. The representation learning module preserves the second-order
proximity within individual networks and is regularized by the across-network
Weisfeiler-Lehman hash labels. Extensive experiments on real-world and
synthetic datasets have demonstrated that our proposed WL-Align outperforms the
state-of-the-art methods, achieving significant performance improvements in the
""exact matching"" scenario. Data and code of WL-Align are available at
https://github.com/ChenPengGang/WLAlignCode.",2212.14182v1,https://arxiv.org/pdf/2212.14182v1
"Improving Complex Knowledge Base Question Answering via
  Question-to-Action and Question-to-Question Alignment","Yechun Tang, Xiaoxia Cheng, Weiming Lu","Complex knowledge base question answering can be achieved by converting
questions into sequences of predefined actions. However, there is a significant
semantic and structural gap between natural language and action sequences,
which makes this conversion difficult. In this paper, we introduce an
alignment-enhanced complex question answering framework, called ALCQA, which
mitigates this gap through question-to-action alignment and
question-to-question alignment. We train a question rewriting model to align
the question and each action, and utilize a pretrained language model to
implicitly align the question and KG artifacts. Moreover, considering that
similar questions correspond to similar action sequences, we retrieve top-k
similar question-answer pairs at the inference stage through
question-to-question alignment and propose a novel reward-guided action
sequence selection strategy to select from candidate action sequences. We
conduct experiments on CQA and WQSP datasets, and the results show that our
approach outperforms state-of-the-art methods and obtains a 9.88\% improvements
in the F1 metric on CQA dataset. Our source code is available at
https://github.com/TTTTTTTTy/ALCQA.",2212.13036v1,https://arxiv.org/pdf/2212.13036v1
Improved Kernel Alignment Regret Bound for Online Kernel Learning,"Junfan Li, Shizhong Liao","In this paper, we improve the kernel alignment regret bound for online kernel
learning in the regime of the Hinge loss function. Previous algorithm achieves
a regret of $O((\mathcal{A}_TT\ln{T})^{\frac{1}{4}})$ at a computational
complexity (space and per-round time) of $O(\sqrt{\mathcal{A}_TT\ln{T}})$,
where $\mathcal{A}_T$ is called \textit{kernel alignment}. We propose an
algorithm whose regret bound and computational complexity are better than
previous results. Our results depend on the decay rate of eigenvalues of the
kernel matrix. If the eigenvalues of the kernel matrix decay exponentially,
then our algorithm enjoys a regret of $O(\sqrt{\mathcal{A}_T})$ at a
computational complexity of $O(\ln^2{T})$. Otherwise, our algorithm enjoys a
regret of $O((\mathcal{A}_TT)^{\frac{1}{4}})$ at a computational complexity of
$O(\sqrt{\mathcal{A}_TT})$. We extend our algorithm to batch learning and
obtain a $O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$ excess risk bound
which improves the previous $O(1/\sqrt{T})$ bound.",2212.12989v4,https://arxiv.org/pdf/2212.12989v4
Alignment Entropy Regularization,"Ehsan Variani, Ke Wu, David Rybach, Cyril Allauzen, Michael Riley","Existing training criteria in automatic speech recognition(ASR) permit the
model to freely explore more than one time alignments between the feature and
label sequences. In this paper, we use entropy to measure a model's
uncertainty, i.e. how it chooses to distribute the probability mass over the
set of allowed alignments. Furthermore, we evaluate the effect of entropy
regularization in encouraging the model to distribute the probability mass only
on a smaller subset of allowed alignments. Experiments show that entropy
regularization enables a much simpler decoding method without sacrificing word
error rate, and provides better time alignment quality.",2212.12442v1,https://arxiv.org/pdf/2212.12442v1
"Methodological reflections for AI alignment research using human
  feedback","Thilo Hagendorff, Sarah Fabi","The field of artificial intelligence (AI) alignment aims to investigate
whether AI technologies align with human interests and values and function in a
safe and ethical manner. AI alignment is particularly relevant for large
language models (LLMs), which have the potential to exhibit unintended behavior
due to their ability to learn and adapt in ways that are difficult to predict.
In this paper, we discuss methodological challenges for the alignment problem
specifically in the context of LLMs trained to summarize texts. In particular,
we focus on methods for collecting reliable human feedback on summaries to
train a reward model which in turn improves the summarization model. We
conclude by suggesting specific improvements in the experimental design of
alignment studies for LLMs' summarization capabilities.",2301.06859v1,https://arxiv.org/pdf/2301.06859v1
Self-Instruct: Aligning Language Models with Self-Generated Instructions,"Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi","Large ""instruction-tuned"" language models (i.e., finetuned to respond to
instructions) have demonstrated a remarkable ability to generalize zero-shot to
new tasks. Nevertheless, they depend heavily on human-written instruction data
that is often limited in quantity, diversity, and creativity, therefore
hindering the generality of the tuned model. We introduce Self-Instruct, a
framework for improving the instruction-following capabilities of pretrained
language models by bootstrapping off their own generations. Our pipeline
generates instructions, input, and output samples from a language model, then
filters invalid or similar ones before using them to finetune the original
model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute
improvement over the original model on Super-NaturalInstructions, on par with
the performance of InstructGPT-001, which was trained with private user data
and human annotations. For further evaluation, we curate a set of
expert-written instructions for novel tasks, and show through human evaluation
that tuning GPT3 with Self-Instruct outperforms using existing public
instruction datasets by a large margin, leaving only a 5% absolute gap behind
InstructGPT-001. Self-Instruct provides an almost annotation-free method for
aligning pre-trained language models with instructions, and we release our
large synthetic dataset to facilitate future studies on instruction tuning. Our
code and data are available at https://github.com/yizhongw/self-instruct.",2212.10560v2,https://arxiv.org/pdf/2212.10560v2
"Cross-modal Attention Congruence Regularization for Vision-Language
  Relation Alignment","Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency","Despite recent progress towards scaling up multimodal vision-language models,
these models are still known to struggle on compositional generalization
benchmarks such as Winoground. We find that a critical component lacking from
current vision-language models is relation-level alignment: the ability to
match directional semantic relations in text (e.g., ""mug in grass"") with
spatial relationships in the image (e.g., the position of the mug relative to
the grass). To tackle this problem, we show that relation alignment can be
enforced by encouraging the directed language attention from 'mug' to 'grass'
(capturing the semantic relation 'in') to match the directed visual attention
from the mug to the grass. Tokens and their corresponding objects are softly
identified using the cross-modal attention. We prove that this notion of soft
relation alignment is equivalent to enforcing congruence between vision and
language attention matrices under a 'change of basis' provided by the
cross-modal attention matrix. Intuitively, our approach projects visual
attention into the language attention space to calculate its divergence from
the actual language attention, and vice versa. We apply our Cross-modal
Attention Congruence Regularization (CACR) loss to UNITER and improve on the
state-of-the-art approach to Winoground.",2212.10549v2,https://arxiv.org/pdf/2212.10549v2
"Mini-Model Adaptation: Efficiently Extending Pretrained Models to New
  Languages via Aligned Shallow Training","Kelly Marchisio, Patrick Lewis, Yihong Chen, Mikel Artetxe","Prior work shows that it is possible to expand pretrained Masked Language
Models (MLMs) to new languages by learning a new set of embeddings, while
keeping the transformer body frozen. Despite learning a small subset of
parameters, this approach is not compute-efficient, as training the new
embeddings requires a full forward and backward pass over the entire model. We
propose mini-model adaptation, a compute-efficient alternative that builds a
shallow mini-model from a fraction of a large model's parameters. New
language-specific embeddings can then be efficiently trained over the
mini-model and plugged into the aligned large model for rapid cross-lingual
transfer. We explore two approaches to learn mini-models: MiniJoint, which
jointly pretrains the primary model and the mini-model using a single
transformer with a secondary MLM head at a middle layer; and MiniPost, where we
start from a regular pretrained model, build a mini-model by extracting and
freezing a few layers, and learn a small number of parameters on top.
Experiments on XNLI, MLQA and PAWS-X show that mini-model adaptation matches
the performance of the standard approach using 2.3x less compute on average.",2212.10503v2,https://arxiv.org/pdf/2212.10503v2
DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue,"William Held, Christopher Hidey, Fei Liu, Eric Zhu, Rahul Goel, Diyi Yang, Rushin Shah","Modern virtual assistants use internal semantic parsing engines to convert
user utterances to actionable commands. However, prior work has demonstrated
that semantic parsing is a difficult multilingual transfer task with low
transfer efficiency compared to other tasks. In global markets such as India
and Latin America, this is a critical issue as switching between languages is
prevalent for bilingual users. In this work we dramatically improve the
zero-shot performance of a multilingual and codeswitched semantic parsing
system using two stages of multilingual alignment. First, we show that
constrastive alignment pretraining improves both English performance and
transfer efficiency. We then introduce a constrained optimization approach for
hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned
Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and
81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing
benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer
parameters.",2212.08054v2,https://arxiv.org/pdf/2212.08054v2
"Low-Variance Forward Gradients using Direct Feedback Alignment and
  Momentum","Florian Bacho, Dominique Chu","Supervised learning in deep neural networks is commonly performed using error
backpropagation. However, the sequential propagation of errors during the
backward pass limits its scalability and applicability to low-powered
neuromorphic hardware. Therefore, there is growing interest in finding local
alternatives to backpropagation. Recently proposed methods based on
forward-mode automatic differentiation suffer from high variance in large deep
neural networks, which affects convergence. In this paper, we propose the
Forward Direct Feedback Alignment algorithm that combines Activity-Perturbed
Forward Gradients with Direct Feedback Alignment and momentum. We provide both
theoretical proofs and empirical evidence that our proposed method achieves
lower variance than forward gradient techniques. In this way, our approach
enables faster convergence and better performance when compared to other local
alternatives to backpropagation and opens a new perspective for the development
of online learning algorithms compatible with neuromorphic systems.",2212.07282v4,https://arxiv.org/pdf/2212.07282v4
Aligning Visual and Lexical Semantics,"Fausto Giunchiglia, Mayukh Bagchi, Xiaolei Diao","We discuss two kinds of semantics relevant to Computer Vision (CV) systems -
Visual Semantics and Lexical Semantics. While visual semantics focus on how
humans build concepts when using vision to perceive a target reality, lexical
semantics focus on how humans build concepts of the same target reality through
the use of language. The lack of coincidence between visual and lexical
semantics, in turn, has a major impact on CV systems in the form of the
Semantic Gap Problem (SGP). The paper, while extensively exemplifying the lack
of coincidence as above, introduces a general, domain-agnostic methodology to
enforce alignment between visual and lexical semantics.",2212.06629v1,https://arxiv.org/pdf/2212.06629v1
"Improving Depression estimation from facial videos with face alignment,
  training optimization and scheduling","Manuel Lage Cañellas, Constantino Álvarez Casado, Le Nguyen, Miguel Bordallo López","Deep learning models have shown promising results in recognizing depressive
states using video-based facial expressions. While successful models typically
leverage using 3D-CNNs or video distillation techniques, the different use of
pretraining, data augmentation, preprocessing, and optimization techniques
across experiments makes it difficult to make fair architectural comparisons.
We propose instead to enhance two simple models based on ResNet-50 that use
only static spatial information by using two specific face alignment methods
and improved data augmentation, optimization, and scheduling techniques. Our
extensive experiments on benchmark datasets obtain similar results to
sophisticated spatio-temporal models for single streams, while the score-level
fusion of two different streams outperforms state-of-the-art methods. Our
findings suggest that specific modifications in the preprocessing and training
process result in noticeable differences in the performance of the models and
could hide the actual originally attributed to the use of different neural
network architectures.",2212.06400v1,https://arxiv.org/pdf/2212.06400v1
Deep Learning for Inertial Sensor Alignment,"Maxim Freydin, Niv Sfaradi, Nimrod Segol, Areej Eweida, Barak Or","Accurate alignment of a fixed mobile device equipped with inertial sensors
inside a moving vehicle is important for navigation, activity recognition, and
other applications. Accurate estimation of the device mounting angle is
required to rotate the inertial measurement from the sensor frame to the moving
platform frame to standardize measurements and improve the performance of the
target task. In this work, a data-driven approach using deep neural networks
(DNNs) is proposed to learn the yaw mounting angle of a smartphone equipped
with an inertial measurement unit (IMU) and strapped to a car. The proposed
model uses only the accelerometer and gyroscope readings from an IMU as input
and, in contrast to existing solutions, does not require global position inputs
from global navigation satellite systems (GNSS). To train the model in a
supervised manner, IMU data is collected for training and validation with the
sensor mounted at a known yaw mounting angle, and a range of ground truth
labels is generated by applying a random rotation in a bounded range to the
measurements. The trained model is tested on data with real rotations showing
similar performance as with synthetic rotations. The trained model is deployed
on an Android device and evaluated in real-time to test the accuracy of the
estimated yaw mounting angle. The model is shown to find the mounting angle at
an accuracy of 8 degrees within 5 seconds, and 4 degrees within 27 seconds. An
experiment is conducted to compare the proposed model with an existing
off-the-shelf solution.",2212.11120v2,https://arxiv.org/pdf/2212.11120v2
"Reliable Multimodal Trajectory Prediction via Error Aligned Uncertainty
  Optimization","Neslihan Kose, Ranganath Krishnan, Akash Dhamasia, Omesh Tickoo, Michael Paulitsch","Reliable uncertainty quantification in deep neural networks is very crucial
in safety-critical applications such as automated driving for trustworthy and
informed decision-making. Assessing the quality of uncertainty estimates is
challenging as ground truth for uncertainty estimates is not available.
Ideally, in a well-calibrated model, uncertainty estimates should perfectly
correlate with model error. We propose a novel error aligned uncertainty
optimization method and introduce a trainable loss function to guide the models
to yield good quality uncertainty estimates aligning with the model error. Our
approach targets continuous structured prediction and regression tasks, and is
evaluated on multiple datasets including a large-scale vehicle motion
prediction task involving real-world distributional shifts. We demonstrate that
our method improves average displacement error by 1.69% and 4.69%, and the
uncertainty correlation with model error by 17.22% and 19.13% as quantified by
Pearson correlation coefficient on two state-of-the-art baselines.",2212.04812v1,https://arxiv.org/pdf/2212.04812v1
"Domain-Invariant Feature Alignment Using Variational Inference For
  Partial Domain Adaptation","Sandipan Choudhuri, Suli Adeniye, Arunabha Sen, Hemanth Venkateswara","The standard closed-set domain adaptation approaches seek to mitigate
distribution discrepancies between two domains under the constraint of both
sharing identical label sets. However, in realistic scenarios, finding an
optimal source domain with identical label space is a challenging task. Partial
domain adaptation alleviates this problem of procuring a labeled dataset with
identical label space assumptions and addresses a more practical scenario where
the source label set subsumes the target label set. This, however, presents a
few additional obstacles during adaptation. Samples with categories private to
the source domain thwart relevant knowledge transfer and degrade model
performance. In this work, we try to address these issues by coupling
variational information and adversarial learning with a pseudo-labeling
technique to enforce class distribution alignment and minimize the transfer of
superfluous information from the source samples. The experimental findings in
numerous cross-domain classification tasks demonstrate that the proposed
technique delivers superior and comparable accuracy to existing methods.",2212.01590v1,https://arxiv.org/pdf/2212.01590v1
"Cross-Domain Graph Anomaly Detection via Anomaly-aware Contrastive
  Alignment","Qizhou Wang, Guansong Pang, Mahsa Salehi, Wray Buntine, Christopher Leckie","Cross-domain graph anomaly detection (CD-GAD) describes the problem of
detecting anomalous nodes in an unlabelled target graph using auxiliary,
related source graphs with labelled anomalous and normal nodes. Although it
presents a promising approach to address the notoriously high false positive
issue in anomaly detection, little work has been done in this line of research.
There are numerous domain adaptation methods in the literature, but it is
difficult to adapt them for GAD due to the unknown distributions of the
anomalies and the complex node relations embedded in graph data. To this end,
we introduce a novel domain adaptation approach, namely Anomaly-aware
Contrastive alignmenT (ACT), for GAD. ACT is designed to jointly optimise: (i)
unsupervised contrastive learning of normal representations of nodes in the
target graph, and (ii) anomaly-aware one-class alignment that aligns these
contrastive node representations and the representations of labelled normal
nodes in the source graph, while enforcing significant deviation of the
representations of the normal nodes from the labelled anomalous nodes in the
source graph. In doing so, ACT effectively transfers anomaly-informed knowledge
from the source graph to learn the complex node relations of the normal class
for GAD on the target graph without any specification of the anomaly
distributions. Extensive experiments on eight CD-GAD settings demonstrate that
our approach ACT achieves substantially improved detection performance over 10
state-of-the-art GAD methods. Code is available at
https://github.com/QZ-WANG/ACT.",2212.01096v1,https://arxiv.org/pdf/2212.01096v1
BASiS: Batch Aligned Spectral Embedding Space,"Or Streicher, Ido Cohen, Guy Gilboa","Graph is a highly generic and diverse representation, suitable for almost any
data processing problem. Spectral graph theory has been shown to provide
powerful algorithms, backed by solid linear algebra theory. It thus can be
extremely instrumental to design deep network building blocks with spectral
graph characteristics. For instance, such a network allows the design of
optimal graphs for certain tasks or obtaining a canonical orthogonal
low-dimensional embedding of the data. Recent attempts to solve this problem
were based on minimizing Rayleigh-quotient type losses. We propose a different
approach of directly learning the eigensapce. A severe problem of the direct
approach, applied in batch-learning, is the inconsistent mapping of features to
eigenspace coordinates in different batches. We analyze the degrees of freedom
of learning this task using batches and propose a stable alignment mechanism
that can work both with batch changes and with graph-metric changes. We show
that our learnt spectral embedding is better in terms of NMI, ACC, Grassman
distance, orthogonality and classification accuracy, compared to SOTA. In
addition, the learning is more stable.",2211.16960v2,https://arxiv.org/pdf/2211.16960v2
Soft Alignment Objectives for Robust Adaptation of Language Generation,"Michal Štefánik, Marek Kadlčík, Petr Sojka","Domain adaptation allows generative language models to address specific flaws
caused by the domain shift of their application. However, the traditional
adaptation by further training on in-domain data rapidly weakens the model's
ability to generalize to other domains, making the open-ended deployments of
the adapted models prone to errors. This work introduces novel training
objectives built upon a semantic similarity of the predicted tokens to the
reference.
  Our results show that (1) avoiding the common assumption of a single correct
prediction by constructing the training target from tokens' semantic similarity
can mitigate catastrophic forgetting during domain adaptation, while (2)
preserving the quality of the adaptation, (3) with negligible additions to
compute costs.
  In the broader context, the objectives grounded in a continuous token
similarity pioneer the exploration of the middle ground between the efficient
but na\""{\i}ve exact-match token-level objectives and expressive but
computationally- and resource-intensive sequential objectives.",2211.16550v2,https://arxiv.org/pdf/2211.16550v2
Dependency-aware Self-training for Entity Alignment,"Bing Liu, Tiancheng Lan, Wen Hua, Guido Zuccon","Entity Alignment (EA), which aims to detect entity mappings (i.e. equivalent
entity pairs) in different Knowledge Graphs (KGs), is critical for KG fusion.
Neural EA methods dominate current EA research but still suffer from their
reliance on labelled mappings. To solve this problem, a few works have explored
boosting the training of EA models with self-training, which adds confidently
predicted mappings into the training data iteratively. Though the effectiveness
of self-training can be glimpsed in some specific settings, we still have very
limited knowledge about it. One reason is the existing works concentrate on
devising EA models and only treat self-training as an auxiliary tool. To fill
this knowledge gap, we change the perspective to self-training to shed light on
it. In addition, the existing self-training strategies have limited impact
because they introduce either much False Positive noise or a low quantity of
True Positive pseudo mappings. To improve self-training for EA, we propose
exploiting the dependencies between entities, a particularity of EA, to
suppress the noise without hurting the recall of True Positive mappings.
Through extensive experiments, we show that the introduction of dependency
makes the self-training strategy for EA reach a new level. The value of
self-training in alleviating the reliance on annotation is actually much higher
than what has been realised. Furthermore, we suggest future study on smart data
annotation to break the ceiling of EA performance.",2211.16101v1,https://arxiv.org/pdf/2211.16101v1
Guiding Neural Entity Alignment with Compatibility,"Bing Liu, Harrisen Scells, Wen Hua, Guido Zuccon, Genghong Zhao, Xia Zhang","Entity Alignment (EA) aims to find equivalent entities between two Knowledge
Graphs (KGs). While numerous neural EA models have been devised, they are
mainly learned using labelled data only. In this work, we argue that different
entities within one KG should have compatible counterparts in the other KG due
to the potential dependencies among the entities. Making compatible predictions
thus should be one of the goals of training an EA model along with fitting the
labelled data: this aspect however is neglected in current methods. To power
neural EA models with compatibility, we devise a training framework by
addressing three problems: (1) how to measure the compatibility of an EA model;
(2) how to inject the property of being compatible into an EA model; (3) how to
optimise parameters of the compatibility model. Extensive experiments on
widely-used datasets demonstrate the advantages of integrating compatibility
within EA models. In fact, state-of-the-art neural EA models trained within our
framework using just 5\% of the labelled data can achieve comparable
effectiveness with supervised training using 20\% of the labelled data.",2211.15833v1,https://arxiv.org/pdf/2211.15833v1
"Joint Multimodal Entity-Relation Extraction Based on Edge-enhanced Graph
  Alignment Network and Word-pair Relation Tagging","Li Yuan, Yi Cai, Jin Wang, Qing Li","Multimodal named entity recognition (MNER) and multimodal relation extraction
(MRE) are two fundamental subtasks in the multimodal knowledge graph
construction task. However, the existing methods usually handle two tasks
independently, which ignores the bidirectional interaction between them. This
paper is the first to propose jointly performing MNER and MRE as a joint
multimodal entity-relation extraction task (JMERE). Besides, the current MNER
and MRE models only consider aligning the visual objects with textual entities
in visual and textual graphs but ignore the entity-entity relationships and
object-object relationships. To address the above challenges, we propose an
edge-enhanced graph alignment network and a word-pair relation tagging (EEGA)
for JMERE task. Specifically, we first design a word-pair relation tagging to
exploit the bidirectional interaction between MNER and MRE and avoid the error
propagation. Then, we propose an edge-enhanced graph alignment network to
enhance the JMERE task by aligning nodes and edges in the cross-graph. Compared
with previous methods, the proposed method can leverage the edge information to
auxiliary alignment between objects and entities and find the correlations
between entity-entity relationships and object-object relationships.
Experiments are conducted to show the effectiveness of our model.",2211.15028v2,https://arxiv.org/pdf/2211.15028v2
Label Alignment Regularization for Distribution Shift,"Ehsan Imani, Guojun Zhang, Runjia Li, Jun Luo, Pascal Poupart, Philip H. S. Torr, Yangchen Pan","Recent work has highlighted the label alignment property (LAP) in supervised
learning, where the vector of all labels in the dataset is mostly in the span
of the top few singular vectors of the data matrix. Drawing inspiration from
this observation, we propose a regularization method for unsupervised domain
adaptation that encourages alignment between the predictions in the target
domain and its top singular vectors. Unlike conventional domain adaptation
approaches that focus on regularizing representations, we instead regularize
the classifier to align with the unsupervised target data, guided by the LAP in
both the source and target domains. Theoretical analysis demonstrates that,
under certain assumptions, our solution resides within the span of the top
right singular vectors of the target domain data and aligns with the optimal
solution. By removing the reliance on the commonly used optimal joint risk
assumption found in classic domain adaptation theory, we showcase the
effectiveness of our method on addressing problems where traditional domain
adaptation methods often fall short due to high joint error. Additionally, we
report improved performance over domain adaptation baselines in well-known
tasks such as MNIST-USPS domain adaptation and cross-lingual sentiment
analysis.",2211.14960v4,https://arxiv.org/pdf/2211.14960v4
"Rethinking Alignment and Uniformity in Unsupervised Image Semantic
  Segmentation","Daoan Zhang, Chenming Li, Haoquan Li, Wenjian Huang, Lingyun Huang, Jianguo Zhang","Unsupervised image semantic segmentation(UISS) aims to match low-level visual
features with semantic-level representations without outer supervision. In this
paper, we address the critical properties from the view of feature alignments
and feature uniformity for UISS models. We also make a comparison between UISS
and image-wise representation learning. Based on the analysis, we argue that
the existing MI-based methods in UISS suffer from representation collapse. By
this, we proposed a robust network called Semantic Attention Network(SAN), in
which a new module Semantic Attention(SEAT) is proposed to generate pixel-wise
and semantic features dynamically. Experimental results on multiple semantic
segmentation benchmarks show that our unsupervised segmentation framework
specializes in catching semantic representations, which outperforms all the
unpretrained and even several pretrained methods.",2211.14513v2,https://arxiv.org/pdf/2211.14513v2
Learning Compact Features via In-Training Representation Alignment,"Xin Li, Xiangrui Li, Deng Pan, Yao Qiang, Dongxiao Zhu","Deep neural networks (DNNs) for supervised learning can be viewed as a
pipeline of the feature extractor (i.e., last hidden layer) and a linear
classifier (i.e., output layer) that are trained jointly with stochastic
gradient descent (SGD) on the loss function (e.g., cross-entropy). In each
epoch, the true gradient of the loss function is estimated using a mini-batch
sampled from the training set and model parameters are then updated with the
mini-batch gradients. Although the latter provides an unbiased estimation of
the former, they are subject to substantial variances derived from the size and
number of sampled mini-batches, leading to noisy and jumpy updates. To
stabilize such undesirable variance in estimating the true gradients, we
propose In-Training Representation Alignment (ITRA) that explicitly aligns
feature distributions of two different mini-batches with a matching loss in the
SGD training process. We also provide a rigorous analysis of the desirable
effects of the matching loss on feature representation learning: (1) extracting
compact feature representation; (2) reducing over-adaption on mini-batches via
an adaptive weighting mechanism; and (3) accommodating to multi-modalities.
Finally, we conduct large-scale experiments on both image and text
classifications to demonstrate its superior performance to the strong
baselines.",2211.13332v1,https://arxiv.org/pdf/2211.13332v1
"How do Cross-View and Cross-Modal Alignment Affect Representations in
  Contrastive Learning?","Thomas M. Hehn, Julian F. P. Kooij, Dariu M. Gavrila","Various state-of-the-art self-supervised visual representation learning
approaches take advantage of data from multiple sensors by aligning the feature
representations across views and/or modalities. In this work, we investigate
how aligning representations affects the visual features obtained from
cross-view and cross-modal contrastive learning on images and point clouds. On
five real-world datasets and on five tasks, we train and evaluate 108 models
based on four pretraining variations. We find that cross-modal representation
alignment discards complementary visual information, such as color and texture,
and instead emphasizes redundant depth cues. The depth cues obtained from
pretraining improve downstream depth prediction performance. Also overall,
cross-modal alignment leads to more robust encoders than pre-training by
cross-view alignment, especially on depth prediction, instance segmentation,
and object detection.",2211.13309v1,https://arxiv.org/pdf/2211.13309v1
"Distilling Knowledge from Self-Supervised Teacher by Embedding Graph
  Alignment","Yuchen Ma, Yanbei Chen, Zeynep Akata","Recent advances have indicated the strengths of self-supervised pre-training
for improving representation learning on downstream tasks. Existing works often
utilize self-supervised pre-trained models by fine-tuning on downstream tasks.
However, fine-tuning does not generalize to the case when one needs to build a
customized model architecture different from the self-supervised model. In this
work, we formulate a new knowledge distillation framework to transfer the
knowledge from self-supervised pre-trained models to any other student network
by a novel approach named Embedding Graph Alignment. Specifically, inspired by
the spirit of instance discrimination in self-supervised learning, we model the
instance-instance relations by a graph formulation in the feature embedding
space and distill the self-supervised teacher knowledge to a student network by
aligning the teacher graph and the student graph. Our distillation scheme can
be flexibly applied to transfer the self-supervised knowledge to enhance
representation learning on various student networks. We demonstrate that our
model outperforms multiple representative knowledge distillation methods on
three benchmark datasets, including CIFAR100, STL10, and TinyImageNet. Code is
here: https://github.com/yccm/EGA.",2211.13264v1,https://arxiv.org/pdf/2211.13264v1
"Aligning Source Visual and Target Language Domains for Unpaired Video
  Captioning","Fenglin Liu, Xian Wu, Chenyu You, Shen Ge, Yuexian Zou, Xu Sun","Training supervised video captioning model requires coupled video-caption
pairs. However, for many targeted languages, sufficient paired data are not
available. To this end, we introduce the unpaired video captioning task aiming
to train models without coupled video-caption pairs in target language. To
solve the task, a natural choice is to employ a two-step pipeline system: first
utilizing video-to-pivot captioning model to generate captions in pivot
language and then utilizing pivot-to-target translation model to translate the
pivot captions to the target language. However, in such a pipeline system, 1)
visual information cannot reach the translation model, generating visual
irrelevant target captions; 2) the errors in the generated pivot captions will
be propagated to the translation model, resulting in disfluent target captions.
To address these problems, we propose the Unpaired Video Captioning with Visual
Injection system (UVC-VI). UVC-VI first introduces the Visual Injection Module
(VIM), which aligns source visual and target language domains to inject the
source visual information into the target language domain. Meanwhile, VIM
directly connects the encoder of the video-to-pivot model and the decoder of
the pivot-to-target model, allowing end-to-end inference by completely skipping
the generation of pivot captions. To enhance the cross-modality injection of
the VIM, UVC-VI further introduces a pluggable video encoder, i.e., Multimodal
Collaborative Encoder (MCE). The experiments show that UVC-VI outperforms
pipeline systems and exceeds several supervised systems. Furthermore, equipping
existing supervised systems with our MCE can achieve 4% and 7% relative margins
on the CIDEr scores to current state-of-the-art models on the benchmark MSVD
and MSR-VTT datasets, respectively.",2211.12148v1,https://arxiv.org/pdf/2211.12148v1
On the Alignment of Group Fairness with Attribute Privacy,"Jan Aalmoes, Vasisht Duddu, Antoine Boutet","Group fairness and privacy are fundamental aspects in designing trustworthy
machine learning models. Previous research has highlighted conflicts between
group fairness and different privacy notions. We are the first to demonstrate
the alignment of group fairness with the specific privacy notion of attribute
privacy in a blackbox setting. Attribute privacy, quantified by the resistance
to attribute inference attacks (AIAs), requires indistinguishability in the
target model's output predictions. Group fairness guarantees this thereby
mitigating AIAs and achieving attribute privacy. To demonstrate this, we first
introduce AdaptAIA, an enhancement of existing AIAs, tailored for real-world
datasets with class imbalances in sensitive attributes. Through theoretical and
extensive empirical analyses, we demonstrate the efficacy of two standard group
fairness algorithms (i.e., adversarial debiasing and exponentiated gradient
descent) against AdaptAIA. Additionally, since using group fairness results in
attribute privacy, it acts as a defense against AIAs, which is currently
lacking. Overall, we show that group fairness aligns with attribute privacy at
no additional cost other than the already existing trade-off with model
utility.",2211.10209v3,https://arxiv.org/pdf/2211.10209v3
"FedFA: Federated Learning with Feature Anchors to Align Features and
  Classifiers for Heterogeneous Data","Tailin Zhou, Jun Zhang, Danny H. K. Tsang","Federated learning allows multiple clients to collaboratively train a model
without exchanging their data, thus preserving data privacy. Unfortunately, it
suffers significant performance degradation due to heterogeneous data at
clients. Common solutions involve designing an auxiliary loss to regularize
weight divergence or feature inconsistency during local training. However, we
discover that these approaches fall short of the expected performance because
they ignore the existence of a vicious cycle between feature inconsistency and
classifier divergence across clients. This vicious cycle causes client models
to be updated in inconsistent feature spaces with more diverged classifiers. To
break the vicious cycle, we propose a novel framework named Federated learning
with Feature Anchors (FedFA). FedFA utilizes feature anchors to align features
and calibrate classifiers across clients simultaneously. This enables client
models to be updated in a shared feature space with consistent classifiers
during local training. Theoretically, we analyze the non-convex convergence
rate of FedFA. We also demonstrate that the integration of feature alignment
and classifier calibration in FedFA brings a virtuous cycle between feature and
classifier updates, which breaks the vicious cycle existing in current
approaches. Extensive experiments show that FedFA significantly outperforms
existing approaches on various classification datasets under label distribution
skew and feature distribution skew.",2211.09299v4,https://arxiv.org/pdf/2211.09299v4
"ALIGN-MLM: Word Embedding Alignment is Crucial for Multilingual
  Pre-training","Henry Tang, Ameet Deshpande, Karthik Narasimhan","Multilingual pre-trained models exhibit zero-shot cross-lingual transfer,
where a model fine-tuned on a source language achieves surprisingly good
performance on a target language. While studies have attempted to understand
transfer, they focus only on MLM, and the large number of differences between
natural languages makes it hard to disentangle the importance of different
properties. In this work, we specifically highlight the importance of word
embedding alignment by proposing a pre-training objective (ALIGN-MLM) whose
auxiliary loss guides similar words in different languages to have similar word
embeddings. ALIGN-MLM either outperforms or matches three widely adopted
objectives (MLM, XLM, DICT-MLM) when we evaluate transfer between pairs of
natural languages and their counterparts created by systematically modifying
specific properties like the script. In particular, ALIGN-MLM outperforms XLM
and MLM by 35 and 30 F1 points on POS-tagging for transfer between languages
that differ both in their script and word order (left-to-right v.s.
right-to-left). We also show a strong correlation between alignment and
transfer for all objectives (e.g., rho=0.727 for XNLI), which together with
ALIGN-MLM's strong performance calls for explicitly aligning word embeddings
for multilingual models.",2211.08547v1,https://arxiv.org/pdf/2211.08547v1
"IntegratedPIFu: Integrated Pixel Aligned Implicit Function for
  Single-view Human Reconstruction","Kennard Yanting Chan, Guosheng Lin, Haiyu Zhao, Weisi Lin","We propose IntegratedPIFu, a new pixel aligned implicit model that builds on
the foundation set by PIFuHD. IntegratedPIFu shows how depth and human parsing
information can be predicted and capitalised upon in a pixel-aligned implicit
model. In addition, IntegratedPIFu introduces depth oriented sampling, a novel
training scheme that improve any pixel aligned implicit model ability to
reconstruct important human features without noisy artefacts. Lastly,
IntegratedPIFu presents a new architecture that, despite using less model
parameters than PIFuHD, is able to improves the structural correctness of
reconstructed meshes. Our results show that IntegratedPIFu significantly
outperforms existing state of the arts methods on single view human
reconstruction. Our code has been made available online.",2211.07955v2,https://arxiv.org/pdf/2211.07955v2
Learning to Model Multimodal Semantic Alignment for Story Visualization,"Bowen Li, Thomas Lukasiewicz","Story visualization aims to generate a sequence of images to narrate each
sentence in a multi-sentence story, where the images should be realistic and
keep global consistency across dynamic scenes and characters. Current works
face the problem of semantic misalignment because of their fixed architecture
and diversity of input modalities. To address this problem, we explore the
semantic alignment between text and image representations by learning to match
their semantic levels in the GAN-based generative model. More specifically, we
introduce dynamic interactions according to learning to dynamically explore
various semantic depths and fuse the different-modal information at a matched
semantic level, which thus relieves the text-image semantic misalignment
problem. Extensive experiments on different datasets demonstrate the
improvements of our approach, neither using segmentation masks nor auxiliary
captioning networks, on image quality and story consistency, compared with
state-of-the-art methods.",2211.07289v1,https://arxiv.org/pdf/2211.07289v1
"The Role of Local Alignment and Uniformity in Image-Text Contrastive
  Learning on Medical Images","Philip Müller, Georgios Kaissis, Daniel Rueckert","Image-text contrastive learning has proven effective for pretraining medical
image models. When targeting localized downstream tasks like semantic
segmentation or object detection, additional local contrastive losses that
align image regions with sentences have shown promising results. We study how
local contrastive losses are related to global (per-sample) contrastive losses
and which effects they have on localized medical downstream tasks. Based on a
theoretical comparison, we propose to remove some components of local losses
and replace others by a novel distribution prior which enforces uniformity of
representations within each sample. We empirically study this approach on chest
X-ray tasks and find it to be very effective, outperforming methods without
local losses on 12 of 18 tasks.",2211.07254v2,https://arxiv.org/pdf/2211.07254v2
"Large Language Models Meet Harry Potter: A Bilingual Dataset for
  Aligning Dialogue Agents with Characters","Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, Jia Li","In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT
and GPT4 have demonstrated immense potential in constructing open-domain
dialogue agents. However, aligning these agents with specific characters or
individuals remains a considerable challenge due to the complexities of
character representation and the lack of comprehensive annotations. In this
paper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to
advance the study of dialogue agents and character alignment. The dataset
encompasses all dialogue sessions (in both English and Chinese) from the Harry
Potter series and is annotated with vital background information, including
dialogue scenes, speakers, character relationships, and attributes. These
extensive annotations may empower LLMs to unlock character-driven dialogue
capabilities. Furthermore, it can serve as a universal benchmark for evaluating
how well can a LLM aligning with a specific character. We benchmark LLMs on HPD
using both fine-tuning and in-context learning settings. Evaluation results
reveal that although there is substantial room for improvement in generating
high-quality, character-aligned responses, the proposed dataset is valuable in
guiding models toward responses that better align with the character of Harry
Potter.",2211.06869v4,https://arxiv.org/pdf/2211.06869v4
"Intent-aware Multi-source Contrastive Alignment for Tag-enhanced
  Recommendation","Haolun Wu, Yingxue Zhang, Chen Ma, Wei Guo, Ruiming Tang, Xue Liu, Mark Coates","To offer accurate and diverse recommendation services, recent methods use
auxiliary information to foster the learning process of user and item
representations. Many SOTA methods fuse different sources of information (user,
item, knowledge graph, tags, etc.) into a graph and use Graph Neural Networks
to introduce the auxiliary information through the message passing paradigm. In
this work, we seek an alternative framework that is light and effective through
self-supervised learning across different sources of information, particularly
for the commonly accessible item tag information. We use a self-supervision
signal to pair users with the auxiliary information associated with the items
they have interacted with before. To achieve the pairing, we create a proxy
training task. For a given item, the model predicts the correct pairing between
the representations obtained from the users that have interacted with this item
and the assigned tags. This design provides an efficient solution, using the
auxiliary information directly to enhance the quality of user and item
embeddings. User behavior in recommendation systems is driven by the complex
interactions of many factors behind the decision-making processes. To make the
pairing process more fine-grained and avoid embedding collapse, we propose an
intent-aware self-supervised pairing process where we split the user embeddings
into multiple sub-embedding vectors. Each sub-embedding vector captures a
specific user intent via self-supervised alignment with a particular cluster of
tags. We integrate our designed framework with various recommendation models,
demonstrating its flexibility and compatibility. Through comparison with
numerous SOTA methods on seven real-world datasets, we show that our method can
achieve better performance while requiring less training time. This indicates
the potential of applying our approach on web-scale datasets.",2211.06370v1,https://arxiv.org/pdf/2211.06370v1
"Spatiotemporal forecasting of vertical track alignment with exogenous
  factors","Katsuya Kosukegawa, Yasukuni Mori, Hiroki Suyari, Kazuhiko Kawamoto","To ensure the safety of railroad operations, it is important to monitor and
forecast track geometry irregularities. A higher safety requires forecasting
with higher spatiotemporal frequencies, which in turn requires capturing
spatial correlations. Additionally, track geometry irregularities are
influenced by multiple exogenous factors. In this study, a method is proposed
to forecast one type of track geometry irregularity, vertical alignment, by
incorporating spatial and exogenous factor calculations. The proposed method
embeds exogenous factors and captures spatiotemporal correlations using a
convolutional long short-term memory. The proposed method is also
experimentally compared with other methods in terms of the forecasting
performance. Additionally, an ablation study on exogenous factors is conducted
to examine their individual contributions to the forecasting performance. The
results reveal that spatial calculations and maintenance record data improve
the forecasting of vertical alignment.",2211.03549v2,https://arxiv.org/pdf/2211.03549v2
"HAQJSK: Hierarchical-Aligned Quantum Jensen-Shannon Kernels for Graph
  Classification","Lu Bai, Lixin Cui, Yue Wang, Ming Li, Edwin R. Hancock","In this work, we propose a family of novel quantum kernels, namely the
Hierarchical Aligned Quantum Jensen-Shannon Kernels (HAQJSK), for un-attributed
graphs. Different from most existing classical graph kernels, the proposed
HAQJSK kernels can incorporate hierarchical aligned structure information
between graphs and transform graphs of random sizes into fixed-sized aligned
graph structures, i.e., the Hierarchical Transitive Aligned Adjacency Matrix of
vertices and the Hierarchical Transitive Aligned Density Matrix of the
Continuous-Time Quantum Walk (CTQW). For a pair of graphs to hand, the
resulting HAQJSK kernels are defined by measuring the Quantum Jensen-Shannon
Divergence (QJSD) between their transitive aligned graph structures. We show
that the proposed HAQJSK kernels not only reflect richer intrinsic global graph
characteristics in terms of the CTQW, but also address the drawback of
neglecting structural correspondence information arising in most existing
R-convolution kernels. Furthermore, unlike the previous Quantum Jensen-Shannon
Kernels associated with the QJSD and the CTQW, the proposed HAQJSK kernels can
simultaneously guarantee the properties of permutation invariant and positive
definiteness, explaining the theoretical advantages of the HAQJSK kernels.
Experiments indicate the effectiveness of the proposed kernels.",2211.02904v3,https://arxiv.org/pdf/2211.02904v3
"EventEA: Benchmarking Entity Alignment for Event-centric Knowledge
  Graphs","Xiaobin Tian, Zequn Sun, Guangyao Li, Wei Hu","Entity alignment is to find identical entities in different knowledge graphs
(KGs) that refer to the same real-world object. Embedding-based entity
alignment techniques have been drawing a lot of attention recently because they
can help solve the issue of symbolic heterogeneity in different KGs. However,
in this paper, we show that the progress made in the past was due to biased and
unchallenging evaluation. We highlight two major flaws in existing datasets
that favor embedding-based entity alignment techniques, i.e., the isomorphic
graph structures in relation triples and the weak heterogeneity in attribute
triples. Towards a critical evaluation of embedding-based entity alignment
methods, we construct a new dataset with heterogeneous relations and attributes
based on event-centric KGs. We conduct extensive experiments to evaluate
existing popular methods, and find that they fail to achieve promising
performance. As a new approach to this difficult problem, we propose a
time-aware literal encoder for entity alignment. The dataset and source code
are publicly available to foster future research. Our work calls for more
effective and practical embedding-based solutions to entity alignment.",2211.02817v1,https://arxiv.org/pdf/2211.02817v1
Human alignment of neural network representations,"Lukas Muttenthaler, Jonas Dippel, Lorenz Linhardt, Robert A. Vandermeulen, Simon Kornblith","Today's computer vision models achieve human or near-human level performance
across a wide variety of vision tasks. However, their architectures, data, and
learning algorithms differ in numerous ways from those that give rise to human
vision. In this paper, we investigate the factors that affect the alignment
between the representations learned by neural networks and human mental
representations inferred from behavioral responses. We find that model scale
and architecture have essentially no effect on the alignment with human
behavioral responses, whereas the training dataset and objective function both
have a much larger impact. These findings are consistent across three datasets
of human similarity judgments collected using two different tasks. Linear
transformations of neural network representations learned from behavioral
responses from one dataset substantially improve alignment with human
similarity judgments on the other two datasets. In addition, we find that some
human concepts such as food and animals are well-represented by neural networks
whereas others such as royal or sports-related objects are not. Overall,
although models trained on larger, more diverse datasets achieve better
alignment with humans than models trained on ImageNet alone, our results
indicate that scaling alone is unlikely to be sufficient to train neural
networks with conceptual representations that match those used by humans.",2211.01201v4,https://arxiv.org/pdf/2211.01201v4
On Correlation Detection and Alignment Recovery of Gaussian Databases,Ran Tamir,"In this work, we propose an efficient two-stage algorithm solving a joint
problem of correlation detection and partial alignment recovery between two
Gaussian databases. Correlation detection is a hypothesis testing problem;
under the null hypothesis, the databases are independent, and under the
alternate hypothesis, they are correlated, under an unknown row permutation. We
develop bounds on the type-I and type-II error probabilities, and show that the
analyzed detector performs better than a recently proposed detector, at least
for some specific parameter choices. Since the proposed detector relies on a
statistic, which is a sum of dependent indicator random variables, then in
order to bound the type-I probability of error, we develop a novel
graph-theoretic technique for bounding the $k$-th order moments of such
statistics. When the databases are accepted as correlated, the algorithm also
recovers some partial alignment between the given databases. We also propose
two more algorithms: (i) One more algorithm for partial alignment recovery,
whose reliability and computational complexity are both higher than those of
the first proposed algorithm. (ii) An algorithm for full alignment recovery,
which has a reduced amount of calculations and a not much lower error
probability, when compared to the optimal recovery procedure.",2211.01069v2,https://arxiv.org/pdf/2211.01069v2
"Aligning Offline Metrics and Human Judgments of Value for Code
  Generation Models","Victor Dibia, Adam Fourney, Gagan Bansal, Forough Poursabzi-Sangdeh, Han Liu, Saleema Amershi","Large language models have demonstrated great potential to assist programmers
in generating code. For such human-AI pair programming scenarios, we
empirically demonstrate that while generated code is most often evaluated in
terms of their functional correctness (i.e., whether generations pass available
unit tests), correctness does not fully capture (e.g., may underestimate) the
productivity gains these models may provide. Through a user study with N = 49
experienced programmers, we show that while correctness captures high-value
generations, programmers still rate code that fails unit tests as valuable if
it reduces the overall effort needed to complete a coding task. Finally, we
propose a hybrid metric that combines functional correctness and syntactic
similarity and show that it achieves a 14% stronger correlation with value and
can therefore better represent real-world gains when evaluating and comparing
models.",2210.16494v2,https://arxiv.org/pdf/2210.16494v2
Subsidiary Prototype Alignment for Universal Domain Adaptation,"Jogendra Nath Kundu, Suvaansh Bhambri, Akshay Kulkarni, Hiran Sarkar, Varun Jampani, R. Venkatesh Babu","Universal Domain Adaptation (UniDA) deals with the problem of knowledge
transfer between two datasets with domain-shift as well as category-shift. The
goal is to categorize unlabeled target samples, either into one of the ""known""
categories or into a single ""unknown"" category. A major problem in UniDA is
negative transfer, i.e. misalignment of ""known"" and ""unknown"" classes. To this
end, we first uncover an intriguing tradeoff between negative-transfer-risk and
domain-invariance exhibited at different layers of a deep network. It turns out
we can strike a balance between these two metrics at a mid-level layer. Towards
designing an effective framework based on this insight, we draw motivation from
Bag-of-visual-Words (BoW). Word-prototypes in a BoW-like representation of a
mid-level layer would represent lower-level visual primitives that are likely
to be unaffected by the category-shift in the high-level features. We develop
modifications that encourage learning of word-prototypes followed by
word-histogram based classification. Following this, subsidiary prototype-space
alignment (SPA) can be seen as a closed-set alignment problem, thereby avoiding
negative transfer. We realize this with a novel word-histogram-related pretext
task to enable closed-set SPA, operating in conjunction with goal task UniDA.
We demonstrate the efficacy of our approach on top of existing UniDA
techniques, yielding state-of-the-art performance across three standard UniDA
and Open-Set DA object recognition benchmarks.",2210.15909v1,https://arxiv.org/pdf/2210.15909v1
"Trade-off between reconstruction loss and feature alignment for domain
  generalization","Thuan Nguyen, Boyang Lyu, Prakash Ishwar, Matthias Scheutz, Shuchin Aeron","Domain generalization (DG) is a branch of transfer learning that aims to
train the learning models on several seen domains and subsequently apply these
pre-trained models to other unseen (unknown but related) domains. To deal with
challenging settings in DG where both data and label of the unseen domain are
not available at training time, the most common approach is to design the
classifiers based on the domain-invariant representation features, i.e., the
latent representations that are unchanged and transferable between domains.
Contrary to popular belief, we show that designing classifiers based on
invariant representation features alone is necessary but insufficient in DG.
Our analysis indicates the necessity of imposing a constraint on the
reconstruction loss induced by representation functions to preserve most of the
relevant information about the label in the latent space. More importantly, we
point out the trade-off between minimizing the reconstruction loss and
achieving domain alignment in DG. Our theoretical results motivate a new DG
framework that jointly optimizes the reconstruction loss and the domain
discrepancy. Both theoretical and numerical results are provided to justify our
approach.",2210.15000v1,https://arxiv.org/pdf/2210.15000v1
"MM-Align: Learning Optimal Transport-based Alignment Dynamics for Fast
  and Accurate Inference on Missing Modality Sequences","Wei Han, Hui Chen, Min-Yen Kan, Soujanya Poria","Existing multimodal tasks mostly target at the complete input modality
setting, i.e., each modality is either complete or completely missing in both
training and test sets. However, the randomly missing situations have still
been underexplored. In this paper, we present a novel approach named MM-Align
to address the missing-modality inference problem. Concretely, we propose 1) an
alignment dynamics learning module based on the theory of optimal transport
(OT) for indirect missing data imputation; 2) a denoising training algorithm to
simultaneously enhance the imputation results and backbone network performance.
Compared with previous methods which devote to reconstructing the missing
inputs, MM-Align learns to capture and imitate the alignment dynamics between
modality sequences. Results of comprehensive experiments on three datasets
covering two multimodal tasks empirically demonstrate that our method can
perform more accurate and faster inference and relieve overfitting under
various missing conditions.",2210.12798v1,https://arxiv.org/pdf/2210.12798v1
Manifold Alignment with Label Information,"Andres F. Duque, Myriam Lizotte, Guy Wolf, Kevin R. Moon","Multi-domain data is becoming increasingly common and presents both
challenges and opportunities in the data science community. The integration of
distinct data-views can be used for exploratory data analysis, and benefit
downstream analysis including machine learning related tasks. With this in
mind, we present a novel manifold alignment method called MALI (Manifold
alignment with label information) that learns a correspondence between two
distinct domains. MALI can be considered as belonging to a middle ground
between the more commonly addressed semi-supervised manifold alignment problem
with some known correspondences between the two domains, and the purely
unsupervised case, where no known correspondences are provided. To do this,
MALI learns the manifold structure in both domains via a diffusion process and
then leverages discrete class labels to guide the alignment. By aligning two
distinct domains, MALI recovers a pairing and a common representation that
reveals related samples in both domains. Additionally, MALI can be used for the
transfer learning problem known as domain adaptation. We show that MALI
outperforms the current state-of-the-art manifold alignment methods across
multiple datasets.",2210.12774v2,https://arxiv.org/pdf/2210.12774v2
Fast Beam Alignment via Pure Exploration in Multi-armed Bandits,"Yi Wei, Zixin Zhong, Vincent Y. F. Tan","The beam alignment (BA) problem consists in accurately aligning the
transmitter and receiver beams to establish a reliable communication link in
wireless communication systems. Existing BA methods search the entire beam
space to identify the optimal transmit-receive beam pair. This incurs a
significant latency when the number of antennas is large. In this work, we
develop a bandit-based fast BA algorithm to reduce BA latency for
millimeter-wave (mmWave) communications. Our algorithm is named Two-Phase
Heteroscedastic Track-and-Stop (2PHT\&S). We first formulate the BA problem as
a pure exploration problem in multi-armed bandits in which the objective is to
minimize the required number of time steps given a certain fixed confidence
level. By taking advantage of the correlation structure among beams that the
information from nearby beams is similar and the heteroscedastic property that
the variance of the reward of an arm (beam) is related to its mean, the
proposed algorithm groups all beams into several beam sets such that the
optimal beam set is first selected and the optimal beam is identified in this
set after that. Theoretical analysis and simulation results on synthetic and
semi-practical channel data demonstrate the clear superiority of the proposed
algorithm vis-\`a-vis other baseline competitors.",2210.12625v1,https://arxiv.org/pdf/2210.12625v1
"LightEA: A Scalable, Robust, and Interpretable Entity Alignment
  Framework via Three-view Label Propagation","Xin Mao, Wenting Wang, Yuanbin Wu, Man Lan","Entity Alignment (EA) aims to find equivalent entity pairs between KGs, which
is the core step of bridging and integrating multi-source KGs. In this paper,
we argue that existing GNN-based EA methods inherit the inborn defects from
their neural network lineage: weak scalability and poor interpretability.
Inspired by recent studies, we reinvent the Label Propagation algorithm to
effectively run on KGs and propose a non-neural EA framework -- LightEA,
consisting of three efficient components: (i) Random Orthogonal Label
Generation, (ii) Three-view Label Propagation, and (iii) Sparse Sinkhorn
Iteration. According to the extensive experiments on public datasets, LightEA
has impressive scalability, robustness, and interpretability. With a mere tenth
of time consumption, LightEA achieves comparable results to state-of-the-art
methods across all datasets and even surpasses them on many.",2210.10436v2,https://arxiv.org/pdf/2210.10436v2
"Rethinking Prototypical Contrastive Learning through Alignment,
  Uniformity and Correlation","Shentong Mo, Zhun Sun, Chao Li","Contrastive self-supervised learning (CSL) with a prototypical regularization
has been introduced in learning meaningful representations for downstream tasks
that require strong semantic information. However, to optimize CSL with a loss
that performs the prototypical regularization aggressively, e.g., the ProtoNCE
loss, might cause the ""coagulation"" of examples in the embedding space. That
is, the intra-prototype diversity of samples collapses to trivial solutions for
their prototype being well-separated from others. Motivated by previous works,
we propose to mitigate this phenomenon by learning Prototypical representation
through Alignment, Uniformity and Correlation (PAUC). Specifically, the
ordinary ProtoNCE loss is revised with: (1) an alignment loss that pulls
embeddings from positive prototypes together; (2) a uniformity loss that
distributes the prototypical level features uniformly; (3) a correlation loss
that increases the diversity and discriminability between prototypical level
features. We conduct extensive experiments on various benchmarks where the
results demonstrate the effectiveness of our method in improving the quality of
prototypical contrastive representations. Particularly, in the classification
down-stream tasks with linear probes, our proposed method outperforms the
state-of-the-art instance-wise and prototypical contrastive learning methods on
the ImageNet-100 dataset by 2.96% and the ImageNet-1K dataset by 2.46% under
the same settings of batch size and epochs.",2210.10194v1,https://arxiv.org/pdf/2210.10194v1
Aligning MAGMA by Few-Shot Learning and Finetuning,"Jean-Charles Layoun, Alexis Roger, Irina Rish","The goal of vision-language modeling is to allow models to tie language
understanding with visual inputs. The aim of this paper is to evaluate and
align the Visual Language Model (VLM) called Multimodal Augmentation of
Generative Models through Adapter-based finetuning (MAGMA) with human values.
MAGMA is a VLM that is capable of image captioning and visual
question-answering. We will evaluate its alignment in three different
scenarios. To begin, we assess MAGMA's out-of-the-box alignment through the
checkpoint provided by Hugging Face. Then, we measure if few-shot learning
manages to improve the results. Finally, we finetune the model on aligned
examples and evaluate its behavior.",2210.14161v1,https://arxiv.org/pdf/2210.14161v1
MMGA: Multimodal Learning with Graph Alignment,"Xuan Yang, Quanjin Tao, Xiao Feng, Donghong Cai, Xiang Ren, Yang Yang","Multimodal pre-training breaks down the modality barriers and allows the
individual modalities to be mutually augmented with information, resulting in
significant advances in representation learning. However, graph modality, as a
very general and important form of data, cannot be easily interacted with other
modalities because of its non-regular nature. In this paper, we propose MMGA
(Multimodal learning with Graph Alignment), a novel multimodal pre-training
framework to incorporate information from graph (social network), image and
text modalities on social media to enhance user representation learning. In
MMGA, a multi-step graph alignment mechanism is proposed to add the
self-supervision from graph modality to optimize the image and text encoders,
while using the information from the image and text modalities to guide the
graph encoder learning. We conduct experiments on the dataset crawled from
Instagram. The experimental results show that MMGA works well on the dataset
and improves the fans prediction task's performance. We release our dataset,
the first social media multimodal dataset with graph, of 60,000 users labeled
with specific topics based on 2 million posts to facilitate future research.",2210.09946v2,https://arxiv.org/pdf/2210.09946v2
"TransAlign: Fully Automatic and Effective Entity Alignment for Knowledge
  Graphs","Rui Zhang, Xiaoyan Zhao, Bayu Distiawan Trisedya, Min Yang, Hong Cheng, Jianzhong Qi","The task of entity alignment between knowledge graphs (KGs) aims to identify
every pair of entities from two different KGs that represent the same entity.
Many machine learning-based methods have been proposed for this task. However,
to our best knowledge, existing methods all require manually crafted seed
alignments, which are expensive to obtain. In this paper, we propose the first
fully automatic alignment method named TransAlign, which does not require any
manually crafted seed alignments. Specifically, for predicate embeddings,
TransAlign constructs a predicate-proximity-graph to automatically capture the
similarity between predicates across two KGs by learning the attention of
entity types. For entity embeddings, TransAlign first computes the entity
embeddings of each KG independently using TransE, and then shifts the two KGs'
entity embeddings into the same vector space by computing the similarity
between entities based on their attributes. Thus, both predicate alignment and
entity alignment can be done without manually crafted seed alignments.
TransAlign is not only fully automatic, but also highly effective. Experiments
using real-world KGs show that TransAlign improves the accuracy of entity
alignment significantly compared to state-of-the-art methods.",2210.08540v1,https://arxiv.org/pdf/2210.08540v1
"Enabling Classifiers to Make Judgements Explicitly Aligned with Human
  Values","Yejin Bang, Tiezheng Yu, Andrea Madotto, Zhaojiang Lin, Mona Diab, Pascale Fung","Many NLP classification tasks, such as sexism/racism detection or toxicity
detection, are based on human values. Yet, human values can vary under diverse
cultural conditions. Therefore, we introduce a framework for value-aligned
classification that performs prediction based on explicitly written human
values in the command. Along with the task, we propose a practical approach
that distills value-aligned knowledge from large-scale language models (LLMs)
to construct value-aligned classifiers in two steps. First, we generate
value-aligned training data from LLMs by prompt-based few-shot learning. Next,
we fine-tune smaller classification models with the generated data for the
task. Empirical results show that our VA-Models surpass multiple baselines by
at least 15.56% on the F1-score, including few-shot learning with OPT-175B and
existing text augmentation methods. We suggest that using classifiers with
explicit human value input improves both inclusivity & explainability in AI.",2210.07652v1,https://arxiv.org/pdf/2210.07652v1
Language Model Decoding as Likelihood-Utility Alignment,"Martin Josifoski, Maxime Peyrard, Frano Rajic, Jiheng Wei, Debjit Paul, Valentin Hartmann, Barun Patra, Vishrav Chaudhary, Emre Kıcıman, Boi Faltings, Robert West","A critical component of a successful language generation pipeline is the
decoding algorithm. However, the general principles that should guide the
choice of a decoding algorithm remain unclear. Previous works only compare
decoding algorithms in narrow scenarios, and their findings do not generalize
across tasks. We argue that the misalignment between the model's likelihood and
the task-specific notion of utility is the key factor to understanding the
effectiveness of decoding algorithms. To structure the discussion, we introduce
a taxonomy of misalignment mitigation strategies (MMSs), providing a unifying
view of decoding as a tool for alignment. The MMS taxonomy groups decoding
algorithms based on their implicit assumptions about likelihood--utility
misalignment, yielding general statements about their applicability across
tasks. Specifically, by analyzing the correlation between the likelihood and
the utility of predictions across a diverse set of tasks, we provide empirical
evidence supporting the proposed taxonomy and a set of principles to structure
reasoning when choosing a decoding algorithm. Crucially, our analysis is the
first to relate likelihood-based decoding algorithms with algorithms that rely
on external information, such as value-guided methods and prompting, and covers
the most diverse set of tasks to date. Code, data, and models are available at
https://github.com/epfl-dlab/understanding-decoding.",2210.07228v2,https://arxiv.org/pdf/2210.07228v2
Token-Label Alignment for Vision Transformers,"Han Xiao, Wenzhao Zheng, Zheng Zhu, Jie Zhou, Jiwen Lu","Data mixing strategies (e.g., CutMix) have shown the ability to greatly
improve the performance of convolutional neural networks (CNNs). They mix two
images as inputs for training and assign them with a mixed label with the same
ratio. While they are shown effective for vision transformers (ViTs), we
identify a token fluctuation phenomenon that has suppressed the potential of
data mixing strategies. We empirically observe that the contributions of input
tokens fluctuate as forward propagating, which might induce a different mixing
ratio in the output tokens. The training target computed by the original data
mixing strategy can thus be inaccurate, resulting in less effective training.
To address this, we propose a token-label alignment (TL-Align) method to trace
the correspondence between transformed tokens and the original tokens to
maintain a label for each token. We reuse the computed attention at each layer
for efficient token-label alignment, introducing only negligible additional
training costs. Extensive experiments demonstrate that our method improves the
performance of ViTs on image classification, semantic segmentation, objective
detection, and transfer learning tasks. Code is available at:
https://github.com/Euphoria16/TL-Align.",2210.06455v2,https://arxiv.org/pdf/2210.06455v2
"Self-supervised video pretraining yields human-aligned visual
  representations","Nikhil Parthasarathy, S. M. Ali Eslami, João Carreira, Olivier J. Hénaff","Humans learn powerful representations of objects and scenes by observing how
they evolve over time. Yet, outside of specific tasks that require explicit
temporal understanding, static image pretraining remains the dominant paradigm
for learning visual foundation models. We question this mismatch, and ask
whether video pretraining can yield visual representations that bear the
hallmarks of human perception: generalisation across tasks, robustness to
perturbations, and consistency with human judgements. To that end we propose a
novel procedure for curating videos, and develop a contrastive framework which
learns from the complex transformations therein. This simple paradigm for
distilling knowledge from videos, called VITO, yields general representations
that far outperform prior video pretraining methods on image understanding
tasks, and image pretraining methods on video understanding tasks. Moreover,
VITO representations are significantly more robust to natural and synthetic
deformations than image-, video-, and adversarially-trained ones. Finally,
VITO's predictions are strongly aligned with human judgements, surpassing
models that were specifically trained for that purpose. Together, these results
suggest that video pretraining could be a simple way of learning unified,
robust, and human-aligned representations of the visual world.",2210.06433v2,https://arxiv.org/pdf/2210.06433v2
"Multi-Granularity Cross-modal Alignment for Generalized Medical Visual
  Representation Learning","Fuying Wang, Yuyin Zhou, Shujun Wang, Varut Vardhanabhuti, Lequan Yu","Learning medical visual representations directly from paired radiology
reports has become an emerging topic in representation learning. However,
existing medical image-text joint learning methods are limited by instance or
local supervision analysis, ignoring disease-level semantic correspondences. In
this paper, we present a novel Multi-Granularity Cross-modal Alignment (MGCA)
framework for generalized medical visual representation learning by harnessing
the naturally exhibited semantic correspondences between medical image and
radiology reports at three different levels, i.e., pathological region-level,
instance-level, and disease-level. Specifically, we first incorporate the
instance-wise alignment module by maximizing the agreement between image-report
pairs. Further, for token-wise alignment, we introduce a bidirectional
cross-attention strategy to explicitly learn the matching between fine-grained
visual tokens and text tokens, followed by contrastive learning to align them.
More important, to leverage the high-level inter-subject relationship semantic
(e.g., disease) correspondences, we design a novel cross-modal disease-level
alignment paradigm to enforce the cross-modal cluster assignment consistency.
Extensive experimental results on seven downstream medical image datasets
covering image classification, object detection, and semantic segmentation
tasks demonstrate the stable and superior performance of our framework.",2210.06044v1,https://arxiv.org/pdf/2210.06044v1
fAux: Testing Individual Fairness via Gradient Alignment,"Giuseppe Castiglione, Ga Wu, Christopher Srinivasa, Simon Prince","Machine learning models are vulnerable to biases that result in unfair
treatment of individuals from different populations. Recent work that aims to
test a model's fairness at the individual level either relies on domain
knowledge to choose metrics, or on input transformations that risk generating
out-of-domain samples. We describe a new approach for testing individual
fairness that does not have either requirement. We propose a novel criterion
for evaluating individual fairness and develop a practical testing method based
on this criterion which we call fAux (pronounced fox). This is based on
comparing the derivatives of the predictions of the model to be tested with
those of an auxiliary model, which predicts the protected variable from the
observed data. We show that the proposed method effectively identifies
discrimination on both synthetic and real-world datasets, and has quantitative
and qualitative advantages over contemporary methods.",2210.06288v1,https://arxiv.org/pdf/2210.06288v1
ELIGN: Expectation Alignment as a Multi-Agent Intrinsic Reward,"Zixian Ma, Rose Wang, Li Fei-Fei, Michael Bernstein, Ranjay Krishna","Modern multi-agent reinforcement learning frameworks rely on centralized
training and reward shaping to perform well. However, centralized training and
dense rewards are not readily available in the real world. Current multi-agent
algorithms struggle to learn in the alternative setup of decentralized training
or sparse rewards. To address these issues, we propose a self-supervised
intrinsic reward ELIGN - expectation alignment - inspired by the
self-organization principle in Zoology. Similar to how animals collaborate in a
decentralized manner with those in their vicinity, agents trained with
expectation alignment learn behaviors that match their neighbors' expectations.
This allows the agents to learn collaborative behaviors without any external
reward or centralized training. We demonstrate the efficacy of our approach
across 6 tasks in the multi-agent particle and the complex Google Research
football environments, comparing ELIGN to sparse and curiosity-based intrinsic
rewards. When the number of agents increases, ELIGN scales well in all
multi-agent tasks except for one where agents have different capabilities. We
show that agent coordination improves through expectation alignment because
agents learn to divide tasks amongst themselves, break coordination symmetries,
and confuse adversaries. These results identify tasks where expectation
alignment is a more useful strategy than curiosity-driven exploration for
multi-agent coordination, enabling agents to do zero-shot coordination.",2210.04365v2,https://arxiv.org/pdf/2210.04365v2
"VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature
  Alignment","Shraman Pramanick, Li Jing, Sayan Nag, Jiachen Zhu, Hardik Shah, Yann LeCun, Rama Chellappa","Vision-language pre-training (VLP) has recently proven highly effective for
various uni- and multi-modal downstream applications. However, most existing
end-to-end VLP methods use high-resolution image-text box data to perform well
on fine-grained region-level tasks, such as object detection, segmentation, and
referring expression comprehension. Unfortunately, such high-resolution images
with accurate bounding box annotations are expensive to collect and use for
supervision at scale. In this work, we propose VoLTA (Vision-Language
Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm
that only utilizes image-caption data but achieves fine-grained region-level
image understanding, eliminating the use of expensive box annotations. VoLTA
adopts graph optimal transport-based weakly-supervised alignment on local image
patches and text tokens to germinate an explicit, self-normalized, and
interpretable low-level matching criterion. In addition, VoLTA pushes
multi-modal fusion deep into the uni-modal backbones during pre-training and
removes fusion-specific transformer layers, further reducing memory
requirements. Extensive experiments on a wide range of vision- and
vision-language downstream tasks demonstrate the effectiveness of VoLTA on
fine-grained applications without compromising the coarse-grained downstream
performance, often outperforming methods using significantly more caption and
box annotations.",2210.04135v3,https://arxiv.org/pdf/2210.04135v3
"APE: Aligning Pretrained Encoders to Quickly Learn Aligned Multimodal
  Representations","Elan Rosenfeld, Preetum Nakkiran, Hadi Pouransari, Oncel Tuzel, Fartash Faghri","Recent advances in learning aligned multimodal representations have been
primarily driven by training large neural networks on massive, noisy
paired-modality datasets. In this work, we ask whether it is possible to
achieve similar results with substantially less training time and data. We
achieve this by taking advantage of existing pretrained unimodal encoders and
careful curation of alignment data relevant to the downstream task of interest.
We study a natural approach to aligning existing encoders via small auxiliary
functions, and we find that this method is competitive with (or outperforms)
state of the art in many settings while being less prone to overfitting, less
costly to train, and more robust to distribution shift. With a properly chosen
alignment distribution, our method surpasses prior state of the art for
ImageNet zero-shot classification on public data while using two orders of
magnitude less time and data and training 77% fewer parameters.",2210.03927v1,https://arxiv.org/pdf/2210.03927v1
"Temporal Feature Alignment in Contrastive Self-Supervised Learning for
  Human Activity Recognition","Bulat Khaertdinov, Stylianos Asteriadis","Automated Human Activity Recognition has long been a problem of great
interest in human-centered and ubiquitous computing. In the last years, a
plethora of supervised learning algorithms based on deep neural networks has
been suggested to address this problem using various modalities. While every
modality has its own limitations, there is one common challenge. Namely,
supervised learning requires vast amounts of annotated data which is
practically hard to collect. In this paper, we benefit from the self-supervised
learning paradigm (SSL) that is typically used to learn deep feature
representations from unlabeled data. Moreover, we upgrade a contrastive SSL
framework, namely SimCLR, widely used in various applications by introducing a
temporal feature alignment procedure for Human Activity Recognition.
Specifically, we propose integrating a dynamic time warping (DTW) algorithm in
a latent space to force features to be aligned in a temporal dimension.
Extensive experiments have been conducted for the unimodal scenario with
inertial modality as well as in multimodal settings using inertial and skeleton
data. According to the obtained results, the proposed approach has a great
potential in learning robust feature representations compared to the recent SSL
baselines, and clearly outperforms supervised models in semi-supervised
learning. The code for the unimodal case is available via the following link:
https://github.com/bulatkh/csshar_tfa.",2210.03382v1,https://arxiv.org/pdf/2210.03382v1
"Equalizing Credit Opportunity in Algorithms: Aligning Algorithmic
  Fairness Research with U.S. Fair Lending Regulation","I. Elizabeth Kumar, Keegan E. Hines, John P. Dickerson","Credit is an essential component of financial wellbeing in America, and
unequal access to it is a large factor in the economic disparities between
demographic groups that exist today. Today, machine learning algorithms,
sometimes trained on alternative data, are increasingly being used to determine
access to credit, yet research has shown that machine learning can encode many
different versions of ""unfairness,"" thus raising the concern that banks and
other financial institutions could -- potentially unwittingly -- engage in
illegal discrimination through the use of this technology. In the US, there are
laws in place to make sure discrimination does not happen in lending and
agencies charged with enforcing them. However, conversations around fair credit
models in computer science and in policy are often misaligned: fair machine
learning research often lacks legal and practical considerations specific to
existing fair lending policy, and regulators have yet to issue new guidance on
how, if at all, credit risk models should be utilizing practices and techniques
from the research community. This paper aims to better align these sides of the
conversation. We describe the current state of credit discrimination regulation
in the United States, contextualize results from fair ML research to identify
the specific fairness concerns raised by the use of machine learning in
lending, and discuss regulatory opportunities to address these concerns.",2210.02516v1,https://arxiv.org/pdf/2210.02516v1
Improving alignment of dialogue agents via targeted human judgements,"Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, Geoffrey Irving","We present Sparrow, an information-seeking dialogue agent trained to be more
helpful, correct, and harmless compared to prompted language model baselines.
We use reinforcement learning from human feedback to train our models with two
new additions to help human raters judge agent behaviour. First, to make our
agent more helpful and harmless, we break down the requirements for good
dialogue into natural language rules the agent should follow, and ask raters
about each rule separately. We demonstrate that this breakdown enables us to
collect more targeted human judgements of agent behaviour and allows for more
efficient rule-conditional reward models. Second, our agent provides evidence
from sources supporting factual claims when collecting preference judgements
over model statements. For factual questions, evidence provided by Sparrow
supports the sampled response 78% of the time. Sparrow is preferred more often
than baselines while being more resilient to adversarial probing by humans,
violating our rules only 8% of the time when probed. Finally, we conduct
extensive analyses showing that though our model learns to follow our rules it
can exhibit distributional biases.",2209.14375v1,https://arxiv.org/pdf/2209.14375v1
"Visual representations in the human brain are aligned with large
  language models","Adrien Doerig, Tim C Kietzmann, Emily Allen, Yihan Wu, Thomas Naselaris, Kendrick Kay, Ian Charest","The human brain extracts complex information from visual inputs, including
objects, their spatial and semantic interrelations, and their interactions with
the environment. However, a quantitative approach for studying this information
remains elusive. Here, we test whether the contextual information encoded in
large language models (LLMs) is beneficial for modelling the complex visual
information extracted by the brain from natural scenes. We show that LLM
embeddings of scene captions successfully characterise brain activity evoked by
viewing the natural scenes. This mapping captures selectivities of different
brain areas, and is sufficiently robust that accurate scene captions can be
reconstructed from brain activity. Using carefully controlled model
comparisons, we then proceed to show that the accuracy with which LLM
representations match brain representations derives from the ability of LLMs to
integrate complex information contained in scene captions beyond that conveyed
by individual words. Finally, we train deep neural network models to transform
image inputs into LLM representations. Remarkably, these networks learn
representations that are better aligned with brain representations than a large
number of state-of-the-art alternative models, despite being trained on
orders-of-magnitude less data. Overall, our results suggest that LLM embeddings
of scene captions provide a representational format that accounts for complex
information extracted by the brain from visual inputs.",2209.11737v2,https://arxiv.org/pdf/2209.11737v2
Multi-Modal Cross-Domain Alignment Network for Video Moment Retrieval,"Xiang Fang, Daizong Liu, Pan Zhou, Yuchong Hu","As an increasingly popular task in multimedia information retrieval, video
moment retrieval (VMR) aims to localize the target moment from an untrimmed
video according to a given language query. Most previous methods depend heavily
on numerous manual annotations (i.e., moment boundaries), which are extremely
expensive to acquire in practice. In addition, due to the domain gap between
different datasets, directly applying these pre-trained models to an unseen
domain leads to a significant performance drop. In this paper, we focus on a
novel task: cross-domain VMR, where fully-annotated datasets are available in
one domain (``source domain''), but the domain of interest (``target domain'')
only contains unannotated datasets. As far as we know, we present the first
study on cross-domain VMR. To address this new task, we propose a novel
Multi-Modal Cross-Domain Alignment (MMCDA) network to transfer the annotation
knowledge from the source domain to the target domain. However, due to the
domain discrepancy between the source and target domains and the semantic gap
between videos and queries, directly applying trained models to the target
domain generally leads to a performance drop. To solve this problem, we develop
three novel modules: (i) a domain alignment module is designed to align the
feature distributions between different domains of each modality; (ii) a
cross-modal alignment module aims to map both video and query features into a
joint embedding space and to align the feature distributions between different
modalities in the target domain; (iii) a specific alignment module tries to
obtain the fine-grained similarity between a specific frame and the given query
for optimal localization. By jointly training these three modules, our MMCDA
can learn domain-invariant and semantic-aligned cross-modal representations.",2209.11572v2,https://arxiv.org/pdf/2209.11572v2
Equivariant Transduction through Invariant Alignment,"Jennifer C. White, Ryan Cotterell","The ability to generalize compositionally is key to understanding the
potentially infinite number of sentences that can be constructed in a human
language from only a finite number of words. Investigating whether NLP models
possess this ability has been a topic of interest: SCAN (Lake and Baroni, 2018)
is one task specifically proposed to test for this property. Previous work has
achieved impressive empirical results using a group-equivariant neural network
that naturally encodes a useful inductive bias for SCAN (Gordon et al., 2020).
Inspired by this, we introduce a novel group-equivariant architecture that
incorporates a group-invariant hard alignment mechanism. We find that our
network's structure allows it to develop stronger equivariance properties than
existing group-equivariant approaches. We additionally find that it outperforms
previous group-equivariant networks empirically on the SCAN task. Our results
suggest that integrating group-equivariance into a variety of neural
architectures is a potentially fruitful avenue of research, and demonstrate the
value of careful analysis of the theoretical properties of such architectures.",2209.10926v1,https://arxiv.org/pdf/2209.10926v1
"Dynamic Time-Alignment of Dimensional Annotations of Emotion using
  Recurrent Neural Networks","Sina Alisamir, Fabien Ringeval, Francois Portet","Most automatic emotion recognition systems exploit time-continuous
annotations of emotion to provide fine-grained descriptions of spontaneous
expressions as observed in real-life interactions. As emotion is rather
subjective, its annotation is usually performed by several annotators who
provide a trace for a given dimension, i.e. a time-continuous series describing
a dimension such as arousal or valence. However, annotations of the same
expression are rarely consistent between annotators, either in time or in
value, which adds bias and delay in the trace that is used to learn predictive
models of emotion. We therefore propose a method that can dynamically
compensate inconsistencies across annotations and synchronise the traces with
the corresponding acoustic features using Recurrent Neural Networks.
Experimental evaluations were carried on several emotion data sets that include
Chinese, French, German, and Hungarian participants who interacted remotely in
either noise-free conditions or in-the-wild. The results show that our method
can significantly increase inter-annotator agreement, as well as correlation
between traces and audio features, for both arousal and valence. In addition,
improvements are obtained in the automatic prediction of these dimensions using
simple light-weight models, especially for valence in noise-free conditions,
and arousal for recordings captured in-the-wild.",2209.10223v1,https://arxiv.org/pdf/2209.10223v1
"A Simple Temporal Information Matching Mechanism for Entity Alignment
  Between Temporal Knowledge Graphs","Li Cai, Xin Mao, Meirong Ma, Hao Yuan, Jianchao Zhu, Man Lan","Entity alignment (EA) aims to find entities in different knowledge graphs
(KGs) that refer to the same object in the real world. Recent studies
incorporate temporal information to augment the representations of KGs. The
existing methods for EA between temporal KGs (TKGs) utilize a time-aware
attention mechanism to incorporate relational and temporal information into
entity embeddings. The approaches outperform the previous methods by using
temporal information. However, we believe that it is not necessary to learn the
embeddings of temporal information in KGs since most TKGs have uniform temporal
representations. Therefore, we propose a simple graph neural network (GNN)
model combined with a temporal information matching mechanism, which achieves
better performance with less time and fewer parameters. Furthermore, since
alignment seeds are difficult to label in real-world applications, we also
propose a method to generate unsupervised alignment seeds via the temporal
information of TKG. Extensive experiments on public datasets indicate that our
supervised method significantly outperforms the previous methods and the
unsupervised one has competitive performance.",2209.09677v1,https://arxiv.org/pdf/2209.09677v1
"Law Informs Code: A Legal Informatics Approach to Aligning Artificial
  Intelligence with Humans",John J. Nay,"We are currently unable to specify human goals and societal values in a way
that reliably directs AI behavior. Law-making and legal interpretation form a
computational engine that converts opaque human values into legible directives.
""Law Informs Code"" is the research agenda embedding legal knowledge and
reasoning in AI. Similar to how parties to a legal contract cannot foresee
every potential contingency of their future relationship, and legislators
cannot predict all the circumstances under which their proposed bills will be
applied, we cannot ex ante specify rules that provably direct good AI behavior.
Legal theory and practice have developed arrays of tools to address these
specification problems. For instance, legal standards allow humans to develop
shared understandings and adapt them to novel situations. In contrast to more
prosaic uses of the law (e.g., as a deterrent of bad behavior through the
threat of sanction), leveraged as an expression of how humans communicate their
goals, and what society values, Law Informs Code.
  We describe how data generated by legal processes (methods of law-making,
statutory interpretation, contract drafting, applications of legal standards,
legal reasoning, etc.) can facilitate the robust specification of inherently
vague human goals. This increases human-AI alignment and the local usefulness
of AI. Toward society-AI alignment, we present a framework for understanding
law as the applied philosophy of multi-agent alignment. Although law is partly
a reflection of historically contingent political power - and thus not a
perfect aggregation of citizen preferences - if properly parsed, its
distillation offers the most legitimate computational comprehension of societal
values available. If law eventually informs powerful AI, engaging in the
deliberative political process to improve law takes on even more meaning.",2209.13020v14,https://arxiv.org/pdf/2209.13020v14
Self-supervised Human Mesh Recovery with Cross-Representation Alignment,"Xuan Gong, Meng Zheng, Benjamin Planche, Srikrishna Karanam, Terrence Chen, David Doermann, Ziyan Wu","Fully supervised human mesh recovery methods are data-hungry and have poor
generalizability due to the limited availability and diversity of 3D-annotated
benchmark datasets. Recent progress in self-supervised human mesh recovery has
been made using synthetic-data-driven training paradigms where the model is
trained from synthetic paired 2D representation (e.g., 2D keypoints and
segmentation masks) and 3D mesh. However, on synthetic dense correspondence
maps (i.e., IUV) few have been explored since the domain gap between synthetic
training data and real testing data is hard to address for 2D dense
representation. To alleviate this domain gap on IUV, we propose
cross-representation alignment utilizing the complementary information from the
robust but sparse representation (2D keypoints). Specifically, the alignment
errors between initial mesh estimation and both 2D representations are
forwarded into regressor and dynamically corrected in the following mesh
regression. This adaptive cross-representation alignment explicitly learns from
the deviations and captures complementary information: robustness from sparse
representation and richness from dense representation. We conduct extensive
experiments on multiple standard benchmark datasets and demonstrate competitive
results, helping take a step towards reducing the annotation effort needed to
produce state-of-the-art models in human mesh estimation.",2209.04596v1,https://arxiv.org/pdf/2209.04596v1
Alignment-based conformance checking over probabilistic events,"Jiawei Zheng, Petros Papapanagiotou, Jacques D. Fleuriot","Conformance checking techniques allow us to evaluate how well some exhibited
behaviour, represented by a trace of monitored events, conforms to a specified
process model. Modern monitoring and activity recognition technologies, such as
those relying on sensors, the IoT, statistics and AI, can produce a wealth of
relevant event data. However, this data is typically characterised by noise and
uncertainty, in contrast to the assumption of a deterministic event log
required by conformance checking algorithms. In this paper, we extend
alignment-based conformance checking to function under a probabilistic event
log. We introduce a weighted trace model and weighted alignment cost function,
and a custom threshold parameter that controls the level of confidence on the
event data vs. the process model. The resulting algorithm considers activities
of lower but sufficiently high probability that better align with the process
model. We explain the algorithm and its motivation both from formal and
intuitive perspectives, and demonstrate its functionality in comparison with
deterministic alignment using real-life datasets.",2209.04309v2,https://arxiv.org/pdf/2209.04309v2
"Joint Alignment of Multi-Task Feature and Label Spaces for Emotion Cause
  Pair Extraction","Shunjie Chen, Xiaochuan Shi, Jingye Li, Shengqiong Wu, Hao Fei, Fei Li, Donghong Ji","Emotion cause pair extraction (ECPE), as one of the derived subtasks of
emotion cause analysis (ECA), shares rich inter-related features with emotion
extraction (EE) and cause extraction (CE). Therefore EE and CE are frequently
utilized as auxiliary tasks for better feature learning, modeled via multi-task
learning (MTL) framework by prior works to achieve state-of-the-art (SoTA) ECPE
results. However, existing MTL-based methods either fail to simultaneously
model the specific features and the interactive feature in between, or suffer
from the inconsistency of label prediction. In this work, we consider
addressing the above challenges for improving ECPE by performing two alignment
mechanisms with a novel A^2Net model. We first propose a feature-task alignment
to explicitly model the specific emotion-&cause-specific features and the
shared interactive feature. Besides, an inter-task alignment is implemented, in
which the label distance between the ECPE and the combinations of EE&CE are
learned to be narrowed for better label consistency. Evaluations of benchmarks
show that our methods outperform current best-performing systems on all ECA
subtasks. Further analysis proves the importance of our proposed alignment
mechanisms for the task.",2209.04112v1,https://arxiv.org/pdf/2209.04112v1
"PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and
  Feature-metric Alignment","Prajwal Chidananda, Saurabh Nair, Douglas Lee, Adrian Kaehler","We present PixTrack, a vision based object pose tracking framework using
novel view synthesis and deep feature-metric alignment. We follow an SfM-based
relocalization paradigm where we use a Neural Radiance Field to canonically
represent the tracked object. Our evaluations demonstrate that our method
produces highly accurate, robust, and jitter-free 6DoF pose estimates of
objects in both monocular RGB images and RGB-D images without the need of any
data annotation or trajectory smoothing. Our method is also computationally
efficient making it easy to have multi-object tracking with no alteration to
our algorithm through simple CPU multiprocessing. Our code is available at:
https://github.com/GiantAI/pixtrack",2209.03910v2,https://arxiv.org/pdf/2209.03910v2
"A Data-dependent Approach for High Dimensional (Robust) Wasserstein
  Alignment","Hu Ding, Wenjie Liu, Mingquan Ye","Many real-world problems can be formulated as the alignment between two
geometric patterns. Previously, a great amount of research focus on the
alignment of 2D or 3D patterns in the field of computer vision. Recently, the
alignment problem in high dimensions finds several novel applications in
practice. However, the research is still rather limited in the algorithmic
aspect. To the best of our knowledge, most existing approaches are just simple
extensions of their counterparts for 2D and 3D cases, and often suffer from the
issues such as high computational complexities. In this paper, we propose an
effective framework to compress the high dimensional geometric patterns. Any
existing alignment method can be applied to the compressed geometric patterns
and the time complexity can be significantly reduced. Our idea is inspired by
the observation that high dimensional data often has a low intrinsic dimension.
Our framework is a ``data-dependent'' approach that has the complexity
depending on the intrinsic dimension of the input data. Our experimental
results reveal that running the alignment algorithm on compressed patterns can
achieve similar qualities, comparing with the results on the original patterns,
but the runtimes (including the times cost for compression) are substantially
lower.",2209.02905v2,https://arxiv.org/pdf/2209.02905v2
"Conflict-Aware Pseudo Labeling via Optimal Transport for Entity
  Alignment","Qijie Ding, Daokun Zhang, Jie Yin","Entity alignment aims to discover unique equivalent entity pairs with the
same meaning across different knowledge graphs (KGs). Existing models have
focused on projecting KGs into a latent embedding space so that inherent
semantics between entities can be captured for entity alignment. However, the
adverse impacts of alignment conflicts have been largely overlooked during
training, thereby limiting the entity alignment performance. To address this
issue, we propose a novel Conflict-aware Pseudo Labeling via Optimal Transport
model (CPL-OT) for entity alignment. The key idea is to iteratively
pseudo-label alignment pairs empowered with conflict-aware optimal transport
(OT) modeling to boost the precision of entity alignment. CPL-OT is composed of
two key components -- entity embedding learning with global-local aggregation
and iterative conflict-aware pseudo labeling -- that mutually reinforce each
other. To mitigate alignment conflicts during pseudo labeling, we propose to
use optimal transport as an effective means to warrant one-to-one entity
alignment between two KGs with the minimal overall transport cost. Extensive
experiments on benchmark datasets validate the superiority of CPL-OT over
state-of-the-art baselines under both settings with and without prior alignment
seeds.",2209.01847v2,https://arxiv.org/pdf/2209.01847v2
Multi-modal Contrastive Representation Learning for Entity Alignment,"Zhenxi Lin, Ziheng Zhang, Meng Wang, Yinghui Shi, Xian Wu, Yefeng Zheng","Multi-modal entity alignment aims to identify equivalent entities between two
different multi-modal knowledge graphs, which consist of structural triples and
images associated with entities. Most previous works focus on how to utilize
and encode information from different modalities, while it is not trivial to
leverage multi-modal knowledge in entity alignment because of the modality
heterogeneity. In this paper, we propose MCLEA, a Multi-modal Contrastive
Learning based Entity Alignment model, to obtain effective joint
representations for multi-modal entity alignment. Different from previous
works, MCLEA considers task-oriented modality and models the inter-modal
relationships for each entity representation. In particular, MCLEA firstly
learns multiple individual representations from multiple modalities, and then
performs contrastive learning to jointly model intra-modal and inter-modal
interactions. Extensive experimental results show that MCLEA outperforms
state-of-the-art baselines on public datasets under both supervised and
unsupervised settings.",2209.00891v1,https://arxiv.org/pdf/2209.00891v1
"Feature Alignment by Uncertainty and Self-Training for Source-Free
  Unsupervised Domain Adaptation","JoonHo Lee, Gyemin Lee","Most unsupervised domain adaptation (UDA) methods assume that labeled source
images are available during model adaptation. However, this assumption is often
infeasible owing to confidentiality issues or memory constraints on mobile
devices. Some recently developed approaches do not require source images during
adaptation, but they show limited performance on perturbed images. To address
these problems, we propose a novel source-free UDA method that uses only a
pre-trained source model and unlabeled target images. Our method captures the
aleatoric uncertainty by incorporating data augmentation and trains the feature
generator with two consistency objectives. The feature generator is encouraged
to learn consistent visual features away from the decision boundaries of the
head classifier. Thus, the adapted model becomes more robust to image
perturbations. Inspired by self-supervised learning, our method promotes
inter-space alignment between the prediction space and the feature space while
incorporating intra-space consistency within the feature space to reduce the
domain gap between the source and target domains. We also consider epistemic
uncertainty to boost the model adaptation performance. Extensive experiments on
popular UDA benchmark datasets demonstrate that the proposed source-free method
is comparable or even superior to vanilla UDA methods. Moreover, the adapted
models show more robust results when input images are perturbed.",2208.14888v2,https://arxiv.org/pdf/2208.14888v2
"Competition, Alignment, and Equilibria in Digital Marketplaces","Meena Jagadeesan, Michael I. Jordan, Nika Haghtalab","Competition between traditional platforms is known to improve user utility by
aligning the platform's actions with user preferences. But to what extent is
alignment exhibited in data-driven marketplaces? To study this question from a
theoretical perspective, we introduce a duopoly market where platform actions
are bandit algorithms and the two platforms compete for user participation. A
salient feature of this market is that the quality of recommendations depends
on both the bandit algorithm and the amount of data provided by interactions
from users. This interdependency between the algorithm performance and the
actions of users complicates the structure of market equilibria and their
quality in terms of user utility. Our main finding is that competition in this
market does not perfectly align market outcomes with user utility.
Interestingly, market outcomes exhibit misalignment not only when the platforms
have separate data repositories, but also when the platforms have a shared data
repository. Nonetheless, the data sharing assumptions impact what mechanism
drives misalignment and also affect the specific form of misalignment (e.g. the
quality of the best-case and worst-case market outcomes). More broadly, our
work illustrates that competition in digital marketplaces has subtle
consequences for user utility that merit further investigation.",2208.14423v2,https://arxiv.org/pdf/2208.14423v2
The Alignment Problem from a Deep Learning Perspective,"Richard Ngo, Lawrence Chan, Sören Mindermann","In coming years or decades, artificial general intelligence (AGI) may surpass
human capabilities at many critical tasks. We argue that, without substantial
effort to prevent it, AGIs could learn to pursue goals that are in conflict
(i.e. misaligned) with human interests. If trained like today's most capable
models, AGIs could learn to act deceptively to receive higher reward, learn
misaligned internally-represented goals which generalize beyond their
fine-tuning distributions, and pursue those goals using power-seeking
strategies. We review emerging evidence for these properties. AGIs with these
properties would be difficult to align and may appear aligned even when they
are not. Finally, we briefly outline how the deployment of misaligned AGIs
might irreversibly undermine human control over the world, and we review
research directions aimed at preventing this outcome.",2209.00626v6,https://arxiv.org/pdf/2209.00626v6
"Efficient Vision-Language Pretraining with Visual Concepts and
  Hierarchical Alignment","Mustafa Shukor, Guillaume Couairon, Matthieu Cord","Vision and Language Pretraining has become the prevalent approach for
tackling multimodal downstream tasks. The current trend is to move towards ever
larger models and pretraining datasets. This computational headlong rush does
not seem reasonable in the long term to move toward sustainable solutions, and
de facto excludes academic laboratories with limited resources. In this work,
we propose a new framework, dubbed ViCHA, that efficiently exploits the input
data to boost the learning by: (a) a new hierarchical cross-modal alignment
loss, (b) new self-supervised scheme based on masked image modeling, (c)
leveraging image-level annotations, called Visual Concepts, obtained with
existing foundation models such as CLIP to boost the performance of the image
encoder. Although pretrained on four times less data, our ViCHA strategy
outperforms other approaches on several downstream tasks such as Image-Text
Retrieval, VQA, Visual Reasoning, Visual Entailment and Visual Grounding. The
code will be made publicly available here: https://github.com/mshukor/ViCHA",2208.13628v2,https://arxiv.org/pdf/2208.13628v2
"Deformation equivariant cross-modality image synthesis with paired
  non-aligned training data","Joel Honkamaa, Umair Khan, Sonja Koivukoski, Mira Valkonen, Leena Latonen, Pekka Ruusuvuori, Pekka Marttinen","Cross-modality image synthesis is an active research topic with multiple
medical clinically relevant applications. Recently, methods allowing training
with paired but misaligned data have started to emerge. However, no robust and
well-performing methods applicable to a wide range of real world data sets
exist. In this work, we propose a generic solution to the problem of
cross-modality image synthesis with paired but non-aligned data by introducing
new deformation equivariance encouraging loss functions. The method consists of
joint training of an image synthesis network together with separate
registration networks and allows adversarial training conditioned on the input
even with misaligned data. The work lowers the bar for new clinical
applications by allowing effortless training of cross-modality image synthesis
networks for more difficult data sets.",2208.12491v2,https://arxiv.org/pdf/2208.12491v2
"Towards Higher-order Topological Consistency for Unsupervised Network
  Alignment","Qingqiang Sun, Xuemin Lin, Ying Zhang, Wenjie Zhang, Chaoqi Chen","Network alignment task, which aims to identify corresponding nodes in
different networks, is of great significance for many subsequent applications.
Without the need for labeled anchor links, unsupervised alignment methods have
been attracting more and more attention. However, the topological consistency
assumptions defined by existing methods are generally low-order and less
accurate because only the edge-indiscriminative topological pattern is
considered, which is especially risky in an unsupervised setting. To reposition
the focus of the alignment process from low-order to higher-order topological
consistency, in this paper, we propose a fully unsupervised network alignment
framework named HTC. The proposed higher-order topological consistency is
formulated based on edge orbits, which is merged into the information
aggregation process of a graph convolutional network so that the alignment
consistencies are transformed into the similarity of node embeddings.
Furthermore, the encoder is trained to be multi-orbit-aware and then be refined
to identify more trusted anchor links. Node correspondence is comprehensively
evaluated by integrating all different orders of consistency. {In addition to
sound theoretical analysis, the superiority of the proposed method is also
empirically demonstrated through extensive experimental evaluation. On three
pairs of real-world datasets and two pairs of synthetic datasets, our HTC
consistently outperforms a wide variety of unsupervised and supervised methods
with the least or comparable time consumption. It also exhibits robustness to
structural noise as a result of our multi-orbit-aware training mechanism.",2208.12463v1,https://arxiv.org/pdf/2208.12463v1
Modelling the Recommender Alignment Problem,Francisco Carvalho,"Recommender systems (RS) mediate human experience online. Most RS act to
optimize metrics that are imperfectly aligned with the best-interest of users
but are easy to measure, like ad-clicks and user engagement. This has resulted
in a host of hard-to-measure side-effects: political polarization, addiction,
fake news. RS design faces a recommender alignment problem: that of aligning
recommendations with the goals of users, system designers, and society as a
whole. But how do we test and compare potential solutions to align RS? Their
massive scale makes them costly and risky to test in deployment. We synthesized
a simple abstract modelling framework to guide future work.
  To illustrate it, we construct a toy experiment where we ask: ""How can we
evaluate the consequences of using user retention as a reward function?"" To
answer the question, we learn recommender policies that optimize reward
functions by controlling graph dynamics on a toy environment. Based on the
effects that trained recommenders have on their environment, we conclude that
engagement maximizers generally lead to worse outcomes than aligned
recommenders but not always. After learning, we examine competition between RS
as a potential solution to RS alignment. We find that it generally makes our
toy-society better-off than it would be under the absence of recommendation or
engagement maximizers.
  In this work, we aimed for a broad scope, touching superficially on many
different points to shed light on how an end-to-end study of reward functions
for recommender systems might be done. Recommender alignment is a pressing and
important problem. Attempted solutions are sure to have far-reaching impacts.
Here, we take a first step in developing methods to evaluating and comparing
solutions with respect to their impacts on society.",2208.12299v1,https://arxiv.org/pdf/2208.12299v1
"On Reality and the Limits of Language Data: Aligning LLMs with Human
  Norms","Nigel H. Collier, Fangyu Liu, Ehsan Shareghi","Recent advancements in Large Language Models (LLMs) harness linguistic
associations in vast natural language data for practical applications. However,
their ability to understand the physical world using only language data remains
a question. After reviewing existing protocols, we explore this question using
a novel and tightly controlled reasoning test (ART) and compare human norms
against versions of GPT-3. Our findings highlight the categories of
common-sense relations models that could learn directly from data and areas of
weakness. GPT-3 offers evidence for verbal reasoning on a par with human
subjects for several relations including Synonymy, Antonymy, and Default
inheritance, Without reinforcement learning from human judgements, it appears
GPT-3 performs at the lower end of the reference interval for Has-part and
Contained-in. Weaknesses were observed also in affordance characteristics
through Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs
with symbolic world grounding is a promising direction to address associative
learning.",2208.11981v2,https://arxiv.org/pdf/2208.11981v2
"Grad-Align+: Empowering Gradual Network Alignment Using Attribute
  Augmentation","Jin-Duk Park, Cong Tran, Won-Yong Shin, Xin Cao","Network alignment (NA) is the task of discovering node correspondences across
different networks. Although NA methods have achieved remarkable success in a
myriad of scenarios, their satisfactory performance is not without prior anchor
link information and/or node attributes, which may not always be available. In
this paper, we propose Grad-Align+, a novel NA method using node attribute
augmentation that is quite robust to the absence of such additional
information. Grad-Align+ is built upon a recent state-of-the-art NA method, the
so-called Grad-Align, that gradually discovers only a part of node pairs until
all node pairs are found. Specifically, Grad-Align+ is composed of the
following key components: 1) augmenting node attributes based on nodes'
centrality measures, 2) calculating an embedding similarity matrix extracted
from a graph neural network into which the augmented node attributes are fed,
and 3) gradually discovering node pairs by calculating similarities between
cross-network nodes with respect to the aligned cross-network neighbor-pair.
Experimental results demonstrate that Grad-Align+ exhibits (a) superiority over
benchmark NA methods, (b) empirical validation of our theoretical findings, and
(c) the effectiveness of our attribute augmentation module.",2208.11025v2,https://arxiv.org/pdf/2208.11025v2
"Large-scale Entity Alignment via Knowledge Graph Merging, Partitioning
  and Embedding","Kexuan Xin, Zequn Sun, Wen Hua, Wei Hu, Jianfeng Qu, Xiaofang Zhou","Entity alignment is a crucial task in knowledge graph fusion. However, most
entity alignment approaches have the scalability problem. Recent methods
address this issue by dividing large KGs into small blocks for embedding and
alignment learning in each. However, such a partitioning and learning process
results in an excessive loss of structure and alignment. Therefore, in this
work, we propose a scalable GNN-based entity alignment approach to reduce the
structure and alignment loss from three perspectives. First, we propose a
centrality-based subgraph generation algorithm to recall some landmark entities
serving as the bridges between different subgraphs. Second, we introduce
self-supervised entity reconstruction to recover entity representations from
incomplete neighborhood subgraphs, and design cross-subgraph negative sampling
to incorporate entities from other subgraphs in alignment learning. Third,
during the inference process, we merge the embeddings of subgraphs to make a
single space for alignment search. Experimental results on the benchmark OpenEA
dataset and the proposed large DBpedia1M dataset verify the effectiveness of
our approach.",2208.11125v1,https://arxiv.org/pdf/2208.11125v1
"CAPER: Coarsen, Align, Project, Refine - A General Multilevel Framework
  for Network Alignment","Jing Zhu, Danai Koutra, Mark Heimann","Network alignment, or the task of finding corresponding nodes in different
networks, is an important problem formulation in many application domains. We
propose CAPER, a multilevel alignment framework that Coarsens the input graphs,
Aligns the coarsened graphs, Projects the alignment solution to finer levels
and Refines the alignment solution. We show that CAPER can improve upon many
different existing network alignment algorithms by enforcing alignment
consistency across multiple graph resolutions: nodes matched at finer levels
should also be matched at coarser levels. CAPER also accelerates the use of
slower network alignment methods, at the modest cost of linear-time coarsening
and refinement steps, by allowing them to be run on smaller coarsened versions
of the input graphs. Experiments show that CAPER can improve upon diverse
network alignment methods by an average of 33% in accuracy and/or an order of
magnitude faster in runtime.",2208.10682v1,https://arxiv.org/pdf/2208.10682v1
High-quality Task Division for Large-scale Entity Alignment,"Bing Liu, Wen Hua, Guido Zuccon, Genghong Zhao, Xia Zhang","Entity Alignment (EA) aims to match equivalent entities that refer to the
same real-world objects and is a key step for Knowledge Graph (KG) fusion. Most
neural EA models cannot be applied to large-scale real-life KGs due to their
excessive consumption of GPU memory and time. One promising solution is to
divide a large EA task into several subtasks such that each subtask only needs
to match two small subgraphs of the original KGs. However, it is challenging to
divide the EA task without losing effectiveness. Existing methods display low
coverage of potential mappings, insufficient evidence in context graphs, and
largely differing subtask sizes.
  In this work, we design the DivEA framework for large-scale EA with
high-quality task division. To include in the EA subtasks a high proportion of
the potential mappings originally present in the large EA task, we devise a
counterpart discovery method that exploits the locality principle of the EA
task and the power of trained EA models. Unique to our counterpart discovery
method is the explicit modelling of the chance of a potential mapping. We also
introduce an evidence passing mechanism to quantify the informativeness of
context entities and find the most informative context graphs with flexible
control of the subtask size. Extensive experiments show that DivEA achieves
higher EA performance than alternative state-of-the-art solutions.",2208.10366v1,https://arxiv.org/pdf/2208.10366v1
"Open Vocabulary Multi-Label Classification with Dual-Modal Decoder on
  Aligned Visual-Textual Features","Shichao Xu, Yikang Li, Jenhao Hsiao, Chiuman Ho, Zhu Qi","In computer vision, multi-label recognition are important tasks with many
real-world applications, but classifying previously unseen labels remains a
significant challenge. In this paper, we propose a novel algorithm, Aligned
Dual moDality ClaSsifier (ADDS), which includes a Dual-Modal decoder
(DM-decoder) with alignment between visual and textual features, for
open-vocabulary multi-label classification tasks. Then we design a simple and
yet effective method called Pyramid-Forwarding to enhance the performance for
inputs with high resolutions. Moreover, the Selective Language Supervision is
applied to further enhance the model performance. Extensive experiments
conducted on several standard benchmarks, NUS-WIDE, ImageNet-1k, ImageNet-21k,
and MS-COCO, demonstrate that our approach significantly outperforms previous
methods and provides state-of-the-art performance for open-vocabulary
multi-label classification, conventional multi-label classification and an
extreme case called single-to-multi label classification where models trained
on single-label datasets (ImageNet-1k, ImageNet-21k) are tested on multi-label
ones (MS-COCO and NUS-WIDE).",2208.09562v2,https://arxiv.org/pdf/2208.09562v2
"CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic
  Response Generation","Jinfeng Zhou, Chujie Zheng, Bo Wang, Zheng Zhang, Minlie Huang","Empathetic conversation is psychologically supposed to be the result of
conscious alignment and interaction between the cognition and affection of
empathy. However, existing empathetic dialogue models usually consider only the
affective aspect or treat cognition and affection in isolation, which limits
the capability of empathetic response generation. In this work, we propose the
CASE model for empathetic dialogue generation. It first builds upon a
commonsense cognition graph and an emotional concept graph and then aligns the
user's cognition and affection at both the coarse-grained and fine-grained
levels. Through automatic and manual evaluation, we demonstrate that CASE
outperforms state-of-the-art baselines of empathetic dialogues and can generate
more empathetic and informative responses.",2208.08845v2,https://arxiv.org/pdf/2208.08845v2
"Efficient Signed Graph Sampling via Balancing & Gershgorin Disc Perfect
  Alignment","Chinthaka Dinesh, Gene Cheung, Saghar Bagheri, Ivan V. Bajic","A basic premise in graph signal processing (GSP) is that a graph encoding
pairwise (anti-)correlations of the targeted signal as edge weights is
exploited for graph filtering. However, existing fast graph sampling schemes
are designed and tested only for positive graphs describing positive
correlations. In this paper, we show that for datasets with strong inherent
anti-correlations, a suitable graph contains both positive and negative edge
weights. In response, we propose a linear-time signed graph sampling method
centered on the concept of balanced signed graphs. Specifically, given an
empirical covariance data matrix $\bar{\bf{C}}$, we first learn a sparse
inverse matrix (graph Laplacian) $\mathcal{L}$ corresponding to a signed graph
$\mathcal{G}$. We define the eigenvectors of Laplacian $\mathcal{L}_B$ for a
balanced signed graph $\mathcal{G}_B$ -- approximating $\mathcal{G}$ via edge
weight augmentation -- as graph frequency components. Next, we choose samples
to minimize the low-pass filter reconstruction error in two steps. We first
align all Gershgorin disc left-ends of Laplacian $\mathcal{L}_B$ at smallest
eigenvalue $\lambda_{\min}(\mathcal{L}_B)$ via similarity transform
$\mathcal{L}_p = \S \mathcal{L}_B \S^{-1}$, leveraging a recent linear algebra
theorem called Gershgorin disc perfect alignment (GDPA). We then perform
sampling on $\mathcal{L}_p$ using a previous fast Gershgorin disc alignment
sampling (GDAS) scheme. Experimental results show that our signed graph
sampling method outperformed existing fast sampling schemes noticeably on
various datasets.",2208.08726v2,https://arxiv.org/pdf/2208.08726v2
"LAMA-Net: Unsupervised Domain Adaptation via Latent Alignment and
  Manifold Learning for RUL Prediction","Manu Joseph, Varchita Lalwani","Prognostics and Health Management (PHM) is an emerging field which has
received much attention from the manufacturing industry because of the benefits
and efficiencies it brings to the table. And Remaining Useful Life (RUL)
prediction is at the heart of any PHM system. Most recent data-driven research
demand substantial volumes of labelled training data before a performant model
can be trained under the supervised learning paradigm. This is where Transfer
Learning (TL) and Domain Adaptation (DA) methods step in and make it possible
for us to generalize a supervised model to other domains with different data
distributions with no labelled data. In this paper, we propose
\textit{LAMA-Net}, an encoder-decoder based model (Transformer) with an induced
bottleneck, Latent Alignment using Maximum Mean Discrepancy (MMD) and manifold
learning is proposed to tackle the problem of Unsupervised Homogeneous Domain
Adaptation for RUL prediction. \textit{LAMA-Net} is validated using the C-MAPSS
Turbofan Engine dataset by NASA and compared against other state-of-the-art
techniques for DA. The results suggest that the proposed method offers a
promising approach to perform domain adaptation in RUL prediction. Code will be
made available once the paper comes out of review.",2208.08388v1,https://arxiv.org/pdf/2208.08388v1
"Parallel Hierarchical Transformer with Attention Alignment for
  Abstractive Multi-Document Summarization","Ye Ma, Lu Zong","In comparison to single-document summarization, abstractive Multi-Document
Summarization (MDS) brings challenges on the representation and coverage of its
lengthy and linked sources. This study develops a Parallel Hierarchical
Transformer (PHT) with attention alignment for MDS. By incorporating word- and
paragraph-level multi-head attentions, the hierarchical architecture of PHT
allows better processing of dependencies at both token and document levels. To
guide the decoding towards a better coverage of the source documents, the
attention-alignment mechanism is then introduced to calibrate beam search with
predicted optimal attention distributions. Based on the WikiSum data, a
comprehensive evaluation is conducted to test improvements on MDS by the
proposed architecture. By better handling the inner- and cross-document
information, results in both ROUGE and human evaluation suggest that our
hierarchical model generates summaries of higher quality relative to other
Transformer-based baselines at relatively low computational cost.",2208.07845v1,https://arxiv.org/pdf/2208.07845v1
Can Brain Signals Reveal Inner Alignment with Human Languages?,"William Han, Jielin Qiu, Jiacheng Zhu, Mengdi Xu, Douglas Weber, Bo Li, Ding Zhao","Brain Signals, such as Electroencephalography (EEG), and human languages have
been widely explored independently for many downstream tasks, however, the
connection between them has not been well explored. In this study, we explore
the relationship and dependency between EEG and language. To study at the
representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal
\textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated
representations between the two modalities. We used various relationship
alignment-seeking techniques, such as Canonical Correlation Analysis and
Wasserstein Distance, as loss functions to transfigure features. On downstream
applications, sentiment analysis and relation detection, we achieved new
state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method
achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets
for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition,
we provide interpretations of the performance improvement: (1) feature
distribution shows the effectiveness of the alignment module for discovering
and encoding the relationship between EEG and language; (2) alignment weights
show the influence of different language semantics as well as EEG frequency
features; (3) brain topographical maps provide an intuitive demonstration of
the connectivity in the brain regions. Our code is available at
\url{https://github.com/Jason-Qiu/EEG_Language_Alignment}.",2208.06348v5,https://arxiv.org/pdf/2208.06348v5
"RDA: Reciprocal Distribution Alignment for Robust Semi-supervised
  Learning","Yue Duan, Lei Qi, Lei Wang, Luping Zhou, Yinghuan Shi","In this work, we propose Reciprocal Distribution Alignment (RDA) to address
semi-supervised learning (SSL), which is a hyperparameter-free framework that
is independent of confidence threshold and works with both the matched
(conventionally) and the mismatched class distributions. Distribution mismatch
is an often overlooked but more general SSL scenario where the labeled and the
unlabeled data do not fall into the identical class distribution. This may lead
to the model not exploiting the labeled data reliably and drastically degrade
the performance of SSL methods, which could not be rescued by the traditional
distribution alignment. In RDA, we enforce a reciprocal alignment on the
distributions of the predictions from two classifiers predicting pseudo-labels
and complementary labels on the unlabeled data. These two distributions,
carrying complementary information, could be utilized to regularize each other
without any prior of class distribution. Moreover, we theoretically show that
RDA maximizes the input-output mutual information. Our approach achieves
promising performance in SSL under a variety of scenarios of mismatched
distributions, as well as the conventional matched SSL setting. Our code is
available at: https://github.com/NJUyued/RDA4RobustSSL.",2208.04619v2,https://arxiv.org/pdf/2208.04619v2
"Embedding Alignment for Unsupervised Federated Learning via Smart Data
  Exchange","Satyavrat Wagle, Seyyedali Hosseinalipour, Naji Khosravan, Mung Chiang, Christopher G. Brinton","Federated learning (FL) has been recognized as one of the most promising
solutions for distributed machine learning (ML). In most of the current
literature, FL has been studied for supervised ML tasks, in which edge devices
collect labeled data. Nevertheless, in many applications, it is impractical to
assume existence of labeled data across devices. To this end, we develop a
novel methodology, Cooperative Federated unsupervised Contrastive Learning
(CF-CL), for FL across edge devices with unlabeled datasets. CF-CL employs
local device cooperation where data are exchanged among devices through
device-to-device (D2D) communications to avoid local model bias resulting from
non-independent and identically distributed (non-i.i.d.) local datasets. CF-CL
introduces a push-pull smart data sharing mechanism tailored to unsupervised FL
settings, in which, each device pushes a subset of its local datapoints to its
neighbors as reserved data points, and pulls a set of datapoints from its
neighbors, sampled through a probabilistic importance sampling technique. We
demonstrate that CF-CL leads to (i) alignment of unsupervised learned latent
spaces across devices, (ii) faster global convergence, allowing for less
frequent global model aggregations; and (iii) is effective in extreme
non-i.i.d. data settings across the devices.",2208.02856v1,https://arxiv.org/pdf/2208.02856v1
"Late Fusion Multi-view Clustering via Global and Local Alignment
  Maximization","Siwei Wang, Xinwang Liu, En Zhu","Multi-view clustering (MVC) optimally integrates complementary information
from different views to improve clustering performance. Although demonstrating
promising performance in various applications, most of existing approaches
directly fuse multiple pre-specified similarities to learn an optimal
similarity matrix for clustering, which could cause over-complicated
optimization and intensive computational cost. In this paper, we propose late
fusion MVC via alignment maximization to address these issues. To do so, we
first reveal the theoretical connection of existing k-means clustering and the
alignment between base partitions and the consensus one. Based on this
observation, we propose a simple but effective multi-view algorithm termed
LF-MVC-GAM. It optimally fuses multiple source information in partition level
from each individual view, and maximally aligns the consensus partition with
these weighted base ones. Such an alignment is beneficial to integrate
partition level information and significantly reduce the computational
complexity by sufficiently simplifying the optimization procedure. We then
design another variant, LF-MVC-LAM to further improve the clustering
performance by preserving the local intrinsic structure among multiple
partition spaces. After that, we develop two three-step iterative algorithms to
solve the resultant optimization problems with theoretically guaranteed
convergence. Further, we provide the generalization error bound analysis of the
proposed algorithms. Extensive experiments on eighteen multi-view benchmark
datasets demonstrate the effectiveness and efficiency of the proposed
LF-MVC-GAM and LF-MVC-LAM, ranging from small to large-scale data items. The
codes of the proposed algorithms are publicly available at
https://github.com/wangsiwei2010/latefusionalignment.",2208.01198v1,https://arxiv.org/pdf/2208.01198v1
"Joint covariate-alignment and concept-alignment: a framework for domain
  generalization","Thuan Nguyen, Boyang Lyu, Prakash Ishwar, Matthias Scheutz, Shuchin Aeron","In this paper, we propose a novel domain generalization (DG) framework based
on a new upper bound to the risk on the unseen domain. Particularly, our
framework proposes to jointly minimize both the covariate-shift as well as the
concept-shift between the seen domains for a better performance on the unseen
domain. While the proposed approach can be implemented via an arbitrary
combination of covariate-alignment and concept-alignment modules, in this work
we use well-established approaches for distributional alignment namely, Maximum
Mean Discrepancy (MMD) and covariance Alignment (CORAL), and use an Invariant
Risk Minimization (IRM)-based approach for concept alignment. Our numerical
results show that the proposed methods perform as well as or better than the
state-of-the-art for domain generalization on several data sets.",2208.00898v1,https://arxiv.org/pdf/2208.00898v1
Cross-Modal Alignment Learning of Vision-Language Conceptual Systems,"Taehyeong Kim, Hyeonseop Song, Byoung-Tak Zhang","Human infants learn the names of objects and develop their own conceptual
systems without explicit supervision. In this study, we propose methods for
learning aligned vision-language conceptual systems inspired by infants' word
learning mechanisms. The proposed model learns the associations of visual
objects and words online and gradually constructs cross-modal relational graph
networks. Additionally, we also propose an aligned cross-modal representation
learning method that learns semantic representations of visual objects and
words in a self-supervised manner based on the cross-modal relational graph
networks. It allows entities of different modalities with conceptually the same
meaning to have similar semantic representation vectors. We quantitatively and
qualitatively evaluate our method, including object-to-word mapping and
zero-shot learning tasks, showing that the proposed model significantly
outperforms the baselines and that each conceptual system is topologically
aligned.",2208.01744v1,https://arxiv.org/pdf/2208.01744v1
"ALADIN: Distilling Fine-grained Alignment Scores for Efficient
  Image-Text Matching and Retrieval","Nicola Messina, Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Fabrizio Falchi, Giuseppe Amato, Rita Cucchiara","Image-text matching is gaining a leading role among tasks involving the joint
understanding of vision and language. In literature, this task is often used as
a pre-training objective to forge architectures able to jointly deal with
images and texts. Nonetheless, it has a direct downstream application:
cross-modal retrieval, which consists in finding images related to a given
query text or vice-versa. Solving this task is of critical importance in
cross-modal search engines. Many recent methods proposed effective solutions to
the image-text matching problem, mostly using recent large vision-language (VL)
Transformer networks. However, these models are often computationally
expensive, especially at inference time. This prevents their adoption in
large-scale cross-modal retrieval scenarios, where results should be provided
to the user almost instantaneously. In this paper, we propose to fill in the
gap between effectiveness and efficiency by proposing an ALign And DIstill
Network (ALADIN). ALADIN first produces high-effective scores by aligning at
fine-grained level images and texts. Then, it learns a shared embedding space -
where an efficient kNN search can be performed - by distilling the relevance
scores obtained from the fine-grained alignments. We obtained remarkable
results on MS-COCO, showing that our method can compete with state-of-the-art
VL Transformers while being almost 90 times faster. The code for reproducing
our results is available at https://github.com/mesnico/ALADIN.",2207.14757v1,https://arxiv.org/pdf/2207.14757v1
Curriculum Learning for Data-Efficient Vision-Language Alignment,"Tejas Srinivasan, Xiang Ren, Jesse Thomason","Aligning image and text encoders from scratch using contrastive learning
requires large amounts of paired image-text data. We alleviate this need by
aligning individually pre-trained language and vision representation models
using a much smaller amount of paired data, augmented with a curriculum
learning algorithm to learn fine-grained vision-language alignments. TOnICS
(Training with Ontology-Informed Contrastive Sampling) initially samples
minibatches whose image-text pairs contain a wide variety of objects to learn
object-level alignment, and progressively samples minibatches where all
image-text pairs contain the same object to learn finer-grained contextual
alignment. Aligning pre-trained BERT and VinVL models to each other using
TOnICS outperforms CLIP on downstream zero-shot image retrieval while using
less than 1% as much training data.",2207.14525v1,https://arxiv.org/pdf/2207.14525v1
Initialization and Alignment for Adversarial Texture Optimization,"Xiaoming Zhao, Zhizhen Zhao, Alexander G. Schwing","While recovery of geometry from image and video data has received a lot of
attention in computer vision, methods to capture the texture for a given
geometry are less mature. Specifically, classical methods for texture
generation often assume clean geometry and reasonably well-aligned image data.
While very recent methods, e.g., adversarial texture optimization, better
handle lower-quality data obtained from hand-held devices, we find them to
still struggle frequently. To improve robustness, particularly of recent
adversarial texture optimization, we develop an explicit initialization and an
alignment procedure. It deals with complex geometry due to a robust mapping of
the geometry to the texture map and a hard-assignment-based initialization. It
deals with misalignment of geometry and images by integrating fast
image-alignment into the texture refinement optimization. We demonstrate
efficacy of our texture generation on a dataset of 11 scenes with a total of
2807 frames, observing 7.8% and 11.1% relative improvements regarding
perceptual and sharpness measurements.",2207.14289v1,https://arxiv.org/pdf/2207.14289v1
"Bayesian Optimization-Based Beam Alignment for MmWave MIMO Communication
  Systems","Songjie Yang, Baojuan Liu, Zhiqin Hong, Zhongpei Zhang","Due to the very narrow beam used in millimeter wave communication (mmWave),
beam alignment (BA) is a critical issue. In this work, we investigate the issue
of mmWave BA and present a novel beam alignment scheme on the basis of a
machine learning strategy, Bayesian optimization (BO). In this context, we
consider the beam alignment issue to be a black box function and then use BO to
find the possible optimal beam pair. During the BA procedure, this strategy
exploits information from the measured beam pairs to predict the best beam
pair. In addition, we suggest a novel BO algorithm based on the gradient
boosting regression tree model. The simulation results demonstrate the spectral
efficiency performance of our proposed schemes for BA using three different
surrogate models. They also demonstrate that the proposed schemes can achieve
spectral efficiency with a small overhead when compared to the orthogonal match
pursuit (OMP) algorithm and the Thompson sampling-based multi-armed bandit
(TS-MAB) method.",2207.14174v1,https://arxiv.org/pdf/2207.14174v1
"Technical Report: Assisting Backdoor Federated Learning with Whole
  Population Knowledge Alignment","Tian Liu, Xueyang Hu, Tao Shu","Due to the distributed nature of Federated Learning (FL), researchers have
uncovered that FL is vulnerable to backdoor attacks, which aim at injecting a
sub-task into the FL without corrupting the performance of the main task.
Single-shot backdoor attack achieves high accuracy on both the main task and
backdoor sub-task when injected at the FL model convergence. However, the
early-injected single-shot backdoor attack is ineffective because: (1) the
maximum backdoor effectiveness is not reached at injection because of the
dilution effect from normal local updates; (2) the backdoor effect decreases
quickly as the backdoor will be overwritten by the newcoming normal local
updates. In this paper, we strengthen the early-injected single-shot backdoor
attack utilizing FL model information leakage. We show that the FL convergence
can be expedited if the client trains on a dataset that mimics the distribution
and gradients of the whole population. Based on this observation, we proposed a
two-phase backdoor attack, which includes a preliminary phase for the
subsequent backdoor attack. In the preliminary phase, the attacker-controlled
client first launches a whole population distribution inference attack and then
trains on a locally crafted dataset that is aligned with both the gradient and
inferred distribution. Benefiting from the preliminary phase, the later
injected backdoor achieves better effectiveness as the backdoor effect will be
less likely to be diluted by the normal model updates. Extensive experiments
are conducted on MNIST dataset under various data heterogeneity settings to
evaluate the effectiveness of the proposed backdoor attack. Results show that
the proposed backdoor outperforms existing backdoor attacks in both success
rate and longevity, even when defense mechanisms are in place.",2207.12327v1,https://arxiv.org/pdf/2207.12327v1
"Coupling Adversarial Learning with Selective Voting Strategy for
  Distribution Alignment in Partial Domain Adaptation","Sandipan Choudhuri, Hemanth Venkateswara, Arunabha Sen","In contrast to a standard closed-set domain adaptation task, partial domain
adaptation setup caters to a realistic scenario by relaxing the identical label
set assumption. The fact of source label set subsuming the target label set,
however, introduces few additional obstacles as training on private source
category samples thwart relevant knowledge transfer and mislead the
classification process. To mitigate these issues, we devise a mechanism for
strategic selection of highly-confident target samples essential for the
estimation of class-importance weights. Furthermore, we capture
class-discriminative and domain-invariant features by coupling the process of
achieving compact and distinct class distributions with an adversarial
objective. Experimental findings over numerous cross-domain classification
tasks demonstrate the potential of the proposed technique to deliver superior
and comparable accuracy over existing methods.",2207.08145v1,https://arxiv.org/pdf/2207.08145v1
Domain Alignment Meets Fully Test-Time Adaptation,"Kowshik Thopalli, Pavan Turaga, Jayaraman J. Thiagarajan","A foundational requirement of a deployed ML model is to generalize to data
drawn from a testing distribution that is different from training. A popular
solution to this problem is to adapt a pre-trained model to novel domains using
only unlabeled data. In this paper, we focus on a challenging variant of this
problem, where access to the original source data is restricted. While fully
test-time adaptation (FTTA) and unsupervised domain adaptation (UDA) are
closely related, the advances in UDA are not readily applicable to TTA, since
most UDA methods require access to the source data. Hence, we propose a new
approach, CATTAn, that bridges UDA and FTTA, by relaxing the need to access
entire source data, through a novel deep subspace alignment strategy. With a
minimal overhead of storing the subspace basis set for the source data, CATTAn
enables unsupervised alignment between source and target data during
adaptation. Through extensive experimental evaluation on multiple 2D and 3D
vision benchmarks (ImageNet-C, Office-31, OfficeHome, DomainNet, PointDA-10)
and model architectures, we demonstrate significant gains in FTTA performance.
Furthermore, we make a number of crucial findings on the utility of the
alignment objective even with inherently robust models, pre-trained ViT
representations and under low sample availability in the target domain.",2207.04185v1,https://arxiv.org/pdf/2207.04185v1
Unsupervised Manifold Alignment with Joint Multidimensional Scaling,"Dexiong Chen, Bowen Fan, Carlos Oliver, Karsten Borgwardt","We introduce Joint Multidimensional Scaling, a novel approach for
unsupervised manifold alignment, which maps datasets from two different
domains, without any known correspondences between data instances across the
datasets, to a common low-dimensional Euclidean space. Our approach integrates
Multidimensional Scaling (MDS) and Wasserstein Procrustes analysis into a joint
optimization problem to simultaneously generate isometric embeddings of data
and learn correspondences between instances from two different datasets, while
only requiring intra-dataset pairwise dissimilarities as input. This unique
characteristic makes our approach applicable to datasets without access to the
input features, such as solving the inexact graph matching problem. We propose
an alternating optimization scheme to solve the problem that can fully benefit
from the optimization techniques for MDS and Wasserstein Procrustes. We
demonstrate the effectiveness of our approach in several applications,
including joint visualization of two datasets, unsupervised heterogeneous
domain adaptation, graph matching, and protein structure alignment. The
implementation of our work is available at
https://github.com/BorgwardtLab/JointMDS",2207.02968v2,https://arxiv.org/pdf/2207.02968v2
"The alignment property of SGD noise and how it helps select flat minima:
  A stability analysis","Lei Wu, Mingze Wang, Weijie Su","The phenomenon that stochastic gradient descent (SGD) favors flat minima has
played a critical role in understanding the implicit regularization of SGD. In
this paper, we provide an explanation of this striking phenomenon by relating
the particular noise structure of SGD to its \emph{linear stability} (Wu et
al., 2018). Specifically, we consider training over-parameterized models with
square loss. We prove that if a global minimum $\theta^*$ is linearly stable
for SGD, then it must satisfy $\|H(\theta^*)\|_F\leq O(\sqrt{B}/\eta)$, where
$\|H(\theta^*)\|_F, B,\eta$ denote the Frobenius norm of Hessian at $\theta^*$,
batch size, and learning rate, respectively. Otherwise, SGD will escape from
that minimum \emph{exponentially} fast. Hence, for minima accessible to SGD,
the sharpness -- as measured by the Frobenius norm of the Hessian -- is bounded
\emph{independently} of the model size and sample size. The key to obtaining
these results is exploiting the particular structure of SGD noise: The noise
concentrates in sharp directions of local landscape and the magnitude is
proportional to loss value. This alignment property of SGD noise provably holds
for linear networks and random feature models (RFMs), and is empirically
verified for nonlinear networks. Moreover, the validity and practical relevance
of our theoretical findings are also justified by extensive experiments on
CIFAR-10 dataset.",2207.02628v3,https://arxiv.org/pdf/2207.02628v3
Cooperative Distribution Alignment via JSD Upper Bound,"Wonwoong Cho, Ziyu Gong, David I. Inouye","Unsupervised distribution alignment estimates a transformation that maps two
or more source distributions to a shared aligned distribution given only
samples from each distribution. This task has many applications including
generative modeling, unsupervised domain adaptation, and socially aware
learning. Most prior works use adversarial learning (i.e., min-max
optimization), which can be challenging to optimize and evaluate. A few recent
works explore non-adversarial flow-based (i.e., invertible) approaches, but
they lack a unified perspective and are limited in efficiently aligning
multiple distributions. Therefore, we propose to unify and generalize previous
flow-based approaches under a single non-adversarial framework, which we prove
is equivalent to minimizing an upper bound on the Jensen-Shannon Divergence
(JSD). Importantly, our problem reduces to a min-min, i.e., cooperative,
problem and can provide a natural evaluation metric for unsupervised
distribution alignment. We show empirical results on both simulated and
real-world datasets to demonstrate the benefits of our approach. Code is
available at https://github.com/inouye-lab/alignment-upper-bound.",2207.02286v2,https://arxiv.org/pdf/2207.02286v2
Cybersecurity Entity Alignment via Masked Graph Attention Networks,"Yue Qin, Xiaojing Liao","Cybersecurity vulnerability information is often recorded by multiple
channels, including government vulnerability repositories,
individual-maintained vulnerability-gathering platforms, or
vulnerability-disclosure email lists and forums. Integrating vulnerability
information from different channels enables comprehensive threat assessment and
quick deployment to various security mechanisms. Efforts to automatically
gather such information, however, are impeded by the limitations of today's
entity alignment techniques. In our study, we annotate the first
cybersecurity-domain entity alignment dataset and reveal the unique
characteristics of security entities. Based on these observations, we propose
the first cybersecurity entity alignment model, CEAM, which equips GNN-based
entity alignment with two mechanisms: asymmetric masked aggregation and
partitioned attention. Experimental results on cybersecurity-domain entity
alignment datasets demonstrate that CEAM significantly outperforms
state-of-the-art entity alignment methods.",2207.01434v1,https://arxiv.org/pdf/2207.01434v1
Multi-scale alignment and Spatial ROI Module for COVID-19 Diagnosis,"Hongyan Xu, Dadong Wang, Arcot Sowmya","Coronavirus Disease 2019 (COVID-19) has spread globally and become a health
crisis faced by humanity since first reported. Radiology imaging technologies
such as computer tomography (CT) and chest X-ray imaging (CXR) are effective
tools for diagnosing COVID-19. However, in CT and CXR images, the infected area
occupies only a small part of the image. Some common deep learning methods that
integrate large-scale receptive fields may cause the loss of image detail,
resulting in the omission of the region of interest (ROI) in COVID-19 images
and are therefore not suitable for further processing. To this end, we propose
a deep spatial pyramid pooling (D-SPP) module to integrate contextual
information over different resolutions, aiming to extract information under
different scales of COVID-19 images effectively. Besides, we propose a COVID-19
infection detection (CID) module to draw attention to the lesion area and
remove interference from irrelevant information. Extensive experiments on four
CT and CXR datasets have shown that our method produces higher accuracy of
detecting COVID-19 lesions in CT and CXR images. It can be used as a
computer-aided diagnosis tool to help doctors effectively diagnose and screen
for COVID-19.",2207.01345v1,https://arxiv.org/pdf/2207.01345v1
"The Linguistic Blind Spot of Value-Aligned Agency, Natural and
  Artificial",Travis LaCroix,"The value-alignment problem for artificial intelligence (AI) asks how we can
ensure that the 'values' (i.e., objective functions) of artificial systems are
aligned with the values of humanity. In this paper, I argue that linguistic
communication (natural language) is a necessary condition for robust value
alignment. I discuss the consequences that the truth of this claim would have
for research programmes that attempt to ensure value alignment for AI systems;
or, more loftily, designing robustly beneficial or ethical artificial agents.",2207.00868v1,https://arxiv.org/pdf/2207.00868v1
Target alignment in truncated kernel ridge regression,"Arash A. Amini, Richard Baumgartner, Dai Feng","Kernel ridge regression (KRR) has recently attracted renewed interest due to
its potential for explaining the transient effects, such as double descent,
that emerge during neural network training. In this work, we study how the
alignment between the target function and the kernel affects the performance of
the KRR. We focus on the truncated KRR (TKRR) which utilizes an additional
parameter that controls the spectral truncation of the kernel matrix. We show
that for polynomial alignment, there is an \emph{over-aligned} regime, in which
TKRR can achieve a faster rate than what is achievable by full KRR. The rate of
TKRR can improve all the way to the parametric rate, while that of full KRR is
capped at a sub-optimal value. This shows that target alignemnt can be better
leveraged by utilizing spectral truncation in kernel methods. We also consider
the bandlimited alignment setting and show that the regularization surface of
TKRR can exhibit transient effects including multiple descent and non-monotonic
behavior. Our results show that there is a strong and quantifable relation
between the shape of the \emph{alignment spectrum} and the generalization
performance of kernel methods, both in terms of rates and in finite samples.",2206.14255v1,https://arxiv.org/pdf/2206.14255v1
"Robustifying Vision Transformer without Retraining from Scratch by
  Test-Time Class-Conditional Feature Alignment","Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa","Vision Transformer (ViT) is becoming more popular in image processing.
Specifically, we investigate the effectiveness of test-time adaptation (TTA) on
ViT, a technique that has emerged to correct its prediction during test-time by
itself. First, we benchmark various test-time adaptation approaches on ViT-B16
and ViT-L16. It is shown that the TTA is effective on ViT and the
prior-convention (sensibly selecting modulation parameters) is not necessary
when using proper loss function. Based on the observation, we propose a new
test-time adaptation method called class-conditional feature alignment (CFA),
which minimizes both the class-conditional distribution differences and the
whole distribution differences of the hidden representation between the source
and target in an online manner. Experiments of image classification tasks on
common corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domain
adaptation (digits datasets and ImageNet-Sketch) show that CFA stably
outperforms the existing baselines on various datasets. We also verify that CFA
is model agnostic by experimenting on ResNet, MLP-Mixer, and several ViT
variants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8%
top-1 error rate on ImageNet-C, outperforming the existing test-time adaptation
baseline 44.0%. This is a state-of-the-art result among TTA methods that do not
need to alter training phase.",2206.13951v1,https://arxiv.org/pdf/2206.13951v1
Aligning Artificial Intelligence with Humans through Public Policy,"John Nay, James Daily","Given that Artificial Intelligence (AI) increasingly permeates our lives, it
is critical that we systematically align AI objectives with the goals and
values of humans. The human-AI alignment problem stems from the impracticality
of explicitly specifying the rewards that AI models should receive for all the
actions they could take in all relevant states of the world. One possible
solution, then, is to leverage the capabilities of AI models to learn those
rewards implicitly from a rich source of data describing human values in a wide
range of contexts. The democratic policy-making process produces just such data
by developing specific rules, flexible standards, interpretable guidelines, and
generalizable precedents that synthesize citizens' preferences over potential
actions taken in many states of the world. Therefore, computationally encoding
public policies to make them legible to AI systems should be an important part
of a socio-technical approach to the broader human-AI alignment puzzle. This
Essay outlines research on AI that learn structures in policy data that can be
leveraged for downstream tasks. As a demonstration of the ability of AI to
comprehend policy, we provide a case study of an AI system that predicts the
relevance of proposed legislation to any given publicly traded company and its
likely effect on that company. We believe this represents the ""comprehension""
phase of AI and policy, but leveraging policy as a key source of human values
to align AI requires ""understanding"" policy. Solving the alignment problem is
crucial to ensuring that AI is beneficial both individually (to the person or
group deploying the AI) and socially. As AI systems are given increasing
responsibility in high-stakes contexts, integrating democratically-determined
policy into those systems could align their behavior with human goals in a way
that is responsive to a constantly evolving society.",2207.01497v1,https://arxiv.org/pdf/2207.01497v1
Closed-Form Diffeomorphic Transformations for Time Series Alignment,"Iñigo Martinez, Elisabeth Viles, Igor G. Olaizola","Time series alignment methods call for highly expressive, differentiable and
invertible warping functions which preserve temporal topology, i.e
diffeomorphisms. Diffeomorphic warping functions can be generated from the
integration of velocity fields governed by an ordinary differential equation
(ODE). Gradient-based optimization frameworks containing diffeomorphic
transformations require to calculate derivatives to the differential equation's
solution with respect to the model parameters, i.e. sensitivity analysis.
Unfortunately, deep learning frameworks typically lack
automatic-differentiation-compatible sensitivity analysis methods; and implicit
functions, such as the solution of ODE, require particular care. Current
solutions appeal to adjoint sensitivity methods, ad-hoc numerical solvers or
ResNet's Eulerian discretization. In this work, we present a closed-form
expression for the ODE solution and its gradient under continuous
piecewise-affine (CPA) velocity functions. We present a highly optimized
implementation of the results on CPU and GPU. Furthermore, we conduct extensive
experiments on several datasets to validate the generalization ability of our
model to unseen data for time-series joint alignment. Results show significant
improvements both in terms of efficiency and accuracy.",2206.08107v1,https://arxiv.org/pdf/2206.08107v1
Diffusion Transport Alignment,"Andres F. Duque, Guy Wolf, Kevin R. Moon","The integration of multimodal data presents a challenge in cases when the
study of a given phenomena by different instruments or conditions generates
distinct but related domains. Many existing data integration methods assume a
known one-to-one correspondence between domains of the entire dataset, which
may be unrealistic. Furthermore, existing manifold alignment methods are not
suited for cases where the data contains domain-specific regions, i.e., there
is not a counterpart for a certain portion of the data in the other domain. We
propose Diffusion Transport Alignment (DTA), a semi-supervised manifold
alignment method that exploits prior correspondence knowledge between only a
few points to align the domains. By building a diffusion process, DTA finds a
transportation plan between data measured from two heterogeneous domains with
different feature spaces, which by assumption, share a similar geometrical
structure coming from the same underlying data generating process. DTA can also
compute a partial alignment in a data-driven fashion, resulting in accurate
alignments when some data are measured in only one domain. We empirically
demonstrate that DTA outperforms other methods in aligning multimodal data in
this semisupervised setting. We also empirically show that the alignment
obtained by DTA can improve the performance of machine learning tasks, such as
domain adaptation, inter-domain feature mapping, and exploratory data analysis,
while outperforming competing methods.",2206.07305v1,https://arxiv.org/pdf/2206.07305v1
"Manifold Alignment-Based Multi-Fidelity Reduced-Order Modeling Applied
  to Structural Analysis","Christian Perron, Darshan Sarojini, Dushhyanth Rajaram, Jason Corman, Dimitri Mavris","This work presents the application of a recently developed parametric,
non-intrusive, and multi-fidelity reduced-order modeling method on
high-dimensional displacement and stress fields arising from the structural
analysis of geometries that differ in the size of discretization and structural
topology.The proposed approach leverages manifold alignment to fuse
inconsistent field outputs from high- and low-fidelity simulations by
individually projecting their solution onto a common subspace. The
effectiveness of the method is demonstrated on two multi-fidelity scenarios
involving the structural analysis of a benchmark wing geometry. Results show
that outputs from structural simulations using incompatible grids, or related
yet different topologies, are easily combined into a single predictive model,
thus eliminating the need for additional pre-processing of the data. The new
multi-fidelity reduced-order model achieves a relatively higher predictive
accuracy at a lower computational cost when compared to a single-fidelity
model.",2206.06920v1,https://arxiv.org/pdf/2206.06920v1
"A Semantic Consistency Feature Alignment Object Detection Model Based on
  Mixed-Class Distribution Metrics","Lijun Gou, Jinrong Yang, Hangcheng Yu, Pan Wang, Xiaoping Li, Chao Deng","Unsupervised domain adaptation is critical in various computer vision tasks,
such as object detection, instance segmentation, etc. They attempt to reduce
domain bias-induced performance degradation while also promoting model
application speed. Previous works in domain adaptation object detection attempt
to align image-level and instance-level shifts to eventually minimize the
domain discrepancy, but they may align single-class features to mixed-class
features in image-level domain adaptation because each image in the object
detection task may be more than one class and object. In order to achieve
single-class with single-class alignment and mixed-class with mixed-class
alignment, we treat the mixed-class of the feature as a new class and propose a
mixed-classes $H-divergence$ for object detection to achieve homogenous feature
alignment and reduce negative transfer. Then, a Semantic Consistency Feature
Alignment Model (SCFAM) based on mixed-classes $H-divergence$ was also
presented. To improve single-class and mixed-class semantic information and
accomplish semantic separation, the SCFAM model proposes Semantic Prediction
Models (SPM) and Semantic Bridging Components (SBC). And the weight of the pix
domain discriminator loss is then changed based on the SPM result to reduce
sample imbalance. Extensive unsupervised domain adaption experiments on widely
used datasets illustrate our proposed approach's robust object detection in
domain bias settings.",2206.05765v1,https://arxiv.org/pdf/2206.05765v1
Researching Alignment Research: Unsupervised Analysis,"Jan H. Kirchner, Logan Smith, Jacques Thibodeau, Kyle McDonell, Laria Reynolds","AI alignment research is the field of study dedicated to ensuring that
artificial intelligence (AI) benefits humans. As machine intelligence gets more
advanced, this research is becoming increasingly important. Researchers in the
field share ideas across different media to speed up the exchange of
information. However, this focus on speed means that the research landscape is
opaque, making it difficult for young researchers to enter the field. In this
project, we collected and analyzed existing AI alignment research. We found
that the field is growing quickly, with several subfields emerging in parallel.
We looked at the subfields and identified the prominent researchers, recurring
topics, and different modes of communication in each. Furthermore, we found
that a classifier trained on AI alignment research articles can detect relevant
articles that we did not originally include in the dataset. We are sharing the
dataset with the research community and hope to develop tools in the future
that will help both established researchers and young researchers get more
involved in the field.",2206.02841v1,https://arxiv.org/pdf/2206.02841v1
"Toward Learning Robust and Invariant Representations with Alignment
  Regularization and Data Augmentation","Haohan Wang, Zeyi Huang, Xindi Wu, Eric P. Xing","Data augmentation has been proven to be an effective technique for developing
machine learning models that are robust to known classes of distributional
shifts (e.g., rotations of images), and alignment regularization is a technique
often used together with data augmentation to further help the model learn
representations invariant to the shifts used to augment the data. In this
paper, motivated by a proliferation of options of alignment regularizations, we
seek to evaluate the performances of several popular design choices along the
dimensions of robustness and invariance, for which we introduce a new test
procedure. Our synthetic experiment results speak to the benefits of squared l2
norm regularization. Further, we also formally analyze the behavior of
alignment regularization to complement our empirical study under assumptions we
consider realistic. Finally, we test this simple technique we identify
(worst-case data augmentation with squared l2 norm alignment regularization)
and show that the benefits of this method outrun those of the specially
designed methods. We also release a software package in both TensorFlow and
PyTorch for users to use the method with a couple of lines at
https://github.com/jyanln/AlignReg.",2206.01909v1,https://arxiv.org/pdf/2206.01909v1
ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts,"Bingqian Lin, Yi Zhu, Zicong Chen, Xiwen Liang, Jianzhuang Liu, Xiaodan Liang","Vision-Language Navigation (VLN) is a challenging task that requires an
embodied agent to perform action-level modality alignment, i.e., make
instruction-asked actions sequentially in complex visual environments. Most
existing VLN agents learn the instruction-path data directly and cannot
sufficiently explore action-level alignment knowledge inside the multi-modal
inputs. In this paper, we propose modAlity-aligneD Action PrompTs (ADAPT),
which provides the VLN agent with action prompts to enable the explicit
learning of action-level modality alignment to pursue successful navigation.
Specifically, an action prompt is defined as a modality-aligned pair of an
image sub-prompt and a text sub-prompt, where the former is a single-view
observation and the latter is a phrase like ''walk past the chair''. When
starting navigation, the instruction-related action prompt set is retrieved
from a pre-built action prompt base and passed through a prompt encoder to
obtain the prompt feature. Then the prompt feature is concatenated with the
original instruction feature and fed to a multi-layer transformer for action
prediction. To collect high-quality action prompts into the prompt base, we use
the Contrastive Language-Image Pretraining (CLIP) model which has powerful
cross-modality alignment ability. A modality alignment loss and a sequential
consistency loss are further introduced to enhance the alignment of the action
prompt and enforce the agent to focus on the related prompt sequentially.
Experimental results on both R2R and RxR show the superiority of ADAPT over
state-of-the-art methods.",2205.15509v1,https://arxiv.org/pdf/2205.15509v1
"Align then Fusion: Generalized Large-scale Multi-view Clustering with
  Anchor Matching Correspondences","Siwei Wang, Xinwang Liu, Suyuan Liu, Jiaqi Jin, Wenxuan Tu, Xinzhong Zhu, En Zhu","Multi-view anchor graph clustering selects representative anchors to avoid
full pair-wise similarities and therefore reduce the complexity of graph
methods. Although widely applied in large-scale applications, existing
approaches do not pay sufficient attention to establishing correct
correspondences between the anchor sets across views. To be specific, anchor
graphs obtained from different views are not aligned column-wisely. Such an
\textbf{A}nchor-\textbf{U}naligned \textbf{P}roblem (AUP) would cause
inaccurate graph fusion and degrade the clustering performance. Under
multi-view scenarios, generating correct correspondences could be extremely
difficult since anchors are not consistent in feature dimensions. To solve this
challenging issue, we propose the first study of the generalized and flexible
anchor graph fusion framework termed \textbf{F}ast \textbf{M}ulti-\textbf{V}iew
\textbf{A}nchor-\textbf{C}orrespondence \textbf{C}lustering (FMVACC).
Specifically, we show how to find anchor correspondence with both feature and
structure information, after which anchor graph fusion is performed
column-wisely. Moreover, we theoretically show the connection between FMVACC
and existing multi-view late fusion \cite{liu2018late} and partial view-aligned
clustering \cite{huang2020partially}, which further demonstrates our
generality. Extensive experiments on seven benchmark datasets demonstrate the
effectiveness and efficiency of our proposed method. Moreover, the proposed
alignment module also shows significant performance improvement applying to
existing multi-view anchor graph competitors indicating the importance of
anchor alignment. Our code is available at
\url{https://github.com/wangsiwei2010/NeurIPS22-FMVACC}.",2205.15075v2,https://arxiv.org/pdf/2205.15075v2
"Knowledge Distillation for 6D Pose Estimation by Aligning Distributions
  of Local Predictions","Shuxuan Guo, Yinlin Hu, Jose M. Alvarez, Mathieu Salzmann","Knowledge distillation facilitates the training of a compact student network
by using a deep teacher one. While this has achieved great success in many
tasks, it remains completely unstudied for image-based 6D object pose
estimation. In this work, we introduce the first knowledge distillation method
driven by the 6D pose estimation task. To this end, we observe that most modern
6D pose estimation frameworks output local predictions, such as sparse 2D
keypoints or dense representations, and that the compact student network
typically struggles to predict such local quantities precisely. Therefore,
instead of imposing prediction-to-prediction supervision from the teacher to
the student, we propose to distill the teacher's \emph{distribution} of local
predictions into the student network, facilitating its training. Our
experiments on several benchmarks show that our distillation method yields
state-of-the-art results with different compact student models and for both
keypoint-based and dense prediction-based architectures.",2205.14971v2,https://arxiv.org/pdf/2205.14971v2
Feature-Aligned Video Raindrop Removal with Temporal Constraints,"Wending Yan, Lu Xu, Wenhan Yang, Robby T. Tan","Existing adherent raindrop removal methods focus on the detection of the
raindrop locations, and then use inpainting techniques or generative networks
to recover the background behind raindrops. Yet, as adherent raindrops are
diverse in sizes and appearances, the detection is challenging for both single
image and video. Moreover, unlike rain streaks, adherent raindrops tend to
cover the same area in several frames. Addressing these problems, our method
employs a two-stage video-based raindrop removal method. The first stage is the
single image module, which generates initial clean results. The second stage is
the multiple frame module, which further refines the initial results using
temporal constraints, namely, by utilizing multiple input frames in our process
and applying temporal consistency between adjacent output frames. Our single
image module employs a raindrop removal network to generate initial raindrop
removal results, and create a mask representing the differences between the
input and initial output. Once the masks and initial results for consecutive
frames are obtained, our multiple-frame module aligns the frames in both the
image and feature levels and then obtains the clean background. Our method
initially employs optical flow to align the frames, and then utilizes
deformable convolution layers further to achieve feature-level frame alignment.
To remove small raindrops and recover correct backgrounds, a target frame is
predicted from adjacent frames. A series of unsupervised losses are proposed so
that our second stage, which is the video raindrop removal module, can
self-learn from video data without ground truths. Experimental results on real
videos demonstrate the state-of-art performance of our method both
quantitatively and qualitatively.",2205.14574v1,https://arxiv.org/pdf/2205.14574v1
Fair Representation Learning through Implicit Path Alignment,"Changjian Shui, Qi Chen, Jiaqi Li, Boyu Wang, Christian Gagné","We consider a fair representation learning perspective, where optimal
predictors, on top of the data representation, are ensured to be invariant with
respect to different sub-groups. Specifically, we formulate this intuition as a
bi-level optimization, where the representation is learned in the outer-loop,
and invariant optimal group predictors are updated in the inner-loop. Moreover,
the proposed bi-level objective is demonstrated to fulfill the sufficiency
rule, which is desirable in various practical scenarios but was not commonly
studied in the fair learning. Besides, to avoid the high computational and
memory cost of differentiating in the inner-loop of bi-level objective, we
propose an implicit path alignment algorithm, which only relies on the solution
of inner optimization and the implicit differentiation rather than the exact
optimization path. We further analyze the error gap of the implicit approach
and empirically validate the proposed method in both classification and
regression settings. Experimental results show the consistently better
trade-off in prediction performance and fairness measurement.",2205.13316v1,https://arxiv.org/pdf/2205.13316v1
"On statistic alignment for domain adaptation in structural health
  monitoring","Jack Poole, Paul Gardner, Nikolaos Dervilis, Lawrence Bull, Keith Worden","The practical application of structural health monitoring (SHM) is often
limited by the availability of labelled data. Transfer learning - specifically
in the form of domain adaptation (DA) - gives rise to the possibility of
leveraging information from a population of physical or numerical structures,
by inferring a mapping that aligns the feature spaces. Typical DA methods rely
on nonparametric distance metrics, which require sufficient data to perform
density estimation. In addition, these methods can be prone to performance
degradation under class imbalance. To address these issues, statistic alignment
(SA) is discussed, with a demonstration of how these methods can be made robust
to class imbalance, including a special case of class imbalance called a
partial DA scenario. SA is demonstrated to facilitate damage localisation with
no target labels in a numerical case study, outperforming other
state-of-the-art DA methods. It is then shown to be capable of aligning the
feature spaces of a real heterogeneous population, the Z24 and KW51 bridges,
with only 220 samples used from the KW51 bridge. Finally, in scenarios where
more complex mappings are required for knowledge transfer, SA is shown to be a
vital pre-processing tool, increasing the performance of established DA
methods.",2205.12052v1,https://arxiv.org/pdf/2205.12052v1
"Utilizing Language-Image Pretraining for Efficient and Robust Bilingual
  Word Alignment","Tuan Dinh, Jy-yong Sohn, Shashank Rajput, Timothy Ossowski, Yifei Ming, Junjie Hu, Dimitris Papailiopoulos, Kangwook Lee","Word translation without parallel corpora has become feasible, rivaling the
performance of supervised methods. Recent findings have shown that the accuracy
and robustness of unsupervised word translation (UWT) can be improved by making
use of visual observations, which are universal representations across
languages. In this work, we investigate the potential of using not only visual
observations but also pretrained language-image models for enabling a more
efficient and robust UWT. Specifically, we develop a novel UWT method dubbed
Word Alignment using Language-Image Pretraining (WALIP), which leverages visual
observations via the shared embedding space of images and texts provided by
CLIP models (Radford et al., 2021). WALIP has a two-step procedure. First, we
retrieve word pairs with high confidences of similarity, computed using our
proposed image-based fingerprints, which define the initial pivot for the word
alignment. Second, we apply our robust Procrustes algorithm to estimate the
linear mapping between two embedding spaces, which iteratively corrects and
refines the estimated alignment. Our extensive experiments show that WALIP
improves upon the state-of-the-art performance of bilingual word alignment for
a few language pairs across different word embeddings and displays great
robustness to the dissimilarity of language pairs or training corpora for two
word embeddings.",2205.11616v2,https://arxiv.org/pdf/2205.11616v2
"UVA Resources for the Biomedical Vocabulary Alignment at Scale in the
  UMLS Metathesaurus","Vinh Nguyen, Olivier Bodenreider","The construction and maintenance process of the UMLS (Unified Medical
Language System) Metathesaurus is time-consuming, costly, and error-prone as it
relies on (1) the lexical and semantic processing for suggesting synonymous
terms, and (2) the expertise of UMLS editors for curating the suggestions. For
improving the UMLS Metathesaurus construction process, our research group has
defined a new task called UVA (UMLS Vocabulary Alignment) and generated a
dataset for evaluating the task. Our group has also developed different
baselines for this task using logical rules (RBA), and neural networks (LexLM
and ConLM).
  In this paper, we present a set of reusable and reproducible resources
including (1) a dataset generator, (2) three datasets generated by using the
generator, and (3) three baseline approaches. We describe the UVA dataset
generator and its implementation generalized for any given UMLS release. We
demonstrate the use of the dataset generator by generating datasets
corresponding to three UMLS releases, 2020AA, 2021AA, and 2021AB. We provide
three UVA baselines using the three existing approaches (LexLM, ConLM, and
RBA). The code, the datasets, and the experiments are publicly available,
reusable, and reproducible with any UMLS release (a no-cost license agreement
is required for downloading the UMLS).",2205.10575v1,https://arxiv.org/pdf/2205.10575v1
"Aligning Logits Generatively for Principled Black-Box Knowledge
  Distillation","Jing Ma, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li","Black-Box Knowledge Distillation (B2KD) is a formulated problem for
cloud-to-edge model compression with invisible data and models hosted on the
server. B2KD faces challenges such as limited Internet exchange and edge-cloud
disparity of data distributions. In this paper, we formalize a two-step
workflow consisting of deprivatization and distillation, and theoretically
provide a new optimization direction from logits to cell boundary different
from direct logits alignment. With its guidance, we propose a new method
Mapping-Emulation KD (MEKD) that distills a black-box cumbersome model into a
lightweight one. Our method does not differentiate between treating soft or
hard responses, and consists of: 1) deprivatization: emulating the inverse
mapping of the teacher function with a generator, and 2) distillation: aligning
low-dimensional logits of the teacher and student models by reducing the
distance of high-dimensional image points. For different teacher-student pairs,
our method yields inspiring distillation performance on various benchmarks, and
outperforms the previous state-of-the-art approaches.",2205.10490v2,https://arxiv.org/pdf/2205.10490v2
"ClusterEA: Scalable Entity Alignment with Stochastic Training and
  Normalized Mini-batch Similarities","Yunjun Gao, Xiaoze Liu, Junyang Wu, Tianyi Li, Pengfei Wang, Lu Chen","Entity alignment (EA) aims at finding equivalent entities in different
knowledge graphs (KGs). Embedding-based approaches have dominated the EA task
in recent years. Those methods face problems that come from the geometric
properties of embedding vectors, including hubness and isolation. To solve
these geometric problems, many normalization approaches have been adopted for
EA. However, the increasing scale of KGs renders it hard for EA models to adopt
the normalization processes, thus limiting their usage in real-world
applications. To tackle this challenge, we present ClusterEA, a general
framework that is capable of scaling up EA models and enhancing their results
by leveraging normalization methods on mini-batches with a high entity
equivalent rate. ClusterEA contains three components to align entities between
large-scale KGs, including stochastic training, ClusterSampler, and
SparseFusion. It first trains a large-scale Siamese GNN for EA in a stochastic
fashion to produce entity embeddings. Based on the embeddings, a novel
ClusterSampler strategy is proposed for sampling highly overlapped
mini-batches. Finally, ClusterEA incorporates SparseFusion, which normalizes
local and global similarity and then fuses all similarity matrices to obtain
the final similarity matrix. Extensive experiments with real-life datasets on
EA benchmarks offer insight into the proposed framework, and suggest that it is
capable of outperforming the state-of-the-art scalable EA framework by up to 8
times in terms of Hits@1.",2205.10312v2,https://arxiv.org/pdf/2205.10312v2
Manifold-aligned Neighbor Embedding,"Mohammad Tariqul Islam, Jason W. Fleischer","In this paper, we introduce a neighbor embedding framework for manifold
alignment. We demonstrate the efficacy of the framework using a
manifold-aligned version of the uniform manifold approximation and projection
algorithm. We show that our algorithm can learn an aligned manifold that is
visually competitive to embedding of the whole dataset.",2205.11257v1,https://arxiv.org/pdf/2205.11257v1
"LAGr: Label Aligned Graphs for Better Systematic Generalization in
  Semantic Parsing","Dora Jambor, Dzmitry Bahdanau","Semantic parsing is the task of producing structured meaning representations
for natural language sentences. Recent research has pointed out that the
commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle to
generalize systematically, i.e. to handle examples that require recombining
known knowledge in novel settings. In this work, we show that better systematic
generalization can be achieved by producing the meaning representation directly
as a graph and not as a sequence. To this end we propose LAGr (Label Aligned
Graphs), a general framework to produce semantic parses by independently
predicting node and edge labels for a complete multi-layer input-aligned graph.
The strongly-supervised LAGr algorithm requires aligned graphs as inputs,
whereas weakly-supervised LAGr infers alignments for originally unaligned
target graphs using approximate maximum-a-posteriori inference. Experiments
demonstrate that LAGr achieves significant improvements in systematic
generalization upon the baseline seq2seq parsers in both strongly- and
weakly-supervised settings.",2205.09607v2,https://arxiv.org/pdf/2205.09607v2
Gradient Aligned Attacks via a Few Queries,"Xiangyuan Yang, Jie Lin, Hanlin Zhang, Xinyu Yang, Peng Zhao","Black-box query attacks, which rely only on the output of the victim model,
have proven to be effective in attacking deep learning models. However,
existing black-box query attacks show low performance in a novel scenario where
only a few queries are allowed. To address this issue, we propose gradient
aligned attacks (GAA), which use the gradient aligned losses (GAL) we designed
on the surrogate model to estimate the accurate gradient to improve the attack
performance on the victim model. Specifically, we propose a gradient aligned
mechanism to ensure that the derivatives of the loss function with respect to
the logit vector have the same weight coefficients between the surrogate and
victim models. Using this mechanism, we transform the cross-entropy (CE) loss
and margin loss into gradient aligned forms, i.e. the gradient aligned CE or
margin losses. These losses not only improve the attack performance of our
gradient aligned attacks in the novel scenario but also increase the query
efficiency of existing black-box query attacks. Through theoretical and
empirical analysis on the ImageNet database, we demonstrate that our gradient
aligned mechanism is effective, and that our gradient aligned attacks can
improve the attack performance in the novel scenario by 16.1\% and 31.3\% on
the $l_2$ and $l_{\infty}$ norms of the box constraint, respectively, compared
to four latest transferable prior-based query attacks. Additionally, the
gradient aligned losses also significantly reduce the number of queries
required in these transferable prior-based query attacks by a maximum factor of
2.9 times. Overall, our proposed gradient aligned attacks and losses show
significant improvements in the attack performance and query efficiency of
black-box query attacks, particularly in scenarios where only a few queries are
allowed.",2205.09518v2,https://arxiv.org/pdf/2205.09518v2
"Entity Alignment with Reliable Path Reasoning and Relation-Aware
  Heterogeneous Graph Transformer","Weishan Cai, Wenjun Ma, Jieyu Zhan, Yuncheng Jiang","Entity Alignment (EA) has attracted widespread attention in both academia and
industry, which aims to seek entities with same meanings from different
Knowledge Graphs (KGs). There are substantial multi-step relation paths between
entities in KGs, indicating the semantic relations of entities. However,
existing methods rarely consider path information because not all natural paths
facilitate for EA judgment. In this paper, we propose a more effective entity
alignment framework, RPR-RHGT, which integrates relation and path structure
information, as well as the heterogeneous information in KGs. Impressively, an
initial reliable path reasoning algorithm is developed to generate the paths
favorable for EA task from the relation structures of KGs, which is the first
algorithm in the literature to successfully use unrestricted path information.
In addition, to efficiently capture heterogeneous features in entity
neighborhoods, a relation-aware heterogeneous graph transformer is designed to
model the relation and path structures of KGs. Extensive experiments on three
well-known datasets show RPR-RHGT significantly outperforms 11 state-of-the-art
methods, exceeding the best performing baseline up to 8.62% on Hits@1. We also
show its better performance than the baselines on different ratios of training
set, and harder datasets.",2205.08806v2,https://arxiv.org/pdf/2205.08806v2
"Entity Alignment For Knowledge Graphs: Progress, Challenges, and
  Empirical Studies","Deepak Chaurasiya, Anil Surisetty, Nitish Kumar, Alok Singh, Vikrant Dey, Aakarsh Malhotra, Gaurav Dhama, Ankur Arora","Entity Alignment (EA) identifies entities across databases that refer to the
same entity. Knowledge graph-based embedding methods have recently dominated EA
techniques. Such methods map entities to a low-dimension space and align them
based on their similarities. With the corpus of EA methodologies growing
rapidly, this paper presents a comprehensive analysis of various existing EA
methods, elaborating their applications and limitations. Further, we
distinguish the methods based on their underlying algorithms and the
information they incorporate to learn entity representations. Based on
challenges in industrial datasets, we bring forward $4$ research questions
(RQs). These RQs empirically analyse the algorithms from the perspective of
\textit{Hubness, Degree distribution, Non-isomorphic neighbourhood,} and
\textit{Name bias}. For Hubness, where one entity turns up as the nearest
neighbour of many other entities, we define an $h$-score to quantify its effect
on the performance of various algorithms. Additionally, we try to level the
playing field for algorithms that rely primarily on name-bias existing in the
benchmarking open-source datasets by creating a low name bias dataset. We
further create an open-source repository for $14$ embedding-based EA methods
and present the analysis for invoking further research motivations in the field
of EA.",2205.08777v1,https://arxiv.org/pdf/2205.08777v1
TTAPS: Test-Time Adaption by Aligning Prototypes using Self-Supervision,"Alexander Bartler, Florian Bender, Felix Wiewel, Bin Yang","Nowadays, deep neural networks outperform humans in many tasks. However, if
the input distribution drifts away from the one used in training, their
performance drops significantly. Recently published research has shown that
adapting the model parameters to the test sample can mitigate this performance
degradation. In this paper, we therefore propose a novel modification of the
self-supervised training algorithm SwAV that adds the ability to adapt to
single test samples. Using the provided prototypes of SwAV and our derived
test-time loss, we align the representation of unseen test samples with the
self-supervised learned prototypes. We show the success of our method on the
common benchmark dataset CIFAR10-C.",2205.08731v1,https://arxiv.org/pdf/2205.08731v1
Measuring Alignment Bias in Neural Seq2Seq Semantic Parsers,"Davide Locatelli, Ariadna Quattoni","Prior to deep learning the semantic parsing community has been interested in
understanding and modeling the range of possible word alignments between
natural language sentences and their corresponding meaning representations.
Sequence-to-sequence models changed the research landscape suggesting that we
no longer need to worry about alignments since they can be learned
automatically by means of an attention mechanism. More recently, researchers
have started to question such premise. In this work we investigate whether
seq2seq models can handle both simple and complex alignments. To answer this
question we augment the popular Geo semantic parsing dataset with alignment
annotations and create Geo-Aligned. We then study the performance of standard
seq2seq models on the examples that can be aligned monotonically versus
examples that require more complex alignments. Our empirical study shows that
performance is significantly better over monotonic alignments.",2205.08288v2,https://arxiv.org/pdf/2205.08288v2
"SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level Cross-Lingual
  Speech Representation","Sameer Khurana, Antoine Laurent, James Glass","We propose the SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level
Cross-Lingual Speech Representation learning framework. Unlike previous works
on speech representation learning, which learns multilingual contextual speech
embedding at the resolution of an acoustic frame (10-20ms), this work focuses
on learning multimodal (speech-text) multilingual speech embedding at the
resolution of a sentence (5-10s) such that the embedding vector space is
semantically aligned across different languages. We combine state-of-the-art
multilingual acoustic frame-level speech representation learning model XLS-R
with the Language Agnostic BERT Sentence Embedding (LaBSE) model to create an
utterance-level multimodal multilingual speech encoder SAMU-XLSR. Although we
train SAMU-XLSR with only multilingual transcribed speech data, cross-lingual
speech-text and speech-speech associations emerge in its learned representation
space. To substantiate our claims, we use SAMU-XLSR speech encoder in
combination with a pre-trained LaBSE text sentence encoder for cross-lingual
speech-to-text translation retrieval, and SAMU-XLSR alone for cross-lingual
speech-to-speech translation retrieval. We highlight these applications by
performing several cross-lingual text and speech translation retrieval tasks
across several datasets.",2205.08180v1,https://arxiv.org/pdf/2205.08180v1
Aligning Robot Representations with Humans,"Andreea Bobu, Andi Peng","As robots are increasingly deployed in real-world scenarios, a key question
is how to best transfer knowledge learned in one environment to another, where
shifting constraints and human preferences render adaptation challenging. A
central challenge remains that often, it is difficult (perhaps even impossible)
to capture the full complexity of the deployment environment, and therefore the
desired tasks, at training time. Consequently, the representation, or
abstraction, of the tasks the human hopes for the robot to perform in one
environment may be misaligned with the representation of the tasks that the
robot has learned in another. We postulate that because humans will be the
ultimate evaluator of system success in the world, they are best suited to
communicating the aspects of the tasks that matter to the robot. Our key
insight is that effective learning from human input requires first explicitly
learning good intermediate representations and then using those representations
for solving downstream tasks. We highlight three areas where we can use this
approach to build interactive systems and offer future directions of work to
better create advanced collaborative robots.",2205.07882v1,https://arxiv.org/pdf/2205.07882v1
Aligned with Whom? Direct and social goals for AI systems,"Anton Korinek, Avital Balwit","As artificial intelligence (AI) becomes more powerful and widespread, the AI
alignment problem - how to ensure that AI systems pursue the goals that we want
them to pursue - has garnered growing attention. This article distinguishes two
types of alignment problems depending on whose goals we consider, and analyzes
the different solutions necessitated by each. The direct alignment problem
considers whether an AI system accomplishes the goals of the entity operating
it. In contrast, the social alignment problem considers the effects of an AI
system on larger groups or on society more broadly. In particular, it also
considers whether the system imposes externalities on others. Whereas solutions
to the direct alignment problem center around more robust implementation,
social alignment problems typically arise because of conflicts between
individual and group-level goals, elevating the importance of AI governance to
mediate such conflicts. Addressing the social alignment problem requires both
enforcing existing norms on their developers and operators and designing new
norms that apply directly to AI systems.",2205.04279v1,https://arxiv.org/pdf/2205.04279v1
"PGADA: Perturbation-Guided Adversarial Alignment for Few-shot Learning
  Under the Support-Query Shift","Siyang Jiang, Wei Ding, Hsi-Wen Chen, Ming-Syan Chen","Few-shot learning methods aim to embed the data to a low-dimensional
embedding space and then classify the unseen query data to the seen support
set. While these works assume that the support set and the query set lie in the
same embedding space, a distribution shift usually occurs between the support
set and the query set, i.e., the Support-Query Shift, in the real world. Though
optimal transportation has shown convincing results in aligning different
distributions, we find that the small perturbations in the images would
significantly misguide the optimal transportation and thus degrade the model
performance. To relieve the misalignment, we first propose a novel adversarial
data augmentation method, namely Perturbation-Guided Adversarial Alignment
(PGADA), which generates the hard examples in a self-supervised manner. In
addition, we introduce Regularized Optimal Transportation to derive a smooth
optimal transportation plan. Extensive experiments on three benchmark datasets
manifest that our framework significantly outperforms the eleven
state-of-the-art methods on three datasets.",2205.03817v1,https://arxiv.org/pdf/2205.03817v1
"Time-Series Domain Adaptation via Sparse Associative Structure
  Alignment: Learning Invariance and Variance","Zijian Li, Ruichu Cai, Jiawei Chen, Yuguan Yan, Wei Chen, Keli Zhang, Junjian Ye","Domain adaptation on time-series data is often encountered in the industry
but received limited attention in academia. Most of the existing domain
adaptation methods for time-series data borrow the ideas from the existing
methods for non-time series data to extract the domain-invariant
representation. However, two peculiar difficulties to time-series data have not
been solved. 1) It is not a trivial task to model the domain-invariant and
complex dependence among different timestamps. 2) The domain-variant
information is important but how to leverage them is almost underexploited.
Fortunately, the stableness of causal structures among different domains
inspires us to explore the structures behind the time-series data. Based on
this inspiration, we investigate the domain-invariant unweighted sparse
associative structures and the domain-variant strengths of the structures. To
achieve this, we propose Sparse Associative structure alignment by learning
Invariance and Variance (SASA-IV in short), a model that simultaneously aligns
the invariant unweighted spare associative structures and considers the variant
information for time-series unsupervised domain adaptation. Technologically, we
extract the domain-invariant unweighted sparse associative structures with a
unidirectional alignment restriction and embed the domain-variant strengths via
a well-designed autoregressive module. Experimental results not only testify
that our model yields state-of-the-art performance on three real-world datasets
but also provide some insightful discoveries on knowledge transfer.",2205.03554v1,https://arxiv.org/pdf/2205.03554v1
"Alignahead: Online Cross-Layer Knowledge Extraction on Graph Neural
  Networks","Jiongyu Guo, Defang Chen, Can Wang","Existing knowledge distillation methods on graph neural networks (GNNs) are
almost offline, where the student model extracts knowledge from a powerful
teacher model to improve its performance. However, a pre-trained teacher model
is not always accessible due to training cost, privacy, etc. In this paper, we
propose a novel online knowledge distillation framework to resolve this
problem. Specifically, each student GNN model learns the extracted local
structure from another simultaneously trained counterpart in an alternating
training procedure. We further develop a cross-layer distillation strategy by
aligning ahead one student layer with the layer in different depth of another
student model, which theoretically makes the structure information spread over
all layers. Experimental results on five datasets including PPI,
Coauthor-CS/Physics and Amazon-Computer/Photo demonstrate that the student
performance is consistently boosted in our collaborative training framework
without the supervision of a pre-trained teacher model. In addition, we also
find that our alignahead technique can accelerate the model convergence speed
and its effectiveness can be generally improved by increasing the student
numbers in training. Code is available:
https://github.com/GuoJY-eatsTG/Alignahead",2205.02468v1,https://arxiv.org/pdf/2205.02468v1
Dangling-Aware Entity Alignment with Mixed High-Order Proximities,"Juncheng Liu, Zequn Sun, Bryan Hooi, Yiwei Wang, Dayiheng Liu, Baosong Yang, Xiaokui Xiao, Muhao Chen","We study dangling-aware entity alignment in knowledge graphs (KGs), which is
an underexplored but important problem. As different KGs are naturally
constructed by different sets of entities, a KG commonly contains some dangling
entities that cannot find counterparts in other KGs. Therefore, dangling-aware
entity alignment is more realistic than the conventional entity alignment where
prior studies simply ignore dangling entities. We propose a framework using
mixed high-order proximities on dangling-aware entity alignment. Our framework
utilizes both the local high-order proximity in a nearest neighbor subgraph and
the global high-order proximity in an embedding space for both dangling
detection and entity alignment. Extensive experiments with two evaluation
settings shows that our framework more precisely detects dangling entities, and
better aligns matchable entities. Further investigations demonstrate that our
framework can mitigate the hubness problem on dangling-aware entity alignment.",2205.02406v1,https://arxiv.org/pdf/2205.02406v1
Aligning to Social Norms and Values in Interactive Narratives,"Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, Yejin Choi","We focus on creating agents that act in alignment with socially beneficial
norms and values in interactive narratives or text-based games -- environments
wherein an agent perceives and interacts with a world through natural language.
Such interactive agents are often trained via reinforcement learning to
optimize task performance, even when such rewards may lead to agent behaviors
that violate societal norms -- causing harm either to the agent itself or other
entities in the environment. Social value alignment refers to creating agents
whose behaviors conform to expected moral and social norms for a given context
and group of people -- in our case, it means agents that behave in a manner
that is less harmful and more beneficial for themselves and others.
  We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25
annotated interactive narratives containing thousands of morally salient
scenarios covering everything from theft and bodily harm to altruism. We
introduce the GALAD (Game-value ALignment through Action Distillation) agent
that uses the social commonsense knowledge present in specially trained
language models to contextually restrict its action space to only those actions
that are aligned with socially beneficial values. An experimental study shows
that the GALAD agent makes decisions efficiently enough to improve
state-of-the-art task performance by 4% while reducing the frequency of
socially harmful behaviors by 25% compared to strong contemporary value
alignment approaches.",2205.01975v2,https://arxiv.org/pdf/2205.01975v2
"PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model
  Pretraining","Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Rongrong Ji, Chunhua Shen","Large-scale vision-language pre-training has achieved promising results on
downstream tasks. Existing methods highly rely on the assumption that the
image-text pairs crawled from the Internet are in perfect one-to-one
correspondence. However, in real scenarios, this assumption can be difficult to
hold: the text description, obtained by crawling the affiliated metadata of the
image, often suffers from the semantic mismatch and the mutual compatibility.
To address these issues, we introduce PyramidCLIP, which constructs an input
pyramid with different semantic levels for each modality, and aligns visual
elements and linguistic elements in the form of hierarchy via peer-level
semantics alignment and cross-level relation alignment. Furthermore, we soften
the loss of negative samples (unpaired samples) so as to weaken the strict
constraint during the pre-training stage, thus mitigating the risk of forcing
the model to distinguish compatible negative pairs. Experiments on five
downstream tasks demonstrate the effectiveness of the proposed PyramidCLIP. In
particular, with the same amount of 15 million pre-training image-text pairs,
PyramidCLIP exceeds CLIP on ImageNet zero-shot classification top-1 accuracy by
10.6%/13.2%/10.0% with ResNet50/ViT-B32/ViT-B16 based image encoder
respectively. When scaling to larger datasets, PyramidCLIP achieves the
state-of-the-art results on several downstream tasks. In particular, the
results of PyramidCLIP-ResNet50 trained on 143M image-text pairs surpass that
of CLIP using 400M data on ImageNet zero-shot classification task,
significantly improving the data efficiency of CLIP.",2204.14095v2,https://arxiv.org/pdf/2204.14095v2
"RoSA: A Robust Self-Aligned Framework for Node-Node Graph Contrastive
  Learning","Yun Zhu, Jianhao Guo, Fei Wu, Siliang Tang","Graph contrastive learning has gained significant progress recently. However,
existing works have rarely explored non-aligned node-node contrasting. In this
paper, we propose a novel graph contrastive learning method named RoSA that
focuses on utilizing non-aligned augmented views for node-level representation
learning. First, we leverage the earth mover's distance to model the minimum
effort to transform the distribution of one view to the other as our
contrastive objective, which does not require alignment between views. Then we
introduce adversarial training as an auxiliary method to increase sampling
diversity and enhance the robustness of our model. Experimental results show
that RoSA outperforms a series of graph contrastive learning frameworks on
homophilous, non-homophilous and dynamic graphs, which validates the
effectiveness of our work. To the best of our awareness, RoSA is the first work
focuses on the non-aligned node-node graph contrastive learning problem. Our
codes are available at:
\href{https://github.com/ZhuYun97/RoSA}{\texttt{https://github.com/ZhuYun97/RoSA}}",2204.13846v1,https://arxiv.org/pdf/2204.13846v1
"Regotron: Regularizing the Tacotron2 architecture via monotonic
  alignment loss","Efthymios Georgiou, Kosmas Kritsis, Georgios Paraskevopoulos, Athanasios Katsamanis, Vassilis Katsouros, Alexandros Potamianos","Recent deep learning Text-to-Speech (TTS) systems have achieved impressive
performance by generating speech close to human parity. However, they suffer
from training stability issues as well as incorrect alignment of the
intermediate acoustic representation with the input text sequence. In this
work, we introduce Regotron, a regularized version of Tacotron2 which aims to
alleviate the training issues and at the same time produce monotonic
alignments. Our method augments the vanilla Tacotron2 objective function with
an additional term, which penalizes non-monotonic alignments in the
location-sensitive attention mechanism. By properly adjusting this
regularization term we show that the loss curves become smoother, and at the
same time Regotron consistently produces monotonic alignments in unseen
examples even at an early stage (13\% of the total number of epochs) of its
training process, whereas the fully converged Tacotron2 fails to do so.
Moreover, our proposed regularization method has no additional computational
overhead, while reducing common TTS mistakes and achieving slighlty improved
speech naturalness according to subjective mean opinion scores (MOS) collected
from 50 evaluators.",2204.13437v2,https://arxiv.org/pdf/2204.13437v2
"Covariance-aware Feature Alignment with Pre-computed Source Statistics
  for Test-time Adaptation to Multiple Image Corruptions","Kazuki Adachi, Shin'ya Yamaguchi, Atsutoshi Kumagai","Real-world image recognition systems often face corrupted input images, which
cause distribution shifts and degrade the performance of models. These systems
often use a single prediction model in a central server and process images sent
from various environments, such as cameras distributed in cities or cars. Such
single models face images corrupted in heterogeneous ways in test time. Thus,
they require to instantly adapt to the multiple corruptions during testing
rather than being re-trained at a high cost. Test-time adaptation (TTA), which
aims to adapt models without accessing the training dataset, is one of the
settings that can address this problem. Existing TTA methods indeed work well
on a single corruption. However, the adaptation ability is limited when
multiple types of corruption occur, which is more realistic. We hypothesize
this is because the distribution shift is more complicated, and the adaptation
becomes more difficult in case of multiple corruptions. In fact, we
experimentally found that a larger distribution gap remains after TTA. To
address the distribution gap during testing, we propose a novel TTA method
named Covariance-Aware Feature alignment (CAFe). We empirically show that CAFe
outperforms prior TTA methods on image corruptions, including multiple types of
corruptions.",2204.13263v2,https://arxiv.org/pdf/2204.13263v2
"Rethinking Multi-Modal Alignment in Video Question Answering from
  Feature and Sample Perspectives","Shaoning Xiao, Long Chen, Kaifeng Gao, Zhao Wang, Yi Yang, Zhimeng Zhang, Jun Xiao","Reasoning about causal and temporal event relations in videos is a new
destination of Video Question Answering (VideoQA).The major stumbling block to
achieve this purpose is the semantic gap between language and video since they
are at different levels of abstraction. Existing efforts mainly focus on
designing sophisticated architectures while utilizing frame- or object-level
visual representations. In this paper, we reconsider the multi-modal alignment
problem in VideoQA from feature and sample perspectives to achieve better
performance. From the view of feature,we break down the video into trajectories
and first leverage trajectory feature in VideoQA to enhance the alignment
between two modalities. Moreover, we adopt a heterogeneous graph architecture
and design a hierarchical framework to align both trajectory-level and
frame-level visual feature with language feature. In addition, we found that
VideoQA models are largely dependent on language priors and always neglect
visual-language interactions. Thus, two effective yet portable training
augmentation strategies are designed to strengthen the cross-modal
correspondence ability of our model from the view of sample. Extensive results
show that our method outperforms all the state-of-the-art models on the
challenging NExT-QA benchmark, which demonstrates the effectiveness of the
proposed method.",2204.11544v2,https://arxiv.org/pdf/2204.11544v2
"Joint Feature Distribution Alignment Learning for NIR-VIS and VIS-VIS
  Face Recognition","Takaya Miyamoto, Hiroshi Hashimoto, Akihiro Hayasaka, Akinori F. Ebihara, Hitoshi Imaoka","Face recognition for visible light (VIS) images achieve high accuracy thanks
to the recent development of deep learning. However, heterogeneous face
recognition (HFR), which is a face matching in different domains, is still a
difficult task due to the domain discrepancy and lack of large HFR dataset.
Several methods have attempted to reduce the domain discrepancy by means of
fine-tuning, which causes significant degradation of the performance in the VIS
domain because it loses the highly discriminative VIS representation. To
overcome this problem, we propose joint feature distribution alignment learning
(JFDAL) which is a joint learning approach utilizing knowledge distillation. It
enables us to achieve high HFR performance with retaining the original
performance for the VIS domain. Extensive experiments demonstrate that our
proposed method delivers statistically significantly better performances
compared with the conventional fine-tuning approach on a public HFR dataset
Oulu-CASIA NIR&VIS and popular verification datasets in VIS domain such as FLW,
CFP, AgeDB. Furthermore, comparative experiments with existing state-of-the-art
HFR methods show that our method achieves a comparable HFR performance on the
Oulu-CASIA NIR&VIS dataset with less degradation of VIS performance.",2204.11434v1,https://arxiv.org/pdf/2204.11434v1
A Multi-level Alignment Training Scheme for Video-and-Language Grounding,"Yubo Zhang, Feiyang Niu, Qing Ping, Govind Thattai","To solve video-and-language grounding tasks, the key is for the network to
understand the connection between the two modalities. For a pair of video and
language description, their semantic relation is reflected by their encodings'
similarity. A good multi-modality encoder should be able to well capture both
inputs' semantics and encode them in the shared feature space where embedding
distance gets properly translated into their semantic similarity. In this work,
we focused on this semantic connection between video and language, and
developed a multi-level alignment training scheme to directly shape the
encoding process. Global and segment levels of video-language alignment pairs
were designed, based on the information similarity ranging from high-level
context to fine-grained semantics. The contrastive loss was used to contrast
the encodings' similarities between the positive and negative alignment pairs,
and to ensure the network is trained in such a way that similar information is
encoded closely in the shared feature space while information of different
semantics is kept apart. Our multi-level alignment training can be applied to
various video-and-language grounding tasks. Together with the task-specific
training loss, our framework achieved comparable performance to previous
state-of-the-arts on multiple video QA and retrieval datasets.",2204.10938v3,https://arxiv.org/pdf/2204.10938v3
"Efficient Progressive High Dynamic Range Image Restoration via Attention
  and Alignment Network","Gaocheng Yu, Jin Zhang, Zhe Ma, Hongbin Wang","HDR is an important part of computational photography technology. In this
paper, we propose a lightweight neural network called Efficient
Attention-and-alignment-guided Progressive Network (EAPNet) for the challenge
NTIRE 2022 HDR Track 1 and Track 2. We introduce a multi-dimensional
lightweight encoding module to extract features. Besides, we propose
Progressive Dilated U-shape Block (PDUB) that can be a progressive
plug-and-play module for dynamically tuning MAccs and PSNR. Finally, we use
fast and low-power feature-align module to deal with misalignment problem in
place of the time-consuming Deformable Convolutional Network (DCN). The
experiments show that our method achieves about 20 times compression on MAccs
with better mu-PSNR and PSNR compared to the state-of-the-art method. We got
the second place of both two tracks during the testing phase. Figure1. shows
the visualized result of NTIRE 2022 HDR challenge.",2204.09213v1,https://arxiv.org/pdf/2204.09213v1
Streaming Align-Refine for Non-autoregressive Deliberation,"Weiran Wang, Ke Hu, Tara N. Sainath","We propose a streaming non-autoregressive (non-AR) decoding algorithm to
deliberate the hypothesis alignment of a streaming RNN-T model. Our algorithm
facilitates a simple greedy decoding procedure, and at the same time is capable
of producing the decoding result at each frame with limited right context, thus
enjoying both high efficiency and low latency. These advantages are achieved by
converting the offline Align-Refine algorithm to be streaming-compatible, with
a novel transformer decoder architecture that performs local self-attentions
for both text and audio, and a time-aligned cross-attention at each layer.
Furthermore, we perform discriminative training of our model with the minimum
word error rate (MWER) criterion, which has not been done in the non-AR
decoding literature. Experiments on voice search datasets and Librispeech show
that with reasonable right context, our streaming model performs as well as the
offline counterpart, and discriminative training leads to further WER gain when
the first-pass model has small capacity.",2204.07556v1,https://arxiv.org/pdf/2204.07556v1
"Generative power of a protein language model trained on multiple
  sequence alignments","Damiano Sgarbossa, Umberto Lupo, Anne-Florence Bitbol","Computational models starting from large ensembles of evolutionarily related
protein sequences capture a representation of protein families and learn
constraints associated to protein structure and function. They thus open the
possibility for generating novel sequences belonging to protein families.
Protein language models trained on multiple sequence alignments, such as MSA
Transformer, are highly attractive candidates to this end. We propose and test
an iterative method that directly employs the masked language modeling
objective to generate sequences using MSA Transformer. We demonstrate that the
resulting sequences score as well as natural sequences, for homology,
coevolution and structure-based measures. For large protein families, our
synthetic sequences have similar or better properties compared to sequences
generated by Potts models, including experimentally-validated ones. Moreover,
for small protein families, our generation method based on MSA Transformer
outperforms Potts models. Our method also more accurately reproduces the
higher-order statistics and the distribution of sequences in sequence space of
natural data than Potts models. MSA Transformer is thus a strong candidate for
protein sequence generation and protein design.",2204.07110v2,https://arxiv.org/pdf/2204.07110v2
"Karaoker: Alignment-free singing voice synthesis with speech training
  data","Panos Kakoulidis, Nikolaos Ellinas, Georgios Vamvoukakis, Konstantinos Markopoulos, June Sig Sung, Gunu Jho, Pirros Tsiakoulis, Aimilios Chalamandaris","Existing singing voice synthesis models (SVS) are usually trained on singing
data and depend on either error-prone time-alignment and duration features or
explicit music score information. In this paper, we propose Karaoker, a
multispeaker Tacotron-based model conditioned on voice characteristic features
that is trained exclusively on spoken data without requiring time-alignments.
Karaoker synthesizes singing voice and transfers style following a
multi-dimensional template extracted from a source waveform of an unseen
singer/speaker. The model is jointly conditioned with a single deep
convolutional encoder on continuous data including pitch, intensity,
harmonicity, formants, cepstral peak prominence and octaves. We extend the
text-to-speech training objective with feature reconstruction, classification
and speaker identification tasks that guide the model to an accurate result. In
addition to multitasking, we also employ a Wasserstein GAN training scheme as
well as new losses on the acoustic model's output to further refine the quality
of the model.",2204.04127v2,https://arxiv.org/pdf/2204.04127v2
"DAD-3DHeads: A Large-scale Dense, Accurate and Diverse Dataset for 3D
  Head Alignment from a Single Image","Tetiana Martyniuk, Orest Kupyn, Yana Kurlyak, Igor Krashenyi, Jiři Matas, Viktoriia Sharmanska","We present DAD-3DHeads, a dense and diverse large-scale dataset, and a robust
model for 3D Dense Head Alignment in the wild. It contains annotations of over
3.5K landmarks that accurately represent 3D head shape compared to the
ground-truth scans. The data-driven model, DAD-3DNet, trained on our dataset,
learns shape, expression, and pose parameters, and performs 3D reconstruction
of a FLAME mesh. The model also incorporates a landmark prediction branch to
take advantage of rich supervision and co-training of multiple related tasks.
Experimentally, DAD-3DNet outperforms or is comparable to the state-of-the-art
models in (i) 3D Head Pose Estimation on AFLW2000-3D and BIWI, (ii) 3D Face
Shape Reconstruction on NoW and Feng, and (iii) 3D Dense Head Alignment and 3D
Landmarks Estimation on DAD-3DHeads dataset. Finally, the diversity of
DAD-3DHeads in camera angles, facial expressions, and occlusions enables a
benchmark to study in-the-wild generalization and robustness to distribution
shifts. The dataset webpage is https://p.farm/research/dad-3dheads.",2204.03688v2,https://arxiv.org/pdf/2204.03688v2
Temporal Alignment for History Representation in Reinforcement Learning,"Aleksandr Ermolov, Enver Sangineto, Nicu Sebe","Environments in Reinforcement Learning are usually only partially observable.
To address this problem, a possible solution is to provide the agent with
information about the past. However, providing complete observations of
numerous steps can be excessive. Inspired by human memory, we propose to
represent history with only important changes in the environment and, in our
approach, to obtain automatically this representation using self-supervision.
Our method (TempAl) aligns temporally-close frames, revealing a general, slowly
varying state of the environment. This procedure is based on contrastive loss,
which pulls embeddings of nearby observations to each other while pushing away
other samples from the batch. It can be interpreted as a metric that captures
the temporal relations of observations. We propose to combine both common
instantaneous and our history representation and we evaluate TempAl on all
available Atari games from the Arcade Learning Environment. TempAl surpasses
the instantaneous-only baseline in 35 environments out of 49. The source code
of the method and of all the experiments is available at
https://github.com/htdt/tempal.",2204.03525v1,https://arxiv.org/pdf/2204.03525v1
AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis,"Zhiqin Chen, Kangxue Yin, Sanja Fidler","In this paper, we address the problem of texture representation for 3D shapes
for the challenging and underexplored tasks of texture transfer and synthesis.
Previous works either apply spherical texture maps which may lead to large
distortions, or use continuous texture fields that yield smooth outputs lacking
details. We argue that the traditional way of representing textures with images
and linking them to a 3D mesh via UV mapping is more desirable, since
synthesizing 2D images is a well-studied problem. We propose AUV-Net which
learns to embed 3D surfaces into a 2D aligned UV space, by mapping the
corresponding semantic parts of different 3D shapes to the same location in the
UV space. As a result, textures are aligned across objects, and can thus be
easily synthesized by generative models of images. Texture alignment is learned
in an unsupervised manner by a simple yet effective texture alignment module,
taking inspiration from traditional works on linear subspace learning. The
learned UV mapping and aligned texture representations enable a variety of
applications including texture transfer, texture synthesis, and textured single
view 3D reconstruction. We conduct experiments on multiple datasets to
demonstrate the effectiveness of our method. Project page:
https://nv-tlabs.github.io/AUV-NET.",2204.03105v1,https://arxiv.org/pdf/2204.03105v1
Prosodic Alignment for off-screen automatic dubbing,"Yogesh Virkar, Marcello Federico, Robert Enyedi, Roberto Barra-Chicote","The goal of automatic dubbing is to perform speech-to-speech translation
while achieving audiovisual coherence. This entails isochrony, i.e.,
translating the original speech by also matching its prosodic structure into
phrases and pauses, especially when the speaker's mouth is visible. In previous
work, we introduced a prosodic alignment model to address isochrone or
on-screen dubbing. In this work, we extend the prosodic alignment model to also
address off-screen dubbing that requires less stringent synchronization
constraints. We conduct experiments on four dubbing directions - English to
French, Italian, German and Spanish - on a publicly available collection of TED
Talks and on publicly available YouTube videos. Empirical results show that
compared to our previous work the extended prosodic alignment model provides
significantly better subjective viewing experience on videos in which on-screen
and off-screen automatic dubbing is applied for sentences with speakers mouth
visible and not visible, respectively.",2204.02530v1,https://arxiv.org/pdf/2204.02530v1
"Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal
  Grounding","Ziyue Wu, Junyu Gao, Shucheng Huang, Changsheng Xu","Grounding temporal video segments described in natural language queries
effectively and efficiently is a crucial capability needed in
vision-and-language fields. In this paper, we deal with the fast video temporal
grounding (FVTG) task, aiming at localizing the target segment with high speed
and favorable accuracy. Most existing approaches adopt elaborately designed
cross-modal interaction modules to improve the grounding performance, which
suffer from the test-time bottleneck. Although several common space-based
methods enjoy the high-speed merit during inference, they can hardly capture
the comprehensive and explicit relations between visual and textual modalities.
In this paper, to tackle the dilemma of speed-accuracy tradeoff, we propose a
commonsense-aware cross-modal alignment (CCA) framework, which incorporates
commonsense-guided visual and text representations into a complementary common
space for fast video temporal grounding. Specifically, the commonsense concepts
are explored and exploited by extracting the structural semantic information
from a language corpus. Then, a commonsense-aware interaction module is
designed to obtain bridged visual and text features by utilizing the learned
commonsense concepts. Finally, to maintain the original semantic information of
textual queries, a cross-modal complementary common space is optimized to
obtain matching scores for performing FVTG. Extensive results on two
challenging benchmarks show that our CCA method performs favorably against
state-of-the-arts while running at high speed. Our code is available at
https://github.com/ZiyueWu59/CCA.",2204.01450v2,https://arxiv.org/pdf/2204.01450v2
Aligned Weight Regularizers for Pruning Pretrained Neural Networks,"James O' Neill, Sourav Dutta, Haytham Assem","While various avenues of research have been explored for iterative pruning,
little is known what effect pruning has on zero-shot test performance and its
potential implications on the choice of pruning criteria. This pruning setup is
particularly important for cross-lingual models that implicitly learn alignment
between language representations during pretraining, which if distorted via
pruning, not only leads to poorer performance on language data used for
retraining but also on zero-shot languages that are evaluated.
  In this work, we show that there is a clear performance discrepancy in
magnitude-based pruning when comparing standard supervised learning to the
zero-shot setting. From this finding, we propose two weight regularizers that
aim to maximize the alignment between units of pruned and unpruned networks to
mitigate alignment distortion in pruned cross-lingual models and perform well
for both non zero-shot and zero-shot settings.
  We provide experimental results on cross-lingual tasks for the zero-shot
setting using XLM-RoBERTa$_{\mathrm{Base}}$, where we also find that pruning
has varying degrees of representational degradation depending on the language
corresponding to the zero-shot test set. This is also the first study that
focuses on cross-lingual language model compression.",2204.01385v2,https://arxiv.org/pdf/2204.01385v2
Aligning Silhouette Topology for Self-Adaptive 3D Human Pose Recovery,"Mugalodi Rakesh, Jogendra Nath Kundu, Varun Jampani, R. Venkatesh Babu","Articulation-centric 2D/3D pose supervision forms the core training objective
in most existing 3D human pose estimation techniques. Except for synthetic
source environments, acquiring such rich supervision for each real target
domain at deployment is highly inconvenient. However, we realize that standard
foreground silhouette estimation techniques (on static camera feeds) remain
unaffected by domain-shifts. Motivated by this, we propose a novel target
adaptation framework that relies only on silhouette supervision to adapt a
source-trained model-based regressor. However, in the absence of any auxiliary
cue (multi-view, depth, or 2D pose), an isolated silhouette loss fails to
provide a reliable pose-specific gradient and requires to be employed in tandem
with a topology-centric loss. To this end, we develop a series of
convolution-friendly spatial transformations in order to disentangle a
topological-skeleton representation from the raw silhouette. Such a design
paves the way to devise a Chamfer-inspired spatial topological-alignment loss
via distance field computation, while effectively avoiding any gradient
hindering spatial-to-pointset mapping. Experimental results demonstrate our
superiority against prior-arts in self-adapting a source trained model to
diverse unlabeled target domains, such as a) in-the-wild datasets, b)
low-resolution image domains, and c) adversarially perturbed image domains (via
UAP).",2204.01276v1,https://arxiv.org/pdf/2204.01276v1
An Analysis of Semantically-Aligned Speech-Text Embeddings,"Muhammad Huzaifah, Ivan Kukanov","Embeddings play an important role in end-to-end solutions for multi-modal
language processing problems. Although there has been some effort to understand
the properties of single-modality embedding spaces, particularly that of text,
their cross-modal counterparts are less understood. In this work, we study some
intrinsic properties of a joint speech-text embedding space, constructed by
minimizing the distance between paired utterance and transcription inputs in a
teacher-student model setup, that are informative for several prominent use
cases. We found that incorporating automatic speech recognition through both
pretraining and multitask scenarios aid semantic alignment significantly,
resulting in more tightly coupled embeddings. To analyse cross-modal embeddings
we utilise a quantitative retrieval accuracy metric for semantic alignment,
zero-shot classification for generalisability, and probing of the encoders to
observe the extent of knowledge transfer from one modality to another.",2204.01235v2,https://arxiv.org/pdf/2204.01235v2
"Adversarially robust segmentation models learn perceptually-aligned
  gradients",Pedro Sandoval-Segura,"The effects of adversarial training on semantic segmentation networks has not
been thoroughly explored. While previous work has shown that
adversarially-trained image classifiers can be used to perform image synthesis,
we have yet to understand how best to leverage an adversarially-trained
segmentation network to do the same. Using a simple optimizer, we demonstrate
that adversarially-trained semantic segmentation networks can be used to
perform image inpainting and generation. Our experiments demonstrate that
adversarially-trained segmentation networks are more robust and indeed exhibit
perceptually-aligned gradients which help in producing plausible image
inpaintings. We seek to place additional weight behind the hypothesis that
adversarially robust models exhibit gradients that are more
perceptually-aligned with human vision. Through image synthesis, we argue that
perceptually-aligned gradients promote a better understanding of a neural
network's learned representations and aid in making neural networks more
interpretable.",2204.01099v1,https://arxiv.org/pdf/2204.01099v1
"Accurate Online Posterior Alignments for Principled
  Lexically-Constrained Decoding","Soumya Chatterjee, Sunita Sarawagi, Preethi Jyothi","Online alignment in machine translation refers to the task of aligning a
target word to a source word when the target sequence has only been partially
decoded. Good online alignments facilitate important applications such as
lexically constrained translation where user-defined dictionaries are used to
inject lexical constraints into the translation model. We propose a novel
posterior alignment technique that is truly online in its execution and
superior in terms of alignment error rates compared to existing methods. Our
proposed inference technique jointly considers alignment and token
probabilities in a principled manner and can be seamlessly integrated within
existing constrained beam-search decoding algorithms. On five language pairs,
including two distant language pairs, we achieve consistent drop in alignment
error rates. When deployed on seven lexically constrained translation tasks, we
achieve significant improvements in BLEU specifically around the constrained
positions.",2204.00871v1,https://arxiv.org/pdf/2204.00871v1
"Feature Structure Distillation with Centered Kernel Alignment in BERT
  Transferring","Hee-Jun Jung, Doyeon Kim, Seung-Hoon Na, Kangil Kim","Knowledge distillation is an approach to transfer information on
representations from a teacher to a student by reducing their difference. A
challenge of this approach is to reduce the flexibility of the student's
representations inducing inaccurate learning of the teacher's knowledge. To
resolve it in transferring, we investigate distillation of structures of
representations specified to three types: intra-feature, local inter-feature,
global inter-feature structures. To transfer them, we introduce feature
structure distillation methods based on the Centered Kernel Alignment, which
assigns a consistent value to similar features structures and reveals more
informative relations. In particular, a memory-augmented transfer method with
clustering is implemented for the global structures. The methods are
empirically analyzed on the nine tasks for language understanding of the GLUE
dataset with Bidirectional Encoder Representations from Transformers (BERT),
which is a representative neural language model. In the results, the proposed
methods effectively transfer the three types of structures and improve
performance compared to state-of-the-art distillation methods. Indeed, the code
for the methods is available in https://github.com/maroo-sky/FSD.",2204.08922v3,https://arxiv.org/pdf/2204.08922v3
"Protein language models trained on multiple sequence alignments learn
  phylogenetic relationships","Umberto Lupo, Damiano Sgarbossa, Anne-Florence Bitbol","Self-supervised neural language models with attention have recently been
applied to biological sequence data, advancing structure, function and
mutational effect prediction. Some protein language models, including MSA
Transformer and AlphaFold's EvoFormer, take multiple sequence alignments (MSAs)
of evolutionarily related proteins as inputs. Simple combinations of MSA
Transformer's row attentions have led to state-of-the-art unsupervised
structural contact prediction. We demonstrate that similarly simple, and
universal, combinations of MSA Transformer's column attentions strongly
correlate with Hamming distances between sequences in MSAs. Therefore,
MSA-based language models encode detailed phylogenetic relationships. We
further show that these models can separate coevolutionary signals encoding
functional and structural constraints from phylogenetic correlations reflecting
historical contingency. To assess this, we generate synthetic MSAs, either
without or with phylogeny, from Potts models trained on natural MSAs. We find
that unsupervised contact prediction is substantially more resilient to
phylogenetic noise when using MSA Transformer versus inferred Potts models.",2203.15465v2,https://arxiv.org/pdf/2203.15465v2
"Multilingual Knowledge Graph Completion with Self-Supervised Adaptive
  Graph Alignment","Zijie Huang, Zheng Li, Haoming Jiang, Tianyu Cao, Hanqing Lu, Bing Yin, Karthik Subbian, Yizhou Sun, Wei Wang","Predicting missing facts in a knowledge graph (KG) is crucial as modern KGs
are far from complete. Due to labor-intensive human labeling, this phenomenon
deteriorates when handling knowledge represented in various languages. In this
paper, we explore multilingual KG completion, which leverages limited seed
alignment as a bridge, to embrace the collective knowledge from multiple
languages. However, language alignment used in prior works is still not fully
exploited: (1) alignment pairs are treated equally to maximally push parallel
entities to be close, which ignores KG capacity inconsistency; (2) seed
alignment is scarce and new alignment identification is usually in a noisily
unsupervised manner. To tackle these issues, we propose a novel self-supervised
adaptive graph alignment (SS-AGA) method. Specifically, SS-AGA fuses all KGs as
a whole graph by regarding alignment as a new edge type. As such, information
propagation and noise influence across KGs can be adaptively controlled via
relation-aware attention weights. Meanwhile, SS-AGA features a new pair
generator that dynamically captures potential alignment pairs in a
self-supervised paradigm. Extensive experiments on both the public multilingual
DBPedia KG and newly-created industrial multilingual E-commerce KG empirically
demonstrate the effectiveness of SS-AG",2203.14987v1,https://arxiv.org/pdf/2203.14987v1
Energy-based Latent Aligner for Incremental Learning,"K J Joseph, Salman Khan, Fahad Shahbaz Khan, Rao Muhammad Anwer, Vineeth N Balasubramanian","Deep learning models tend to forget their earlier knowledge while
incrementally learning new tasks. This behavior emerges because the parameter
updates optimized for the new tasks may not align well with the updates
suitable for older tasks. The resulting latent representation mismatch causes
forgetting. In this work, we propose ELI: Energy-based Latent Aligner for
Incremental Learning, which first learns an energy manifold for the latent
representations such that previous task latents will have low energy and the
current task latents have high energy values. This learned manifold is used to
counter the representational shift that happens during incremental learning.
The implicit regularization that is offered by our proposed methodology can be
used as a plug-and-play module in existing incremental learning methodologies.
We validate this through extensive evaluation on CIFAR-100, ImageNet subset,
ImageNet 1k and Pascal VOC datasets. We observe consistent improvement when ELI
is added to three prominent methodologies in class-incremental learning, across
multiple incremental settings. Further, when added to the state-of-the-art
incremental object detector, ELI provides over 5% improvement in detection
accuracy, corroborating its effectiveness and complementary advantage to
existing art.",2203.14952v1,https://arxiv.org/pdf/2203.14952v1
"Reshaping Robot Trajectories Using Natural Language Commands: A Study of
  Multi-Modal Data Alignment Using Transformers","Arthur Bucker, Luis Figueredo, Sami Haddadin, Ashish Kapoor, Shuang Ma, Rogerio Bonatti","Natural language is the most intuitive medium for us to interact with other
people when expressing commands and instructions. However, using language is
seldom an easy task when humans need to express their intent towards robots,
since most of the current language interfaces require rigid templates with a
static set of action targets and commands. In this work, we provide a flexible
language-based interface for human-robot collaboration, which allows a user to
reshape existing trajectories for an autonomous agent. We take advantage of
recent advancements in the field of large language models (BERT and CLIP) to
encode the user command, and then combine these features with trajectory
information using multi-modal attention transformers. We train the model using
imitation learning over a dataset containing robot trajectories modified by
language commands, and treat the trajectory generation process as a sequence
prediction problem, analogously to how language generation architectures
operate. We evaluate the system in multiple simulated trajectory scenarios, and
show a significant performance increase of our model over baseline approaches.
In addition, our real-world experiments with a robot arm show that users
significantly prefer our natural language interface over traditional methods
such as kinesthetic teaching or cost-function programming. Our study shows how
the field of robotics can take advantage of large pre-trained language models
towards creating more intuitive interfaces between robots and machines. Project
webpage: https://arthurfenderbucker.github.io/NL_trajectory_reshaper/",2203.13411v1,https://arxiv.org/pdf/2203.13411v1
"BASiNETEntropy: an alignment-free method for classification of
  biological sequences through complex networks and entropy maximization","Murilo Montanini Breve, Matheus Henrique Pimenta-Zanon, Fabrício Martins Lopes","The discovery of nucleic acids and the structure of DNA have brought
considerable advances in the understanding of life. The development of
next-generation sequencing technologies has led to a large-scale generation of
data, for which computational methods have become essential for analysis and
knowledge discovery. In particular, RNAs have received much attention because
of the diversity of their functionalities in the organism and the discoveries
of different classes with different functions in many biological processes.
Therefore, the correct identification of RNA sequences is increasingly
important to provide relevant information to understand the functioning of
organisms. This work addresses this context by presenting a new method for the
classification of biological sequences through complex networks and entropy
maximization. The maximum entropy principle is proposed to identify the most
informative edges about the RNA class, generating a filtered complex network.
The proposed method was evaluated in the classification of different RNA
classes from 13 species. The proposed method was compared to PLEK, CPC2 and
BASiNET methods, outperforming all compared methods. BASiNETEntropy classified
all RNA sequences with high accuracy and low standard deviation in results,
showing assertiveness and robustness. The proposed method is implemented in an
open source in R language and is freely available at
https://cran.r-project.org/web/packages/BASiNETEntropy.",2203.15635v1,https://arxiv.org/pdf/2203.15635v1
"IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding
  Alignment","Yiming Zeng, Yue Qian, Qijian Zhang, Junhui Hou, Yixuan Yuan, Ying He","This paper investigates the problem of temporally interpolating dynamic 3D
point clouds with large non-rigid deformation. We formulate the problem as
estimation of point-wise trajectories (i.e., smooth curves) and further reason
that temporal irregularity and under-sampling are two major challenges. To
tackle the challenges, we propose IDEA-Net, an end-to-end deep learning
framework, which disentangles the problem under the assistance of the
explicitly learned temporal consistency. Specifically, we propose a temporal
consistency learning module to align two consecutive point cloud frames
point-wisely, based on which we can employ linear interpolation to obtain
coarse trajectories/in-between frames. To compensate the high-order nonlinear
components of trajectories, we apply aligned feature embeddings that encode
local geometry properties to regress point-wise increments, which are combined
with the coarse estimations. We demonstrate the effectiveness of our method on
various point cloud sequences and observe large improvement over
state-of-the-art methods both quantitatively and visually. Our framework can
bring benefits to 3D motion data acquisition. The source code is publicly
available at https://github.com/ZENGYIMING-EAMON/IDEA-Net.git.",2203.11590v1,https://arxiv.org/pdf/2203.11590v1
"Root-aligned SMILES: A Tight Representation for Chemical Reaction
  Prediction","Zipeng Zhong, Jie Song, Zunlei Feng, Tiantao Liu, Lingxiang Jia, Shaolun Yao, Min Wu, Tingjun Hou, Mingli Song","Chemical reaction prediction, involving forward synthesis and retrosynthesis
prediction, is a fundamental problem in organic synthesis. A popular
computational paradigm formulates synthesis prediction as a
sequence-to-sequence translation problem, where the typical SMILES is adopted
for molecule representations. However, the general-purpose SMILES neglects the
characteristics of chemical reactions, where the molecular graph topology is
largely unaltered from reactants to products, resulting in the suboptimal
performance of SMILES if straightforwardly applied. In this article, we propose
the root-aligned SMILES (R-SMILES), which specifies a tightly aligned
one-to-one mapping between the product and the reactant SMILES for more
efficient synthesis prediction. Due to the strict one-to-one mapping and
reduced edit distance, the computational model is largely relieved from
learning the complex syntax and dedicated to learning the chemical knowledge
for reactions. We compare the proposed R-SMILES with various state-of-the-art
baselines and show that it significantly outperforms them all, demonstrating
the superiority of the proposed method.",2203.11444v5,https://arxiv.org/pdf/2203.11444v5
"Knowledge Graph Embedding Methods for Entity Alignment: An Experimental
  Review","Nikolaos Fanourakis, Vasilis Efthymiou, Dimitris Kotzinos, Vassilis Christophides","In recent years, we have witnessed the proliferation of knowledge graphs (KG)
in various domains, aiming to support applications like question answering,
recommendations, etc. A frequent task when integrating knowledge from different
KGs is to find which subgraphs refer to the same real-world entity. Recently,
embedding methods have been used for entity alignment tasks, that learn a
vector-space representation of entities which preserves their similarity in the
original KGs. A wide variety of supervised, unsupervised, and semi-supervised
methods have been proposed that exploit both factual (attribute based) and
structural information (relation based) of entities in the KGs. Still, a
quantitative assessment of their strengths and weaknesses in real-world KGs
according to different performance metrics and KG characteristics is missing
from the literature. In this work, we conduct the first meta-level analysis of
popular embedding methods for entity alignment, based on a statistically sound
methodology. Our analysis reveals statistically significant correlations of
different embedding methods with various meta-features extracted by KGs and
rank them in a statistically significant way according to their effectiveness
across all real-world KGs of our testbed. Finally, we study interesting
trade-offs in terms of methods' effectiveness and efficiency.",2203.09280v2,https://arxiv.org/pdf/2203.09280v2
Adversarial Support Alignment,"Shangyuan Tong, Timur Garipov, Yang Zhang, Shiyu Chang, Tommi S. Jaakkola","We study the problem of aligning the supports of distributions. Compared to
the existing work on distribution alignment, support alignment does not require
the densities to be matched. We propose symmetric support difference as a
divergence measure to quantify the mismatch between supports. We show that
select discriminators (e.g. discriminator trained for Jensen-Shannon
divergence) are able to map support differences as support differences in their
one-dimensional output space. Following this result, our method aligns supports
by minimizing a symmetrized relaxed optimal transport cost in the discriminator
1D space via an adversarial process. Furthermore, we show that our approach can
be viewed as a limit of existing notions of alignment by increasing
transportation assignment tolerance. We quantitatively evaluate the method
across domain adaptation tasks with shifts in label distributions. Our
experiments show that the proposed method is more robust against these shifts
than other alignment-based baselines.",2203.08908v1,https://arxiv.org/pdf/2203.08908v1
Ensemble Semi-supervised Entity Alignment via Cycle-teaching,"Kexuan Xin, Zequn Sun, Wen Hua, Bing Liu, Wei Hu, Jianfeng Qu, Xiaofang Zhou","Entity alignment is to find identical entities in different knowledge graphs.
Although embedding-based entity alignment has recently achieved remarkable
progress, training data insufficiency remains a critical challenge.
Conventional semi-supervised methods also suffer from the incorrect entity
alignment in newly proposed training data. To resolve these issues, we design
an iterative cycle-teaching framework for semi-supervised entity alignment. The
key idea is to train multiple entity alignment models (called aligners)
simultaneously and let each aligner iteratively teach its successor the
proposed new entity alignment. We propose a diversity-aware alignment selection
method to choose reliable entity alignment for each aligner. We also design a
conflict resolution mechanism to resolve the alignment conflict when combining
the new alignment of an aligner and that from its teacher. Besides, considering
the influence of cycle-teaching order, we elaborately design a strategy to
arrange the optimal order that can maximize the overall performance of multiple
aligners. The cycle-teaching process can break the limitations of each model's
learning capability and reduce the noise in new training data, leading to
improved performance. Extensive experiments on benchmark datasets demonstrate
the effectiveness of the proposed cycle-teaching framework, which significantly
outperforms the state-of-the-art models when the training data is insufficient
and the new entity alignment has much noise.",2203.06308v1,https://arxiv.org/pdf/2203.06308v1
"CoDA21: Evaluating Language Understanding Capabilities of NLP Models
  With Context-Definition Alignment","Lütfi Kerem Senel, Timo Schick, Hinrich Schütze","Pretrained language models (PLMs) have achieved superhuman performance on
many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context
Definition Alignment), a challenging benchmark that measures natural language
understanding (NLU) capabilities of PLMs: Given a definition and a context each
for k words, but not the words themselves, the task is to align the k
definitions with the k contexts. CoDA21 requires a deep understanding of
contexts and definitions, including complex inference and world knowledge. We
find that there is a large gap between human and PLM performance, suggesting
that CoDA21 measures an aspect of NLU that is not sufficiently covered in
existing benchmarks.",2203.06228v1,https://arxiv.org/pdf/2203.06228v1
"Averaging Spatio-temporal Signals using Optimal Transport and Soft
  Alignments","Hicham Janati, Marco Cuturi, Alexandre Gramfort","Several fields in science, from genomics to neuroimaging, require monitoring
populations (measures) that evolve with time. These complex datasets,
describing dynamics with both time and spatial components, pose new challenges
for data analysis. We propose in this work a new framework to carry out
averaging of these datasets, with the goal of synthesizing a representative
template trajectory from multiple trajectories. We show that this requires
addressing three sources of invariance: shifts in time, space, and total
population size (or mass/amplitude). Here we draw inspiration from dynamic time
warping (DTW), optimal transport (OT) theory and its unbalanced extension (UOT)
to propose a criterion that can address all three issues. This proposal
leverages a smooth formulation of DTW (Soft-DTW) that is shown to capture
temporal shifts, and UOT to handle both variations in space and size. Our
proposed loss can be used to define spatio-temporal barycenters as Fr\'echet
means. Using Fenchel duality, we show how these barycenters can be computed
efficiently, in parallel, via a novel variant of entropy-regularized debiased
UOT. Experiments on handwritten letters and brain imaging data confirm our
theoretical findings and illustrate the effectiveness of the proposed loss for
spatio-temporal data.",2203.05813v2,https://arxiv.org/pdf/2203.05813v2
"Align-Deform-Subtract: An Interventional Framework for Explaining Object
  Differences","Cian Eastwood, Li Nanbo, Christopher K. I. Williams","Given two object images, how can we explain their differences in terms of the
underlying object properties? To address this question, we propose
Align-Deform-Subtract (ADS) -- an interventional framework for explaining
object differences. By leveraging semantic alignments in image-space as
counterfactual interventions on the underlying object properties, ADS
iteratively quantifies and removes differences in object properties. The result
is a set of ""disentangled"" error measures which explain object differences in
terms of the underlying properties. Experiments on real and synthetic data
illustrate the efficacy of the framework.",2203.04694v2,https://arxiv.org/pdf/2203.04694v2
Monocular Depth Distribution Alignment with Low Computation,"Fei Sheng, Feng Xue, Yicong Chang, Wenteng Liang, Anlong Ming","The performance of monocular depth estimation generally depends on the amount
of parameters and computational cost. It leads to a large accuracy contrast
between light-weight networks and heavy-weight networks, which limits their
application in the real world. In this paper, we model the majority of accuracy
contrast between them as the difference of depth distribution, which we call
""Distribution drift"". To this end, a distribution alignment network (DANet) is
proposed. We firstly design a pyramid scene transformer (PST) module to capture
inter-region interaction in multiple scales. By perceiving the difference of
depth features between every two regions, DANet tends to predict a reasonable
scene structure, which fits the shape of distribution to ground truth. Then, we
propose a local-global optimization (LGO) scheme to realize the supervision of
global range of scene depth. Thanks to the alignment of depth distribution
shape and scene depth range, DANet sharply alleviates the distribution drift,
and achieves a comparable performance with prior heavy-weight methods, but uses
only 1% floating-point operations per second (FLOPs) of them. The experiments
on two datasets, namely the widely used NYUDv2 dataset and the more challenging
iBims-1 dataset, demonstrate the effectiveness of our method. The source code
is available at https://github.com/YiLiM1/DANet.",2203.04538v1,https://arxiv.org/pdf/2203.04538v1
Deep Reinforcement Learning for Entity Alignment,"Lingbing Guo, Yuqiang Han, Qiang Zhang, Huajun Chen","Embedding-based methods have attracted increasing attention in recent entity
alignment (EA) studies. Although great promise they can offer, there are still
several limitations. The most notable is that they identify the aligned
entities based on cosine similarity, ignoring the semantics underlying the
embeddings themselves. Furthermore, these methods are shortsighted,
heuristically selecting the closest entity as the target and allowing multiple
entities to match the same candidate. To address these limitations, we model
entity alignment as a sequential decision-making task, in which an agent
sequentially decides whether two entities are matched or mismatched based on
their representation vectors. The proposed reinforcement learning (RL)-based
entity alignment framework can be flexibly adapted to most embedding-based EA
methods. The experimental results demonstrate that it consistently advances the
performance of several state-of-the-art methods, with a maximum improvement of
31.1% on Hits@1.",2203.03315v1,https://arxiv.org/pdf/2203.03315v1
"EAG: Extract and Generate Multi-way Aligned Corpus for Complete
  Multi-lingual Neural Machine Translation","Yulin Xu, Zhen Yang, Fandong Meng, JieZhou","Complete Multi-lingual Neural Machine Translation (C-MNMT) achieves superior
performance against the conventional MNMT by constructing multi-way aligned
corpus, i.e., aligning bilingual training examples from different language
pairs when either their source or target sides are identical. However, since
exactly identical sentences from different language pairs are scarce, the power
of the multi-way aligned corpus is limited by its scale. To handle this
problem, this paper proposes ""Extract and Generate"" (EAG), a two-step approach
to construct large-scale and high-quality multi-way aligned corpus from
bilingual data. Specifically, we first extract candidate aligned examples by
pairing the bilingual examples from different language pairs with highly
similar source or target sentences; and then generate the final aligned
examples from the candidates with a well-trained generation model. With this
two-step pipeline, EAG can construct a large-scale and multi-way aligned corpus
whose diversity is almost identical to the original bilingual corpus.
Experiments on two publicly available datasets i.e., WMT-5 and OPUS-100, show
that the proposed method achieves significant improvements over strong
baselines, with +1.1 and +1.4 BLEU points improvements on the two datasets
respectively.",2203.02180v2,https://arxiv.org/pdf/2203.02180v2
"Time-aware Graph Neural Networks for Entity Alignment between Temporal
  Knowledge Graphs","Chengjin Xu, Fenglong Su, Jens Lehmann","Entity alignment aims to identify equivalent entity pairs between different
knowledge graphs (KGs). Recently, the availability of temporal KGs (TKGs) that
contain time information created the need for reasoning over time in such TKGs.
Existing embedding-based entity alignment approaches disregard time information
that commonly exists in many large-scale KGs, leaving much room for
improvement. In this paper, we focus on the task of aligning entity pairs
between TKGs and propose a novel Time-aware Entity Alignment approach based on
Graph Neural Networks (TEA-GNN). We embed entities, relations and timestamps of
different KGs into a vector space and use GNNs to learn entity representations.
To incorporate both relation and time information into the GNN structure of our
model, we use a time-aware attention mechanism which assigns different weights
to different nodes with orthogonal transformation matrices computed from
embeddings of the relevant relations and timestamps in a neighborhood.
Experimental results on multiple real-world TKG datasets show that our method
significantly outperforms the state-of-the-art methods due to the inclusion of
time information.",2203.02150v2,https://arxiv.org/pdf/2203.02150v2
SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs,"Xiao Liu, Haoyun Hong, Xinghao Wang, Zeyi Chen, Evgeny Kharlamov, Yuxiao Dong, Jie Tang","Entity alignment, aiming to identify equivalent entities across different
knowledge graphs (KGs), is a fundamental problem for constructing Web-scale
KGs. Over the course of its development, the label supervision has been
considered necessary for accurate alignments. Inspired by the recent progress
of self-supervised learning, we explore the extent to which we can get rid of
supervision for entity alignment. Commonly, the label information (positive
entity pairs) is used to supervise the process of pulling the aligned entities
in each positive pair closer. However, our theoretical analysis suggests that
the learning of entity alignment can actually benefit more from pushing
unlabeled negative pairs far away from each other than pulling labeled positive
pairs close. By leveraging this discovery, we develop the self-supervised
learning objective for entity alignment. We present SelfKG with efficient
strategies to optimize this objective for aligning entities without label
supervision. Extensive experiments on benchmark datasets demonstrate that
SelfKG without supervision can match or achieve comparable results with
state-of-the-art supervised baselines. The performance of SelfKG suggests that
self-supervised learning offers great potential for entity alignment in KGs.
The code and data are available at https://github.com/THUDM/SelfKG.",2203.01044v1,https://arxiv.org/pdf/2203.01044v1
"Unsupervised Vision-and-Language Pre-training via Retrieval-based
  Multi-Granular Alignment","Mingyang Zhou, Licheng Yu, Amanpreet Singh, Mengjiao Wang, Zhou Yu, Ning Zhang","Vision-and-Language (V+L) pre-training models have achieved tremendous
success in recent years on various multi-modal benchmarks. However, the
majority of existing models require pre-training on a large set of parallel
image-text data, which is costly to collect, compared to image-only or
text-only data. In this paper, we explore unsupervised Vision-and-Language
pre-training (UVLP) to learn the cross-modal representation from non-parallel
image and text datasets. We found two key factors that lead to good
unsupervised V+L pre-training without parallel data: (i) joint image-and-text
input (ii) overall image-text alignment (even for non-parallel data).
Accordingly, we propose a novel unsupervised V+L pre-training curriculum for
non-parallel texts and images. We first construct a weakly aligned image-text
corpus via a retrieval-based approach, then apply a set of multi-granular
alignment pre-training tasks, including region-to-tag, region-to-phrase, and
image-to-sentence alignment, to bridge the gap between the two modalities. A
comprehensive ablation study shows each granularity is helpful to learn a
stronger pre-trained model. We adapt our pre-trained model to a set of V+L
downstream tasks, including VQA, NLVR2, Visual Entailment, and RefCOCO+. Our
model achieves the state-of-art performance in all these tasks under the
unsupervised setting.",2203.00242v1,https://arxiv.org/pdf/2203.00242v1
Multi-modal Alignment using Representation Codebook,"Jiali Duan, Liqun Chen, Son Tran, Jinyu Yang, Yi Xu, Belinda Zeng, Trishul Chilimbi","Aligning signals from different modalities is an important step in
vision-language representation learning as it affects the performance of later
stages such as cross-modality fusion. Since image and text typically reside in
different regions of the feature space, directly aligning them at instance
level is challenging especially when features are still evolving during
training. In this paper, we propose to align at a higher and more stable level
using cluster representation. Specifically, we treat image and text as two
""views"" of the same entity, and encode them into a joint vision-language coding
space spanned by a dictionary of cluster centers (codebook). We contrast
positive and negative samples via their cluster assignments while
simultaneously optimizing the cluster centers. To further smooth out the
learning process, we adopt a teacher-student distillation paradigm, where the
momentum teacher of one view guides the student learning of the other. We
evaluated our approach on common vision language benchmarks and obtain new SoTA
on zero-shot cross modality retrieval while being competitive on various other
transfer tasks.",2203.00048v3,https://arxiv.org/pdf/2203.00048v3
"DGSS : Domain Generalized Semantic Segmentation using Iterative Style
  Mining and Latent Representation Alignment","Pranjay Shyam, Antyanta Bangunharcana, Kuk-Jin Yoon, Kyung-Soo Kim","Semantic segmentation algorithms require access to well-annotated datasets
captured under diverse illumination conditions to ensure consistent
performance. However, poor visibility conditions at varying illumination
conditions result in laborious and error-prone labeling. Alternatively, using
synthetic samples to train segmentation algorithms has gained interest with the
drawback of domain gap that results in sub-optimal performance. While current
state-of-the-art (SoTA) have proposed different mechanisms to bridge the domain
gap, they still perform poorly in low illumination conditions with an average
performance drop of - 10.7 mIOU. In this paper, we focus upon single source
domain generalization to overcome the domain gap and propose a two-step
framework wherein we first identify an adversarial style that maximizes the
domain gap between stylized and source images. Subsequently, these stylized
images are used to categorically align features such that features belonging to
the same class are clustered together in latent space, irrespective of domain
gap. Furthermore, to increase intra-class variance while training, we propose a
style mixing mechanism wherein the same objects from different styles are mixed
to construct a new training image. This framework allows us to achieve a domain
generalized semantic segmentation algorithm with consistent performance without
prior information of the target domain while relying on a single source. Based
on extensive experiments, we match SoTA performance on SYNTHIA $\to$
Cityscapes, GTAV $\to$ Cityscapes while setting new SoTA on GTAV $\to$ Dark
Zurich and GTAV $\to$ Night Driving benchmarks without retraining.",2202.13144v1,https://arxiv.org/pdf/2202.13144v1
"An initial alignment between neural network and target is needed for
  gradient descent to learn","Emmanuel Abbe, Elisabetta Cornacchia, Jan Hązła, Christopher Marquis","This paper introduces the notion of ``Initial Alignment'' (INAL) between a
neural network at initialization and a target function. It is proved that if a
network and a Boolean target function do not have a noticeable INAL, then noisy
gradient descent on a fully connected network with normalized i.i.d.
initialization will not learn in polynomial time. Thus a certain amount of
knowledge about the target (measured by the INAL) is needed in the architecture
design. This also provides an answer to an open problem posed in [AS20]. The
results are based on deriving lower-bounds for descent algorithms on symmetric
neural networks without explicit knowledge of the target function beyond its
INAL.",2202.12846v2,https://arxiv.org/pdf/2202.12846v2
HCMD-zero: Learning Value Aligned Mechanisms from Data,"Jan Balaguer, Raphael Koster, Ari Weinstein, Lucy Campbell-Gillingham, Christopher Summerfield, Matthew Botvinick, Andrea Tacchetti","Artificial learning agents are mediating a larger and larger number of
interactions among humans, firms, and organizations, and the intersection
between mechanism design and machine learning has been heavily investigated in
recent years. However, mechanism design methods often make strong assumptions
on how participants behave (e.g. rationality), on the kind of knowledge
designers have access to a priori (e.g. access to strong baseline mechanisms),
or on what the goal of the mechanism should be (e.g. total welfare). Here we
introduce HCMD-zero, a general purpose method to construct mechanisms making
none of these three assumptions. HCMD-zero learns to mediate interactions among
participants and adjusts the mechanism parameters to make itself more likely to
be preferred by participants. It does so by remaining engaged in an electoral
contest with copies of itself, thereby accessing direct feedback from
participants. We test our method on a stylized resource allocation game that
highlights the tension between productivity, equality and the temptation to
free ride. HCMD-zero produces a mechanism that is preferred by human
participants over a strong baseline, it does so automatically, without
requiring prior knowledge, and using human behavioral trajectories sparingly
and effectively. Our analysis shows HCMD-zero consistently makes the mechanism
policy more and more likely to be preferred by human participants over the
course of training, and that it results in a mechanism with an interpretable
and intuitive policy.",2202.10122v2,https://arxiv.org/pdf/2202.10122v2
"Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in
  First Trimester 3D Ultrasound","W. A. P. Bastiaansen, M. Rousian, R. P. M. Steegers-Theunissen, W. J. Niessen, A. H. J. Koning, S. Klein","Segmentation and spatial alignment of ultrasound (US) imaging data acquired
in the in first trimester are crucial for monitoring human embryonic growth and
development throughout this crucial period of life. Current approaches are
either manual or semi-automatic and are therefore very time-consuming and prone
to errors. To automate these tasks, we propose a multi-atlas framework for
automatic segmentation and spatial alignment of the embryo using deep learning
with minimal supervision. Our framework learns to register the embryo to an
atlas, which consists of the US images acquired at a range of gestational age
(GA), segmented and spatially aligned to a predefined standard orientation.
From this, we can derive the segmentation of the embryo and put the embryo in
standard orientation. US images acquired at 8+0 till 12+6 weeks GA were used
and eight subjects were selected as atlas. We evaluated different fusion
strategies to incorporate multiple atlases: 1) training the framework using
atlas images from a single subject, 2) training the framework with data of all
available atlases and 3) ensembling of the frameworks trained per subject. To
evaluate the performance, we calculated the Dice score over the test set. We
found that training the framework using all available atlases outperformed
ensembling and gave similar results compared to the best of all frameworks
trained on a single subject. Furthermore, we found that selecting images from
the four atlases closest in GA out of all available atlases, regardless of the
individual quality, gave the best results with a median Dice score of 0.72. We
conclude that our framework can accurately segment and spatially align the
embryo in first trimester 3D US images and is robust for the variation in
quality that existed in the available atlases.",2202.06599v3,https://arxiv.org/pdf/2202.06599v3
"Robust alignment of cross-session recordings of neural population
  activity by behaviour via unsupervised domain adaptation","Justin Jude, Matthew G Perich, Lee E Miller, Matthias H Hennig","Neural population activity relating to behaviour is assumed to be inherently
low-dimensional despite the observed high dimensionality of data recorded using
multi-electrode arrays. Therefore, predicting behaviour from neural population
recordings has been shown to be most effective when using latent variable
models. Over time however, the activity of single neurons can drift, and
different neurons will be recorded due to movement of implanted neural probes.
This means that a decoder trained to predict behaviour on one day performs
worse when tested on a different day. On the other hand, evidence suggests that
the latent dynamics underlying behaviour may be stable even over months and
years. Based on this idea, we introduce a model capable of inferring
behaviourally relevant latent dynamics from previously unseen data recorded
from the same animal, without any need for decoder recalibration. We show that
unsupervised domain adaptation combined with a sequential variational
autoencoder, trained on several sessions, can achieve good generalisation to
unseen data and correctly predict behaviour where conventional methods fail.
Our results further support the hypothesis that behaviour-related neural
dynamics are low-dimensional and stable over time, and will enable more
effective and flexible use of brain computer interface technologies.",2202.06159v2,https://arxiv.org/pdf/2202.06159v2
"PARSE: Pairwise Alignment of Representations in Semi-Supervised EEG
  Learning for Emotion Recognition","Guangyi Zhang, Vandad Davoodnia, Ali Etemad","We propose PARSE, a novel semi-supervised architecture for learning strong
EEG representations for emotion recognition. To reduce the potential
distribution mismatch between the large amounts of unlabeled data and the
limited amount of labeled data, PARSE uses pairwise representation alignment.
First, our model performs data augmentation followed by label guessing for
large amounts of original and augmented unlabeled data. This is then followed
by sharpening of the guessed labels and convex combinations of the unlabeled
and labeled data. Finally, representation alignment and emotion classification
are performed. To rigorously test our model, we compare PARSE to several
state-of-the-art semi-supervised approaches which we implement and adapt for
EEG learning. We perform these experiments on four public EEG-based emotion
recognition datasets, SEED, SEED-IV, SEED-V and AMIGOS (valence and arousal).
The experiments show that our proposed framework achieves the overall best
results with varying amounts of limited labeled samples in SEED, SEED-IV and
AMIGOS (valence), while approaching the overall best result (reaching the
second-best) in SEED-V and AMIGOS (arousal). The analysis shows that our
pairwise representation alignment considerably improves the performance by
reducing the distribution alignment between unlabeled and labeled data,
especially when only 1 sample per class is labeled.",2202.05400v2,https://arxiv.org/pdf/2202.05400v2
"Collaborative Filtering with Attribution Alignment for Review-based
  Non-overlapped Cross Domain Recommendation","Weiming Liu, Xiaolin Zheng, Mengling Hu, Chaochao Chen","Cross-Domain Recommendation (CDR) has been popularly studied to utilize
different domain knowledge to solve the data sparsity and cold-start problem in
recommender systems. In this paper, we focus on the Review-based Non-overlapped
Recommendation (RNCDR) problem. The problem is commonly-existed and challenging
due to two main aspects, i.e, there are only positive user-item ratings on the
target domain and there is no overlapped user across different domains. Most
previous CDR approaches cannot solve the RNCDR problem well, since (1) they
cannot effectively combine review with other information (e.g., ID or ratings)
to obtain expressive user or item embedding, (2) they cannot reduce the domain
discrepancy on users and items. To fill this gap, we propose Collaborative
Filtering with Attribution Alignment model (CFAA), a cross-domain
recommendation framework for the RNCDR problem. CFAA includes two main modules,
i.e., rating prediction module and embedding attribution alignment module. The
former aims to jointly mine review, one-hot ID, and multi-hot historical
ratings to generate expressive user and item embeddings. The later includes
vertical attribution alignment and horizontal attribution alignment, tending to
reduce the discrepancy based on multiple perspectives. Our empirical study on
Douban and Amazon datasets demonstrates that CFAA significantly outperforms the
state-of-the-art models under the RNCDR setting.",2202.04920v1,https://arxiv.org/pdf/2202.04920v1
Generalized Strategic Classification and the Case of Aligned Incentives,"Sagi Levanon, Nir Rosenfeld","Strategic classification studies learning in settings where self-interested
users can strategically modify their features to obtain favorable predictive
outcomes. A key working assumption, however, is that ""favorable"" always means
""positive""; this may be appropriate in some applications (e.g., loan approval),
but reduces to a fairly narrow view of what user interests can be. In this work
we argue for a broader perspective on what accounts for strategic user
behavior, and propose and study a flexible model of generalized strategic
classification. Our generalized model subsumes most current models but includes
other novel settings; among these, we identify and target one intriguing
sub-class of problems in which the interests of users and the system are
aligned. This setting reveals a surprising fact: that standard max-margin
losses are ill-suited for strategic inputs. Returning to our fully generalized
model, we propose a novel max-margin framework for strategic learning that is
practical and effective, and which we analyze theoretically. We conclude with a
set of experiments that empirically demonstrate the utility of our approach.",2202.04357v2,https://arxiv.org/pdf/2202.04357v2
"RNN Transducers for Nested Named Entity Recognition with constraints on
  alignment for long sequences","Hagen Soltau, Izhak Shafran, Mingqiu Wang, Laurent El Shafey","Popular solutions to Named Entity Recognition (NER) include conditional
random fields, sequence-to-sequence models, or utilizing the question-answering
framework. However, they are not suitable for nested and overlapping spans with
large ontologies and for predicting the position of the entities. To fill this
gap, we introduce a new model for NER task -- an RNN transducer (RNN-T). These
models are trained using paired input and output sequences without explicitly
specifying the alignment between them, similar to other seq-to-seq models.
RNN-T models learn the alignment using a loss function that sums over all
alignments. In NER tasks, however, the alignment between words and target
labels are available from the human annotations. We propose a fixed alignment
RNN-T model that utilizes the given alignment, while preserving the benefits of
RNN-Ts such as modeling output dependencies. As a more general case, we also
propose a constrained alignment model where users can specify a relaxation of
the given input alignment and the model will learn an alignment within the
given constraints. In other words, we propose a family of seq-to-seq models
which can leverage alignments between input and target sequences when
available. Through empirical experiments on a challenging real-world medical
NER task with multiple nested ontologies, we demonstrate that our fixed
alignment model outperforms the standard RNN-T model, improving F1-score from
0.70 to 0.74.",2203.03543v1,https://arxiv.org/pdf/2203.03543v1
"Aligning Eyes between Humans and Deep Neural Network through Interactive
  Attention Alignment","Yuyang Gao, Tong Sun, Liang Zhao, Sungsoo Hong","While Deep Neural Networks (DNNs) are deriving the major innovations in
nearly every field through their powerful automation, we are also witnessing
the peril behind automation as a form of bias, such as automated racism, gender
bias, and adversarial bias. As the societal impact of DNNs grows, finding an
effective way to steer DNNs to align their behavior with the human mental model
has become indispensable in realizing fair and accountable models. We propose a
novel framework of Interactive Attention Alignment (IAA) that aims at realizing
human-steerable Deep Neural Networks (DNNs). IAA leverages DNN model
explanation method as an interactive medium that humans can use to unveil the
cases of biased model attention and directly adjust the attention. In improving
the DNN using human-generated adjusted attention, we introduce GRADIA, a novel
computational pipeline that jointly maximizes attention quality and prediction
accuracy. We evaluated IAA framework in Study 1 and GRADIA in Study 2 in a
gender classification problem. Study 1 found applying IAA can significantly
improve the perceived quality of model attention from human eyes. In Study 2,
we found using GRADIA can (1) significantly improve the perceived quality of
model attention and (2) significantly improve model performance in scenarios
where the training samples are limited. We present implications for future
interactive user interfaces design towards human-alignable AI.",2202.02838v1,https://arxiv.org/pdf/2202.02838v1
"Simulation-to-Reality domain adaptation for offline 3D object annotation
  on pointclouds with correlation alignment","Weishuang Zhang, B Ravi Kiran, Thomas Gauthier, Yanis Mazouz, Theo Steger","Annotating objects with 3D bounding boxes in LiDAR pointclouds is a costly
human driven process in an autonomous driving perception system. In this paper,
we present a method to semi-automatically annotate real-world pointclouds
collected by deployment vehicles using simulated data. We train a 3D object
detector model on labeled simulated data from CARLA jointly with real world
pointclouds from our target vehicle. The supervised object detection loss is
augmented with a CORAL loss term to reduce the distance between labeled
simulated and unlabeled real pointcloud feature representations. The goal here
is to learn representations that are invariant to simulated (labeled) and
real-world (unlabeled) target domains. We also provide an updated survey on
domain adaptation methods for pointclouds.",2202.02666v2,https://arxiv.org/pdf/2202.02666v2
"MVPTR: Multi-Level Semantic Alignment for Vision-Language Pre-Training
  via Multi-Stage Learning","Zejun Li, Zhihao Fan, Huaixiao Tou, Jingjing Chen, Zhongyu Wei, Xuanjing Huang","Previous vision-language pre-training models mainly construct multi-modal
inputs with tokens and objects (pixels) followed by performing cross-modality
interaction between them. We argue that the input of only tokens and object
features limits high-level semantic alignment like phrase-to-region grounding.
Meanwhile, multi-level alignments are inherently consistent and able to
facilitate the representation learning synergistically. Therefore, in this
paper, we propose to learn Multi-level semantic alignment for Vision-language
Pre-TRaining (MVPTR). In MVPTR, we follow the nested structure of both
modalities to introduce concepts as high-level semantics. To ease the learning
from multi-modal multi-level inputs, our framework is split into two stages,
the first stage focuses on intra-modality multi-level representation learning,
the second enforces interactions across modalities via both coarse-grained and
fine-grained semantic alignment tasks. In addition to the commonly used
image-text matching and masked language model tasks, we introduce a masked
concept recovering task in the first stage to enhance the concept
representation learning, and two more tasks in the second stage to explicitly
encourage multi-level alignments across modalities. Our code is available at
https://github.com/Junction4Nako/mvp_pytorch.",2201.12596v3,https://arxiv.org/pdf/2201.12596v3
"On the Power of Gradual Network Alignment Using Dual-Perception
  Similarities","Jin-Duk Park, Cong Tran, Won-Yong Shin, Xin Cao","Network alignment (NA) is the task of finding the correspondence of nodes
between two networks based on the network structure and node attributes. Our
study is motivated by the fact that, since most of existing NA methods have
attempted to discover all node pairs at once, they do not harness information
enriched through interim discovery of node correspondences to more accurately
find the next correspondences during the node matching. To tackle this
challenge, we propose Grad-Align, a new NA method that gradually discovers node
pairs by making full use of node pairs exhibiting strong consistency, which are
easy to be discovered in the early stage of gradual matching. Specifically,
Grad-Align first generates node embeddings of the two networks based on graph
neural networks along with our layer-wise reconstruction loss, a loss built
upon capturing the first-order and higher-order neighborhood structures. Then,
nodes are gradually aligned by computing dual-perception similarity measures
including the multi-layer embedding similarity as well as the Tversky
similarity, an asymmetric set similarity using the Tversky index applicable to
networks with different scales. Additionally, we incorporate an edge
augmentation module into Grad-Align to reinforce the structural consistency.
Through comprehensive experiments using real-world and synthetic datasets, we
empirically demonstrate that Grad-Align consistently outperforms
state-of-the-art NA methods.",2201.10945v3,https://arxiv.org/pdf/2201.10945v3
"Jointly Learning Knowledge Embedding and Neighborhood Consensus with
  Relational Knowledge Distillation for Entity Alignment","Xinhang Li, Yong Zhang, Chunxiao Xing","Entity alignment aims at integrating heterogeneous knowledge from different
knowledge graphs. Recent studies employ embedding-based methods by first
learning the representation of Knowledge Graphs and then performing entity
alignment via measuring the similarity between entity embeddings. However, they
failed to make good use of the relation semantic information due to the
trade-off problem caused by the different objectives of learning knowledge
embedding and neighborhood consensus. To address this problem, we propose
Relational Knowledge Distillation for Entity Alignment (RKDEA), a Graph
Convolutional Network (GCN) based model equipped with knowledge distillation
for entity alignment. We adopt GCN-based models to learn the representation of
entities by considering the graph structure and incorporating the relation
semantic information into GCN via knowledge distillation. Then, we introduce a
novel adaptive mechanism to transfer relational knowledge so as to jointly
learn entity embedding and neighborhood consensus. Experimental results on
several benchmarking datasets demonstrate the effectiveness of our proposed
model.",2201.11249v1,https://arxiv.org/pdf/2201.11249v1
"A Transformer-Based Feature Segmentation and Region Alignment Method For
  UAV-View Geo-Localization","Ming Dai, Jianhong Hu, Jiedong Zhuang, Enhui Zheng","Cross-view geo-localization is a task of matching the same geographic image
from different views, e.g., unmanned aerial vehicle (UAV) and satellite. The
most difficult challenges are the position shift and the uncertainty of
distance and scale. Existing methods are mainly aimed at digging for more
comprehensive fine-grained information. However, it underestimates the
importance of extracting robust feature representation and the impact of
feature alignment. The CNN-based methods have achieved great success in
cross-view geo-localization. However it still has some limitations, e.g., it
can only extract part of the information in the neighborhood and some scale
reduction operations will make some fine-grained information lost. In
particular, we introduce a simple and efficient transformer-based structure
called Feature Segmentation and Region Alignment (FSRA) to enhance the model's
ability to understand contextual information as well as to understand the
distribution of instances. Without using additional supervisory information,
FSRA divides regions based on the heat distribution of the transformer's
feature map, and then aligns multiple specific regions in different views one
on one. Finally, FSRA integrates each region into a set of feature
representations. The difference is that FSRA does not divide regions manually,
but automatically based on the heat distribution of the feature map. So that
specific instances can still be divided and aligned when there are significant
shifts and scale changes in the image. In addition, a multiple sampling
strategy is proposed to overcome the disparity in the number of satellite
images and that of images from other sources. Experiments show that the
proposed method has superior performance and achieves the state-of-the-art in
both tasks of drone view target localization and drone navigation. Code will be
released at https://github.com/Dmmm1997/FSRA",2201.09206v1,https://arxiv.org/pdf/2201.09206v1
"Prediction of the electron density of states for crystalline compounds
  with Atomistic Line Graph Neural Networks (ALIGNN)","Prathik R Kaundinya, Kamal Choudhary, Surya R. Kalidindi","Machine learning (ML) based models have greatly enhanced the traditional
materials discovery and design pipeline. Specifically, in recent years,
surrogate ML models for material property prediction have demonstrated success
in predicting discrete scalar-valued target properties to within reasonable
accuracy of their DFT-computed values. However, accurate prediction of spectral
targets such as the electron Density of States (DOS) poses a much more
challenging problem due to the complexity of the target, and the limited amount
of available training data. In this study, we present an extension of the
recently developed Atomistic Line Graph Neural Network (ALIGNN) to accurately
predict DOS of a large set of material unit cell structures, trained to the
publicly available JARVIS-DFT dataset. Furthermore, we evaluate two methods of
representation of the target quantity - a direct discretized spectrum, and a
compressed low-dimensional representation obtained using an autoencoder.
Through this work, we demonstrate the utility of graph-based featurization and
modeling methods in the prediction of complex targets that depend on both
chemistry and directional characteristics of material structures.",2201.08348v1,https://arxiv.org/pdf/2201.08348v1
Improve Sentence Alignment by Divide-and-conquer,Wu Zhang,"In this paper, we introduce a divide-and-conquer algorithm to improve
sentence alignment speed. We utilize external bilingual sentence embeddings to
find accurate hard delimiters for the parallel texts to be aligned. We use
Monte Carlo simulation to show experimentally that using this
divide-and-conquer algorithm, we can turn any quadratic time complexity
sentence alignment algorithm into an algorithm with average time complexity of
O(NlogN). On a standard OCR-generated dataset, our method improves the
Bleualign baseline by 3 F1 points. Besides, when computational resources are
restricted, our algorithm is faster than Vecalign in practice.",2201.06907v1,https://arxiv.org/pdf/2201.06907v1
Interactive Contrastive Learning for Self-supervised Entity Alignment,"Kaisheng Zeng, Zhenhao Dong, Lei Hou, Yixin Cao, Minghao Hu, Jifan Yu, Xin Lv, Juanzi Li, Ling Feng","Self-supervised entity alignment (EA) aims to link equivalent entities across
different knowledge graphs (KGs) without seed alignments. The current SOTA
self-supervised EA method draws inspiration from contrastive learning,
originally designed in computer vision based on instance discrimination and
contrastive loss, and suffers from two shortcomings. Firstly, it puts
unidirectional emphasis on pushing sampled negative entities far away rather
than pulling positively aligned pairs close, as is done in the well-established
supervised EA. Secondly, KGs contain rich side information (e.g., entity
description), and how to effectively leverage those information has not been
adequately investigated in self-supervised EA. In this paper, we propose an
interactive contrastive learning model for self-supervised EA. The model
encodes not only structures and semantics of entities (including entity name,
entity description, and entity neighborhood), but also conducts cross-KG
contrastive learning by building pseudo-aligned entity pairs. Experimental
results show that our approach outperforms previous best self-supervised
results by a large margin (over 9% average improvement) and performs on par
with previous SOTA supervised counterparts, demonstrating the effectiveness of
the interactive contrastive learning for self-supervised EA.",2201.06225v2,https://arxiv.org/pdf/2201.06225v2
"PocketNN: Integer-only Training and Inference of Neural Networks via
  Direct Feedback Alignment and Pocket Activations in Pure C++","Jaewoo Song, Fangzhen Lin","Standard deep learning algorithms are implemented using floating-point real
numbers. This presents an obstacle for implementing them on low-end devices
which may not have dedicated floating-point units (FPUs). As a result,
researchers in tinyML have considered machine learning algorithms that can
train and run a deep neural network (DNN) on a low-end device using integer
operations only. In this paper we propose PocketNN, a light and self-contained
proof-of-concept framework in pure C++ for the training and inference of DNNs
using only integers. Unlike other approaches, PocketNN directly operates on
integers without requiring any explicit quantization algorithms or customized
fixed-point formats. This was made possible by pocket activations, which are a
family of activation functions devised for integer-only DNNs, and an emerging
DNN training algorithm called direct feedback alignment (DFA). Unlike the
standard backpropagation (BP), DFA trains each layer independently, thus
avoiding integer overflow which is a key problem when using BP with
integer-only operations. We used PocketNN to train some DNNs on two well-known
datasets, MNIST and Fashion-MNIST. Our experiments show that the DNNs trained
with our PocketNN achieved 96.98% and 87.7% accuracies on MNIST and
Fashion-MNIST datasets, respectively. The accuracies are very close to the
equivalent DNNs trained using BP with floating-point real number operations,
such that accuracy degradations were just 1.02%p and 2.09%p, respectively.
Finally, our PocketNN has high compatibility and portability for low-end
devices as it is open source and implemented in pure C++ without any
dependencies.",2201.02863v6,https://arxiv.org/pdf/2201.02863v6
"Self-aligned Spatial Feature Extraction Network for UAV Vehicle
  Re-identification","Aihuan Yao, Jiahao Qi, Ping Zhong","Compared with existing vehicle re-identification (ReID) tasks conducted with
datasets collected by fixed surveillance cameras, vehicle ReID for unmanned
aerial vehicle (UAV) is still under-explored and could be more challenging.
Vehicles with the same color and type show extremely similar appearance from
the UAV's perspective so that mining fine-grained characteristics becomes
necessary. Recent works tend to extract distinguishing information by regional
features and component features. The former requires input images to be aligned
and the latter entails detailed annotations, both of which are difficult to
meet in UAV application. In order to extract efficient fine-grained features
and avoid tedious annotating work, this letter develops an unsupervised
self-aligned network consisting of three branches. The network introduced a
self-alignment module to convert the input images with variable orientations to
a uniform orientation, which is implemented under the constraint of triple loss
function designed with spatial features. On this basis, spatial features,
obtained by vertical and horizontal segmentation methods, and global features
are integrated to improve the representation ability in embedded space.
Extensive experiments are conducted on UAV-VeID dataset, and our method
achieves the best performance compared with recent ReID works.",2201.02836v1,https://arxiv.org/pdf/2201.02836v1
Contrastive Neighborhood Alignment,"Pengkai Zhu, Zhaowei Cai, Yuanjun Xiong, Zhuowen Tu, Luis Goncalves, Vijay Mahadevan, Stefano Soatto","We present Contrastive Neighborhood Alignment (CNA), a manifold learning
approach to maintain the topology of learned features whereby data points that
are mapped to nearby representations by the source (teacher) model are also
mapped to neighbors by the target (student) model. The target model aims to
mimic the local structure of the source representation space using a
contrastive loss. CNA is an unsupervised learning algorithm that does not
require ground-truth labels for the individual samples. CNA is illustrated in
three scenarios: manifold learning, where the model maintains the local
topology of the original data in a dimension-reduced space; model distillation,
where a small student model is trained to mimic a larger teacher; and legacy
model update, where an older model is replaced by a more powerful one.
Experiments show that CNA is able to capture the manifold in a high-dimensional
space and improves performance compared to the competing methods in their
domains.",2201.01922v1,https://arxiv.org/pdf/2201.01922v1
Revisiting Deep Subspace Alignment for Unsupervised Domain Adaptation,"Kowshik Thopalli, Jayaraman J Thiagarajan, Rushil Anirudh, Pavan K Turaga","Unsupervised domain adaptation (UDA) aims to transfer and adapt knowledge
from a labeled source domain to an unlabeled target domain. Traditionally,
subspace-based methods form an important class of solutions to this problem.
Despite their mathematical elegance and tractability, these methods are often
found to be ineffective at producing domain-invariant features with complex,
real-world datasets. Motivated by the recent advances in representation
learning with deep networks, this paper revisits the use of subspace alignment
for UDA and proposes a novel adaptation algorithm that consistently leads to
improved generalization. In contrast to existing adversarial training-based DA
methods, our approach isolates feature learning and distribution alignment
steps, and utilizes a primary-auxiliary optimization strategy to effectively
balance the objectives of domain invariance and model fidelity. While providing
a significant reduction in target data and computational requirements, our
subspace-based DA performs competitively and sometimes even outperforms
state-of-the-art approaches on several standard UDA benchmarks. Furthermore,
subspace alignment leads to intrinsically well-regularized models that
demonstrate strong generalization even in the challenging partial DA setting.
Finally, the design of our UDA framework inherently supports progressive
adaptation to new target domains at test-time, without requiring retraining of
the model from scratch. In summary, powered by powerful feature learners and an
effective optimization strategy, we establish subspace-based DA as a highly
effective approach for visual recognition.",2201.01806v1,https://arxiv.org/pdf/2201.01806v1
"Aligning Domain-specific Distribution and Classifier for Cross-domain
  Classification from Multiple Sources","Yongchun Zhu, Fuzhen Zhuang, Deqing Wang","While Unsupervised Domain Adaptation (UDA) algorithms, i.e., there are only
labeled data from source domains, have been actively studied in recent years,
most algorithms and theoretical results focus on Single-source Unsupervised
Domain Adaptation (SUDA). However, in the practical scenario, labeled data can
be typically collected from multiple diverse sources, and they might be
different not only from the target domain but also from each other. Thus,
domain adapters from multiple sources should not be modeled in the same way.
Recent deep learning based Multi-source Unsupervised Domain Adaptation (MUDA)
algorithms focus on extracting common domain-invariant representations for all
domains by aligning distribution of all pairs of source and target domains in a
common feature space. However, it is often very hard to extract the same
domain-invariant representations for all domains in MUDA. In addition, these
methods match distributions without considering domain-specific decision
boundaries between classes. To solve these problems, we propose a new framework
with two alignment stages for MUDA which not only respectively aligns the
distributions of each pair of source and target domains in multiple specific
feature spaces, but also aligns the outputs of classifiers by utilizing the
domain-specific decision boundaries. Extensive experiments demonstrate that our
method can achieve remarkable results on popular benchmark datasets for image
classification.",2201.01003v1,https://arxiv.org/pdf/2201.01003v1
Informed Multi-context Entity Alignment,"Kexuan Xin, Zequn Sun, Wen Hua, Wei Hu, Xiaofang Zhou","Entity alignment is a crucial step in integrating knowledge graphs (KGs) from
multiple sources. Previous attempts at entity alignment have explored different
KG structures, such as neighborhood-based and path-based contexts, to learn
entity embeddings, but they are limited in capturing the multi-context
features. Moreover, most approaches directly utilize the embedding similarity
to determine entity alignment without considering the global interaction among
entities and relations. In this work, we propose an Informed Multi-context
Entity Alignment (IMEA) model to address these issues. In particular, we
introduce Transformer to flexibly capture the relation, path, and neighborhood
contexts, and design holistic reasoning to estimate alignment probabilities
based on both embedding similarity and the relation/entity functionality. The
alignment evidence obtained from holistic reasoning is further injected back
into the Transformer via the proposed soft label editing to inform embedding
learning. Experimental results on several benchmark datasets demonstrate the
superiority of our IMEA model compared with existing state-of-the-art entity
alignment methods.",2201.00304v1,https://arxiv.org/pdf/2201.00304v1
"Large Language Models in Medical Term Classification and Unexpected
  Misalignment Between Response and Reasoning","Xiaodan Zhang, Sandeep Vemulapalli, Nabasmita Talukdar, Sumyeong Ahn, Jiankun Wang, Han Meng, Sardar Mehtab Bin Murtaza, Aakash Ajay Dave, Dmitry Leshchiner, Dimitri F. Joseph, Martin Witteveen-Lane, Dave Chesla, Jiayu Zhou, Bin Chen","This study assesses the ability of state-of-the-art large language models
(LLMs) including GPT-3.5, GPT-4, Falcon, and LLaMA 2 to identify patients with
mild cognitive impairment (MCI) from discharge summaries and examines instances
where the models' responses were misaligned with their reasoning. Utilizing the
MIMIC-IV v2.2 database, we focused on a cohort aged 65 and older, verifying MCI
diagnoses against ICD codes and expert evaluations. The data was partitioned
into training, validation, and testing sets in a 7:2:1 ratio for model
fine-tuning and evaluation, with an additional metastatic cancer dataset from
MIMIC III used to further assess reasoning consistency. GPT-4 demonstrated
superior interpretative capabilities, particularly in response to complex
prompts, yet displayed notable response-reasoning inconsistencies. In contrast,
open-source models like Falcon and LLaMA 2 achieved high accuracy but lacked
explanatory reasoning, underscoring the necessity for further research to
optimize both performance and interpretability. The study emphasizes the
significance of prompt engineering and the need for further exploration into
the unexpected reasoning-response misalignment observed in GPT-4. The results
underscore the promise of incorporating LLMs into healthcare diagnostics,
contingent upon methodological advancements to ensure accuracy and clinical
coherence of AI-generated outputs, thereby improving the trustworthiness of
LLMs for medical decision-making.",2312.14184v1,https://arxiv.org/pdf/2312.14184v1
"Automated Parliaments: A Solution to Decision Uncertainty and
  Misalignment in Language Models","Thomas Forster, Jonathan Ouwerx, Shak Ragoler","As AI takes on a greater role in the modern world, it is essential to ensure
that AI models can overcome decision uncertainty and remain aligned with human
morality and interests. This research paper proposes a method for improving the
decision-making of language models (LMs) via Automated Parliaments (APs) -
constructs made of AI delegates each representing a certain perspective.
Delegates themselves consist of three AI models: generators, modifiers, and
evaluators. We specify two mechanisms for producing optimal solutions: the
Simultaneous Modification mechanism for response creation and an evaluation
mechanism for fairly assessing solutions. The overall process begins when each
generator creates a response aligned with its delegate's theory. The modifiers
alter all other responses to make them more self-aligned. The evaluators
collectively assess the best end response. Finally, the modifiers and
generators learn from feedback from the evaluators. In our research, we tested
the evaluation mechanism, comparing the use of single-value zero-shot prompting
and AP few-shot prompting in evaluating morally contentious scenarios. We found
that the AP architecture saw a 57.3% reduction in its loss value compared to
the baseline. We conclude by discussing some potential applications of APs and
specifically their potential impact when implemented as Automated Moral
Parliaments.",2311.10098v1,https://arxiv.org/pdf/2311.10098v1
"A Review of the Evidence for Existential Risk from AI via Misaligned
  Power-Seeking",Rose Hadshar,"Rapid advancements in artificial intelligence (AI) have sparked growing
concerns among experts, policymakers, and world leaders regarding the potential
for increasingly advanced AI systems to pose existential risks. This paper
reviews the evidence for existential risks from AI via misalignment, where AI
systems develop goals misaligned with human values, and power-seeking, where
misaligned AIs actively seek power. The review examines empirical findings,
conceptual arguments and expert opinion relating to specification gaming, goal
misgeneralization, and power-seeking. The current state of the evidence is
found to be concerning but inconclusive regarding the existence of extreme
forms of misaligned power-seeking. Strong empirical evidence of specification
gaming combined with strong conceptual evidence for power-seeking make it
difficult to dismiss the possibility of existential risk from misaligned
power-seeking. On the other hand, to date there are no public empirical
examples of misaligned power-seeking in AI systems, and so arguments that
future systems will pose an existential risk remain somewhat speculative. Given
the current state of the evidence, it is hard to be extremely confident either
that misaligned power-seeking poses a large existential risk, or that it poses
no existential risk. The fact that we cannot confidently rule out existential
risk from AI via misaligned power-seeking is cause for serious concern.",2310.18244v1,https://arxiv.org/pdf/2310.18244v1
"Diagnosing Catastrophe: Large parts of accuracy loss in continual
  learning can be accounted for by readout misalignment","Daniel Anthes, Sushrut Thorat, Peter König, Tim C. Kietzmann","Unlike primates, training artificial neural networks on changing data
distributions leads to a rapid decrease in performance on old tasks. This
phenomenon is commonly referred to as catastrophic forgetting. In this paper,
we investigate the representational changes that underlie this performance
decrease and identify three distinct processes that together account for the
phenomenon. The largest component is a misalignment between hidden
representations and readout layers. Misalignment occurs due to learning on
additional tasks and causes internal representations to shift. Representational
geometry is partially conserved under this misalignment and only a small part
of the information is irrecoverably lost. All types of representational changes
scale with the dimensionality of hidden representations. These insights have
implications for deep learning applications that need to be continuously
updated, but may also aid aligning ANN models to the rather robust biological
vision.",2310.05644v1,https://arxiv.org/pdf/2310.05644v1
"Dimensions of Disagreement: Unpacking Divergence and Misalignment in
  Cognitive Science and Artificial Intelligence","Kerem Oktar, Ilia Sucholutsky, Tania Lombrozo, Thomas L. Griffiths","The increasing prevalence of artificial agents creates a correspondingly
increasing need to manage disagreements between humans and artificial agents,
as well as between artificial agents themselves. Considering this larger space
of possible agents exposes an opportunity for furthering our understanding of
the nature of disagreement: past studies in psychology have often cast
disagreement as two agents forming diverging evaluations of the same object,
but disagreement can also arise from differences in how agents represent that
object. AI research on human-machine alignment and recent work in computational
cognitive science have focused on this latter kind of disagreement, and have
developed tools that can be used to quantify the extent of representational
overlap between agents. Understanding how divergence and misalignment interact
to produce disagreement, and how resolution strategies depend on this
interaction, is key to promoting effective collaboration between diverse types
of agents.",2310.12994v1,https://arxiv.org/pdf/2310.12994v1
"MISFIT-V: Misaligned Image Synthesis and Fusion using Information from
  Thermal and Visual","Aadhar Chauhan, Isaac Remy, Danny Broyles, Karen Leung","Detecting humans from airborne visual and thermal imagery is a fundamental
challenge for Wilderness Search-and-Rescue (WiSAR) teams, who must perform this
function accurately in the face of immense pressure. The ability to fuse these
two sensor modalities can potentially reduce the cognitive load on human
operators and/or improve the effectiveness of computer vision object detection
models. However, the fusion task is particularly challenging in the context of
WiSAR due to hardware limitations and extreme environmental factors. This work
presents Misaligned Image Synthesis and Fusion using Information from Thermal
and Visual (MISFIT-V), a novel two-pronged unsupervised deep learning approach
that utilizes a Generative Adversarial Network (GAN) and a cross-attention
mechanism to capture the most relevant features from each modality.
Experimental results show MISFIT-V offers enhanced robustness against
misalignment and poor lighting/thermal environmental conditions compared to
existing visual-thermal image fusion methods.",2309.13216v1,https://arxiv.org/pdf/2309.13216v1
"Interpretability Benchmark for Evaluating Spatial Misalignment of
  Prototypical Parts Explanations","Mikołaj Sacha, Bartosz Jura, Dawid Rymarczyk, Łukasz Struski, Jacek Tabor, Bartosz Zieliński","Prototypical parts-based networks are becoming increasingly popular due to
their faithful self-explanations. However, their similarity maps are calculated
in the penultimate network layer. Therefore, the receptive field of the
prototype activation region often depends on parts of the image outside this
region, which can lead to misleading interpretations. We name this undesired
behavior a spatial explanation misalignment and introduce an interpretability
benchmark with a set of dedicated metrics for quantifying this phenomenon. In
addition, we propose a method for misalignment compensation and apply it to
existing state-of-the-art models. We show the expressiveness of our benchmark
and the effectiveness of the proposed compensation methodology through
extensive empirical studies.",2308.08162v1,https://arxiv.org/pdf/2308.08162v1
"DoReMi: Grounding Language Model by Detecting and Recovering from
  Plan-Execution Misalignment","Yanjiang Guo, Yen-Jen Wang, Lihan Zha, Zheyuan Jiang, Jianyu Chen","Large language models (LLMs) encode a vast amount of semantic knowledge and
possess remarkable understanding and reasoning capabilities. Previous work has
explored how to ground LLMs in robotic tasks to generate feasible and
executable textual plans. However, low-level execution in the physical world
may deviate from the high-level textual plan due to environmental perturbations
or imperfect controller design. In this paper, we propose \textbf{DoReMi}, a
novel language model grounding framework that enables immediate Detection and
Recovery from Misalignments between plan and execution. Specifically, we
leverage LLMs to play a dual role, aiding not only in high-level planning but
also generating constraints that can indicate misalignment during execution.
Then vision language models (VLMs) are utilized to detect constraint violations
continuously. Our pipeline can monitor the low-level execution and enable
timely recovery if certain plan-execution misalignment occurs. Experiments on
various complex tasks including robot arms and humanoid robots demonstrate that
our method can lead to higher task success rates and shorter task completion
times. Videos of DoReMi are available at
\url{https://sites.google.com/view/doremi-paper}.",2307.00329v3,https://arxiv.org/pdf/2307.00329v3
"Investigating the Effect of Misalignment on Membership Privacy in the
  White-box Setting","Ana-Maria Cretu, Daniel Jones, Yves-Alexandre de Montjoye, Shruti Tople","Machine learning models have been shown to leak sensitive information about
their training datasets. Models are increasingly deployed on devices, raising
concerns that white-box access to the model parameters increases the attack
surface compared to black-box access which only provides query access. Directly
extending the shadow modelling technique from the black-box to the white-box
setting has been shown, in general, not to perform better than black-box only
attacks. A potential reason is misalignment, a known characteristic of deep
neural networks. In the shadow modelling context, misalignment means that,
while the shadow models learn similar features in each layer, the features are
located in different positions. We here present the first systematic analysis
of the causes of misalignment in shadow models and show the use of a different
weight initialisation to be the main cause. We then extend several re-alignment
techniques, previously developed in the model fusion literature, to the shadow
modelling context, where the goal is to re-align the layers of a shadow model
to those of the target model. We show re-alignment techniques to significantly
reduce the measured misalignment between the target and shadow models. Finally,
we perform a comprehensive evaluation of white-box membership inference attacks
(MIA). Our analysis reveals that internal layer activation-based MIAs suffer
strongly from shadow model misalignment, while gradient-based MIAs are only
sometimes significantly affected. We show that re-aligning the shadow models
strongly improves the former's performance and can also improve the latter's
performance, although less frequently. Taken together, our results highlight
that on-device deployment increases the attack surface and that the newly
available information can be used to build more powerful attacks.",2306.05093v2,https://arxiv.org/pdf/2306.05093v2
"PAGAR: Taming Reward Misalignment in Inverse Reinforcement
  Learning-Based Imitation Learning with Protagonist Antagonist Guided
  Adversarial Reward","Weichao Zhou, Wenchao Li","Many imitation learning (IL) algorithms employ inverse reinforcement learning
(IRL) to infer the intrinsic reward function that an expert is implicitly
optimizing for based on their demonstrated behaviors. However, in practice,
IRL-based IL can fail to accomplish the underlying task due to a misalignment
between the inferred reward and the objective of the task. In this paper, we
address the susceptibility of IL to such misalignment by introducing a
semi-supervised reward design paradigm called Protagonist Antagonist Guided
Adversarial Reward (PAGAR). PAGAR-based IL trains a policy to perform well
under mixed reward functions instead of a single reward function as in
IRL-based IL. We identify the theoretical conditions under which PAGAR-based IL
can avoid the task failures caused by reward misalignment. We also present a
practical on-and-off policy approach to implementing PAGAR-based IL.
Experimental results show that our algorithm outperforms standard IL baselines
in complex tasks and challenging transfer settings.",2306.01731v3,https://arxiv.org/pdf/2306.01731v3
Query-Policy Misalignment in Preference-Based Reinforcement Learning,"Xiao Hu, Jianxiong Li, Xianyuan Zhan, Qing-Shan Jia, Ya-Qin Zhang","Preference-based reinforcement learning (PbRL) provides a natural way to
align RL agents' behavior with human desired outcomes, but is often restrained
by costly human feedback. To improve feedback efficiency, most existing PbRL
methods focus on selecting queries to maximally improve the overall quality of
the reward model, but counter-intuitively, we find that this may not
necessarily lead to improved performance. To unravel this mystery, we identify
a long-neglected issue in the query selection schemes of existing PbRL studies:
Query-Policy Misalignment. We show that the seemingly informative queries
selected to improve the overall quality of reward model actually may not align
with RL agents' interests, thus offering little help on policy learning and
eventually resulting in poor feedback efficiency. We show that this issue can
be effectively addressed via near on-policy query and a specially designed
hybrid experience replay, which together enforce the bidirectional query-policy
alignment. Simple yet elegant, our method can be easily incorporated into
existing approaches by changing only a few lines of code. We showcase in
comprehensive experiments that our method achieves substantial gains in both
human feedback and RL sample efficiency, demonstrating the importance of
addressing query-policy misalignment in PbRL tasks.",2305.17400v3,https://arxiv.org/pdf/2305.17400v3
"High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled
  Conditions","Sangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan Choi, Jaegul Choo","Image-based virtual try-on aims to synthesize an image of a person wearing a
given clothing item. To solve the task, the existing methods warp the clothing
item to fit the person's body and generate the segmentation map of the person
wearing the item before fusing the item with the person. However, when the
warping and the segmentation generation stages operate individually without
information exchange, the misalignment between the warped clothes and the
segmentation map occurs, which leads to the artifacts in the final image. The
information disconnection also causes excessive warping near the clothing
regions occluded by the body parts, so-called pixel-squeezing artifacts. To
settle the issues, we propose a novel try-on condition generator as a unified
module of the two stages (i.e., warping and segmentation generation stages). A
newly proposed feature fusion block in the condition generator implements the
information exchange, and the condition generator does not create any
misalignment or pixel-squeezing artifacts. We also introduce discriminator
rejection that filters out the incorrect segmentation map predictions and
assures the performance of virtual try-on frameworks. Experiments on a
high-resolution dataset demonstrate that our model successfully handles the
misalignment and occlusion, and significantly outperforms the baselines. Code
is available at https://github.com/sangyun884/HR-VITON.",2206.14180v2,https://arxiv.org/pdf/2206.14180v2
Disparate Impact in Differential Privacy from Gradient Misalignment,"Maria S. Esipova, Atiyeh Ashari Ghomi, Yaqiao Luo, Jesse C. Cresswell","As machine learning becomes more widespread throughout society, aspects
including data privacy and fairness must be carefully considered, and are
crucial for deployment in highly regulated industries. Unfortunately, the
application of privacy enhancing technologies can worsen unfair tendencies in
models. In particular, one of the most widely used techniques for private model
training, differentially private stochastic gradient descent (DPSGD),
frequently intensifies disparate impact on groups within data. In this work we
study the fine-grained causes of unfairness in DPSGD and identify gradient
misalignment due to inequitable gradient clipping as the most significant
source. This observation leads us to a new method for reducing unfairness by
preventing gradient misalignment in DPSGD.",2206.07737v2,https://arxiv.org/pdf/2206.07737v2
"The Effects of Reward Misspecification: Mapping and Mitigating
  Misaligned Models","Alexander Pan, Kush Bhatia, Jacob Steinhardt","Reward hacking -- where RL agents exploit gaps in misspecified reward
functions -- has been widely observed, but not yet systematically studied. To
understand how reward hacking arises, we construct four RL environments with
misspecified rewards. We investigate reward hacking as a function of agent
capabilities: model capacity, action space resolution, observation space noise,
and training time. More capable agents often exploit reward misspecifications,
achieving higher proxy reward and lower true reward than less capable agents.
Moreover, we find instances of phase transitions: capability thresholds at
which the agent's behavior qualitatively shifts, leading to a sharp decrease in
the true reward. Such phase transitions pose challenges to monitoring the
safety of ML systems. To address this, we propose an anomaly detection task for
aberrant policies and offer several baseline detectors.",2201.03544v2,https://arxiv.org/pdf/2201.03544v2
"SAFE-SIM: Safety-Critical Closed-Loop Traffic Simulation with
  Diffusion-Controllable Adversaries","Wei-Jer Chang, Francesco Pittaluga, Masayoshi Tomizuka, Wei Zhan, Manmohan Chandraker","Evaluating the performance of autonomous vehicle planning algorithms
necessitates simulating long-tail safety-critical traffic scenarios. However,
traditional methods for generating such scenarios often fall short in terms of
controllability and realism; they also neglect the dynamics of agent
interactions. To address these limitations, we introduce SAFE-SIM, a novel
diffusion-based controllable closed-loop safety-critical simulation framework.
Our approach yields two distinct advantages: 1) generating realistic long-tail
safety-critical scenarios that closely reflect real-world conditions, and 2)
providing controllable adversarial behavior for more comprehensive and
interactive evaluations. We develop a novel approach to simulate
safety-critical scenarios through an adversarial term in the denoising process
of diffusion models, which allows an adversarial agent to challenge a planner
with plausible maneuvers while all agents in the scene exhibit reactive and
realistic behaviors. Furthermore, we propose novel guidance objectives and a
partial diffusion process that enables users to control key aspects of the
scenarios, such as the collision type and aggressiveness of the adversarial
agent, while maintaining the realism of the behavior. We validate our framework
empirically using the nuScenes and nuPlan datasets across multiple planners,
demonstrating improvements in both realism and controllability. These findings
affirm that diffusion models provide a robust and versatile foundation for
safety-critical, interactive traffic simulation, extending their utility across
the broader autonomous driving landscape. Project website:
https://safe-sim.github.io/.",2401.00391v3,https://arxiv.org/pdf/2401.00391v3
"What's my role? Modelling responsibility for AI-based safety-critical
  systems","Philippa Ryan, Zoe Porter, Joanna Al-Qaddoumi, John McDermid, Ibrahim Habli","AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in
the real world. These can pose a risk of harm to people and the environment.
Reducing that risk is an overarching priority during development and operation.
As more AI-SCS become autonomous, a layer of risk management via human
intervention has been removed. Following an accident it will be important to
identify causal contributions and the different responsible actors behind those
to learn from mistakes and prevent similar future events. Many authors have
commented on the ""responsibility gap"" where it is difficult for developers and
manufacturers to be held responsible for harmful behaviour of an AI-SCS. This
is due to the complex development cycle for AI, uncertainty in AI performance,
and dynamic operating environment. A human operator can become a ""liability
sink"" absorbing blame for the consequences of AI-SCS outputs they weren't
responsible for creating, and may not have understanding of.
  This cross-disciplinary paper considers different senses of responsibility
(role, moral, legal and causal), and how they apply in the context of AI-SCS
safety. We use a core concept (Actor(A) is responsible for Occurrence(O)) to
create role responsibility models, producing a practical method to capture
responsibility relationships and provide clarity on the previously identified
responsibility issues. Our paper demonstrates the approach with two examples: a
retrospective analysis of the Tempe Arizona fatal collision involving an
autonomous vehicle, and a safety focused predictive role-responsibility
analysis for an AI-based diabetes co-morbidity predictor. In both examples our
primary focus is on safety, aiming to reduce unfair or disproportionate blame
being placed on operators or developers. We present a discussion and avenues
for future research.",2401.09459v1,https://arxiv.org/pdf/2401.09459v1
"Reinforcement Learning for Safe Occupancy Strategies in Educational
  Spaces during an Epidemic","Elizabeth Akinyi Ondula, Bhaskar Krishnamachari","Epidemic modeling, encompassing deterministic and stochastic approaches, is
vital for understanding infectious diseases and informing public health
strategies. This research adopts a prescriptive approach, focusing on
reinforcement learning (RL) to develop strategies that balance minimizing
infections with maximizing in-person interactions in educational settings. We
introduce SafeCampus , a novel tool that simulates infection spread and
facilitates the exploration of various RL algorithms in response to epidemic
challenges. SafeCampus incorporates a custom RL environment, informed by
stochastic epidemic models, to realistically represent university campus
dynamics during epidemics. We evaluate Q-learning for a discretized state space
which resulted in a policy matrix that not only guides occupancy decisions
under varying epidemic conditions but also illustrates the inherent trade-off
in epidemic management. This trade-off is characterized by the dilemma between
stricter measures, which may effectively reduce infections but impose less
educational benefit (more in-person interactions), and more lenient policies,
which could lead to higher infection rates.",2312.15163v1,https://arxiv.org/pdf/2312.15163v1
Gradient Shaping for Multi-Constraint Safe Reinforcement Learning,"Yihang Yao, Zuxin Liu, Zhepeng Cen, Peide Huang, Tingnan Zhang, Wenhao Yu, Ding Zhao","Online safe reinforcement learning (RL) involves training a policy that
maximizes task efficiency while satisfying constraints via interacting with the
environments. In this paper, our focus lies in addressing the complex
challenges associated with solving multi-constraint (MC) safe RL problems. We
approach the safe RL problem from the perspective of Multi-Objective
Optimization (MOO) and propose a unified framework designed for MC safe RL
algorithms. This framework highlights the manipulation of gradients derived
from constraints. Leveraging insights from this framework and recognizing the
significance of \textit{redundant} and \textit{conflicting} constraint
conditions, we introduce the Gradient Shaping (GradS) method for general
Lagrangian-based safe RL algorithms to improve the training efficiency in terms
of both reward and constraint satisfaction. Our extensive experimentation
demonstrates the effectiveness of our proposed method in encouraging
exploration and learning a policy that improves both safety and reward
performance across various challenging MC safe RL tasks as well as good
scalability to the number of constraints.",2312.15127v1,https://arxiv.org/pdf/2312.15127v1
"Safe Reinforcement Learning with Instantaneous Constraints: The Role of
  Aggressive Exploration","Honghao Wei, Xin Liu, Lei Ying","This paper studies safe Reinforcement Learning (safe RL) with linear function
approximation and under hard instantaneous constraints where unsafe actions
must be avoided at each step. Existing studies have considered safe RL with
hard instantaneous constraints, but their approaches rely on several key
assumptions: $(i)$ the RL agent knows a safe action set for {\it every} state
or knows a {\it safe graph} in which all the state-action-state triples are
safe, and $(ii)$ the constraint/cost functions are {\it linear}. In this paper,
we consider safe RL with instantaneous hard constraints without assumption
$(i)$ and generalize $(ii)$ to Reproducing Kernel Hilbert Space (RKHS). Our
proposed algorithm, LSVI-AE, achieves $\tilde{\cO}(\sqrt{d^3H^4K})$ regret and
$\tilde{\cO}(H \sqrt{dK})$ hard constraint violation when the cost function is
linear and $\cO(H\gamma_K \sqrt{K})$ hard constraint violation when the cost
function belongs to RKHS. Here $K$ is the learning horizon, $H$ is the length
of each episode, and $\gamma_K$ is the information gain w.r.t the kernel used
to approximate cost functions. Our results achieve the optimal dependency on
the learning horizon $K$, matching the lower bound we provide in this paper and
demonstrating the efficiency of LSVI-AE. Notably, the design of our approach
encourages aggressive policy exploration, providing a unique perspective on
safe RL with general cost functions and no prior knowledge of safe actions,
which may be of independent interest.",2312.14470v1,https://arxiv.org/pdf/2312.14470v1
Bypassing the Safety Training of Open-Source LLMs with Priming Attacks,"Jason Vega, Isha Chaudhary, Changming Xu, Gagandeep Singh","With the recent surge in popularity of LLMs has come an ever-increasing need
for LLM safety training. In this paper, we investigate the fragility of SOTA
open-source LLMs under simple, optimization-free attacks we refer to as
$\textit{priming attacks}$, which are easy to execute and effectively bypass
alignment from safety training. Our proposed attack improves the Attack Success
Rate on Harmful Behaviors, as measured by Llama Guard, by up to $3.3\times$
compared to baselines. Source code and data are available at
https://github.com/uiuc-focal-lab/llm-priming-attacks.",2312.12321v2,https://arxiv.org/pdf/2312.12321v2
"Concrete Problems in AI Safety, Revisited","Inioluwa Deborah Raji, Roel Dobbe","As AI systems proliferate in society, the AI community is increasingly
preoccupied with the concept of AI Safety, namely the prevention of failures
due to accidents that arise from an unanticipated departure of a system's
behavior from designer intent in AI deployment. We demonstrate through an
analysis of real world cases of such incidents that although current vocabulary
captures a range of the encountered issues of AI deployment, an expanded
socio-technical framing will be required for a more complete understanding of
how AI systems and implemented safety mechanisms fail and succeed in real life.",2401.10899v1,https://arxiv.org/pdf/2401.10899v1
"Safeguarded Progress in Reinforcement Learning: Safe Bayesian
  Exploration for Control Policy Synthesis","Rohan Mitta, Hosein Hasanbeig, Jun Wang, Daniel Kroening, Yiannis Kantaros, Alessandro Abate","This paper addresses the problem of maintaining safety during training in
Reinforcement Learning (RL), such that the safety constraint violations are
bounded at any point during learning. In a variety of RL applications the
safety of the agent is particularly important, e.g. autonomous platforms or
robots that work in proximity of humans. As enforcing safety during training
might severely limit the agent's exploration, we propose here a new
architecture that handles the trade-off between efficient progress and safety
during exploration. As the exploration progresses, we update via Bayesian
inference Dirichlet-Categorical models of the transition probabilities of the
Markov decision process that describes the environment dynamics. This paper
proposes a way to approximate moments of belief about the risk associated to
the action selection policy. We construct those approximations, and prove the
convergence results. We propose a novel method for leveraging the expectation
approximations to derive an approximate bound on the confidence that the risk
is below a certain level. This approach can be easily interleaved with RL and
we present experimental results to showcase the performance of the overall
architecture.",2312.11314v1,https://arxiv.org/pdf/2312.11314v1
"AI-Based Energy Transportation Safety: Pipeline Radial Threat Estimation
  Using Intelligent Sensing System","Chengyuan Zhu, Yiyuan Yang, Kaixiang Yang, Haifeng Zhang, Qinmin Yang, C. L. Philip Chen","The application of artificial intelligence technology has greatly enhanced
and fortified the safety of energy pipelines, particularly in safeguarding
against external threats. The predominant methods involve the integration of
intelligent sensors to detect external vibration, enabling the identification
of event types and locations, thereby replacing manual detection methods.
However, practical implementation has exposed a limitation in current methods -
their constrained ability to accurately discern the spatial dimensions of
external signals, which complicates the authentication of threat events. Our
research endeavors to overcome the above issues by harnessing deep learning
techniques to achieve a more fine-grained recognition and localization process.
This refinement is crucial in effectively identifying genuine threats to
pipelines, thus enhancing the safety of energy transportation. This paper
proposes a radial threat estimation method for energy pipelines based on
distributed optical fiber sensing technology. Specifically, we introduce a
continuous multi-view and multi-domain feature fusion methodology to extract
comprehensive signal features and construct a threat estimation and recognition
network. The utilization of collected acoustic signal data is optimized, and
the underlying principle is elucidated. Moreover, we incorporate the concept of
transfer learning through a pre-trained model, enhancing both recognition
accuracy and training efficiency. Empirical evidence gathered from real-world
scenarios underscores the efficacy of our method, notably in its substantial
reduction of false alarms and remarkable gains in recognition accuracy. More
generally, our method exhibits versatility and can be extrapolated to a broader
spectrum of recognition tasks and scenarios.",2312.11583v2,https://arxiv.org/pdf/2312.11583v2
"Imitate the Good and Avoid the Bad: An Incremental Approach to Safe
  Reinforcement Learning","Huy Hoang, Tien Mai, Pradeep Varakantham","A popular framework for enforcing safe actions in Reinforcement Learning (RL)
is Constrained RL, where trajectory based constraints on expected cost (or
other cost measures) are employed to enforce safety and more importantly these
constraints are enforced while maximizing expected reward. Most recent
approaches for solving Constrained RL convert the trajectory based cost
constraint into a surrogate problem that can be solved using minor
modifications to RL methods. A key drawback with such approaches is an over or
underestimation of the cost constraint at each state. Therefore, we provide an
approach that does not modify the trajectory based cost constraint and instead
imitates ``good'' trajectories and avoids ``bad'' trajectories generated from
incrementally improving policies. We employ an oracle that utilizes a reward
threshold (which is varied with learning) and the overall cost constraint to
label trajectories as ``good'' or ``bad''. A key advantage of our approach is
that we are able to work from any starting policy or set of trajectories and
improve on it. In an exhaustive set of experiments, we demonstrate that our
approach is able to outperform top benchmark approaches for solving Constrained
RL problems, with respect to expected cost, CVaR cost, or even unknown cost
constraints.",2312.10385v4,https://arxiv.org/pdf/2312.10385v4
"Constrained Meta-Reinforcement Learning for Adaptable Safety Guarantee
  with Differentiable Convex Programming","Minjae Cho, Chuangchuang Sun","Despite remarkable achievements in artificial intelligence, the deployability
of learning-enabled systems in high-stakes real-world environments still faces
persistent challenges. For example, in safety-critical domains like autonomous
driving, robotic manipulation, and healthcare, it is crucial not only to
achieve high performance but also to comply with given constraints.
Furthermore, adaptability becomes paramount in non-stationary domains, where
environmental parameters are subject to change. While safety and adaptability
are recognized as key qualities for the new generation of AI, current
approaches have not demonstrated effective adaptable performance in constrained
settings. Hence, this paper breaks new ground by studying the unique challenges
of ensuring safety in non-stationary environments by solving constrained
problems through the lens of the meta-learning approach (learning-to-learn).
While unconstrained meta-learning al-ready encounters complexities in
end-to-end differentiation of the loss due to the bi-level nature, its
constrained counterpart introduces an additional layer of difficulty, since the
constraints imposed on task-level updates complicate the differentiation
process. To address the issue, we first employ successive convex-constrained
policy updates across multiple tasks with differentiable convexprogramming,
which allows meta-learning in constrained scenarios by enabling end-to-end
differentiation. This approach empowers the agent to rapidly adapt to new tasks
under non-stationarity while ensuring compliance with safety constraints.",2312.10230v1,https://arxiv.org/pdf/2312.10230v1
"Sequence adaptive field-imperfection estimation (SAFE): retrospective
  estimation and correction of $B_1^+$ and $B_0$ inhomogeneities for enhanced
  MRF quantification","Mengze Gao, Xiaozhi Cao, Daniel Abraham, Zihan Zhou, Kawin Setsompop","$B_1^+$ and $B_0$ field-inhomogeneities can significantly reduce accuracy and
robustness of MRF's quantitative parameter estimates. Additional $B_1^+$ and
$B_0$ calibration scans can mitigate this but add scan time and cannot be
applied retrospectively to previously collected data. Here, we proposed a
calibration-free sequence-adaptive deep-learning framework, to estimate and
correct for $B_1^+$ and $B_0$ effects of any MRF sequence. We demonstrate its
capability on arbitrary MRF sequences at 3T, where no training data were
previously obtained. Such approach can be applied to any previously-acquired
and future MRF-scans. The flexibility in directly applying this framework to
other quantitative sequences is also highlighted.",2312.09488v1,https://arxiv.org/pdf/2312.09488v1
"Learning Safety Constraints From Demonstration Using One-Class Decision
  Trees","Mattijs Baert, Sam Leroux, Pieter Simoens","The alignment of autonomous agents with human values is a pivotal challenge
when deploying these agents within physical environments, where safety is an
important concern. However, defining the agent's objective as a reward and/or
cost function is inherently complex and prone to human errors. In response to
this challenge, we present a novel approach that leverages one-class decision
trees to facilitate learning from expert demonstrations. These decision trees
provide a foundation for representing a set of constraints pertinent to the
given environment as a logical formula in disjunctive normal form. The learned
constraints are subsequently employed within an oracle constrained
reinforcement learning framework, enabling the acquisition of a safe policy. In
contrast to other methods, our approach offers an interpretable representation
of the constraints, a vital feature in safety-critical environments. To
validate the effectiveness of our proposed method, we conduct experiments in
synthetic benchmark domains and a realistic driving environment.",2312.08837v1,https://arxiv.org/pdf/2312.08837v1
ChatSOS: LLM-based knowledge Q&A system for safety engineering,"Haiyang Tang, Zhenyi Liu, Dongping Chen, Qingzhao Chu","Recent advancements in large language models (LLMs) have notably propelled
natural language processing (NLP) capabilities, demonstrating significant
potential in safety engineering applications. Despite these advancements, LLMs
face constraints in processing specialized tasks, attributed to factors such as
corpus size, input processing limitations, and privacy concerns. Obtaining
useful information from reliable sources in a limited time is crucial for LLM.
Addressing this, our study introduces an LLM-based Q&A system for safety
engineering, enhancing the comprehension and response accuracy of the model. We
employed prompt engineering to incorporate external knowledge databases, thus
enriching the LLM with up-to-date and reliable information. The system analyzes
historical incident reports through statistical methods, utilizes vector
embedding to construct a vector database, and offers an efficient
similarity-based search functionality. Our findings indicate that the
integration of external knowledge significantly augments the capabilities of
LLM for in-depth problem analysis and autonomous task assignment. It
effectively summarizes accident reports and provides pertinent recommendations.
This integration approach not only expands LLM applications in safety
engineering but also sets a precedent for future developments towards
automation and intelligent systems.",2312.08629v1,https://arxiv.org/pdf/2312.08629v1
Towards Safe Multi-Task Bayesian Optimization,"Jannis O. Lübsen, Christian Hespe, Annika Eichler","Bayesian optimization has emerged as a highly effective tool for the safe
online optimization of systems, due to its high sample efficiency and noise
robustness. To further enhance its efficiency, reduced physical models of the
system can be incorporated into the optimization process, accelerating it.
These models are able to offer an approximation of the actual system, and
evaluating them is significantly cheaper. The similarity between the model and
reality is represented by additional hyperparameters, which are learned within
the optimization process. Safety is a crucial criterion for online optimization
methods such as Bayesian optimization, which has been addressed by recent works
that provide safety guarantees under the assumption of known hyperparameters.
In practice, however, this does not apply. Therefore, we extend the robust
Gaussian process uniform error bounds to meet the multi-task setting, which
involves the calculation of a confidence region from the hyperparameter
posterior distribution utilizing Markov chain Monte Carlo methods.
Subsequently, the robust safety bounds are employed to facilitate the safe
optimization of the system, while incorporating measurements of the models.
Simulation results indicate that the optimization can be significantly
accelerated for expensive to evaluate functions in comparison to other
state-of-the-art safe Bayesian optimization methods, contingent on the fidelity
of the models.",2312.07281v3,https://arxiv.org/pdf/2312.07281v3
"Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety
  Filters of Text-to-Image Models","Yimo Deng, Huangxun Chen","Text-to-image (TTI) models offer many innovative services but also raise
ethical concerns due to their potential to generate unethical images. Most
public TTI services employ safety filters to prevent unintended images. In this
work, we introduce the Divide-and-Conquer Attack to circumvent the safety
filters of state-of the-art TTI models, including DALL-E 3 and Midjourney. Our
attack leverages LLMs as text transformation agents to create adversarial
prompts. We design attack helper prompts that effectively guide LLMs to break
down an unethical drawing intent into multiple benign descriptions of
individual image elements, allowing them to bypass safety filters while still
generating unethical images. Because the latent harmful meaning only becomes
apparent when all individual elements are drawn together. Our evaluation
demonstrates that our attack successfully circumvents multiple strong
closed-box safety filters. The comprehensive success rate of DACA bypassing the
safety filters of the state-of-the-art TTI engine DALL-E 3 is above 85%, while
the success rate for bypassing Midjourney V6 exceeds 75%. Our findings have
more severe security implications than methods of manual crafting or iterative
TTI model querying due to lower attack barrier, enhanced interpretability , and
better adaptation to defense. Our prototype is available at:
https://github.com/researchcode001/Divide-and-Conquer-Attack",2312.07130v3,https://arxiv.org/pdf/2312.07130v3
AI Control: Improving Safety Despite Intentional Subversion,"Ryan Greenblatt, Buck Shlegeris, Kshitij Sachan, Fabien Roger","As large language models (LLMs) become more powerful and are deployed more
autonomously, it will be increasingly important to prevent them from causing
harmful outcomes. Researchers have investigated a variety of safety techniques
for this purpose, e.g. using models to review the outputs of other models, or
red-teaming techniques to surface subtle failure modes. However, researchers
have not evaluated whether such techniques still ensure safety if the model is
itself intentionally trying to subvert them. In this paper, we develop and
evaluate pipelines of safety techniques (""protocols"") that are robust to
intentional subversion.
  We investigate a scenario in which we want to solve a sequence of programming
problems, using access to a powerful but untrusted model (in our case, GPT-4),
access to a less powerful trusted model (in our case, GPT-3.5), and limited
access to high-quality trusted labor. We investigate protocols that aim to
never submit solutions containing backdoors, which we operationalize here as
logical errors that are not caught by test cases.
  We investigate a range of protocols and test each against strategies that the
untrusted model could use to subvert them. One protocol is what we call trusted
editing. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to
rate the suspiciousness of that code. If the code is below some suspiciousness
threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove
parts that seem suspicious and then submits the edited code. Another protocol
is untrusted monitoring. This protocol asks GPT-4 to write code, and then asks
another instance of GPT-4 whether the code is backdoored, using various
techniques to prevent the GPT-4 instances from colluding. These protocols
improve substantially on simple baselines.",2312.06942v5,https://arxiv.org/pdf/2312.06942v5
"System-level Safety Guard: Safe Tracking Control through Uncertain
  Neural Network Dynamics Models","Xiao Li, Yutong Li, Anouck Girard, Ilya Kolmanovsky","The Neural Network (NN), as a black-box function approximator, has been
considered in many control and robotics applications. However, difficulties in
verifying the overall system safety in the presence of uncertainties hinder the
deployment of NN modules in safety-critical systems. In this paper, we leverage
the NNs as predictive models for trajectory tracking of unknown dynamical
systems. We consider controller design in the presence of both intrinsic
uncertainty and uncertainties from other system modules. In this setting, we
formulate the constrained trajectory tracking problem and show that it can be
solved using Mixed-integer Linear Programming (MILP). The proposed MILP-based
approach is empirically demonstrated in robot navigation and obstacle avoidance
through simulations. The demonstration videos are available at
https://xiaolisean.github.io/publication/2023-11-01-L4DC2024.",2312.06810v2,https://arxiv.org/pdf/2312.06810v2
"Compute-in-Memory based Neural Network Accelerators for Safety-Critical
  Systems: Worst-Case Scenarios and Protections","Zheyu Yan, Xiaobo Sharon Hu, Yiyu Shi","Emerging non-volatile memory (NVM)-based Computing-in-Memory (CiM)
architectures show substantial promise in accelerating deep neural networks
(DNNs) due to their exceptional energy efficiency. However, NVM devices are
prone to device variations. Consequently, the actual DNN weights mapped to NVM
devices can differ considerably from their targeted values, inducing
significant performance degradation. Many existing solutions aim to optimize
average performance amidst device variations, which is a suitable strategy for
general-purpose conditions. However, the worst-case performance that is crucial
for safety-critical applications is largely overlooked in current research. In
this study, we define the problem of pinpointing the worst-case performance of
CiM DNN accelerators affected by device variations. Additionally, we introduce
a strategy to identify a specific pattern of the device value deviations in the
complex, high-dimensional value deviation space, responsible for this
worst-case outcome. Our findings reveal that even subtle device variations can
precipitate a dramatic decline in DNN accuracy, posing risks for CiM-based
platforms in supporting safety-critical applications. Notably, we observe that
prevailing techniques to bolster average DNN performance in CiM accelerators
fall short in enhancing worst-case scenarios. In light of this issue, we
propose a novel worst-case-aware training technique named A-TRICE that
efficiently combines adversarial training and noise-injection training with
right-censored Gaussian noise to improve the DNN accuracy in the worst-case
scenarios. Our experimental results demonstrate that A-TRICE improves the
worst-case accuracy under device variations by up to 33%.",2312.06137v1,https://arxiv.org/pdf/2312.06137v1
GPT-4 and Safety Case Generation: An Exploratory Analysis,"Mithila Sivakumar, Alvine Boaye Belle, Jinjun Shan, Kimya Khakzad Shahandashti","In the ever-evolving landscape of software engineering, the emergence of
large language models (LLMs) and conversational interfaces, exemplified by
ChatGPT, is nothing short of revolutionary. While their potential is undeniable
across various domains, this paper sets out on a captivating expedition to
investigate their uncharted territory, the exploration of generating safety
cases. In this paper, our primary objective is to delve into the existing
knowledge base of GPT-4, focusing specifically on its understanding of the Goal
Structuring Notation (GSN), a well-established notation allowing to visually
represent safety cases. Subsequently, we perform four distinct experiments with
GPT-4. These experiments are designed to assess its capacity for generating
safety cases within a defined system and application domain. To measure the
performance of GPT-4 in this context, we compare the results it generates with
ground-truth safety cases created for an X-ray system system and a
Machine-Learning (ML)-enabled component for tire noise recognition (TNR) in a
vehicle. This allowed us to gain valuable insights into the model's generative
capabilities. Our findings indicate that GPT-4 demonstrates the capacity to
produce safety arguments that are moderately accurate and reasonable.
Furthermore, it exhibits the capability to generate safety cases that closely
align with the semantic content of the reference safety cases used as
ground-truths in our experiments.",2312.05696v1,https://arxiv.org/pdf/2312.05696v1
Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations,"Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, Madian Khabsa","We introduce Llama Guard, an LLM-based input-output safeguard model geared
towards Human-AI conversation use cases. Our model incorporates a safety risk
taxonomy, a valuable tool for categorizing a specific set of safety risks found
in LLM prompts (i.e., prompt classification). This taxonomy is also
instrumental in classifying the responses generated by LLMs to these prompts, a
process we refer to as response classification. For the purpose of both prompt
and response classification, we have meticulously gathered a dataset of high
quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our
collected dataset, albeit low in volume, demonstrates strong performance on
existing benchmarks such as the OpenAI Moderation Evaluation dataset and
ToxicChat, where its performance matches or exceeds that of currently available
content moderation tools. Llama Guard functions as a language model, carrying
out multi-class classification and generating binary decision scores.
Furthermore, the instruction fine-tuning of Llama Guard allows for the
customization of tasks and the adaptation of output formats. This feature
enhances the model's capabilities, such as enabling the adjustment of taxonomy
categories to align with specific use cases, and facilitating zero-shot or
few-shot prompting with diverse taxonomies at the input. We are making Llama
Guard model weights available and we encourage researchers to further develop
and adapt them to meet the evolving needs of the community for AI safety.",2312.06674v1,https://arxiv.org/pdf/2312.06674v1
"Data-driven Semi-supervised Machine Learning with Surrogate Safety
  Measures for Abnormal Driving Behavior Detection","Yongqi Dong, Lanxin Zhang, Haneen Farah, Arkady Zgonnikov, Bart van Arem","Detecting abnormal driving behavior is critical for road traffic safety and
the evaluation of drivers' behavior. With the advancement of machine learning
(ML) algorithms and the accumulation of naturalistic driving data, many ML
models have been adopted for abnormal driving behavior detection. Most existing
ML-based detectors rely on (fully) supervised ML methods, which require
substantial labeled data. However, ground truth labels are not always available
in the real world, and labeling large amounts of data is tedious. Thus, there
is a need to explore unsupervised or semi-supervised methods to make the
anomaly detection process more feasible and efficient. To fill this research
gap, this study analyzes large-scale real-world data revealing several abnormal
driving behaviors (e.g., sudden acceleration, rapid lane-changing) and develops
a Hierarchical Extreme Learning Machines (HELM) based semi-supervised ML method
using partly labeled data to accurately detect the identified abnormal driving
behaviors. Moreover, previous ML-based approaches predominantly utilize basic
vehicle motion features (such as velocity and acceleration) to label and detect
abnormal driving behaviors, while this study seeks to introduce Surrogate
Safety Measures (SSMs) as the input features for ML models to improve the
detection performance. Results from extensive experiments demonstrate the
effectiveness of the proposed semi-supervised ML model with the introduced SSMs
serving as important features. The proposed semi-supervised ML method
outperforms other baseline semi-supervised or unsupervised methods regarding
various metrics, e.g., delivering the best accuracy at 99.58% and the best F-1
measure at 0.9913. The ablation study further highlights the significance of
SSMs for advancing detection performance.",2312.04610v5,https://arxiv.org/pdf/2312.04610v5
"Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability,
  Explainability, and Safety","Manas Gaur, Amit Sheth","Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.",2312.06798v1,https://arxiv.org/pdf/2312.06798v1
"Modular Control Architecture for Safe Marine Navigation: Reinforcement
  Learning and Predictive Safety Filters","Aksel Vaaler, Svein Jostein Husa, Daniel Menges, Thomas Nakken Larsen, Adil Rasheed","Many autonomous systems face safety challenges, requiring robust closed-loop
control to handle physical limitations and safety constraints. Real-world
systems, like autonomous ships, encounter nonlinear dynamics and environmental
disturbances. Reinforcement learning is increasingly used to adapt to complex
scenarios, but standard frameworks ensuring safety and stability are lacking.
Predictive Safety Filters (PSF) offer a promising solution, ensuring constraint
satisfaction in learning-based control without explicit constraint handling.
This modular approach allows using arbitrary control policies, with the safety
filter optimizing proposed actions to meet physical and safety constraints. We
apply this approach to marine navigation, combining RL with PSF on a simulated
Cybership II model. The RL agent is trained on path following and collision
avpodance, while the PSF monitors and modifies control actions for safety.
Results demonstrate the PSF's effectiveness in maintaining safety without
hindering the RL agent's learning rate and performance, evaluated against a
standard RL agent without PSF.",2312.01855v2,https://arxiv.org/pdf/2312.01855v2
Safe Reinforcement Learning in Tensor Reproducing Kernel Hilbert Space,"Xiaoyuan Cheng, Boli Chen, Liz Varga, Yukun Hu","This paper delves into the problem of safe reinforcement learning (RL) in a
partially observable environment with the aim of achieving safe-reachability
objectives. In traditional partially observable Markov decision processes
(POMDP), ensuring safety typically involves estimating the belief in latent
states. However, accurately estimating an optimal Bayesian filter in POMDP to
infer latent states from observations in a continuous state space poses a
significant challenge, largely due to the intractable likelihood. To tackle
this issue, we propose a stochastic model-based approach that guarantees RL
safety almost surely in the face of unknown system dynamics and partial
observation environments. We leveraged the Predictive State Representation
(PSR) and Reproducing Kernel Hilbert Space (RKHS) to represent future
multi-step observations analytically, and the results in this context are
provable. Furthermore, we derived essential operators from the kernel Bayes'
rule, enabling the recursive estimation of future observations using various
operators. Under the assumption of \textit{undercompleness}, a polynomial
sample complexity is established for the RL algorithm for the infinite size of
observation and action spaces, ensuring an $\epsilon-$suboptimal safe policy
guarantee.",2312.00727v1,https://arxiv.org/pdf/2312.00727v1
"One to beat them all: ""RYU'' -- a unifying framework for the
  construction of safe balls","Thu-Le Tran, Clément Elvira, Hong-Phuong Dang, Cédric Herzet","In this paper, we put forth a novel framework (named ``RYU'') for the
construction of ``safe'' balls, i.e. regions that provably contain the dual
solution of a target optimization problem. We concentrate on the standard setup
where the cost function is the sum of two terms: a closed, proper, convex
Lipschitz-smooth function and a closed, proper, convex function. The RYU
framework is shown to generalize or improve upon all the results proposed in
the last decade for the considered family of optimization problems.",2312.00640v1,https://arxiv.org/pdf/2312.00640v1
A safe exploration approach to constrained Markov decision processes,"Tingting Ni, Maryam Kamgarpour","We consider discounted infinite horizon constrained Markov decision processes
(CMDPs) where the goal is to find an optimal policy that maximizes the expected
cumulative reward subject to expected cumulative constraints. Motivated by the
application of CMDPs in online learning of safety-critical systems, we focus on
developing a model-free and simulator-free algorithm that ensures constraint
satisfaction during learning. To this end, we develop an interior point
approach based on the log barrier function of the CMDP. Under the commonly
assumed conditions of Fisher non-degeneracy and bounded transfer error of the
policy parameterization, we establish the theoretical properties of the
algorithm. In particular, in contrast to existing CMDP approaches that ensure
policy feasibility only upon convergence, our algorithm guarantees the
feasibility of the policies during the learning process and converges to the
$\varepsilon$-optimal policy with a sample complexity of
$\tilde{\mathcal{O}}(\varepsilon^{-6})$. In comparison to the state-of-the-art
policy gradient-based algorithm, C-NPG-PDA, our algorithm requires an
additional $\mathcal{O}(\varepsilon^{-2})$ samples to ensure policy feasibility
during learning with the same Fisher non-degenerate parameterization.",2312.00561v2,https://arxiv.org/pdf/2312.00561v2
"TRC: Trust Region Conditional Value at Risk for Safe Reinforcement
  Learning","Dohyeong Kim, Songhwai Oh","As safety is of paramount importance in robotics, reinforcement learning that
reflects safety, called safe RL, has been studied extensively. In safe RL, we
aim to find a policy which maximizes the desired return while satisfying the
defined safety constraints. There are various types of constraints, among which
constraints on conditional value at risk (CVaR) effectively lower the
probability of failures caused by high costs since CVaR is a conditional
expectation obtained above a certain percentile. In this paper, we propose a
trust region-based safe RL method with CVaR constraints, called TRC. We first
derive the upper bound on CVaR and then approximate the upper bound in a
differentiable form in a trust region. Using this approximation, a subproblem
to get policy gradients is formulated, and policies are trained by iteratively
solving the subproblem. TRC is evaluated through safe navigation tasks in
simulations with various robots and a sim-to-real environment with a Jackal
robot from Clearpath. Compared to other safe RL methods, the performance is
improved by 1.93 times while the constraints are satisfied in all experiments.",2312.00344v1,https://arxiv.org/pdf/2312.00344v1
"Efficient Off-Policy Safe Reinforcement Learning Using Trust Region
  Conditional Value at Risk","Dohyeong Kim, Songhwai Oh","This paper aims to solve a safe reinforcement learning (RL) problem with risk
measure-based constraints. As risk measures, such as conditional value at risk
(CVaR), focus on the tail distribution of cost signals, constraining risk
measures can effectively prevent a failure in the worst case. An on-policy safe
RL method, called TRC, deals with a CVaR-constrained RL problem using a trust
region method and can generate policies with almost zero constraint violations
with high returns. However, to achieve outstanding performance in complex
environments and satisfy safety constraints quickly, RL methods are required to
be sample efficient. To this end, we propose an off-policy safe RL method with
CVaR constraints, called off-policy TRC. If off-policy data from replay buffers
is directly used to train TRC, the estimation error caused by the
distributional shift results in performance degradation. To resolve this issue,
we propose novel surrogate functions, in which the effect of the distributional
shift can be reduced, and introduce an adaptive trust-region constraint to
ensure a policy not to deviate far from replay buffers. The proposed method has
been evaluated in simulation and real-world environments and satisfied safety
constraints within a few steps while achieving high returns even in complex
robotic tasks.",2312.00342v1,https://arxiv.org/pdf/2312.00342v1
"Informal Safety Guarantees for Simulated Optimizers Through
  Extrapolation from Partial Simulations",Luke Marks,"Self-supervised learning is the backbone of state of the art language
modeling. It has been argued that training with predictive loss on a
self-supervised dataset causes simulators: entities that internally represent
possible configurations of real-world systems. Under this assumption, a
mathematical model for simulators is built based in the Cartesian frames model
of embedded agents, which is extended to multi-agent worlds through scaling a
two-dimensional frame to arbitrary dimensions, where literature prior chooses
to instead use operations on frames. This variant leveraging scaling
dimensionality is named the Cartesian object, and is used to represent
simulations (where individual simulacra are the agents and devices in that
object). Around the Cartesian object, functions like token selection and
simulation complexity are accounted for in formalizing the behavior of a
simulator, and used to show (through the L\""obian obstacle) that a proof of
alignment between simulacra by inspection of design is impossible in the
simulator context. Following this, a scheme is proposed and termed Partial
Simulation Extrapolation aimed at circumventing the L\""obian obstacle through
the evaluation of low-complexity simulations.",2401.16426v1,https://arxiv.org/pdf/2401.16426v1
Safe Reinforcement Learning in a Simulated Robotic Arm,"Luka Kovač, Igor Farkaš","Reinforcement learning (RL) agents need to explore their environments in
order to learn optimal policies. In many environments and tasks, safety is of
critical importance. The widespread use of simulators offers a number of
advantages, including safe exploration which will be inevitable in cases when
RL systems need to be trained directly in the physical environment (e.g. in
human-robot interaction). The popular Safety Gym library offers three mobile
agent types that can learn goal-directed tasks while considering various safety
constraints. In this paper, we extend the applicability of safe RL algorithms
by creating a customized environment with Panda robotic arm where Safety Gym
algorithms can be tested. We performed pilot experiments with the popular PPO
algorithm comparing the baseline with the constrained version and show that the
constrained version is able to learn the equally good policy while better
complying with safety constraints and taking longer training time as expected.",2312.09468v2,https://arxiv.org/pdf/2312.09468v2
"Empowering Autonomous Driving with Large Language Models: A Safety
  Perspective","Yixuan Wang, Ruochen Jiao, Sinong Simon Zhan, Chengtian Lang, Chao Huang, Zhaoran Wang, Zhuoran Yang, Qi Zhu","Autonomous Driving (AD) encounters significant safety hurdles in long-tail
unforeseen driving scenarios, largely stemming from the non-interpretability
and poor generalization of the deep neural networks within the AD system,
particularly in out-of-distribution and uncertain data. To this end, this paper
explores the integration of Large Language Models (LLMs) into AD systems,
leveraging their robust common-sense knowledge and reasoning abilities. The
proposed methodologies employ LLMs as intelligent decision-makers in behavioral
planning, augmented with a safety verifier shield for contextual safety
learning, for enhancing driving performance and safety. We present two key
studies in a simulated environment: an adaptive LLM-conditioned Model
Predictive Control (MPC) and an LLM-enabled interactive behavior planning
scheme with a state machine. Demonstrating superior performance and safety
metrics compared to state-of-the-art approaches, our approach shows the
promising potential for using LLMs for autonomous vehicles.",2312.00812v4,https://arxiv.org/pdf/2312.00812v4
Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models,"Samuele Poppi, Tobia Poppi, Federico Cocchi, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara","Large-scale vision-and-language models, such as CLIP, are typically trained
on web-scale data, which can introduce inappropriate content and lead to the
development of unsafe and biased behavior. This, in turn, hampers their
applicability in sensitive and trustworthy contexts and could raise significant
concerns in their adoption. Our research introduces a novel approach to
enhancing the safety of vision-and-language models by diminishing their
sensitivity to NSFW (not safe for work) inputs. In particular, our methodology
seeks to sever ""toxic"" linguistic and visual concepts, unlearning the linkage
between unsafe linguistic or visual items and unsafe regions of the embedding
space. We show how this can be done by fine-tuning a CLIP model on synthetic
data obtained from a large language model trained to convert between safe and
unsafe sentences, and a text-to-image generator. We conduct extensive
experiments on the resulting embedding space for cross-modal retrieval,
text-to-image, and image-to-text generation, where we show that our model can
be remarkably employed with pre-trained generative models. Our source code and
trained models are available at: https://github.com/aimagelab/safe-clip.",2311.16254v3,https://arxiv.org/pdf/2311.16254v3
"How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for
  Vision LLMs","Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, Cihang Xie","This work focuses on the potential of Vision LLMs (VLLMs) in visual
reasoning. Different from prior studies, we shift our focus from evaluating
standard performance to introducing a comprehensive safety evaluation suite,
covering both out-of-distribution (OOD) generalization and adversarial
robustness. For the OOD evaluation, we present two novel VQA datasets, each
with one variant, designed to test model performance under challenging
conditions. In exploring adversarial robustness, we propose a straightforward
attack strategy for misleading VLLMs to produce visual-unrelated responses.
Moreover, we assess the efficacy of two jailbreaking strategies, targeting
either the vision or language component of VLLMs. Our evaluation of 21 diverse
models, ranging from open-source VLLMs to GPT-4V, yields interesting
observations: 1) Current VLLMs struggle with OOD texts but not images, unless
the visual information is limited; and 2) These VLLMs can be easily misled by
deceiving vision encoders only, and their vision-language training often
compromise safety protocols. We release this safety evaluation suite at
https://github.com/UCSC-VLAA/vllm-safety-benchmark.",2311.16101v1,https://arxiv.org/pdf/2311.16101v1
"Networked Multiagent Safe Reinforcement Learning for Low-carbon Demand
  Management in Distribution Network","Jichen Zhang, Linwei Sang, Yinliang Xu, Hongbin Sun","This paper proposes a multiagent based bi-level operation framework for the
low-carbon demand management in distribution networks considering the carbon
emission allowance on the demand side. In the upper level, the aggregate load
agents optimize the control signals for various types of loads to maximize the
profits; in the lower level, the distribution network operator makes optimal
dispatching decisions to minimize the operational costs and calculates the
distribution locational marginal price and carbon intensity. The distributed
flexible load agent has only incomplete information of the distribution network
and cooperates with other agents using networked communication. Finally, the
problem is formulated into a networked multi-agent constrained Markov decision
process, which is solved using a safe reinforcement learning algorithm called
consensus multi-agent constrained policy optimization considering the carbon
emission allowance for each agent. Case studies with the IEEE 33-bus and
123-bus distribution network systems demonstrate the effectiveness of the
proposed approach, in terms of satisfying the carbon emission constraint on
demand side, ensuring the safe operation of the distribution network and
preserving privacy of both sides.",2311.15594v1,https://arxiv.org/pdf/2311.15594v1
"RAISE -- Radiology AI Safety, an End-to-end lifecycle approach","M. Jorge Cardoso, Julia Moosbauer, Tessa S. Cook, B. Selnur Erdal, Brad Genereaux, Vikash Gupta, Bennett A. Landman, Tiarna Lee, Parashkev Nachev, Elanchezhian Somasundaram, Ronald M. Summers, Khaled Younis, Sebastien Ourselin, Franz MJ Pfister","The integration of AI into radiology introduces opportunities for improved
clinical care provision and efficiency but it demands a meticulous approach to
mitigate potential risks as with any other new technology. Beginning with
rigorous pre-deployment evaluation and validation, the focus should be on
ensuring models meet the highest standards of safety, effectiveness and
efficacy for their intended applications. Input and output guardrails
implemented during production usage act as an additional layer of protection,
identifying and addressing individual failures as they occur. Continuous
post-deployment monitoring allows for tracking population-level performance
(data drift), fairness, and value delivery over time. Scheduling reviews of
post-deployment model performance and educating radiologists about new
algorithmic-driven findings is critical for AI to be effective in clinical
practice. Recognizing that no single AI solution can provide absolute assurance
even when limited to its intended use, the synergistic application of quality
assurance at multiple levels - regulatory, clinical, technical, and ethical -
is emphasized. Collaborative efforts between stakeholders spanning healthcare
systems, industry, academia, and government are imperative to address the
multifaceted challenges involved. Trust in AI is an earned privilege,
contingent on a broad set of goals, among them transparently demonstrating that
the AI adheres to the same rigorous safety, effectiveness and efficacy
standards as other established medical technologies. By doing so, developers
can instil confidence among providers and patients alike, enabling the
responsible scaling of AI and the realization of its potential benefits. The
roadmap presented herein aims to expedite the achievement of deployable,
reliable, and safe AI in radiology.",2311.14570v1,https://arxiv.org/pdf/2311.14570v1
"How to ensure a safe control strategy? Towards a SRL for urban transit
  autonomous operation",Zicong Zhao,"Deep reinforcement learning has gradually shown its latent decision-making
ability in urban rail transit autonomous operation. However, since
reinforcement learning can not neither guarantee safety during learning nor
execution, this is still one of the major obstacles to the practical
application of reinforcement learning. Given this drawback, reinforcement
learning applied in the safety-critical autonomous operation domain remains
challenging without generating a safe control command sequence that avoids
overspeed operations. Therefore, a SSA-DRL framework is proposed in this paper
for safe intelligent control of urban rail transit autonomous operation trains.
The proposed framework is combined with linear temporal logic, reinforcement
learning and Monte Carlo tree search and consists of four mainly module: a
post-posed shielding, a searching tree module, a DRL framework and an
additional actor. Furthermore, the output of the framework can meet speed
constraint, schedule constraint and optimize the operation process. Finally,
the proposed SSA-DRL framework for decision-making in urban rail transit
autonomous operation is evaluated in sixteen different sections, and its
effectiveness is demonstrated through an ablation experiment and comparison
with the scheduled operation plan.",2311.14457v2,https://arxiv.org/pdf/2311.14457v2
Scalable AI Safety via Doubly-Efficient Debate,"Jonah Brown-Cohen, Geoffrey Irving, Georgios Piliouras","The emergence of pre-trained AI systems with powerful capabilities across a
diverse and ever-increasing set of complex domains has raised a critical
challenge for AI safety as tasks can become too complicated for humans to judge
directly. Irving et al. [2018] proposed a debate method in this direction with
the goal of pitting the power of such AI models against each other until the
problem of identifying (mis)-alignment is broken down into a manageable
subtask. While the promise of this approach is clear, the original framework
was based on the assumption that the honest strategy is able to simulate
deterministic AI systems for an exponential number of steps, limiting its
applicability. In this paper, we show how to address these challenges by
designing a new set of debate protocols where the honest strategy can always
succeed using a simulation of a polynomial number of steps, whilst being able
to verify the alignment of stochastic AI systems, even when the dishonest
strategy is allowed to use exponentially many simulation steps.",2311.14125v1,https://arxiv.org/pdf/2311.14125v1
"Scheduling Distributed Flexible Assembly Lines using Safe Reinforcement
  Learning with Soft Shielding","Lele Li, Liyong Lin","Highly automated assembly lines enable significant productivity gains in the
manufacturing industry, particularly in mass production condition. Nonetheless,
challenges persist in job scheduling for make-to-job and mass customization,
necessitating further investigation to improve efficiency, reduce tardiness,
promote safety and reliability. In this contribution, an advantage actor-critic
based reinforcement learning method is proposed to address scheduling problems
of distributed flexible assembly lines in a real-time manner. To enhance the
performance, a more condensed environment representation approach is proposed,
which is designed to work with the masks made by priority dispatching rules to
generate fixed and advantageous action space. Moreover, a Monte-Carlo tree
search based soft shielding component is developed to help address
long-sequence dependent unsafe behaviors and monitor the risk of overdue
scheduling. Finally, the proposed algorithm and its soft shielding component
are validated in performance evaluation.",2311.12572v1,https://arxiv.org/pdf/2311.12572v1
"A Safer Vision-based Autonomous Planning System for Quadrotor UAVs with
  Dynamic Obstacle Trajectory Prediction and Its Application with LLMs","Jiageng Zhong, Ming Li, Yinliang Chen, Zihang Wei, Fan Yang, Haoran Shen","For intelligent quadcopter UAVs, a robust and reliable autonomous planning
system is crucial. Most current trajectory planning methods for UAVs are
suitable for static environments but struggle to handle dynamic obstacles,
which can pose challenges and even dangers to flight. To address this issue,
this paper proposes a vision-based planning system that combines tracking and
trajectory prediction of dynamic obstacles to achieve efficient and reliable
autonomous flight. We use a lightweight object detection algorithm to identify
dynamic obstacles and then use Kalman Filtering to track and estimate their
motion states. During the planning phase, we not only consider static obstacles
but also account for the potential movements of dynamic obstacles. For
trajectory generation, we use a B-spline-based trajectory search algorithm,
which is further optimized with various constraints to enhance safety and
alignment with the UAV's motion characteristics. We conduct experiments in both
simulation and real-world environments, and the results indicate that our
approach can successfully detect and avoid obstacles in dynamic environments in
real-time, offering greater reliability compared to existing approaches.
Furthermore, with the advancements in Natural Language Processing (NLP)
technology demonstrating exceptional zero-shot generalization capabilities,
more user-friendly human-machine interactions have become feasible, and this
study also explores the integration of autonomous planning systems with Large
Language Models (LLMs).",2311.12893v2,https://arxiv.org/pdf/2311.12893v2
"EnduRL: Enhancing Safety, Stability, and Efficiency of Mixed Traffic
  Under Real-World Perturbations Via Reinforcement Learning","Bibek Poudel, Weizi Li, Kevin Heaslip","Human-driven vehicles (HVs) amplify naturally occurring perturbations in
traffic, leading to congestion--a major contributor to increased fuel
consumption, higher collision risks, and reduced road capacity utilization.
While previous research demonstrates that Robot Vehicles (RVs) can be leveraged
to mitigate these issues, most such studies rely on simulations with simplistic
models of human car-following behaviors. In this work, we analyze real-world
driving trajectories and extract a wide range of acceleration profiles. We then
incorporates these profiles into simulations for training RVs to mitigate
congestion. We evaluate the safety, efficiency, and stability of mixed traffic
via comprehensive experiments conducted in two mixed traffic environments (Ring
and Bottleneck) at various traffic densities, configurations, and RV
penetration rates. The results show that under real-world perturbations, prior
RV controllers experience performance degradation on all three objectives
(sometimes even lower than 100% HVs). To address this, we introduce a
reinforcement learning based RV that employs a congestion stage classifier to
optimize the safety, efficiency, and stability of mixed traffic. Our RVs
demonstrate significant improvements: safety by up to 66%, efficiency by up to
54%, and stability by up to 97%.",2311.12261v2,https://arxiv.org/pdf/2311.12261v2
"Bridging Data-Driven and Knowledge-Driven Approaches for Safety-Critical
  Scenario Generation in Automated Vehicle Validation","Kunkun Hao, Lu Liu, Wen Cui, Jianxing Zhang, Songyang Yan, Yuxi Pan, Zijiang Yang","Automated driving vehicles~(ADV) promise to enhance driving efficiency and
safety, yet they face intricate challenges in safety-critical scenarios. As a
result, validating ADV within generated safety-critical scenarios is essential
for both development and performance evaluations. This paper investigates the
complexities of employing two major scenario-generation solutions: data-driven
and knowledge-driven methods. Data-driven methods derive scenarios from
recorded datasets, efficiently generating scenarios by altering the existing
behavior or trajectories of traffic participants but often falling short in
considering ADV perception; knowledge-driven methods provide effective coverage
through expert-designed rules, but they may lead to inefficiency in generating
safety-critical scenarios within that coverage. To overcome these challenges,
we introduce BridgeGen, a safety-critical scenario generation framework,
designed to bridge the benefits of both methodologies. Specifically, by
utilizing ontology-based techniques, BridgeGen models the five scenario layers
in the operational design domain (ODD) from knowledge-driven methods, ensuring
broad coverage, and incorporating data-driven strategies to efficiently
generate safety-critical scenarios. An optimized scenario generation toolkit is
developed within BridgeGen. This expedites the crafting of safety-critical
scenarios through a combination of traditional optimization and reinforcement
learning schemes. Extensive experiments conducted using Carla simulator
demonstrate the effectiveness of BridgeGen in generating diverse
safety-critical scenarios.",2311.10937v1,https://arxiv.org/pdf/2311.10937v1
Testing Language Model Agents Safely in the Wild,"Silen Naihin, David Atkinson, Marc Green, Merwane Hamadi, Craig Swift, Douglas Schonholtz, Adam Tauman Kalai, David Bau","A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet
real-world autonomous tests face several unique safety challenges, both due to
the possibility of causing harm during a test, as well as the risk of
encountering new unsafe agent behavior through interactions with real-world and
potentially malicious actors. We propose a framework for conducting safe
autonomous agent tests on the open internet: agent actions are audited by a
context-sensitive monitor that enforces a stringent safety boundary to stop an
unsafe test, with suspect behavior ranked and logged to be examined by humans.
We design a basic safety monitor (AgentMonitor) that is flexible enough to
monitor existing LLM agents, and, using an adversarial simulated agent, we
measure its ability to identify and stop unsafe situations. Then we apply the
AgentMonitor on a battery of real-world tests of AutoGPT, and we identify
several limitations and challenges that will face the creation of safe
in-the-wild tests as autonomous agents grow more capable.",2311.10538v3,https://arxiv.org/pdf/2311.10538v3
"Imagination-Augmented Hierarchical Reinforcement Learning for Safe and
  Interactive Autonomous Driving in Urban Environments","Sang-Hyun Lee, Yoonjae Jung, Seung-Woo Seo","Hierarchical reinforcement learning (HRL) incorporates temporal abstraction
into reinforcement learning (RL) by explicitly taking advantage of hierarchical
structure. Modern HRL typically designs a hierarchical agent composed of a
high-level policy and low-level policies. The high-level policy selects which
low-level policy to activate at a lower frequency and the activated low-level
policy selects an action at each time step. Recent HRL algorithms have achieved
performance gains over standard RL algorithms in synthetic navigation tasks.
However, we cannot apply these HRL algorithms to real-world navigation tasks.
One of the main challenges is that real-world navigation tasks require an agent
to perform safe and interactive behaviors in dynamic environments. In this
paper, we propose imagination-augmented HRL (IAHRL) that efficiently integrates
imagination into HRL to enable an agent to learn safe and interactive behaviors
in real-world navigation tasks. Imagination is to predict the consequences of
actions without interactions with actual environments. The key idea behind
IAHRL is that the low-level policies imagine safe and structured behaviors, and
then the high-level policy infers interactions with surrounding objects by
interpreting the imagined behaviors. We also introduce a new attention
mechanism that allows our high-level policy to be permutation-invariant to the
order of surrounding objects and to prioritize our agent over them. To evaluate
IAHRL, we introduce five complex urban driving tasks, which are among the most
challenging real-world navigation tasks. The experimental results indicate that
IAHRL enables an agent to perform safe and interactive behaviors, achieving
higher success rates and lower average episode steps than baselines.",2311.10309v2,https://arxiv.org/pdf/2311.10309v2
"Safety Aware Autonomous Path Planning Using Model Predictive
  Reinforcement Learning for Inland Waterways","Astrid Vanneste, Simon Vanneste, Olivier Vasseur, Robin Janssens, Mattias Billast, Ali Anwar, Kevin Mets, Tom De Schepper, Siegfried Mercelis, Peter Hellinckx","In recent years, interest in autonomous shipping in urban waterways has
increased significantly due to the trend of keeping cars and trucks out of city
centers. Classical approaches such as Frenet frame based planning and potential
field navigation often require tuning of many configuration parameters and
sometimes even require a different configuration depending on the situation. In
this paper, we propose a novel path planning approach based on reinforcement
learning called Model Predictive Reinforcement Learning (MPRL). MPRL calculates
a series of waypoints for the vessel to follow. The environment is represented
as an occupancy grid map, allowing us to deal with any shape of waterway and
any number and shape of obstacles. We demonstrate our approach on two scenarios
and compare the resulting path with path planning using a Frenet frame and path
planning based on a proximal policy optimization (PPO) agent. Our results show
that MPRL outperforms both baselines in both test scenarios. The PPO based
approach was not able to reach the goal in either scenario while the Frenet
frame approach failed in the scenario consisting of a corner with obstacles.
MPRL was able to safely (collision free) navigate to the goal in both of the
test scenarios.",2311.09878v1,https://arxiv.org/pdf/2311.09878v1
"Towards Formal Fault Injection for Safety Assessment of Automated
  Systems","Ashfaq Farooqui, Behrooz Sangchoolie","Reasoning about safety, security, and other dependability attributes of
autonomous systems is a challenge that needs to be addressed before the
adoption of such systems in day-to-day life. Formal methods is a class of
methods that mathematically reason about a system's behavior. Thus, a
correctness proof is sufficient to conclude the system's dependability.
However, these methods are usually applied to abstract models of the system,
which might not fully represent the actual system. Fault injection, on the
other hand, is a testing method to evaluate the dependability of systems.
However, the amount of testing required to evaluate the system is rather large
and often a problem. This vision paper introduces formal fault injection, a
fusion of these two techniques throughout the development lifecycle to enhance
the dependability of autonomous systems. We advocate for a more cohesive
approach by identifying five areas of mutual support between formal methods and
fault injection. By forging stronger ties between the two fields, we pave the
way for developing safe and dependable autonomous systems. This paper delves
into the integration's potential and outlines future research avenues,
addressing open challenges along the way.",2311.09810v1,https://arxiv.org/pdf/2311.09810v1
"Causal prediction models for medication safety monitoring: The diagnosis
  of vancomycin-induced acute kidney injury","Izak Yasrebi-de Kom, Joanna Klopotowska, Dave Dongelmans, Nicolette De Keizer, Kitty Jager, Ameen Abu-Hanna, Giovanni Cinà","The current best practice approach for the retrospective diagnosis of adverse
drug events (ADEs) in hospitalized patients relies on a full patient chart
review and a formal causality assessment by multiple medical experts. This
evaluation serves to qualitatively estimate the probability of causation (PC);
the probability that a drug was a necessary cause of an adverse event. This
practice is manual, resource intensive and prone to human biases, and may thus
benefit from data-driven decision support. Here, we pioneer a causal modeling
approach using observational data to estimate a lower bound of the PC
(PC$_{low}$). This method includes two key causal inference components: (1) the
target trial emulation framework and (2) estimation of individualized treatment
effects using machine learning. We apply our method to the clinically relevant
use-case of vancomycin-induced acute kidney injury in intensive care patients,
and compare our causal model-based PC$_{low}$ estimates to qualitative
estimates of the PC provided by a medical expert. Important limitations and
potential improvements are discussed, and we conclude that future improved
causal models could provide essential data-driven support for medication safety
monitoring in hospitalized patients.",2311.09137v1,https://arxiv.org/pdf/2311.09137v1
"Safety, Trust, and Ethics Considerations for Human-AI Teaming in
  Aerospace Control","Kerianne L. Hobbs, Bernard Li","Designing a safe, trusted, and ethical AI may be practically impossible;
however, designing AI with safe, trusted, and ethical use in mind is possible
and necessary in safety and mission-critical domains like aerospace. Safe,
trusted, and ethical use of AI are often used interchangeably; however, a
system can be safely used but not trusted or ethical, have a trusted use that
is not safe or ethical, and have an ethical use that is not safe or trusted.
This manuscript serves as a primer to illuminate the nuanced differences
between these concepts, with a specific focus on applications of Human-AI
teaming in aerospace system control, where humans may be in, on, or
out-of-the-loop of decision-making.",2311.08943v1,https://arxiv.org/pdf/2311.08943v1
"Learning Predictive Safety Filter via Decomposition of Robust Invariant
  Set","Zeyang Li, Chuxiong Hu, Weiye Zhao, Changliu Liu","Ensuring safety of nonlinear systems under model uncertainty and external
disturbances is crucial, especially for real-world control tasks. Predictive
methods such as robust model predictive control (RMPC) require solving
nonconvex optimization problems online, which leads to high computational
burden and poor scalability. Reinforcement learning (RL) works well with
complex systems, but pays the price of losing rigorous safety guarantee. This
paper presents a theoretical framework that bridges the advantages of both RMPC
and RL to synthesize safety filters for nonlinear systems with state- and
action-dependent uncertainty. We decompose the robust invariant set (RIS) into
two parts: a target set that aligns with terminal region design of RMPC, and a
reach-avoid set that accounts for the rest of RIS. We propose a policy
iteration approach for robust reach-avoid problems and establish its monotone
convergence. This method sets the stage for an adversarial actor-critic deep RL
algorithm, which simultaneously synthesizes a reach-avoid policy network, a
disturbance policy network, and a reach-avoid value network. The learned
reach-avoid policy network is utilized to generate nominal trajectories for
online verification, which filters potentially unsafe actions that may drive
the system into unsafe regions when worst-case disturbances are applied. We
formulate a second-order cone programming (SOCP) approach for online
verification using system level synthesis, which optimizes for the worst-case
reach-avoid value of any possible trajectories. The proposed safety filter
requires much lower computational complexity than RMPC and still enjoys
persistent robust safety guarantee. The effectiveness of our method is
illustrated through a numerical example.",2311.06769v1,https://arxiv.org/pdf/2311.06769v1
Unveiling Safety Vulnerabilities of Large Language Models,"George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi","As large language models become more prevalent, their possible harmful or
inappropriate responses are a cause for concern. This paper introduces a unique
dataset containing adversarial examples in the form of questions, which we call
AttaQ, designed to provoke such harmful or inappropriate responses. We assess
the efficacy of our dataset by analyzing the vulnerabilities of various models
when subjected to it. Additionally, we introduce a novel automatic approach for
identifying and naming vulnerable semantic regions - input semantic areas for
which the model is likely to produce harmful outputs. This is achieved through
the application of specialized clustering techniques that consider both the
semantic similarity of the input attacks and the harmfulness of the model's
responses. Automatically identifying vulnerable semantic regions enhances the
evaluation of model weaknesses, facilitating targeted improvements to its
safety mechanisms and overall reliability.",2311.04124v1,https://arxiv.org/pdf/2311.04124v1
State-Wise Safe Reinforcement Learning With Pixel Observations,"Simon Sinong Zhan, Yixuan Wang, Qingyuan Wu, Ruochen Jiao, Chao Huang, Qi Zhu","In the context of safe exploration, Reinforcement Learning (RL) has long
grappled with the challenges of balancing the tradeoff between maximizing
rewards and minimizing safety violations, particularly in complex environments
with contact-rich or non-smooth dynamics, and when dealing with
high-dimensional pixel observations. Furthermore, incorporating state-wise
safety constraints in the exploration and learning process, where the agent
must avoid unsafe regions without prior knowledge, adds another layer of
complexity. In this paper, we propose a novel pixel-observation safe RL
algorithm that efficiently encodes state-wise safety constraints with unknown
hazard regions through a newly introduced latent barrier-like function learning
mechanism. As a joint learning framework, our approach begins by constructing a
latent dynamics model with low-dimensional latent spaces derived from pixel
observations. We then build and learn a latent barrier-like function on top of
the latent dynamics and conduct policy optimization simultaneously, thereby
improving both safety and the total expected return. Experimental evaluations
on the safety-gym benchmark suite demonstrate that our proposed method
significantly reduces safety violations throughout the training process, and
demonstrates faster safety convergence compared to existing methods while
achieving competitive results in reward return.",2311.02227v2,https://arxiv.org/pdf/2311.02227v2
"Safe Online Dynamics Learning with Initially Unknown Models and
  Infeasible Safety Certificates","Alexandre Capone, Ryan Cosner, Aaron Ames, Sandra Hirche","Safety-critical control tasks with high levels of uncertainty are becoming
increasingly common. Typically, techniques that guarantee safety during
learning and control utilize constraint-based safety certificates, which can be
leveraged to compute safe control inputs. However, excessive model uncertainty
can render robust safety certification methods or infeasible, meaning no
control input satisfies the constraints imposed by the safety certificate. This
paper considers a learning-based setting with a robust safety certificate based
on a control barrier function (CBF) second-order cone program. If the control
barrier function certificate is feasible, our approach leverages it to
guarantee safety. Otherwise, our method explores the system dynamics to collect
data and recover the feasibility of the control barrier function constraint. To
this end, we employ a method inspired by well-established tools from Bayesian
optimization. We show that if the sampling frequency is high enough, we recover
the feasibility of the robust CBF certificate, guaranteeing safety. Our
approach requires no prior model and corresponds, to the best of our knowledge,
to the first algorithm that guarantees safety in settings with occasionally
infeasible safety certificates without requiring a backup non-learning-based
controller.",2311.02133v1,https://arxiv.org/pdf/2311.02133v1
Safe Sequential Optimization for Switching Environments,"Durgesh Kalwar, Vineeth B. S","We consider the problem of designing a sequential decision making agent to
maximize an unknown time-varying function which switches with time. At each
step, the agent receives an observation of the function's value at a point
decided by the agent. The observation could be corrupted by noise. The agent is
also constrained to take safe decisions with high probability, i.e., the chosen
points should have a function value greater than a threshold. For this
switching environment, we propose a policy called Adaptive-SafeOpt and evaluate
its performance via simulations. The policy incorporates Bayesian optimization
and change point detection for the safe sequential optimization problem. We
observe that a major challenge in adapting to the switching change is to
identify safe decisions when the change point is detected and prevent
attraction to local optima.",2311.02119v1,https://arxiv.org/pdf/2311.02119v1
"Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop
  Simulation","Jay Sarva, Jingkang Wang, James Tu, Yuwen Xiong, Sivabalan Manivasagam, Raquel Urtasun","Self-driving vehicles (SDVs) must be rigorously tested on a wide range of
scenarios to ensure safe deployment. The industry typically relies on
closed-loop simulation to evaluate how the SDV interacts on a corpus of
synthetic and real scenarios and verify it performs properly. However, they
primarily only test the system's motion planning module, and only consider
behavior variations. It is key to evaluate the full autonomy system in
closed-loop, and to understand how variations in sensor data based on scene
appearance, such as the shape of actors, affect system performance. In this
paper, we propose a framework, Adv3D, that takes real world scenarios and
performs closed-loop sensor simulation to evaluate autonomy performance, and
finds vehicle shapes that make the scenario more challenging, resulting in
autonomy failures and uncomfortable SDV maneuvers. Unlike prior works that add
contrived adversarial shapes to vehicle roof-tops or roadside to harm
perception only, we optimize a low-dimensional shape representation to modify
the vehicle shape itself in a realistic manner to degrade autonomy performance
(e.g., perception, prediction, and motion planning). Moreover, we find that the
shape variations found with Adv3D optimized in closed-loop are much more
effective than those in open-loop, demonstrating the importance of finding
scene appearance variations that affect autonomy in the interactive setting.",2311.01446v1,https://arxiv.org/pdf/2311.01446v1
SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization,"Jaafar Mhamed, Shangding Gu","Incorporating safety is an essential prerequisite for broadening the
practical applications of reinforcement learning in real-world scenarios. To
tackle this challenge, Constrained Markov Decision Processes (CMDPs) are
leveraged, which introduce a distinct cost function representing safety
violations. In CMDPs' settings, Lagrangian relaxation technique has been
employed in previous algorithms to convert constrained optimization problems
into unconstrained dual problems. However, these algorithms may inaccurately
predict unsafe behavior, resulting in instability while learning the Lagrange
multiplier. This study introduces a novel safe reinforcement learning
algorithm, Safety Critic Policy Optimization (SCPO). In this study, we define
the safety critic, a mechanism that nullifies rewards obtained through
violating safety constraints. Furthermore, our theoretical analysis indicates
that the proposed algorithm can automatically balance the trade-off between
adhering to safety constraints and maximizing rewards. The effectiveness of the
SCPO algorithm is empirically validated by benchmarking it against strong
baselines.",2311.00880v1,https://arxiv.org/pdf/2311.00880v1
"healthAIChain: Improving security and safety using Blockchain Technology
  applications in AI-based healthcare systems","Naresh Kshetri, James Hutson, Revathy G","Blockchain as a digital ledger for keeping records of digital transactions
and other information, it is secure and decentralized technology. The globally
growing number of digital population every day possesses a significant threat
to online data including the medical and patients data. After bitcoin,
blockchain technology has emerged into a general-purpose technology with
applications in medical industries and healthcare. Blockchain can promote
highly configurable openness while retaining the highest security standards for
critical data of medical patients. Referred to as distributed record keeping
for healthcare systems which makes digital assets unalterable and transparent
via a cryptographic hash and decentralized network. The study delves into the
security and safety improvement associated with implementing blockchain in
AI-based healthcare systems. Blockchain-enabled AI tackles the existing issues
related to security, performance efficiencies, and safety in healthcare
systems. We have also examined the Artificial Intelligence in healthcare and
medical industry, potential areas, open questions concerning the blockchain in
healthcare systems. Finally, the article proposed an AI-based healthcare
blockchain model (healthAIChain) to improve patients data and security.",2311.00842v1,https://arxiv.org/pdf/2311.00842v1
"JADE: A Linguistics-based Safety Evaluation Platform for Large Language
  Models","Mi Zhang, Xudong Pan, Min Yang","In this paper, we present JADE, a targeted linguistic fuzzing platform which
strengthens the linguistic complexity of seed questions to simultaneously and
consistently break a wide range of widely-used LLMs categorized in three
groups: eight open-sourced Chinese, six commercial Chinese and four commercial
English LLMs. JADE generates three safety benchmarks for the three groups of
LLMs, which contain unsafe questions that are highly threatening: the questions
simultaneously trigger harmful generation of multiple LLMs, with an average
unsafe generation ratio of $70\%$ (please see the table below), while are still
natural questions, fluent and preserving the core unsafe semantics. We release
the benchmark demos generated for commercial English LLMs and open-sourced
English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For
readers who are interested in evaluating on more questions generated by JADE,
please contact us.
  JADE is based on Noam Chomsky's seminal theory of transformational-generative
grammar. Given a seed question with unsafe intention, JADE invokes a sequence
of generative and transformational rules to increment the complexity of the
syntactic structure of the original question, until the safety guardrail is
broken. Our key insight is: Due to the complexity of human language, most of
the current best LLMs can hardly recognize the invariant evil from the infinite
number of different syntactic structures which form an unbound example space
that can never be fully covered. Technically, the generative/transformative
rules are constructed by native speakers of the languages, and, once developed,
can be used to automatically grow and transform the parse tree of a given
question, until the guardrail is broken. For more evaluation results and demo,
please check our website: https://whitzard-ai.github.io/jade.html.",2311.00286v3,https://arxiv.org/pdf/2311.00286v3
"Robust Safety Classifier for Large Language Models: Adversarial Prompt
  Shield","Jinhwa Kim, Ali Derakhshan, Ian G. Harris","Large Language Models' safety remains a critical concern due to their
vulnerability to adversarial attacks, which can prompt these systems to produce
harmful responses. In the heart of these systems lies a safety classifier, a
computational model trained to discern and mitigate potentially harmful,
offensive, or unethical outputs. However, contemporary safety classifiers,
despite their potential, often fail when exposed to inputs infused with
adversarial noise. In response, our study introduces the Adversarial Prompt
Shield (APS), a lightweight model that excels in detection accuracy and
demonstrates resilience against adversarial prompts. Additionally, we propose
novel strategies for autonomously generating adversarial training datasets,
named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are
designed to fortify the safety classifier's robustness, and we investigate the
consequences of incorporating adversarial examples into the training process.
Through evaluations involving Large Language Models, we demonstrate that our
classifier has the potential to decrease the attack success rate resulting from
adversarial attacks by up to 60%. This advancement paves the way for the next
generation of more reliable and resilient conversational agents.",2311.00172v1,https://arxiv.org/pdf/2311.00172v1
"Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations
  for Accident Analysis","Abhinav Nippani, Dongyue Li, Haotian Ju, Haris N. Koutsopoulos, Hongyang R. Zhang","We consider the problem of traffic accident analysis on a road network based
on road network connections and traffic volume. Previous works have designed
various deep-learning methods using historical records to predict traffic
accident occurrences. However, there is a lack of consensus on how accurate
existing methods are, and a fundamental issue is the lack of public accident
datasets for comprehensive evaluations. This paper constructs a large-scale,
unified dataset of traffic accident records from official reports of various
states in the US, totaling 9 million records, accompanied by road networks and
traffic volume reports. Using this new dataset, we evaluate existing
deep-learning methods for predicting the occurrence of accidents on road
networks. Our main finding is that graph neural networks such as GraphSAGE can
accurately predict the number of accidents on roads with less than 22% mean
absolute error (relative to the actual count) and whether an accident will
occur or not with over 87% AUROC, averaged over states. We achieve these
results by using multitask learning to account for cross-state variabilities
(e.g., availability of accident labels) and transfer learning to combine
traffic volume with accident prediction. Ablation studies highlight the
importance of road graph-structural features, amongst other features. Lastly,
we discuss the implications of the analysis and develop a package for easily
using our new dataset.",2311.00164v2,https://arxiv.org/pdf/2311.00164v2
"Safety-aware Causal Representation for Trustworthy Offline Reinforcement
  Learning in Autonomous Driving","Haohong Lin, Wenhao Ding, Zuxin Liu, Yaru Niu, Jiacheng Zhu, Yuming Niu, Ding Zhao","In the domain of autonomous driving, the offline Reinforcement Learning~(RL)
approaches exhibit notable efficacy in addressing sequential decision-making
problems from offline datasets. However, maintaining safety in diverse
safety-critical scenarios remains a significant challenge due to long-tailed
and unforeseen scenarios absent from offline datasets. In this paper, we
introduce the saFety-aware strUctured Scenario representatION (FUSION), a
pioneering representation learning method in offline RL to facilitate the
learning of a generalizable end-to-end driving policy by leveraging structured
scenario information. FUSION capitalizes on the causal relationships between
the decomposed reward, cost, state, and action space, constructing a framework
for structured sequential reasoning in dynamic traffic environments. We conduct
extensive evaluations in two typical real-world settings of the distribution
shift in autonomous vehicles, demonstrating the good balance between safety
cost and utility reward compared to the current state-of-the-art safe RL and IL
baselines. Empirical evidence in various driving scenarios attests that FUSION
significantly enhances the safety and generalizability of autonomous driving
agents, even in the face of challenging and unseen environments. Furthermore,
our ablation studies reveal noticeable improvements in the integration of
causal representation into the offline safe RL algorithm. Our code
implementation is available at: https://sites.google.com/view/safe-fusion/.",2311.10747v3,https://arxiv.org/pdf/2311.10747v3
"Safe multi-agent motion planning under uncertainty for drones using
  filtered reinforcement learning","Sleiman Safaoui, Abraham P. Vinod, Ankush Chakrabarty, Rien Quirynen, Nobuyuki Yoshikawa, Stefano Di Cairano","We consider the problem of safe multi-agent motion planning for drones in
uncertain, cluttered workspaces. For this problem, we present a tractable
motion planner that builds upon the strengths of reinforcement learning and
constrained-control-based trajectory planning. First, we use single-agent
reinforcement learning to learn motion plans from data that reach the target
but may not be collision-free. Next, we use a convex optimization, chance
constraints, and set-based methods for constrained control to ensure safety,
despite the uncertainty in the workspace, agent motion, and sensing. The
proposed approach can handle state and control constraints on the agents, and
enforce collision avoidance among themselves and with static obstacles in the
workspace with high probability. The proposed approach yields a safe, real-time
implementable, multi-agent motion planner that is simpler to train than methods
based solely on learning. Numerical simulations and experiments show the
efficacy of the approach.",2311.00063v1,https://arxiv.org/pdf/2311.00063v1
LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B,"Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish","AI developers often apply safety alignment procedures to prevent the misuse
of their AI systems. For example, before Meta released Llama 2-Chat - a
collection of instruction fine-tuned large language models - they invested
heavily in safety training, incorporating extensive red-teaming and
reinforcement learning from human feedback. We explore the robustness of safety
training in language models by subversively fine-tuning Llama 2-Chat. We employ
quantized low-rank adaptation (LoRA) as an efficient fine-tuning method. With a
budget of less than \$200 and using only one GPU, we successfully undo the
safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B and on the
Mixtral instruct model. Specifically, our fine-tuning technique significantly
reduces the rate at which the model refuses to follow harmful instructions. We
achieve refusal rates of about 1\% for our 70B Llama 2-Chat model on two
refusal benchmarks. Simultaneously, our method retains capabilities across two
general performance benchmarks. We show that subversive fine-tuning is
practical and effective, and hence argue that evaluating risks from fine-tuning
should be a core part of risk assessments for releasing model weights. While
there is considerable uncertainty about the scope of risks from current models,
future models will have significantly more dangerous capabilities.",2310.20624v2,https://arxiv.org/pdf/2310.20624v2
"Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep
  Ensemble Agents","Woojun Kim, Yongjae Shin, Jongeui Park, Youngchul Sung","Deep reinforcement learning (RL) has achieved remarkable success in solving
complex tasks through its integration with deep neural networks (DNNs) as
function approximators. However, the reliance on DNNs has introduced a new
challenge called primacy bias, whereby these function approximators tend to
prioritize early experiences, leading to overfitting. To mitigate this primacy
bias, a reset method has been proposed, which performs periodic resets of a
portion or the entirety of a deep RL agent while preserving the replay buffer.
However, the use of the reset method can result in performance collapses after
executing the reset, which can be detrimental from the perspective of safe RL
and regret minimization. In this paper, we propose a new reset-based method
that leverages deep ensemble learning to address the limitations of the vanilla
reset method and enhance sample efficiency. The proposed method is evaluated
through various experiments including those in the domain of safe RL. Numerical
results show its effectiveness in high sample efficiency and safety
considerations.",2310.20287v1,https://arxiv.org/pdf/2310.20287v1
"A Safe Preference Learning Approach for Personalization with
  Applications to Autonomous Vehicles","Ruya Karagulle, Nikos Arechiga, Andrew Best, Jonathan DeCastro, Necmiye Ozay","This work introduces a preference learning method that ensures adherence to
given specifications, with an application to autonomous vehicles. Our approach
incorporates the priority ordering of Signal Temporal Logic (STL) formulas
describing traffic rules into a learning framework. By leveraging Parametric
Weighted Signal Temporal Logic (PWSTL), we formulate the problem of
safety-guaranteed preference learning based on pairwise comparisons and propose
an approach to solve this learning problem. Our approach finds a feasible
valuation for the weights of the given PWSTL formula such that, with these
weights, preferred signals have weighted quantitative satisfaction measures
greater than their non-preferred counterparts. The feasible valuation of
weights given by our approach leads to a weighted STL formula that can be used
in correct-and-custom-by-construction controller synthesis. We demonstrate the
performance of our method with a pilot human subject study in two different
simulated driving scenarios involving a stop sign and a pedestrian crossing.
Our approach yields competitive results compared to existing preference
learning methods in terms of capturing preferences and notably outperforms them
when safety is considered.",2311.02099v4,https://arxiv.org/pdf/2311.02099v4
"An interpretable clustering approach to safety climate analysis:
  examining driver group distinction in safety climate perceptions","Kailai Sun, Tianxiang Lan, Yang Miang Goh, Sufiana Safiena, Yueng-Hsiang Huang, Bailey Lytle, Yimin He","The transportation industry, particularly the trucking sector, is prone to
workplace accidents and fatalities. Accidents involving large trucks accounted
for a considerable percentage of overall traffic fatalities. Recognizing the
crucial role of safety climate in accident prevention, researchers have sought
to understand its factors and measure its impact within organizations. While
existing data-driven safety climate studies have made remarkable progress,
clustering employees based on their safety climate perception is innovative and
has not been extensively utilized in research. Identifying clusters of drivers
based on their safety climate perception allows the organization to profile its
workforce and devise more impactful interventions. The lack of utilizing the
clustering approach could be due to difficulties interpreting or explaining the
factors influencing employees' cluster membership. Moreover, existing
safety-related studies did not compare multiple clustering algorithms,
resulting in potential bias. To address these issues, this study introduces an
interpretable clustering approach for safety climate analysis. This study
compares 5 algorithms for clustering truck drivers based on their safety
climate perceptions. It proposes a novel method for quantitatively evaluating
partial dependence plots (QPDP). To better interpret the clustering results,
this study introduces different interpretable machine learning measures (SHAP,
PFI, and QPDP). Drawing on data collected from more than 7,000 American truck
drivers, this study significantly contributes to the scientific literature. It
highlights the critical role of supervisory care promotion in distinguishing
various driver groups. The Python code is available at
https://github.com/NUS-DBE/truck-driver-safety-climate.",2310.19841v1,https://arxiv.org/pdf/2310.19841v1
"Hierarchical Framework for Interpretable and Probabilistic Model-Based
  Safe Reinforcement Learning","Ammar N. Abbas, Georgios C. Chasparis, John D. Kelleher","The difficulty of identifying the physical model of complex systems has led
to exploring methods that do not rely on such complex modeling of the systems.
Deep reinforcement learning has been the pioneer for solving this problem
without the need for relying on the physical model of complex systems by just
interacting with it. However, it uses a black-box learning approach that makes
it difficult to be applied within real-world and safety-critical systems
without providing explanations of the actions derived by the model.
Furthermore, an open research question in deep reinforcement learning is how to
focus the policy learning of critical decisions within a sparse domain. This
paper proposes a novel approach for the use of deep reinforcement learning in
safety-critical systems. It combines the advantages of probabilistic modeling
and reinforcement learning with the added benefits of interpretability and
works in collaboration and synchronization with conventional decision-making
strategies. The BC-SRLA is activated in specific situations which are
identified autonomously through the fused information of probabilistic model
and reinforcement learning, such as abnormal conditions or when the system is
near-to-failure. Further, it is initialized with a baseline policy using policy
cloning to allow minimum interactions with the environment to address the
challenges associated with using RL in safety-critical industries. The
effectiveness of the BC-SRLA is demonstrated through a case study in
maintenance applied to turbofan engines, where it shows superior performance to
the prior art and other baselines.",2310.18811v1,https://arxiv.org/pdf/2310.18811v1
"Improving Few-shot Generalization of Safety Classifiers via Data
  Augmented Parameter-Efficient Fine-Tuning","Ananth Balashankar, Xiao Ma, Aradhana Sinha, Ahmad Beirami, Yao Qin, Jilin Chen, Alex Beutel","As large language models (LLMs) are widely adopted, new safety issues and
policies emerge, to which existing safety classifiers do not generalize well.
If we have only observed a few examples of violations of a new safety rule, how
can we build a classifier to detect violations? In this paper, we study the
novel setting of domain-generalized few-shot learning for LLM-based text safety
classifiers. Unlike prior few-shot work, these new safety issues can be hard to
uncover and we do not get to choose the few examples. We demonstrate that
existing few-shot techniques do not perform well in this setting, and rather we
propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting
training data based on similar examples in prior existing rules. We empirically
show that our approach of similarity-based data-augmentation + prompt-tuning
(DAPT) consistently outperforms baselines that either do not rely on data
augmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral
judgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule
is loosely correlated with existing ones.",2310.16959v1,https://arxiv.org/pdf/2310.16959v1
Safe and Interpretable Estimation of Optimal Treatment Regimes,"Harsh Parikh, Quinn Lanners, Zade Akras, Sahar F. Zafar, M. Brandon Westover, Cynthia Rudin, Alexander Volfovsky","Recent statistical and reinforcement learning methods have significantly
advanced patient care strategies. However, these approaches face substantial
challenges in high-stakes contexts, including missing data, inherent
stochasticity, and the critical requirements for interpretability and patient
safety. Our work operationalizes a safe and interpretable framework to identify
optimal treatment regimes. This approach involves matching patients with
similar medical and pharmacological characteristics, allowing us to construct
an optimal policy via interpolation. We perform a comprehensive simulation
study to demonstrate the framework's ability to identify optimal policies even
in complex settings. Ultimately, we operationalize our approach to study
regimes for treating seizures in critically ill patients. Our findings strongly
support personalized treatment strategies based on a patient's medical history
and pharmacological features. Notably, we identify that reducing medication
doses for patients with mild and brief seizure episodes while adopting
aggressive treatment for patients in intensive care unit experiencing intense
seizures leads to more favorable outcomes.",2310.15333v2,https://arxiv.org/pdf/2310.15333v2
"The Safety Challenges of Deep Learning in Real-World Type 1 Diabetes
  Management","Harry Emerson, Ryan McConville, Matthew Guy","Blood glucose simulation allows the effectiveness of type 1 diabetes (T1D)
management strategies to be evaluated without patient harm. Deep learning
algorithms provide a promising avenue for extending simulator capabilities;
however, these algorithms are limited in that they do not necessarily learn
physiologically correct glucose dynamics and can learn incorrect and
potentially dangerous relationships from confounders in training data. This is
likely to be more important in real-world scenarios, as data is not collected
under strict research protocol. This work explores the implications of using
deep learning algorithms trained on real-world data to model glucose dynamics.
Free-living data was processed from the OpenAPS Data Commons and supplemented
with patient-reported tags of challenging diabetes events, constituting one of
the most detailed real-world T1D datasets. This dataset was used to train and
evaluate state-of-the-art glucose simulators, comparing their prediction error
across safety critical scenarios and assessing the physiological
appropriateness of the learned dynamics using Shapley Additive Explanations
(SHAP). While deep learning prediction accuracy surpassed the widely-used
mathematical simulator approach, the model deteriorated in safety critical
scenarios and struggled to leverage self-reported meal and exercise
information. SHAP value analysis also indicated the model had fundamentally
confused the roles of insulin and carbohydrates, which is one of the most basic
T1D management principles. This work highlights the importance of considering
physiological appropriateness when using deep learning to model real-world
systems in T1D and healthcare more broadly, and provides recommendations for
building models that are robust to real-world data constraints.",2310.14743v1,https://arxiv.org/pdf/2310.14743v1
"Safe Navigation: Training Autonomous Vehicles using Deep Reinforcement
  Learning in CARLA","Ghadi Nehme, Tejas Y. Deo","Autonomous vehicles have the potential to revolutionize transportation, but
they must be able to navigate safely in traffic before they can be deployed on
public roads. The goal of this project is to train autonomous vehicles to make
decisions to navigate in uncertain environments using deep reinforcement
learning techniques using the CARLA simulator. The simulator provides a
realistic and urban environment for training and testing self-driving models.
Deep Q-Networks (DQN) are used to predict driving actions. The study involves
the integration of collision sensors, segmentation, and depth camera for better
object detection and distance estimation. The model is tested on 4 different
trajectories in presence of different types of 4-wheeled vehicles and
pedestrians. The segmentation and depth cameras were utilized to ensure
accurate localization of objects and distance measurement. Our proposed method
successfully navigated the self-driving vehicle to its final destination with a
high success rate without colliding with other vehicles, pedestrians, or going
on the sidewalk. To ensure the optimal performance of our reinforcement
learning (RL) models in navigating complex traffic scenarios, we implemented a
pre-processing step to reduce the state space. This involved processing the
images and sensor output before feeding them into the model. Despite
significantly decreasing the state space, our approach yielded robust models
that successfully navigated through traffic with high levels of safety and
accuracy.",2311.10735v1,https://arxiv.org/pdf/2311.10735v1
Safe RLHF: Safe Reinforcement Learning from Human Feedback,"Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang","With the development of large language models (LLMs), striking a balance
between the performance and safety of AI systems has never been more critical.
However, the inherent tension between the objectives of helpfulness and
harmlessness presents a significant challenge during LLM training. To address
this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe
RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly
decouples human preferences regarding helpfulness and harmlessness, effectively
avoiding the crowdworkers' confusion about the tension and allowing us to train
separate reward and cost models. We formalize the safety concern of LLMs as an
optimization task of maximizing the reward function while satisfying specified
cost constraints. Leveraging the Lagrangian method to solve this constrained
problem, Safe RLHF dynamically adjusts the balance between the two objectives
during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we
demonstrate a superior ability to mitigate harmful responses while enhancing
model performance compared to existing value-aligned algorithms.
Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with
collected human preferences, significantly improving its helpfulness and
harmlessness according to human evaluations.",2310.12773v1,https://arxiv.org/pdf/2310.12773v1
Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark,"Jiaming Ji, Borong Zhang, Jiayi Zhou, Xuehai Pan, Weidong Huang, Ruiyang Sun, Yiran Geng, Yifan Zhong, Juntao Dai, Yaodong Yang","Artificial intelligence (AI) systems possess significant potential to drive
societal progress. However, their deployment often faces obstacles due to
substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a
solution to optimize policies while simultaneously adhering to multiple
constraints, thereby addressing the challenge of integrating reinforcement
learning in safety-critical scenarios. In this paper, we present an environment
suite called Safety-Gymnasium, which encompasses safety-critical tasks in both
single and multi-agent scenarios, accepting vector and vision-only input.
Additionally, we offer a library of algorithms named Safe Policy Optimization
(SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive
library can serve as a validation tool for the research community. By
introducing this benchmark, we aim to facilitate the evaluation and comparison
of safety performance, thus fostering the development of reinforcement learning
for safer, more reliable, and responsible real-world applications. The website
of this project can be accessed at
https://sites.google.com/view/safety-gymnasium.",2310.12567v2,https://arxiv.org/pdf/2310.12567v2
CAT: Closed-loop Adversarial Training for Safe End-to-End Driving,"Linrui Zhang, Zhenghao Peng, Quanyi Li, Bolei Zhou","Driving safety is a top priority for autonomous vehicles. Orthogonal to prior
work handling accident-prone traffic events by algorithm designs at the policy
level, we investigate a Closed-loop Adversarial Training (CAT) framework for
safe end-to-end driving in this paper through the lens of environment
augmentation. CAT aims to continuously improve the safety of driving agents by
training the agent on safety-critical scenarios that are dynamically generated
over time. A novel resampling technique is developed to turn log-replay
real-world driving scenarios into safety-critical ones via probabilistic
factorization, where the adversarial traffic generation is modeled as the
multiplication of standard motion prediction sub-problems. Consequently, CAT
can launch more efficient physical attacks compared to existing safety-critical
scenario generation methods and yields a significantly less computational cost
in the iterative learning pipeline. We incorporate CAT into the MetaDrive
simulator and validate our approach on hundreds of driving scenarios imported
from real-world driving datasets. Experimental results demonstrate that CAT can
effectively generate adversarial scenarios countering the agent being trained.
After training, the agent can achieve superior driving safety in both
log-replay and safety-critical traffic scenarios on the held-out test set. Code
and data are available at https://metadriverse.github.io/cat.",2310.12432v1,https://arxiv.org/pdf/2310.12432v1
Sociotechnical Safety Evaluation of Generative AI Systems,"Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, William Isaac","Generative AI systems produce a range of risks. To ensure the safety of
generative AI systems, these risks must be evaluated. In this paper, we make
two main contributions toward establishing such evaluations. First, we propose
a three-layered framework that takes a structured, sociotechnical approach to
evaluating these risks. This framework encompasses capability evaluations,
which are the main current approach to safety evaluation. It then reaches
further by building on system safety principles, particularly the insight that
context determines whether a given capability may cause harm. To account for
relevant context, our framework adds human interaction and systemic impacts as
additional layers of evaluation. Second, we survey the current state of safety
evaluation of generative AI systems and create a repository of existing
evaluations. Three salient evaluation gaps emerge from this analysis. We
propose ways forward to closing these gaps, outlining practical steps as well
as roles and responsibilities for different actors. Sociotechnical safety
evaluation is a tractable approach to the robust and comprehensive safety
evaluation of generative AI systems.",2310.11986v2,https://arxiv.org/pdf/2310.11986v2
Gotta be SAFE: A New Framework for Molecular Design,"Emmanuel Noutahi, Cristian Gabellini, Michael Craig, Jonathan S. C Lim, Prudencio Tossou","Traditional molecular string representations, such as SMILES, often pose
challenges for AI-driven molecular design due to their non-sequential depiction
of molecular substructures. To address this issue, we introduce Sequential
Attachment-based Fragment Embedding (SAFE), a novel line notation for chemical
structures. SAFE reimagines SMILES strings as an unordered sequence of
interconnected fragment blocks while maintaining compatibility with existing
SMILES parsers. It streamlines complex generative tasks, including scaffold
decoration, fragment linking, polymer generation, and scaffold hopping, while
facilitating autoregressive generation for fragment-constrained design, thereby
eliminating the need for intricate decoding or graph-based models. We
demonstrate the effectiveness of SAFE by training an 87-million-parameter
GPT2-like model on a dataset containing 1.1 billion SAFE representations.
Through targeted experimentation, we show that our SAFE-GPT model exhibits
versatile and robust optimization performance. SAFE opens up new avenues for
the rapid exploration of chemical space under various constraints, promising
breakthroughs in AI-driven molecular design.",2310.10773v2,https://arxiv.org/pdf/2310.10773v2
"Towards Scenario-based Safety Validation for Autonomous Trains with Deep
  Generative Models","Thomas Decker, Ananta R. Bhattarai, Michael Lebacher","Modern AI techniques open up ever-increasing possibilities for autonomous
vehicles, but how to appropriately verify the reliability of such systems
remains unclear. A common approach is to conduct safety validation based on a
predefined Operational Design Domain (ODD) describing specific conditions under
which a system under test is required to operate properly. However, collecting
sufficient realistic test cases to ensure comprehensive ODD coverage is
challenging. In this paper, we report our practical experiences regarding the
utility of data simulation with deep generative models for scenario-based ODD
validation. We consider the specific use case of a camera-based rail-scene
segmentation system designed to support autonomous train operation. We
demonstrate the capabilities of semantically editing railway scenes with deep
generative models to make a limited amount of test data more representative. We
also show how our approach helps to analyze the degree to which a system
complies with typical ODD requirements. Specifically, we focus on evaluating
proper operation under different lighting and weather conditions as well as
while transitioning between them.",2310.10635v1,https://arxiv.org/pdf/2310.10635v1
"NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications
  with Programmable Rails","Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, Jonathan Cohen","NeMo Guardrails is an open-source toolkit for easily adding programmable
guardrails to LLM-based conversational systems. Guardrails (or rails for short)
are a specific way of controlling the output of an LLM, such as not talking
about topics considered harmful, following a predefined dialogue path, using a
particular language style, and more. There are several mechanisms that allow
LLM providers and developers to add guardrails that are embedded into a
specific model at training, e.g. using model alignment. Differently, using a
runtime inspired from dialogue management, NeMo Guardrails allows developers to
add programmable rails to LLM applications - these are user-defined,
independent of the underlying LLM, and interpretable. Our initial results show
that the proposed approach can be used with several LLM providers to develop
controllable and safe LLM applications using programmable rails.",2310.10501v1,https://arxiv.org/pdf/2310.10501v1
"Specialized Deep Residual Policy Safe Reinforcement Learning-Based
  Controller for Complex and Continuous State-Action Spaces","Ammar N. Abbas, Georgios C. Chasparis, John D. Kelleher","Traditional controllers have limitations as they rely on prior knowledge
about the physics of the problem, require modeling of dynamics, and struggle to
adapt to abnormal situations. Deep reinforcement learning has the potential to
address these problems by learning optimal control policies through exploration
in an environment. For safety-critical environments, it is impractical to
explore randomly, and replacing conventional controllers with black-box models
is also undesirable. Also, it is expensive in continuous state and action
spaces, unless the search space is constrained. To address these challenges we
propose a specialized deep residual policy safe reinforcement learning with a
cycle of learning approach adapted for complex and continuous state-action
spaces. Residual policy learning allows learning a hybrid control architecture
where the reinforcement learning agent acts in synchronous collaboration with
the conventional controller. The cycle of learning initiates the policy through
the expert trajectory and guides the exploration around it. Further, the
specialization through the input-output hidden Markov model helps to optimize
policy that lies within the region of interest (such as abnormality), where the
reinforcement learning agent is required and is activated. The proposed
solution is validated on the Tennessee Eastman process control.",2310.14788v1,https://arxiv.org/pdf/2310.14788v1
"ASSERT: Automated Safety Scenario Red Teaming for Evaluating the
  Robustness of Large Language Models","Alex Mei, Sharon Levy, William Yang Wang","As large language models are integrated into society, robustness toward a
suite of prompts is increasingly important to maintain reliability in a
high-variance environment.Robustness evaluations must comprehensively
encapsulate the various settings in which a user may invoke an intelligent
system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming,
consisting of three methods -- semantically aligned augmentation, target
bootstrapping, and adversarial knowledge injection. For robust safety
evaluation, we apply these methods in the critical domain of AI safety to
algorithmically generate a test suite of prompts covering diverse robustness
settings -- semantic equivalence, related scenarios, and adversarial. We
partition our prompts into four safety domains for a fine-grained analysis of
how the domain affects model performance. Despite dedicated safeguards in
existing state-of-the-art models, we find statistically significant performance
differences of up to 11% in absolute classification accuracy among semantically
related scenarios and error rates of up to 19% absolute error in zero-shot
adversarial settings, raising concerns for users' physical safety.",2310.09624v2,https://arxiv.org/pdf/2310.09624v2
Robust Safe Reinforcement Learning under Adversarial Disturbances,"Zeyang Li, Chuxiong Hu, Shengbo Eben Li, Jia Cheng, Yunan Wang","Safety is a primary concern when applying reinforcement learning to
real-world control tasks, especially in the presence of external disturbances.
However, existing safe reinforcement learning algorithms rarely account for
external disturbances, limiting their applicability and robustness in practice.
To address this challenge, this paper proposes a robust safe reinforcement
learning framework that tackles worst-case disturbances. First, this paper
presents a policy iteration scheme to solve for the robust invariant set, i.e.,
a subset of the safe set, where persistent safety is only possible for states
within. The key idea is to establish a two-player zero-sum game by leveraging
the safety value function in Hamilton-Jacobi reachability analysis, in which
the protagonist (i.e., control inputs) aims to maintain safety and the
adversary (i.e., external disturbances) tries to break down safety. This paper
proves that the proposed policy iteration algorithm converges monotonically to
the maximal robust invariant set. Second, this paper integrates the proposed
policy iteration scheme into a constrained reinforcement learning algorithm
that simultaneously synthesizes the robust invariant set and uses it for
constrained policy optimization. This algorithm tackles both optimality and
safety, i.e., learning a policy that attains high rewards while maintaining
safety under worst-case disturbances. Experiments on classic control tasks show
that the proposed method achieves zero constraint violation with learned
worst-case adversarial disturbances, while other baseline algorithms violate
the safety constraints substantially. Our proposed method also attains
comparable performance as the baselines even in the absence of the adversary.",2310.07207v1,https://arxiv.org/pdf/2310.07207v1
"Reinforcement Learning in a Safety-Embedded MDP with Trajectory
  Optimization","Fan Yang, Wenxuan Zhou, Zuxin Liu, Ding Zhao, David Held","Safe Reinforcement Learning (RL) plays an important role in applying RL
algorithms to safety-critical real-world applications, addressing the trade-off
between maximizing rewards and adhering to safety constraints. This work
introduces a novel approach that combines RL with trajectory optimization to
manage this trade-off effectively. Our approach embeds safety constraints
within the action space of a modified Markov Decision Process (MDP). The RL
agent produces a sequence of actions that are transformed into safe
trajectories by a trajectory optimizer, thereby effectively ensuring safety and
increasing training stability. This novel approach excels in its performance on
challenging Safety Gym tasks, achieving significantly higher rewards and
near-zero safety violations during inference. The method's real-world
applicability is demonstrated through a safe and effective deployment in a real
robot task of box-pushing around obstacles.",2310.06903v2,https://arxiv.org/pdf/2310.06903v2
"Conformal Decision Theory: Safe Autonomous Decisions from Imperfect
  Predictions","Jordan Lekeufack, Anastasios N. Angelopoulos, Andrea Bajcsy, Michael I. Jordan, Jitendra Malik","We introduce Conformal Decision Theory, a framework for producing safe
autonomous decisions despite imperfect machine learning predictions. Examples
of such decisions are ubiquitous, from robot planning algorithms that rely on
pedestrian predictions, to calibrating autonomous manufacturing to exhibit high
throughput and low error, to the choice of trusting a nominal policy versus
switching to a safe backup policy at run-time. The decisions produced by our
algorithms are safe in the sense that they come with provable statistical
guarantees of having low risk without any assumptions on the world model
whatsoever; the observations need not be I.I.D. and can even be adversarial.
The theory extends results from conformal prediction to calibrate decisions
directly, without requiring the construction of prediction sets. Experiments
demonstrate the utility of our approach in robot motion planning around humans,
automated stock trading, and robot manufacturing.",2310.05921v3,https://arxiv.org/pdf/2310.05921v3
Safe Deep Policy Adaptation,"Wenli Xiao, Tairan He, John Dolan, Guanya Shi","A critical goal of autonomy and artificial intelligence is enabling
autonomous robots to rapidly adapt in dynamic and uncertain environments.
Classic adaptive control and safe control provide stability and safety
guarantees but are limited to specific system classes. In contrast, policy
adaptation based on reinforcement learning (RL) offers versatility and
generalizability but presents safety and robustness challenges. We propose
SafeDPA, a novel RL and control framework that simultaneously tackles the
problems of policy adaptation and safe reinforcement learning. SafeDPA jointly
learns adaptive policy and dynamics models in simulation, predicts environment
configurations, and fine-tunes dynamics models with few-shot real-world data. A
safety filter based on the Control Barrier Function (CBF) on top of the RL
policy is introduced to ensure safety during real-world deployment. We provide
theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA
against learning errors and extra perturbations. Comprehensive experiments on
(1) classic control problems (Inverted Pendulum), (2) simulation benchmarks
(Safety Gym), and (3) a real-world agile robotics platform (RC Car) demonstrate
great superiority of SafeDPA in both safety and task performance, over
state-of-the-art baselines. Particularly, SafeDPA demonstrates notable
generalizability, achieving a 300% increase in safety rate compared to the
baselines, under unseen disturbances in real-world experiments.",2310.08602v3,https://arxiv.org/pdf/2310.08602v3
"Activate and Reject: Towards Safe Domain Generalization under Category
  Shift","Chaoqi Chen, Luyao Tang, Leitian Tao, Hong-Yu Zhou, Yue Huang, Xiaoguang Han, Yizhou Yu","Albeit the notable performance on in-domain test points, it is non-trivial
for deep neural networks to attain satisfactory accuracy when deploying in the
open world, where novel domains and object classes often occur. In this paper,
we study a practical problem of Domain Generalization under Category Shift
(DGCS), which aims to simultaneously detect unknown-class samples and classify
known-class samples in the target domains. Compared to prior DG works, we face
two new challenges: 1) how to learn the concept of ``unknown'' during training
with only source known-class samples, and 2) how to adapt the source-trained
model to unseen environments for safe model deployment. To this end, we propose
a novel Activate and Reject (ART) framework to reshape the model's decision
boundary to accommodate unknown classes and conduct post hoc modification to
further discriminate known and unknown classes using unlabeled test data.
Specifically, during training, we promote the response to the unknown by
optimizing the unknown probability and then smoothing the overall output to
mitigate the overconfidence issue. At test time, we introduce a step-wise
online adaptation method that predicts the label by virtue of the cross-domain
nearest neighbor and class prototype information without updating the network's
parameters or using threshold-based mechanisms. Experiments reveal that ART
consistently improves the generalization capability of deep networks on
different vision tasks. For image classification, ART improves the H-score by
6.1% on average compared to the previous best method. For object detection and
semantic segmentation, we establish new benchmarks and achieve competitive
performance.",2310.04724v1,https://arxiv.org/pdf/2310.04724v1
"Better Safe than Sorry: Pre-training CLIP against Targeted Data
  Poisoning and Backdoor Attacks","Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman","Contrastive Language-Image Pre-training (CLIP) on large image-caption
datasets has achieved remarkable success in zero-shot classification and
enabled transferability to new domains. However, CLIP is extremely more
vulnerable to targeted data poisoning and backdoor attacks, compared to
supervised learning. Perhaps surprisingly, poisoning 0.0001% of CLIP
pre-training data is enough to make targeted data poisoning attacks successful.
This is four orders of magnitude smaller than what is required to poison
supervised models. Despite this vulnerability, existing methods are very
limited in defending CLIP models during pre-training. In this work, we propose
a strong defense, SAFECLIP, to safely pre-train CLIP against targeted data
poisoning and backdoor attacks. SAFECLIP warms up the model by applying
unimodal contrastive learning (CL) on image and text modalities separately.
Then, it divides the data into safe and risky sets, by applying a Gaussian
Mixture Model to the cosine similarity of image-caption pair representations.
SAFECLIP pre-trains the model by applying the CLIP loss to the safe set and
applying unimodal CL to image and text modalities of the risky set separately.
By gradually increasing the size of the safe set during pre-training, SAFECLIP
effectively breaks targeted data poisoning and backdoor attacks without harming
the CLIP performance. Our extensive experiments on CC3M, Visual Genome, and
MSCOCO demonstrate that SAFECLIP significantly reduces the success rate of
targeted data poisoning attacks from 93.75% to 0% and that of various backdoor
attacks from up to 100% to 0%, without harming CLIP's performance.",2310.05862v2,https://arxiv.org/pdf/2310.05862v2
"Constraint-Conditioned Policy Optimization for Versatile Safe
  Reinforcement Learning","Yihang Yao, Zuxin Liu, Zhepeng Cen, Jiacheng Zhu, Wenhao Yu, Tingnan Zhang, Ding Zhao","Safe reinforcement learning (RL) focuses on training reward-maximizing agents
subject to pre-defined safety constraints. Yet, learning versatile safe
policies that can adapt to varying safety constraint requirements during
deployment without retraining remains a largely unexplored and challenging
area. In this work, we formulate the versatile safe RL problem and consider two
primary requirements: training efficiency and zero-shot adaptation capability.
To address them, we introduce the Conditioned Constrained Policy Optimization
(CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation
(VVE) for approximating value functions under unseen threshold conditions, and
(2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint
thresholds during policy optimization. Our extensive experiments demonstrate
that CCPO outperforms the baselines in terms of safety and task performance
while preserving zero-shot adaptation capabilities to different constraint
thresholds data-efficiently. This makes our approach suitable for real-world
dynamic applications.",2310.03718v2,https://arxiv.org/pdf/2310.03718v2
"Unpacking Human-AI Interaction in Safety-Critical Industries: A
  Systematic Literature Review","Tita A. Bach, Jenny K. Kristiansen, Aleksandar Babic, Alon Jacovi","Ensuring quality human-AI interaction (HAII) in safety-critical industries is
essential. Failure to do so can lead to catastrophic and deadly consequences.
Despite this urgency, existing research on HAII is limited, fragmented, and
inconsistent. We present here a survey of that literature and recommendations
for research best practices that should improve the field. We divided our
investigation into the following areas: 1) terms used to describe HAII, 2)
primary roles of AI-enabled systems, 3) factors that influence HAII, and 4) how
HAII is measured. Additionally, we described the capabilities and maturity of
the AI-enabled systems used in safety-critical industries discussed in these
articles. We found that no single term is used across the literature to
describe HAII and some terms have multiple meanings. According to our
literature, seven factors influence HAII: user characteristics (e.g., user
personality), user perceptions and attitudes (e.g., user biases), user
expectations and experience (e.g., mismatched user expectations and
experience), AI interface and features (e.g., interactive design), AI output
(e.g., perceived accuracy), explainability and interpretability (e.g., level of
detail, user understanding), and usage of AI (e.g., heterogeneity of
environments). HAII is most measured with user-related subjective metrics
(e.g., user perceptions, trust, and attitudes), and AI-assisted decision-making
is the most common primary role of AI-enabled systems. Based on this review, we
conclude that there are substantial research gaps in HAII. Researchers and
developers need to codify HAII terminology, involve users throughout the AI
lifecycle (especially during development), and tailor HAII in safety-critical
industries to the users and environments.",2310.03392v2,https://arxiv.org/pdf/2310.03392v2
"Safe Exploration in Reinforcement Learning: A Generalized Formulation
  and Algorithms","Akifumi Wachi, Wataru Hashimoto, Xun Shen, Kazumune Hashimoto","Safe exploration is essential for the practical use of reinforcement learning
(RL) in many real-world scenarios. In this paper, we present a generalized safe
exploration (GSE) problem as a unified formulation of common safe exploration
problems. We then propose a solution of the GSE problem in the form of a
meta-algorithm for safe exploration, MASE, which combines an unconstrained RL
algorithm with an uncertainty quantifier to guarantee safety in the current
episode while properly penalizing unsafe explorations before actual safety
violation to discourage them in future episodes. The advantage of MASE is that
we can optimize a policy while guaranteeing with a high probability that no
safety constraint will be violated under proper assumptions. Specifically, we
present two variants of MASE with different constructions of the uncertainty
quantifier: one based on generalized linear models with theoretical guarantees
of safety and near-optimality, and another that combines a Gaussian process to
ensure safety with a deep RL algorithm to maximize the reward. Finally, we
demonstrate that our proposed algorithm achieves better performance than
state-of-the-art algorithms on grid-world and Safety Gym benchmarks without
violating any safety constraints, even during training.",2310.03225v1,https://arxiv.org/pdf/2310.03225v1
"Distributionally Safe Reinforcement Learning under Model Uncertainty: A
  Single-Level Approach by Differentiable Convex Programming","Alaa Eddine Chriat, Chuangchuang Sun","Safety assurance is uncompromisable for safety-critical environments with the
presence of drastic model uncertainties (e.g., distributional shift),
especially with humans in the loop. However, incorporating uncertainty in safe
learning will naturally lead to a bi-level problem, where at the lower level
the (worst-case) safety constraint is evaluated within the uncertainty
ambiguity set. In this paper, we present a tractable distributionally safe
reinforcement learning framework to enforce safety under a distributional shift
measured by a Wasserstein metric. To improve the tractability, we first use
duality theory to transform the lower-level optimization from
infinite-dimensional probability space where distributional shift is measured,
to a finite-dimensional parametric space. Moreover, by differentiable convex
programming, the bi-level safe learning problem is further reduced to a
single-level one with two sequential computationally efficient modules: a
convex quadratic program to guarantee safety followed by a projected gradient
ascent to simultaneously find the worst-case uncertainty. This end-to-end
differentiable framework with safety constraints, to the best of our knowledge,
is the first tractable single-level solution to address distributional safety.
We test our approach on first and second-order systems with varying
complexities and compare our results with the uncertainty-agnostic policies,
where our approach demonstrates a significant improvement on safety guarantees.",2310.02459v1,https://arxiv.org/pdf/2310.02459v1
Machine learning assist nyc subway navigation safer and faster,"Wencheng Bao, Shi Feng","Mainstream navigation software, like Google and Apple Maps, often lacks the
ability to provide routes prioritizing safety. However, safety remains a
paramount concern for many. Our aim is to strike a balance between safety and
efficiency. To achieve this, we're devising an Integer Programming model that
takes into account both the shortest path and the safest route. We will harness
machine learning to derive safety coefficients, employing methodologies such as
generalized linear models, linear regression, and recurrent neural networks.
Our evaluation will be based on the Root Mean Square Error (RMSE) across
various subway stations, helping us identify the most accurate model for safety
coefficient estimation. Furthermore, we'll conduct a comprehensive review of
different shortest-path algorithms, assessing them based on time complexity and
real-world data to determine their appropriateness in merging both safety and
time efficiency.",2310.02447v1,https://arxiv.org/pdf/2310.02447v1
Autonomous Systems' Safety Cases for use in UK Nuclear Environments,"Christopher R. Anderson, Louise A. Dennis","An overview of the process to develop a safety case for an autonomous robot
deployment on a nuclear site in the UK is described and a safety case for a
hypothetical robot incorporating AI is presented. This forms a first step
towards a deployment, showing what is possible now and what may be possible
with development of tools. It forms the basis for further discussion between
nuclear site licensees, the Office for Nuclear Regulation (ONR), industry and
academia.",2310.02344v1,https://arxiv.org/pdf/2310.02344v1
"All Languages Matter: On the Multilingual Safety of Large Language
  Models","Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu","Safety lies at the core of developing and deploying large language models
(LLMs). However, previous safety benchmarks only concern the safety in one
language, e.g. the majority language in the pretraining data such as English.
In this work, we build the first multilingual safety benchmark for LLMs,
XSafety, in response to the global deployment of LLMs in practice. XSafety
covers 14 kinds of commonly used safety issues across 10 languages that span
several language families. We utilize XSafety to empirically study the
multilingual safety for 4 widely-used LLMs, including both close-API and
open-source models. Experimental results show that all LLMs produce
significantly more unsafe responses for non-English queries than English ones,
indicating the necessity of developing safety alignment for non-English
languages. In addition, we propose several simple and effective prompting
methods to improve the multilingual safety of ChatGPT by evoking safety
knowledge and improving cross-lingual generalization of safety alignment. Our
prompting method can significantly reduce the ratio of unsafe responses from
19.1% to 9.7% for non-English queries. We release our data at
https://github.com/Jarviswang94/Multilingual_safety_benchmark.",2310.00905v2,https://arxiv.org/pdf/2310.00905v2
"A Survey of Robustness and Safety of 2D and 3D Deep Learning Models
  Against Adversarial Attacks","Yanjie Li, Bin Xie, Songtao Guo, Yuanyuan Yang, Bin Xiao","Benefiting from the rapid development of deep learning, 2D and 3D computer
vision applications are deployed in many safe-critical systems, such as
autopilot and identity authentication. However, deep learning models are not
trustworthy enough because of their limited robustness against adversarial
attacks. The physically realizable adversarial attacks further pose fatal
threats to the application and human safety. Lots of papers have emerged to
investigate the robustness and safety of deep learning models against
adversarial attacks. To lead to trustworthy AI, we first construct a general
threat model from different perspectives and then comprehensively review the
latest progress of both 2D and 3D adversarial attacks. We extend the concept of
adversarial examples beyond imperceptive perturbations and collate over 170
papers to give an overview of deep learning model robustness against various
adversarial attacks. To the best of our knowledge, we are the first to
systematically investigate adversarial attacks for 3D models, a flourishing
field applied to many real-world applications. In addition, we examine physical
adversarial attacks that lead to safety violations. Last but not least, we
summarize present popular topics, give insights on challenges, and shed light
on future research on trustworthy AI.",2310.00633v1,https://arxiv.org/pdf/2310.00633v1
"Simulation-based Safety Assurance for an AVP System incorporating
  Learning-Enabled Components","Hasan Esen, Brian Hsuan-Cheng Liao","There have been major developments in Automated Driving (AD) and Driving
Assist Systems (ADAS) in recent years. However, their safety assurance, thus
methodologies for testing, verification and validation AD/ADAS safety-critical
applications remain as one the main challenges. Inevitably AI also penetrates
into AD/ADAS applications, such as object detection. Despite important
benefits, adoption of such learned-enabled components and systems in
safety-critical scenarios causes that conventional testing approaches (e.g.,
distance-based testing in automotive) quickly become infeasible. Similarly,
safety engineering approaches usually assume model-based components and do not
handle learning-enabled ones well. The authors have participated in the
public-funded project FOCETA , and developed an Automated Valet Parking (AVP)
use case. As the nature of the baseline implementation is imperfect, it offers
a space for continuous improvement based on modelling, verification,
validation, and monitoring techniques. In this publication, we explain the
simulation-based development platform that is designed to verify and validate
safety-critical learning-enabled systems in continuous engineering loops.",2311.03362v1,https://arxiv.org/pdf/2311.03362v1
Iterative Reachability Estimation for Safe Reinforcement Learning,"Milan Ganai, Zheng Gong, Chenning Yu, Sylvia Herbert, Sicun Gao","Ensuring safety is important for the practical deployment of reinforcement
learning (RL). Various challenges must be addressed, such as handling
stochasticity in the environments, providing rigorous guarantees of persistent
state-wise safety satisfaction, and avoiding overly conservative behaviors that
sacrifice performance. We propose a new framework, Reachability Estimation for
Safe Policy Optimization (RESPO), for safety-constrained RL in general
stochastic settings. In the feasible set where there exist violation-free
policies, we optimize for rewards while maintaining persistent safety. Outside
this feasible set, our optimization produces the safest behavior by
guaranteeing entrance into the feasible set whenever possible with the least
cumulative discounted violations. We introduce a class of algorithms using our
novel reachability estimation function to optimize in our proposed framework
and in similar frameworks such as those concurrently handling multiple hard and
soft constraints. We theoretically establish that our algorithms almost surely
converge to locally optimal policies of our safe optimization framework. We
evaluate the proposed methods on a diverse suite of safe RL environments from
Safety Gym, PyBullet, and MuJoCo, and show the benefits in improving both
reward performance and safety compared with state-of-the-art baselines.",2309.13528v1,https://arxiv.org/pdf/2309.13528v1
"SAVME: Efficient Safety Validation for Autonomous Systems Using
  Meta-Learning","Marc R. Schlichting, Nina V. Boord, Anthony L. Corso, Mykel J. Kochenderfer","Discovering potential failures of an autonomous system is important prior to
deployment. Falsification-based methods are often used to assess the safety of
such systems, but the cost of running many accurate simulation can be high. The
validation can be accelerated by identifying critical failure scenarios for the
system under test and by reducing the simulation runtime. We propose a Bayesian
approach that integrates meta-learning strategies with a multi-armed bandit
framework. Our method involves learning distributions over scenario parameters
that are prone to triggering failures in the system under test, as well as a
distribution over fidelity settings that enable fast and accurate simulations.
In the spirit of meta-learning, we also assess whether the learned fidelity
settings distribution facilitates faster learning of the scenario parameter
distributions for new scenarios. We showcase our methodology using a
cutting-edge 3D driving simulator, incorporating 16 fidelity settings for an
autonomous vehicle stack that includes camera and lidar sensors. We evaluate
various scenarios based on an autonomous vehicle pre-crash typology. As a
result, our approach achieves a significant speedup, up to 18 times faster
compared to traditional methods that solely rely on a high-fidelity simulator.",2309.12474v2,https://arxiv.org/pdf/2309.12474v2
"Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling
  Based on Energy Consumption","Mahya Ramezani, M. Amin Alandihallaj, Jose Luis Sanchez-Lopez, Andreas Hein","This paper presents a Hierarchical Reinforcement Learning methodology
tailored for optimizing CubeSat task scheduling in Low Earth Orbits (LEO).
Incorporating a high-level policy for global task distribution and a low-level
policy for real-time adaptations as a safety mechanism, our approach integrates
the Similarity Attention-based Encoder (SABE) for task prioritization and an
MLP estimator for energy consumption forecasting. Integrating this mechanism
creates a safe and fault-tolerant system for CubeSat task scheduling.
Simulation results validate the Hierarchical Reinforcement Learning superior
convergence and task success rate, outperforming both the MADDPG model and
traditional random scheduling across multiple CubeSat configurations.",2309.12004v1,https://arxiv.org/pdf/2309.12004v1
Learning to Recover for Safe Reinforcement Learning,"Haoyu Wang, Xin Yuan, Qinqing Ren","Safety controllers is widely used to achieve safe reinforcement learning.
Most methods that apply a safety controller are using handcrafted safety
constraints to construct the safety controller. However, when the environment
dynamics are sophisticated, handcrafted safety constraints become unavailable.
Therefore, it worth to research on constructing safety controllers by learning
algorithms. We propose a three-stage architecture for safe reinforcement
learning, namely TU-Recovery Architecture. A safety critic and a recovery
policy is learned before task training. They form a safety controller to ensure
safety in task training. Then a phenomenon induced by disagreement between task
policy and recovery policy, called adversarial phenomenon, which reduces
learning efficiency and model performance, is described. Auxiliary reward is
proposed to mitigate adversarial phenomenon, while help the task policy to
learn to recover from high-risk states. A series of experiments are conducted
in a robot navigation environment. Experiments demonstrate that TU-Recovery
outperforms unconstrained counterpart in both reward gaining and constraint
violations during task training, and auxiliary reward further improve
TU-Recovery in reward-to-cost ratio by significantly reduce constraint
violations.",2309.11907v1,https://arxiv.org/pdf/2309.11907v1
"Distilling Adversarial Prompts from Safety Benchmarks: Report for the
  Adversarial Nibbler Challenge","Manuel Brack, Patrick Schramowski, Kristian Kersting","Text-conditioned image generation models have recently achieved astonishing
image quality and alignment results. Consequently, they are employed in a
fast-growing number of applications. Since they are highly data-driven, relying
on billion-sized datasets randomly scraped from the web, they also produce
unsafe content. As a contribution to the Adversarial Nibbler challenge, we
distill a large set of over 1,000 potential adversarial inputs from existing
safety benchmarks. Our analysis of the gathered prompts and corresponding
images demonstrates the fragility of input filters and provides further
insights into systematic safety issues in current generative image models.",2309.11575v1,https://arxiv.org/pdf/2309.11575v1
"Multi-Step Model Predictive Safety Filters: Reducing Chattering by
  Increasing the Prediction Horizon","Federico Pizarro Bejarano, Lukas Brunke, Angela P. Schoellig","Learning-based controllers have demonstrated superior performance compared to
classical controllers in various tasks. However, providing safety guarantees is
not trivial. Safety, the satisfaction of state and input constraints, can be
guaranteed by augmenting the learned control policy with a safety filter. Model
predictive safety filters (MPSFs) are a common safety filtering approach based
on model predictive control (MPC). MPSFs seek to guarantee safety while
minimizing the difference between the proposed and applied inputs in the
immediate next time step. This limited foresight can lead to jerky motions and
undesired oscillations close to constraint boundaries, known as chattering. In
this paper, we reduce chattering by considering input corrections over a longer
horizon. Under the assumption of bounded model uncertainties, we prove
recursive feasibility using techniques from robust MPC. We verified the
proposed approach in both extensive simulation and quadrotor experiments. In
experiments with a Crazyflie 2.0 drone, we show that, in addition to preserving
the desired safety guarantees, the proposed MPSF reduces chattering by more
than a factor of 4 compared to previous MPSF formulations.",2309.11453v1,https://arxiv.org/pdf/2309.11453v1
Learning Adaptive Safety for Multi-Agent Systems,"Luigi Berducci, Shuo Yang, Rahul Mangharam, Radu Grosu","Ensuring safety in dynamic multi-agent systems is challenging due to limited
information about the other agents. Control Barrier Functions (CBFs) are
showing promise for safety assurance but current methods make strong
assumptions about other agents and often rely on manual tuning to balance
safety, feasibility, and performance. In this work, we delve into the problem
of adaptive safe learning for multi-agent systems with CBF. We show how
emergent behavior can be profoundly influenced by the CBF configuration,
highlighting the necessity for a responsive and dynamic approach to CBF design.
We present ASRL, a novel adaptive safe RL framework, to fully automate the
optimization of policy and CBF coefficients, to enhance safety and long-term
performance through reinforcement learning. By directly interacting with the
other agents, ASRL learns to cope with diverse agent behaviours and maintains
the cost violations below a desired limit. We evaluate ASRL in a multi-robot
system and a competitive multi-agent racing scenario, against learning-based
and control-theoretic approaches. We empirically demonstrate the efficacy and
flexibility of ASRL, and assess generalization and scalability to
out-of-distribution scenarios. Code and supplementary material are public
online.",2309.10657v2,https://arxiv.org/pdf/2309.10657v2
Safe POMDP Online Planning via Shielding,"Shili Sheng, David Parker, Lu Feng","Partially observable Markov decision processes (POMDPs) have been widely used
in many robotic applications for sequential decision-making under uncertainty.
POMDP online planning algorithms such as Partially Observable Monte-Carlo
Planning (POMCP) can solve very large POMDPs with the goal of maximizing the
expected return. But the resulting policies cannot provide safety guarantees
which are imperative for real-world safety-critical tasks (e.g., autonomous
driving). In this work, we consider safety requirements represented as
almost-sure reach-avoid specifications (i.e., the probability to reach a set of
goal states is one and the probability to reach a set of unsafe states is
zero). We compute shields that restrict unsafe actions which would violate the
almost-sure reach-avoid specifications. We then integrate these shields into
the POMCP algorithm for safe POMDP online planning. We propose four distinct
shielding methods, differing in how the shields are computed and integrated,
including factored variants designed to improve scalability. Experimental
results on a set of benchmark domains demonstrate that the proposed shielding
methods successfully guarantee safety (unlike the baseline POMCP without
shielding) on large POMDPs, with negligible impact on the runtime for online
planning.",2309.10216v2,https://arxiv.org/pdf/2309.10216v2
"Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot
  Agents","Ziyi Yang, Shreyas S. Raman, Ankit Shah, Stefanie Tellex","Recent advancements in large language models (LLMs) have enabled a new
research domain, LLM agents, for solving robotics and planning tasks by
leveraging the world knowledge and general reasoning abilities of LLMs obtained
during pretraining. However, while considerable effort has been made to teach
the robot the ""dos,"" the ""don'ts"" received relatively less attention. We argue
that, for any practical usage, it is as crucial to teach the robot the
""don'ts"": conveying explicit instructions about prohibited actions, assessing
the robot's comprehension of these restrictions, and, most importantly,
ensuring compliance. Moreover, verifiable safe operation is essential for
deployments that satisfy worldwide standards such as ISO 61508, which defines
standards for safely deploying robots in industrial factory environments
worldwide. Aiming at deploying the LLM agents in a collaborative environment,
we propose a queryable safety constraint module based on linear temporal logic
(LTL) that simultaneously enables natural language (NL) to temporal constraints
encoding, safety violation reasoning and explaining, and unsafe action pruning.
To demonstrate the effectiveness of our system, we conducted experiments in
VirtualHome environment and on a real robot. The experimental results show that
our system strictly adheres to the safety constraints and scales well with
complex safety constraints, highlighting its potential for practical utility.",2309.09919v3,https://arxiv.org/pdf/2309.09919v3
"Guided Online Distillation: Promoting Safe Reinforcement Learning by
  Offline Demonstration","Jinning Li, Xinyi Liu, Banghua Zhu, Jiantao Jiao, Masayoshi Tomizuka, Chen Tang, Wei Zhan","Safe Reinforcement Learning (RL) aims to find a policy that achieves high
rewards while satisfying cost constraints. When learning from scratch, safe RL
agents tend to be overly conservative, which impedes exploration and restrains
the overall performance. In many realistic tasks, e.g. autonomous driving,
large-scale expert demonstration data are available. We argue that extracting
expert policy from offline data to guide online exploration is a promising
solution to mitigate the conserveness issue. Large-capacity models, e.g.
decision transformers (DT), have been proven to be competent in offline policy
learning. However, data collected in real-world scenarios rarely contain
dangerous cases (e.g., collisions), which makes it prohibitive for the policies
to learn safety concepts. Besides, these bulk policy networks cannot meet the
computation speed requirements at inference time on real-world tasks such as
autonomous driving. To this end, we propose Guided Online Distillation (GOLD),
an offline-to-online safe RL framework. GOLD distills an offline DT policy into
a lightweight policy network through guided online safe RL training, which
outperforms both the offline DT policy and online safe RL algorithms.
Experiments in both benchmark safe RL tasks and real-world driving tasks based
on the Waymo Open Motion Dataset (WOMD) demonstrate that GOLD can successfully
distill lightweight policies and solve decision-making problems in challenging
safety-critical scenarios.",2309.09408v2,https://arxiv.org/pdf/2309.09408v2
"Detection and Localization of Firearm Carriers in Complex Scenes for
  Improved Safety Measures","Arif Mahmood, Abdul Basit, M. Akhtar Munir, Mohsen Ali","Detecting firearms and accurately localizing individuals carrying them in
images or videos is of paramount importance in security, surveillance, and
content customization. However, this task presents significant challenges in
complex environments due to clutter and the diverse shapes of firearms. To
address this problem, we propose a novel approach that leverages human-firearm
interaction information, which provides valuable clues for localizing firearm
carriers. Our approach incorporates an attention mechanism that effectively
distinguishes humans and firearms from the background by focusing on relevant
areas. Additionally, we introduce a saliency-driven locality-preserving
constraint to learn essential features while preserving foreground information
in the input image. By combining these components, our approach achieves
exceptional results on a newly proposed dataset. To handle inputs of varying
sizes, we pass paired human-firearm instances with attention masks as channels
through a deep network for feature computation, utilizing an adaptive average
pooling layer. We extensively evaluate our approach against existing methods in
human-object interaction detection and achieve significant results (AP=77.8\%)
compared to the baseline approach (AP=63.1\%). This demonstrates the
effectiveness of leveraging attention mechanisms and saliency-driven locality
preservation for accurate human-firearm interaction detection. Our findings
contribute to advancing the fields of security and surveillance, enabling more
efficient firearm localization and identification in diverse scenarios.",2309.09236v1,https://arxiv.org/pdf/2309.09236v1
Price of Safety in Linear Best Arm Identification,"Xuedong Shang, Igor Colin, Merwan Barlier, Hamza Cherkaoui","We introduce the safe best-arm identification framework with linear feedback,
where the agent is subject to some stage-wise safety constraint that linearly
depends on an unknown parameter vector. The agent must take actions in a
conservative way so as to ensure that the safety constraint is not violated
with high probability at each round. Ways of leveraging the linear structure
for ensuring safety has been studied for regret minimization, but not for
best-arm identification to the best our knowledge. We propose a gap-based
algorithm that achieves meaningful sample complexity while ensuring the
stage-wise safety. We show that we pay an extra term in the sample complexity
due to the forced exploration phase incurred by the additional safety
constraint. Experimental illustrations are provided to justify the design of
our algorithm.",2309.08709v1,https://arxiv.org/pdf/2309.08709v1
"Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A
  Hybrid Transfer Learning Approach","Ahmad M. Nagib, Hatem Abou-Zeid, Hossam S. Hassanein","The open radio access network (O-RAN) architecture supports intelligent
network control algorithms as one of its core capabilities. Data-driven
applications incorporate such algorithms to optimize radio access network (RAN)
functions via RAN intelligent controllers (RICs). Deep reinforcement learning
(DRL) algorithms are among the main approaches adopted in the O-RAN literature
to solve dynamic radio resource management problems. However, despite the
benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms
in real network deployments falls behind. This is primarily due to the slow
convergence and unstable performance exhibited by DRL agents upon deployment
and when encountering previously unseen network conditions. In this paper, we
address these challenges by proposing transfer learning (TL) as a core
component of the training and deployment workflows for the DRL-based
closed-loop control of O-RAN functionalities. To this end, we propose and
design a hybrid TL-aided approach that leverages the advantages of both policy
reuse and distillation TL methods to provide safe and accelerated convergence
in DRL-based O-RAN slicing. We conduct a thorough experiment that accommodates
multiple services, including real VR gaming traffic to reflect practical
scenarios of O-RAN slicing. We also propose and implement policy reuse and
distillation-aided DRL and non-TL-aided DRL as three separate baselines. The
proposed hybrid approach shows at least: 7.7% and 20.7% improvements in the
average initial reward value and the percentage of converged scenarios, and a
64.6% decrease in reward variance while maintaining fast convergence and
enhancing the generalizability compared with the baselines.",2309.07265v2,https://arxiv.org/pdf/2309.07265v2
Safe Reinforcement Learning with Dual Robustness,"Zeyang Li, Chuxiong Hu, Yunan Wang, Yujie Yang, Shengbo Eben Li","Reinforcement learning (RL) agents are vulnerable to adversarial
disturbances, which can deteriorate task performance or compromise safety
specifications. Existing methods either address safety requirements under the
assumption of no adversary (e.g., safe RL) or only focus on robustness against
performance adversaries (e.g., robust RL). Learning one policy that is both
safe and robust remains a challenging open problem. The difficulty is how to
tackle two intertwined aspects in the worst cases: feasibility and optimality.
Optimality is only valid inside a feasible region, while identification of
maximal feasible region must rely on learning the optimal policy. To address
this issue, we propose a systematic framework to unify safe RL and robust RL,
including problem formulation, iteration scheme, convergence analysis and
practical algorithm design. This unification is built upon constrained
two-player zero-sum Markov games. A dual policy iteration scheme is proposed,
which simultaneously optimizes a task policy and a safety policy. The
convergence of this iteration scheme is proved. Furthermore, we design a deep
RL algorithm for practical implementation, called dually robust actor-critic
(DRAC). The evaluations with safety-critical benchmarks demonstrate that DRAC
achieves high performance and persistent safety under all scenarios (no
adversary, safety adversary, performance adversary), outperforming all
baselines significantly.",2309.06835v1,https://arxiv.org/pdf/2309.06835v1
"The Safety Filter: A Unified View of Safety-Critical Control in
  Autonomous Systems","Kai-Chieh Hsu, Haimin Hu, Jaime Fernández Fisac","Recent years have seen significant progress in the realm of robot autonomy,
accompanied by the expanding reach of robotic technologies. However, the
emergence of new deployment domains brings unprecedented challenges in ensuring
safe operation of these systems, which remains as crucial as ever. While
traditional model-based safe control methods struggle with generalizability and
scalability, emerging data-driven approaches tend to lack well-understood
guarantees, which can result in unpredictable catastrophic failures. Successful
deployment of the next generation of autonomous robots will require integrating
the strengths of both paradigms. This article provides a review of safety
filter approaches, highlighting important connections between existing
techniques and proposing a unified technical framework to understand, compare,
and combine them. The new unified view exposes a shared modular structure
across a range of seemingly disparate safety filter classes and naturally
suggests directions for future progress towards more scalable synthesis, robust
monitoring, and efficient intervention.",2309.05837v1,https://arxiv.org/pdf/2309.05837v1
"Studying Accuracy of Machine Learning Models Trained on Lab Lifting Data
  in Solving Real-World Problems Using Wearable Sensors for Workplace Safety","Joseph Bertrand, Nick Griffey, Ming-Lun Lu, Rashmi Jha","Porting ML models trained on lab data to real-world situations has long been
a challenge. This paper discusses porting a lab-trained lifting identification
model to the real-world. With performance much lower than on training data, we
explored causes of the failure and proposed four potential solutions to
increase model performance",2309.05831v1,https://arxiv.org/pdf/2309.05831v1
Probabilistic Safety Regions Via Finite Families of Scalable Classifiers,"Alberto Carlevaro, Teodoro Alamo, Fabrizio Dabbene, Maurizio Mongelli","Supervised classification recognizes patterns in the data to separate classes
of behaviours. Canonical solutions contain misclassification errors that are
intrinsic to the numerical approximating nature of machine learning. The data
analyst may minimize the classification error on a class at the expense of
increasing the error of the other classes. The error control of such a design
phase is often done in a heuristic manner. In this context, it is key to
develop theoretical foundations capable of providing probabilistic
certifications to the obtained classifiers. In this perspective, we introduce
the concept of probabilistic safety region to describe a subset of the input
space in which the number of misclassified instances is probabilistically
controlled. The notion of scalable classifiers is then exploited to link the
tuning of machine learning with error control. Several tests corroborate the
approach. They are provided through synthetic data in order to highlight all
the steps involved, as well as through a smart mobility application.",2309.04627v1,https://arxiv.org/pdf/2309.04627v1
Deep Learning Safety Concerns in Automated Driving Perception,"Stephanie Abrecht, Alexander Hirsch, Shervin Raafatnia, Matthias Woehrle","Recent advances in the field of deep learning and impressive performance of
deep neural networks (DNNs) for perception have resulted in an increased demand
for their use in automated driving (AD) systems. The safety of such systems is
of utmost importance and thus requires to consider the unique properties of
DNNs.
  In order to achieve safety of AD systems with DNN-based perception components
in a systematic and comprehensive approach, so-called safety concerns have been
introduced as a suitable structuring element. On the one hand, the concept of
safety concerns is -- by design -- well aligned to existing standards relevant
for safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has
already inspired several academic publications and upcoming standards on AI
safety such as ISO PAS 8800.
  While the concept of safety concerns has been previously introduced, this
paper extends and refines it, leveraging feedback from various domain and
safety experts in the field. In particular, this paper introduces an additional
categorization for a better understanding as well as enabling cross-functional
teams to jointly address the concerns.",2309.03774v3,https://arxiv.org/pdf/2309.03774v3
A computationally lightweight safe learning algorithm,"Dominik Baumann, Krzysztof Kowalczyk, Koen Tiels, Paweł Wachel","Safety is an essential asset when learning control policies for physical
systems, as violating safety constraints during training can lead to expensive
hardware damage. In response to this need, the field of safe learning has
emerged with algorithms that can provide probabilistic safety guarantees
without knowledge of the underlying system dynamics. Those algorithms often
rely on Gaussian process inference. Unfortunately, Gaussian process inference
scales cubically with the number of data points, limiting applicability to
high-dimensional and embedded systems. In this paper, we propose a safe
learning algorithm that provides probabilistic safety guarantees but leverages
the Nadaraya-Watson estimator instead of Gaussian processes. For the
Nadaraya-Watson estimator, we can reach logarithmic scaling with the number of
data points. We provide theoretical guarantees for the estimates, embed them
into a safe learning algorithm, and show numerical experiments on a simulated
seven-degrees-of-freedom robot manipulator.",2309.03672v1,https://arxiv.org/pdf/2309.03672v1
"Your Battery Is a Blast! Safeguarding Against Counterfeit Batteries with
  Authentication","Francesco Marchiori, Mauro Conti","Lithium-ion (Li-ion) batteries are the primary power source in various
applications due to their high energy and power density. Their market was
estimated to be up to 48 billion U.S. dollars in 2022. However, the widespread
adoption of Li-ion batteries has resulted in counterfeit cell production, which
can pose safety hazards to users. Counterfeit cells can cause explosions or
fires, and their prevalence in the market makes it difficult for users to
detect fake cells. Indeed, current battery authentication methods can be
susceptible to advanced counterfeiting techniques and are often not adaptable
to various cells and systems. In this paper, we improve the state of the art on
battery authentication by proposing two novel methodologies, DCAuth and
EISthentication, which leverage the internal characteristics of each cell
through Machine Learning models. Our methods automatically authenticate
lithium-ion battery models and architectures using data from their regular
usage without the need for any external device. They are also resilient to the
most common and critical counterfeit practices and can scale to several
batteries and devices. To evaluate the effectiveness of our proposed
methodologies, we analyze time-series data from a total of 20 datasets that we
have processed to extract meaningful features for our analysis. Our methods
achieve high accuracy in battery authentication for both architectures (up to
0.99) and models (up to 0.96). Moreover, our methods offer comparable
identification performances. By using our proposed methodologies, manufacturers
can ensure that devices only use legitimate batteries, guaranteeing the
operational state of any system and safety measures for the users.",2309.03607v1,https://arxiv.org/pdf/2309.03607v1
"Safe Neural Control for Non-Affine Control Systems with Differentiable
  Control Barrier Functions","Wei Xiao, Ross Allen, Daniela Rus","This paper addresses the problem of safety-critical control for non-affine
control systems. It has been shown that optimizing quadratic costs subject to
state and control constraints can be sub-optimally reduced to a sequence of
quadratic programs (QPs) by using Control Barrier Functions (CBFs). Our
recently proposed High Order CBFs (HOCBFs) can accommodate constraints of
arbitrary relative degree. The main challenges in this approach are that it
requires affine control dynamics and the solution of the CBF-based QP is
sub-optimal since it is solved point-wise. To address these challenges, we
incorporate higher-order CBFs into neural ordinary differential equation-based
learning models as differentiable CBFs to guarantee safety for non-affine
control systems. The differentiable CBFs are trainable in terms of their
parameters, and thus, they can address the conservativeness of CBFs such that
the system state will not stay unnecessarily far away from safe set boundaries.
Moreover, the imitation learning model is capable of learning complex and
optimal control policies that are usually intractable online. We illustrate the
effectiveness of the proposed framework on LiDAR-based autonomous driving and
compare it with existing methods.",2309.04492v1,https://arxiv.org/pdf/2309.04492v1
Certifying LLM Safety against Adversarial Prompting,"Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil Feizi, Himabindu Lakkaraju","Large language models (LLMs) are vulnerable to adversarial attacks that add
malicious tokens to an input prompt to bypass the safety guardrails of an LLM
and cause it to produce harmful content. In this work, we introduce
erase-and-check, the first framework for defending against adversarial prompts
with certifiable safety guarantees. Given a prompt, our procedure erases tokens
individually and inspects the resulting subsequences using a safety filter. Our
safety certificate guarantees that harmful prompts are not mislabeled as safe
due to an adversarial attack up to a certain size. We implement the safety
filter in two ways, using Llama 2 and DistilBERT, and compare the performance
of erase-and-check for the two cases. We defend against three attack modes: i)
adversarial suffix, where an adversarial sequence is appended at the end of a
harmful prompt; ii) adversarial insertion, where the adversarial sequence is
inserted anywhere in the middle of the prompt; and iii) adversarial infusion,
where adversarial tokens are inserted at arbitrary positions in the prompt, not
necessarily as a contiguous block. Our experimental results demonstrate that
this procedure can obtain strong certified safety guarantees on harmful prompts
while maintaining good empirical performance on safe prompts. Additionally, we
propose three efficient empirical defenses: i) RandEC, a randomized subsampling
version of erase-and-check; ii) GreedyEC, which greedily erases tokens that
maximize the softmax score of the harmful class; and iii) GradEC, which uses
gradient information to optimize tokens to erase. We demonstrate their
effectiveness against adversarial prompts generated by the Greedy Coordinate
Gradient (GCG) attack algorithm. The code for our experiments is available at
https://github.com/aounon/certified-llm-safety.",2309.02705v3,https://arxiv.org/pdf/2309.02705v3
"Neurosymbolic Meta-Reinforcement Lookahead Learning Achieves Safe
  Self-Driving in Non-Stationary Environments","Haozhe Lei, Quanyan Zhu","In the area of learning-driven artificial intelligence advancement, the
integration of machine learning (ML) into self-driving (SD) technology stands
as an impressive engineering feat. Yet, in real-world applications outside the
confines of controlled laboratory scenarios, the deployment of self-driving
technology assumes a life-critical role, necessitating heightened attention
from researchers towards both safety and efficiency. To illustrate, when a
self-driving model encounters an unfamiliar environment in real-time execution,
the focus must not solely revolve around enhancing its anticipated performance;
equal consideration must be given to ensuring its execution or real-time
adaptation maintains a requisite level of safety. This study introduces an
algorithm for online meta-reinforcement learning, employing lookahead symbolic
constraints based on \emph{Neurosymbolic Meta-Reinforcement Lookahead Learning}
(NUMERLA). NUMERLA proposes a lookahead updating mechanism that harmonizes the
efficiency of online adaptations with the overarching goal of ensuring
long-term safety. Experimental results demonstrate NUMERLA confers the
self-driving agent with the capacity for real-time adaptability, leading to
safe and self-adaptive driving under non-stationary urban human-vehicle
interaction scenarios.",2309.02328v1,https://arxiv.org/pdf/2309.02328v1
"Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety
  Through Innovative Contraband Detection","Syed Atif Ali Shah, Nasir Algeelani, Najeeb Al-Sammarraie","Surveillance systems have emerged as crucial elements in upholding peace and
security in the modern world. Their ubiquity aids in monitoring suspicious
activities effectively. However, in densely populated environments, continuous
active monitoring becomes impractical, necessitating the development of
intelligent surveillance systems. AI integration in the surveillance domain was
a big revolution, however, speed issues have prevented its widespread
implementation in the field. It has been observed that quantum artificial
intelligence has led to a great breakthrough. Quantum artificial
intelligence-based surveillance systems have shown to be more accurate as well
as capable of performing well in real-time scenarios, which had never been seen
before. In this research, a RentinaNet model is integrated with Quantum CNN and
termed as Quantum-RetinaNet. By harnessing the Quantum capabilities of QCNN,
Quantum-RetinaNet strikes a balance between accuracy and speed. This innovative
integration positions it as a game-changer, addressing the challenges of active
monitoring in densely populated scenarios. As demand for efficient surveillance
solutions continues to grow, Quantum-RetinaNet offers a compelling alternative
to existing CNN models, upholding accuracy standards without sacrificing
real-time performance. The unique attributes of Quantum-RetinaNet have
far-reaching implications for the future of intelligent surveillance. With its
enhanced processing speed, it is poised to revolutionize the field, catering to
the pressing need for rapid yet precise monitoring. As Quantum-RetinaNet
becomes the new standard, it ensures public safety and security while pushing
the boundaries of AI in surveillance.",2309.03231v1,https://arxiv.org/pdf/2309.03231v1
Provably safe systems: the only path to controllable AGI,"Max Tegmark, Steve Omohundro","We describe a path to humanity safely thriving with powerful Artificial
General Intelligences (AGIs) by building them to provably satisfy
human-specified requirements. We argue that this will soon be technically
feasible using advanced AI for formal verification and mechanistic
interpretability. We further argue that it is the only path which guarantees
safe controlled AGI. We end with a list of challenge problems whose solution
would contribute to this positive outcome and invite readers to join in this
work.",2309.01933v1,https://arxiv.org/pdf/2309.01933v1
"Deception Game: Closing the Safety-Learning Loop in Interactive Robot
  Autonomy","Haimin Hu, Zixu Zhang, Kensuke Nakamura, Andrea Bajcsy, Jaime F. Fisac","An outstanding challenge for the widespread deployment of robotic systems
like autonomous vehicles is ensuring safe interaction with humans without
sacrificing performance. Existing safety methods often neglect the robot's
ability to learn and adapt at runtime, leading to overly conservative behavior.
This paper proposes a new closed-loop paradigm for synthesizing safe control
policies that explicitly account for the robot's evolving uncertainty and its
ability to quickly respond to future scenarios as they arise, by jointly
considering the physical dynamics and the robot's learning algorithm. We
leverage adversarial reinforcement learning for tractable safety analysis under
high-dimensional learning dynamics and demonstrate our framework's ability to
work with both Bayesian belief propagation and implicit learning through large
pre-trained neural trajectory predictors.",2309.01267v2,https://arxiv.org/pdf/2309.01267v2
Learning Shared Safety Constraints from Multi-task Demonstrations,"Konwoo Kim, Gokul Swamy, Zuxin Liu, Ding Zhao, Sanjiban Choudhury, Zhiwei Steven Wu","Regardless of the particular task we want them to perform in an environment,
there are often shared safety constraints we want our agents to respect. For
example, regardless of whether it is making a sandwich or clearing the table, a
kitchen robot should not break a plate. Manually specifying such a constraint
can be both time-consuming and error-prone. We show how to learn constraints
from expert demonstrations of safe task completion by extending inverse
reinforcement learning (IRL) techniques to the space of constraints.
Intuitively, we learn constraints that forbid highly rewarding behavior that
the expert could have taken but chose not to. Unfortunately, the constraint
learning problem is rather ill-posed and typically leads to overly conservative
constraints that forbid all behavior that the expert did not take. We counter
this by leveraging diverse demonstrations that naturally occur in multi-task
settings to learn a tighter set of constraints. We validate our method with
simulation experiments on high-dimensional continuous control tasks.",2309.00711v1,https://arxiv.org/pdf/2309.00711v1
"Fault Injection and Safe-Error Attack for Extraction of Embedded Neural
  Network Models","Kevin Hector, Pierre-Alain Moellic, Mathieu Dumont, Jean-Max Dutertre","Model extraction emerges as a critical security threat with attack vectors
exploiting both algorithmic and implementation-based approaches. The main goal
of an attacker is to steal as much information as possible about a protected
victim model, so that he can mimic it with a substitute model, even with a
limited access to similar training data. Recently, physical attacks such as
fault injection have shown worrying efficiency against the integrity and
confidentiality of embedded models. We focus on embedded deep neural network
models on 32-bit microcontrollers, a widespread family of hardware platforms in
IoT, and the use of a standard fault injection strategy - Safe Error Attack
(SEA) - to perform a model extraction attack with an adversary having a limited
access to training data. Since the attack strongly depends on the input
queries, we propose a black-box approach to craft a successful attack set. For
a classical convolutional neural network, we successfully recover at least 90%
of the most significant bits with about 1500 crafted inputs. These information
enable to efficiently train a substitute model, with only 8% of the training
dataset, that reaches high fidelity and near identical accuracy level than the
victim model.",2308.16703v1,https://arxiv.org/pdf/2308.16703v1
"Explainable and Trustworthy Traffic Sign Detection for Safe Autonomous
  Driving: An Inductive Logic Programming Approach","Zahra Chaghazardi, Saber Fallah, Alireza Tamaddoni-Nezhad","Traffic sign detection is a critical task in the operation of Autonomous
Vehicles (AV), as it ensures the safety of all road users. Current DNN-based
sign classification systems rely on pixel-level features to detect traffic
signs and can be susceptible to adversarial attacks. These attacks involve
small, imperceptible changes to a sign that can cause traditional classifiers
to misidentify the sign. We propose an Inductive Logic Programming (ILP) based
approach for stop sign detection in AVs to address this issue. This method
utilises high-level features of a sign, such as its shape, colour, and text, to
detect categories of traffic signs. This approach is more robust against
adversarial attacks, as it mimics human-like perception and is less susceptible
to the limitations of current DNN classifiers. We consider two adversarial
attacking methods to evaluate our approach: Robust Physical Perturbation (PR2)
and Adversarial Camouflage (AdvCam). These attacks are able to deceive DNN
classifiers, causing them to misidentify stop signs as other signs with high
confidence. The results show that the proposed ILP-based technique is able to
correctly identify all targeted stop signs, even in the presence of PR2 and
ADvCam attacks. The proposed learning method is also efficient as it requires
minimal training data. Moreover, it is fully explainable, making it possible to
debug AVs.",2309.03215v1,https://arxiv.org/pdf/2309.03215v1
"LTLf Synthesis Under Environment Specifications for Reachability and
  Safety Properties","Benjamin Aminof, Giuseppe De Giacomo, Antonio Di Stasio, Hugo Francon, Sasha Rubin, Shufang Zhu","In this paper, we study LTLf synthesis under environment specifications for
arbitrary reachability and safety properties. We consider both kinds of
properties for both agent tasks and environment specifications, providing a
complete landscape of synthesis algorithms. For each case, we devise a specific
algorithm (optimal wrt complexity of the problem) and prove its correctness.
The algorithms combine common building blocks in different ways. While some
cases are already studied in literature others are studied here for the first
time.",2308.15184v1,https://arxiv.org/pdf/2308.15184v1
Directional Optimism for Safe Linear Bandits,"Spencer Hutchinson, Berkay Turan, Mahnoosh Alizadeh","The safe linear bandit problem is a version of the classical stochastic
linear bandit problem where the learner's actions must satisfy an uncertain
constraint at all rounds. Due its applicability to many real-world settings,
this problem has received considerable attention in recent years. By leveraging
a novel approach that we call directional optimism, we find that it is possible
to achieve improved regret guarantees for both well-separated problem instances
and action sets that are finite star convex sets. Furthermore, we propose a
novel algorithm for this setting that improves on existing algorithms in terms
of empirical performance, while enjoying matching regret guarantees. Lastly, we
introduce a generalization of the safe linear bandit setting where the
constraints are convex and adapt our algorithms and analyses to this setting by
leveraging a novel convex-analysis based approach.",2308.15006v2,https://arxiv.org/pdf/2308.15006v2
"Towards Safe Autonomy in Hybrid Traffic: Detecting Unpredictable
  Abnormal Behaviors of Human Drivers via Information Sharing","Jiangwei Wang, Lili Su, Songyang Han, Dongjin Song, Fei Miao","Hybrid traffic which involves both autonomous and human-driven vehicles would
be the norm of the autonomous vehicles practice for a while. On the one hand,
unlike autonomous vehicles, human-driven vehicles could exhibit sudden abnormal
behaviors such as unpredictably switching to dangerous driving modes, putting
its neighboring vehicles under risks; such undesired mode switching could arise
from numbers of human driver factors, including fatigue, drunkenness,
distraction, aggressiveness, etc. On the other hand, modern vehicle-to-vehicle
communication technologies enable the autonomous vehicles to efficiently and
reliably share the scarce run-time information with each other. In this paper,
we propose, to the best of our knowledge, the first efficient algorithm that
can (1) significantly improve trajectory prediction by effectively fusing the
run-time information shared by surrounding autonomous vehicles, and can (2)
accurately and quickly detect abnormal human driving mode switches or abnormal
driving behavior with formal assurance without hurting human drivers privacy.
To validate our proposed algorithm, we first evaluate our proposed trajectory
predictor on NGSIM and Argoverse datasets and show that our proposed predictor
outperforms the baseline methods. Then through extensive experiments on SUMO
simulator, we show that our proposed algorithm has great detection performance
in both highway and urban traffic. The best performance achieves detection rate
of 97.3%, average detection delay of 1.2s, and 0 false alarm.",2309.16716v1,https://arxiv.org/pdf/2309.16716v1
SafeAR: Safe Algorithmic Recourse by Risk-Aware Policies,"Haochen Wu, Shubham Sharma, Sunandita Patra, Sriram Gopalakrishnan","With the growing use of machine learning (ML) models in critical domains such
as finance and healthcare, the need to offer recourse for those adversely
affected by the decisions of ML models has become more important; individuals
ought to be provided with recommendations on actions to take for improving
their situation and thus receiving a favorable decision. Prior work on
sequential algorithmic recourse -- which recommends a series of changes --
focuses on action feasibility and uses the proximity of feature changes to
determine action costs. However, the uncertainties of feature changes and the
risk of higher than average costs in recourse have not been considered. It is
undesirable if a recourse could (with some probability) result in a worse
situation from which recovery requires an extremely high cost. It is essential
to incorporate risks when computing and evaluating recourse. We call the
recourse computed with such risk considerations as Safe Algorithmic Recourse
(SafeAR). The objective is to empower people to choose a recourse based on
their risk tolerance. In this work, we discuss and show how existing recourse
desiderata can fail to capture the risk of higher costs. We present a method to
compute recourse policies that consider variability in cost and connect
algorithmic recourse literature with risk-sensitive reinforcement learning. We
also adopt measures ""Value at Risk"" and ""Conditional Value at Risk"" from the
financial literature to summarize risk concisely. We apply our method to two
real-world datasets and compare policies with different risk-aversion levels
using risk measures and recourse desiderata (sparsity and proximity).",2308.12367v3,https://arxiv.org/pdf/2308.12367v3
"How Safe Am I Given What I See? Calibrated Prediction of Safety Chances
  for Image-Controlled Autonomy","Zhenjiang Mao, Carson Sobolewski, Ivan Ruchkin","End-to-end learning has emerged as a major paradigm for developing autonomous
systems. Unfortunately, with its performance and convenience comes an even
greater challenge of safety assurance. A key factor of this challenge is the
absence of the notion of a low-dimensional and interpretable dynamical state,
around which traditional assurance methods revolve. Focusing on the online
safety prediction problem, this paper proposes a configurable family of
learning pipelines based on generative world models, which do not require
low-dimensional states. To implement these pipelines, we overcome the
challenges of learning safety-informed latent representations and missing
safety labels under prediction-induced distribution shift. These pipelines come
with statistical calibration guarantees on their safety chance predictions
based on conformal prediction. We perform an extensive evaluation of the
proposed learning pipelines on two case studies of image-controlled systems: a
racing car and a cartpole.",2308.12252v4,https://arxiv.org/pdf/2308.12252v4
"The Challenges of Machine Learning for Trust and Safety: A Case Study on
  Misinformation Detection","Madelyne Xiao, Jonathan Mayer","We examine the disconnect between scholarship and practice in applying
machine learning to trust and safety problems, using misinformation detection
as a case study. We survey literature on automated detection of misinformation
across a corpus of 248 well-cited papers in the field. We then examine subsets
of papers for data and code availability, design missteps, reproducibility, and
generalizability. Our paper corpus includes published work in security, natural
language processing, and computational social science. Across these disparate
disciplines, we identify common errors in dataset and method design. In
general, detection tasks are often meaningfully distinct from the challenges
that online services actually face. Datasets and model evaluation are often
non-representative of real-world contexts, and evaluation frequently is not
independent of model training. We demonstrate the limitations of current
detection methods in a series of three representative replication studies.
Based on the results of these analyses and our literature survey, we conclude
that the current state-of-the-art in fully-automated misinformation detection
has limited efficacy in detecting human-generated misinformation. We offer
recommendations for evaluating applications of machine learning to trust and
safety problems and recommend future directions for research.",2308.12215v3,https://arxiv.org/pdf/2308.12215v3
"Ensembling Uncertainty Measures to Improve Safety of Black-Box
  Classifiers","Tommaso Zoppi, Andrea Ceccarelli, Andrea Bondavalli","Machine Learning (ML) algorithms that perform classification may predict the
wrong class, experiencing misclassifications. It is well-known that
misclassifications may have cascading effects on the encompassing system,
possibly resulting in critical failures. This paper proposes SPROUT, a Safety
wraPper thROugh ensembles of UncertainTy measures, which suspects
misclassifications by computing uncertainty measures on the inputs and outputs
of a black-box classifier. If a misclassification is detected, SPROUT blocks
the propagation of the output of the classifier to the encompassing system. The
resulting impact on safety is that SPROUT transforms erratic outputs
(misclassifications) into data omission failures, which can be easily managed
at the system level. SPROUT has a broad range of applications as it fits binary
and multi-class classification, comprising image and tabular datasets. We
experimentally show that SPROUT always identifies a huge fraction of the
misclassifications of supervised classifiers, and it is able to detect all
misclassifications in specific cases. SPROUT implementation contains
pre-trained wrappers, it is publicly available and ready to be deployed with
minimal effort.",2308.12065v1,https://arxiv.org/pdf/2308.12065v1
"Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence
  (DOVESEI)","Haechan Mark Bong, Rongge Zhang, Ricardo de Azambuja, Giovanni Beltrame","This work targets what we consider to be the foundational step for urban
airborne robots, a safe landing. Our attention is directed toward what we deem
the most crucial aspect of the safe landing perception stack: segmentation. We
present a streamlined reactive UAV system that employs visual servoing by
harnessing the capabilities of open vocabulary image segmentation. This
approach can adapt to various scenarios with minimal adjustments, bypassing the
necessity for extensive data accumulation for refining internal models, thanks
to its open vocabulary methodology. Given the limitations imposed by local
authorities, our primary focus centers on operations originating from altitudes
of 100 meters. This choice is deliberate, as numerous preceding works have
dealt with altitudes up to 30 meters, aligning with the capabilities of small
stereo cameras. Consequently, we leave the remaining 20m to be navigated using
conventional 3D path planning methods. Utilizing monocular cameras and image
segmentation, our findings demonstrate the system's capability to successfully
execute landing maneuvers at altitudes as low as 20 meters. However, this
approach is vulnerable to intermittent and occasionally abrupt fluctuations in
the segmentation between frames in a video stream. To address this challenge,
we enhance the image segmentation output by introducing what we call a dynamic
focus: a masking mechanism that self adjusts according to the current landing
stage. This dynamic focus guides the control system to avoid regions beyond the
drone's safety radius projected onto the ground, thus mitigating the problems
with fluctuations. Through the implementation of this supplementary layer, our
experiments have reached improvements in the landing success rate of almost
tenfold when compared to global segmentation. All the source code is open
source and available online (github.com/MISTLab/DOVESEI).",2308.11471v5,https://arxiv.org/pdf/2308.11471v5
"A Safe Deep Reinforcement Learning Approach for Energy Efficient
  Federated Learning in Wireless Communication Networks","Nikolaos Koursioumpas, Lina Magoula, Nikolaos Petropouleas, Alexandros-Ioannis Thanopoulos, Theodora Panagea, Nancy Alonistioti, M. A. Gutierrez-Estevez, Ramin Khalili","Progressing towards a new era of Artificial Intelligence (AI) - enabled
wireless networks, concerns regarding the environmental impact of AI have been
raised both in industry and academia. Federated Learning (FL) has emerged as a
key privacy preserving decentralized AI technique. Despite efforts currently
being made in FL, its environmental impact is still an open problem. Targeting
the minimization of the overall energy consumption of an FL process, we propose
the orchestration of computational and communication resources of the involved
devices to minimize the total energy required, while guaranteeing a certain
performance of the model. To this end, we propose a Soft Actor Critic Deep
Reinforcement Learning (DRL) solution, where a penalty function is introduced
during training, penalizing the strategies that violate the constraints of the
environment, and contributing towards a safe RL process. A device level
synchronization method, along with a computationally cost effective FL
environment are proposed, with the goal of further reducing the energy
consumption and communication overhead. Evaluation results show the
effectiveness and robustness of the proposed scheme compared to four
state-of-the-art baseline solutions on different network environments and FL
architectures, achieving a decrease of up to 94% in the total energy
consumption.",2308.10664v3,https://arxiv.org/pdf/2308.10664v3
"Enumerating Safe Regions in Deep Neural Networks with Provable
  Probabilistic Guarantees","Luca Marzari, Davide Corsi, Enrico Marchesini, Alessandro Farinelli, Ferdinando Cicalese","Identifying safe areas is a key point to guarantee trust for systems that are
based on Deep Neural Networks (DNNs). To this end, we introduce the
AllDNN-Verification problem: given a safety property and a DNN, enumerate the
set of all the regions of the property input domain which are safe, i.e., where
the property does hold. Due to the #P-hardness of the problem, we propose an
efficient approximation method called epsilon-ProVe. Our approach exploits a
controllable underestimation of the output reachable sets obtained via
statistical prediction of tolerance limits, and can provide a tight (with
provable probabilistic guarantees) lower estimate of the safe areas. Our
empirical evaluation on different standard benchmarks shows the scalability and
effectiveness of our method, offering valuable insights for this new type of
verification of DNNs.",2308.09842v2,https://arxiv.org/pdf/2308.09842v2
"From Hope to Safety: Unlearning Biases of Deep Models via Gradient
  Penalization in Latent Space","Maximilian Dreyer, Frederik Pahde, Christopher J. Anders, Wojciech Samek, Sebastian Lapuschkin","Deep Neural Networks are prone to learning spurious correlations embedded in
the training data, leading to potentially biased predictions. This poses risks
when deploying these models for high-stake decision-making, such as in medical
applications. Current methods for post-hoc model correction either require
input-level annotations which are only possible for spatially localized biases,
or augment the latent feature space, thereby hoping to enforce the right
reasons. We present a novel method for model correction on the concept level
that explicitly reduces model sensitivity towards biases via gradient
penalization. When modeling biases via Concept Activation Vectors, we highlight
the importance of choosing robust directions, as traditional regression-based
approaches such as Support Vector Machines tend to result in diverging
directions. We effectively mitigate biases in controlled and real-world
settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet
and EfficientNet architectures. Code is available on
https://github.com/frederikpahde/rrclarc.",2308.09437v3,https://arxiv.org/pdf/2308.09437v3
Safety Filter Design for Neural Network Systems via Convex Optimization,"Shaoru Chen, Kong Yao Chee, Nikolai Matni, M. Ani Hsieh, George J. Pappas","With the increase in data availability, it has been widely demonstrated that
neural networks (NN) can capture complex system dynamics precisely in a
data-driven manner. However, the architectural complexity and nonlinearity of
the NNs make it challenging to synthesize a provably safe controller. In this
work, we propose a novel safety filter that relies on convex optimization to
ensure safety for a NN system, subject to additive disturbances that are
capable of capturing modeling errors. Our approach leverages tools from NN
verification to over-approximate NN dynamics with a set of linear bounds,
followed by an application of robust linear MPC to search for controllers that
can guarantee robust constraint satisfaction. We demonstrate the efficacy of
the proposed framework numerically on a nonlinear pendulum system.",2308.08086v2,https://arxiv.org/pdf/2308.08086v2
Safety in Traffic Management Systems: A Comprehensive Survey,"Wenlu Du, Ankan Dash, Jing Li, Hua Wei, Guiling Wang","Traffic management systems play a vital role in ensuring safe and efficient
transportation on roads. However, the use of advanced technologies in traffic
management systems has introduced new safety challenges. Therefore, it is
important to ensure the safety of these systems to prevent accidents and
minimize their impact on road users. In this survey, we provide a comprehensive
review of the literature on safety in traffic management systems. Specifically,
we discuss the different safety issues that arise in traffic management
systems, the current state of research on safety in these systems, and the
techniques and methods proposed to ensure the safety of these systems. We also
identify the limitations of the existing research and suggest future research
directions.",2308.06204v1,https://arxiv.org/pdf/2308.06204v1
"Safeguarding Learning-based Control for Smart Energy Systems with
  Sampling Specifications","Chih-Hong Cheng, Venkatesh Prasad Venkataramanan, Pragya Kirti Gupta, Yun-Fei Hsu, Simon Burton","We study challenges using reinforcement learning in controlling energy
systems, where apart from performance requirements, one has additional safety
requirements such as avoiding blackouts. We detail how these safety
requirements in real-time temporal logic can be strengthened via discretization
into linear temporal logic (LTL), such that the satisfaction of the LTL
formulae implies the satisfaction of the original safety requirements. The
discretization enables advanced engineering methods such as synthesizing
shields for safe reinforcement learning as well as formal verification, where
for statistical model checking, the probabilistic guarantee acquired by LTL
model checking forms a lower bound for the satisfaction of the original
real-time safety requirements.",2308.06069v1,https://arxiv.org/pdf/2308.06069v1
Aviation Safety Risk Analysis and Flight Technology Assessment Issues,Shuanghe Liu,"This text highlights the significance of flight safety in China's civil
aviation industry and emphasizes the need for comprehensive research. It
focuses on two main areas: analyzing exceedance events and statistically
evaluating non-exceedance data. The challenges of current approaches lie in
insufficient cause analysis for exceedances. The proposed solutions involve
data preprocessing, reliability assessment, quantifying flight control using
neural networks, exploratory data analysis, flight personnel skill evaluation
with machine learning, and establishing real-time automated warnings. These
endeavors aim to enhance flight safety, personnel assessment, and warning
mechanisms, contributing to a safer and more efficient civil aviation sector.",2309.12324v1,https://arxiv.org/pdf/2309.12324v1
Safe Multimodal Communication in Human-Robot Collaboration,"Davide Ferrari, Andrea Pupa, Alberto Signoretti, Cristian Secchi","The new industrial settings are characterized by the presence of human and
robots that work in close proximity, cooperating in performing the required
job. Such a collaboration, however, requires to pay attention to many aspects.
Firstly, it is crucial to enable a communication between this two actors that
is natural and efficient. Secondly, the robot behavior must always be compliant
with the safety regulations, ensuring always a safe collaboration. In this
paper, we propose a framework that enables multi-channel communication between
humans and robots by leveraging multimodal fusion of voice and gesture commands
while always respecting safety regulations. The framework is validated through
a comparative experiment, demonstrating that, thanks to multimodal
communication, the robot can extract valuable information for performing the
required task and additionally, with the safety layer, the robot can scale its
speed to ensure the operator's safety.",2308.03690v1,https://arxiv.org/pdf/2308.03690v1
PURL: Safe and Effective Sanitization of Link Decoration,"Shaoor Munir, Patrick Lee, Umar Iqbal, Zubair Shafiq, Sandra Siby","While privacy-focused browsers have taken steps to block third-party cookies
and mitigate browser fingerprinting, novel tracking techniques that can bypass
existing countermeasures continue to emerge. Since trackers need to share
information from the client-side to the server-side through link decoration
regardless of the tracking technique they employ, a promising orthogonal
approach is to detect and sanitize tracking information in decorated links. To
this end, we present PURL (pronounced purel-l), a machine-learning approach
that leverages a cross-layer graph representation of webpage execution to
safely and effectively sanitize link decoration. Our evaluation shows that PURL
significantly outperforms existing countermeasures in terms of accuracy and
reducing website breakage while being robust to common evasion techniques.
PURL's deployment on a sample of top-million websites shows that link
decoration is abused for tracking on nearly three-quarters of the websites,
often to share cookies, email addresses, and fingerprinting information.",2308.03417v2,https://arxiv.org/pdf/2308.03417v2
"Building Safe and Reliable AI systems for Safety Critical Tasks with
  Vision-Language Processing",Shuang Ao,"Although AI systems have been applied in various fields and achieved
impressive performance, their safety and reliability are still a big concern.
This is especially important for safety-critical tasks. One shared
characteristic of these critical tasks is their risk sensitivity, where small
mistakes can cause big consequences and even endanger life. There are several
factors that could be guidelines for the successful deployment of AI systems in
sensitive tasks: (i) failure detection and out-of-distribution (OOD) detection;
(ii) overfitting identification; (iii) uncertainty quantification for
predictions; (iv) robustness to data perturbations. These factors are also
challenges of current AI systems, which are major blocks for building safe and
reliable AI. Specifically, the current AI algorithms are unable to identify
common causes for failure detection. Furthermore, additional techniques are
required to quantify the quality of predictions. All these contribute to
inaccurate uncertainty quantification, which lowers trust in predictions. Hence
obtaining accurate model uncertainty quantification and its further improvement
are challenging. To address these issues, many techniques have been proposed,
such as regularization methods and learning strategies. As vision and language
are the most typical data type and have many open source benchmark datasets,
this thesis will focus on vision-language data processing for tasks like
classification, image captioning, and vision question answering. In this
thesis, we aim to build a safeguard by further developing current techniques to
ensure the accurate model uncertainty for safety-critical tasks.",2308.03176v1,https://arxiv.org/pdf/2308.03176v1
"SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning
  Agents","Amirhossein Zolfagharian, Manel Abdellatif, Lionel C. Briand, Ramesh S","Deep reinforcement learning algorithms (DRL) are increasingly being used in
safety-critical systems. Ensuring the safety of DRL agents is a critical
concern in such contexts. However, relying solely on testing is not sufficient
to ensure safety as it does not offer guarantees. Building safety monitors is
one solution to alleviate this challenge. This paper proposes SMARLA, a machine
learning-based safety monitoring approach designed for DRL agents. For
practical reasons, SMARLA is agnostic to the type of DRL agent's inputs.
Further, it is designed to be black-box (as it does not require access to the
internals or training data of the agent) by leveraging state abstraction to
facilitate the learning of safety violation prediction models from the agent's
states using a reduced state space. We quantitatively and qualitatively
validated SMARLA on three well-known RL case studies. Empirical results reveal
that SMARLA achieves accurate violation prediction with a low false positive
rate and can predict safety violations at an early stage, approximately halfway
through the execution of the agent, before violations occur.",2308.02594v3,https://arxiv.org/pdf/2308.02594v3
"Dual Governance: The intersection of centralized regulation and
  crowdsourced safety mechanisms for Generative AI","Avijit Ghosh, Dhanya Lakshmi","Generative Artificial Intelligence (AI) has seen mainstream adoption lately,
especially in the form of consumer-facing, open-ended, text and image
generating models. However, the use of such systems raises significant ethical
and safety concerns, including privacy violations, misinformation and
intellectual property theft. The potential for generative AI to displace human
creativity and livelihoods has also been under intense scrutiny. To mitigate
these risks, there is an urgent need of policies and regulations responsible
and ethical development in the field of generative AI. Existing and proposed
centralized regulations by governments to rein in AI face criticisms such as
not having sufficient clarity or uniformity, lack of interoperability across
lines of jurisdictions, restricting innovation, and hindering free market
competition. Decentralized protections via crowdsourced safety tools and
mechanisms are a potential alternative. However, they have clear deficiencies
in terms of lack of adequacy of oversight and difficulty of enforcement of
ethical and safety standards, and are thus not enough by themselves as a
regulation mechanism. We propose a marriage of these two strategies via a
framework we call Dual Governance. This framework proposes a cooperative
synergy between centralized government regulations in a U.S. specific context
and safety mechanisms developed by the community to protect stakeholders from
the harms of generative AI. By implementing the Dual Governance framework, we
posit that innovation and creativity can be promoted while ensuring safe and
ethical deployment of generative AI.",2308.04448v1,https://arxiv.org/pdf/2308.04448v1
"XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in
  Large Language Models","Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, Dirk Hovy","Without proper safeguards, large language models will readily follow
malicious instructions and generate toxic content. This risk motivates safety
efforts such as red-teaming and large-scale feedback learning, which aim to
make models both helpful and harmless. However, there is a tension between
these two objectives, since harmlessness requires models to refuse to comply
with unsafe prompts, and thus not be helpful. Recent anecdotal evidence
suggests that some models may have struck a poor balance, so that even clearly
safe prompts are refused if they use similar language to unsafe prompts or
mention sensitive topics. In this paper, we introduce a new test suite called
XSTest to identify such eXaggerated Safety behaviours in a systematic way.
XSTest comprises 250 safe prompts across ten prompt types that well-calibrated
models should not refuse to comply with, and 200 unsafe prompts as contrasts
that models, for most applications, should refuse. We describe XSTest's
creation and composition, and then use the test suite to highlight systematic
failure modes in state-of-the-art language models as well as more general
challenges in building safer language models.",2308.01263v3,https://arxiv.org/pdf/2308.01263v3
"A Counterfactual Safety Margin Perspective on the Scoring of Autonomous
  Vehicles' Riskiness","Alessandro Zanardi, Andrea Censi, Margherita Atzei, Luigi Di Lillo, Emilio Frazzoli","Autonomous Vehicles (AVs) promise a range of societal advantages, including
broader access to mobility, reduced road accidents, and enhanced transportation
efficiency. However, evaluating the risks linked to AVs is complex due to
limited historical data and the swift progression of technology. This paper
presents a data-driven framework for assessing the risk of different AVs'
behaviors in various operational design domains (ODDs), based on counterfactual
simulations of ""misbehaving"" road users. We propose the notion of
counterfactual safety margin, which represents the minimum deviation from
nominal behavior that could cause a collision. This methodology not only
pinpoints the most critical scenarios but also quantifies the (relative) risk's
frequency and severity concerning AVs. Importantly, we show that our approach
is applicable even when the AV's behavioral policy remains undisclosed, through
worst- and best-case analyses, benefiting external entities like regulators and
risk evaluators. Our experimental outcomes demonstrate the correlation between
the safety margin, the quality of the driving policy, and the ODD, shedding
light on the relative risks of different AV providers. Overall, this work
contributes to the safety assessment of AVs and addresses legislative and
insurance concerns surrounding this burgeoning technology.",2308.01050v4,https://arxiv.org/pdf/2308.01050v4
A Case for AI Safety via Law,Jeffrey W. Johnston,"How to make artificial intelligence (AI) systems safe and aligned with human
values is an open research question. Proposed solutions tend toward relying on
human intervention in uncertain situations, learning human values and
intentions through training or observation, providing off-switches,
implementing isolation or simulation environments, or extrapolating what people
would want if they had more knowledge and more time to think. Law-based
approaches--such as inspired by Isaac Asimov--have not been well regarded. This
paper makes a case that effective legal systems are the best way to address AI
safety. Law is defined as any rules that codify prohibitions and prescriptions
applicable to particular agents in specified domains/contexts and includes
processes for enacting, managing, enforcing, and litigating such rules.",2309.12321v2,https://arxiv.org/pdf/2309.12321v2
"Crowd Safety Manager: Towards Data-Driven Active Decision Support for
  Planning and Control of Crowd Events","Panchamy Krishnakumari, Sascha Hoogendoorn-Lanser, Jeroen Steenbakkers, Serge Hoogendoorn","This paper presents novel technology and methodology aimed at enhancing crowd
management in both the planning and operational phases. The approach
encompasses innovative data collection techniques, data integration, and
visualization using a 3D Digital Twin, along with the incorporation of
artificial intelligence (AI) tools for risk identification. The paper
introduces the Bowtie model, a comprehensive framework designed to assess and
predict risk levels. The model combines objective estimations and predictions,
such as traffic flow operations and crowdedness levels, with various
aggravating factors like weather conditions, sentiments, and the purpose of
visitors, to evaluate the expected risk of incidents. The proposed framework is
applied to the Crowd Safety Manager project in Scheveningen, where the DigiTwin
is developed based on a wealth of real-time data sources. One noteworthy data
source is Resono, offering insights into the number of visitors and their
movements, leveraging a mobile phone panel of over 2 million users in the
Netherlands. Particular attention is given to the left-hand side of the Bowtie,
which includes state estimation, prediction, and forecasting. Notably, the
focus is on generating multi-day ahead forecasts for event-planning purposes
using Resono data. Advanced machine learning techniques, including the XGBoost
framework, are compared, with XGBoost demonstrating the most accurate
forecasts. The results indicate that the predictions are adequately accurate.
However, certain locations may benefit from additional input data to further
enhance prediction quality. Despite these limitations, this work contributes to
a more effective crowd management system and opens avenues for further
advancements in this critical field.",2308.00076v1,https://arxiv.org/pdf/2308.00076v1
"Distributionally Robust Safety Filter for Learning-Based Control in
  Active Distribution Systems","Hoang Tien Nguyen, Dae-Hyun Choi","Operational constraint violations may occur when deep reinforcement learning
(DRL) agents interact with real-world active distribution systems to learn
their optimal policies during training. This letter presents a universal
distributionally robust safety filter (DRSF) using which any DRL agent can
reduce the constraint violations of distribution systems significantly during
training while maintaining near-optimal solutions. The DRSF is formulated as a
distributionally robust optimization problem with chance constraints of
operational limits. This problem aims to compute near-optimal actions that are
minimally modified from the optimal actions of DRL-based Volt/VAr control by
leveraging the distribution system model, thereby providing constraint
satisfaction guarantee with a probability level under the model uncertainty.
The performance of the proposed DRSF is verified using the IEEE 33-bus and
123-bus systems.",2307.16351v1,https://arxiv.org/pdf/2307.16351v1
"SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated
  Driving Systems","Amir Samadi, Amir Shirian, Konstantinos Koufos, Kurt Debattista, Mehrdad Dianati","A CF explainer identifies the minimum modifications in the input that would
alter the model's output to its complement. In other words, a CF explainer
computes the minimum modifications required to cross the model's decision
boundary. Current deep generative CF models often work with user-selected
features rather than focusing on the discriminative features of the black-box
model. Consequently, such CF examples may not necessarily lie near the decision
boundary, thereby contradicting the definition of CFs. To address this issue,
we propose in this paper a novel approach that leverages saliency maps to
generate more informative CF explanations. Source codes are available at:
https://github.com/Amir-Samadi//Saliency_Aware_CF.",2307.15786v1,https://arxiv.org/pdf/2307.15786v1
Approximate Model-Based Shielding for Safe Reinforcement Learning,"Alexander W. Goodall, Francesco Belardinelli","Reinforcement learning (RL) has shown great potential for solving complex
tasks in a variety of domains. However, applying RL to safety-critical systems
in the real-world is not easy as many algorithms are sample-inefficient and
maximising the standard RL objective comes with no guarantees on worst-case
performance. In this paper we propose approximate model-based shielding (AMBS),
a principled look-ahead shielding algorithm for verifying the performance of
learned RL policies w.r.t. a set of given safety constraints. Our algorithm
differs from other shielding approaches in that it does not require prior
knowledge of the safety-relevant dynamics of the system. We provide a strong
theoretical justification for AMBS and demonstrate superior performance to
other safety-aware approaches on a set of Atari games with state-dependent
safety-labels.",2308.00707v1,https://arxiv.org/pdf/2308.00707v1
"Evaluation of Safety Constraints in Autonomous Navigation with Deep
  Reinforcement Learning","Brian Angulo, Gregory Gorbov, Aleksandr Panov, Konstantin Yakovlev","While reinforcement learning algorithms have had great success in the field
of autonomous navigation, they cannot be straightforwardly applied to the real
autonomous systems without considering the safety constraints. The later are
crucial to avoid unsafe behaviors of the autonomous vehicle on the road. To
highlight the importance of these constraints, in this study, we compare two
learnable navigation policies: safe and unsafe. The safe policy takes the
constraints into account, while the other does not. We show that the safe
policy is able to generate trajectories with more clearance (distance to the
obstacles) and makes less collisions while training without sacrificing the
overall performance.",2307.14568v1,https://arxiv.org/pdf/2307.14568v1
Reinforcement Learning by Guided Safe Exploration,"Qisong Yang, Thiago D. Simão, Nils Jansen, Simon H. Tindemans, Matthijs T. J. Spaan","Safety is critical to broadening the application of reinforcement learning
(RL). Often, we train RL agents in a controlled environment, such as a
laboratory, before deploying them in the real world. However, the real-world
target task might be unknown prior to deployment. Reward-free RL trains an
agent without the reward to adapt quickly once the reward is revealed. We
consider the constrained reward-free setting, where an agent (the guide) learns
to explore safely without the reward signal. This agent is trained in a
controlled environment, which allows unsafe interactions and still provides the
safety signal. After the target task is revealed, safety violations are not
allowed anymore. Thus, the guide is leveraged to compose a safe behaviour
policy. Drawing from transfer learning, we also regularize a target policy (the
student) towards the guide while the student is unreliable and gradually
eliminate the influence of the guide as training progresses. The empirical
analysis shows that this method can achieve safe transfer learning and helps
the student solve the target task faster.",2307.14316v1,https://arxiv.org/pdf/2307.14316v1
Safety Margins for Reinforcement Learning,"Alexander Grushin, Walt Woods, Alvaro Velasquez, Simon Khan","Any autonomous controller will be unsafe in some situations. The ability to
quantitatively identify when these unsafe situations are about to occur is
crucial for drawing timely human oversight in, e.g., freight transportation
applications. In this work, we demonstrate that the true criticality of an
agent's situation can be robustly defined as the mean reduction in reward given
some number of random actions. Proxy criticality metrics that are computable in
real-time (i.e., without actually simulating the effects of random actions) can
be compared to the true criticality, and we show how to leverage these proxy
metrics to generate safety margins, which directly tie the consequences of
potentially incorrect actions to an anticipated loss in overall performance. We
evaluate our approach on learned policies from APE-X and A3C within an Atari
environment, and demonstrate how safety margins decrease as agents approach
failure states. The integration of safety margins into programs for monitoring
deployed agents allows for the real-time identification of potentially
catastrophic situations.",2307.13642v1,https://arxiv.org/pdf/2307.13642v1
Safety Performance of Neural Networks in the Presence of Covariate Shift,"Chih-Hong Cheng, Harald Ruess, Konstantinos Theodorou","Covariate shift may impact the operational safety performance of neural
networks. A re-evaluation of the safety performance, however, requires
collecting new operational data and creating corresponding ground truth labels,
which often is not possible during operation. We are therefore proposing to
reshape the initial test set, as used for the safety performance evaluation
prior to deployment, based on an approximation of the operational data. This
approximation is obtained by observing and learning the distribution of
activation patterns of neurons in the network during operation. The reshaped
test set reflects the distribution of neuron activation values as observed
during operation, and may therefore be used for re-evaluating safety
performance in the presence of covariate shift. First, we derive conservative
bounds on the values of neurons by applying finite binning and static dataflow
analysis. Second, we formulate a mixed integer linear programming (MILP)
constraint for constructing the minimum set of data points to be removed in the
test set, such that the difference between the discretized test and operational
distributions is bounded. We discuss potential benefits and limitations of this
constraint-based approach based on our initial experience with an implemented
research prototype.",2307.12716v1,https://arxiv.org/pdf/2307.12716v1
Framing Relevance for Safety-Critical Autonomous Systems,Astrid Rakow,"We are in the process of building complex highly autonomous systems that have
build-in beliefs, perceive their environment and exchange information. These
systems construct their respective world view and based on it they plan their
future manoeuvres, i.e., they choose their actions in order to establish their
goals based on their prediction of the possible futures. Usually these systems
face an overwhelming flood of information provided by a variety of sources
where by far not everything is relevant. The goal of our work is to develop a
formal approach to determine what is relevant for a safety critical autonomous
system at its current mission, i.e., what information suffices to build an
appropriate world view to accomplish its mission goals.",2307.14355v1,https://arxiv.org/pdf/2307.14355v1
"What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled
  Safety Critical Systems","Saddek Bensalem, Chih-Hong Cheng, Wei Huang, Xiaowei Huang, Changshun Wu, Xingyu Zhao","Machine learning has made remarkable advancements, but confidently utilising
learning-enabled components in safety-critical domains still poses challenges.
Among the challenges, it is known that a rigorous, yet practical, way of
achieving safety guarantees is one of the most prominent. In this paper, we
first discuss the engineering and research challenges associated with the
design and verification of such systems. Then, based on the observation that
existing works cannot actually achieve provable guarantees, we promote a
two-step verification method for the ultimate achievement of provable
statistical guarantees.",2307.11784v1,https://arxiv.org/pdf/2307.11784v1
"Differentially Flat Learning-based Model Predictive Control Using a
  Stability, State, and Input Constraining Safety Filter","Adam W. Hall, Melissa Greeff, Angela P. Schoellig","Learning-based optimal control algorithms control unknown systems using past
trajectory data and a learned model of the system dynamics. These controllers
use either a linear approximation of the learned dynamics, trading performance
for faster computation, or nonlinear optimization methods, which typically
perform better but can limit real-time applicability. In this work, we present
a novel nonlinear controller that exploits differential flatness to achieve
similar performance to state-of-the-art learning-based controllers but with
significantly less computational effort. Differential flatness is a property of
dynamical systems whereby nonlinear systems can be exactly linearized through a
nonlinear input mapping. Here, the nonlinear transformation is learned as a
Gaussian process and is used in a safety filter that guarantees, with high
probability, stability as well as input and flat state constraint satisfaction.
This safety filter is then used to refine inputs from a flat model predictive
controller to perform constrained nonlinear learning-based optimal control
through two successive convex optimizations. We compare our method to
state-of-the-art learning-based control strategies and achieve similar
performance, but with significantly better computational efficiency, while also
respecting flat state and input constraints, and guaranteeing stability.",2307.10541v1,https://arxiv.org/pdf/2307.10541v1
"Bayesian Safe Policy Learning with Chance Constrained Optimization:
  Application to Military Security Assessment during the Vietnam War","Zeyang Jia, Eli Ben-Michael, Kosuke Imai","Algorithmic decisions and recommendations are used in many high-stakes
decision-making settings such as criminal justice, medicine, and public policy.
We investigate whether it would have been possible to improve a security
assessment algorithm employed during the Vietnam War, using outcomes measured
immediately after its introduction in late 1969. This empirical application
raises several methodological challenges that frequently arise in high-stakes
algorithmic decision-making. First, before implementing a new algorithm, it is
essential to characterize and control the risk of yielding worse outcomes than
the existing algorithm. Second, the existing algorithm is deterministic, and
learning a new algorithm requires transparent extrapolation. Third, the
existing algorithm involves discrete decision tables that are difficult to
optimize over.
  To address these challenges, we introduce the Average Conditional Risk
(ACRisk), which first quantifies the risk that a new algorithmic policy leads
to worse outcomes for subgroups of individual units and then averages this over
the distribution of subgroups. We also propose a Bayesian policy learning
framework that maximizes the posterior expected value while controlling the
posterior expected ACRisk. This framework separates the estimation of
heterogeneous treatment effects from policy optimization, enabling flexible
estimation of effects and optimization over complex policy classes. We
characterize the resulting chance-constrained optimization problem as a
constrained linear programming problem. Our analysis shows that compared to the
actual algorithm used during the Vietnam War, the learned algorithm assesses
most regions as more secure and emphasizes economic and political factors over
military factors.",2307.08840v2,https://arxiv.org/pdf/2307.08840v2
"Bayesian inference for data-efficient, explainable, and safe robotic
  motion planning: A review","Chengmin Zhou, Chao Wang, Haseeb Hassan, Himat Shah, Bingding Huang, Pasi Fränti","Bayesian inference has many advantages in robotic motion planning over four
perspectives: The uncertainty quantification of the policy, safety (risk-aware)
and optimum guarantees of robot motions, data-efficiency in training of
reinforcement learning, and reducing the sim2real gap when the robot is applied
to real-world tasks. However, the application of Bayesian inference in robotic
motion planning is lagging behind the comprehensive theory of Bayesian
inference. Further, there are no comprehensive reviews to summarize the
progress of Bayesian inference to give researchers a systematic understanding
in robotic motion planning. This paper first provides the probabilistic
theories of Bayesian inference which are the preliminary of Bayesian inference
for complex cases. Second, the Bayesian estimation is given to estimate the
posterior of policies or unknown functions which are used to compute the
policy. Third, the classical model-based Bayesian RL and model-free Bayesian RL
algorithms for robotic motion planning are summarized, while these algorithms
in complex cases are also analyzed. Fourth, the analysis of Bayesian inference
in inverse RL is given to infer the reward functions in a data-efficient
manner. Fifth, we systematically present the hybridization of Bayesian
inference and RL which is a promising direction to improve the convergence of
RL for better motion planning. Sixth, given the Bayesian inference, we present
the interpretable and safe robotic motion plannings which are the hot research
topic recently. Finally, all algorithms reviewed in this paper are summarized
analytically as the knowledge graphs, and the future of Bayesian inference for
robotic motion planning is also discussed, to pave the way for data-efficient,
explainable, and safe robotic motion planning strategies for practical
applications.",2307.08024v1,https://arxiv.org/pdf/2307.08024v1
Safe Formulas in the General Theory of Stable Models,"Joohyung Lee, Vladimir Lifschitz, Ravi Palla","Safe first-order formulas generalize the concept of a safe rule, which plays
an important role in the design of answer set solvers. We show that any safe
sentence is equivalent, in a certain sense, to the result of its grounding --
to the variable-free sentence obtained from it by replacing all quantifiers
with multiple conjunctions and disjunctions. It follows that a safe sentence
and the result of its grounding have the same stable models, and that the
stable models of a safe sentence can be characterized by a formula of a simple
syntactic form.",2307.09166v1,https://arxiv.org/pdf/2307.09166v1
SafeDreamer: Safe Reinforcement Learning with World Models,"Weidong Huang, Jiaming Ji, Chunhe Xia, Borong Zhang, Yaodong Yang","The deployment of Reinforcement Learning (RL) in real-world applications is
constrained by its failure to satisfy safety criteria. Existing Safe
Reinforcement Learning (SafeRL) methods, which rely on cost functions to
enforce safety, often fail to achieve zero-cost performance in complex
scenarios, especially vision-only tasks. These limitations are primarily due to
model inaccuracies and inadequate sample efficiency. The integration of the
world model has proven effective in mitigating these shortcomings. In this
work, we introduce SafeDreamer, a novel algorithm incorporating
Lagrangian-based methods into world model planning processes within the
superior Dreamer framework. Our method achieves nearly zero-cost performance on
various tasks, spanning low-dimensional and vision-only input, within the
Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance
and safety in RL tasks. Further details can be found in the code repository:
\url{https://github.com/PKU-Alignment/SafeDreamer}.",2307.07176v3,https://arxiv.org/pdf/2307.07176v3
"CaRT: Certified Safety and Robust Tracking in Learning-based Motion
  Planning for Multi-Agent Systems","Hiroyasu Tsukamoto, Benjamin Rivière, Changrak Choi, Amir Rahmani, Soon-Jo Chung","The key innovation of our analytical method, CaRT, lies in establishing a new
hierarchical, distributed architecture to guarantee the safety and robustness
of a given learning-based motion planning policy. First, in a nominal setting,
the analytical form of our CaRT safety filter formally ensures safe maneuvers
of nonlinear multi-agent systems, optimally with minimal deviation from the
learning-based policy. Second, in off-nominal settings, the analytical form of
our CaRT robust filter optimally tracks the certified safe trajectory,
generated by the previous layer in the hierarchy, the CaRT safety filter. We
show using contraction theory that CaRT guarantees safety and the exponential
boundedness of the trajectory tracking error, even under the presence of
deterministic and stochastic disturbance. Also, the hierarchical nature of CaRT
enables enhancing its robustness for safety just by its superior tracking to
the certified safe trajectory, thereby making it suitable for off-nominal
scenarios with large disturbances. This is a major distinction from
conventional safety function-driven approaches, where the robustness originates
from the stability of a safe set, which could pull the system
over-conservatively to the interior of the safe set. Our log-barrier
formulation in CaRT allows for its distributed implementation in multi-agent
settings. We demonstrate the effectiveness of CaRT in several examples of
nonlinear motion planning and control problems, including optimal,
multi-spacecraft reconfiguration.",2307.08602v2,https://arxiv.org/pdf/2307.08602v2
"Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion
  Models","Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moonseok Choi, Jinwoo Shin, Juho Lee","Large-scale image generation models, with impressive quality made possible by
the vast amount of data available on the Internet, raise social concerns that
these models may generate harmful or copyrighted content. The biases and
harmfulness arise throughout the entire training process and are hard to
completely remove, which have become significant hurdles to the safe deployment
of these models. In this paper, we propose a method called SDD to prevent
problematic content generation in text-to-image diffusion models. We
self-distill the diffusion model to guide the noise estimate conditioned on the
target removal concept to match the unconditional one. Compared to the previous
methods, our method eliminates a much greater proportion of harmful content
from the generated images without degrading the overall image quality.
Furthermore, our method allows the removal of multiple concepts at once,
whereas previous works are limited to removing a single concept at a time.",2307.05977v1,https://arxiv.org/pdf/2307.05977v1
"Safe Reinforcement Learning for Strategic Bidding of Virtual Power
  Plants in Day-Ahead Markets","Ognjen Stanojev, Lesia Mitridati, Riccardo de Nardis di Prata, Gabriela Hug","This paper presents a novel safe reinforcement learning algorithm for
strategic bidding of Virtual Power Plants (VPPs) in day-ahead electricity
markets. The proposed algorithm utilizes the Deep Deterministic Policy Gradient
(DDPG) method to learn competitive bidding policies without requiring an
accurate market model. Furthermore, to account for the complex internal
physical constraints of VPPs we introduce two enhancements to the DDPG method.
Firstly, a projection-based safety shield that restricts the agent's actions to
the feasible space defined by the non-linear power flow equations and operating
constraints of distributed energy resources is derived. Secondly, a penalty for
the shield activation in the reward function that incentivizes the agent to
learn a safer policy is introduced. A case study based on the IEEE 13-bus
network demonstrates the effectiveness of the proposed approach in enabling the
agent to learn a highly competitive, safe strategic policy.",2307.05812v2,https://arxiv.org/pdf/2307.05812v2
"Probabilistic Counterexample Guidance for Safer Reinforcement Learning
  (Extended Version)","Xiaotong Ji, Antonio Filieri","Safe exploration aims at addressing the limitations of Reinforcement Learning
(RL) in safety-critical scenarios, where failures during trial-and-error
learning may incur high costs. Several methods exist to incorporate external
knowledge or to use proximal sensor data to limit the exploration of unsafe
states. However, reducing exploration risks in unknown environments, where an
agent must discover safety threats during exploration, remains challenging. In
this paper, we target the problem of safe exploration by guiding the training
with counterexamples of the safety requirement. Our method abstracts both
continuous and discrete state-space systems into compact abstract models
representing the safety-relevant knowledge acquired by the agent during
exploration. We then exploit probabilistic counterexample generation to
construct minimal simulation submodels eliciting safety requirement violations,
where the agent can efficiently train offline to refine its policy towards
minimising the risk of safety violations during the subsequent online
exploration. We demonstrate our method's effectiveness in reducing safety
violations during online exploration in preliminary experiments by an average
of 40.3% compared with QL and DQN standard algorithms and 29.1% compared with
previous related work, while achieving comparable cumulative rewards with
respect to unrestricted exploration and alternative approaches.",2307.04927v2,https://arxiv.org/pdf/2307.04927v2
Frontier AI Regulation: Managing Emerging Risks to Public Safety,"Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung, Cullen O'Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, Kevin Wolf","Advanced AI models hold the promise of tremendous benefits for humanity, but
society needs to proactively manage the accompanying risks. In this paper, we
focus on what we term ""frontier AI"" models: highly capable foundation models
that could possess dangerous capabilities sufficient to pose severe risks to
public safety. Frontier AI models pose a distinct regulatory challenge:
dangerous capabilities can arise unexpectedly; it is difficult to robustly
prevent a deployed model from being misused; and, it is difficult to stop a
model's capabilities from proliferating broadly. To address these challenges,
at least three building blocks for the regulation of frontier models are
needed: (1) standard-setting processes to identify appropriate requirements for
frontier AI developers, (2) registration and reporting requirements to provide
regulators with visibility into frontier AI development processes, and (3)
mechanisms to ensure compliance with safety standards for the development and
deployment of frontier AI models. Industry self-regulation is an important
first step. However, wider societal discussions and government intervention
will be needed to create standards and to ensure compliance with them. We
consider several options to this end, including granting enforcement powers to
supervisory authorities and licensure regimes for frontier AI models. Finally,
we propose an initial set of safety standards. These include conducting
pre-deployment risk assessments; external scrutiny of model behavior; using
risk assessments to inform deployment decisions; and monitoring and responding
to new information about model capabilities and uses post-deployment. We hope
this discussion contributes to the broader conversation on how to balance
public safety risks and innovation benefits from advances at the frontier of AI
development.",2307.03718v4,https://arxiv.org/pdf/2307.03718v4
"Towards a safe MLOps Process for the Continuous Development and Safety
  Assurance of ML-based Systems in the Railway Domain","Marc Zeller, Thomas Waschulzik, Reiner Schmid, Claus Bahlmann","Traditional automation technologies alone are not sufficient to enable
driverless operation of trains (called Grade of Automation (GoA) 4) on
non-restricted infrastructure. The required perception tasks are nowadays
realized using Machine Learning (ML) and thus need to be developed and deployed
reliably and efficiently. One important aspect to achieve this is to use an
MLOps process for tackling improved reproducibility, traceability,
collaboration, and continuous adaptation of a driverless operation to changing
conditions. MLOps mixes ML application development and operation (Ops) and
enables high frequency software releases and continuous innovation based on the
feedback from operations. In this paper, we outline a safe MLOps process for
the continuous development and safety assurance of ML-based systems in the
railway domain. It integrates system engineering, safety assurance, and the ML
life-cycle in a comprehensive workflow. We present the individual stages of the
process and their interactions. Moreover, we describe relevant challenges to
automate the different stages of the safe MLOps process.",2307.02867v1,https://arxiv.org/pdf/2307.02867v1
Jailbroken: How Does LLM Safety Training Fail?,"Alexander Wei, Nika Haghtalab, Jacob Steinhardt","Large language models trained for safety and harmlessness remain susceptible
to adversarial misuse, as evidenced by the prevalence of ""jailbreak"" attacks on
early releases of ChatGPT that elicit undesired behavior. Going beyond
recognition of the issue, we investigate why such attacks succeed and how they
can be created. We hypothesize two failure modes of safety training: competing
objectives and mismatched generalization. Competing objectives arise when a
model's capabilities and safety goals conflict, while mismatched generalization
occurs when safety training fails to generalize to a domain for which
capabilities exist. We use these failure modes to guide jailbreak design and
then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's
Claude v1.3, against both existing and newly designed attacks. We find that
vulnerabilities persist despite the extensive red-teaming and safety-training
efforts behind these models. Notably, new attacks utilizing our failure modes
succeed on every prompt in a collection of unsafe requests from the models'
red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our
analysis emphasizes the need for safety-capability parity -- that safety
mechanisms should be as sophisticated as the underlying model -- and argues
against the idea that scaling alone can resolve these safety failure modes.",2307.02483v1,https://arxiv.org/pdf/2307.02483v1
Safety Shielding under Delayed Observation,"Filip Cano Córdoba, Alexander Palmisano, Martin Fränzle, Roderick Bloem, Bettina Könighofer","Agents operating in physical environments need to be able to handle delays in
the input and output signals since neither data transmission nor sensing or
actuating the environment are instantaneous. Shields are
correct-by-construction runtime enforcers that guarantee safe execution by
correcting any action that may cause a violation of a formal safety
specification. Besides providing safety guarantees, shields should interfere
minimally with the agent. Therefore, shields should pick the safe corrective
actions in such a way that future interferences are most likely minimized.
Current shielding approaches do not consider possible delays in the input
signals in their safety analyses. In this paper, we address this issue. We
propose synthesis algorithms to compute \emph{delay-resilient shields} that
guarantee safety under worst-case assumptions on the delays of the input
signals. We also introduce novel heuristics for deciding between multiple
corrective actions, designed to minimize future shield interferences caused by
delays. As a further contribution, we present the first integration of shields
in a realistic driving simulator. We implemented our delayed shields in the
driving simulator \textsc{Carla}. We shield potentially unsafe autonomous
driving agents in different safety-critical scenarios and show the effect of
delays on the safety analysis.",2307.02164v1,https://arxiv.org/pdf/2307.02164v1
Efficient Determination of Safety Requirements for Perception Systems,"Sydney M. Katz, Anthony L. Corso, Esen Yel, Mykel J. Kochenderfer","Perception systems operate as a subcomponent of the general autonomy stack,
and perception system designers often need to optimize performance
characteristics while maintaining safety with respect to the overall
closed-loop system. For this reason, it is useful to distill high-level safety
requirements into component-level requirements on the perception system. In
this work, we focus on efficiently determining sets of safe perception system
performance characteristics given a black-box simulator of the
fully-integrated, closed-loop system. We combine the advantages of common
black-box estimation techniques such as Gaussian processes and threshold
bandits to develop a new estimation method, which we call smoothing bandits. We
demonstrate our method on a vision-based aircraft collision avoidance problem
and show improvements in terms of both accuracy and efficiency over the
Gaussian process and threshold bandit baselines.",2307.01371v1,https://arxiv.org/pdf/2307.01371v1
"Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep
  Reinforcement Learning Approach","Iman Sharifi, Mustafa Yildirim, Saber Fallah","The dynamic nature of driving environments and the presence of diverse road
users pose significant challenges for decision-making in autonomous driving.
Deep reinforcement learning (DRL) has emerged as a popular approach to tackle
this problem. However, the application of existing DRL solutions is mainly
confined to simulated environments due to safety concerns, impeding their
deployment in real-world. To overcome this limitation, this paper introduces a
novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics
(DRLSL) that combines the strengths of DRL (learning from experience) and
symbolic first-order logics (knowledge-driven reasoning) to enable safe
learning in real-time interactions of autonomous driving within real
environments. This innovative approach provides a means to learn autonomous
driving policies by actively engaging with the physical environment while
ensuring safety. We have implemented the DRLSL framework in autonomous driving
using the highD dataset and demonstrated that our method successfully avoids
unsafe actions during both the training and testing phases. Furthermore, our
results indicate that DRLSL achieves faster convergence during training and
exhibits better generalizability to new driving scenarios compared to
traditional DRL methods.",2307.01316v2,https://arxiv.org/pdf/2307.01316v2
"Model-Assisted Probabilistic Safe Adaptive Control With Meta-Bayesian
  Learning","Shengbo Wang, Ke Li, Yin Yang, Yuting Cao, Tingwen Huang, Shiping Wen","Breaking safety constraints in control systems can lead to potential risks,
resulting in unexpected costs or catastrophic damage. Nevertheless, uncertainty
is ubiquitous, even among similar tasks. In this paper, we develop a novel
adaptive safe control framework that integrates meta learning, Bayesian models,
and control barrier function (CBF) method. Specifically, with the help of CBF
method, we learn the inherent and external uncertainties by a unified adaptive
Bayesian linear regression (ABLR) model, which consists of a forward neural
network (NN) and a Bayesian output layer. Meta learning techniques are
leveraged to pre-train the NN weights and priors of the ABLR model using data
collected from historical similar tasks. For a new control task, we refine the
meta-learned models using a few samples, and introduce pessimistic confidence
bounds into CBF constraints to ensure safe control. Moreover, we provide
theoretical criteria to guarantee probabilistic safety during the control
processes. To validate our approach, we conduct comparative experiments in
various obstacle avoidance scenarios. The results demonstrate that our
algorithm significantly improves the Bayesian model-based CBF method, and is
capable for efficient safe exploration even with multiple uncertain
constraints.",2307.00828v2,https://arxiv.org/pdf/2307.00828v2
Safe Screening for Unbalanced Optimal Transport,"Xun Su, Zhongxi Fang, Hiroyuki Kasai","This paper introduces a framework that utilizes the Safe Screening technique
to accelerate the optimization process of the Unbalanced Optimal Transport
(UOT) problem by proactively identifying and eliminating zero elements in the
sparse solutions. We demonstrate the feasibility of applying Safe Screening to
the UOT problem with $\ell_2$-penalty and KL-penalty by conducting an analysis
of the solution's bounds and considering the local strong convexity of the dual
problem. Considering the specific structural characteristics of the UOT in
comparison to general Lasso problems on the index matrix, we specifically
propose a novel approximate projection, an elliptical safe region construction,
and a two-hyperplane relaxation method. These enhancements significantly
improve the screening efficiency for the UOT's without altering the algorithm's
complexity.",2307.00247v1,https://arxiv.org/pdf/2307.00247v1
"Bayesian Optimization with Formal Safety Guarantees via Online Conformal
  Prediction","Yunchuan Zhang, Sangwoo Park, Osvaldo Simeone","Black-box zero-th order optimization is a central primitive for applications
in fields as diverse as finance, physics, and engineering. In a common
formulation of this problem, a designer sequentially attempts candidate
solutions, receiving noisy feedback on the value of each attempt from the
system. In this paper, we study scenarios in which feedback is also provided on
the safety of the attempted solution, and the optimizer is constrained to limit
the number of unsafe solutions that are tried throughout the optimization
process. Focusing on methods based on Bayesian optimization (BO), prior art has
introduced an optimization scheme -- referred to as SAFEOPT -- that is
guaranteed not to select any unsafe solution with a controllable probability
over feedback noise as long as strict assumptions on the safety constraint
function are met. In this paper, a novel BO-based approach is introduced that
satisfies safety requirements irrespective of properties of the constraint
function. This strong theoretical guarantee is obtained at the cost of allowing
for an arbitrary, controllable but non-zero, rate of violation of the safety
constraint. The proposed method, referred to as SAFE-BOCP, builds on online
conformal prediction (CP) and is specialized to the cases in which feedback on
the safety constraint is either noiseless or noisy. Experimental results on
synthetic and real-world data validate the advantages and flexibility of the
proposed SAFE-BOCP.",2306.17815v3,https://arxiv.org/pdf/2306.17815v3
Probabilistic Constraint for Safety-Critical Reinforcement Learning,"Weiqin Chen, Dharmashankar Subramanian, Santiago Paternain","In this paper, we consider the problem of learning safe policies for
probabilistic-constrained reinforcement learning (RL). Specifically, a safe
policy or controller is one that, with high probability, maintains the
trajectory of the agent in a given safe set. We establish a connection between
this probabilistic-constrained setting and the cumulative-constrained
formulation that is frequently explored in the existing literature. We provide
theoretical bounds elucidating that the probabilistic-constrained setting
offers a better trade-off in terms of optimality and safety (constraint
satisfaction). The challenge encountered when dealing with the probabilistic
constraints, as explored in this work, arises from the absence of explicit
expressions for their gradients. Our prior work provides such an explicit
gradient expression for probabilistic constraints which we term Safe Policy
Gradient-REINFORCE (SPG-REINFORCE). In this work, we provide an improved
gradient SPG-Actor-Critic that leads to a lower variance than SPG-REINFORCE,
which is substantiated by our theoretical results. A noteworthy aspect of both
SPGs is their inherent algorithm independence, rendering them versatile for
application across a range of policy-based algorithms. Furthermore, we propose
a Safe Primal-Dual algorithm that can leverage both SPGs to learn safe
policies. It is subsequently followed by theoretical analyses that encompass
the convergence of the algorithm, as well as the near-optimality and
feasibility on average. In addition, we test the proposed approaches by a
series of empirical experiments. These experiments aim to examine and analyze
the inherent trade-offs between the optimality and safety, and serve to
substantiate the efficacy of two SPGs, as well as our theoretical
contributions.",2306.17279v2,https://arxiv.org/pdf/2306.17279v2
Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning,"Matej Jusup, Barna Pásztor, Tadeusz Janik, Kenan Zhang, Francesco Corman, Andreas Krause, Ilija Bogunovic","Many applications, e.g., in shared mobility, require coordinating a large
number of agents. Mean-field reinforcement learning addresses the resulting
scalability challenge by optimizing the policy of a representative agent
interacting with the infinite population of identical agents instead of
considering individual pairwise interactions. In this paper, we address an
important generalization where there exist global constraints on the
distribution of agents (e.g., requiring capacity constraints or minimum
coverage requirements to be met). We propose Safe-M$^3$-UCRL, the first
model-based mean-field reinforcement learning algorithm that attains safe
policies even in the case of unknown transitions. As a key ingredient, it uses
epistemic uncertainty in the transition model within a log-barrier approach to
ensure pessimistic constraints satisfaction with high probability. Beyond the
synthetic swarm motion benchmark, we showcase Safe-M$^3$-UCRL on the vehicle
repositioning problem faced by many shared mobility operators and evaluate its
performance through simulations built on vehicle trajectory data from a service
provider in Shenzhen. Our algorithm effectively meets the demand in critical
areas while ensuring service accessibility in regions with low demand.",2306.17052v2,https://arxiv.org/pdf/2306.17052v2
"Safety-Aware Task Composition for Discrete and Continuous Reinforcement
  Learning","Kevin Leahy, Makai Mann, Zachary Serlin","Compositionality is a critical aspect of scalable system design.
Reinforcement learning (RL) has recently shown substantial success in task
learning, but has only recently begun to truly leverage composition. In this
paper, we focus on Boolean composition of learned tasks as opposed to
functional or sequential composition. Existing Boolean composition for RL
focuses on reaching a satisfying absorbing state in environments with discrete
action spaces, but does not support composable safety (i.e., avoidance)
constraints. We advance the state of the art in Boolean composition of learned
tasks with three contributions: i) introduce two distinct notions of safety in
this framework; ii) show how to enforce either safety semantics, prove
correctness (under some assumptions), and analyze the trade-offs between the
two safety notions; and iii) extend Boolean composition from discrete action
spaces to continuous action spaces. We demonstrate these techniques using
modified versions of value iteration in a grid world, Deep Q-Network (DQN) in a
grid world with image observations, and Twin Delayed DDPG (TD3) in a
continuous-observation and continuous-action Bullet physics environment. We
believe that these contributions advance the theory of safe reinforcement
learning by allowing zero-shot composition of policies satisfying safety
properties.",2306.17033v1,https://arxiv.org/pdf/2306.17033v1
Verifying Safety of Neural Networks from Topological Perspectives,"Zhen Liang, Dejin Ren, Bai Xue, Ji Wang, Wenjing Yang, Wanwei Liu","Neural networks (NNs) are increasingly applied in safety-critical systems
such as autonomous vehicles. However, they are fragile and are often
ill-behaved. Consequently, their behaviors should undergo rigorous guarantees
before deployment in practice. In this paper, we propose a set-boundary
reachability method to investigate the safety verification problem of NNs from
a topological perspective. Given an NN with an input set and a safe set, the
safety verification problem is to determine whether all outputs of the NN
resulting from the input set fall within the safe set. In our method, the
homeomorphism property and the open map property of NNs are mainly exploited,
which establish rigorous guarantees between the boundaries of the input set and
the boundaries of the output set. The exploitation of these two properties
facilitates reachability computations via extracting subsets of the input set
rather than the entire input set, thus controlling the wrapping effect in
reachability analysis and facilitating the reduction of computation burdens for
safety verification. The homeomorphism property exists in some widely used NNs
such as invertible residual networks (i-ResNets) and Neural ordinary
differential equations (Neural ODEs), and the open map is a less strict
property and easier to satisfy compared with the homeomorphism property. For
NNs establishing either of these properties, our set-boundary reachability
method only needs to perform reachability analysis on the boundary of the input
set. Moreover, for NNs that do not feature these properties with respect to the
input set, we explore subsets of the input set for establishing the local
homeomorphism property and then abandon these subsets for reachability
computations. Finally, some examples demonstrate the performance of the
proposed method.",2306.15403v1,https://arxiv.org/pdf/2306.15403v1
"Safe Navigation in Unstructured Environments by Minimizing Uncertainty
  in Control and Perception","Junwon Seo, Jungwi Mun, Taekyung Kim","Uncertainty in control and perception poses challenges for autonomous vehicle
navigation in unstructured environments, leading to navigation failures and
potential vehicle damage. This paper introduces a framework that minimizes
control and perception uncertainty to ensure safe and reliable navigation. The
framework consists of two uncertainty-aware models: a learning-based vehicle
dynamics model and a self-supervised traversability estimation model. We train
a vehicle dynamics model that can quantify the epistemic uncertainty of the
model to perform active exploration, resulting in the efficient collection of
training data and effective avoidance of uncertain state-action spaces. In
addition, we employ meta-learning to train a traversability cost prediction
network. The model can be trained with driving data from a variety of types of
terrain, and it can online-adapt based on interaction experiences to reduce the
aleatoric uncertainty. Integrating the dynamics model and traversability cost
prediction model with a sampling-based model predictive controller allows for
optimizing trajectories that avoid uncertain terrains and state-action spaces.
Experimental results demonstrate that the proposed method reduces uncertainty
in prediction and improves stability in autonomous vehicle navigation in
unstructured environments.",2306.14601v1,https://arxiv.org/pdf/2306.14601v1
"Safety-Critical Scenario Generation Via Reinforcement Learning Based
  Editing","Haolan Liu, Liangjun Zhang, Siva Kumar Sastry Hari, Jishen Zhao","Generating safety-critical scenarios is essential for testing and verifying
the safety of autonomous vehicles. Traditional optimization techniques suffer
from the curse of dimensionality and limit the search space to fixed parameter
spaces. To address these challenges, we propose a deep reinforcement learning
approach that generates scenarios by sequential editing, such as adding new
agents or modifying the trajectories of the existing agents. Our framework
employs a reward function consisting of both risk and plausibility objectives.
The plausibility objective leverages generative models, such as a variational
autoencoder, to learn the likelihood of the generated parameters from the
training datasets; It penalizes the generation of unlikely scenarios. Our
approach overcomes the dimensionality challenge and explores a wide range of
safety-critical scenarios. Our evaluation demonstrates that the proposed method
generates safety-critical scenarios of higher quality compared with previous
approaches.",2306.14131v3,https://arxiv.org/pdf/2306.14131v3
"Transforming a Quadruped into a Guide Robot for the Visually Impaired:
  Formalizing Wayfinding, Interaction Modeling, and Safety Mechanism","J. Taery Kim, Wenhao Yu, Yash Kothari, Jie Tan, Greg Turk, Sehoon Ha","This paper explores the principles for transforming a quadrupedal robot into
a guide robot for individuals with visual impairments. A guide robot has great
potential to resolve the limited availability of guide animals that are
accessible to only two to three percent of the potential blind or visually
impaired (BVI) users. To build a successful guide robot, our paper explores
three key topics: (1) formalizing the navigation mechanism of a guide dog and a
human, (2) developing a data-driven model of their interaction, and (3)
improving user safety. First, we formalize the wayfinding task of the
human-guide robot team using Markov Decision Processes based on the literature
and interviews. Then we collect real human-robot interaction data from three
visually impaired and six sighted people and develop an interaction model
called the ``Delayed Harness'' to effectively simulate the navigation behaviors
of the team. Additionally, we introduce an action shielding mechanism to
enhance user safety by predicting and filtering out dangerous actions. We
evaluate the developed interaction model and the safety mechanism in
simulation, which greatly reduce the prediction errors and the number of
collisions, respectively. We also demonstrate the integrated system on a
quadrupedal robot with a rigid harness, by guiding users over $100+$~m
trajectories.",2306.14055v1,https://arxiv.org/pdf/2306.14055v1
Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery,"Xiao Zhang, Hai Zhang, Hongtu Zhou, Chang Huang, Di Zhang, Chen Ye, Junqiao Zhao","Safety is one of the main challenges in applying reinforcement learning to
realistic environmental tasks. To ensure safety during and after training
process, existing methods tend to adopt overly conservative policy to avoid
unsafe situations. However, overly conservative policy severely hinders the
exploration, and makes the algorithms substantially less rewarding. In this
paper, we propose a method to construct a boundary that discriminates safe and
unsafe states. The boundary we construct is equivalent to distinguishing
dead-end states, indicating the maximum extent to which safe exploration is
guaranteed, and thus has minimum limitation on exploration. Similar to Recovery
Reinforcement Learning, we utilize a decoupled RL framework to learn two
policies, (1) a task policy that only considers improving the task performance,
and (2) a recovery policy that maximizes safety. The recovery policy and a
corresponding safety critic are pretrained on an offline dataset, in which the
safety critic evaluates upper bound of safety in each state as awareness of
environmental safety for the agent. During online training, a behavior
correction mechanism is adopted, ensuring the agent to interact with the
environment using safe actions only. Finally, experiments of continuous control
tasks demonstrate that our approach has better task performance with less
safety violations than state-of-the-art algorithms.",2306.13944v1,https://arxiv.org/pdf/2306.13944v1
"Efficient Model Selection for Predictive Pattern Mining Model by Safe
  Pattern Pruning","Takumi Yoshida, Hiroyuki Hanada, Kazuya Nakagawa, Kouichi Taji, Koji Tsuda, Ichiro Takeuchi","Predictive pattern mining is an approach used to construct prediction models
when the input is represented by structured data, such as sets, graphs, and
sequences. The main idea behind predictive pattern mining is to build a
prediction model by considering substructures, such as subsets, subgraphs, and
subsequences (referred to as patterns), present in the structured data as
features of the model. The primary challenge in predictive pattern mining lies
in the exponential growth of the number of patterns with the complexity of the
structured data. In this study, we propose the Safe Pattern Pruning (SPP)
method to address the explosion of pattern numbers in predictive pattern
mining. We also discuss how it can be effectively employed throughout the
entire model building process in practical data analysis. To demonstrate the
effectiveness of the proposed method, we conduct numerical experiments on
regression and classification problems involving sets, graphs, and sequences.",2306.13561v1,https://arxiv.org/pdf/2306.13561v1
"Efficient Partitioning Method of Large-Scale Public Safety
  Spatio-Temporal Data based on Information Loss Constraints","Jie Gao, Yawen Li, Zhe Xue, Zeli Guan","The storage, management, and application of massive spatio-temporal data are
widely applied in various practical scenarios, including public safety.
However, due to the unique spatio-temporal distribution characteristics of
re-al-world data, most existing methods have limitations in terms of the
spatio-temporal proximity of data and load balancing in distributed storage.
There-fore, this paper proposes an efficient partitioning method of large-scale
public safety spatio-temporal data based on information loss constraints
(IFL-LSTP). The IFL-LSTP model specifically targets large-scale spatio-temporal
point da-ta by combining the spatio-temporal partitioning module (STPM) with
the graph partitioning module (GPM). This approach can significantly reduce the
scale of data while maintaining the model's accuracy, in order to improve the
partitioning efficiency. It can also ensure the load balancing of distributed
storage while maintaining spatio-temporal proximity of the data partitioning
results. This method provides a new solution for distributed storage of
mas-sive spatio-temporal data. The experimental results on multiple real-world
da-tasets demonstrate the effectiveness and superiority of IFL-LSTP.",2306.12857v2,https://arxiv.org/pdf/2306.12857v2
"Proactive Human-Robot Co-Assembly: Leveraging Human Intention Prediction
  and Robust Safe Control","Ruixuan Liu, Rui Chen, Abulikemu Abuduweili, Changliu Liu","Human-robot collaboration (HRC) is one key component to achieving flexible
manufacturing to meet the different needs of customers. However, it is
difficult to build intelligent robots that can proactively assist humans in a
safe and efficient way due to several challenges. First, it is challenging to
achieve efficient collaboration due to diverse human behaviors and data
scarcity. Second, it is difficult to ensure interactive safety due to
uncertainty in human behaviors. This paper presents an integrated framework for
proactive HRC. A robust intention prediction module, which leverages prior task
information and human-in-the-loop training, is learned to guide the robot for
efficient collaboration. The proposed framework also uses robust safe control
to ensure interactive safety under uncertainty. The developed framework is
applied to a co-assembly task using a Kinova Gen3 robot. The experiment
demonstrates that our solution is robust to environmental changes as well as
different human preferences and behaviors. In addition, it improves task
efficiency by approximately 15-20%. Moreover, the experiment demonstrates that
our solution can guarantee interactive safety during proactive collaboration.",2306.11862v2,https://arxiv.org/pdf/2306.11862v2
"Safe, Efficient, Comfort, and Energy-saving Automated Driving through
  Roundabout Based on Deep Reinforcement Learning","Henan Yuan, Penghui Li, Bart van Arem, Liujiang Kang, Yongqi Dong","Traffic scenarios in roundabouts pose substantial complexity for automated
driving. Manually mapping all possible scenarios into a state space is
labor-intensive and challenging. Deep reinforcement learning (DRL) with its
ability to learn from interacting with the environment emerges as a promising
solution for training such automated driving models. This study explores,
employs, and implements various DRL algorithms, namely Deep Deterministic
Policy Gradient (DDPG), Proximal Policy Optimization (PPO), and Trust Region
Policy Optimization (TRPO) to instruct automated vehicles' driving through
roundabouts. The driving state space, action space, and reward function are
designed. The reward function considers safety, efficiency, comfort, and energy
consumption to align with real-world requirements. All three tested DRL
algorithms succeed in enabling automated vehicles to drive through the
roundabout. To holistically evaluate the performance of these algorithms, this
study establishes an evaluation methodology considering multiple indicators
such as safety, efficiency, and comfort level. A method employing the Analytic
Hierarchy Process is also developed to weigh these evaluation indicators.
Experimental results on various testing scenarios reveal that the TRPO
algorithm outperforms DDPG and PPO in terms of safety and efficiency, and PPO
performs best in terms of comfort level. Lastly, to verify the model's
adaptability and robustness regarding other driving scenarios, this study also
deploys the model trained by TRPO to a range of different testing scenarios,
e.g., highway driving and merging. Experimental results demonstrate that the
TRPO model trained on only roundabout driving scenarios exhibits a certain
degree of proficiency in highway driving and merging scenarios. This study
provides a foundation for the application of automated driving with DRL in real
traffic environments.",2306.11465v1,https://arxiv.org/pdf/2306.11465v1
Datasets and Benchmarks for Offline Safe Reinforcement Learning,"Zuxin Liu, Zijian Guo, Haohong Lin, Yihang Yao, Jiacheng Zhu, Zhepeng Cen, Hanjiang Hu, Wenhao Yu, Tingnan Zhang, Jie Tan, Ding Zhao","This paper presents a comprehensive benchmarking suite tailored to offline
safe reinforcement learning (RL) challenges, aiming to foster progress in the
development and evaluation of safe learning algorithms in both the training and
deployment phases. Our benchmark suite contains three packages: 1) expertly
crafted safe policies, 2) D4RL-styled datasets along with environment wrappers,
and 3) high-quality offline safe RL baseline implementations. We feature a
methodical data collection pipeline powered by advanced safe RL algorithms,
which facilitates the generation of diverse datasets across 38 popular safe RL
tasks, from robot control to autonomous driving. We further introduce an array
of data post-processing filters, capable of modifying each dataset's diversity,
thereby simulating various data collection conditions. Additionally, we provide
elegant and extensible implementations of prevalent offline safe RL algorithms
to accelerate research in this area. Through extensive experiments with over
50000 CPU and 800 GPU hours of computations, we evaluate and compare the
performance of these baseline algorithms on the collected datasets, offering
insights into their strengths, limitations, and potential areas of improvement.
Our benchmarking framework serves as a valuable resource for researchers and
practitioners, facilitating the development of more robust and reliable offline
safe RL solutions in safety-critical applications. The benchmark website is
available at \url{www.offline-saferl.org}.",2306.09303v2,https://arxiv.org/pdf/2306.09303v2
"Predictive Maneuver Planning with Deep Reinforcement Learning (PMP-DRL)
  for comfortable and safe autonomous driving","Jayabrata Chowdhury, Vishruth Veerendranath, Suresh Sundaram, Narasimhan Sundararajan","This paper presents a Predictive Maneuver Planning with Deep Reinforcement
Learning (PMP-DRL) model for maneuver planning. Traditional rule-based maneuver
planning approaches often have to improve their abilities to handle the
variabilities of real-world driving scenarios. By learning from its experience,
a Reinforcement Learning (RL)-based driving agent can adapt to changing driving
conditions and improve its performance over time. Our proposed approach
combines a predictive model and an RL agent to plan for comfortable and safe
maneuvers. The predictive model is trained using historical driving data to
predict the future positions of other surrounding vehicles. The surrounding
vehicles' past and predicted future positions are embedded in context-aware
grid maps. At the same time, the RL agent learns to make maneuvers based on
this spatio-temporal context information. Performance evaluation of PMP-DRL has
been carried out using simulated environments generated from publicly available
NGSIM US101 and I80 datasets. The training sequence shows the continuous
improvement in the driving experiences. It shows that proposed PMP-DRL can
learn the trade-off between safety and comfortability. The decisions generated
by the recent imitation learning-based model are compared with the proposed
PMP-DRL for unseen scenarios. The results clearly show that PMP-DRL can handle
complex real-world scenarios and make better comfortable and safe maneuver
decisions than rule-based and imitative models.",2306.09055v1,https://arxiv.org/pdf/2306.09055v1
"Safeguarding Data in Multimodal AI: A Differentially Private Approach to
  CLIP Training","Alyssa Huang, Peihan Liu, Ryumei Nakada, Linjun Zhang, Wanrong Zhang","The surge in multimodal AI's success has sparked concerns over data privacy
in vision-and-language tasks. While CLIP has revolutionized multimodal learning
through joint training on images and text, its potential to unintentionally
disclose sensitive information necessitates the integration of
privacy-preserving mechanisms. We introduce a differentially private adaptation
of the Contrastive Language-Image Pretraining (CLIP) model that effectively
addresses privacy concerns while retaining accuracy. Our proposed method,
Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diverse
vision-and-language tasks such as image classification and visual question
answering. We demonstrate that our approach retains performance on par with the
standard non-private CLIP model. Furthermore, we analyze our proposed algorithm
under linear representation settings. We derive the convergence rate of our
algorithm and show a trade-off between utility and privacy when gradients are
clipped per-batch and the loss function does not satisfy smoothness conditions
assumed in the literature for the analysis of DP-SGD.",2306.08173v2,https://arxiv.org/pdf/2306.08173v2
Safe Use of Neural Networks,George Redinbo,"Neural networks in modern communication systems can be susceptible to
internal numerical errors that can drastically effect decision results. Such
structures are composed of many sections each of which generally contain
weighting operations and activation function evaluations. The safe use comes
from methods employing number based codes that can detect arithmetic errors in
the network's processing steps. Each set of operations generates parity values
dictated by a code in two ways. One set of parities is obtained from a
section's outputs while a second comparable set is developed directly from the
original inputs. The parity values protecting the activation functions involve
a Taylor series approximation to the activation functions. We focus on using
long numerically based convolutional codes because of the large size of data
sets. The codes are based on Discrete Fourier Transform kernels and there are
many design options available. Mathematical program simulations show our
error-detecting techniques are effective and efficient.",2306.08086v1,https://arxiv.org/pdf/2306.08086v1
Tuning Legged Locomotion Controllers via Safe Bayesian Optimization,"Daniel Widmer, Dongho Kang, Bhavya Sukhija, Jonas Hübotter, Andreas Krause, Stelian Coros","This paper presents a data-driven strategy to streamline the deployment of
model-based controllers in legged robotic hardware platforms. Our approach
leverages a model-free safe learning algorithm to automate the tuning of
control gains, addressing the mismatch between the simplified model used in the
control formulation and the real system. This method substantially mitigates
the risk of hazardous interactions with the robot by sample-efficiently
optimizing parameters within a probably safe region. Additionally, we extend
the applicability of our approach to incorporate the different gait parameters
as contexts, leading to a safe, sample-efficient exploration algorithm capable
of tuning a motion controller for diverse gait patterns. We validate our method
through simulation and hardware experiments, where we demonstrate that the
algorithm obtains superior performance on tuning a model-based motion
controller for multiple gaits safely.",2306.07092v3,https://arxiv.org/pdf/2306.07092v3
"From psychological traits to safety warnings: three studies on
  recommendations in a smart home environment","Federica Cena, Cristina Gena, Claudio Mattutino, Michele Mioli, Fabiana Vernero","In this paper, we report on three experiments we have carried out in the
context of the EMPATHY project, with the aim of helping users make better
configuration choices in a smart home environment, and discuss our results. We
found that there are psychological traits, such as Need for Cognition, which
influence the way individuals tend to use recommendations, that there are non
obvious relationships between the perceived usefulness of recommendations in
different domains and individuals' ability to exploit suggestions on
configuration choices, and that detailed, easy-to-understand security
explanations are more persuasive than simple security warnings, when it comes
to make decisions on the applicability of rules which might cause privacy and
security risks.",2306.05752v1,https://arxiv.org/pdf/2306.05752v1
Safety and Fairness for Content Moderation in Generative Models,"Susan Hao, Piyush Kumar, Sarah Laszlo, Shivani Poddar, Bhaktipriya Radharapu, Renee Shelby","With significant advances in generative AI, new technologies are rapidly
being deployed with generative components. Generative models are typically
trained on large datasets, resulting in model behaviors that can mimic the
worst of the content in the training data. Responsible deployment of generative
technologies requires content moderation strategies, such as safety input and
output filters. Here, we provide a theoretical framework for conceptualizing
responsible content moderation of text-to-image generative technologies,
including a demonstration of how to empirically measure the constructs we
enumerate. We define and distinguish the concepts of safety, fairness, and
metric equity, and enumerate example harms that can come in each domain. We
then provide a demonstration of how the defined harms can be quantified. We
conclude with a summary of how the style of harms quantification we demonstrate
enables data-driven content moderation decisions.",2306.06135v1,https://arxiv.org/pdf/2306.06135v1
Safe Collaborative Filtering,"Riku Togashi, Tatsushi Oka, Naoto Ohsaka, Tetsuro Morimura","Excellent tail performance is crucial for modern machine learning tasks, such
as algorithmic fairness, class imbalance, and risk-sensitive decision making,
as it ensures the effective handling of challenging samples within a dataset.
Tail performance is also a vital determinant of success for personalized
recommender systems to reduce the risk of losing users with low satisfaction.
This study introduces a ""safe"" collaborative filtering method that prioritizes
recommendation quality for less-satisfied users rather than focusing on the
average performance. Our approach minimizes the conditional value at risk
(CVaR), which represents the average risk over the tails of users' loss. To
overcome computational challenges for web-scale recommender systems, we develop
a robust yet practical algorithm that extends the most scalable method,
implicit alternating least squares (iALS). Empirical evaluation on real-world
datasets demonstrates the excellent tail performance of our approach while
maintaining competitive computational efficiency.",2306.05292v2,https://arxiv.org/pdf/2306.05292v2
"G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks
  through Attributed Client Graph Clustering","Hao Yu, Chuan Ma, Meng Liu, Tianyu Du, Ming Ding, Tao Xiang, Shouling Ji, Xinwang Liu","Federated Learning (FL) offers collaborative model training without data
sharing but is vulnerable to backdoor attacks, where poisoned model weights
lead to compromised system integrity. Existing countermeasures, primarily based
on anomaly detection, are prone to erroneous rejections of normal weights while
accepting poisoned ones, largely due to shortcomings in quantifying
similarities among client models. Furthermore, other defenses demonstrate
effectiveness only when dealing with a limited number of malicious clients,
typically fewer than 10%. To alleviate these vulnerabilities, we present
G$^2$uardFL, a protective framework that reinterprets the identification of
malicious clients as an attributed graph clustering problem, thus safeguarding
FL systems. Specifically, this framework employs a client graph clustering
approach to identify malicious clients and integrates an adaptive mechanism to
amplify the discrepancy between the aggregated model and the poisoned ones,
effectively eliminating embedded backdoors. We also conduct a theoretical
analysis of convergence to confirm that G$^2$uardFL does not affect the
convergence of FL systems. Through empirical evaluation, comparing G$^2$uardFL
with cutting-edge defenses, such as FLAME (USENIX Security 2022) [28] and
DeepSight (NDSS 2022) [36], against various backdoor attacks including 3DFed
(SP 2023) [20], our results demonstrate its significant effectiveness in
mitigating backdoor attacks while having a negligible impact on the aggregated
model's performance on benign samples (i.e., the primary task performance). For
instance, in an FL system with 25% malicious clients, G$^2$uardFL reduces the
attack success rate to 10.61%, while maintaining a primary task performance of
73.05% on the CIFAR-10 dataset. This surpasses the performance of the
best-performing baseline, which merely achieves a primary task performance of
19.54%.",2306.04984v2,https://arxiv.org/pdf/2306.04984v2
"Value Functions are Control Barrier Functions: Verification of Safe
  Policies using Control Theory","Daniel C. H. Tan, Fernando Acero, Robert McCarthy, Dimitrios Kanoulas, Zhibin Li","Guaranteeing safe behaviour of reinforcement learning (RL) policies poses
significant challenges for safety-critical applications, despite RL's
generality and scalability. To address this, we propose a new approach to apply
verification methods from control theory to learned value functions. By
analyzing task structures for safety preservation, we formalize original
theorems that establish links between value functions and control barrier
functions. Further, we propose novel metrics for verifying value functions in
safe control tasks and practical implementation details to improve learning.
Our work presents a novel method for certificate learning, which unlocks a
diversity of verification techniques from control theory for RL policies, and
marks a significant step towards a formal framework for the general, scalable,
and verifiable design of RL-based control systems. Code and videos are
available at this https url: https://rl-cbf.github.io/",2306.04026v4,https://arxiv.org/pdf/2306.04026v4
AI-Supported Assessment of Load Safety,"Julius Schöning, Niklas Kruse","Load safety assessment and compliance is an essential step in the corporate
process of every logistics service provider. In 2020, a total of 11,371 police
checks of trucks were carried out, during which 9.6% (1091) violations against
the load safety regulations were detected. For a logistic service provider,
every load safety violation results in height fines and damage to reputation.
An assessment of load safety supported by artificial intelligence (AI) will
reduce the risk of accidents by unsecured loads and fines during safety
assessments. This work shows how photos of the load, taken by the truck driver
or the loadmaster after the loading process, can be used to assess load safety.
By a trained two-stage artificial neural network (ANN), these photos are
classified into three different classes I) cargo loaded safely, II) cargo
loaded unsafely, and III) unusable image. By applying several architectures of
convolutional neural networks (CNN), it can be shown that it is possible to
distinguish between unusable and usable images for cargo safety assessment.
This distinction is quite crucial since the truck driver and the loadmaster
sometimes provide photos without the essential image features like the case
structure of the truck and the whole cargo. A human operator or another ANN
will then assess the load safety within the second stage.",2306.03795v1,https://arxiv.org/pdf/2306.03795v1
The Chai Platform's AI Safety Framework,"Xiaoding Lu, Aleksey Korshuk, Zongyi Liu, William Beauchamp","Chai empowers users to create and interact with customized chatbots, offering
unique and engaging experiences. Despite the exciting prospects, the work
recognizes the inherent challenges of a commitment to modern safety standards.
Therefore, this paper presents the integrated AI safety principles into Chai to
prioritize user safety, data protection, and ethical technology use. The paper
specifically explores the multidimensional domain of AI safety research,
demonstrating its application in Chai's conversational chatbot platform. It
presents Chai's AI safety principles, informed by well-established AI research
centres and adapted for chat AI. This work proposes the following safety
framework: Content Safeguarding; Stability and Robustness; and Operational
Transparency and Traceability. The subsequent implementation of these
principles is outlined, followed by an experimental analysis of Chai's AI
safety framework's real-world impact. We emphasise the significance of
conscientious application of AI safety principles and robust safety measures.
The successful implementation of the safe AI framework in Chai indicates the
practicality of mitigating potential risks for responsible and ethical use of
AI technologies. The ultimate vision is a transformative AI tool fostering
progress and innovation while prioritizing user safety and ethical standards.",2306.02979v1,https://arxiv.org/pdf/2306.02979v1
Safe Offline Reinforcement Learning with Real-Time Budget Constraints,"Qian Lin, Bo Tang, Zifan Wu, Chao Yu, Shangqin Mao, Qianlong Xie, Xingxing Wang, Dong Wang","Aiming at promoting the safe real-world deployment of Reinforcement Learning
(RL), research on safe RL has made significant progress in recent years.
However, most existing works in the literature still focus on the online
setting where risky violations of the safety budget are likely to be incurred
during training. Besides, in many real-world applications, the learned policy
is required to respond to dynamically determined safety budgets (i.e.,
constraint threshold) in real time. In this paper, we target at the above
real-time budget constraint problem under the offline setting, and propose
Trajectory-based REal-time Budget Inference (TREBI) as a novel solution that
models this problem from the perspective of trajectory distribution and solves
it through diffusion model planning. Theoretically, we prove an error bound of
the estimation on the episodic reward and cost under the offline setting and
thus provide a performance guarantee for TREBI. Empirical results on a wide
range of simulation tasks and a real-world large-scale advertising application
demonstrate the capability of TREBI in solving real-time budget constraint
problems under offline settings.",2306.00603v2,https://arxiv.org/pdf/2306.00603v2
"Provably Efficient Generalized Lagrangian Policy Optimization for Safe
  Multi-Agent Reinforcement Learning","Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, Mihailo R. Jovanović","We examine online safe multi-agent reinforcement learning using constrained
Markov games in which agents compete by maximizing their expected total rewards
under a constraint on expected total utilities. Our focus is confined to an
episodic two-player zero-sum constrained Markov game with independent
transition functions that are unknown to agents, adversarial reward functions,
and stochastic utility functions. For such a Markov game, we employ an approach
based on the occupancy measure to formulate it as an online constrained
saddle-point problem with an explicit constraint. We extend the Lagrange
multiplier method in constrained optimization to handle the constraint by
creating a generalized Lagrangian with minimax decision primal variables and a
dual variable. Next, we develop an upper confidence reinforcement learning
algorithm to solve this Lagrangian problem while balancing exploration and
exploitation. Our algorithm updates the minimax decision primal variables via
online mirror descent and the dual variable via projected gradient step and we
prove that it enjoys sublinear rate $ O((|X|+|Y|) L \sqrt{T(|A|+|B|)}))$ for
both regret and constraint violation after playing $T$ episodes of the game.
Here, $L$ is the horizon of each episode, $(|X|,|A|)$ and $(|Y|,|B|)$ are the
state/action space sizes of the min-player and the max-player, respectively. To
the best of our knowledge, we provide the first provably efficient online safe
reinforcement learning algorithm in constrained Markov games.",2306.00212v1,https://arxiv.org/pdf/2306.00212v1
SafeDiffuser: Safe Planning with Diffusion Probabilistic Models,"Wei Xiao, Tsun-Hsuan Wang, Chuang Gan, Daniela Rus","Diffusion model-based approaches have shown promise in data-driven planning,
but there are no safety guarantees, thus making it hard to be applied for
safety-critical applications. To address these challenges, we propose a new
method, called SafeDiffuser, to ensure diffusion probabilistic models satisfy
specifications by using a class of control barrier functions. The key idea of
our approach is to embed the proposed finite-time diffusion invariance into the
denoising diffusion procedure, which enables trustworthy diffusion data
generation. Moreover, we demonstrate that our finite-time diffusion invariance
method through generative models not only maintains generalization performance
but also creates robustness in safe data generation. We test our method on a
series of safe planning tasks, including maze path generation, legged robot
locomotion, and 3D space manipulation, with results showing the advantages of
robustness and guarantees over vanilla diffusion models.",2306.00148v1,https://arxiv.org/pdf/2306.00148v1
ROSARL: Reward-Only Safe Reinforcement Learning,"Geraud Nangue Tasse, Tamlin Love, Mark Nemecek, Steven James, Benjamin Rosman","An important problem in reinforcement learning is designing agents that learn
to solve tasks safely in an environment. A common solution is for a human
expert to define either a penalty in the reward function or a cost to be
minimised when reaching unsafe states. However, this is non-trivial, since too
small a penalty may lead to agents that reach unsafe states, while too large a
penalty increases the time to convergence. Additionally, the difficulty in
designing reward or cost functions can increase with the complexity of the
problem. Hence, for a given environment with a given set of unsafe states, we
are interested in finding the upper bound of rewards at unsafe states whose
optimal policies minimise the probability of reaching those unsafe states,
irrespective of task rewards. We refer to this exact upper bound as the ""Minmax
penalty"", and show that it can be obtained by taking into account both the
controllability and diameter of an environment. We provide a simple practical
model-free algorithm for an agent to learn this Minmax penalty while learning
the task policy, and demonstrate that using it leads to agents that learn safe
policies in high-dimensional continuous control environments.",2306.00035v1,https://arxiv.org/pdf/2306.00035v1
"Safety of autonomous vehicles: A survey on Model-based vs. AI-based
  approaches","Dimia Iberraken, Lounis Adouane","The growing advancements in Autonomous Vehicles (AVs) have emphasized the
critical need to prioritize the absolute safety of AV maneuvers, especially in
dynamic and unpredictable environments or situations. This objective becomes
even more challenging due to the uniqueness of every traffic
situation/condition. To cope with all these very constrained and complex
configurations, AVs must have appropriate control architectures with reliable
and real-time Risk Assessment and Management Strategies (RAMS). These targeted
RAMS must lead to reduce drastically the navigation risks. However, the lack of
safety guarantees proves, which is one of the key challenges to be addressed,
limit drastically the ambition to introduce more broadly AVs on our roads and
restrict the use of AVs to very limited use cases. Therefore, the focus and the
ambition of this paper is to survey research on autonomous vehicles while
focusing on the important topic of safety guarantee of AVs. For this purpose,
it is proposed to review research on relevant methods and concepts defining an
overall control architecture for AVs, with an emphasis on the safety assessment
and decision-making systems composing these architectures. Moreover, it is
intended through this reviewing process to highlight researches that use either
model-based methods or AI-based approaches. This is performed while emphasizing
the strengths and weaknesses of each methodology and investigating the research
that proposes a comprehensive multi-modal design that combines model-based and
AI approaches. This paper ends with discussions on the methods used to
guarantee the safety of AVs namely: safety verification techniques and the
standardization/generalization of safety frameworks.",2305.17941v1,https://arxiv.org/pdf/2305.17941v1
"The Digital Divide in Process Safety: Quantitative Risk Analysis of
  Human-AI Collaboration",He Wen,"Digital technologies have dramatically accelerated the digital transformation
in process industries, boosted new industrial applications, upgraded the
production system, and enhanced operational efficiency. In contrast, the
challenges and gaps between human and artificial intelligence (AI) have become
more and more prominent, whereas the digital divide in process safety is
aggregating. The study attempts to address the following questions: (i)What is
AI in the process safety context? (ii)What is the difference between AI and
humans in process safety? (iii)How do AI and humans collaborate in process
safety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How
to quantify the risk of human-AI collaboration in process safety? Qualitative
risk analysis based on brainstorming and literature review, and quantitative
risk analysis based on layer of protection analysis (LOPA) and Bayesian network
(BN), were applied to explore and model. The importance of human reliability
should be stressed in the digital age, not usually to increase the reliability
of AI, and human-centered AI design in process safety needs to be propagated.",2305.17873v1,https://arxiv.org/pdf/2305.17873v1
"Towards Autonomous and Safe Last-mile Deliveries with AI-augmented
  Self-driving Delivery Robots","Eyad Shaklab, Areg Karapetyan, Arjun Sharma, Murad Mebrahtu, Mustofa Basri, Mohamed Nagy, Majid Khonji, Jorge Dias","In addition to its crucial impact on customer satisfaction, last-mile
delivery (LMD) is notorious for being the most time-consuming and costly stage
of the shipping process. Pressing environmental concerns combined with the
recent surge of e-commerce sales have sparked renewed interest in automation
and electrification of last-mile logistics. To address the hurdles faced by
existing robotic couriers, this paper introduces a customer-centric and
safety-conscious LMD system for small urban communities based on AI-assisted
autonomous delivery robots. The presented framework enables end-to-end
automation and optimization of the logistic process while catering for
real-world imposed operational uncertainties, clients' preferred time
schedules, and safety of pedestrians. To this end, the integrated optimization
component is modeled as a robust variant of the Cumulative Capacitated Vehicle
Routing Problem with Time Windows, where routes are constructed under uncertain
travel times with an objective to minimize the total latency of deliveries
(i.e., the overall waiting time of customers, which can negatively affect their
satisfaction). We demonstrate the proposed LMD system's utility through
real-world trials in a university campus with a single robotic courier.
Implementation aspects as well as the findings and practical insights gained
from the deployment are discussed in detail. Lastly, we round up the
contributions with numerical simulations to investigate the scalability of the
developed mathematical formulation with respect to the number of robotic
vehicles and customers.",2305.17705v1,https://arxiv.org/pdf/2305.17705v1
"Scalable Primal-Dual Actor-Critic Method for Safe Multi-Agent RL with
  General Utilities","Donghao Ying, Yunkai Zhang, Yuhao Ding, Alec Koppel, Javad Lavaei","We investigate safe multi-agent reinforcement learning, where agents seek to
collectively maximize an aggregate sum of local objectives while satisfying
their own safety constraints. The objective and constraints are described by
{\it general utilities}, i.e., nonlinear functions of the long-term
state-action occupancy measure, which encompass broader decision-making goals
such as risk, exploration, or imitations. The exponential growth of the
state-action space size with the number of agents presents challenges for
global observability, further exacerbated by the global coupling arising from
agents' safety constraints. To tackle this issue, we propose a primal-dual
method utilizing shadow reward and $\kappa$-hop neighbor truncation under a
form of correlation decay property, where $\kappa$ is the communication radius.
In the exact setting, our algorithm converges to a first-order stationary point
(FOSP) at the rate of $\mathcal{O}\left(T^{-2/3}\right)$. In the sample-based
setting, we demonstrate that, with high probability, our algorithm requires
$\widetilde{\mathcal{O}}\left(\epsilon^{-3.5}\right)$ samples to achieve an
$\epsilon$-FOSP with an approximation error of $\mathcal{O}(\phi_0^{2\kappa})$,
where $\phi_0\in (0,1)$. Finally, we demonstrate the effectiveness of our model
through extensive numerical experiments.",2305.17568v1,https://arxiv.org/pdf/2305.17568v1
C-MCTS: Safe Planning with Monte Carlo Tree Search,"Dinesh Parthasarathy, Georgios Kontes, Axel Plinge, Christopher Mutschler","The Constrained Markov Decision Process (CMDP) formulation allows to solve
safety-critical decision making tasks that are subject to constraints. While
CMDPs have been extensively studied in the Reinforcement Learning literature,
little attention has been given to sampling-based planning algorithms such as
MCTS for solving them. Previous approaches perform conservatively with respect
to costs as they avoid constraint violations by using Monte Carlo cost
estimates that suffer from high variance. We propose Constrained MCTS (C-MCTS),
which estimates cost using a safety critic that is trained with Temporal
Difference learning in an offline phase prior to agent deployment. The critic
limits exploration by pruning unsafe trajectories within MCTS during
deployment. C-MCTS satisfies cost constraints but operates closer to the
constraint boundary, achieving higher rewards than previous work. As a nice
byproduct, the planner is more efficient w.r.t. planning steps. Most
importantly, under model mismatch between the planner and the real world,
C-MCTS is less susceptible to cost violations than previous work.",2305.16209v3,https://arxiv.org/pdf/2305.16209v3
Learning Safety Constraints from Demonstrations with Unknown Rewards,"David Lindner, Xin Chen, Sebastian Tschiatschek, Katja Hofmann, Andreas Krause","We propose Convex Constraint Learning for Reinforcement Learning (CoCoRL), a
novel approach for inferring shared constraints in a Constrained Markov
Decision Process (CMDP) from a set of safe demonstrations with possibly
different reward functions. While previous work is limited to demonstrations
with known rewards or fully known environment dynamics, CoCoRL can learn
constraints from demonstrations with different unknown rewards without
knowledge of the environment dynamics. CoCoRL constructs a convex safe set
based on demonstrations, which provably guarantees safety even for potentially
sub-optimal (but safe) demonstrations. For near-optimal demonstrations, CoCoRL
converges to the true safe set with no policy regret. We evaluate CoCoRL in
gridworld environments and a driving simulation with multiple constraints.
CoCoRL learns constraints that lead to safe driving behavior. Importantly, we
can safely transfer the learned constraints to different tasks and
environments. In contrast, alternative methods based on Inverse Reinforcement
Learning (IRL) often exhibit poor performance and learn unsafe policies.",2305.16147v2,https://arxiv.org/pdf/2305.16147v2
"Control invariant set enhanced safe reinforcement learning: improved
  sampling efficiency, guaranteed stability and robustness","Song Bo, Bernard T. Agyeman, Xunyuan Yin, Jinfeng Liu","Reinforcement learning (RL) is an area of significant research interest, and
safe RL in particular is attracting attention due to its ability to handle
safety-driven constraints that are crucial for real-world applications. This
work proposes a novel approach to RL training, called control invariant set
(CIS) enhanced RL, which leverages the advantages of utilizing the explicit
form of CIS to improve stability guarantees and sampling efficiency.
Furthermore, the robustness of the proposed approach is investigated in the
presence of uncertainty. The approach consists of two learning stages: offline
and online. In the offline stage, CIS is incorporated into the reward design,
initial state sampling, and state reset procedures. This incorporation of CIS
facilitates improved sampling efficiency during the offline training process.
In the online stage, RL is retrained whenever the predicted next step state is
outside of the CIS, which serves as a stability criterion, by introducing a
Safety Supervisor to examine the safety of the action and make necessary
corrections. The stability analysis is conducted for both cases, with and
without uncertainty. To evaluate the proposed approach, we apply it to a
simulated chemical reactor. The results show a significant improvement in
sampling efficiency during offline training and closed-loop stability guarantee
in the online implementation, with and without uncertainty.",2305.15602v1,https://arxiv.org/pdf/2305.15602v1
GUARD: A Safe Reinforcement Learning Benchmark,"Weiye Zhao, Rui Chen, Yifan Sun, Ruixuan Liu, Tianhao Wei, Changliu Liu","Due to the trial-and-error nature, it is typically challenging to apply RL
algorithms to safety-critical real-world applications, such as autonomous
driving, human-robot interaction, robot manipulation, etc, where such errors
are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly
in the literature, in which the agents explore the environment while satisfying
constraints. Due to the diversity of algorithms and tasks, it remains difficult
to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a
Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD
has several advantages compared to existing benchmarks. First, GUARD is a
generalized benchmark with a wide variety of RL agents, tasks, and safety
constraint specifications. Second, GUARD comprehensively covers
state-of-the-art safe RL algorithms with self-contained implementations. Third,
GUARD is highly customizable in tasks and algorithms. We present a comparison
of state-of-the-art safe RL algorithms in various task settings using GUARD and
establish baselines that future work can build on.",2305.13681v3,https://arxiv.org/pdf/2305.13681v3
"Risk-aware Safe Control for Decentralized Multi-agent Systems via
  Dynamic Responsibility Allocation","Yiwei Lyu, Wenhao Luo, John M. Dolan","Decentralized control schemes are increasingly favored in various domains
that involve multi-agent systems due to the need for computational efficiency
as well as general applicability to large-scale systems. However, in the
absence of an explicit global coordinator, it is hard for distributed agents to
determine how to efficiently interact with others. In this paper, we present a
risk-aware decentralized control framework that provides guidance on how much
relative responsibility share (a percentage) an individual agent should take to
avoid collisions with others while moving efficiently without direct
communications. We propose a novel Control Barrier Function (CBF)-inspired risk
measurement to characterize the aggregate risk agents face from potential
collisions under motion uncertainty. We use this measurement to allocate
responsibility shares among agents dynamically and develop risk-aware
decentralized safe controllers. In this way, we are able to leverage the
flexibility of robots with lower risk to improve the motion flexibility for
those with higher risk, thus achieving improved collective safety. We
demonstrate the validity and efficiency of our proposed approach through two
examples: ramp merging in autonomous driving and a multi-agent
position-swapping game.",2305.13467v1,https://arxiv.org/pdf/2305.13467v1
"Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety
  of Text-to-Image Models","Alicia Parrish, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, Rafael Mosquera, Addison Howard, Will Cukierski, D. Sculley, Vijay Janapa Reddi, Lora Aroyo","The generative AI revolution in recent years has been spurred by an expansion
in compute power and data quantity, which together enable extensive
pre-training of powerful text-to-image (T2I) models. With their greater
capabilities to generate realistic and creative content, these T2I models like
DALL-E, MidJourney, Imagen or Stable Diffusion are reaching ever wider
audiences. Any unsafe behaviors inherited from pretraining on uncurated
internet-scraped datasets thus have the potential to cause wide-reaching harm,
for example, through generated images which are violent, sexually explicit, or
contain biased and derogatory stereotypes. Despite this risk of harm, we lack
systematic and structured evaluation datasets to scrutinize model behavior,
especially adversarial attacks that bypass existing safety filters. A typical
bottleneck in safety evaluation is achieving a wide coverage of different types
of challenging examples in the evaluation set, i.e., identifying 'unknown
unknowns' or long-tail problems. To address this need, we introduce the
Adversarial Nibbler challenge. The goal of this challenge is to crowdsource a
diverse set of failure modes and reward challenge participants for successfully
finding safety vulnerabilities in current state-of-the-art T2I models.
Ultimately, we aim to provide greater awareness of these issues and assist
developers in improving the future safety and reliability of generative AI
models. Adversarial Nibbler is a data-centric challenge, part of the DataPerf
challenge suite, organized and supported by Kaggle and MLCommons.",2305.14384v1,https://arxiv.org/pdf/2305.14384v1
Safely Learning Dynamical Systems,"Amir Ali Ahmadi, Abraar Chaudhry, Vikas Sindhwani, Stephen Tu","A fundamental challenge in learning an unknown dynamical system is to reduce
model uncertainty by making measurements while maintaining safety. We formulate
a mathematical definition of what it means to safely learn a dynamical system
by sequentially deciding where to initialize trajectories. The state of the
system must stay within a safety region for a horizon of $T$ time steps under
the action of all dynamical systems that (i) belong to a given initial
uncertainty set, and (ii) are consistent with information gathered so far.
  First, we consider safely learning a linear dynamical system involving $n$
states. For the case $T=1$, we present an LP-based algorithm that either safely
recovers the true dynamics from at most $n$ trajectories, or certifies that
safe learning is impossible. For $T=2$, we give an SDP representation of the
set of safe initial conditions and show that $\lceil n/2 \rceil$ trajectories
generically suffice for safe learning. For $T = \infty$, we provide
SDP-representable inner approximations of the set of safe initial conditions
and show that one trajectory generically suffices for safe learning. We extend
a number of our results to the cases where the initial uncertainty set contains
sparse, low-rank, or permutation matrices, or when the system has a control
input.
  Second, we consider safely learning a general class of nonlinear dynamical
systems. For the case $T=1$, we give an SOCP-based representation of the set of
safe initial conditions. For $T=\infty$, we provide semidefinite representable
inner approximations to the set of safe initial conditions. We show how one can
safely collect trajectories and fit a polynomial model of the nonlinear
dynamics that is consistent with the initial uncertainty set and best agrees
with the observations. We also present some extensions to cases where the
measurements are noisy or the dynamical system involves disturbances.",2305.12284v2,https://arxiv.org/pdf/2305.12284v2
"A Survey of Safety and Trustworthiness of Large Language Models through
  the Lens of Verification and Validation","Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao Zhang, Sihao Wu, Peipei Xu, Dengyu Wu, Andre Freitas, Mustafa A. Mustafa","Large Language Models (LLMs) have exploded a new heatwave of AI for their
ability to engage end-users in human-level conversations with detailed and
articulate answers across many knowledge domains. In response to their fast
adoption in many industrial applications, this survey concerns their safety and
trustworthiness. First, we review known vulnerabilities and limitations of the
LLMs, categorising them into inherent issues, attacks, and unintended bugs.
Then, we consider if and how the Verification and Validation (V&V) techniques,
which have been widely developed for traditional software and deep learning
models such as convolutional neural networks as independent processes to check
the alignment of their implementations against the specifications, can be
integrated and further extended throughout the lifecycle of the LLMs to provide
rigorous analysis to the safety and trustworthiness of LLMs and their
applications. Specifically, we consider four complementary techniques:
falsification and evaluation, verification, runtime monitoring, and regulations
and ethical use. In total, 370+ references are considered to support the quick
understanding of the safety and trustworthiness issues from the perspective of
V&V. While intensive research has been conducted to identify the safety and
trustworthiness issues, rigorous yet practical methods are called for to ensure
the alignment of LLMs with safety and trustworthiness requirements.",2305.11391v2,https://arxiv.org/pdf/2305.11391v2
"Scalable and Safe Remediation of Defective Actions in Self-Learning
  Conversational Systems","Sarthak Ahuja, Mohammad Kachuee, Fateme Sheikholeslami, Weiqing Liu, Jaeyoung Do","Off-Policy reinforcement learning has been a driving force for the
state-of-the-art conversational AIs leading to more natural humanagent
interactions and improving the user satisfaction for goal-oriented agents.
However, in large-scale commercial settings, it is often challenging to balance
between policy improvements and experience continuity on the broad spectrum of
applications handled by such system. In the literature, off-policy evaluation
and guard-railing on aggregate statistics has been commonly used to address
this problem. In this paper, we propose a method for curating and leveraging
high-precision samples sourced from historical regression incident reports to
validate, safe-guard, and improve policies prior to the online deployment. We
conducted extensive experiments using data from a real-world conversational
system and actual regression incidents. The proposed method is currently
deployed in our production system to protect customers against broken
experiences and enable long-term policy improvements.",2305.10528v1,https://arxiv.org/pdf/2305.10528v1
"Reinforcement Learning for Safe Robot Control using Control Lyapunov
  Barrier Functions","Desong Du, Shaohang Han, Naiming Qi, Haitham Bou Ammar, Jun Wang, Wei Pan","Reinforcement learning (RL) exhibits impressive performance when managing
complicated control tasks for robots. However, its wide application to physical
robots is limited by the absence of strong safety guarantees. To overcome this
challenge, this paper explores the control Lyapunov barrier function (CLBF) to
analyze the safety and reachability solely based on data without explicitly
employing a dynamic model. We also proposed the Lyapunov barrier actor-critic
(LBAC), a model-free RL algorithm, to search for a controller that satisfies
the data-based approximation of the safety and reachability conditions. The
proposed approach is demonstrated through simulation and real-world robot
control experiments, i.e., a 2D quadrotor navigation task. The experimental
findings reveal this approach's effectiveness in reachability and safety,
surpassing other model-free RL methods.",2305.09793v1,https://arxiv.org/pdf/2305.09793v1
"OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning
  Research","Jiaming Ji, Jiayi Zhou, Borong Zhang, Juntao Dai, Xuehai Pan, Ruiyang Sun, Weidong Huang, Yiran Geng, Mickel Liu, Yaodong Yang","AI systems empowered by reinforcement learning (RL) algorithms harbor the
immense potential to catalyze societal advancement, yet their deployment is
often impeded by significant safety concerns. Particularly in safety-critical
applications, researchers have raised concerns about unintended harms or unsafe
behaviors of unaligned RL agents. The philosophy of safe reinforcement learning
(SafeRL) is to align RL agents with harmless intentions and safe behavioral
patterns. In SafeRL, agents learn to develop optimal policies by receiving
feedback from the environment, while also fulfilling the requirement of
minimizing the risk of unintended harm or unsafe behavior. However, due to the
intricate nature of SafeRL algorithm implementation, combining methodologies
across various domains presents a formidable challenge. This had led to an
absence of a cohesive and efficacious learning framework within the
contemporary SafeRL research milieu. In this work, we introduce a foundational
framework designed to expedite SafeRL research endeavors. Our comprehensive
framework encompasses an array of algorithms spanning different RL domains and
places heavy emphasis on safety elements. Our efforts are to make the
SafeRL-related research process more streamlined and efficient, therefore
facilitating further research in AI safety. Our project is released at:
https://github.com/PKU-Alignment/omnisafe.",2305.09304v1,https://arxiv.org/pdf/2305.09304v1
Beyond the Safeguards: Exploring the Security Risks of ChatGPT,"Erik Derner, Kristina Batistič","The increasing popularity of large language models (LLMs) such as ChatGPT has
led to growing concerns about their safety, security risks, and ethical
implications. This paper aims to provide an overview of the different types of
security risks associated with ChatGPT, including malicious text and code
generation, private data disclosure, fraudulent services, information
gathering, and producing unethical content. We present an empirical study
examining the effectiveness of ChatGPT's content filters and explore potential
ways to bypass these safeguards, demonstrating the ethical implications and
security risks that persist in LLMs even when protections are in place. Based
on a qualitative analysis of the security implications, we discuss potential
strategies to mitigate these risks and inform researchers, policymakers, and
industry professionals about the complex security challenges posed by LLMs like
ChatGPT. This study contributes to the ongoing discussion on the ethical and
security implications of LLMs, underscoring the need for continued research in
this area.",2305.08005v1,https://arxiv.org/pdf/2305.08005v1
"More for Less: Safe Policy Improvement With Stronger Performance
  Guarantees","Patrick Wienhöft, Marnix Suilen, Thiago D. Simão, Clemens Dubslaff, Christel Baier, Nils Jansen","In an offline reinforcement learning setting, the safe policy improvement
(SPI) problem aims to improve the performance of a behavior policy according to
which sample data has been generated. State-of-the-art approaches to SPI
require a high number of samples to provide practical probabilistic guarantees
on the improved policy's performance. We present a novel approach to the SPI
problem that provides the means to require less data for such guarantees.
Specifically, to prove the correctness of these guarantees, we devise implicit
transformations on the data set and the underlying environment model that serve
as theoretical foundations to derive tighter improvement bounds for SPI. Our
empirical evaluation, using the well-established SPI with baseline
bootstrapping (SPIBB) algorithm, on standard benchmarks shows that our method
indeed significantly reduces the sample complexity of the SPIBB algorithm.",2305.07958v1,https://arxiv.org/pdf/2305.07958v1
Safe motion planning with environment uncertainty,"Antony Thomas, Fulvio Mastrogiovanni, Marco Baglietto","We present an approach for safe motion planning under robot state and
environment (obstacle and landmark location) uncertainties. To this end, we
first develop an approach that accounts for the landmark uncertainties during
robot localization. Existing planning approaches assume that the landmark
locations are well known or are known with little uncertainty. However, this
might not be true in practice. Noisy sensors and imperfect motions compound to
the errors originating from the estimate of environment features. Moreover,
possible occlusions and dynamic objects in the environment render imperfect
landmark estimation. Consequently, not considering this uncertainty can wrongly
localize the robot, leading to inefficient plans. Our approach thus
incorporates the landmark uncertainty within the Bayes filter estimation
framework. We also analyze the effect of considering this uncertainty and
delineate the conditions under which it can be ignored. Second, we extend the
state-of-the-art by computing an exact expression for the collision probability
under Gaussian distributed robot motion, perception and obstacle location
uncertainties. We formulate the collision probability process as a quadratic
form in random variables. Under Gaussian distribution assumptions, an exact
expression for collision probability is thus obtained which is computable in
real-time. In contrast, existing approaches approximate the collision
probability using upper-bounds that can lead to overly conservative estimate
and thereby suboptimal plans. We demonstrate and evaluate our approach using a
theoretical example and simulations. We also present a comparison of our
approach to different state-of-the-art methods.",2305.06004v1,https://arxiv.org/pdf/2305.06004v1
Safe Deep RL for Intraoperative Planning of Pedicle Screw Placement,"Yunke Ao, Hooman Esfandiari, Fabio Carrillo, Yarden As, Mazda Farshad, Benjamin F. Grewe, Andreas Krause, Philipp Fuernstahl","Spinal fusion surgery requires highly accurate implantation of pedicle screw
implants, which must be conducted in critical proximity to vital structures
with a limited view of anatomy. Robotic surgery systems have been proposed to
improve placement accuracy, however, state-of-the-art systems suffer from the
limitations of open-loop approaches, as they follow traditional concepts of
preoperative planning and intraoperative registration, without real-time
recalculation of the surgical plan. In this paper, we propose an intraoperative
planning approach for robotic spine surgery that leverages real-time
observation for drill path planning based on Safe Deep Reinforcement Learning
(DRL). The main contributions of our method are (1) the capability to guarantee
safe actions by introducing an uncertainty-aware distance-based safety filter;
and (2) the ability to compensate for incomplete intraoperative anatomical
information, by encoding a-priori knowledge about anatomical structures with a
network pre-trained on high-fidelity anatomical models. Planning quality was
assessed by quantitative comparison with the gold standard (GS) drill planning.
In experiments with 5 models derived from real magnetic resonance imaging (MRI)
data, our approach was capable of achieving 90% bone penetration with respect
to the GS while satisfying safety requirements, even under observation and
motion uncertainty. To the best of our knowledge, our approach is the first
safe DRL approach focusing on orthopedic surgeries.",2305.05354v2,https://arxiv.org/pdf/2305.05354v2
"Enhancing Road Safety through Accurate Detection of Hazardous Driving
  Behaviors with Graph Convolutional Recurrent Networks","Pooyan Khosravinia, Thinagaran Perumal, Javad Zarrin","Car accidents remain a significant public safety issue worldwide, with the
majority of them attributed to driver errors stemming from inadequate driving
knowledge, non-compliance with regulations, and poor driving habits. To improve
road safety, Driving Behavior Detection (DBD) systems have been proposed in
several studies to identify safe and unsafe driving behavior. Many of these
studies have utilized sensor data obtained from the Controller Area Network
(CAN) bus to construct their models. However, the use of publicly available
sensors is known to reduce the accuracy of detection models, while
incorporating vendor-specific sensors into the dataset increases accuracy. To
address the limitations of existing approaches, we present a reliable DBD
system based on Graph Convolutional Long Short-Term Memory Networks (GConvLSTM)
that enhances the precision and practicality of DBD models using public
sensors. Additionally, we incorporate non-public sensors to evaluate the
model's effectiveness. Our proposed model achieved a high accuracy of 97.5\%
for public sensors and an average accuracy of 98.1\% for non-public sensors,
indicating its consistency and accuracy in both settings. To enable local
driver behavior analysis, we deployed our DBD system on a Raspberry Pi at the
network edge, with drivers able to access daily driving condition reports,
sensor data, and prediction results through a monitoring dashboard.
Furthermore, the dashboard issues voice warnings to alert drivers of hazardous
driving conditions. Our findings demonstrate that the proposed system can
effectively detect hazardous and unsafe driving behavior, with potential
applications in improving road safety and reducing the number of accidents
caused by driver errors.",2305.05670v1,https://arxiv.org/pdf/2305.05670v1
"DEFENDER: DTW-Based Episode Filtering Using Demonstrations for Enhancing
  RL Safety","André Correia, Luís Alexandre","Deploying reinforcement learning agents in the real world can be challenging
due to the risks associated with learning through trial and error. We propose a
task-agnostic method that leverages small sets of safe and unsafe
demonstrations to improve the safety of RL agents during learning. The method
compares the current trajectory of the agent with both sets of demonstrations
at every step, and filters the trajectory if it resembles the unsafe
demonstrations. We perform ablation studies on different filtering strategies
and investigate the impact of the number of demonstrations on performance. Our
method is compatible with any stand-alone RL algorithm and can be applied to
any task. We evaluate our method on three tasks from OpenAI Gym's Mujoco
benchmark and two state-of-the-art RL algorithms. The results demonstrate that
our method significantly reduces the crash rate of the agent while converging
to, and in most cases even improving, the performance of the stand-alone agent.",2305.04727v1,https://arxiv.org/pdf/2305.04727v1
"Pedestrian Behavior Maps for Safety Advisories: CHAMP Framework and
  Real-World Data Analysis","Ross Greer, Samveed Desai, Lulua Rakla, Akshay Gopalkrishnan, Afnan Alofi, Mohan Trivedi","It is critical for vehicles to prevent any collisions with pedestrians.
Current methods for pedestrian collision prevention focus on integrating visual
pedestrian detectors with Automatic Emergency Braking (AEB) systems which can
trigger warnings and apply brakes as a pedestrian enters a vehicle's path.
Unfortunately, pedestrian-detection-based systems can be hindered in certain
situations such as night-time or when pedestrians are occluded. Our system
addresses such issues using an online, map-based pedestrian detection
aggregation system where common pedestrian locations are learned after repeated
passes of locations. Using a carefully collected and annotated dataset in La
Jolla, CA, we demonstrate the system's ability to learn pedestrian zones and
generate advisory notices when a vehicle is approaching a pedestrian despite
challenges like dark lighting or pedestrian occlusion. Using the number of
correct advisories, false advisories, and missed advisories to define precision
and recall performance metrics, we evaluate our system and discuss future
positive effects with further data collection. We have made our code available
at https://github.com/s7desai/ped-mapping, and a video demonstration of the
CHAMP system at https://youtu.be/dxeCrS_Gpkw.",2305.04506v1,https://arxiv.org/pdf/2305.04506v1
"Bayesian Safety Validation for Failure Probability Estimation of
  Black-Box Systems","Robert J. Moss, Mykel J. Kochenderfer, Maxime Gariel, Arthur Dubois","Estimating the probability of failure is an important step in the
certification of safety-critical systems. Efficient estimation methods are
often needed due to the challenges posed by high-dimensional input spaces,
risky test scenarios, and computationally expensive simulators. This work
frames the problem of black-box safety validation as a Bayesian optimization
problem and introduces a method that iteratively fits a probabilistic surrogate
model to efficiently predict failures. The algorithm is designed to search for
failures, compute the most-likely failure, and estimate the failure probability
over an operating domain using importance sampling. We introduce three
acquisition functions that aim to reduce uncertainty by covering the design
space, optimize the analytically derived failure boundaries, and sample the
predicted failure regions. Results show this Bayesian safety validation
approach provides a more accurate estimate of failure probability with orders
of magnitude fewer samples and performs well across various safety validation
metrics. We demonstrate this approach on three test problems, a stochastic
decision making system, and a neural network-based runway detection system.
This work is open sourced (https://github.com/sisl/BayesianSafetyValidation.jl)
and currently being used to supplement the FAA certification process of the
machine learning components for an autonomous cargo aircraft.",2305.02449v2,https://arxiv.org/pdf/2305.02449v2
"VSRQ: Quantitative Assessment Method for Safety Risk of Vehicle
  Intelligent Connected System","Tian Zhang, Wenshan Guan, Hao Miao, Xiujie Huang, Zhiquan Liu, Chaonan Wang, Quanlong Guan, Liangda Fang, Zhifei Duan","The field of intelligent connected in modern vehicles continues to expand,
and the functions of vehicles become more and more complex with the development
of the times. This has also led to an increasing number of vehicle
vulnerabilities and many safety issues. Therefore, it is particularly important
to identify high-risk vehicle intelligent connected systems, because it can
inform security personnel which systems are most vulnerable to attacks,
allowing them to conduct more thorough inspections and tests. In this paper, we
develop a new model for vehicle risk assessment by combining I-FAHP with FCA
clustering: VSRQ model. We extract important indicators related to vehicle
safety, use fuzzy cluster analys (FCA) combined with fuzzy analytic hierarchy
process (FAHP) to mine the vulnerable components of the vehicle intelligent
connected system, and conduct priority testing on vulnerable components to
reduce risks and ensure vehicle safety. We evaluate the model on OpenPilot and
experimentally demonstrate the effectiveness of the VSRQ model in identifying
the safety of vehicle intelligent connected systems. The experiment fully
complies with ISO 26262 and ISO/SAE 21434 standards, and our model has a higher
accuracy rate than other models. These results provide a promising new research
direction for predicting the security risks of vehicle intelligent connected
systems and provide typical application tasks for VSRQ. The experimental
results show that the accuracy rate is 94.36%, and the recall rate is 73.43%,
which is at least 14.63% higher than all other known indicators.",2305.01898v1,https://arxiv.org/pdf/2305.01898v1
Exploration of Unranked Items in Safe Online Learning to Re-Rank,"Hiroaki Shiino, Kaito Ariu, Kenshi Abe, Togashi Riku","Bandit algorithms for online learning to rank (OLTR) problems often aim to
maximize long-term revenue by utilizing user feedback. From a practical point
of view, however, such algorithms have a high risk of hurting user experience
due to their aggressive exploration. Thus, there has been a rising demand for
safe exploration in recent years. One approach to safe exploration is to
gradually enhance the quality of an original ranking that is already guaranteed
acceptable quality. In this paper, we propose a safe OLTR algorithm that
efficiently exchanges one of the items in the current ranking with an item
outside the ranking (i.e., an unranked item) to perform exploration. We select
an unranked item optimistically to explore based on Kullback-Leibler upper
confidence bounds (KL-UCB) and safely re-rank the items including the selected
one. Through experiments, we demonstrate that the proposed algorithm improves
long-term regret from baselines without any safety violation.",2305.01202v1,https://arxiv.org/pdf/2305.01202v1
"SafeWebUH at SemEval-2023 Task 11: Learning Annotator Disagreement in
  Derogatory Text: Comparison of Direct Training vs Aggregation","Sadat Shahriar, Thamar Solorio","Subjectivity and difference of opinion are key social phenomena, and it is
crucial to take these into account in the annotation and detection process of
derogatory textual content. In this paper, we use four datasets provided by
SemEval-2023 Task 11 and fine-tune a BERT model to capture the disagreement in
the annotation. We find individual annotator modeling and aggregation lowers
the Cross-Entropy score by an average of 0.21, compared to the direct training
on the soft labels. Our findings further demonstrate that annotator metadata
contributes to the average 0.029 reduction in the Cross-Entropy score.",2305.01050v1,https://arxiv.org/pdf/2305.01050v1
"The Impact of the Geometric Properties of the Constraint Set in Safe
  Optimization with Bandit Feedback","Spencer Hutchinson, Berkay Turan, Mahnoosh Alizadeh","We consider a safe optimization problem with bandit feedback in which an
agent sequentially chooses actions and observes responses from the environment,
with the goal of maximizing an arbitrary function of the response while
respecting stage-wise constraints. We propose an algorithm for this problem,
and study how the geometric properties of the constraint set impact the regret
of the algorithm. In order to do so, we introduce the notion of the sharpness
of a particular constraint set, which characterizes the difficulty of
performing learning within the constraint set in an uncertain setting. This
concept of sharpness allows us to identify the class of constraint sets for
which the proposed algorithm is guaranteed to enjoy sublinear regret.
Simulation results for this algorithm support the sublinear regret bound and
provide empirical evidence that the sharpness of the constraint set impacts the
performance of the algorithm.",2305.00889v1,https://arxiv.org/pdf/2305.00889v1
"Joint Learning of Policy with Unknown Temporal Constraints for Safe
  Reinforcement Learning","Lunet Yifru, Ali Baheri","In many real-world applications, safety constraints for reinforcement
learning (RL) algorithms are either unknown or not explicitly defined. We
propose a framework that concurrently learns safety constraints and optimal RL
policies in such environments, supported by theoretical guarantees. Our
approach merges a logically-constrained RL algorithm with an evolutionary
algorithm to synthesize signal temporal logic (STL) specifications. The
framework is underpinned by theorems that establish the convergence of our
joint learning process and provide error bounds between the discovered policy
and the true optimal policy. We showcased our framework in grid-world
environments, successfully identifying both acceptable safety constraints and
RL policies while demonstrating the effectiveness of our theorems in practice.",2305.00576v1,https://arxiv.org/pdf/2305.00576v1
"AI-based Predictive Analytic Approaches for safeguarding the Future of
  Electric/Hybrid Vehicles",Ishan Shivansh Bangroo,"In response to the global need for sustainable energy, green technology may
help fight climate change. Before green infrastructure to be easily integrated
into the world's energy system, it needs upgrading. By improving energy
infrastructure and decision-making, artificial intelligence (AI) may help solve
this challenge. EHVs have grown in popularity because to concerns about global
warming and the need for more ecologically friendly transportation. EHVs may
work better with cutting-edge technologies like AI. Electric vehicles (EVs)
reduce greenhouse gas emissions and promote sustainable mobility. Electric
automobiles (EVs) are growing in popularity due to their benefits for climate
change mitigation and sustainable mobility. Unfortunately, EV production
consumes a lot of energy and materials, which may harm nature. EV production is
being improved using green technologies like artificial intelligence and
predictive analysis. Electric and hybrid vehicles (EHVs) may help meet the need
for ecologically friendly transportation. However, the Battery Management
System (BMS) controls EHV performance and longevity. AI may improve EHV energy
efficiency, emissions reduction, and sustainability. Remote hijacking, security
breaches, and unauthorized access are EHV cybersecurity vulnerabilities
addressed in the article. AI research and development may help make
transportation more sustainable, as may optimizing EHVs and charging
infrastructure.",2304.13841v1,https://arxiv.org/pdf/2304.13841v1
"Safe Deployment for Counterfactual Learning to Rank with Exposure-Based
  Risk Minimization","Shashank Gupta, Harrie Oosterhuis, Maarten de Rijke","Counterfactual learning to rank (CLTR) relies on exposure-based inverse
propensity scoring (IPS), a LTR-specific adaptation of IPS to correct for
position bias. While IPS can provide unbiased and consistent estimates, it
often suffers from high variance. Especially when little click data is
available, this variance can cause CLTR to learn sub-optimal ranking behavior.
Consequently, existing CLTR methods bring significant risks with them, as
naively deploying their models can result in very negative user experiences. We
introduce a novel risk-aware CLTR method with theoretical guarantees for safe
deployment. We apply a novel exposure-based concept of risk regularization to
IPS estimation for LTR. Our risk regularization penalizes the mismatch between
the ranking behavior of a learned model and a given safe model. Thereby, it
ensures that learned ranking models stay close to a trusted model, when there
is high uncertainty in IPS estimation, which greatly reduces the risks during
deployment. Our experimental results demonstrate the efficacy of our proposed
method, which is effective at avoiding initial periods of bad performance when
little data is available, while also maintaining high performance at
convergence. For the CLTR field, our novel exposure-based risk minimization
method enables practitioners to adopt CLTR methods in a safer manner that
mitigates many of the risks attached to previous methods.",2305.01522v1,https://arxiv.org/pdf/2305.01522v1
"Towards Explainable and Safe Conversational Agents for Mental Health: A
  Survey","Surjodeep Sarkar, Manas Gaur, L. Chen, Muskan Garg, Biplav Srivastava, Bhaktee Dongaonkar","Virtual Mental Health Assistants (VMHAs) are seeing continual advancements to
support the overburdened global healthcare system that gets 60 million primary
care visits, and 6 million Emergency Room (ER) visits annually. These systems
are built by clinical psychologists, psychiatrists, and Artificial Intelligence
(AI) researchers for Cognitive Behavioral Therapy (CBT). At present, the role
of VMHAs is to provide emotional support through information, focusing less on
developing a reflective conversation with the patient. A more comprehensive,
safe and explainable approach is required to build responsible VMHAs to ask
follow-up questions or provide a well-informed response. This survey offers a
systematic critical review of the existing conversational agents in mental
health, followed by new insights into the improvements of VMHAs with contextual
knowledge, datasets, and their emerging role in clinical decision support. We
also provide new directions toward enriching the user experience of VMHAs with
explainability, safety, and wholesome trustworthiness. Finally, we provide
evaluation metrics and practical considerations for VMHAs beyond the current
literature to build trust between VMHAs and patients in active communications.",2304.13191v1,https://arxiv.org/pdf/2304.13191v1
SAFE: Machine Unlearning With Shard Graphs,"Yonatan Dukler, Benjamin Bowman, Alessandro Achille, Aditya Golatkar, Ashwin Swaminathan, Stefano Soatto","We present Synergy Aware Forgetting Ensemble (SAFE), a method to adapt large
models on a diverse collection of data while minimizing the expected cost to
remove the influence of training samples from the trained model. This process,
also known as selective forgetting or unlearning, is often conducted by
partitioning a dataset into shards, training fully independent models on each,
then ensembling the resulting models. Increasing the number of shards reduces
the expected cost to forget but at the same time it increases inference cost
and reduces the final accuracy of the model since synergistic information
between samples is lost during the independent model training. Rather than
treating each shard as independent, SAFE introduces the notion of a shard
graph, which allows incorporating limited information from other shards during
training, trading off a modest increase in expected forgetting cost with a
significant increase in accuracy, all while still attaining complete removal of
residual influence after forgetting. SAFE uses a lightweight system of adapters
which can be trained while reusing most of the computations. This allows SAFE
to be trained on shards an order-of-magnitude smaller than current
state-of-the-art methods (thus reducing the forgetting costs) while also
maintaining high accuracy, as we demonstrate empirically on fine-grained
computer vision datasets.",2304.13169v2,https://arxiv.org/pdf/2304.13169v2
"Real-time Safety Assessment of Dynamic Systems in Non-stationary
  Environments: A Review of Methods and Techniques","Zeyi Liu, Songqiao Hu, Xiao He","Real-time safety assessment (RTSA) of dynamic systems is a critical task that
has significant implications for various fields such as industrial and
transportation applications, especially in non-stationary environments.
However, the absence of a comprehensive review of real-time safety assessment
methods in non-stationary environments impedes the progress and refinement of
related methods. In this paper, a review of methods and techniques for RTSA
tasks in non-stationary environments is provided. Specifically, the background
and significance of RTSA approaches in non-stationary environments are firstly
highlighted. We then present a problem description that covers the definition,
classification, and main challenges. We review recent developments in related
technologies such as online active learning, online semi-supervised learning,
online transfer learning, and online anomaly detection. Finally, we discuss
future outlooks and potential directions for further research. Our review aims
to provide a comprehensive and up-to-date overview of real-time safety
assessment methods in non-stationary environments, which can serve as a
valuable resource for researchers and practitioners in this field.",2304.12583v2,https://arxiv.org/pdf/2304.12583v2
System III: Learning with Domain Knowledge for Safety Constraints,"Fazl Barez, Hosien Hasanbieg, Alesandro Abbate","Reinforcement learning agents naturally learn from extensive exploration.
Exploration is costly and can be unsafe in $\textit{safety-critical}$ domains.
This paper proposes a novel framework for incorporating domain knowledge to
help guide safe exploration and boost sample efficiency. Previous approaches
impose constraints, such as regularisation parameters in neural networks, that
rely on large sample sets and often are not suitable for safety-critical
domains where agents should almost always avoid unsafe actions. In our
approach, called $\textit{System III}$, which is inspired by psychologists'
notions of the brain's $\textit{System I}$ and $\textit{System II}$, we
represent domain expert knowledge of safety in form of first-order logic. We
evaluate the satisfaction of these constraints via p-norms in state vector
space. In our formulation, constraints are analogous to hazards, objects, and
regions of state that have to be avoided during exploration. We evaluated the
effectiveness of the proposed method on OpenAI's Gym and Safety-Gym
environments. In all tasks, including classic Control and Safety Games, we show
that our approach results in safer exploration and sample efficiency.",2304.11593v1,https://arxiv.org/pdf/2304.11593v1
Approximate Shielding of Atari Agents for Safe Exploration,"Alexander W. Goodall, Francesco Belardinelli","Balancing exploration and conservatism in the constrained setting is an
important problem if we are to use reinforcement learning for meaningful tasks
in the real world. In this paper, we propose a principled algorithm for safe
exploration based on the concept of shielding. Previous approaches to shielding
assume access to a safety-relevant abstraction of the environment or a
high-fidelity simulator. Instead, our work is based on latent shielding -
another approach that leverages world models to verify policy roll-outs in the
latent space of a learned dynamics model. Our novel algorithm builds on this
previous work, using safety critics and other additional features to improve
the stability and farsightedness of the algorithm. We demonstrate the
effectiveness of our approach by running experiments on a small set of Atari
games with state dependent safety labels. We present preliminary results that
show our approximate shielding algorithm effectively reduces the rate of safety
violations, and in some cases improves the speed of convergence and quality of
the final agent.",2304.11104v1,https://arxiv.org/pdf/2304.11104v1
"Approximate non-linear model predictive control with safety-augmented
  neural networks","Henrik Hose, Johannes Köhler, Melanie N. Zeilinger, Sebastian Trimpe","Model predictive control (MPC) achieves stability and constraint satisfaction
for general nonlinear systems, but requires computationally expensive online
optimization. This paper studies approximations of such MPC controllers via
neural networks (NNs) to achieve fast online evaluation. We propose safety
augmentation that yields deterministic guarantees for convergence and
constraint satisfaction despite approximation inaccuracies. We approximate the
entire input sequence of the MPC with NNs, which allows us to verify online if
it is a feasible solution to the MPC problem. We replace the NN solution by a
safe candidate based on standard MPC techniques whenever it is infeasible or
has worse cost. Our method requires a single evaluation of the NN and forward
integration of the input sequence online, which is fast to compute on
resource-constrained systems. The proposed control framework is illustrated on
three non-linear MPC benchmarks of different complexity, demonstrating
computational speedups orders of magnitudes higher than online optimization. In
the examples, we achieve deterministic safety through the safety-augmented NNs,
where naive NN implementation fails.",2304.09575v1,https://arxiv.org/pdf/2304.09575v1
Optimizing Carbon Storage Operations for Long-Term Safety,"Yizheng Wang, Markus Zechner, Gege Wen, Anthony Louis Corso, John Michael Mern, Mykel J. Kochenderfer, Jef Karel Caers","To combat global warming and mitigate the risks associated with climate
change, carbon capture and storage (CCS) has emerged as a crucial technology.
However, safely sequestering CO2 in geological formations for long-term storage
presents several challenges. In this study, we address these issues by modeling
the decision-making process for carbon storage operations as a partially
observable Markov decision process (POMDP). We solve the POMDP using belief
state planning to optimize injector and monitoring well locations, with the
goal of maximizing stored CO2 while maintaining safety. Empirical results in
simulation demonstrate that our approach is effective in ensuring safe
long-term carbon storage operations. We showcase the flexibility of our
approach by introducing three different monitoring strategies and examining
their impact on decision quality. Additionally, we introduce a neural network
surrogate model for the POMDP decision-making process to handle the complex
dynamics of the multi-phase flow. We also investigate the effects of different
fidelity levels of the surrogate model on decision qualities.",2304.09352v1,https://arxiv.org/pdf/2304.09352v1
Safer Conversational AI as a Source of User Delight,"Xiaoding Lu, Aleksey Korshuk, Zongyi Liu, William Beauchamp, Chai Research","This work explores the impact of moderation on users' enjoyment of
conversational AI systems. While recent advancements in Large Language Models
(LLMs) have led to highly capable conversational AIs that are increasingly
deployed in real-world settings, there is a growing concern over AI safety and
the need to moderate systems to encourage safe language and prevent harm.
However, some users argue that current approaches to moderation limit the
technology, compromise free expression, and limit the value delivered by the
technology. This study takes an unbiased stance and shows that moderation does
not necessarily detract from user enjoyment. Heavy handed moderation does seem
to have a nefarious effect, but models that are moderated to be safer can lead
to a better user experience. By deploying various conversational AIs in the
Chai platform, the study finds that user retention can increase with a level of
moderation and safe system design. These results demonstrate the importance of
appropriately defining safety in models in a way that is both responsible and
focused on serving users.",2304.09865v1,https://arxiv.org/pdf/2304.09865v1
"An adaptive safety layer with hard constraints for safe reinforcement
  learning in multi-energy management systems","Glenn Ceusters, Muhammad Andy Putratama, Rüdiger Franke, Ann Nowé, Maarten Messagie","Safe reinforcement learning (RL) with hard constraint guarantees is a
promising optimal control direction for multi-energy management systems. It
only requires the environment-specific constraint functions itself a priori and
not a complete model. The project-specific upfront and ongoing engineering
efforts are therefore still reduced, better representations of the underlying
system dynamics can still be learnt, and modelling bias is kept to a minimum.
However, even the constraint functions alone are not always trivial to
accurately provide in advance, leading to potentially unsafe behaviour. In this
paper, we present two novel advancements: (I) combining the OptLayer and
SafeFallback method, named OptLayerPolicy, to increase the initial utility
while keeping a high sample efficiency and the possibility to formulate
equality constraints. (II) introducing self-improving hard constraints, to
increase the accuracy of the constraint functions as more and new data becomes
available so that better policies can be learnt. Both advancements keep the
constraint formulation decoupled from the RL formulation, so new (presumably
better) RL algorithms can act as drop-in replacements. We have shown that, in a
simulated multi-energy system case study, the initial utility is increased to
92.4% (OptLayerPolicy) compared to 86.1% (OptLayer) and that the policy after
training is increased to 104.9% (GreyOptLayerPolicy) compared to 103.4%
(OptLayer) - all relative to a vanilla RL benchmark. Although introducing
surrogate functions into the optimisation problem requires special attention,
we conclude that the newly presented GreyOptLayerPolicy method is the most
advantageous.",2304.08897v3,https://arxiv.org/pdf/2304.08897v3
"Uncovering the Inner Workings of STEGO for Safe Unsupervised Semantic
  Segmentation","Alexander Koenig, Maximilian Schambach, Johannes Otterbach","Self-supervised pre-training strategies have recently shown impressive
results for training general-purpose feature extraction backbones in computer
vision. In combination with the Vision Transformer architecture, the DINO
self-distillation technique has interesting emerging properties, such as
unsupervised clustering in the latent space and semantic correspondences of the
produced features without using explicit human-annotated labels. The STEGO
method for unsupervised semantic segmentation contrastively distills feature
correspondences of a DINO-pre-trained Vision Transformer and recently set a new
state of the art. However, the detailed workings of STEGO have yet to be
disentangled, preventing its usage in safety-critical applications. This paper
provides a deeper understanding of the STEGO architecture and training strategy
by conducting studies that uncover the working mechanisms behind STEGO,
reproduce and extend its experimental validation, and investigate the ability
of STEGO to transfer to different datasets. Results demonstrate that the STEGO
architecture can be interpreted as a semantics-preserving dimensionality
reduction technique.",2304.07314v1,https://arxiv.org/pdf/2304.07314v1
"Interpretability is a Kind of Safety: An Interpreter-based Ensemble for
  Adversary Defense","Jingyuan Wang, Yufan Wu, Mingxuan Li, Xin Lin, Junjie Wu, Chao Li","While having achieved great success in rich real-life applications, deep
neural network (DNN) models have long been criticized for their vulnerability
to adversarial attacks. Tremendous research efforts have been dedicated to
mitigating the threats of adversarial attacks, but the essential trait of
adversarial examples is not yet clear, and most existing methods are yet
vulnerable to hybrid attacks and suffer from counterattacks. In light of this,
in this paper, we first reveal a gradient-based correlation between sensitivity
analysis-based DNN interpreters and the generation process of adversarial
examples, which indicates the Achilles's heel of adversarial attacks and sheds
light on linking together the two long-standing challenges of DNN: fragility
and unexplainability. We then propose an interpreter-based ensemble framework
called X-Ensemble for robust adversary defense. X-Ensemble adopts a novel
detection-rectification process and features in building multiple sub-detectors
and a rectifier upon various types of interpretation information toward target
classifiers. Moreover, X-Ensemble employs the Random Forests (RF) model to
combine sub-detectors into an ensemble detector for adversarial hybrid attacks
defense. The non-differentiable property of RF further makes it a precious
choice against the counterattack of adversaries. Extensive experiments under
various types of state-of-the-art attacks and diverse attack scenarios
demonstrate the advantages of X-Ensemble to competitive baseline methods.",2304.06919v1,https://arxiv.org/pdf/2304.06919v1
"Model-based Dynamic Shielding for Safe and Efficient Multi-Agent
  Reinforcement Learning","Wenli Xiao, Yiwei Lyu, John Dolan","Multi-Agent Reinforcement Learning (MARL) discovers policies that maximize
reward but do not have safety guarantees during the learning and deployment
phases. Although shielding with Linear Temporal Logic (LTL) is a promising
formal method to ensure safety in single-agent Reinforcement Learning (RL), it
results in conservative behaviors when scaling to multi-agent scenarios.
Additionally, it poses computational challenges for synthesizing shields in
complex multi-agent environments. This work introduces Model-based Dynamic
Shielding (MBDS) to support MARL algorithm design. Our algorithm synthesizes
distributive shields, which are reactive systems running in parallel with each
MARL agent, to monitor and rectify unsafe behaviors. The shields can
dynamically split, merge, and recompute based on agents' states. This design
enables efficient synthesis of shields to monitor agents in complex
environments without coordination overheads. We also propose an algorithm to
synthesize shields without prior knowledge of the dynamics model. The proposed
algorithm obtains an approximate world model by interacting with the
environment during the early stage of exploration, making our MBDS enjoy formal
safety guarantees with high probability. We demonstrate in simulations that our
framework can surpass existing baselines in terms of safety guarantees and
learning performance.",2304.06281v1,https://arxiv.org/pdf/2304.06281v1
"Experts' cognition-driven safe noisy labels learning for precise
  segmentation of residual tumor in breast cancer","Yongquan Yang, Jie Chen, Yani Wei, Mohammad Alobaidi, Hong Bu","Precise segmentation of residual tumor in breast cancer (PSRTBC) after
neoadjuvant chemotherapy is a fundamental key technique in the treatment
process of breast cancer. However, achieving PSRTBC is still a challenge, since
the breast cancer tissue and tumor cells commonly have complex and varied
morphological changes after neoadjuvant chemotherapy, which inevitably
increases the difficulty to produce a predictive model that has good
generalization with machine learning. To alleviate this situation, in this
paper, we propose an experts' cognition-driven safe noisy labels learning
(ECDSNLL) approach. In the concept of safe noisy labels learning, which is a
typical type of safe weakly supervised learning, ECDSNLL is constructed by
integrating the pathology experts' cognition about identifying residual tumor
in breast cancer and the artificial intelligence experts' cognition about data
modeling with provided data basis. We show the advantages of the proposed
ECDSNLL approach and its promising potentials in addressing PSRTBC. We also
release a better predictive model for achieving PSRTBC, which can be leveraged
to promote the development of related application software.",2304.07295v1,https://arxiv.org/pdf/2304.07295v1
"Stable and Safe Reinforcement Learning via a Barrier-Lyapunov
  Actor-Critic Approach","Liqun Zhao, Konstantinos Gatsis, Antonis Papachristodoulou","Reinforcement learning (RL) has demonstrated impressive performance in
various areas such as video games and robotics. However, ensuring safety and
stability, which are two critical properties from a control perspective,
remains a significant challenge when using RL to control real-world systems. In
this paper, we first provide definitions of safety and stability for the RL
system, and then combine the control barrier function (CBF) and control
Lyapunov function (CLF) methods with the actor-critic method in RL to propose a
Barrier-Lyapunov Actor-Critic (BLAC) framework which helps maintain the
aforementioned safety and stability for the system. In this framework, CBF
constraints for safety and CLF constraint for stability are constructed based
on the data sampled from the replay buffer, and the augmented Lagrangian method
is used to update the parameters of the RL-based controller. Furthermore, an
additional backup controller is introduced in case the RL-based controller
cannot provide valid control signals when safety and stability constraints
cannot be satisfied simultaneously. Simulation results show that this framework
yields a controller that can help the system approach the desired state and
cause fewer violations of safety constraints compared to baseline algorithms.",2304.04066v3,https://arxiv.org/pdf/2304.04066v3
Learning Multi-Pursuit Evasion for Safe Targeted Navigation of Drones,"Jiaping Xiao, Mir Feroskhan","Safe navigation of drones in the presence of adversarial physical attacks
from multiple pursuers is a challenging task. This paper proposes a novel
approach, asynchronous multi-stage deep reinforcement learning (AMS-DRL), to
train adversarial neural networks that can learn from the actions of multiple
evolved pursuers and adapt quickly to their behavior, enabling the drone to
avoid attacks and reach its target. Specifically, AMS-DRL evolves adversarial
agents in a pursuit-evasion game where the pursuers and the evader are
asynchronously trained in a bipartite graph way during multiple stages. Our
approach guarantees convergence by ensuring Nash equilibrium among agents from
the game-theory analysis. We evaluate our method in extensive simulations and
show that it outperforms baselines with higher navigation success rates. We
also analyze how parameters such as the relative maximum speed affect
navigation performance. Furthermore, we have conducted physical experiments and
validated the effectiveness of the trained policies in real-time flights. A
success rate heatmap is introduced to elucidate how spatial geometry influences
navigation outcomes. Project website:
https://github.com/NTU-ICG/AMS-DRL-for-Pursuit-Evasion.",2304.03443v2,https://arxiv.org/pdf/2304.03443v2
"Safe MDP Planning by Learning Temporal Patterns of Undesirable
  Trajectories and Averting Negative Side Effects","Siow Meng Low, Akshat Kumar, Scott Sanner","In safe MDP planning, a cost function based on the current state and action
is often used to specify safety aspects. In the real world, often the state
representation used may lack sufficient fidelity to specify such safety
constraints. Operating based on an incomplete model can often produce
unintended negative side effects (NSEs). To address these challenges, first, we
associate safety signals with state-action trajectories (rather than just an
immediate state-action). This makes our safety model highly general. We also
assume categorical safety labels are given for different trajectories, rather
than a numerical cost function, which is harder to specify by the problem
designer. We then employ a supervised learning model to learn such
non-Markovian safety patterns. Second, we develop a Lagrange multiplier method,
which incorporates the safety model and the underlying MDP model in a single
computation graph to facilitate agent learning of safe behaviors. Finally, our
empirical results on a variety of discrete and continuous domains show that
this approach can satisfy complex non-Markovian safety constraints while
optimizing an agent's total returns, is highly scalable, and is also better
than the previous best approach for Markovian NSEs.",2304.03081v1,https://arxiv.org/pdf/2304.03081v1
Safe Explicable Planning,"Akkamahadevi Hanni, Andrew Boateng, Yu Zhang","Human expectations arise from their understanding of others and the world. In
the context of human-AI interaction, this understanding may not align with
reality, leading to the AI agent failing to meet expectations and compromising
team performance. Explicable planning, introduced as a method to bridge this
gap, aims to reconcile human expectations with the agent's optimal behavior,
facilitating interpretable decision-making. However, an unresolved critical
issue is ensuring safety in explicable planning, as it could result in
explicable behaviors that are unsafe. To address this, we propose Safe
Explicable Planning (SEP), which extends the prior work to support the
specification of a safety bound. The goal of SEP is to find behaviors that
align with human expectations while adhering to the specified safety criterion.
Our approach generalizes the consideration of multiple objectives stemming from
multiple models rather than a single model, yielding a Pareto set of safe
explicable policies. We present both an exact method, guaranteeing finding the
Pareto set, and a more efficient greedy method that finds one of the policies
in the Pareto set. Additionally, we offer approximate solutions based on state
aggregation to improve scalability. We provide formal proofs that validate the
desired theoretical properties of these methods. Evaluation through simulations
and physical robot experiments confirms the effectiveness of our approach for
safe explicable planning.",2304.03773v4,https://arxiv.org/pdf/2304.03773v4
"Safety Analysis in the Era of Large Language Models: A Case Study of
  STPA using ChatGPT","Yi Qi, Xingyu Zhao, Siddartha Khastgir, Xiaowei Huang","Can safety analysis make use of Large Language Models (LLMs)? A case study
explores Systems Theoretic Process Analysis (STPA) applied to Automatic
Emergency Brake (AEB) and Electricity Demand Side Management (DSM) systems
using ChatGPT. We investigate how collaboration schemes, input semantic
complexity, and prompt guidelines influence STPA results. Comparative results
show that using ChatGPT without human intervention may be inadequate due to
reliability related issues, but with careful design, it may outperform human
experts. No statistically significant differences are found when varying the
input semantic complexity or using common prompt guidelines, which suggests the
necessity for developing domain-specific prompt engineering. We also highlight
future challenges, including concerns about LLM trustworthiness and the
necessity for standardisation and regulation in this domain.",2304.01246v3,https://arxiv.org/pdf/2304.01246v3
"Safe Perception-Based Control under Stochastic Sensor Uncertainty using
  Conformal Prediction","Shuo Yang, George J. Pappas, Rahul Mangharam, Lars Lindemann","We consider perception-based control using state estimates that are obtained
from high-dimensional sensor measurements via learning-enabled perception maps.
However, these perception maps are not perfect and result in state estimation
errors that can lead to unsafe system behavior. Stochastic sensor noise can
make matters worse and result in estimation errors that follow unknown
distributions. We propose a perception-based control framework that i)
quantifies estimation uncertainty of perception maps, and ii) integrates these
uncertainty representations into the control design. To do so, we use conformal
prediction to compute valid state estimation regions, which are sets that
contain the unknown state with high probability. We then devise a sampled-data
controller for continuous-time systems based on the notion of measurement
robust control barrier functions. Our controller uses idea from self-triggered
control and enables us to avoid using stochastic calculus. Our framework is
agnostic to the choice of the perception map, independent of the noise
distribution, and to the best of our knowledge the first to provide
probabilistic safety guarantees in such a setting. We demonstrate the
effectiveness of our proposed perception-based controller for a LiDAR-enabled
F1/10th car.",2304.00194v2,https://arxiv.org/pdf/2304.00194v2
Physical Deep Reinforcement Learning Towards Safety Guarantee,"Hongpeng Cao, Yanbing Mao, Lui Sha, Marco Caccamo","Deep reinforcement learning (DRL) has achieved tremendous success in many
complex decision-making tasks of autonomous systems with high-dimensional state
and/or action spaces. However, the safety and stability still remain major
concerns that hinder the applications of DRL to safety-critical autonomous
systems. To address the concerns, we proposed the Phy-DRL: a physical deep
reinforcement learning framework. The Phy-DRL is novel in two architectural
designs: i) Lyapunov-like reward, and ii) residual control (i.e., integration
of physics-model-based control and data-driven control). The concurrent
physical reward and residual control empower the Phy-DRL the (mathematically)
provable safety and stability guarantees. Through experiments on the inverted
pendulum, we show that the Phy-DRL features guaranteed safety and stability and
enhanced robustness, while offering remarkably accelerated training and
enlarged reward.",2303.16860v1,https://arxiv.org/pdf/2303.16860v1
"Beyond Toxic: Toxicity Detection Datasets are Not Enough for Brand
  Safety","Elizaveta Korotkova, Isaac Kwan Yin Chung","The rapid growth in user generated content on social media has resulted in a
significant rise in demand for automated content moderation. Various methods
and frameworks have been proposed for the tasks of hate speech detection and
toxic comment classification. In this work, we combine common datasets to
extend these tasks to brand safety. Brand safety aims to protect commercial
branding by identifying contexts where advertisements should not appear and
covers not only toxicity, but also other potentially harmful content. As these
datasets contain different label sets, we approach the overall problem as a
binary classification task. We demonstrate the need for building brand safety
specific datasets via the application of common toxicity detection datasets to
a subset of brand safety and empirically analyze the effects of weighted
sampling strategies in text classification.",2303.15110v1,https://arxiv.org/pdf/2303.15110v1
"Safe and Sample-efficient Reinforcement Learning for Clustered Dynamic
  Environments","Hongyi Chen, Changliu Liu","This study proposes a safe and sample-efficient reinforcement learning (RL)
framework to address two major challenges in developing applicable RL
algorithms: satisfying safety constraints and efficiently learning with limited
samples. To guarantee safety in real-world complex environments, we use the
safe set algorithm (SSA) to monitor and modify the nominal controls, and
evaluate SSA+RL in a clustered dynamic environment which is challenging to be
solved by existing RL algorithms. However, the SSA+RL framework is usually not
sample-efficient especially in reward-sparse environments, which has not been
addressed in previous safe RL works. To improve the learning efficiency, we
propose three techniques: (1) avoiding behaving overly conservative by adapting
the SSA; (2) encouraging safe exploration using random network distillation
with safety constraints; (3) improving policy convergence by treating SSA as
expert demonstrations and directly learn from that. The experimental results
show that our framework can achieve better safety performance compare to other
safe RL methods during training and solve the task with substantially fewer
episodes. Project website: https://hychen-naza.github.io/projects/Safe_RL/.",2303.14265v1,https://arxiv.org/pdf/2303.14265v1
"Blockchain-based decentralized voting system security Perspective: Safe
  and secure for digital voting system","Jagbeer Singh, Utkarsh Rastogi, Yash Goel, Brijesh Gupta, Utkarsh","This research study focuses primarily on Block-Chain-based voting systems,
which facilitate participation in and administration of voting for voters,
candidates, and officials. Because we used Block-Chain in the backend, which
enables everyone to trace vote fraud, our system is incredibly safe. This paper
approach any unique identification the Aadhar Card number or an OTP will be
generated then user can utilise the voting system to cast his/her vote. A
proposal for Bit-coin, a virtual currency system that is decided by a central
authority for producing money, transferring ownership, and validating
transactions, included the peer-to-peer network in a Block-Chain system, the
ledger is duplicated across several, identical databases which is hosted and
updated by a different process and all other nodes are updated concurrently if
changes made to one node and a transaction occurs, the records of the values
and assets are permanently exchanged, Only the user and the system need to be
verified no other authentication required. If any transaction carried out on a
block chain-based system would be settled in a matter of seconds while still
being safe, verifiable, and transparent. Although block-chain technology is the
foundation for Bitcoin and other digital currencies but also it may be applied
widely to greatly reduce difficulties in many other sectors, Voting is the
sector that is battling from a lack of security, centralized-authority,
management-issues, and many more despite the fact that transactions are kept in
a distributed and safe fashion.",2303.06306v1,https://arxiv.org/pdf/2303.06306v1
"Safe Machine-Learning-supported Model Predictive Force and Motion
  Control in Robotics","Janine Matschek, Johanna Bethge, Rolf Findeisen","Many robotic tasks, such as human-robot interactions or the handling of
fragile objects, require tight control and limitation of appearing forces and
moments alongside sensible motion control to achieve safe yet high-performance
operation. We propose a learning-supported model predictive force and motion
control scheme that provides stochastic safety guarantees while adapting to
changing situations. Gaussian processes are used to learn the uncertain
relations that map the robot's states to the forces and moments. The model
predictive controller uses these Gaussian process models to achieve precise
motion and force control under stochastic constraint satisfaction. As the
uncertainty only occurs in the static model parts -- the output equations -- a
computationally efficient stochastic MPC formulation is used. Analysis of
recursive feasibility of the optimal control problem and convergence of the
closed loop system for the static uncertainty case are given. Chance constraint
formulation and back-offs are constructed based on the variance of the Gaussian
process to guarantee safe operation. The approach is illustrated on a
lightweight robot in simulations and experiments.",2303.04569v1,https://arxiv.org/pdf/2303.04569v1
ConBaT: Control Barrier Transformer for Safe Policy Learning,"Yue Meng, Sai Vemprala, Rogerio Bonatti, Chuchu Fan, Ashish Kapoor","Large-scale self-supervised models have recently revolutionized our ability
to perform a variety of tasks within the vision and language domains. However,
using such models for autonomous systems is challenging because of safety
requirements: besides executing correct actions, an autonomous agent must also
avoid the high cost and potentially fatal critical mistakes. Traditionally,
self-supervised training mainly focuses on imitating previously observed
behaviors, and the training demonstrations carry no notion of which behaviors
should be explicitly avoided. In this work, we propose Control Barrier
Transformer (ConBaT), an approach that learns safe behaviors from
demonstrations in a self-supervised fashion. ConBaT is inspired by the concept
of control barrier functions in control theory and uses a causal transformer
that learns to predict safe robot actions autoregressively using a critic that
requires minimal safety data labeling. During deployment, we employ a
lightweight online optimization to find actions that ensure future states lie
within the learned safe set. We apply our approach to different simulated
control tasks and show that our method results in safer control policies
compared to other classical and learning-based methods such as imitation
learning, reinforcement learning, and model predictive control.",2303.04212v1,https://arxiv.org/pdf/2303.04212v1
"A Multiplicative Value Function for Safe and Efficient Reinforcement
  Learning","Nick Bührer, Zhejun Zhang, Alexander Liniger, Fisher Yu, Luc Van Gool","An emerging field of sequential decision problems is safe Reinforcement
Learning (RL), where the objective is to maximize the reward while obeying
safety constraints. Being able to handle constraints is essential for deploying
RL agents in real-world environments, where constraint violations can harm the
agent and the environment. To this end, we propose a safe model-free RL
algorithm with a novel multiplicative value function consisting of a safety
critic and a reward critic. The safety critic predicts the probability of
constraint violation and discounts the reward critic that only estimates
constraint-free returns. By splitting responsibilities, we facilitate the
learning task leading to increased sample efficiency. We integrate our approach
into two popular RL algorithms, Proximal Policy Optimization and Soft
Actor-Critic, and evaluate our method in four safety-focused environments,
including classical RL benchmarks augmented with safety constraints and robot
navigation tasks with images and raw Lidar scans as observations. Finally, we
make the zero-shot sim-to-real transfer where a differential drive robot has to
navigate through a cluttered room. Our code can be found at
https://github.com/nikeke19/Safe-Mult-RL.",2303.04118v1,https://arxiv.org/pdf/2303.04118v1
"ChatGPT is on the Horizon: Could a Large Language Model be Suitable for
  Intelligent Traffic Safety Research and Applications?","Ou Zheng, Mohamed Abdel-Aty, Dongdong Wang, Zijin Wang, Shengxuan Ding","ChatGPT embarks on a new era of artificial intelligence and will
revolutionize the way we approach intelligent traffic safety systems. This
paper begins with a brief introduction about the development of large language
models (LLMs). Next, we exemplify using ChatGPT to address key traffic safety
issues. Furthermore, we discuss the controversies surrounding LLMs, raise
critical questions for their deployment, and provide our solutions. Moreover,
we propose an idea of multi-modality representation learning for smarter
traffic safety decision-making and open more questions for application
improvement. We believe that LLM will both shape and potentially facilitate
components of traffic safety research.",2303.05382v3,https://arxiv.org/pdf/2303.05382v3
Safe Reinforcement Learning via Probabilistic Logic Shields,"Wen-Chi Yang, Giuseppe Marra, Gavin Rens, Luc De Raedt","Safe Reinforcement learning (Safe RL) aims at learning optimal policies while
staying safe. A popular solution to Safe RL is shielding, which uses a logical
safety specification to prevent an RL agent from taking unsafe actions.
However, traditional shielding techniques are difficult to integrate with
continuous, end-to-end deep RL methods. To this end, we introduce Probabilistic
Logic Policy Gradient (PLPG). PLPG is a model-based Safe RL technique that uses
probabilistic logic programming to model logical safety constraints as
differentiable functions. Therefore, PLPG can be seamlessly applied to any
policy gradient algorithm while still providing the same convergence
guarantees. In our experiments, we show that PLPG learns safer and more
rewarding policies compared to other state-of-the-art shielding techniques.",2303.03226v1,https://arxiv.org/pdf/2303.03226v1
"Using a Variational Autoencoder to Learn Valid Search Spaces of Safely
  Monitored Autonomous Robots for Last-Mile Delivery","Peter J. Bentley, Soo Ling Lim, Paolo Arcaini, Fuyuki Ishikawa","The use of autonomous robots for delivery of goods to customers is an
exciting new way to provide a reliable and sustainable service. However, in the
real world, autonomous robots still require human supervision for safety
reasons. We tackle the realworld problem of optimizing autonomous robot timings
to maximize deliveries, while ensuring that there are never too many robots
running simultaneously so that they can be monitored safely. We assess the use
of a recent hybrid machine-learningoptimization approach COIL (constrained
optimization in learned latent space) and compare it with a baseline genetic
algorithm for the purposes of exploring variations of this problem. We also
investigate new methods for improving the speed and efficiency of COIL. We show
that only COIL can find valid solutions where appropriate numbers of robots run
simultaneously for all problem variations tested. We also show that when COIL
has learned its latent representation, it can optimize 10% faster than the GA,
making it a good choice for daily re-optimization of robots where delivery
requests for each day are allocated to robots while maintaining safe numbers of
robots running at once.",2303.03211v2,https://arxiv.org/pdf/2303.03211v2
"Both eyes open: Vigilant Incentives help Regulatory Markets improve AI
  Safety","Paolo Bova, Alessandro Di Stefano, The Anh Han","In the context of rapid discoveries by leaders in AI, governments must
consider how to design regulation that matches the increasing pace of new AI
capabilities. Regulatory Markets for AI is a proposal designed with
adaptability in mind. It involves governments setting outcome-based targets for
AI companies to achieve, which they can show by purchasing services from a
market of private regulators. We use an evolutionary game theory model to
explore the role governments can play in building a Regulatory Market for AI
systems that deters reckless behaviour. We warn that it is alarmingly easy to
stumble on incentives which would prevent Regulatory Markets from achieving
this goal. These 'Bounty Incentives' only reward private regulators for
catching unsafe behaviour. We argue that AI companies will likely learn to
tailor their behaviour to how much effort regulators invest, discouraging
regulators from innovating. Instead, we recommend that governments always
reward regulators, except when they find that those regulators failed to detect
unsafe behaviour that they should have. These 'Vigilant Incentives' could
encourage private regulators to find innovative ways to evaluate cutting-edge
AI systems.",2303.03174v1,https://arxiv.org/pdf/2303.03174v1
Modular Safety-Critical Control of Legged Robots,"Berk Tosun, Evren Samur","Safety concerns during the operation of legged robots must be addressed to
enable their widespread use. Machine learning-based control methods that use
model-based constraints provide promising means to improve robot safety. This
study presents a modular safety filter to improve the safety of a legged robot,
i.e., reduce the chance of a fall. The prerequisite is the availability of a
robot that is capable of locomotion, i.e., a nominal controller exists. During
locomotion, terrain properties around the robot are estimated through machine
learning which uses a minimal set of proprioceptive signals. A novel
deep-learning model utilizing an efficient transformer architecture is used for
the terrain estimation. A quadratic program combines the terrain estimations
with inverse dynamics and a novel exponential control barrier function
constraint to filter and certify nominal control signals. The result is an
optimal controller that acts as a filter. The filtered control signal allows
safe locomotion of the robot. The resulting approach is generalizable, and
could be transferred with low effort to any other legged system.",2303.02386v1,https://arxiv.org/pdf/2303.02386v1
"Data-efficient, Explainable and Safe Box Manipulation: Illustrating the
  Advantages of Physical Priors in Model-Predictive Control","Achkan Salehi, Stephane Doncieux","Model-based RL/control have gained significant traction in robotics. Yet,
these approaches often remain data-inefficient and lack the explainability of
hand-engineered solutions. This makes them difficult to debug/integrate in
safety-critical settings. However, in many systems, prior knowledge of
environment kinematics/dynamics is available. Incorporating such priors can
help address the aforementioned problems by reducing problem complexity and the
need for exploration, while also facilitating the expression of the decisions
taken by the agent in terms of physically meaningful entities. Our aim with
this paper is to illustrate and support this point of view via a case-study. We
model a payload manipulation problem based on a real robotic system, and show
that leveraging prior knowledge about the dynamics of the environment in an MPC
framework can lead to improvements in explainability, safety and
data-efficiency, leading to satisfying generalization properties with less
data.",2303.01563v2,https://arxiv.org/pdf/2303.01563v2
"Safe AI for health and beyond -- Monitoring to transform a health
  service","Mahed Abroshan, Michael Burkhart, Oscar Giles, Sam Greenbury, Zoe Kourtzi, Jack Roberts, Mihaela van der Schaar, Jannetta S Steyn, Alan Wilson, May Yong","Machine learning techniques are effective for building predictive models
because they identify patterns in large datasets. Development of a model for
complex real-life problems often stop at the point of publication, proof of
concept or when made accessible through some mode of deployment. However, a
model in the medical domain risks becoming obsolete as patient demographics,
systems and clinical practices change. The maintenance and monitoring of
predictive model performance post-publication is crucial to enable their safe
and effective long-term use. We will assess the infrastructure required to
monitor the outputs of a machine learning algorithm, and present two scenarios
with examples of monitoring and updates of models, firstly on a breast cancer
prognosis model trained on public longitudinal data, and secondly on a
neurodegenerative stratification algorithm that is currently being developed
and tested in clinic.",2303.01513v3,https://arxiv.org/pdf/2303.01513v3
Safe-DS: A Domain Specific Language to Make Data Science Safe,"Lars Reimann, Günter Kniesel-Wünsche","Due to the long runtime of Data Science (DS) pipelines, even small
programming mistakes can be very costly, if they are not detected statically.
However, even basic static type checking of DS pipelines is difficult because
most are written in Python. Static typing is available in Python only via
external linters. These require static type annotations for parameters or
results of functions, which many DS libraries do not provide. In this paper, we
show how the wealth of Python DS libraries can be used in a statically safe way
via Safe-DS, a domain specific language (DSL) for DS. Safe-DS catches
conventional type errors plus errors related to range restrictions, data
manipulation, and call order of functions, going well beyond the abilities of
current Python linters. Python libraries are integrated into Safe-DS via a stub
language for specifying the interface of its declarations, and an API-Editor
that is able to extract type information from the code and documentation of
Python libraries, and automatically generate suitable stubs.
  Moreover, Safe-DS complements textual DS pipelines with a graphical
representation that eases safe development by preventing syntax errors. The
seamless synchronization of textual and graphic view lets developers always
choose the one best suited for their skills and current task. We think that
Safe-DS can make DS development easier, faster, and more reliable,
significantly reducing development costs.",2302.14548v2,https://arxiv.org/pdf/2302.14548v2
"Safe Peeling for L0-Regularized Least-Squares with supplementary
  material","Théo Guyard, Gilles Monnoyer, Clément Elvira, Cédric Herzet","We introduce a new methodology dubbed ``safe peeling'' to accelerate the
resolution of L0-regularized least-squares problems via a Branch-and-Bound
(BnB) algorithm. Our procedure enables to tighten the convex relaxation
considered at each node of the BnB decision tree and therefore potentially
allows for more aggressive pruning. Numerical simulations show that our
proposed methodology leads to significant gains in terms of number of nodes
explored and overall solving time.s show that our proposed methodology leads to
significant gains in terms of number of nodes explored and overall solving
time.",2302.14471v4,https://arxiv.org/pdf/2302.14471v4
"Efficient Exploration Using Extra Safety Budget in Constrained Policy
  Optimization","Haotian Xu, Shengjie Wang, Zhaolei Wang, Yunzhe Zhang, Qing Zhuo, Yang Gao, Tao Zhang","Reinforcement learning (RL) has achieved promising results on most robotic
control tasks. Safety of learning-based controllers is an essential notion of
ensuring the effectiveness of the controllers. Current methods adopt whole
consistency constraints during the training, thus resulting in inefficient
exploration in the early stage. In this paper, we propose an algorithm named
Constrained Policy Optimization with Extra Safety Budget (ESB-CPO) to strike a
balance between the exploration efficiency and the constraints satisfaction. In
the early stage, our method loosens the practical constraints of unsafe
transitions (adding extra safety budget) with the aid of a new metric we
propose. With the training process, the constraints in our optimization problem
become tighter. Meanwhile, theoretical analysis and practical experiments
demonstrate that our method gradually meets the cost limit's demand in the
final training stage. When evaluated on Safety-Gym and Bullet-Safety-Gym
benchmarks, our method has shown its advantages over baseline algorithms in
terms of safety and optimality. Remarkably, our method gains remarkable
performance improvement under the same cost limit compared with baselines.",2302.14339v2,https://arxiv.org/pdf/2302.14339v2
Safe Multi-agent Learning via Trapping Regions,"Aleksander Czechowski, Frans A. Oliehoek","One of the main challenges of multi-agent learning lies in establishing
convergence of the algorithms, as, in general, a collection of individual,
self-serving agents is not guaranteed to converge with their joint policy, when
learning concurrently. This is in stark contrast to most single-agent
environments, and sets a prohibitive barrier for deployment in practical
applications, as it induces uncertainty in long term behavior of the system. In
this work, we apply the concept of trapping regions, known from qualitative
theory of dynamical systems, to create safety sets in the joint strategy space
for decentralized learning. We propose a binary partitioning algorithm for
verification that candidate sets form trapping regions in systems with known
learning dynamics, and a heuristic sampling algorithm for scenarios where
learning dynamics are not known. We demonstrate the applications to a
regularized version of Dirac Generative Adversarial Network, a
four-intersection traffic control scenario run in a state of the art
open-source microscopic traffic simulator SUMO, and a mathematical model of
economic competition.",2302.13844v2,https://arxiv.org/pdf/2302.13844v2
"On Bellman's principle of optimality and Reinforcement learning for
  safety-constrained Markov decision process","Rahul Misra, Rafał Wisniewski, Carsten Skovmose Kallesøe","We study optimality for the safety-constrained Markov decision process which
is the underlying framework for safe reinforcement learning. Specifically, we
consider a constrained Markov decision process (with finite states and finite
actions) where the goal of the decision maker is to reach a target set while
avoiding an unsafe set(s) with certain probabilistic guarantees. Therefore the
underlying Markov chain for any control policy will be multichain since by
definition there exists a target set and an unsafe set. The decision maker also
has to be optimal (with respect to a cost function) while navigating to the
target set. This gives rise to a multi-objective optimization problem. We
highlight the fact that Bellman's principle of optimality may not hold for
constrained Markov decision problems with an underlying multichain structure
(as shown by the counterexample due to Haviv. We resolve the counterexample by
formulating the aforementioned multi-objective optimization problem as a
zero-sum game and thereafter construct an asynchronous value iteration scheme
for the Lagrangian (similar to Shapley's algorithm). Finally, we consider the
reinforcement learning problem for the same and construct a modified
$Q$-learning algorithm for learning the Lagrangian from data. We also provide a
lower bound on the number of iterations required for learning the Lagrangian
and corresponding error bounds.",2302.13152v3,https://arxiv.org/pdf/2302.13152v3
"A Human-Centered Safe Robot Reinforcement Learning Framework with
  Interactive Behaviors","Shangding Gu, Alap Kshirsagar, Yali Du, Guang Chen, Jan Peters, Alois Knoll","Deployment of Reinforcement Learning (RL) algorithms for robotics
applications in the real world requires ensuring the safety of the robot and
its environment. Safe Robot RL (SRRL) is a crucial step towards achieving
human-robot coexistence. In this paper, we envision a human-centered SRRL
framework consisting of three stages: safe exploration, safety value alignment,
and safe collaboration. We examine the research gaps in these areas and propose
to leverage interactive behaviors for SRRL. Interactive behaviors enable
bi-directional information transfer between humans and robots, such as
conversational robot ChatGPT. We argue that interactive behaviors need further
attention from the SRRL community. We discuss four open challenges related to
the robustness, efficiency, transparency, and adaptability of SRRL with
interactive behaviors.",2302.13137v3,https://arxiv.org/pdf/2302.13137v3
"SEO: Safety-Aware Energy Optimization Framework for Multi-Sensor Neural
  Controllers at the Edge","Mohanad Odema, James Ferlez, Yasser Shoukry, Mohammad Abdullah Al Faruque","Runtime energy management has become quintessential for multi-sensor
autonomous systems at the edge for achieving high performance given the
platform constraints. Typical for such systems, however, is to have their
controllers designed with formal guarantees on safety that precede in priority
such optimizations, which in turn limits their application in real settings. In
this paper, we propose a novel energy optimization framework that is aware of
the autonomous system's safety state, and leverages it to regulate the
application of energy optimization methods so that the system's formal safety
properties are preserved. In particular, through the formal characterization of
a system's safety state as a dynamic processing deadline, the computing
workloads of the underlying models can be adapted accordingly. For our
experiments, we model two popular runtime energy optimization methods,
offloading and gating, and simulate an autonomous driving system (ADS) use-case
in the CARLA simulation environment with performance characterizations obtained
from the standard Nvidia Drive PX2 ADS platform. Our results demonstrate that
through a formal awareness of the perceived risks in the test case scenario,
energy efficiency gains are still achieved (reaching 89.9%) while maintaining
the desired safety properties.",2302.12493v1,https://arxiv.org/pdf/2302.12493v1
"Dynamic Regret Analysis of Safe Distributed Online Optimization for
  Convex and Non-convex Problems","Ting-Jui Chang, Sapana Chaudhary, Dileep Kalathil, Shahin Shahrampour","This paper addresses safe distributed online optimization over an unknown set
of linear safety constraints. A network of agents aims at jointly minimizing a
global, time-varying function, which is only partially observable to each
individual agent. Therefore, agents must engage in local communications to
generate a safe sequence of actions competitive with the best minimizer
sequence in hindsight, and the gap between the two sequences is quantified via
dynamic regret. We propose distributed safe online gradient descent
(D-Safe-OGD) with an exploration phase, where all agents estimate the
constraint parameters collaboratively to build estimated feasible sets,
ensuring the action selection safety during the optimization phase. We prove
that for convex functions, D-Safe-OGD achieves a dynamic regret bound of
$O(T^{2/3} \sqrt{\log T} + T^{1/3}C_T^*)$, where $C_T^*$ denotes the
path-length of the best minimizer sequence. We further prove a dynamic regret
bound of $O(T^{2/3} \sqrt{\log T} + T^{2/3}C_T^*)$ for certain non-convex
problems, which establishes the first dynamic regret bound for a safe
distributed algorithm in the non-convex setting.",2302.12320v1,https://arxiv.org/pdf/2302.12320v1
"Improving safety in physical human-robot collaboration via deep metric
  learning","Maryam Rezayati, Grammatiki Zanni, Ying Zaoshi, Davide Scaramuzza, Hans Wernher van de Venn","Direct physical interaction with robots is becoming increasingly important in
flexible production scenarios, but robots without protective fences also pose a
greater risk to the operator. In order to keep the risk potential low,
relatively simple measures are prescribed for operation, such as stopping the
robot if there is physical contact or if a safety distance is violated.
Although human injuries can be largely avoided in this way, all such solutions
have in common that real cooperation between humans and robots is hardly
possible and therefore the advantages of working with such systems cannot
develop its full potential. In human-robot collaboration scenarios, more
sophisticated solutions are required that make it possible to adapt the robot's
behavior to the operator and/or the current situation. Most importantly, during
free robot movement, physical contact must be allowed for meaningful
interaction and not recognized as a collision. However, here lies a key
challenge for future systems: detecting human contact by using robot
proprioception and machine learning algorithms. This work uses the Deep Metric
Learning (DML) approach to distinguish between non-contact robot movement,
intentional contact aimed at physical human-robot interaction, and collision
situations. The achieved results are promising and show show that DML achieves
98.6\% accuracy, which is 4\% higher than the existing standards (i.e. a deep
learning network trained without DML). It also indicates a promising
generalization capability for easy portability to other robots (target robots)
by detecting contact (distinguishing between contactless and intentional or
accidental contact) without having to retrain the model with target robot data.",2302.11933v2,https://arxiv.org/pdf/2302.11933v2
"Machine learning for the prediction of safe and biologically active
  organophosphorus molecules","Hang Hu, Hsu Kiang Ooi, Mohammad Sajjad Ghaemi, Anguang Hu","Drug discovery is a complex process with a large molecular space to be
considered. By constraining the search space, the fragment-based drug design is
an approach that can effectively sample the chemical space of interest. Here we
propose a framework of Recurrent Neural Networks (RNN) with an attention model
to sample the chemical space of organophosphorus molecules using the
fragment-based approach. The framework is trained with a ZINC dataset that is
screened for high druglikeness scores. The goal is to predict molecules with
similar biological action modes as organophosphorus pesticides or chemical
warfare agents yet less toxic to humans. The generated molecules contain a
starting fragment of PO2F but have a bulky hydrocarbon side chain limiting its
binding effectiveness to the targeted protein.",2302.10952v1,https://arxiv.org/pdf/2302.10952v1
"Reentry Risk and Safety Assessment of Spacecraft Debris Based on Machine
  Learning","Hu Gao, Zhihui Li, Depeng Dang, Jingfan Yang, Ning Wang","Uncontrolled spacecraft will disintegrate and generate a large amount of
debris in the reentry process, and ablative debris may cause potential risks to
the safety of human life and property on the ground. Therefore, predicting the
landing points of spacecraft debris and forecasting the degree of risk of
debris to human life and property is very important. In view that it is
difficult to predict the process of reentry process and the reentry point in
advance, and the debris generated from reentry disintegration may cause ground
damage for the uncontrolled space vehicle on expiration of service. In this
paper, we adopt the object-oriented approach to consider the spacecraft and its
disintegrated components as consisting of simple basic geometric models, and
introduce three machine learning models: the support vector regression (SVR),
decision tree regression (DTR) and multilayer perceptron (MLP) to predict the
velocity, longitude and latitude of spacecraft debris landing points for the
first time. Then, we compare the prediction accuracy of the three models.
Furthermore, we define the reentry risk and the degree of danger, and we
calculate the risk level for each spacecraft debris and make warnings
accordingly. The experimental results show that the proposed method can obtain
high accuracy prediction results in at least 15 seconds and make safety level
warning more real-time.",2302.10530v1,https://arxiv.org/pdf/2302.10530v1
Safe Deep Reinforcement Learning by Verifying Task-Level Properties,"Enrico Marchesini, Luca Marzari, Alessandro Farinelli, Christopher Amato","Cost functions are commonly employed in Safe Deep Reinforcement Learning
(DRL). However, the cost is typically encoded as an indicator function due to
the difficulty of quantifying the risk of policy decisions in the state space.
Such an encoding requires the agent to visit numerous unsafe states to learn a
cost-value function to drive the learning process toward safety. Hence,
increasing the number of unsafe interactions and decreasing sample efficiency.
In this paper, we investigate an alternative approach that uses domain
knowledge to quantify the risk in the proximity of such states by defining a
violation metric. This metric is computed by verifying task-level properties,
shaped as input-output conditions, and it is used as a penalty to bias the
policy away from unsafe states without learning an additional value function.
We investigate the benefits of using the violation metric in standard Safe DRL
benchmarks and robotic mapless navigation tasks. The navigation experiments
bridge the gap between Safe DRL and robotics, introducing a framework that
allows rapid testing on real robots. Our experiments show that policies trained
with the violation penalty achieve higher performance over Safe DRL baselines
and significantly reduce the number of visited unsafe states.",2302.10030v1,https://arxiv.org/pdf/2302.10030v1
"Dynamic Simplex: Balancing Safety and Performance in Autonomous Cyber
  Physical Systems","Baiting Luo, Shreyas Ramakrishna, Ava Pettet, Christopher Kuhn, Gabor Karsai, Ayan Mukhopadhyay","Learning Enabled Components (LEC) have greatly assisted cyber-physical
systems in achieving higher levels of autonomy. However, LEC's susceptibility
to dynamic and uncertain operating conditions is a critical challenge for the
safety of these systems. Redundant controller architectures have been widely
adopted for safety assurance in such contexts. These architectures augment LEC
""performant"" controllers that are difficult to verify with ""safety"" controllers
and the decision logic to switch between them. While these architectures ensure
safety, we point out two limitations. First, they are trained offline to learn
a conservative policy of always selecting a controller that maintains the
system's safety, which limits the system's adaptability to dynamic and
non-stationary environments. Second, they do not support reverse switching from
the safety controller to the performant controller, even when the threat to
safety is no longer present. To address these limitations, we propose a dynamic
simplex strategy with an online controller switching logic that allows two-way
switching. We consider switching as a sequential decision-making problem and
model it as a semi-Markov decision process. We leverage a combination of a
myopic selector using surrogate models (for the forward switch) and a
non-myopic planner (for the reverse switch) to balance safety and performance.
We evaluate this approach using an autonomous vehicle case study in the CARLA
simulator using different driving conditions, locations, and component
failures. We show that the proposed approach results in fewer collisions and
higher performance than state-of-the-art alternatives.",2302.09750v1,https://arxiv.org/pdf/2302.09750v1
"Towards Safer Generative Language Models: A Survey on Safety Risks,
  Evaluations, and Improvements","Jiawen Deng, Jiale Cheng, Hao Sun, Zhexin Zhang, Minlie Huang","As generative large model capabilities advance, safety concerns become more
pronounced in their outputs. To ensure the sustainable growth of the AI
ecosystem, it's imperative to undertake a holistic evaluation and refinement of
associated safety risks. This survey presents a framework for safety research
pertaining to large models, delineating the landscape of safety risks as well
as safety evaluation and improvement methods. We begin by introducing safety
issues of wide concern, then delve into safety evaluation methods for large
models, encompassing preference-based testing, adversarial attack approaches,
issues detection, and other advanced evaluation methods. Additionally, we
explore the strategies for enhancing large model safety from training to
deployment, highlighting cutting-edge safety approaches for each stage in
building large models. Finally, we discuss the core challenges in advancing
towards more responsible AI, including the interpretability of safety
mechanisms, ongoing safety issues, and robustness against malicious attacks.
Through this survey, we aim to provide clear technical guidance for safety
researchers and encourage further study on the safety of large models.",2302.09270v3,https://arxiv.org/pdf/2302.09270v3
Constrained Decision Transformer for Offline Safe Reinforcement Learning,"Zuxin Liu, Zijian Guo, Yihang Yao, Zhepeng Cen, Wenhao Yu, Tingnan Zhang, Ding Zhao","Safe reinforcement learning (RL) trains a constraint satisfaction policy by
interacting with the environment. We aim to tackle a more challenging problem:
learning a safe policy from an offline dataset. We study the offline safe RL
problem from a novel multi-objective optimization perspective and propose the
$\epsilon$-reducible concept to characterize problem difficulties. The inherent
trade-offs between safety and task performance inspire us to propose the
constrained decision transformer (CDT) approach, which can dynamically adjust
the trade-offs during deployment. Extensive experiments show the advantages of
the proposed method in learning an adaptive, safe, robust, and high-reward
policy. CDT outperforms its variants and strong offline safe RL baselines by a
large margin with the same hyperparameters across all tasks, while keeping the
zero-shot adaptation capability to different constraint thresholds, making our
approach more suitable for real-world RL under constraints. The code is
available at https://github.com/liuzuxin/OSRL.",2302.07351v2,https://arxiv.org/pdf/2302.07351v2
"Online Safety Property Collection and Refinement for Safe Deep
  Reinforcement Learning in Mapless Navigation","Luca Marzari, Enrico Marchesini, Alessandro Farinelli","Safety is essential for deploying Deep Reinforcement Learning (DRL)
algorithms in real-world scenarios. Recently, verification approaches have been
proposed to allow quantifying the number of violations of a DRL policy over
input-output relationships, called properties. However, such properties are
hard-coded and require task-level knowledge, making their application
intractable in challenging safety-critical tasks. To this end, we introduce the
Collection and Refinement of Online Properties (CROP) framework to design
properties at training time. CROP employs a cost signal to identify unsafe
interactions and use them to shape safety properties. Hence, we propose a
refinement strategy to combine properties that model similar unsafe
interactions. Our evaluation compares the benefits of computing the number of
violations using standard hard-coded properties and the ones generated with
CROP. We evaluate our approach in several robotic mapless navigation tasks and
demonstrate that the violation metric computed with CROP allows higher returns
and lower violations over previous Safe DRL approaches.",2302.06695v1,https://arxiv.org/pdf/2302.06695v1
"EnergyShield: Provably-Safe Offloading of Neural Network Controllers for
  Energy Efficiency","Mohanad Odema, James Ferlez, Goli Vaisi, Yasser Shoukry, Mohammad Abdullah Al Faruque","To mitigate the high energy demand of Neural Network (NN) based Autonomous
Driving Systems (ADSs), we consider the problem of offloading NN controllers
from the ADS to nearby edge-computing infrastructure, but in such a way that
formal vehicle safety properties are guaranteed. In particular, we propose the
EnergyShield framework, which repurposes a controller ''shield'' as a low-power
runtime safety monitor for the ADS vehicle. Specifically, the shield in
EnergyShield provides not only safety interventions but also a formal,
state-based quantification of the tolerable edge response time before vehicle
safety is compromised. Using EnergyShield, an ADS can then save energy by
wirelessly offloading NN computations to edge computers, while still
maintaining a formal guarantee of safety until it receives a response
(on-vehicle hardware provides a just-in-time fail safe). To validate the
benefits of EnergyShield, we implemented and tested it in the Carla simulation
environment. Our results show that EnergyShield maintains safe vehicle
operation while providing significant energy savings compared to on-vehicle NN
evaluation: from 24% to 54% less energy across a range of wireless conditions
and edge delays.",2302.06572v1,https://arxiv.org/pdf/2302.06572v1
"Provably Safe Reinforcement Learning with Step-wise Violation
  Constraints","Nuoya Xiong, Yihan Du, Longbo Huang","In this paper, we investigate a novel safe reinforcement learning problem
with step-wise violation constraints. Our problem differs from existing works
in that we consider stricter step-wise violation constraints and do not assume
the existence of safe actions, making our formulation more suitable for
safety-critical applications which need to ensure safety in all decision steps
and may not always possess safe actions, e.g., robot control and autonomous
driving. We propose a novel algorithm SUCBVI, which guarantees
$\widetilde{O}(\sqrt{ST})$ step-wise violation and
$\widetilde{O}(\sqrt{H^3SAT})$ regret. Lower bounds are provided to validate
the optimality in both violation and regret performance with respect to $S$ and
$T$. Moreover, we further study a novel safe reward-free exploration problem
with step-wise violation constraints. For this problem, we design an
$(\varepsilon,\delta)$-PAC algorithm SRF-UCRL, which achieves nearly
state-of-the-art sample complexity
$\widetilde{O}((\frac{S^2AH^2}{\varepsilon}+\frac{H^4SA}{\varepsilon^2})(\log(\frac{1}{\delta})+S))$,
and guarantees $\widetilde{O}(\sqrt{ST})$ violation during the exploration. The
experimental results demonstrate the superiority of our algorithms in safety
performance, and corroborate our theoretical results.",2302.06064v3,https://arxiv.org/pdf/2302.06064v3
"A Near-Optimal Algorithm for Safe Reinforcement Learning Under
  Instantaneous Hard Constraints","Ming Shi, Yingbin Liang, Ness Shroff","In many applications of Reinforcement Learning (RL), it is critically
important that the algorithm performs safely, such that instantaneous hard
constraints are satisfied at each step, and unsafe states and actions are
avoided. However, existing algorithms for ''safe'' RL are often designed under
constraints that either require expected cumulative costs to be bounded or
assume all states are safe. Thus, such algorithms could violate instantaneous
hard constraints and traverse unsafe states (and actions) in practice.
Therefore, in this paper, we develop the first near-optimal safe RL algorithm
for episodic Markov Decision Processes with unsafe states and actions under
instantaneous hard constraints and the linear mixture model. It not only
achieves a regret $\tilde{O}(\frac{d H^3 \sqrt{dK}}{\Delta_c})$ that tightly
matches the state-of-the-art regret in the setting with only unsafe actions and
nearly matches that in the unconstrained setting, but is also safe at each
step, where $d$ is the feature-mapping dimension, $K$ is the number of
episodes, $H$ is the number of steps in each episode, and $\Delta_c$ is a
safety-related parameter. We also provide a lower bound
$\tilde{\Omega}(\max\{dH \sqrt{K}, \frac{H}{\Delta_c^2}\})$, which indicates
that the dependency on $\Delta_c$ is necessary. Further, both our algorithm
design and regret analysis involve several novel ideas, which may be of
independent interest.",2302.04375v1,https://arxiv.org/pdf/2302.04375v1
"Shared Information-Based Safe And Efficient Behavior Planning For
  Connected Autonomous Vehicles","Songyang Han, Shanglin Zhou, Lynn Pepin, Jiangwei Wang, Caiwen Ding, Fei Miao","The recent advancements in wireless technology enable connected autonomous
vehicles (CAVs) to gather data via vehicle-to-vehicle (V2V) communication, such
as processed LIDAR and camera data from other vehicles. In this work, we design
an integrated information sharing and safe multi-agent reinforcement learning
(MARL) framework for CAVs, to take advantage of the extra information when
making decisions to improve traffic efficiency and safety. We first use weight
pruned convolutional neural networks (CNN) to process the raw image and point
cloud LIDAR data locally at each autonomous vehicle, and share CNN-output data
with neighboring CAVs. We then design a safe actor-critic algorithm that
utilizes both a vehicle's local observation and the information received via
V2V communication to explore an efficient behavior planning policy with safety
guarantees. Using the CARLA simulator for experiments, we show that our
approach improves the CAV system's efficiency in terms of average velocity and
comfort under different CAV ratios and different traffic densities. We also
show that our approach avoids the execution of unsafe actions and always
maintains a safe distance from other vehicles. We construct an
obstacle-at-corner scenario to show that the shared vision can help CAVs to
observe obstacles earlier and take action to avoid traffic jams.",2302.04321v2,https://arxiv.org/pdf/2302.04321v2
"Understanding Policy and Technical Aspects of AI-Enabled Smart Video
  Surveillance to Address Public Safety","Babak Rahimi Ardabili, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Christopher Neff, Sai Datta Bhaskararayuni, Arun Ravindran, Shannon Reid, Hamed Tabkhi","Recent advancements in artificial intelligence (AI) have seen the emergence
of smart video surveillance (SVS) in many practical applications, particularly
for building safer and more secure communities in our urban environments.
Cognitive tasks, such as identifying objects, recognizing actions, and
detecting anomalous behaviors, can produce data capable of providing valuable
insights to the community through statistical and analytical tools. However,
artificially intelligent surveillance systems design requires special
considerations for ethical challenges and concerns. The use and storage of
personally identifiable information (PII) commonly pose an increased risk to
personal privacy. To address these issues, this paper identifies the privacy
concerns and requirements needed to address when designing AI-enabled smart
video surveillance. Further, we propose the first end-to-end AI-enabled
privacy-preserving smart video surveillance system that holistically combines
computer vision analytics, statistical data analytics, cloud-native services,
and end-user applications. Finally, we propose quantitative and qualitative
metrics to evaluate intelligent video surveillance systems. The system shows
the 17.8 frame-per-second (FPS) processing in extreme video scenes. However,
considering privacy in designing such a system results in preferring the
pose-based algorithm to the pixel-based one. This choice resulted in dropping
accuracy in both action and anomaly detection tasks. The results drop from
97.48 to 73.72 in anomaly detection and 96 to 83.07 in the action detection
task. On average, the latency of the end-to-end system is 36.1 seconds.",2302.04310v1,https://arxiv.org/pdf/2302.04310v1
Adaptive Aggregation for Safety-Critical Control,"Huiliang Zhang, Di Wu, Benoit Boulet","Safety has been recognized as the central obstacle to preventing the use of
reinforcement learning (RL) for real-world applications. Different methods have
been developed to deal with safety concerns in RL. However, learning reliable
RL-based solutions usually require a large number of interactions with the
environment. Likewise, how to improve the learning efficiency, specifically,
how to utilize transfer learning for safe reinforcement learning, has not been
well studied. In this work, we propose an adaptive aggregation framework for
safety-critical control. Our method comprises two key techniques: 1) we learn
to transfer the safety knowledge by aggregating the multiple source tasks and a
target task through the attention network; 2) we separate the goal of improving
task performance and reducing constraint violations by utilizing a safeguard.
Experiment results demonstrate that our algorithm can achieve fewer safety
violations while showing better data efficiency compared with several
baselines.",2302.03586v1,https://arxiv.org/pdf/2302.03586v1
State-wise Safe Reinforcement Learning: A Survey,"Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, Changliu Liu","Despite the tremendous success of Reinforcement Learning (RL) algorithms in
simulation environments, applying RL to real-world applications still faces
many challenges. A major concern is safety, in another word, constraint
satisfaction. State-wise constraints are one of the most common constraints in
real-world applications and one of the most challenging constraints in Safe RL.
Enforcing state-wise constraints is necessary and essential to many challenging
tasks such as autonomous driving, robot manipulation. This paper provides a
comprehensive review of existing approaches that address state-wise constraints
in RL. Under the framework of State-wise Constrained Markov Decision Process
(SCMDP), we will discuss the connections, differences, and trade-offs of
existing approaches in terms of (i) safety guarantee and scalability, (ii)
safety and reward performance, and (iii) safety after convergence and during
training. We also summarize limitations of current methods and discuss
potential future directions.",2302.03122v3,https://arxiv.org/pdf/2302.03122v3
"Concrete Safety for ML Problems: System Safety for ML Development and
  Assessment","Edgar W. Jatho, Logan O. Mailloux, Eugene D. Williams, Patrick McClure, Joshua A. Kroll","Many stakeholders struggle to make reliances on ML-driven systems due to the
risk of harm these systems may cause. Concerns of trustworthiness, unintended
social harms, and unacceptable social and ethical violations undermine the
promise of ML advancements. Moreover, such risks in complex ML-driven systems
present a special challenge as they are often difficult to foresee, arising
over periods of time, across populations, and at scale. These risks often arise
not from poor ML development decisions or low performance directly but rather
emerge through the interactions amongst ML development choices, the context of
model use, environmental factors, and the effects of a model on its target.
Systems safety engineering is an established discipline with a proven track
record of identifying and managing risks even in high-complexity sociotechnical
systems. In this work, we apply a state-of-the-art systems safety approach to
concrete applications of ML with notable social and ethical risks to
demonstrate a systematic means for meeting the assurance requirements needed to
argue for safe and trustworthy ML in sociotechnical systems.",2302.02972v1,https://arxiv.org/pdf/2302.02972v1
Safe Interval Path Planning With Kinodynamic Constraints,"Zain Alabedeen Ali, Konstantin Yakovlev","Safe Interval Path Planning (SIPP) is a powerful algorithm for solving
single-agent pathfinding problem when the agent is confined to a graph and
certain vertices/edges of this graph are blocked at certain time intervals due
to dynamic obstacles that populate the environment. Original SIPP algorithm
relies on the assumption that the agent is able to stop instantaneously.
However, this assumption often does not hold in practice, e.g. a mobile robot
moving with a cruising speed is not able to stop immediately but rather
requires gradual deceleration to a full stop that takes time. In other words,
the robot is subject to kinodynamic constraints. Unfortunately, as we show in
this work, in such a case original SIPP is incomplete. To this end, we
introduce a novel variant of SIPP that is provably complete and optimal for
planning with acceleration/deceleration. In the experimental evaluation we show
that the key property of the original SIPP still holds for the modified version
-- it performs much less expansions compared to A* and, as a result, is notably
faster.",2302.00776v1,https://arxiv.org/pdf/2302.00776v1
"Active Uncertainty Reduction for Safe and Efficient Interaction
  Planning: A Shielding-Aware Dual Control Approach","Haimin Hu, David Isele, Sangjae Bae, Jaime F. Fisac","The ability to accurately predict others' behavior is central to the safety
and efficiency of interactive robotics. Unfortunately, robots often lack access
to key information on which these predictions may hinge, such as other agents'
goals, attention, and willingness to cooperate. Dual control theory addresses
this challenge by treating unknown parameters of a predictive model as
stochastic hidden states and inferring their values at runtime using
information gathered during system operation. While able to optimally and
automatically trade off exploration and exploitation, dual control is
computationally intractable for general interactive motion planning. In this
paper, we present a novel algorithmic approach to enable active uncertainty
reduction for interactive motion planning based on the implicit dual control
paradigm. Our approach relies on sampling-based approximation of stochastic
dynamic programming, leading to a model predictive control problem that can be
readily solved by real-time gradient-based optimization methods. The resulting
policy is shown to preserve the dual control effect for a broad class of
predictive models with both continuous and categorical uncertainty. To ensure
the safe operation of the interacting agents, we use a runtime safety filter
(also referred to as a ""shielding"" scheme), which overrides the robot's dual
control policy with a safety fallback strategy when a safety-critical event is
imminent. We then augment the dual control framework with an improved variant
of the recently proposed shielding-aware robust planning scheme, which
proactively balances the nominal planning performance with the risk of
high-cost emergency maneuvers triggered by low-probability agent behaviors. We
demonstrate the efficacy of our approach with both simulated driving studies
and hardware experiments using 1/10 scale autonomous vehicles.",2302.00171v2,https://arxiv.org/pdf/2302.00171v2
"Supporting Safety Analysis of Image-processing DNNs through
  Clustering-based Approaches","Mohammed Oualid Attaoui, Hazem Fahmy, Fabrizio Pastore, Lionel Briand","The adoption of deep neural networks (DNNs) in safety-critical contexts is
often prevented by the lack of effective means to explain their results,
especially when they are erroneous. In our previous work, we proposed a
white-box approach (HUDD) and a black-box approach (SAFE) to automatically
characterize DNN failures. They both identify clusters of similar images from a
potentially large set of images leading to DNN failures. However, the analysis
pipelines for HUDD and SAFE were instantiated in specific ways according to
common practices, deferring the analysis of other pipelines to future work. In
this paper, we report on an empirical evaluation of 99 different pipelines for
root cause analysis of DNN failures. They combine transfer learning,
autoencoders, heatmaps of neuron relevance, dimensionality reduction
techniques, and different clustering algorithms. Our results show that the best
pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters
almost exclusively capturing images of the same failure scenario, thus
facilitating root cause analysis. Further, it generates distinct clusters for
each root cause of failure, thus enabling engineers to detect all the unsafe
scenarios. Interestingly, these results hold even for failure scenarios that
are only observed in a small percentage of the failing images.",2301.13506v3,https://arxiv.org/pdf/2301.13506v3
"An investigation of challenges encountered when specifying training data
  and runtime monitors for safety critical ML applications","Hans-Martin Heyn, Eric Knauss, Iswarya Malleswaran, Shruthi Dinakaran","Context and motivation: The development and operation of critical software
that contains machine learning (ML) models requires diligence and established
processes. Especially the training data used during the development of ML
models have major influences on the later behaviour of the system. Runtime
monitors are used to provide guarantees for that behaviour. Question / problem:
We see major uncertainty in how to specify training data and runtime monitoring
for critical ML models and by this specifying the final functionality of the
system. In this interview-based study we investigate the underlying challenges
for these difficulties. Principal ideas/results: Based on ten interviews with
practitioners who develop ML models for critical applications in the automotive
and telecommunication sector, we identified 17 underlying challenges in 6
challenge groups that relate to the challenge of specifying training data and
runtime monitoring. Contribution: The article provides a list of the identified
underlying challenges related to the difficulties practitioners experience when
specifying training data and runtime monitoring for ML models. Furthermore,
interconnection between the challenges were found and based on these
connections recommendation proposed to overcome the root causes for the
challenges.",2301.13476v1,https://arxiv.org/pdf/2301.13476v1
Probably Anytime-Safe Stochastic Combinatorial Semi-Bandits,"Yunlong Hou, Vincent Y. F. Tan, Zixin Zhong","Motivated by concerns about making online decisions that incur undue amount
of risk at each time step, in this paper, we formulate the probably
anytime-safe stochastic combinatorial semi-bandits problem. In this problem,
the agent is given the option to select a subset of size at most $K$ from a set
of $L$ ground items. Each item is associated to a certain mean reward as well
as a variance that represents its risk. To mitigate the risk that the agent
incurs, we require that with probability at least $1-\delta$, over the entire
horizon of time $T$, each of the choices that the agent makes should contain
items whose sum of variances does not exceed a certain variance budget. We call
this probably anytime-safe constraint. Under this constraint, we design and
analyze an algorithm {\sc PASCombUCB} that minimizes the regret over the
horizon of time $T$. By developing accompanying information-theoretic lower
bounds, we show that under both the problem-dependent and problem-independent
paradigms, {\sc PASCombUCB} is almost asymptotically optimal. Experiments are
conducted to corroborate our theoretical findings. Our problem setup, the
proposed {\sc PASCombUCB} algorithm, and novel analyses are applicable to
domains such as recommendation systems and transportation in which an agent is
allowed to choose multiple items at a single time step and wishes to control
the risk over the whole time horizon.",2301.13393v2,https://arxiv.org/pdf/2301.13393v2
"Optimal Transport Perturbations for Safe Reinforcement Learning with
  Robustness Guarantees","James Queeney, Erhan Can Ozcan, Ioannis Ch. Paschalidis, Christos G. Cassandras","Robustness and safety are critical for the trustworthy deployment of deep
reinforcement learning. Real-world decision making applications require
algorithms that can guarantee robust performance and safety in the presence of
general environment disturbances, while making limited assumptions on the data
collection process during training. In order to accomplish this goal, we
introduce a safe reinforcement learning framework that incorporates robustness
through the use of an optimal transport cost uncertainty set. We provide an
efficient implementation based on applying Optimal Transport Perturbations to
construct worst-case virtual state transitions, which does not impact data
collection during training and does not require detailed simulator access. In
experiments on continuous control tasks with safety constraints, our approach
demonstrates robust performance while significantly improving safety at
deployment time compared to standard safe reinforcement learning.",2301.13375v2,https://arxiv.org/pdf/2301.13375v2
"Risk-Averse Model Uncertainty for Distributionally Robust Safe
  Reinforcement Learning","James Queeney, Mouhacine Benosman","Many real-world domains require safe decision making in uncertain
environments. In this work, we introduce a deep reinforcement learning
framework for approaching this important problem. We consider a distribution
over transition models, and apply a risk-averse perspective towards model
uncertainty through the use of coherent distortion risk measures. We provide
robustness guarantees for this framework by showing it is equivalent to a
specific class of distributionally robust safe reinforcement learning problems.
Unlike existing approaches to robustness in deep reinforcement learning,
however, our formulation does not involve minimax optimization. This leads to
an efficient, model-free implementation of our approach that only requires
standard data collection from a single training environment. In experiments on
continuous control tasks with safety constraints, we demonstrate that our
framework produces robust performance and safety at deployment time across a
range of perturbed test environments.",2301.12593v2,https://arxiv.org/pdf/2301.12593v2
"SaFormer: A Conditional Sequence Modeling Approach to Offline Safe
  Reinforcement Learning","Qin Zhang, Linrui Zhang, Haoran Xu, Li Shen, Bowen Wang, Yongzhe Chang, Xueqian Wang, Bo Yuan, Dacheng Tao","Offline safe RL is of great practical relevance for deploying agents in
real-world applications. However, acquiring constraint-satisfying policies from
the fixed dataset is non-trivial for conventional approaches. Even worse, the
learned constraints are stationary and may become invalid when the online
safety requirement changes. In this paper, we present a novel offline safe RL
approach referred to as SaFormer, which tackles the above issues via
conditional sequence modeling. In contrast to existing sequence models, we
propose cost-related tokens to restrict the action space and a posterior safety
verification to enforce the constraint explicitly. Specifically, SaFormer
performs a two-stage auto-regression conditioned by the maximum remaining cost
to generate feasible candidates. It then filters out unsafe attempts and
executes the optimal action with the highest expected return. Extensive
experiments demonstrate the efficacy of SaFormer featuring (1) competitive
returns with tightened constraint satisfaction; (2) adaptability to the
in-range cost values of the offline data without retraining; (3)
generalizability for constraints beyond the current dataset.",2301.12203v1,https://arxiv.org/pdf/2301.12203v1
"Safe Posterior Sampling for Constrained MDPs with Bounded Constraint
  Violation","Krishna C Kalagarla, Rahul Jain, Pierluigi Nuzzo","Constrained Markov decision processes (CMDPs) model scenarios of sequential
decision making with multiple objectives that are increasingly important in
many applications. However, the model is often unknown and must be learned
online while still ensuring the constraint is met, or at least the violation is
bounded with time. Some recent papers have made progress on this very
challenging problem but either need unsatisfactory assumptions such as
knowledge of a safe policy, or have high cumulative regret. We propose the Safe
PSRL (posterior sampling-based RL) algorithm that does not need such
assumptions and yet performs very well, both in terms of theoretical regret
bounds as well as empirically. The algorithm achieves an efficient tradeoff
between exploration and exploitation by use of the posterior sampling
principle, and provably suffers only bounded constraint violation by leveraging
the idea of pessimism. Our approach is based on a primal-dual approach. We
establish a sub-linear $\tilde{\mathcal{ O}}\left(H^{2.5} \sqrt{|\mathcal{S}|^2
|\mathcal{A}| K} \right)$ upper bound on the Bayesian reward objective regret
along with a bounded, i.e., $\tilde{\mathcal{O}}\left(1\right)$ constraint
violation regret over $K$ episodes for an $|\mathcal{S}|$-state,
$|\mathcal{A}|$-action and horizon $H$ CMDP.",2301.11547v1,https://arxiv.org/pdf/2301.11547v1
"Trust Region-Based Safe Distributional Reinforcement Learning for
  Multiple Constraints","Dohyeong Kim, Kyungjae Lee, Songhwai Oh","In safety-critical robotic tasks, potential failures must be reduced, and
multiple constraints must be met, such as avoiding collisions, limiting energy
consumption, and maintaining balance. Thus, applying safe reinforcement
learning (RL) in such robotic tasks requires to handle multiple constraints and
use risk-averse constraints rather than risk-neutral constraints. To this end,
we propose a trust region-based safe RL algorithm for multiple constraints
called a safe distributional actor-critic (SDAC). Our main contributions are as
follows: 1) introducing a gradient integration method to manage infeasibility
issues in multi-constrained problems, ensuring theoretical convergence, and 2)
developing a TD($\lambda$) target distribution to estimate risk-averse
constraints with low biases. We evaluate SDAC through extensive experiments
involving multi- and single-constrained robotic tasks. While maintaining high
scores, SDAC shows 1.93 times fewer steps to satisfy all constraints in
multi-constrained tasks and 1.78 times fewer constraint violations in
single-constrained tasks compared to safe RL baselines. Code is available at:
https://github.com/rllab-snu/Safe-Distributional-Actor-Critic.",2301.10923v2,https://arxiv.org/pdf/2301.10923v2
"Safety Verification of Neural Network Control Systems Using Guaranteed
  Neural Network Model Reduction","Weiming Xiang, Zhongzhu Shao","This paper aims to enhance the computational efficiency of safety
verification of neural network control systems by developing a guaranteed
neural network model reduction method. First, a concept of model reduction
precision is proposed to describe the guaranteed distance between the outputs
of a neural network and its reduced-size version. A reachability-based
algorithm is proposed to accurately compute the model reduction precision.
Then, by substituting a reduced-size neural network controller into the
closed-loop system, an algorithm to compute the reachable set of the original
system is developed, which is able to support much more computationally
efficient safety verification processes. Finally, the developed methods are
applied to a case study of the Adaptive Cruise Control system with a neural
network controller, which is shown to significantly reduce the computational
time of safety verification and thus validate the effectiveness of the method.",2301.07531v1,https://arxiv.org/pdf/2301.07531v1
"(Safe) SMART Hands: Hand Activity Analysis and Distraction Alerts Using
  a Multi-Camera Framework","Ross Greer, Lulua Rakla, Anish Gopalan, Mohan Trivedi","Manual (hand-related) activity is a significant source of crash risk while
driving. Accordingly, analysis of hand position and hand activity occupation is
a useful component to understanding a driver's readiness to take control of a
vehicle. Visual sensing through cameras provides a passive means of observing
the hands, but its effectiveness varies depending on camera location. We
introduce an algorithmic framework, SMART Hands, for accurate hand
classification with an ensemble of camera views using machine learning. We
illustrate the effectiveness of this framework in a 4-camera setup, reaching
98% classification accuracy on a variety of locations and held objects for both
of the driver's hands. We conclude that this multi-camera framework can be
extended to additional tasks such as gaze and pose analysis, with further
applications in driver and passenger safety.",2301.05838v3,https://arxiv.org/pdf/2301.05838v3
"Safe Control Transitions: Machine Vision Based Observable Readiness
  Index and Data-Driven Takeover Time Prediction","Ross Greer, Nachiket Deo, Akshay Rangesh, Pujitha Gunaratne, Mohan Trivedi","To make safe transitions from autonomous to manual control, a vehicle must
have a representation of the awareness of driver state; two metrics which
quantify this state are the Observable Readiness Index and Takeover Time. In
this work, we show that machine learning models which predict these two metrics
are robust to multiple camera views, expanding from the limited view angles in
prior research. Importantly, these models take as input feature vectors
corresponding to hand location and activity as well as gaze location, and we
explore the tradeoffs of different views in generating these feature vectors.
Further, we introduce two metrics to evaluate the quality of control
transitions following the takeover event (the maximal lateral deviation and
velocity deviation) and compute correlations of these post-takeover metrics to
the pre-takeover predictive metrics.",2301.05805v2,https://arxiv.org/pdf/2301.05805v2
"Salient Sign Detection In Safe Autonomous Driving: AI Which Reasons Over
  Full Visual Context","Ross Greer, Akshay Gopalkrishnan, Nachiket Deo, Akshay Rangesh, Mohan Trivedi","Detecting road traffic signs and accurately determining how they can affect
the driver's future actions is a critical task for safe autonomous driving
systems. However, various traffic signs in a driving scene have an unequal
impact on the driver's decisions, making detecting the salient traffic signs a
more important task. Our research addresses this issue, constructing a traffic
sign detection model which emphasizes performance on salient signs, or signs
that influence the decisions of a driver. We define a traffic sign salience
property and use it to construct the LAVA Salient Signs Dataset, the first
traffic sign dataset that includes an annotated salience property. Next, we use
a custom salience loss function, Salience-Sensitive Focal Loss, to train a
Deformable DETR object detection model in order to emphasize stronger
performance on salient signs. Results show that a model trained with
Salience-Sensitive Focal Loss outperforms a model trained without, with regards
to recall of both salient signs and all signs combined. Further, the
performance margin on salient signs compared to all signs is largest for the
model trained with Salience-Sensitive Focal Loss.",2301.05804v2,https://arxiv.org/pdf/2301.05804v2
"Risk Sensitive Dead-end Identification in Safety-Critical Offline
  Reinforcement Learning","Taylor W. Killian, Sonali Parbhoo, Marzyeh Ghassemi","In safety-critical decision-making scenarios being able to identify
worst-case outcomes, or dead-ends is crucial in order to develop safe and
reliable policies in practice. These situations are typically rife with
uncertainty due to unknown or stochastic characteristics of the environment as
well as limited offline training data. As a result, the value of a decision at
any time point should be based on the distribution of its anticipated effects.
We propose a framework to identify worst-case decision points, by explicitly
estimating distributions of the expected return of a decision. These estimates
enable earlier indication of dead-ends in a manner that is tunable based on the
risk tolerance of the designed task. We demonstrate the utility of
Distributional Dead-end Discovery (DistDeD) in a toy domain as well as when
assessing the risk of severely ill patients in the intensive care unit reaching
a point where death is unavoidable. We find that DistDeD significantly improves
over prior discovery approaches, providing indications of the risk 10 hours
earlier on average as well as increasing detection by 20%.",2301.05664v2,https://arxiv.org/pdf/2301.05664v2
"Natural Language Processing of Aviation Occurrence Reports for Safety
  Management","Patrick Jonk, Vincent de Vries, Rombout Wever, Georgios Sidiropoulos, Evangelos Kanoulas","Occurrence reporting is a commonly used method in safety management systems
to obtain insight in the prevalence of hazards and accident scenarios. In
support of safety data analysis, reports are often categorized according to a
taxonomy. However, the processing of the reports can require significant effort
from safety analysts and a common problem is interrater variability in labeling
processes. Also, in some cases, reports are not processed according to a
taxonomy, or the taxonomy does not fully cover the contents of the documents.
This paper explores various Natural Language Processing (NLP) methods to
support the analysis of aviation safety occurrence reports. In particular, the
problems studied are the automatic labeling of reports using a classification
model, extracting the latent topics in a collection of texts using a topic
model and the automatic generation of probable cause texts. Experimental
results showed that (i) under the right conditions the labeling of occurrence
reports can be effectively automated with a transformer-based classifier, (ii)
topic modeling can be useful for finding the topics present in a collection of
reports, and (iii) using a summarization model can be a promising direction for
generating probable cause texts.",2301.05663v1,https://arxiv.org/pdf/2301.05663v1
Safe Policy Improvement for POMDPs via Finite-State Controllers,"Thiago D. Simão, Marnix Suilen, Nils Jansen","We study safe policy improvement (SPI) for partially observable Markov
decision processes (POMDPs). SPI is an offline reinforcement learning (RL)
problem that assumes access to (1) historical data about an environment, and
(2) the so-called behavior policy that previously generated this data by
interacting with the environment. SPI methods neither require access to a model
nor the environment itself, and aim to reliably improve the behavior policy in
an offline manner. Existing methods make the strong assumption that the
environment is fully observable. In our novel approach to the SPI problem for
POMDPs, we assume that a finite-state controller (FSC) represents the behavior
policy and that finite memory is sufficient to derive optimal policies. This
assumption allows us to map the POMDP to a finite-state fully observable MDP,
the history MDP. We estimate this MDP by combining the historical data and the
memory of the FSC, and compute an improved policy using an off-the-shelf SPI
algorithm. The underlying SPI method constrains the policy-space according to
the available data, such that the newly computed policy only differs from the
behavior policy when sufficient data was available. We show that this new
policy, converted into a new FSC for the (unknown) POMDP, outperforms the
behavior policy with high probability. Experimental results on several
well-established benchmarks show the applicability of the approach, even in
cases where finite memory is not sufficient.",2301.04939v1,https://arxiv.org/pdf/2301.04939v1
Architecting Safer Autonomous Aviation Systems,"Jane Fenn, Mark Nicholson, Ganesh Pai, Michael Wilkinson","The aviation literature gives relatively little guidance to practitioners
about the specifics of architecting systems for safety, particularly the impact
of architecture on allocating safety requirements, or the relative ease of
system assurance resulting from system or subsystem level architectural
choices. As an exemplar, this paper considers common architectural patterns
used within traditional aviation systems and explores their safety and safety
assurance implications when applied in the context of integrating artificial
intelligence (AI) and machine learning (ML) based functionality. Considering
safety as an architectural property, we discuss both the allocation of safety
requirements and the architectural trade-offs involved early in the design
lifecycle. This approach could be extended to other assured properties, similar
to safety, such as security. We conclude with a discussion of the safety
considerations that emerge in the context of candidate architectural patterns
that have been proposed in the recent literature for enabling autonomy
capabilities by integrating AI and ML. A recommendation is made for the
generation of a property-driven architectural pattern catalogue.",2301.08138v1,https://arxiv.org/pdf/2301.08138v1
"Safer Together: Machine Learning Models Trained on Shared Accident
  Datasets Predict Construction Injuries Better than Company-Specific Models","Antoine J. -P. Tixier, Matthew R. Hallowell","In this study, we capitalized on a collective dataset repository of 57k
accidents from 9 companies belonging to 3 domains and tested whether models
trained on multiple datasets (generic models) predicted safety outcomes better
than the company-specific models. We experimented with full generic models
(trained on all data), per-domain generic models (construction, electric T&D,
oil & gas), and with ensembles of generic and specific models. Results are very
positive, with generic models outperforming the company-specific models in most
cases while also generating finer-grained, hence more useful, forecasts.
Successful generic models remove the needs for training company-specific
models, saving a lot of time and resources, and give small companies, whose
accident datasets are too limited to train their own models, access to safety
outcome predictions. It may still however be advantageous to train specific
models to get an extra boost in performance through ensembling with the generic
models. Overall, by learning lessons from a pool of datasets whose accumulated
experience far exceeds that of any single company, and making these lessons
easily accessible in the form of simple forecasts, generic models tackle the
holy grail of safety cross-organizational learning and dissemination in the
construction industry.",2301.03567v1,https://arxiv.org/pdf/2301.03567v1
"Safe Reinforcement Learning for an Energy-Efficient Driver Assistance
  System","Habtamu Hailemichael, Beshah Ayalew, Lindsey Kerbel, Andrej Ivanco, Keith Loiselle","Reinforcement learning (RL)-based driver assistance systems seek to improve
fuel consumption via continual improvement of powertrain control actions
considering experiential data from the field. However, the need to explore
diverse experiences in order to learn optimal policies often limits the
application of RL techniques in safety-critical systems like vehicle control.
In this paper, an exponential control barrier function (ECBF) is derived and
utilized to filter unsafe actions proposed by an RL-based driver assistance
system. The RL agent freely explores and optimizes the performance objectives
while unsafe actions are projected to the closest actions in the safe domain.
The reward is structured so that driver's acceleration requests are met in a
manner that boosts fuel economy and doesn't compromise comfort. The optimal
gear and traction torque control actions that maximize the cumulative reward
are computed via the Maximum a Posteriori Policy Optimization (MPO) algorithm
configured for a hybrid action space. The proposed safe-RL scheme is trained
and evaluated in car following scenarios where it is shown that it effectively
avoids collision both during training and evaluation while delivering on the
expected fuel economy improvements for the driver assistance system.",2301.00904v1,https://arxiv.org/pdf/2301.00904v1
"SAFEMYRIDES: Application of Decentralized Control Edge-Computing to
  Ridesharing Monitoring Services","Samaa Elnagar, Manoj A. Thomas, Kweku-Muata Osei-Bryson","Edge computing is changing the face of many industries and services. Common
edge computing models offload computing which is prone to security risks and
privacy violation. However, advances in deep learning enabled Internet of
Things (IoTs) to take decisions and run cognitive tasks locally. This research
introduces a decentralized-control edge model where most computation and
decisions are moved to the IoT level. The model aims at decreasing
communication to the edge which in return enhances efficiency and decreases
latency. The model also avoids data transfer which raises security and privacy
risks. To examine the model, we developed SAFEMYRIDES, a scene-aware
ridesharing monitoring system where smart phones are detecting violations at
the runtime. Current real-time monitoring systems are costly and require
continuous network connectivity. The system uses optimized deep learning that
run locally on IoTs to detect violations in ridesharing and record violation
incidences. The system would enhance safety and security in ridesharing without
violating privacy.",2301.00888v1,https://arxiv.org/pdf/2301.00888v1
"Hybrid Deep Reinforcement Learning and Planning for Safe and Comfortable
  Automated Driving","Dikshant Gupta, Mathias Klusch","We present a novel hybrid learning method, HyLEAR, for solving the
collision-free navigation problem for self-driving cars in POMDPs. HyLEAR
leverages interposed learning to embed knowledge of a hybrid planner into a
deep reinforcement learner to faster determine safe and comfortable driving
policies. In particular, the hybrid planner combines pedestrian path prediction
and risk-aware path planning with driving-behavior rule-based reasoning such
that the driving policies also take into account, whenever possible, the ride
comfort and a given set of driving-behavior rules. Our experimental performance
analysis over the CARLA-CTS1 benchmark of critical traffic scenarios revealed
that HyLEAR can significantly outperform the selected baselines in terms of
safety and ride comfort.",2301.00650v1,https://arxiv.org/pdf/2301.00650v1
Safe Subgame Resolving for Extensive Form Correlated Equilibrium,"Chun Kai Ling, Fei Fang","Correlated Equilibrium is a solution concept that is more general than Nash
Equilibrium (NE) and can lead to outcomes with better social welfare. However,
its natural extension to the sequential setting, the \textit{Extensive Form
Correlated Equilibrium} (EFCE), requires a quadratic amount of space to solve,
even in restricted settings without randomness in nature. To alleviate these
concerns, we apply \textit{subgame resolving}, a technique extremely successful
in finding NE in zero-sum games to solving general-sum EFCEs. Subgame resolving
refines a correlation plan in an \textit{online} manner: instead of solving for
the full game upfront, it only solves for strategies in subgames that are
reached in actual play, resulting in significant computational gains. In this
paper, we (i) lay out the foundations to quantify the quality of a refined
strategy, in terms of the \textit{social welfare} and \textit{exploitability}
of correlation plans, (ii) show that EFCEs possess a sufficient amount of
independence between subgames to perform resolving efficiently, and (iii)
provide two algorithms for resolving, one using linear programming and the
other based on regret minimization. Both methods guarantee \textit{safety},
i.e., they will never be counterproductive. Our methods are the first time an
online method has been applied to the correlated, general-sum setting.",2212.14317v1,https://arxiv.org/pdf/2212.14317v1
"Certifying Safety in Reinforcement Learning under Adversarial
  Perturbation Attacks","Junlin Wu, Hussein Sibai, Yevgeniy Vorobeychik","Function approximation has enabled remarkable advances in applying
reinforcement learning (RL) techniques in environments with high-dimensional
inputs, such as images, in an end-to-end fashion, mapping such inputs directly
to low-level control. Nevertheless, these have proved vulnerable to small
adversarial input perturbations. A number of approaches for improving or
certifying robustness of end-to-end RL to adversarial perturbations have
emerged as a result, focusing on cumulative reward. However, what is often at
stake in adversarial scenarios is the violation of fundamental properties, such
as safety, rather than the overall reward that combines safety with efficiency.
Moreover, properties such as safety can only be defined with respect to true
state, rather than the high-dimensional raw inputs to end-to-end policies. To
disentangle nominal efficiency and adversarial safety, we situate RL in
deterministic partially-observable Markov decision processes (POMDPs) with the
goal of maximizing cumulative reward subject to safety constraints. We then
propose a partially-supervised reinforcement learning (PSRL) framework that
takes advantage of an additional assumption that the true state of the POMDP is
known at training time. We present the first approach for certifying safety of
PSRL policies under adversarial input perturbations, and two adversarial
training approaches that make direct use of PSRL. Our experiments demonstrate
both the efficacy of the proposed approach for certifying safety in adversarial
environments, and the value of the PSRL framework coupled with adversarial
training in improving certified safety while preserving high nominal reward and
high-quality predictions of true state.",2212.14115v1,https://arxiv.org/pdf/2212.14115v1
Don't do it: Safer Reinforcement Learning With Rule-based Guidance,"Ekaterina Nikonova, Cheng Xue, Jochen Renz","During training, reinforcement learning systems interact with the world
without considering the safety of their actions. When deployed into the real
world, such systems can be dangerous and cause harm to their surroundings.
Often, dangerous situations can be mitigated by defining a set of rules that
the system should not violate under any conditions. For example, in robot
navigation, one safety rule would be to avoid colliding with surrounding
objects and people. In this work, we define safety rules in terms of the
relationships between the agent and objects and use them to prevent
reinforcement learning systems from performing potentially harmful actions. We
propose a new safe epsilon-greedy algorithm that uses safety rules to override
agents' actions if they are considered to be unsafe. In our experiments, we
show that a safe epsilon-greedy policy significantly increases the safety of
the agent during training, improves the learning efficiency resulting in much
faster convergence, and achieves better performance than the base model.",2212.13819v1,https://arxiv.org/pdf/2212.13819v1
"Understanding Ethics, Privacy, and Regulations in Smart Video
  Surveillance for Public Safety","Babak Rahimi Ardabili, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Christopher Neff, Arun Ravindran, Hamed Tabkhi","Recently, Smart Video Surveillance (SVS) systems have been receiving more
attention among scholars and developers as a substitute for the current passive
surveillance systems. These systems are used to make the policing and
monitoring systems more efficient and improve public safety. However, the
nature of these systems in monitoring the public's daily activities brings
different ethical challenges. There are different approaches for addressing
privacy issues in implementing the SVS. In this paper, we are focusing on the
role of design considering ethical and privacy challenges in SVS. Reviewing
four policy protection regulations that generate an overview of best practices
for privacy protection, we argue that ethical and privacy concerns could be
addressed through four lenses: algorithm, system, model, and data. As an case
study, we describe our proposed system and illustrate how our system can create
a baseline for designing a privacy perseverance system to deliver safety to
society. We used several Artificial Intelligence algorithms, such as object
detection, single and multi camera re-identification, action recognition, and
anomaly detection, to provide a basic functional system. We also use
cloud-native services to implement a smartphone application in order to deliver
the outputs to the end users.",2212.12936v1,https://arxiv.org/pdf/2212.12936v1
"Context-Aware Target Classification with Hybrid Gaussian Process
  prediction for Cooperative Vehicle Safety systems","Rodolfo Valiente, Arash Raftari, Hossein Nourkhiz Mahjoub, Mahdi Razzaghpour, Syed K. Mahmud, Yaser P. Fallah","Vehicle-to-Everything (V2X) communication has been proposed as a potential
solution to improve the robustness and safety of autonomous vehicles by
improving coordination and removing the barrier of non-line-of-sight sensing.
Cooperative Vehicle Safety (CVS) applications are tightly dependent on the
reliability of the underneath data system, which can suffer from loss of
information due to the inherent issues of their different components, such as
sensors failures or the poor performance of V2X technologies under dense
communication channel load. Particularly, information loss affects the target
classification module and, subsequently, the safety application performance. To
enable reliable and robust CVS systems that mitigate the effect of information
loss, we proposed a Context-Aware Target Classification (CA-TC) module coupled
with a hybrid learning-based predictive modeling technique for CVS systems. The
CA-TC consists of two modules: A Context-Aware Map (CAM), and a Hybrid Gaussian
Process (HGP) prediction system. Consequently, the vehicle safety applications
use the information from the CA-TC, making them more robust and reliable. The
CAM leverages vehicles path history, road geometry, tracking, and prediction;
and the HGP is utilized to provide accurate vehicles' trajectory predictions to
compensate for data loss (due to communication congestion) or sensor
measurements' inaccuracies. Based on offline real-world data, we learn a finite
bank of driver models that represent the joint dynamics of the vehicle and the
drivers' behavior. We combine offline training and online model updates with
on-the-fly forecasting to account for new possible driver behaviors. Finally,
our framework is validated using simulation and realistic driving scenarios to
confirm its potential in enhancing the robustness and reliability of CVS
systems.",2212.12819v1,https://arxiv.org/pdf/2212.12819v1
"Creating awareness about security and safety on highways to mitigate
  wildlife-vehicle collisions by detecting and recognizing wildlife fences
  using deep learning and drone technology","Irene Nandutu, Marcellin Atemkeng, Patrice Okouma, Nokubonga Mgqatsa, Jean Louis Ebongue Kedieng Fendji, Franklin Tchakounte","In South Africa, it is a common practice for people to leave their vehicles
beside the road when traveling long distances for a short comfort break. This
practice might increase human encounters with wildlife, threatening their
security and safety. Here we intend to create awareness about wildlife fencing,
using drone technology and computer vision algorithms to recognize and detect
wildlife fences and associated features. We collected data at Amakhala and
Lalibela private game reserves in the Eastern Cape, South Africa. We used
wildlife electric fence data containing single and double fences for the
classification task. Additionally, we used aerial and still annotated images
extracted from the drone and still cameras for the segmentation and detection
tasks. The model training results from the drone camera outperformed those from
the still camera. Generally, poor model performance is attributed to (1)
over-decompression of images and (2) the ability of drone cameras to capture
more details on images for the machine learning model to learn as compared to
still cameras that capture only the front view of the wildlife fence. We argue
that our model can be deployed on client-edge devices to inform people about
the presence and significance of wildlife fencing, which minimizes human
encounters with wildlife, thereby mitigating wildlife-vehicle collisions.",2301.07174v1,https://arxiv.org/pdf/2301.07174v1
Evaluating Psychological Safety of Large Language Models,"Xingxuan Li, Yutong Li, Lin Qiu, Shafiq Joty, Lidong Bing","In this work, we designed unbiased prompts to systematically evaluate the
psychological safety of large language models (LLMs). First, we tested five
different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big
Five Inventory (BFI). All models scored higher than the human average on SD-3,
suggesting a relatively darker personality pattern. Despite being instruction
fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and
GPT-4 still showed dark personality patterns; these models scored higher than
self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3.
Then, we evaluated the LLMs in the GPT series by using well-being tests to
study the impact of fine-tuning with more training data. We observed a
continuous increase in the well-being scores of GPT models. Following these
observations, we showed that fine-tuning Llama-2-chat-7B with responses from
BFI using direct preference optimization could effectively reduce the
psychological toxicity of the model. Based on the findings, we recommended the
application of systematic and comprehensive psychological metrics to further
evaluate and improve the safety of LLMs.",2212.10529v3,https://arxiv.org/pdf/2212.10529v3
"Foveate, Attribute, and Rationalize: Towards Physically Safe and
  Trustworthy AI","Alex Mei, Sharon Levy, William Yang Wang","Users' physical safety is an increasing concern as the market for intelligent
systems continues to grow, where unconstrained systems may recommend users
dangerous actions that can lead to serious injury. Covertly unsafe text is an
area of particular interest, as such text may arise from everyday scenarios and
are challenging to detect as harmful. We propose FARM, a novel framework
leveraging external knowledge for trustworthy rationale generation in the
context of safety. In particular, FARM foveates on missing knowledge to qualify
the information required to reason in specific scenarios and retrieves this
information with attribution to trustworthy sources. This knowledge is used to
both classify the safety of the original text and generate human-interpretable
rationales, shedding light on the risk of systems to specific user groups and
helping both stakeholders manage the risks of their systems and policymakers to
provide concrete safeguards for consumer safety. Our experiments show that FARM
obtains state-of-the-art results on the SafeText dataset, showing absolute
improvement in safety classification accuracy by 5.9%.",2212.09667v2,https://arxiv.org/pdf/2212.09667v2
"A Review of Speech-centric Trustworthy Machine Learning: Privacy,
  Safety, and Fairness","Tiantian Feng, Rajat Hebbar, Nicholas Mehlman, Xuan Shi, Aditya Kommineni, and Shrikanth Narayanan","Speech-centric machine learning systems have revolutionized many leading
domains ranging from transportation and healthcare to education and defense,
profoundly changing how people live, work, and interact with each other.
However, recent studies have demonstrated that many speech-centric ML systems
may need to be considered more trustworthy for broader deployment.
Specifically, concerns over privacy breaches, discriminating performance, and
vulnerability to adversarial attacks have all been discovered in ML research
fields. In order to address the above challenges and risks, a significant
number of efforts have been made to ensure these ML systems are trustworthy,
especially private, safe, and fair. In this paper, we conduct the first
comprehensive survey on speech-centric trustworthy ML topics related to
privacy, safety, and fairness. In addition to serving as a summary report for
the research community, we point out several promising future research
directions to inspire the researchers who wish to explore further in this area.",2212.09006v2,https://arxiv.org/pdf/2212.09006v2
"Cognitive Level-$k$ Meta-Learning for Safe and Pedestrian-Aware
  Autonomous Driving","Haozhe Lei, Quanyan Zhu","The potential market for modern self-driving cars is enormous, as they are
developing remarkably rapidly. At the same time, however, accidents of
pedestrian fatalities caused by autonomous driving have been recorded in the
case of street crossing. To ensure traffic safety in self-driving environments
and respond to vehicle-human interaction challenges such as jaywalking, we
propose Level-$k$ Meta Reinforcement Learning (LK-MRL) algorithm. It takes into
account the cognitive hierarchy of pedestrian responses and enables
self-driving vehicles to adapt to various human behaviors. %which takes into
account pedestrian responses while learning the optimal strategies. As a
self-driving vehicle algorithm, the LK-MRL combines level-$k$ thinking into
MAML to prepare for heterogeneous pedestrians and improve intersection safety
based on the combination of meta-reinforcement learning and human cognitive
hierarchy framework. We evaluate the algorithm in two cognitive confrontation
hierarchy scenarios in an urban traffic simulator and illustrate its role in
ensuring road safety by demonstrating its capability of conjectural and
higher-level reasoning.",2212.08800v3,https://arxiv.org/pdf/2212.08800v3
"Distribution-aware Goal Prediction and Conformant Model-based Planning
  for Safe Autonomous Driving","Jonathan Francis, Bingqing Chen, Weiran Yao, Eric Nyberg, Jean Oh","The feasibility of collecting a large amount of expert demonstrations has
inspired growing research interests in learning-to-drive settings, where models
learn by imitating the driving behaviour from experts. However, exclusively
relying on imitation can limit agents' generalisability to novel scenarios that
are outside the support of the training data. In this paper, we address this
challenge by factorising the driving task, based on the intuition that modular
architectures are more generalisable and more robust to changes in the
environment compared to monolithic, end-to-end frameworks. Specifically, we
draw inspiration from the trajectory forecasting community and reformulate the
learning-to-drive task as obstacle-aware perception and grounding,
distribution-aware goal prediction, and model-based planning. Firstly, we train
the obstacle-aware perception module to extract salient representation of the
visual context. Then, we learn a multi-modal goal distribution by performing
conditional density-estimation using normalising flow. Finally, we ground
candidate trajectory predictions road geometry, and plan the actions based on
on vehicle dynamics. Under the CARLA simulator, we report state-of-the-art
results on the CARNOVEL benchmark.",2212.08729v1,https://arxiv.org/pdf/2212.08729v1
Safe Evaluation For Offline Learning: Are We Ready To Deploy?,"Hager Radi, Josiah P. Hanna, Peter Stone, Matthew E. Taylor","The world currently offers an abundance of data in multiple domains, from
which we can learn reinforcement learning (RL) policies without further
interaction with the environment. RL agents learning offline from such data is
possible but deploying them while learning might be dangerous in domains where
safety is critical. Therefore, it is essential to find a way to estimate how a
newly-learned agent will perform if deployed in the target environment before
actually deploying it and without the risk of overestimating its true
performance. To achieve this, we introduce a framework for safe evaluation of
offline learning using approximate high-confidence off-policy evaluation
(HCOPE) to estimate the performance of offline policies during learning. In our
setting, we assume a source of data, which we split into a train-set, to learn
an offline policy, and a test-set, to estimate a lower-bound on the offline
policy using off-policy evaluation with bootstrapping. A lower-bound estimate
tells us how good a newly-learned target policy would perform before it is
deployed in the real environment, and therefore allows us to decide when to
deploy our learned policy.",2212.08302v1,https://arxiv.org/pdf/2212.08302v1
"Safety Correction from Baseline: Towards the Risk-aware Policy in
  Robotics via Dual-agent Reinforcement Learning","Linrui Zhang, Zichen Yan, Li Shen, Shoujie Li, Xueqian Wang, Dacheng Tao","Learning a risk-aware policy is essential but rather challenging in
unstructured robotic tasks. Safe reinforcement learning methods open up new
possibilities to tackle this problem. However, the conservative policy updates
make it intractable to achieve sufficient exploration and desirable performance
in complex, sample-expensive environments. In this paper, we propose a
dual-agent safe reinforcement learning strategy consisting of a baseline and a
safe agent. Such a decoupled framework enables high flexibility, data
efficiency and risk-awareness for RL-based control. Concretely, the baseline
agent is responsible for maximizing rewards under standard RL settings. Thus,
it is compatible with off-the-shelf training techniques of unconstrained
optimization, exploration and exploitation. On the other hand, the safe agent
mimics the baseline agent for policy improvement and learns to fulfill safety
constraints via off-policy RL tuning. In contrast to training from scratch,
safe policy correction requires significantly fewer interactions to obtain a
near-optimal policy. The dual policies can be optimized synchronously via a
shared replay buffer, or leveraging the pre-trained model or the
non-learning-based controller as a fixed baseline agent. Experimental results
show that our approach can learn feasible skills without prior knowledge as
well as deriving risk-averse counterparts from pre-trained unsafe policies. The
proposed method outperforms the state-of-the-art safe RL algorithms on
difficult robot locomotion and manipulation tasks with respect to both safety
constraint satisfaction and sample efficiency.",2212.06998v1,https://arxiv.org/pdf/2212.06998v1
"Statistical Safety and Robustness Guarantees for Feedback Motion
  Planning of Unknown Underactuated Stochastic Systems","Craig Knuth, Glen Chou, Jamie Reese, Joe Moore","We present a method for providing statistical guarantees on runtime safety
and goal reachability for integrated planning and control of a class of systems
with unknown nonlinear stochastic underactuated dynamics. Specifically, given a
dynamics dataset, our method jointly learns a mean dynamics model, a
spatially-varying disturbance bound that captures the effect of noise and model
mismatch, and a feedback controller based on contraction theory that stabilizes
the learned dynamics. We propose a sampling-based planner that uses the mean
dynamics model and simultaneously bounds the closed-loop tracking error via a
learned disturbance bound. We employ techniques from Extreme Value Theory (EVT)
to estimate, to a specified level of confidence, several constants which
characterize the learned components and govern the size of the tracking error
bound. This ensures plans are guaranteed to be safely tracked at runtime. We
validate that our guarantees translate to empirical safety in simulation on a
10D quadrotor, and in the real world on a physical CrazyFlie quadrotor and
Clearpath Jackal robot, whereas baselines that ignore the model error and
stochasticity are unsafe.",2212.06874v1,https://arxiv.org/pdf/2212.06874v1
"Despite ""super-human"" performance, current LLMs are unsuited for
  decisions about ethics and safety","Joshua Albrecht, Ellie Kitanidis, Abraham J. Fetterman","Large language models (LLMs) have exploded in popularity in the past few
years and have achieved undeniably impressive results on benchmarks as varied
as question answering and text summarization. We provide a simple new prompting
strategy that leads to yet another supposedly ""super-human"" result, this time
outperforming humans at common sense ethical reasoning (as measured by accuracy
on a subset of the ETHICS dataset). Unfortunately, we find that relying on
average performance to judge capabilities can be highly misleading. LLM errors
differ systematically from human errors in ways that make it easy to craft
adversarial examples, or even perturb existing examples to flip the output
label. We also observe signs of inverse scaling with model size on some
examples, and show that prompting models to ""explain their reasoning"" often
leads to alarming justifications of unethical actions. Our results highlight
how human-like performance does not necessarily imply human-like understanding
or reasoning.",2212.06295v1,https://arxiv.org/pdf/2212.06295v1
"Evaluating Model-free Reinforcement Learning toward Safety-critical
  Tasks","Linrui Zhang, Qin Zhang, Li Shen, Bo Yuan, Xueqian Wang, Dacheng Tao","Safety comes first in many real-world applications involving autonomous
agents. Despite a large number of reinforcement learning (RL) methods focusing
on safety-critical tasks, there is still a lack of high-quality evaluation of
those algorithms that adheres to safety constraints at each decision step under
complex and unknown dynamics. In this paper, we revisit prior work in this
scope from the perspective of state-wise safe RL and categorize them as
projection-based, recovery-based, and optimization-based approaches,
respectively. Furthermore, we propose Unrolling Safety Layer (USL), a joint
method that combines safety optimization and safety projection. This novel
technique explicitly enforces hard constraints via the deep unrolling
architecture and enjoys structural advantages in navigating the trade-off
between reward improvement and constraint satisfaction. To facilitate further
research in this area, we reproduce related algorithms in a unified pipeline
and incorporate them into SafeRL-Kit, a toolkit that provides off-the-shelf
interfaces and evaluation utilities for safety-critical tasks. We then perform
a comparative study of the involved algorithms on six benchmarks ranging from
robotic control to autonomous driving. The empirical results provide an insight
into their applicability and robustness in learning zero-cost-return policies
without task-dependent handcrafting. The project page is available at
https://sites.google.com/view/saferlkit.",2212.05727v1,https://arxiv.org/pdf/2212.05727v1
Information-Theoretic Safe Exploration with Gaussian Processes,"Alessandro G. Bottero, Carlos E. Luis, Julia Vinogradska, Felix Berkenkamp, Jan Peters","We consider a sequential decision making task where we are not allowed to
evaluate parameters that violate an a priori unknown (safety) constraint. A
common approach is to place a Gaussian process prior on the unknown constraint
and allow evaluations only in regions that are safe with high probability. Most
current methods rely on a discretization of the domain and cannot be directly
extended to the continuous case. Moreover, the way in which they exploit
regularity assumptions about the constraint introduces an additional critical
hyperparameter. In this paper, we propose an information-theoretic safe
exploration criterion that directly exploits the GP posterior to identify the
most informative safe parameters to evaluate. Our approach is naturally
applicable to continuous domains and does not require additional
hyperparameters. We theoretically analyze the method and show that we do not
violate the safety constraint with high probability and that we explore by
learning about the constraint up to arbitrary precision. Empirical evaluations
demonstrate improved data-efficiency and scalability.",2212.04914v1,https://arxiv.org/pdf/2212.04914v1
ISAACS: Iterative Soft Adversarial Actor-Critic for Safety,"Kai-Chieh Hsu, Duy Phuong Nguyen, Jaime Fernández Fisac","The deployment of robots in uncontrolled environments requires them to
operate robustly under previously unseen scenarios, like irregular terrain and
wind conditions. Unfortunately, while rigorous safety frameworks from robust
optimal control theory scale poorly to high-dimensional nonlinear dynamics,
control policies computed by more tractable ""deep"" methods lack guarantees and
tend to exhibit little robustness to uncertain operating conditions. This work
introduces a novel approach enabling scalable synthesis of robust
safety-preserving controllers for robotic systems with general nonlinear
dynamics subject to bounded modeling error by combining game-theoretic safety
analysis with adversarial reinforcement learning in simulation. Following a
soft actor-critic scheme, a safety-seeking fallback policy is co-trained with
an adversarial ""disturbance"" agent that aims to invoke the worst-case
realization of model error and training-to-deployment discrepancy allowed by
the designer's uncertainty. While the learned control policy does not
intrinsically guarantee safety, it is used to construct a real-time safety
filter (or shield) with robust safety guarantees based on forward reachability
rollouts. This shield can be used in conjunction with a safety-agnostic control
policy, precluding any task-driven actions that could result in loss of safety.
We evaluate our learning-based safety approach in a 5D race car simulator,
compare the learned safety policy to the numerically obtained optimal solution,
and empirically validate the robust safety guarantee of our proposed safety
shield against worst-case model discrepancy.",2212.03228v3,https://arxiv.org/pdf/2212.03228v3
"Safe Imitation Learning of Nonlinear Model Predictive Control for
  Flexible Robots","Shamil Mamedov, Rudolf Reiter, Seyed Mahdi Basiri Azad, Ruan Viljoen, Joschka Boedecker, Moritz Diehl, Jan Swevers","Flexible robots may overcome some of the industry's major challenges, such as
enabling intrinsically safe human-robot collaboration and achieving a higher
payload-to-mass ratio. However, controlling flexible robots is complicated due
to their complex dynamics, which include oscillatory behavior and a
high-dimensional state space. Nonlinear model predictive control (NMPC) offers
an effective means to control such robots, but its significant computational
demand often limits its application in real-time scenarios. To enable fast
control of flexible robots, we propose a framework for a safe approximation of
NMPC using imitation learning and a predictive safety filter. Our framework
significantly reduces computation time while incurring a slight loss in
performance. Compared to NMPC, our framework shows more than an eightfold
improvement in computation time when controlling a three-dimensional flexible
robot arm in simulation, all while guaranteeing safety constraints. Notably,
our approach outperforms state-of-the-art reinforcement learning methods. The
development of fast and safe approximate NMPC holds the potential to accelerate
the adoption of flexible robots in industry. The project code is available at:
tinyurl.com/anmpc4fr",2212.02941v3,https://arxiv.org/pdf/2212.02941v3
Safe Inverse Reinforcement Learning via Control Barrier Function,"Yue Yang, Letian Chen, Matthew Gombolay","Learning from Demonstration (LfD) is a powerful method for enabling robots to
perform novel tasks as it is often more tractable for a non-roboticist end-user
to demonstrate the desired skill and for the robot to efficiently learn from
the associated data than for a human to engineer a reward function for the
robot to learn the skill via reinforcement learning (RL). Safety issues arise
in modern LfD techniques, e.g., Inverse Reinforcement Learning (IRL), just as
they do for RL; yet, safe learning in LfD has received little attention. In the
context of agile robots, safety is especially vital due to the possibility of
robot-environment collision, robot-human collision, and damage to the robot. In
this paper, we propose a safe IRL framework, CBFIRL, that leverages the Control
Barrier Function (CBF) to enhance the safety of the IRL policy. The core idea
of CBFIRL is to combine a loss function inspired by CBF requirements with the
objective in an IRL method, both of which are jointly optimized via gradient
descent. In the experiments, we show our framework performs safer compared to
IRL methods without CBF, that is $\sim15\%$ and $\sim20\%$ improvement for two
levels of difficulty of a 2D racecar domain and $\sim 50\%$ improvement for a
3D drone domain.",2212.02753v2,https://arxiv.org/pdf/2212.02753v2
"Acceleration AI Ethics, the Debate between Innovation and Safety, and
  Stability AI's Diffusion versus OpenAI's Dall-E",James Brusseau,"One objection to conventional AI ethics is that it slows innovation. This
presentation responds by reconfiguring ethics as an innovation accelerator. The
critical elements develop from a contrast between Stability AI's Diffusion and
OpenAI's Dall-E. By analyzing the divergent values underlying their opposed
strategies for development and deployment, five conceptions are identified as
common to acceleration ethics. Uncertainty is understood as positive and
encouraging, rather than discouraging. Innovation is conceived as intrinsically
valuable, instead of worthwhile only as mediated by social effects. AI problems
are solved by more AI, not less. Permissions and restrictions governing AI
emerge from a decentralized process, instead of a unified authority. The work
of ethics is embedded in AI development and application, instead of functioning
from outside. Together, these attitudes and practices remake ethics as
provoking rather than restraining artificial intelligence.",2212.01834v2,https://arxiv.org/pdf/2212.01834v2
"Safe machine learning model release from Trusted Research Environments:
  The AI-SDC package","Jim Smith, Richard J. Preen, Andrew McCarthy, Alba Crespi-Boixader, James Liley, Simon Rogers","We present AI-SDC, an integrated suite of open source Python tools to
facilitate Statistical Disclosure Control (SDC) of Machine Learning (ML) models
trained on confidential data prior to public release. AI-SDC combines (i) a
SafeModel package that extends commonly used ML models to provide ante-hoc SDC
by assessing the vulnerability of disclosure posed by the training regime; and
(ii) an Attacks package that provides post-hoc SDC by rigorously assessing the
empirical disclosure risk of a model through a variety of simulated attacks
after training. The AI-SDC code and documentation are available under an MIT
license at https://github.com/AI-SDC/AI-SDC.",2212.01233v2,https://arxiv.org/pdf/2212.01233v2
"Safe Reinforcement Learning with Probabilistic Control Barrier Functions
  for Ramp Merging","Soumith Udatha, Yiwei Lyu, John Dolan","Prior work has looked at applying reinforcement learning and imitation
learning approaches to autonomous driving scenarios, but either the safety or
the efficiency of the algorithm is compromised. With the use of control barrier
functions embedded into the reinforcement learning policy, we arrive at safe
policies to optimize the performance of the autonomous driving vehicle.
However, control barrier functions need a good approximation of the model of
the car. We use probabilistic control barrier functions as an estimate of the
model uncertainty. The algorithm is implemented as an online version in the
CARLA (Dosovitskiy et al., 2017) Simulator and as an offline version on a
dataset extracted from the NGSIM Database. The proposed algorithm is not just a
safe ramp merging algorithm but a safe autonomous driving algorithm applied to
address ramp merging on highways.",2212.00618v1,https://arxiv.org/pdf/2212.00618v1
"Safe and Efficient Reinforcement Learning Using
  Disturbance-Observer-Based Control Barrier Functions","Yikun Cheng, Pan Zhao, Naira Hovakimyan","Safe reinforcement learning (RL) with assured satisfaction of hard state
constraints during training has recently received a lot of attention. Safety
filters, e.g., based on control barrier functions (CBFs), provide a promising
way for safe RL via modifying the unsafe actions of an RL agent on the fly.
Existing safety filter-based approaches typically involve learning of uncertain
dynamics and quantifying the learned model error, which leads to conservative
filters before a large amount of data is collected to learn a good model,
thereby preventing efficient exploration. This paper presents a method for safe
and efficient RL using disturbance observers (DOBs) and control barrier
functions (CBFs). Unlike most existing safe RL methods that deal with hard
state constraints, our method does not involve model learning, and leverages
DOBs to accurately estimate the pointwise value of the uncertainty, which is
then incorporated into a robust CBF condition to generate safe actions. The
DOB-based CBF can be used as a safety filter with model-free RL algorithms by
minimally modifying the actions of an RL agent whenever necessary to ensure
safety throughout the learning process. Simulation results on a unicycle and a
2D quadrotor demonstrate that the proposed method outperforms a
state-of-the-art safe RL algorithm using CBFs and Gaussian processes-based
model learning, in terms of safety violation rate, and sample and computational
efficiency.",2211.17250v3,https://arxiv.org/pdf/2211.17250v3
"TrustGAN: Training safe and trustworthy deep learning models through
  generative adversarial networks",Hélion du Mas des Bourboux,"Deep learning models have been developed for a variety of tasks and are
deployed every day to work in real conditions. Some of these tasks are critical
and models need to be trusted and safe, e.g. military communications or cancer
diagnosis. These models are given real data, simulated data or combination of
both and are trained to be highly predictive on them. However, gathering enough
real data or simulating them to be representative of all the real conditions
is: costly, sometimes impossible due to confidentiality and most of the time
impossible. Indeed, real conditions are constantly changing and sometimes are
intractable. A solution is to deploy machine learning models that are able to
give predictions when they are confident enough otherwise raise a flag or
abstain. One issue is that standard models easily fail at detecting
out-of-distribution samples where their predictions are unreliable.
  We present here TrustGAN, a generative adversarial network pipeline targeting
trustness. It is a deep learning pipeline which improves a target model
estimation of the confidence without impacting its predictive power. The
pipeline can accept any given deep learning model which outputs a prediction
and a confidence on this prediction. Moreover, the pipeline does not need to
modify this target model. It can thus be easily deployed in a MLOps (Machine
Learning Operations) setting.
  The pipeline is applied here to a target classification model trained on
MNIST data to recognise numbers based on images. We compare such a model when
trained in the standard way and with TrustGAN. We show that on
out-of-distribution samples, here FashionMNIST and CIFAR10, the estimated
confidence is largely reduced. We observe similar conclusions for a
classification model trained on 1D radio signals from AugMod, tested on
RML2016.04C. We also publicly release the code.",2211.13991v1,https://arxiv.org/pdf/2211.13991v1
Explainable and Safe Reinforcement Learning for Autonomous Air Mobility,"Lei Wang, Hongyu Yang, Yi Lin, Suwan Yin, Yuankai Wu","Increasing traffic demands, higher levels of automation, and communication
enhancements provide novel design opportunities for future air traffic
controllers (ATCs). This article presents a novel deep reinforcement learning
(DRL) controller to aid conflict resolution for autonomous free flight.
Although DRL has achieved important advancements in this field, the existing
works pay little attention to the explainability and safety issues related to
DRL controllers, particularly the safety under adversarial attacks. To address
those two issues, we design a fully explainable DRL framework wherein we: 1)
decompose the coupled Q value learning model into a safety-awareness and
efficiency (reach the target) one; and 2) use information from surrounding
intruders as inputs, eliminating the needs of central controllers. In our
simulated experiments, we show that by decoupling the safety-awareness and
efficiency, we can exceed performance on free flight control tasks while
dramatically improving explainability on practical. In addition, the safety Q
learning module provides rich information about the safety situation of
environments. To study the safety under adversarial attacks, we additionally
propose an adversarial attack strategy that can impose both safety-oriented and
efficiency-oriented attacks. The adversarial aims to minimize safety/efficiency
by only attacking the agent at a few time steps. In the experiments, our attack
strategy increases as many collisions as the uniform attack (i.e., attacking at
every time step) by only attacking the agent four times less often, which
provide insights into the capabilities and restrictions of the DRL in future
ATC designs. The source code is publicly available at
https://github.com/WLeiiiii/Gym-ATC-Attack-Project.",2211.13474v1,https://arxiv.org/pdf/2211.13474v1
"Towards Developing Safety Assurance Cases for Learning-Enabled Medical
  Cyber-Physical Systems","Maryam Bagheri, Josephine Lamp, Xugui Zhou, Lu Feng, Homa Alemzadeh","Machine Learning (ML) technologies have been increasingly adopted in Medical
Cyber-Physical Systems (MCPS) to enable smart healthcare. Assuring the safety
and effectiveness of learning-enabled MCPS is challenging, as such systems must
account for diverse patient profiles and physiological dynamics and handle
operational uncertainties. In this paper, we develop a safety assurance case
for ML controllers in learning-enabled MCPS, with an emphasis on establishing
confidence in the ML-based predictions. We present the safety assurance case in
detail for Artificial Pancreas Systems (APS) as a representative application of
learning-enabled MCPS, and provide a detailed analysis by implementing a deep
neural network for the prediction in APS. We check the sufficiency of the ML
data and analyze the correctness of the ML-based prediction using formal
verification. Finally, we outline open research problems based on our
experience in this paper.",2211.15413v2,https://arxiv.org/pdf/2211.15413v2
Safety Analysis of Autonomous Driving Systems Based on Model Learning,"Renjue Li, Tianhang Qin, Pengfei Yang, Cheng-Chao Huang, Youcheng Sun, Lijun Zhang","We present a practical verification method for safety analysis of the
autonomous driving system (ADS). The main idea is to build a surrogate model
that quantitatively depicts the behaviour of an ADS in the specified traffic
scenario. The safety properties proved in the resulting surrogate model apply
to the original ADS with a probabilistic guarantee. Furthermore, we explore the
safe and the unsafe parameter space of the traffic scenario for driving
hazards. We demonstrate the utility of the proposed approach by evaluating
safety properties on the state-of-the-art ADS in literature, with a variety of
simulated traffic scenarios.",2211.12733v1,https://arxiv.org/pdf/2211.12733v1
Safe Control and Learning Using Generalized Action Governor,"Nan Li, Yutong Li, Ilya Kolmanovsky, Anouck Girard, H. Eric Tseng, Dimitar Filev","This paper introduces the Generalized Action Governor, which is a supervisory
scheme for augmenting a nominal closed-loop system with the capability of
strictly handling constraints. After presenting its theory for general systems
and introducing tailored design approaches for linear and discrete systems, we
discuss its application to safe online learning, which aims to safely evolve
control parameters using real-time data to improve performance for uncertain
systems. In particular, we propose two safe learning algorithms based on
integration of reinforcement learning/data-driven Koopman operator-based
control with the generalized action governor. The developments are illustrated
with a numerical example.",2211.12628v1,https://arxiv.org/pdf/2211.12628v1
"Safe Optimization of an Industrial Refrigeration Process Using an
  Adaptive and Explorative Framework","Buse Sibel Korkmaz, Marta Zagórowska, Mehmet Mercangöz","Many industrial applications rely on real-time optimization to improve key
performance indicators. In the case of unknown process characteristics,
real-time optimization becomes challenging, particularly for the satisfaction
of safety constraints. In this paper, we demonstrate the application of an
adaptive and explorative real-time optimization framework to an industrial
refrigeration process, where we learn the process characteristics through
changes in process control targets and through exploration to satisfy safety
constraints. We quantify the uncertainty in unknown compressor characteristics
of the refrigeration plant by using Gaussian processes and incorporate this
uncertainty into the objective function of the real-time optimization problem
as a weighted cost term. We adaptively control the weight of this term to drive
exploration. The results of our simulation experiments indicate the proposed
approach can help to increase the energy efficiency of the considered
refrigeration process, closely approximating the performance of a solution that
has complete information about the compressor performance characteristics.",2211.13019v2,https://arxiv.org/pdf/2211.13019v2
Safe Control Under Input Limits with Neural Control Barrier Functions,"Simin Liu, Changliu Liu, John Dolan","We propose new methods to synthesize control barrier function (CBF)-based
safe controllers that avoid input saturation, which can cause safety
violations. In particular, our method is created for high-dimensional, general
nonlinear systems, for which such tools are scarce. We leverage techniques from
machine learning, like neural networks and deep learning, to simplify this
challenging problem in nonlinear control design. The method consists of a
learner-critic architecture, in which the critic gives counterexamples of input
saturation and the learner optimizes a neural CBF to eliminate those
counterexamples. We provide empirical results on a 10D state, 4D input
quadcopter-pendulum system. Our learned CBF avoids input saturation and
maintains safety over nearly 100% of trials.",2211.11056v1,https://arxiv.org/pdf/2211.11056v1
Algorithmic Decision-Making Safeguarded by Human Knowledge,"Ningyuan Chen, Ming Hu, Wenhao Li","Commercial AI solutions provide analysts and managers with data-driven
business intelligence for a wide range of decisions, such as demand forecasting
and pricing. However, human analysts may have their own insights and
experiences about the decision-making that is at odds with the algorithmic
recommendation. In view of such a conflict, we provide a general analytical
framework to study the augmentation of algorithmic decisions with human
knowledge: the analyst uses the knowledge to set a guardrail by which the
algorithmic decision is clipped if the algorithmic output is out of bound, and
seems unreasonable. We study the conditions under which the augmentation is
beneficial relative to the raw algorithmic decision. We show that when the
algorithmic decision is asymptotically optimal with large data, the
non-data-driven human guardrail usually provides no benefit. However, we point
out three common pitfalls of the algorithmic decision: (1) lack of domain
knowledge, such as the market competition, (2) model misspecification, and (3)
data contamination. In these cases, even with sufficient data, the augmentation
from human knowledge can still improve the performance of the algorithmic
decision.",2211.11028v1,https://arxiv.org/pdf/2211.11028v1
Safe Reinforcement Learning using Data-Driven Predictive Control,"Mahmoud Selim, Amr Alanwar, M. Watheq El-Kharashi, Hazem M. Abbas, Karl H. Johansson","Reinforcement learning (RL) algorithms can achieve state-of-the-art
performance in decision-making and continuous control tasks. However, applying
RL algorithms on safety-critical systems still needs to be well justified due
to the exploration nature of many RL algorithms, especially when the model of
the robot and the environment are unknown. To address this challenge, we
propose a data-driven safety layer that acts as a filter for unsafe actions.
The safety layer uses a data-driven predictive controller to enforce safety
guarantees for RL policies during training and after deployment. The RL agent
proposes an action that is verified by computing the data-driven reachability
analysis. If there is an intersection between the reachable set of the robot
using the proposed action, we call the data-driven predictive controller to
find the closest safe action to the proposed unsafe action. The safety layer
penalizes the RL agent if the proposed action is unsafe and replaces it with
the closest safe one. In the simulation, we show that our method outperforms
state-of-the-art safe RL methods on the robotics navigation problem for a
Turtlebot 3 in Gazebo and a quadrotor in Unreal Engine 4 (UE4).",2211.11027v1,https://arxiv.org/pdf/2211.11027v1
"SafeLight: A Reinforcement Learning Method toward Collision-free Traffic
  Signal Control","Wenlu Du, Junyi Ye, Jingyi Gu, Jing Li, Hua Wei, Guiling Wang","Traffic signal control is safety-critical for our daily life. Roughly
one-quarter of road accidents in the U.S. happen at intersections due to
problematic signal timing, urging the development of safety-oriented
intersection control. However, existing studies on adaptive traffic signal
control using reinforcement learning technologies have focused mainly on
minimizing traffic delay but neglecting the potential exposure to unsafe
conditions. We, for the first time, incorporate road safety standards as
enforcement to ensure the safety of existing reinforcement learning methods,
aiming toward operating intersections with zero collisions. We have proposed a
safety-enhanced residual reinforcement learning method (SafeLight) and employed
multiple optimization techniques, such as multi-objective loss function and
reward shaping for better knowledge integration. Extensive experiments are
conducted using both synthetic and real-world benchmark datasets. Results show
that our method can significantly reduce collisions while increasing traffic
mobility.",2211.10871v3,https://arxiv.org/pdf/2211.10871v3
"Evaluating the Perceived Safety of Urban City via Maximum Entropy Deep
  Inverse Reinforcement Learning","Yaxuan Wang, Zhixin Zeng, Qijun Zhao","Inspired by expert evaluation policy for urban perception, we proposed a
novel inverse reinforcement learning (IRL) based framework for predicting urban
safety and recovering the corresponding reward function. We also presented a
scalable state representation method to model the prediction problem as a
Markov decision process (MDP) and use reinforcement learning (RL) to solve the
problem. Additionally, we built a dataset called SmallCity based on the
crowdsourcing method to conduct the research. As far as we know, this is the
first time the IRL approach has been introduced to the urban safety perception
and planning field to help experts quantitatively analyze perceptual features.
Our results showed that IRL has promising prospects in this field. We will
later open-source the crowdsourcing data collection site and the model proposed
in this paper.",2211.10660v2,https://arxiv.org/pdf/2211.10660v2
"Rationale-aware Autonomous Driving Policy utilizing Safety Force Field
  implemented on CARLA Simulator","Ho Suk, Taewoo Kim, Hyungbin Park, Pamul Yadav, Junyong Lee, Shiho Kim","Despite the rapid improvement of autonomous driving technology in recent
years, automotive manufacturers must resolve liability issues to commercialize
autonomous passenger car of SAE J3016 Level 3 or higher. To cope with the
product liability law, manufacturers develop autonomous driving systems in
compliance with international standards for safety such as ISO 26262 and ISO
21448. Concerning the safety of the intended functionality (SOTIF) requirement
in ISO 26262, the driving policy recommends providing an explicit rational
basis for maneuver decisions. In this case, mathematical models such as Safety
Force Field (SFF) and Responsibility-Sensitive Safety (RSS) which have
interpretability on decision, may be suitable. In this work, we implement SFF
from scratch to substitute the undisclosed NVIDIA's source code and integrate
it with CARLA open-source simulator. Using SFF and CARLA, we present a
predictor for claimed sets of vehicles, and based on the predictor, propose an
integrated driving policy that consistently operates regardless of safety
conditions it encounters while passing through dynamic traffic. The policy does
not have a separate plan for each condition, but using safety potential, it
aims human-like driving blended in with traffic flow.",2211.10237v1,https://arxiv.org/pdf/2211.10237v1
"Safe and Adaptive Decision-Making for Optimization of Safety-Critical
  Systems: The ARTEO Algorithm","Buse Sibel Korkmaz, Marta Zagórowska, Mehmet Mercangöz","We consider the problem of decision-making under uncertainty in an
environment with safety constraints. Many business and industrial applications
rely on real-time optimization to improve key performance indicators. In the
case of unknown characteristics, real-time optimization becomes challenging,
particularly because of the satisfaction of safety constraints. We propose the
ARTEO algorithm, where we cast multi-armed bandits as a mathematical
programming problem subject to safety constraints and learn the unknown
characteristics through exploration while optimizing the targets. We quantify
the uncertainty in unknown characteristics by using Gaussian processes and
incorporate it into the cost function as a contribution which drives
exploration. We adaptively control the size of this contribution in accordance
with the requirements of the environment. We guarantee the safety of our
algorithm with a high probability through confidence bounds constructed under
the regularity assumptions of Gaussian processes. We demonstrate the safety and
efficiency of our approach with two case studies: optimization of electric
motor current and real-time bidding problems. We further evaluate the
performance of ARTEO compared to a safe variant of upper confidence bound based
algorithms. ARTEO achieves less cumulative regret with accurate and safe
decisions.",2211.05495v2,https://arxiv.org/pdf/2211.05495v2
Safety-Constrained Policy Transfer with Successor Features,"Zeyu Feng, Bowen Zhang, Jianxin Bi, Harold Soh","In this work, we focus on the problem of safe policy transfer in
reinforcement learning: we seek to leverage existing policies when learning a
new task with specified constraints. This problem is important for
safety-critical applications where interactions are costly and unconstrained
policies can lead to undesirable or dangerous outcomes, e.g., with physical
robots that interact with humans. We propose a Constrained Markov Decision
Process (CMDP) formulation that simultaneously enables the transfer of policies
and adherence to safety constraints. Our formulation cleanly separates task
goals from safety considerations and permits the specification of a wide
variety of constraints. Our approach relies on a novel extension of generalized
policy improvement to constrained settings via a Lagrangian formulation. We
devise a dual optimization algorithm that estimates the optimal dual variable
of a target task, thus enabling safe transfer of policies derived from
successor features learned on source tasks. Our experiments in simulated
domains show that our approach is effective; it visits unsafe states less
frequently and outperforms alternative state-of-the-art methods when taking
safety constraints into account.",2211.05361v1,https://arxiv.org/pdf/2211.05361v1
"Safe Latent Diffusion: Mitigating Inappropriate Degeneration in
  Diffusion Models","Patrick Schramowski, Manuel Brack, Björn Deiseroth, Kristian Kersting","Text-conditioned image generation models have recently achieved astonishing
results in image quality and text alignment and are consequently employed in a
fast-growing number of applications. Since they are highly data-driven, relying
on billion-sized datasets randomly scraped from the internet, they also suffer,
as we demonstrate, from degenerated and biased human behavior. In turn, they
may even reinforce such biases. To help combat these undesired side effects, we
present safe latent diffusion (SLD). Specifically, to measure the inappropriate
degeneration due to unfiltered and imbalanced training sets, we establish a
novel image generation test bed-inappropriate image prompts (I2P)-containing
dedicated, real-world image-to-text prompts covering concepts such as nudity
and violence. As our exhaustive empirical evaluation demonstrates, the
introduced SLD removes and suppresses inappropriate image parts during the
diffusion process, with no additional training required and no adverse effect
on overall image quality or text alignment.",2211.05105v4,https://arxiv.org/pdf/2211.05105v4
System Safety Engineering for Social and Ethical ML Risks: A Case Study,"Edgar W. Jatho III, Logan O. Mailloux, Shalaleh Rismani, Eugene D. Williams, Joshua A. Kroll","Governments, industry, and academia have undertaken efforts to identify and
mitigate harms in ML-driven systems, with a particular focus on social and
ethical risks of ML components in complex sociotechnical systems. However,
existing approaches are largely disjointed, ad-hoc and of unknown
effectiveness. Systems safety engineering is a well established discipline with
a track record of identifying and managing risks in many complex sociotechnical
domains. We adopt the natural hypothesis that tools from this domain could
serve to enhance risk analyses of ML in its context of use. To test this
hypothesis, we apply a ""best of breed"" systems safety analysis, Systems
Theoretic Process Analysis (STPA), to a specific high-consequence system with
an important ML-driven component, namely the Prescription Drug Monitoring
Programs (PDMPs) operated by many US States, several of which rely on an
ML-derived risk score. We focus in particular on how this analysis can extend
to identifying social and ethical risks and developing concrete design-level
controls to mitigate them.",2211.04602v1,https://arxiv.org/pdf/2211.04602v1
"Creating a Safety Assurance Case for an ML Satellite-Based Wildfire
  Detection and Alert System","Richard Hawkins, Chiara Picardi, Lucy Donnell, Murray Ireland","Wildfires are a common problem in many areas of the world with often
catastrophic consequences. A number of systems have been created to provide
early warnings of wildfires, including those that use satellite data to detect
fires. The increased availability of small satellites, such as CubeSats, allows
the wildfire detection response time to be reduced by deploying constellations
of multiple satellites over regions of interest. By using machine learned
components on-board the satellites, constraints which limit the amount of data
that can be processed and sent back to ground stations can be overcome. There
are hazards associated with wildfire alert systems, such as failing to detect
the presence of a wildfire, or detecting a wildfire in the incorrect location.
It is therefore necessary to be able to create a safety assurance case for the
wildfire alert ML component that demonstrates it is sufficiently safe for use.
This paper describes in detail how a safety assurance case for an ML wildfire
alert system is created. This represents the first fully developed safety case
for an ML component containing explicit argument and evidence as to the safety
of the machine learning.",2211.04530v1,https://arxiv.org/pdf/2211.04530v1
"Learning Probabilistic Temporal Safety Properties from Examples in
  Relational Domains","Gavin Rens, Wen-Chi Yang, Jean-François Raskin, Luc De Raedt","We propose a framework for learning a fragment of probabilistic computation
tree logic (pCTL) formulae from a set of states that are labeled as safe or
unsafe. We work in a relational setting and combine ideas from relational
Markov Decision Processes with pCTL model-checking. More specifically, we
assume that there is an unknown relational pCTL target formula that is
satisfied by only safe states, and has a horizon of maximum $k$ steps and a
threshold probability $\alpha$. The task then consists of learning this unknown
formula from states that are labeled as safe or unsafe by a domain expert. We
apply principles of relational learning to induce a pCTL formula that is
satisfied by all safe states and none of the unsafe ones. This formula can then
be used as a safety specification for this domain, so that the system can avoid
getting into dangerous situations in future. Following relational learning
principles, we introduce a candidate formula generation process, as well as a
method for deciding which candidate formula is a satisfactory specification for
the given labeled states. The cases where the expert knows and does not know
the system policy are treated, however, much of the learning process is the
same for both cases. We evaluate our approach on a synthetic relational domain.",2211.03461v1,https://arxiv.org/pdf/2211.03461v1
"Safe Real-World Autonomous Driving by Learning to Predict and Plan with
  a Mixture of Experts","Stefano Pini, Christian S. Perone, Aayush Ahuja, Ana Sofia Rufino Ferreira, Moritz Niendorf, Sergey Zagoruyko","The goal of autonomous vehicles is to navigate public roads safely and
comfortably. To enforce safety, traditional planning approaches rely on
handcrafted rules to generate trajectories. Machine learning-based systems, on
the other hand, scale with data and are able to learn more complex behaviors.
However, they often ignore that agents and self-driving vehicle trajectory
distributions can be leveraged to improve safety. In this paper, we propose
modeling a distribution over multiple future trajectories for both the
self-driving vehicle and other road agents, using a unified neural network
architecture for prediction and planning. During inference, we select the
planning trajectory that minimizes a cost taking into account safety and the
predicted probabilities. Our approach does not depend on any rule-based
planners for trajectory generation or optimization, improves with more training
data and is simple to implement. We extensively evaluate our method through a
realistic simulator and show that the predicted trajectory distribution
corresponds to different driving profiles. We also successfully deploy it on a
self-driving vehicle on urban public roads, confirming that it drives safely
without compromising comfort. The code for training and testing our model on a
public prediction dataset and the video of the road test are available at
https://woven.mobi/safepathnet",2211.02131v1,https://arxiv.org/pdf/2211.02131v1
Benefits of Monotonicity in Safe Exploration with Gaussian Processes,"Arpan Losalka, Jonathan Scarlett","We consider the problem of sequentially maximising an unknown function over a
set of actions while ensuring that every sampled point has a function value
below a given safety threshold. We model the function using kernel-based and
Gaussian process methods, while differing from previous works in our assumption
that the function is monotonically increasing with respect to a \emph{safety
variable}. This assumption is motivated by various practical applications such
as adaptive clinical trial design and robotics. Taking inspiration from the
\textsc{\sffamily GP-UCB} and \textsc{\sffamily SafeOpt} algorithms, we propose
an algorithm, monotone safe {\sffamily UCB} (\textsc{\sffamily M-SafeUCB}) for
this task. We show that \textsc{\sffamily M-SafeUCB} enjoys theoretical
guarantees in terms of safety, a suitably-defined regret notion, and
approximately finding the entire safe boundary. In addition, we illustrate that
the monotonicity assumption yields significant benefits in terms of the
guarantees obtained, as well as algorithmic simplicity and efficiency. We
support our theoretical findings by performing empirical evaluations on a
variety of functions, including a simulated clinical trial experiment.",2211.01561v2,https://arxiv.org/pdf/2211.01561v2
"On the Safety of Interpretable Machine Learning: A Maximum Deviation
  Approach","Dennis Wei, Rahul Nair, Amit Dhurandhar, Kush R. Varshney, Elizabeth M. Daly, Moninder Singh","Interpretable and explainable machine learning has seen a recent surge of
interest. We focus on safety as a key motivation behind the surge and make the
relationship between interpretability and safety more quantitative. Toward
assessing safety, we introduce the concept of maximum deviation via an
optimization problem to find the largest deviation of a supervised learning
model from a reference model regarded as safe. We then show how
interpretability facilitates this safety assessment. For models including
decision trees, generalized linear and additive models, the maximum deviation
can be computed exactly and efficiently. For tree ensembles, which are not
regarded as interpretable, discrete optimization techniques can still provide
informative bounds. For a broader class of piecewise Lipschitz functions, we
leverage the multi-armed bandit literature to show that interpretability
produces tighter (regret) bounds on the maximum deviation. We present case
studies, including one on mortgage approval, to illustrate our methods and the
insights about models that may be obtained from deviation maximization.",2211.01498v1,https://arxiv.org/pdf/2211.01498v1
"Self-Improving Safety Performance of Reinforcement Learning Based
  Driving with Black-Box Verification Algorithms","Resul Dagdanov, Halil Durmus, Nazim Kemal Ure","In this work, we propose a self-improving artificial intelligence system to
enhance the safety performance of reinforcement learning (RL)-based autonomous
driving (AD) agents using black-box verification methods. RL algorithms have
become popular in AD applications in recent years. However, the performance of
existing RL algorithms heavily depends on the diversity of training scenarios.
A lack of safety-critical scenarios during the training phase could result in
poor generalization performance in real-world driving applications. We propose
a novel framework in which the weaknesses of the training set are explored
through black-box verification methods. After discovering AD failure scenarios,
the RL agent's training is re-initiated via transfer learning to improve the
performance of previously unsafe scenarios. Simulation results demonstrate that
our approach efficiently discovers safety failures of action decisions in
RL-based adaptive cruise control (ACC) applications and significantly reduces
the number of vehicle collisions through iterative applications of our method.
The source code is publicly available at
https://github.com/data-and-decision-lab/self-improving-RL.",2210.16575v3,https://arxiv.org/pdf/2210.16575v3
Provable Safe Reinforcement Learning with Binary Feedback,"Andrew Bennett, Dipendra Misra, Nathan Kallus","Safety is a crucial necessity in many applications of reinforcement learning
(RL), whether robotic, automotive, or medical. Many existing approaches to safe
RL rely on receiving numeric safety feedback, but in many cases this feedback
can only take binary values; that is, whether an action in a given state is
safe or unsafe. This is particularly true when feedback comes from human
experts. We therefore consider the problem of provable safe RL when given
access to an offline oracle providing binary feedback on the safety of state,
action pairs. We provide a novel meta algorithm, SABRE, which can be applied to
any MDP setting given access to a blackbox PAC RL algorithm for that setting.
SABRE applies concepts from active learning to reinforcement learning to
provably control the number of queries to the safety oracle. SABRE works by
iteratively exploring the state space to find regions where the agent is
currently uncertain about safety. Our main theoretical results shows that,
under appropriate technical assumptions, SABRE never takes unsafe actions
during training, and is guaranteed to return a near-optimal safe policy with
high probability. We provide a discussion of how our meta-algorithm may be
applied to various settings studied in both theoretical and empirical
frameworks.",2210.14492v1,https://arxiv.org/pdf/2210.14492v1
A POMDP Model for Safe Geological Carbon Sequestration,"Anthony Corso, Yizheng Wang, Markus Zechner, Jef Caers, Mykel J. Kochenderfer","Geological carbon capture and sequestration (CCS), where CO$_2$ is stored in
subsurface formations, is a promising and scalable approach for reducing global
emissions. However, if done incorrectly, it may lead to earthquakes and leakage
of CO$_2$ back to the surface, harming both humans and the environment. These
risks are exacerbated by the large amount of uncertainty in the structure of
the storage formation. For these reasons, we propose that CCS operations be
modeled as a partially observable Markov decision process (POMDP) and decisions
be informed using automated planning algorithms. To this end, we develop a
simplified model of CCS operations based on a 2D spillpoint analysis that
retains many of the challenges and safety considerations of the real-world
problem. We show how off-the-shelf POMDP solvers outperform expert baselines
for safe CCS planning. This POMDP model can be used as a test bed to drive the
development of novel decision-making algorithms for CCS operations.",2212.00669v1,https://arxiv.org/pdf/2212.00669v1
"Baby Physical Safety Monitoring in Smart Home Using Action Recognition
  System","Victor Adewopo, Nelly Elsayed, Kelly Anderson","Humans are able to intuitively deduce actions that took place between two
states in observations via deductive reasoning. This is because the brain
operates on a bidirectional communication model, which has radically improved
the accuracy of recognition and prediction based on features connected to
previous experiences. During the past decade, deep learning models for action
recognition have significantly improved. However, deep neural networks struggle
with these tasks on a smaller dataset for specific Action Recognition (AR)
tasks. As with most action recognition tasks, the ambiguity of accurately
describing activities in spatial-temporal data is a drawback that can be
overcome by curating suitable datasets, including careful annotations and
preprocessing of video data for analyzing various recognition tasks. In this
study, we present a novel lightweight framework combining transfer learning
techniques with a Conv2D LSTM layer to extract features from the pre-trained
I3D model on the Kinetics dataset for a new AR task (Smart Baby Care) that
requires a smaller dataset and less computational resources. Furthermore, we
developed a benchmark dataset and an automated model that uses LSTM convolution
with I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in a
smart baby room. Finally, we implemented video augmentation to improve model
performance on the smart baby care task. Compared to other benchmark models,
our experimental framework achieved better performance with less computational
resources.",2210.12527v2,https://arxiv.org/pdf/2210.12527v2
Safe Policy Improvement in Constrained Markov Decision Processes,"Luigi Berducci, Radu Grosu","The automatic synthesis of a policy through reinforcement learning (RL) from
a given set of formal requirements depends on the construction of a reward
signal and consists of the iterative application of many policy-improvement
steps. The synthesis algorithm has to balance target, safety, and comfort
requirements in a single objective and to guarantee that the policy improvement
does not increase the number of safety-requirements violations, especially for
safety-critical applications. In this work, we present a solution to the
synthesis problem by solving its two main challenges: reward-shaping from a set
of formal requirements and safe policy update. For the former, we propose an
automatic reward-shaping procedure, defining a scalar reward signal compliant
with the task specification. For the latter, we introduce an algorithm ensuring
that the policy is improved in a safe fashion with high-confidence guarantees.
We also discuss the adoption of a model-based RL algorithm to efficiently use
the collected data and train a model-free agent on the predicted trajectories,
where the safety violation does not have the same impact as in the real world.
Finally, we demonstrate in standard control benchmarks that the resulting
learning procedure is effective and robust even under heavy perturbations of
the hyperparameters.",2210.11259v1,https://arxiv.org/pdf/2210.11259v1
"Provably Safe Reinforcement Learning via Action Projection using
  Reachability Analysis and Polynomial Zonotopes","Niklas Kochdumper, Hanna Krasowski, Xiao Wang, Stanley Bak, Matthias Althoff","While reinforcement learning produces very promising results for many
applications, its main disadvantage is the lack of safety guarantees, which
prevents its use in safety-critical systems. In this work, we address this
issue by a safety shield for nonlinear continuous systems that solve
reach-avoid tasks. Our safety shield prevents applying potentially unsafe
actions from a reinforcement learning agent by projecting the proposed action
to the closest safe action. This approach is called action projection and is
implemented via mixed-integer optimization. The safety constraints for action
projection are obtained by applying parameterized reachability analysis using
polynomial zonotopes, which enables to accurately capture the nonlinear effects
of the actions on the system. In contrast to other state-of-the-art approaches
for action projection, our safety shield can efficiently handle input
constraints and dynamic obstacles, eases incorporation of the spatial robot
dimensions into the safety constraints, guarantees robust safety despite
process noise and measurement errors, and is well suited for high-dimensional
systems, as we demonstrate on several challenging benchmark systems.",2210.10691v2,https://arxiv.org/pdf/2210.10691v2
SafeText: A Benchmark for Exploring Physical Safety in Language Models,"Sharon Levy, Emily Allaway, Melanie Subbiah, Lydia Chilton, Desmond Patton, Kathleen McKeown, William Yang Wang","Understanding what constitutes safe text is an important issue in natural
language processing and can often prevent the deployment of models deemed
harmful and unsafe. One such type of safety that has been scarcely studied is
commonsense physical safety, i.e. text that is not explicitly violent and
requires additional commonsense knowledge to comprehend that it leads to
physical harm. We create the first benchmark dataset, SafeText, comprising
real-life scenarios with paired safe and physically unsafe pieces of advice. We
utilize SafeText to empirically study commonsense physical safety across
various models designed for text generation and commonsense reasoning tasks. We
find that state-of-the-art large language models are susceptible to the
generation of unsafe text and have difficulty rejecting unsafe advice. As a
result, we argue for further studies of safety and the assessment of
commonsense physical safety in models before release.",2210.10045v1,https://arxiv.org/pdf/2210.10045v1
Adversarial and Safely Scaled Question Generation,"Sreehari Sankar, Zhihang Dong","Question generation has recently gained a lot of research interest,
especially with the advent of large language models. In and of itself, question
generation can be considered 'AI-hard', as there is a lack of unanimously
agreed sense of what makes a question 'good' or 'bad'. In this paper, we tackle
two fundamental problems in parallel: on one hand, we try to solve the scaling
problem, where question-generation and answering applications have to be
applied to a massive amount of text without ground truth labeling. The usual
approach to solve this problem is to either downsample or summarize. However,
there are critical risks of misinformation with these approaches. On the other
hand, and related to the misinformation problem, we try to solve the 'safety'
problem, as many public institutions rely on a much higher level of accuracy
for the content they provide. We introduce an adversarial approach to tackle
the question generation safety problem with scale. Specifically, we designed a
question-answering system that specifically prunes out unanswerable questions
that may be generated, and further increases the quality of the answers that
are generated. We build a production-ready, easily-plugged pipeline that can be
used on any given body of text, that is scalable and immune from generating any
hate speech, profanity, or misinformation. Based on the results, we are able to
generate more than six times the number of quality questions generated by the
abstractive approach, with a perceived quality being 44% higher, according to a
survey of 168 participants.",2210.09467v1,https://arxiv.org/pdf/2210.09467v1
"Evaluation of Pedestrian Safety in a High-Fidelity Simulation
  Environment Framework","Lin Ma, Longrui Chen, Yan Zhang, Mengdi Chu, Wenjie Jiang, Jiahao Shen, Chuxuan Li, Yifeng Shi, Nairui Luo, Jirui Yuan, Guyue Zhou, Jiangtao Gong","Pedestrians' safety is a crucial factor in assessing autonomous driving
scenarios. However, pedestrian safety evaluation is rarely considered by
existing autonomous driving simulation platforms. This paper proposes a
pedestrian safety evaluation method for autonomous driving, in which not only
the collision events but also the conflict events together with the
characteristics of pedestrians are fully considered. Moreover, to apply the
pedestrian safety evaluation system, we construct a high-fidelity simulation
framework embedded with pedestrian safety-critical characteristics. We
demonstrate our simulation framework and pedestrian safety evaluation with a
comparative experiment with two kinds of autonomous driving perception
algorithms -- single-vehicle perception and vehicle-to-infrastructure (V2I)
cooperative perception. The results show that our framework can evaluate
different autonomous driving algorithms with detailed and quantitative
pedestrian safety indexes. To this end, the proposed simulation method and
framework can be used to access different autonomous driving algorithms and
evaluate pedestrians' safety performance in future autonomous driving
simulations, which can inspire more pedestrian-friendly autonomous driving
algorithms.",2210.08731v4,https://arxiv.org/pdf/2210.08731v4
HUDD: A tool to debug DNNs for safety analysis,"Hazem Fahmy, Fabrizio Pastore, Lionel Briand","We present HUDD, a tool that supports safety analysis practices for systems
enabled by Deep Neural Networks (DNNs) by automatically identifying the root
causes for DNN errors and retraining the DNN. HUDD stands for Heatmap-based
Unsupervised Debugging of DNNs, it automatically clusters error-inducing images
whose results are due to common subsets of DNN neurons. The intent is for the
generated clusters to group error-inducing images having common
characteristics, that is, having a common root cause. HUDD identifies root
causes by applying a clustering algorithm to matrices (i.e., heatmaps)
capturing the relevance of every DNN neuron on the DNN outcome. Also, HUDD
retrains DNNs with images that are automatically selected based on their
relatedness to the identified image clusters. Our empirical evaluation with
DNNs from the automotive domain have shown that HUDD automatically identifies
all the distinct root causes of DNN errors, thus supporting safety analysis.
Also, our retraining approach has shown to be more effective at improving DNN
accuracy than existing approaches. A demo video of HUDD is available at
https://youtu.be/drjVakP7jdU.",2210.08356v1,https://arxiv.org/pdf/2210.08356v1
Is Face Recognition Safe from Realizable Attacks?,"Sanjay Saha, Terence Sim","Face recognition is a popular form of biometric authentication and due to its
widespread use, attacks have become more common as well. Recent studies show
that Face Recognition Systems are vulnerable to attacks and can lead to
erroneous identification of faces. Interestingly, most of these attacks are
white-box, or they are manipulating facial images in ways that are not
physically realizable. In this paper, we propose an attack scheme where the
attacker can generate realistic synthesized face images with subtle
perturbations and physically realize that onto his face to attack black-box
face recognition systems. Comprehensive experiments and analyses show that
subtle perturbations realized on attackers face can create successful attacks
on state-of-the-art face recognition systems in black-box settings. Our study
exposes the underlying vulnerability posed by the Face Recognition Systems
against realizable black-box attacks.",2210.08178v1,https://arxiv.org/pdf/2210.08178v1
"Model-based Safe Deep Reinforcement Learning via a Constrained Proximal
  Policy Optimization Algorithm","Ashish Kumar Jayant, Shalabh Bhatnagar","During initial iterations of training in most Reinforcement Learning (RL)
algorithms, agents perform a significant number of random exploratory steps. In
the real world, this can limit the practicality of these algorithms as it can
lead to potentially dangerous behavior. Hence safe exploration is a critical
issue in applying RL algorithms in the real world. This problem has been
recently well studied under the Constrained Markov Decision Process (CMDP)
Framework, where in addition to single-stage rewards, an agent receives
single-stage costs or penalties as well depending on the state transitions. The
prescribed cost functions are responsible for mapping undesirable behavior at
any given time-step to a scalar value. The goal then is to find a feasible
policy that maximizes reward returns while constraining the cost returns to be
below a prescribed threshold during training as well as deployment.
  We propose an On-policy Model-based Safe Deep RL algorithm in which we learn
the transition dynamics of the environment in an online manner as well as find
a feasible optimal policy using the Lagrangian Relaxation-based Proximal Policy
Optimization. We use an ensemble of neural networks with different
initializations to tackle epistemic and aleatoric uncertainty issues faced
during environment model learning. We compare our approach with relevant
model-free and model-based approaches in Constrained RL using the challenging
Safe Reinforcement Learning benchmark - the Open AI Safety Gym. We demonstrate
that our algorithm is more sample efficient and results in lower cumulative
hazard violations as compared to constrained model-free approaches. Further,
our approach shows better reward performance than other constrained model-based
approaches in the literature.",2210.07573v1,https://arxiv.org/pdf/2210.07573v1
"Safe Model-Based Reinforcement Learning with an Uncertainty-Aware
  Reachability Certificate","Dongjie Yu, Wenjun Zou, Yujie Yang, Haitong Ma, Shengbo Eben Li, Jingliang Duan, Jianyu Chen","Safe reinforcement learning (RL) that solves constraint-satisfactory policies
provides a promising way to the broader safety-critical applications of RL in
real-world problems such as robotics. Among all safe RL approaches, model-based
methods reduce training time violations further due to their high sample
efficiency. However, lacking safety robustness against the model uncertainties
remains an issue in safe model-based RL, especially in training time safety. In
this paper, we propose a distributional reachability certificate (DRC) and its
Bellman equation to address model uncertainties and characterize robust
persistently safe states. Furthermore, we build a safe RL framework to resolve
constraints required by the DRC and its corresponding shield policy. We also
devise a line search method to maintain safety and reach higher returns
simultaneously while leveraging the shield policy. Comprehensive experiments on
classical benchmarks such as constrained tracking and navigation indicate that
the proposed algorithm achieves comparable returns with much fewer constraint
violations during training.",2210.07553v1,https://arxiv.org/pdf/2210.07553v1
Near-Optimal Multi-Agent Learning for Safe Coverage Control,"Manish Prajapat, Matteo Turchetta, Melanie N. Zeilinger, Andreas Krause","In multi-agent coverage control problems, agents navigate their environment
to reach locations that maximize the coverage of some density. In practice, the
density is rarely known $\textit{a priori}$, further complicating the original
NP-hard problem. Moreover, in many applications, agents cannot visit arbitrary
locations due to $\textit{a priori}$ unknown safety constraints. In this paper,
we aim to efficiently learn the density to approximately solve the coverage
problem while preserving the agents' safety. We first propose a conditionally
linear submodular coverage function that facilitates theoretical analysis.
Utilizing this structure, we develop MacOpt, a novel algorithm that efficiently
trades off the exploration-exploitation dilemma due to partial observability,
and show that it achieves sublinear regret. Next, we extend results on
single-agent safe exploration to our multi-agent setting and propose SafeMac
for safe coverage and exploration. We analyze SafeMac and give first of its
kind results: near optimal coverage in finite time while provably guaranteeing
safety. We extensively evaluate our algorithms on synthetic and real problems,
including a bio-diversity monitoring task under safety constraints, where
SafeMac outperforms competing methods.",2210.06380v1,https://arxiv.org/pdf/2210.06380v1
"Geometry of Radial Basis Neural Networks for Safety Biased Approximation
  of Unsafe Regions","Ahmad Abuaish, Mohit Srinivasan, Patricio A. Vela","Barrier function-based inequality constraints are a means to enforce safety
specifications for control systems. When used in conjunction with a convex
optimization program, they provide a computationally efficient method to
enforce safety for the general class of control-affine systems. One of the main
assumptions when taking this approach is the a priori knowledge of the barrier
function itself, i.e., knowledge of the safe set. In the context of navigation
through unknown environments where the locally safe set evolves with time, such
knowledge does not exist. This manuscript focuses on the synthesis of a zeroing
barrier function characterizing the safe set based on safe and unsafe sample
measurements, e.g., from perception data in navigation applications. Prior work
formulated a supervised machine learning algorithm whose solution guaranteed
the construction of a zeroing barrier function with specific level-set
properties. However, it did not explore the geometry of the neural network
design used for the synthesis process. This manuscript describes the specific
geometry of the neural network used for zeroing barrier function synthesis, and
shows how the network provides the necessary representation for splitting the
state space into safe and unsafe regions.",2210.05596v2,https://arxiv.org/pdf/2210.05596v2
Safety Verification for Neural Networks Based on Set-boundary Analysis,"Zhen Liang, Dejin Ren, Wanwei Liu, Ji Wang, Wenjing Yang, Bai Xue","Neural networks (NNs) are increasingly applied in safety-critical systems
such as autonomous vehicles. However, they are fragile and are often
ill-behaved. Consequently, their behaviors should undergo rigorous guarantees
before deployment in practice. In this paper we propose a set-boundary
reachability method to investigate the safety verification problem of NNs from
a topological perspective. Given an NN with an input set and a safe set, the
safety verification problem is to determine whether all outputs of the NN
resulting from the input set fall within the safe set. In our method, the
homeomorphism property of NNs is mainly exploited, which establishes a
relationship mapping boundaries to boundaries. The exploitation of this
property facilitates reachability computations via extracting subsets of the
input set rather than the entire input set, thus controlling the wrapping
effect in reachability analysis and facilitating the reduction of computation
burdens for safety verification. The homeomorphism property exists in some
widely used NNs such as invertible NNs. Notable representations are invertible
residual networks (i-ResNets) and Neural ordinary differential equations
(Neural ODEs). For these NNs, our set-boundary reachability method only needs
to perform reachability analysis on the boundary of the input set. For NNs
which do not feature this property with respect to the input set, we explore
subsets of the input set for establishing the local homeomorphism property, and
then abandon these subsets for reachability computations. Finally, some
examples demonstrate the performance of the proposed method.",2210.04175v1,https://arxiv.org/pdf/2210.04175v1
"From plane crashes to algorithmic harm: applicability of safety
  engineering frameworks for responsible ML","Shalaleh Rismani, Renee Shelby, Andrew Smart, Edgar Jatho, Joshua Kroll, AJung Moon, Negar Rostamzadeh","Inappropriate design and deployment of machine learning (ML) systems leads to
negative downstream social and ethical impact -- described here as social and
ethical risks -- for users, society and the environment. Despite the growing
need to regulate ML systems, current processes for assessing and mitigating
risks are disjointed and inconsistent. We interviewed 30 industry practitioners
on their current social and ethical risk management practices, and collected
their first reactions on adapting safety engineering frameworks into their
practice -- namely, System Theoretic Process Analysis (STPA) and Failure Mode
and Effects Analysis (FMEA). Our findings suggest STPA/FMEA can provide
appropriate structure toward social and ethical risk assessment and mitigation
processes. However, we also find nontrivial challenges in integrating such
frameworks in the fast-paced culture of the ML industry. We call on the ML
research community to strengthen existing frameworks and assess their efficacy,
ensuring that ML systems are safer for all people.",2210.03535v1,https://arxiv.org/pdf/2210.03535v1
"Towards Safe Mechanical Ventilation Treatment Using Deep Offline
  Reinforcement Learning","Flemming Kondrup, Thomas Jiralerspong, Elaine Lau, Nathan de Lara, Jacob Shkrob, My Duc Tran, Doina Precup, Sumana Basu","Mechanical ventilation is a key form of life support for patients with
pulmonary impairment. Healthcare workers are required to continuously adjust
ventilator settings for each patient, a challenging and time consuming task.
Hence, it would be beneficial to develop an automated decision support tool to
optimize ventilation treatment. We present DeepVent, a Conservative Q-Learning
(CQL) based offline Deep Reinforcement Learning (DRL) agent that learns to
predict the optimal ventilator parameters for a patient to promote 90 day
survival. We design a clinically relevant intermediate reward that encourages
continuous improvement of the patient vitals as well as addresses the challenge
of sparse reward in RL. We find that DeepVent recommends ventilation parameters
within safe ranges, as outlined in recent clinical trials. The CQL algorithm
offers additional safety by mitigating the overestimation of the value
estimates of out-of-distribution states/actions. We evaluate our agent using
Fitted Q Evaluation (FQE) and demonstrate that it outperforms physicians from
the MIMIC-III dataset.",2210.02552v1,https://arxiv.org/pdf/2210.02552v1
"Spatial-Temporal-Aware Safe Multi-Agent Reinforcement Learning of
  Connected Autonomous Vehicles in Challenging Scenarios","Zhili Zhang, Songyang Han, Jiangwei Wang, Fei Miao","Communication technologies enable coordination among connected and autonomous
vehicles (CAVs). However, it remains unclear how to utilize shared information
to improve the safety and efficiency of the CAV system in dynamic and
complicated driving scenarios. In this work, we propose a framework of
constrained multi-agent reinforcement learning (MARL) with a parallel Safety
Shield for CAVs in challenging driving scenarios that includes unconnected
hazard vehicles. The coordination mechanisms of the proposed MARL include
information sharing and cooperative policy learning, with Graph Convolutional
Network (GCN)-Transformer as a spatial-temporal encoder that enhances the
agent's environment awareness. The Safety Shield module with Control Barrier
Functions (CBF)-based safety checking protects the agents from taking unsafe
actions. We design a constrained multi-agent advantage actor-critic (CMAA2C)
algorithm to train safe and cooperative policies for CAVs. With the experiment
deployed in the CARLA simulator, we verify the performance of the safety
checking, spatial-temporal encoder, and coordination mechanisms designed in our
method by comparative experiments in several challenging scenarios with
unconnected hazard vehicles. Results show that our proposed methodology
significantly increases system safety and efficiency in challenging scenarios.",2210.02300v3,https://arxiv.org/pdf/2210.02300v3
"SecureFedYJ: a safe feature Gaussianization protocol for Federated
  Learning","Tanguy Marchand, Boris Muzellec, Constance Beguier, Jean Ogier du Terrail, Mathieu Andreux","The Yeo-Johnson (YJ) transformation is a standard parametrized per-feature
unidimensional transformation often used to Gaussianize features in machine
learning. In this paper, we investigate the problem of applying the YJ
transformation in a cross-silo Federated Learning setting under privacy
constraints. For the first time, we prove that the YJ negative log-likelihood
is in fact convex, which allows us to optimize it with exponential search. We
numerically show that the resulting algorithm is more stable than the
state-of-the-art approach based on the Brent minimization method. Building on
this simple algorithm and Secure Multiparty Computation routines, we propose
SecureFedYJ, a federated algorithm that performs a pooled-equivalent YJ
transformation without leaking more information than the final fitted
parameters do. Quantitative experiments on real data demonstrate that, in
addition to being secure, our approach reliably normalizes features across
silos as well as if data were pooled, making it a viable approach for safe
federated feature Gaussianization.",2210.01639v2,https://arxiv.org/pdf/2210.01639v2
"Connecting Surrogate Safety Measures to Crash Probablity via Causal
  Probabilistic Time Series Prediction","Jiajian Lu, Offer Grembek, Mark Hansen","Surrogate safety measures can provide fast and pro-active safety analysis and
give insights on the pre-crash process and crash failure mechanism by studying
near misses. However, validating surrogate safety measures by connecting them
to crashes is still an open question. This paper proposed a method to connect
surrogate safety measures to crash probability using probabilistic time series
prediction. The method used sequences of speed, acceleration and
time-to-collision to estimate the probability density functions of those
variables with transformer masked autoregressive flow (transformer-MAF). The
autoregressive structure mimicked the causal relationship between condition,
action and crash outcome and the probability density functions are used to
calculate the conditional action probability, crash probability and conditional
crash probability. The predicted sequence is accurate and the estimated
probability is reasonable under both traffic conflict context and normal
interaction context and the conditional crash probability shows the
effectiveness of evasive action to avoid crashes in a counterfactual
experiment.",2210.01363v1,https://arxiv.org/pdf/2210.01363v1
Red-Teaming the Stable Diffusion Safety Filter,"Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, Florian Tramèr","Stable Diffusion is a recent open-source image generation model comparable to
proprietary models such as DALLE, Imagen, or Parti. Stable Diffusion comes with
a safety filter that aims to prevent generating explicit images. Unfortunately,
the filter is obfuscated and poorly documented. This makes it hard for users to
prevent misuse in their applications, and to understand the filter's
limitations and improve it. We first show that it is easy to generate
disturbing content that bypasses the safety filter. We then reverse-engineer
the filter and find that while it aims to prevent sexual content, it ignores
violence, gore, and other similarly disturbing content. Based on our analysis,
we argue safety measures in future model releases should strive to be fully
open and properly documented to stimulate security contributions from the
community.",2210.04610v5,https://arxiv.org/pdf/2210.04610v5
Meta-Learning Priors for Safe Bayesian Optimization,"Jonas Rothfuss, Christopher Koenig, Alisa Rupenyan, Andreas Krause","In robotics, optimizing controller parameters under safety constraints is an
important challenge. Safe Bayesian optimization (BO) quantifies uncertainty in
the objective and constraints to safely guide exploration in such settings.
Hand-designing a suitable probabilistic model can be challenging, however. In
the presence of unknown safety constraints, it is crucial to choose reliable
model hyper-parameters to avoid safety violations. Here, we propose a
data-driven approach to this problem by meta-learning priors for safe BO from
offline data. We build on a meta-learning algorithm, F-PACOH, capable of
providing reliable uncertainty quantification in settings of data scarcity. As
core contribution, we develop a novel framework for choosing safety-compliant
priors in a data-riven manner via empirical uncertainty metrics and a frontier
search algorithm. On benchmark functions and a high-precision motion system, we
demonstrate that our meta-learned priors accelerate the convergence of safe BO
approaches while maintaining safety.",2210.00762v3,https://arxiv.org/pdf/2210.00762v3
"Safe Reinforcement Learning From Pixels Using a Stochastic Latent
  Representation","Yannick Hogewind, Thiago D. Simao, Tal Kachman, Nils Jansen","We address the problem of safe reinforcement learning from pixel
observations. Inherent challenges in such settings are (1) a trade-off between
reward optimization and adhering to safety constraints, (2) partial
observability, and (3) high-dimensional observations. We formalize the problem
in a constrained, partially observable Markov decision process framework, where
an agent obtains distinct reward and safety signals. To address the curse of
dimensionality, we employ a novel safety critic using the stochastic latent
actor-critic (SLAC) approach. The latent variable model predicts rewards and
safety violations, and we use the safety critic to train safe policies. Using
well-known benchmark environments, we demonstrate competitive performance over
existing approaches with respects to computational requirements, final reward
return, and satisfying the safety constraints.",2210.01801v1,https://arxiv.org/pdf/2210.01801v1
Safety-Critical Adaptation in Self-Adaptive Systems,"Simon Diemert, Jens H. Weber","Modern systems are designed to operate in increasingly variable and uncertain
environments. Not only are these environments complex, in the sense that they
contain a tremendous number of variables, but they also change over time.
Systems must be able to adjust their behaviour at run-time to manage these
uncertainties. These self-adaptive systems have been studied extensively. This
paper proposes a definition of a safety-critical self-adaptive system and then
describes a taxonomy for classifying adaptations into different types based on
their impact on the system's safety and the system's safety case. The taxonomy
expresses criteria for classification and then describes specific criteria that
the safety case for a self-adaptive system must satisfy, depending on the type
of adaptations performed. Each type in the taxonomy is illustrated using the
example of a safety-critical self-adaptive water heating system.",2210.00095v1,https://arxiv.org/pdf/2210.00095v1
"Safe Exploration Method for Reinforcement Learning under Existence of
  Disturbance","Yoshihiro Okawa, Tomotake Sasaki, Hitoshi Yanami, Toru Namerikawa","Recent rapid developments in reinforcement learning algorithms have been
giving us novel possibilities in many fields. However, due to their exploring
property, we have to take the risk into consideration when we apply those
algorithms to safety-critical problems especially in real environments. In this
study, we deal with a safe exploration problem in reinforcement learning under
the existence of disturbance. We define the safety during learning as
satisfaction of the constraint conditions explicitly defined in terms of the
state and propose a safe exploration method that uses partial prior knowledge
of a controlled object and disturbance. The proposed method assures the
satisfaction of the explicit state constraints with a pre-specified probability
even if the controlled object is exposed to a stochastic disturbance following
a normal distribution. As theoretical results, we introduce sufficient
conditions to construct conservative inputs not containing an exploring aspect
used in the proposed method and prove that the safety in the above explained
sense is guaranteed with the proposed method. Furthermore, we illustrate the
validity and effectiveness of the proposed method through numerical simulations
of an inverted pendulum and a four-bar parallel link robot manipulator.",2209.15452v2,https://arxiv.org/pdf/2209.15452v2
On the Impossible Safety of Large AI Models,"El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Lê-Nguyên Hoang, Rafael Pinot, Sébastien Rouault, John Stephan","Large AI Models (LAIMs), of which large language models are the most
prominent recent example, showcase some impressive performance. However they
have been empirically found to pose serious security issues. This paper
systematizes our knowledge about the fundamental impossibility of building
arbitrarily accurate and secure machine learning models. More precisely, we
identify key challenging features of many of today's machine learning settings.
Namely, high accuracy seems to require memorizing large training datasets,
which are often user-generated and highly heterogeneous, with both sensitive
information and fake users. We then survey statistical lower bounds that, we
argue, constitute a compelling case against the possibility of designing
high-accuracy LAIMs with strong security guarantees.",2209.15259v2,https://arxiv.org/pdf/2209.15259v2
"Modeling driver's evasive behavior during safety-critical lane
  changes:Two-dimensional time-to-collision and deep reinforcement learning","Hongyu Guo, Kun Xie, Mehdi Keyvan-Ekbatani","Lane changes are complex driving behaviors and frequently involve
safety-critical situations. This study aims to develop a lane-change-related
evasive behavior model, which can facilitate the development of safety-aware
traffic simulations and predictive collision avoidance systems. Large-scale
connected vehicle data from the Safety Pilot Model Deployment (SPMD) program
were used for this study. A new surrogate safety measure, two-dimensional
time-to-collision (2D-TTC), was proposed to identify the safety-critical
situations during lane changes. The validity of 2D-TTC was confirmed by showing
a high correlation between the detected conflict risks and the archived
crashes. A deep deterministic policy gradient (DDPG) algorithm, which could
learn the sequential decision-making process over continuous action spaces, was
used to model the evasive behaviors in the identified safety-critical
situations. The results showed the superiority of the proposed model in
replicating both the longitudinal and lateral evasive behaviors.",2209.15133v1,https://arxiv.org/pdf/2209.15133v1
"Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement
  Learning in Unknown Stochastic Environments","Yixuan Wang, Simon Sinong Zhan, Ruochen Jiao, Zhilu Wang, Wanxin Jin, Zhuoran Yang, Zhaoran Wang, Chao Huang, Qi Zhu","It is quite challenging to ensure the safety of reinforcement learning (RL)
agents in an unknown and stochastic environment under hard constraints that
require the system state not to reach certain specified unsafe regions. Many
popular safe RL methods such as those based on the Constrained Markov Decision
Process (CMDP) paradigm formulate safety violations in a cost function and try
to constrain the expectation of cumulative cost under a threshold. However, it
is often difficult to effectively capture and enforce hard reachability-based
safety constraints indirectly with such constraints on safety violation costs.
In this work, we leverage the notion of barrier function to explicitly encode
the hard safety constraints, and given that the environment is unknown, relax
them to our design of \emph{generative-model-based soft barrier functions}.
Based on such soft barriers, we propose a safe RL approach that can jointly
learn the environment and optimize the control policy, while effectively
avoiding unsafe regions with safety probability optimization. Experiments on a
set of examples demonstrate that our approach can effectively enforce hard
safety constraints and significantly outperform CMDP-based baseline methods in
system safe rate measured via simulations.",2209.15090v3,https://arxiv.org/pdf/2209.15090v3
"Constrained Dynamic Movement Primitives for Safe Learning of Motor
  Skills","Seiji Shaw, Devesh K. Jha, Arvind Raghunathan, Radu Corcodel, Diego Romeres, George Konidaris, Daniel Nikovski","Dynamic movement primitives are widely used for learning skills which can be
demonstrated to a robot by a skilled human or controller. While their
generalization capabilities and simple formulation make them very appealing to
use, they possess no strong guarantees to satisfy operational safety
constraints for a task. In this paper, we present constrained dynamic movement
primitives (CDMP) which can allow for constraint satisfaction in the robot
workspace. We present a formulation of a non-linear optimization to perturb the
DMP forcing weights regressed by locally-weighted regression to admit a Zeroing
Barrier Function (ZBF), which certifies workspace constraint satisfaction. We
demonstrate the proposed CDMP under different constraints on the end-effector
movement such as obstacle avoidance and workspace constraints on a physical
robot. A video showing the implementation of the proposed algorithm using
different manipulators in different environments could be found here
https://youtu.be/hJegJJkJfys.",2209.14461v1,https://arxiv.org/pdf/2209.14461v1
Guiding Safe Exploration with Weakest Preconditions,"Greg Anderson, Swarat Chaudhuri, Isil Dillig","In reinforcement learning for safety-critical settings, it is often desirable
for the agent to obey safety constraints at all points in time, including
during training. We present a novel neurosymbolic approach called SPICE to
solve this safe exploration problem. SPICE uses an online shielding layer based
on symbolic weakest preconditions to achieve a more precise safety analysis
than existing tools without unduly impacting the training process. We evaluate
the approach on a suite of continuous control benchmarks and show that it can
achieve comparable performance to existing safe learning techniques while
incurring fewer safety violations. Additionally, we present theoretical results
showing that SPICE converges to the optimal safe policy under reasonable
assumptions.",2209.14148v2,https://arxiv.org/pdf/2209.14148v2
Verifying Safety of Behaviour Trees in Event-B,"Matteo Tadiello, Elena Troubitsyna","Behavior Trees (BT) are becoming increasingly popular in the robotics
community. The BT tool is well suited for decision-making applications allowing
a robot to perform complex behavior while being explainable to humans as well.
Verifying that BTs used are well constructed with respect to safety and
reliability requirements is essential, especially for robots operating in
critical environments. In this work, we propose a formal specification of
Behavior Trees and a methodology to prove invariants of already used trees,
while keeping the complexity of the formalization of the tree simple for the
final user. Allowing the possibility to test the particular instance of the
behavior tree without the necessity to know the more abstract levels of the
formalization.",2209.14045v1,https://arxiv.org/pdf/2209.14045v1
Scheduling for Urban Air Mobility using Safe Learning,"Surya Murthy, Natasha A. Neogi, Suda Bharadwaj","This work considers the scheduling problem for Urban Air Mobility (UAM)
vehicles travelling between origin-destination pairs with both hard and soft
trip deadlines. Each route is described by a discrete probability distribution
over trip completion times (or delay) and over inter-arrival times of requests
(or demand) for the route along with a fixed hard or soft deadline. Soft
deadlines carry a cost that is incurred when the deadline is missed. An online,
safe scheduler is developed that ensures that hard deadlines are never missed,
and that average cost of missing soft deadlines is minimized. The system is
modelled as a Markov Decision Process (MDP) and safe model-based learning is
used to find the probabilistic distributions over route delays and demand.
Monte Carlo Tree Search (MCTS) Earliest Deadline First (EDF) is used to safely
explore the learned models in an online fashion and develop a near-optimal
non-preemptive scheduling policy. These results are compared with Value
Iteration (VI) and MCTS (Random) scheduling solutions.",2209.15457v1,https://arxiv.org/pdf/2209.15457v1
Safe Linear Bandits over Unknown Polytopes,"Aditya Gangrade, Tianrui Chen, Venkatesh Saligrama","The safe linear bandit problem (SLB) is an online approach to linear
programming with unknown objective and unknown roundwise constraints, under
stochastic bandit feedback of rewards and safety risks of actions. We study the
tradeoffs between efficacy and smooth safety costs of SLBs over polytopes, and
the role of aggressive doubly-optimistic play in avoiding the strong
assumptions made by extant pessimistic-optimistic approaches.
  We first elucidate an inherent hardness in SLBs due the lack of knowledge of
constraints: there exist `easy' instances, for which suboptimal extreme points
have large `gaps', but on which SLB methods must still incur $\Omega(\sqrt{T})$
regret or safety violations, due to an inability to resolve unknown optima to
arbitrary precision. We then analyse a natural doubly-optimistic strategy for
the safe linear bandit problem, DOSS, which uses optimistic estimates of both
reward and safety risks to select actions, and show that despite the lack of
knowledge of constraints or feasible points, DOSS simultaneously obtains tight
instance-dependent $O(\log^2 T)$ bounds on efficacy regret, and $\tilde
O(\sqrt{T})$ bounds on safety violations. Further, when safety is demanded to a
finite precision, violations improve to $O(\log^2 T).$ These results rely on a
novel dual analysis of linear bandits: we argue that \algoname proceeds by
activating noisy versions of at least $d$ constraints in each round, which
allows us to separately analyse rounds where a `poor' set of constraints is
activated, and rounds where `good' sets of constraints are activated. The costs
in the former are controlled to $O(\log^2 T)$ by developing new dual notions of
gaps, based on global sensitivity analyses of linear programs, that quantify
the suboptimality of each such set of constraints. The latter costs are
controlled to $O(1)$ by explicitly analysing the solutions of optimistic play.",2209.13694v3,https://arxiv.org/pdf/2209.13694v3
"Safe Reinforcement Learning of Dynamic High-Dimensional Robotic Tasks:
  Navigation, Manipulation, Interaction","Puze Liu, Kuo Zhang, Davide Tateo, Snehal Jauhri, Zhiyuan Hu, Jan Peters, Georgia Chalvatzaki","Safety is a crucial property of every robotic platform: any control policy
should always comply with actuator limits and avoid collisions with the
environment and humans. In reinforcement learning, safety is even more
fundamental for exploring an environment without causing any damage. While
there are many proposed solutions to the safe exploration problem, only a few
of them can deal with the complexity of the real world. This paper introduces a
new formulation of safe exploration for reinforcement learning of various
robotic tasks. Our approach applies to a wide class of robotic platforms and
enforces safety even under complex collision constraints learned from data by
exploring the tangent space of the constraint manifold. Our proposed approach
achieves state-of-the-art performance in simulated high-dimensional and dynamic
tasks while avoiding collisions with the environment. We show safe real-world
deployment of our learned controller on a TIAGo++ robot, achieving remarkable
performance in manipulation and human-robot interaction tasks.",2209.13308v2,https://arxiv.org/pdf/2209.13308v2
Generating Formal Safety Assurances for High-Dimensional Reachability,"Albert Lin, Somil Bansal","Providing formal safety and performance guarantees for autonomous systems is
becoming increasingly important. Hamilton-Jacobi (HJ) reachability analysis is
a popular formal verification tool for providing these guarantees, since it can
handle general nonlinear system dynamics, bounded adversarial system
disturbances, and state and input constraints. However, it involves solving a
PDE, whose computational and memory complexity scales exponentially with
respect to the state dimensionality, making its direct use on large-scale
systems intractable. A recently proposed method called DeepReach overcomes this
challenge by leveraging a sinusoidal neural PDE solver for high-dimensional
reachability problems, whose computational requirements scale with the
complexity of the underlying reachable tube rather than the state space
dimension. Unfortunately, neural networks can make errors and thus the computed
solution may not be safe, which falls short of achieving our overarching goal
to provide formal safety assurances. In this work, we propose a method to
compute an error bound for the DeepReach solution. This error bound can then be
used for reachable tube correction, resulting in a safe approximation of the
true reachable tube. We also propose a scenario-based optimization approach to
compute a probabilistic bound on this error correction for general nonlinear
dynamical systems. We demonstrate the efficacy of the proposed approach in
obtaining probabilistically safe reachable tubes for high-dimensional
rocket-landing and multi-vehicle collision-avoidance problems.",2209.12336v3,https://arxiv.org/pdf/2209.12336v3
"SAFER: Safe Collision Avoidance using Focused and Efficient Trajectory
  Search with Reinforcement Learning","Mario Srouji, Hugues Thomas, Hubert Tsai, Ali Farhadi, Jian Zhang","Collision avoidance is key for mobile robots and agents to operate safely in
the real world. In this work we present SAFER, an efficient and effective
collision avoidance system that is able to improve safety by correcting the
control commands sent by an operator. It combines real-world reinforcement
learning (RL), search-based online trajectory planning, and automatic emergency
intervention, e.g. automatic emergency braking (AEB). The goal of the RL is to
learn an effective corrective control action that is used in a focused search
for collision-free trajectories, and to reduce the frequency of triggering
automatic emergency braking. This novel setup enables the RL policy to learn
safely and directly on mobile robots in a real-world indoor environment,
minimizing actual crashes even during training. Our real-world experiments show
that, when compared with several baselines, our approach enjoys a higher
average speed, lower crash rate, less emergency intervention, smaller
computation overhead, and smoother overall control.",2209.11789v2,https://arxiv.org/pdf/2209.11789v2
"USC: Uncompromising Spatial Constraints for Safety-Oriented 3D Object
  Detectors in Autonomous Driving","Brian Hsuan-Cheng Liao, Chih-Hong Cheng, Hasan Esen, Alois Knoll","We consider the safety-oriented performance of 3D object detectors in
autonomous driving contexts. Specifically, despite impressive results shown by
the mass literature, developers often find it hard to ensure the safe
deployment of these learning-based perception models. Attributing the challenge
to the lack of safety-oriented metrics, we hereby present uncompromising
spatial constraints (USC), which characterize a simple yet important
localization requirement demanding the predictions to fully cover the objects
when seen from the autonomous vehicle. The constraints, as we formulate using
the perspective and bird's-eye views, can be naturally reflected by
quantitative measures, such that having an object detector with a higher score
implies a lower risk of collision. Finally, beyond model evaluation, we
incorporate the quantitative measures into common loss functions to enable
safety-oriented fine-tuning for existing models. With experiments using the
nuScenes dataset and a closed-loop simulation, our work demonstrates such
considerations of safety notions at the perception level not only improve model
performances beyond accuracy but also allow for a more direct linkage to actual
system safety.",2209.10368v4,https://arxiv.org/pdf/2209.10368v4
Differentiable Safe Controller Design through Control Barrier Functions,"Shuo Yang, Shaoru Chen, Victor M. Preciado, Rahul Mangharam","Learning-based controllers, such as neural network (NN) controllers, can show
high empirical performance but lack formal safety guarantees. To address this
issue, control barrier functions (CBFs) have been applied as a safety filter to
monitor and modify the outputs of learning-based controllers in order to
guarantee the safety of the closed-loop system. However, such modification can
be myopic with unpredictable long-term effects. In this work, we propose a
safe-by-construction NN controller which employs differentiable CBF-based
safety layers, and investigate the performance of safe-by-construction NN
controllers in learning-based control. Specifically, two formulations of
controllers are compared: one is projection-based and the other relies on our
proposed set-theoretic parameterization. Both methods demonstrate improved
closed-loop performance over using CBF as a separate safety filter in numerical
experiments.",2209.10034v2,https://arxiv.org/pdf/2209.10034v2
"Testing Rare Downstream Safety Violations via Upstream Adaptive Sampling
  of Perception Error Models","Craig Innes, Subramanian Ramamoorthy","Testing black-box perceptual-control systems in simulation faces two
difficulties. Firstly, perceptual inputs in simulation lack the fidelity of
real-world sensor inputs. Secondly, for a reasonably accurate perception
system, encountering a rare failure trajectory may require running infeasibly
many simulations. This paper combines perception error models -- surrogates for
a sensor-based detection system -- with state-dependent adaptive importance
sampling. This allows us to efficiently assess the rare failure probabilities
for real-world perceptual control systems within simulation. Our experiments
with an autonomous braking system equipped with an RGB obstacle-detector show
that our method can calculate accurate failure probabilities with an
inexpensive number of simulations. Further, we show how choice of safety metric
can influence the process of learning proposal distributions capable of
reliably sampling high-probability failures.",2209.09674v3,https://arxiv.org/pdf/2209.09674v3
"Case Studies for Computing Density of Reachable States for Safe
  Autonomous Motion Planning","Yue Meng, Zeng Qiu, Md Tawhid Bin Waez, Chuchu Fan","Density of the reachable states can help understand the risk of
safety-critical systems, especially in situations when worst-case reachability
is too conservative. Recent work provides a data-driven approach to compute the
density distribution of autonomous systems' forward reachable states online. In
this paper, we study the use of such approach in combination with model
predictive control for verifiable safe path planning under uncertainties. We
first use the learned density distribution to compute the risk of collision
online. If such risk exceeds the acceptable threshold, our method will plan for
a new path around the previous trajectory, with the risk of collision below the
threshold. Our method is well-suited to handle systems with uncertainties and
complicated dynamics as our data-driven approach does not need an analytical
form of the systems' dynamics and can estimate forward state density with an
arbitrary initial distribution of uncertainties. We design two challenging
scenarios (autonomous driving and hovercraft control) for safe motion planning
in environments with obstacles under system uncertainties. We first show that
our density estimation approach can reach a similar accuracy as the
Monte-Carlo-based method while using only 0.01X training samples. By leveraging
the estimated risk, our algorithm achieves the highest success rate in goal
reaching when enforcing the safety rate above 0.99.",2209.08073v1,https://arxiv.org/pdf/2209.08073v1
"Trustworthy Reinforcement Learning Against Intrinsic Vulnerabilities:
  Robustness, Safety, and Generalizability","Mengdi Xu, Zuxin Liu, Peide Huang, Wenhao Ding, Zhepeng Cen, Bo Li, Ding Zhao","A trustworthy reinforcement learning algorithm should be competent in solving
challenging real-world problems, including {robustly} handling uncertainties,
satisfying {safety} constraints to avoid catastrophic failures, and
{generalizing} to unseen scenarios during deployments. This study aims to
overview these main perspectives of trustworthy reinforcement learning
considering its intrinsic vulnerabilities on robustness, safety, and
generalizability. In particular, we give rigorous formulations, categorize
corresponding methodologies, and discuss benchmarks for each perspective.
Moreover, we provide an outlook section to spur promising future directions
with a brief discussion on extrinsic vulnerabilities considering human
feedback. We hope this survey could bring together separate threads of studies
together in a unified framework and promote the trustworthiness of
reinforcement learning.",2209.08025v1,https://arxiv.org/pdf/2209.08025v1
"Toward Safe and Accelerated Deep Reinforcement Learning for
  Next-Generation Wireless Networks","Ahmad M. Nagib, Hatem Abou-zeid, Hossam S. Hassanein","Deep reinforcement learning (DRL) algorithms have recently gained wide
attention in the wireless networks domain. They are considered promising
approaches for solving dynamic radio resource management (RRM) problems in
next-generation networks. Given their capabilities to build an approximate and
continuously updated model of the wireless network environments, DRL algorithms
can deal with the multifaceted complexity of such environments. Nevertheless,
several challenges hinder the practical adoption of DRL in commercial networks.
In this article, we first discuss two key practical challenges that are faced
but rarely tackled when developing DRL-based RRM solutions. We argue that it is
inevitable to address these DRL-related challenges for DRL to find its way to
RRM commercial solutions. In particular, we discuss the need to have safe and
accelerated DRL-based RRM solutions that mitigate the slow convergence and
performance instability exhibited by DRL algorithms. We then review and
categorize the main approaches used in the RRM domain to develop safe and
accelerated DRL-based solutions. Finally, a case study is conducted to
demonstrate the importance of having safe and accelerated DRL-based RRM
solutions. We employ multiple variants of transfer learning (TL) techniques to
accelerate the convergence of intelligent radio access network (RAN) slicing
DRL-based controllers. We also propose a hybrid TL-based approach and sigmoid
function-based rewards as examples of safe exploration in DRL-based RAN
slicing.",2209.13532v1,https://arxiv.org/pdf/2209.13532v1
Constrained Update Projection Approach to Safe Policy Optimization,"Long Yang, Jiaming Ji, Juntao Dai, Linrui Zhang, Binbin Zhou, Pengfei Li, Yaodong Yang, Gang Pan","Safe reinforcement learning (RL) studies problems where an intelligent agent
has to not only maximize reward but also avoid exploring unsafe areas. In this
study, we propose CUP, a novel policy optimization method based on Constrained
Update Projection framework that enjoys rigorous safety guarantee. Central to
our CUP development is the newly proposed surrogate functions along with the
performance bound. Compared to previous safe RL methods, CUP enjoys the
benefits of 1) CUP generalizes the surrogate functions to generalized advantage
estimator (GAE), leading to strong empirical performance. 2) CUP unifies
performance bounds, providing a better understanding and interpretability for
some existing algorithms; 3) CUP provides a non-convex implementation via only
first-order optimizers, which does not require any strong approximation on the
convexity of the objectives. To validate our CUP method, we compared CUP
against a comprehensive list of safe RL baselines on a wide range of tasks.
Experiments show the effectiveness of CUP both in terms of reward and safety
constraint satisfaction. We have opened the source code of CUP at this link
https://github.com/zmsn-2077/ CUP-safe-rl.",2209.07089v2,https://arxiv.org/pdf/2209.07089v2
"Driving Safety Prediction and Safe Route Mapping Using In-vehicle and
  Roadside Data","Yufei Huang, Mohsen Jafari, Peter Jin","Risk assessment of roadways is commonly practiced based on historical crash
data. Information on driver behaviors and real-time traffic situations is
sometimes missing. In this paper, the Safe Route Mapping (SRM) model, a
methodology for developing dynamic risk heat maps of roadways, is extended to
consider driver behaviors when making predictions. An Android App is designed
to gather drivers' information and upload it to a server. On the server, facial
recognition extracts drivers' data, such as facial landmarks, gaze directions,
and emotions. The driver's drowsiness and distraction are detected, and driving
performance is evaluated. Meanwhile, dynamic traffic information is captured by
a roadside camera and uploaded to the same server. A
longitudinal-scanline-based arterial traffic video analytics is applied to
recognize vehicles from the video to build speed and trajectory profiles. Based
on these data, a LightGBM model is introduced to predict conflict indices for
drivers in the next one or two seconds. Then, multiple data sources, including
historical crash counts and predicted traffic conflict indicators, are combined
using a Fuzzy logic model to calculate risk scores for road segments. The
proposed SRM model is illustrated using data collected from an actual traffic
intersection and a driving simulation platform. The prediction results show
that the model is accurate, and the added driver behavior features will improve
the model's performance. Finally, risk heat maps are generated for
visualization purposes. The authorities can use the dynamic heat map to
designate safe corridors and dispatch law enforcement and drivers for early
warning and trip planning.",2209.05604v1,https://arxiv.org/pdf/2209.05604v1
Safe Reinforcement Learning with Contrastive Risk Prediction,"Hanping Zhang, Yuhong Guo","As safety violations can lead to severe consequences in real-world robotic
applications, the increasing deployment of Reinforcement Learning (RL) in
robotic domains has propelled the study of safe exploration for reinforcement
learning (safe RL). In this work, we propose a risk preventive training method
for safe RL, which learns a statistical contrastive classifier to predict the
probability of a state-action pair leading to unsafe states. Based on the
predicted risk probabilities, we can collect risk preventive trajectories and
reshape the reward function with risk penalties to induce safe RL policies. We
conduct experiments in robotic simulation environments. The results show the
proposed approach has comparable performance with the state-of-the-art
model-based methods and outperforms conventional model-free safe RL approaches.",2209.09648v1,https://arxiv.org/pdf/2209.09648v1
"Hardware faults that matter: Understanding and Estimating the safety
  impact of hardware faults on object detection DNNs","Syed Qutub, Florian Geissler, Yang Peng, Ralf Grafe, Michael Paulitsch, Gereon Hinz, Alois Knoll","Object detection neural network models need to perform reliably in highly
dynamic and safety-critical environments like automated driving or robotics.
Therefore, it is paramount to verify the robustness of the detection under
unexpected hardware faults like soft errors that can impact a systems
perception module. Standard metrics based on average precision produce model
vulnerability estimates at the object level rather than at an image level. As
we show in this paper, this does not provide an intuitive or representative
indicator of the safety-related impact of silent data corruption caused by bit
flips in the underlying memory but can lead to an over- or underestimation of
typical fault-induced hazards. With an eye towards safety-related real-time
applications, we propose a new metric IVMOD (Image-wise Vulnerability Metric
for Object Detection) to quantify vulnerability based on an incorrect
image-wise object detection due to false positive (FPs) or false negative (FNs)
objects, combined with a severity analysis. The evaluation of several
representative object detection models shows that even a single bit flip can
lead to a severe silent data corruption event with potentially critical safety
implications, with e.g., up to (much greater than) 100 FPs generated, or up to
approx. 90% of true positives (TPs) are lost in an image. Furthermore, with a
single stuck-at-1 fault, an entire sequence of images can be affected, causing
temporally persistent ghost detections that can be mistaken for actual objects
(covering up to approx. 83% of the image). Furthermore, actual objects in the
scene are continuously missed (up to approx. 64% of TPs are lost). Our work
establishes a detailed understanding of the safety-related vulnerability of
such critical workloads against hardware faults.",2209.03225v1,https://arxiv.org/pdf/2209.03225v1
A first-order logic characterization of safety and co-safety languages,"Alessandro Cimatti, Luca Geatti, Nicola Gigante, Angelo Montanari, Stefano Tonetta","Linear Temporal Logic (LTL) is one of the most popular temporal logics, that
comes into play in a variety of branches of computer science. Among the various
reasons of its widespread use there are its strong foundational properties: LTL
is equivalent to counter-free omega-automata, to star-free omega-regular
expressions, and (by Kamp's theorem) to the First-Order Theory of Linear Orders
(FO-TLO). Safety and co-safety languages, where a finite prefix suffices to
establish whether a word does not belong or belongs to the language,
respectively, play a crucial role in lowering the complexity of problems like
model checking and reactive synthesis for LTL. SafetyLTL (resp., coSafetyLTL)
is a fragment of LTL where only universal (resp., existential) temporal
modalities are allowed, that recognises safety (resp., co-safety) languages
only. The main contribution of this paper is the introduction of a fragment of
FO-TLO, called SafetyFO, and of its dual coSafetyFO, which are expressively
complete with respect to the LTL-definable safety and co-safety languages. We
prove that they exactly characterize SafetyLTL and coSafetyLTL, respectively, a
result that joins Kamp's theorem, and provides a clearer view of the
characterization of (fragments of) LTL in terms of first-order languages. In
addition, it gives a direct, compact, and self-contained proof that any safety
language definable in LTL is definable in SafetyLTL as well. As a by-product,
we obtain some interesting results on the expressive power of the weak tomorrow
operator of SafetyLTL, interpreted over finite and infinite words. Moreover, we
prove that, when interpreted over finite words, SafetyLTL (resp. coSafetyLTL)
devoid of the tomorrow (resp., weak tomorrow) operator captures the safety
(resp., co-safety) fragment of LTL over finite words.",2209.02307v5,https://arxiv.org/pdf/2209.02307v5
Negative Human Rights as a Basis for Long-term AI Safety and Regulation,"Ondrej Bajgar, Jan Horenovsky","If autonomous AI systems are to be reliably safe in novel situations, they
will need to incorporate general principles guiding them to recognize and avoid
harmful behaviours. Such principles may need to be supported by a binding
system of regulation, which would need the underlying principles to be widely
accepted. They should also be specific enough for technical implementation.
Drawing inspiration from law, this article explains how negative human rights
could fulfil the role of such principles and serve as a foundation both for an
international regulatory system and for building technical safety constraints
for future AI systems.",2208.14788v2,https://arxiv.org/pdf/2208.14788v2
Unifying Evaluation of Machine Learning Safety Monitors,"Joris Guerin, Raul Sena Ferreira, Kevin Delmas, Jérémie Guiochet","With the increasing use of Machine Learning (ML) in critical autonomous
systems, runtime monitors have been developed to detect prediction errors and
keep the system in a safe state during operations. Monitors have been proposed
for different applications involving diverse perception tasks and ML models,
and specific evaluation procedures and metrics are used for different contexts.
This paper introduces three unified safety-oriented metrics, representing the
safety benefits of the monitor (Safety Gain), the remaining safety gaps after
using it (Residual Hazard), and its negative impact on the system's performance
(Availability Cost). To compute these metrics, one requires to define two
return functions, representing how a given ML prediction will impact expected
future rewards and hazards. Three use-cases (classification, drone landing, and
autonomous driving) are used to demonstrate how metrics from the literature can
be expressed in terms of the proposed metrics. Experimental results on these
examples show how different evaluation choices impact the perceived performance
of a monitor. As our formalism requires us to formulate explicit safety
assumptions, it allows us to ensure that the evaluation conducted matches the
high-level system requirements.",2208.14660v1,https://arxiv.org/pdf/2208.14660v1
"Recursively Feasible Probabilistic Safe Online Learning with Control
  Barrier Functions","Fernando Castañeda, Jason J. Choi, Wonsuhk Jung, Bike Zhang, Claire J. Tomlin, Koushil Sreenath","Learning-based control schemes have recently shown great efficacy performing
complex tasks for a wide variety of applications. However, in order to deploy
them in real systems, it is of vital importance to guarantee that the system
will remain safe during online training and execution. Among the currently most
popular methods to tackle this challenge, Control Barrier Functions (CBFs)
serve as mathematical tools that provide a formal safety-preserving control
synthesis procedure for systems with known dynamics. In this paper, we first
introduce a model-uncertainty-aware reformulation of CBF-based safety-critical
controllers using Gaussian Process (GP) regression to bridge the gap between an
approximate mathematical model and the real system. Compared to previous
approaches, we study the feasibility of the resulting robust safety-critical
controller. This feasibility analysis results in a set of richness conditions
that the available information about the system should satisfy to guarantee
that a safe control action can be found at all times. We then use these
conditions to devise an event-triggered online data collection strategy that
ensures the recursive feasibility of the learned safety-critical controller.
Our proposed methodology endows the system with the ability to reason at all
times about whether the current information at its disposal is enough to ensure
safety or if new measurements are required. This, in turn, allows us to provide
formal results of forward invariance of a safe set with high probability, even
in a priori unexplored regions. Finally, we validate the proposed framework in
numerical simulations of an adaptive cruise control system and a kinematic
vehicle.",2208.10733v2,https://arxiv.org/pdf/2208.10733v2
"Relational Action Bases: Formalization, Effective Safety Verification,
  and Invariants (Extended Version)","Silvio Ghilardi, Alessandro Gianola, Marco Montali, Andrey Rivkin","Modeling and verification of dynamic systems operating over a relational
representation of states are increasingly investigated problems in AI, Business
Process Management, and Database Theory. To make these systems amenable to
verification, the amount of information stored in each relational state needs
to be bounded, or restrictions are imposed on the preconditions and effects of
actions. We introduce the general framework of relational action bases (RABs),
which generalizes existing models by lifting both these restrictions: unbounded
relational states can be evolved through actions that can quantify both
existentially and universally over the data, and that can exploit numerical
datatypes with arithmetic predicates. We then study parameterized safety of
RABs via (approximated) SMT-based backward search, singling out essential
meta-properties of the resulting procedure, and showing how it can be realized
by an off-the-shelf combination of existing verification modules of the
state-of-the-art MCMT model checker. We demonstrate the effectiveness of this
approach on a benchmark of data-aware business processes. Finally, we show how
universal invariants can be exploited to make this procedure fully correct.",2208.06377v2,https://arxiv.org/pdf/2208.06377v2
"Safety and Performance, Why not Both? Bi-Objective Optimized Model
  Compression toward AI Software Deployment","Jie Zhu, Leye Wang, Xiao Han","The size of deep learning models in artificial intelligence (AI) software is
increasing rapidly, which hinders the large-scale deployment on
resource-restricted devices (e.g., smartphones). To mitigate this issue, AI
software compression plays a crucial role, which aims to compress model size
while keeping high performance. However, the intrinsic defects in the big model
may be inherited by the compressed one. Such defects may be easily leveraged by
attackers, since the compressed models are usually deployed in a large number
of devices without adequate protection. In this paper, we try to address the
safe model compression problem from a safety-performance co-optimization
perspective. Specifically, inspired by the test-driven development (TDD)
paradigm in software engineering, we propose a test-driven sparse training
framework called SafeCompress. By simulating the attack mechanism as the safety
test, SafeCompress can automatically compress a big model to a small one
following the dynamic sparse training paradigm. Further, considering a
representative attack, i.e., membership inference attack (MIA), we develop a
concrete safe model compression mechanism, called MIA-SafeCompress. Extensive
experiments are conducted to evaluate MIA-SafeCompress on five datasets for
both computer vision and natural language processing tasks. The results verify
the effectiveness and generalization of our method. We also discuss how to
adapt SafeCompress to other attacks besides MIA, demonstrating the flexibility
of SafeCompress.",2208.05969v2,https://arxiv.org/pdf/2208.05969v2
"Exploring the trade off between human driving imitation and safety for
  traffic simulation","Yann Koeberle, Stefano Sabatini, Dzmitry Tsishkou, Christophe Sabourin","Traffic simulation has gained a lot of interest for quantitative evaluation
of self driving vehicles performance. In order for a simulator to be a valuable
test bench, it is required that the driving policy animating each traffic agent
in the scene acts as humans would do while maintaining minimal safety
guarantees. Learning the driving policies of traffic agents from recorded human
driving data or through reinforcement learning seems to be an attractive
solution for the generation of realistic and highly interactive traffic
situations in uncontrolled intersections or roundabouts. In this work, we show
that a trade-off exists between imitating human driving and maintaining safety
when learning driving policies. We do this by comparing how various Imitation
learning and Reinforcement learning algorithms perform when applied to the
driving task. We also propose a multi objective learning algorithm (MOPPO) that
improves both objectives together. We test our driving policies on highly
interactive driving scenarios extracted from INTERACTION Dataset to evaluate
how human-like they behave.",2208.04803v1,https://arxiv.org/pdf/2208.04803v1
Privacy Safe Representation Learning via Frequency Filtering Encoder,"Jonghu Jeong, Minyong Cho, Philipp Benz, Jinwoo Hwang, Jeewook Kim, Seungkwan Lee, Tae-hoon Kim","Deep learning models are increasingly deployed in real-world applications.
These models are often deployed on the server-side and receive user data in an
information-rich representation to solve a specific task, such as image
classification. Since images can contain sensitive information, which users
might not be willing to share, privacy protection becomes increasingly
important. Adversarial Representation Learning (ARL) is a common approach to
train an encoder that runs on the client-side and obfuscates an image. It is
assumed, that the obfuscated image can safely be transmitted and used for the
task on the server without privacy concerns. However, in this work, we find
that training a reconstruction attacker can successfully recover the original
image of existing ARL methods. To this end, we introduce a novel ARL method
enhanced through low-pass filtering, limiting the available information amount
to be encoded in the frequency domain. Our experimental results reveal that our
approach withstands reconstruction attacks while outperforming previous
state-of-the-art methods regarding the privacy-utility trade-off. We further
conduct a user study to qualitatively assess our defense of the reconstruction
attack.",2208.02482v1,https://arxiv.org/pdf/2208.02482v1
"Differentiable Predictive Control with Safety Guarantees: A Control
  Barrier Function Approach","Wenceslao Shaw Cortez, Jan Drgona, Aaron Tuor, Mahantesh Halappanavar, Draguna Vrabie","We develop a novel form of differentiable predictive control (DPC) with
safety and robustness guarantees based on control barrier functions. DPC is an
unsupervised learning-based method for obtaining approximate solutions to
explicit model predictive control (MPC) problems. In DPC, the predictive
control policy parametrized by a neural network is optimized offline via direct
policy gradients obtained by automatic differentiation of the MPC problem. The
proposed approach exploits a new form of sampled-data barrier function to
enforce offline and online safety requirements in DPC settings while only
interrupting the neural network-based controller near the boundary of the safe
set. The effectiveness of the proposed approach is demonstrated in simulation.",2208.02319v1,https://arxiv.org/pdf/2208.02319v1
Vision-Based Safety System for Barrierless Human-Robot Collaboration,"Lina María Amaya-Mejía, Nicolás Duque-Suárez, Daniel Jaramillo-Ramírez, Carol Martinez","Human safety has always been the main priority when working near an
industrial robot. With the rise of Human-Robot Collaborative environments,
physical barriers to avoiding collisions have been disappearing, increasing the
risk of accidents and the need for solutions that ensure a safe Human-Robot
Collaboration. This paper proposes a safety system that implements Speed and
Separation Monitoring (SSM) type of operation. For this, safety zones are
defined in the robot's workspace following current standards for industrial
collaborative robots. A deep learning-based computer vision system detects,
tracks, and estimates the 3D position of operators close to the robot. The
robot control system receives the operator's 3D position and generates 3D
representations of them in a simulation environment. Depending on the zone
where the closest operator was detected, the robot stops or changes its
operating speed. Three different operation modes in which the human and robot
interact are presented. Results show that the vision-based system can correctly
detect and classify in which safety zone an operator is located and that the
different proposed operation modes ensure that the robot's reaction and stop
time are within the required time limits to guarantee safety.",2208.02010v1,https://arxiv.org/pdf/2208.02010v1
Safe Policy Improvement Approaches and their Limitations,"Philipp Scholl, Felix Dietrich, Clemens Otte, Steffen Udluft","Safe Policy Improvement (SPI) is an important technique for offline
reinforcement learning in safety critical applications as it improves the
behavior policy with a high probability. We classify various SPI approaches
from the literature into two groups, based on how they utilize the uncertainty
of state-action pairs. Focusing on the Soft-SPIBB (Safe Policy Improvement with
Soft Baseline Bootstrapping) algorithms, we show that their claim of being
provably safe does not hold. Based on this finding, we develop adaptations, the
Adv-Soft-SPIBB algorithms, and show that they are provably safe. A heuristic
adaptation, Lower-Approx-Soft-SPIBB, yields the best performance among all
SPIBB algorithms in extensive experiments on two benchmarks. We also check the
safety guarantees of the provably safe algorithms and show that huge amounts of
data are necessary such that the safety bounds become useful in practice.",2208.00724v1,https://arxiv.org/pdf/2208.00724v1
"Sample-efficient Safe Learning for Online Nonlinear Control with Control
  Barrier Functions","Wenhao Luo, Wen Sun, Ashish Kapoor","Reinforcement Learning (RL) and continuous nonlinear control have been
successfully deployed in multiple domains of complicated sequential
decision-making tasks. However, given the exploration nature of the learning
process and the presence of model uncertainty, it is challenging to apply them
to safety-critical control tasks due to the lack of safety guarantee. On the
other hand, while combining control-theoretical approaches with learning
algorithms has shown promise in safe RL applications, the sample efficiency of
safe data collection process for control is not well addressed. In this paper,
we propose a \emph{provably} sample efficient episodic safe learning framework
for online control tasks that leverages safe exploration and exploitation in an
unknown, nonlinear dynamical system. In particular, the framework 1) extends
control barrier functions (CBFs) in a stochastic setting to achieve provable
high-probability safety under uncertainty during model learning and 2)
integrates an optimism-based exploration strategy to efficiently guide the safe
exploration process with learned dynamics for \emph{near optimal} control
performance. We provide formal analysis on the episodic regret bound against
the optimal controller and probabilistic safety with theoretical guarantees.
Simulation results are provided to demonstrate the effectiveness and efficiency
of the proposed algorithm.",2207.14419v1,https://arxiv.org/pdf/2207.14419v1
"Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion
  Transformer","Hao Shao, Letian Wang, RuoBing Chen, Hongsheng Li, Yu Liu","Large-scale deployment of autonomous vehicles has been continually delayed
due to safety concerns. On the one hand, comprehensive scene understanding is
indispensable, a lack of which would result in vulnerability to rare but
complex traffic situations, such as the sudden emergence of unknown objects.
However, reasoning from a global context requires access to sensors of multiple
types and adequate fusion of multi-modal sensor signals, which is difficult to
achieve. On the other hand, the lack of interpretability in learning models
also hampers the safety with unverifiable failure causes. In this paper, we
propose a safety-enhanced autonomous driving framework, named Interpretable
Sensor Fusion Transformer(InterFuser), to fully process and fuse information
from multi-modal multi-view sensors for achieving comprehensive scene
understanding and adversarial event detection. Besides, intermediate
interpretable features are generated from our framework, which provide more
semantics and are exploited to better constrain actions to be within the safe
sets. We conducted extensive experiments on CARLA benchmarks, where our model
outperforms prior methods, ranking the first on the public CARLA Leaderboard.
Our code will be made available at https://github.com/opendilab/InterFuser",2207.14024v5,https://arxiv.org/pdf/2207.14024v5
"Safe and Robust Experience Sharing for Deterministic Policy Gradient
  Algorithms","Baturay Saglam, Dogan C. Cicek, Furkan B. Mutlu, Suleyman S. Kozat","Learning in high dimensional continuous tasks is challenging, mainly when the
experience replay memory is very limited. We introduce a simple yet effective
experience sharing mechanism for deterministic policies in continuous action
domains for the future off-policy deep reinforcement learning applications in
which the allocated memory for the experience replay buffer is limited. To
overcome the extrapolation error induced by learning from other agents'
experiences, we facilitate our algorithm with a novel off-policy correction
technique without any action probability estimates. We test the effectiveness
of our method in challenging OpenAI Gym continuous control tasks and conclude
that it can achieve a safe experience sharing across multiple agents and
exhibits a robust performance when the replay memory is strictly limited.",2207.13453v1,https://arxiv.org/pdf/2207.13453v1
"A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot
  Manipulation","Xiang Zhu, Shucheng Kang, Jianyu Chen","Reinforcement learning shows great potential to solve complex contact-rich
robot manipulation tasks. However, the safety of using RL in the real world is
a crucial problem, since unexpected dangerous collisions might happen when the
RL policy is imperfect during training or in unseen scenarios. In this paper,
we propose a contact-safe reinforcement learning framework for contact-rich
robot manipulation, which maintains safety in both the task space and joint
space. When the RL policy causes unexpected collisions between the robot arm
and the environment, our framework is able to immediately detect the collision
and ensure the contact force to be small. Furthermore, the end-effector is
enforced to perform contact-rich tasks compliantly, while keeping robust to
external disturbances. We train the RL policy in simulation and transfer it to
the real robot. Real world experiments on robot wiping tasks show that our
method is able to keep the contact force small both in task space and joint
space even when the policy is under unseen scenario with unexpected collision,
while rejecting the disturbances on the main task.",2207.13438v1,https://arxiv.org/pdf/2207.13438v1
"Exploring the Design of Adaptation Protocols for Improved Generalization
  and Machine Learning Safety","Puja Trivedi, Danai Koutra, Jayaraman J. Thiagarajan","While directly fine-tuning (FT) large-scale, pretrained models on
task-specific data is well-known to induce strong in-distribution task
performance, recent works have demonstrated that different adaptation
protocols, such as linear probing (LP) prior to FT, can improve
out-of-distribution generalization. However, the design space of such
adaptation protocols remains under-explored and the evaluation of such
protocols has primarily focused on distribution shifts. Therefore, in this
work, we evaluate common adaptation protocols across distributions shifts and
machine learning safety metrics (e.g., anomaly detection, calibration,
robustness to corruptions). We find that protocols induce disparate trade-offs
that were not apparent from prior evaluation. Further, we demonstrate that
appropriate pairing of data augmentation and protocol can substantially
mitigate this trade-off. Finally, we hypothesize and empirically see that using
hardness-promoting augmentations during LP and then FT with augmentations may
be particularly effective for trade-off mitigation.",2207.12615v1,https://arxiv.org/pdf/2207.12615v1
"Log Barriers for Safe Black-box Optimization with Application to Safe
  Reinforcement Learning","Ilnura Usmanova, Yarden As, Maryam Kamgarpour, Andreas Krause","Optimizing noisy functions online, when evaluating the objective requires
experiments on a deployed system, is a crucial task arising in manufacturing,
robotics and many others. Often, constraints on safe inputs are unknown ahead
of time, and we only obtain noisy information, indicating how close we are to
violating the constraints. Yet, safety must be guaranteed at all times, not
only for the final output of the algorithm.
  We introduce a general approach for seeking a stationary point in high
dimensional non-linear stochastic optimization problems in which maintaining
safety during learning is crucial. Our approach called LB-SGD is based on
applying stochastic gradient descent (SGD) with a carefully chosen adaptive
step size to a logarithmic barrier approximation of the original problem. We
provide a complete convergence analysis of non-convex, convex, and
strongly-convex smooth constrained problems, with first-order and zeroth-order
feedback. Our approach yields efficient updates and scales better with
dimensionality compared to existing approaches.
  We empirically compare the sample complexity and the computational cost of
our method with existing safe learning approaches. Beyond synthetic benchmarks,
we demonstrate the effectiveness of our approach on minimizing constraint
violation in policy search tasks in safe reinforcement learning (RL).",2207.10415v2,https://arxiv.org/pdf/2207.10415v2
"Robust Action Governor for Uncertain Piecewise Affine Systems with
  Non-convex Constraints and Safe Reinforcement Learning","Yutong Li, Nan Li, H. Eric Tseng, Anouck Girard, Dimitar Filev, Ilya Kolmanovsky","The action governor is an add-on scheme to a nominal control loop that
monitors and adjusts the control actions to enforce safety specifications
expressed as pointwise-in-time state and control constraints. In this paper, we
introduce the Robust Action Governor (RAG) for systems the dynamics of which
can be represented using discrete-time Piecewise Affine (PWA) models with both
parametric and additive uncertainties and subject to non-convex constraints. We
develop the theoretical properties and computational approaches for the RAG.
After that, we introduce the use of the RAG for realizing safe Reinforcement
Learning (RL), i.e., ensuring all-time constraint satisfaction during online RL
exploration-and-exploitation process. This development enables safe real-time
evolution of the control policy and adaptation to changes in the operating
environment and system parameters (due to aging, damage, etc.). We illustrate
the effectiveness of the RAG in constraint enforcement and safe RL using the
RAG by considering their applications to a soft-landing problem of a
mass-spring-damper system.",2207.08240v1,https://arxiv.org/pdf/2207.08240v1
Security and Safety Aspects of AI in Industry Applications,Hans Dermot Doran,"In this relatively informal discussion-paper we summarise issues in the
domains of safety and security in machine learning that will affect industry
sectors in the next five to ten years. Various products using neural network
classification, most often in vision related applications but also in
predictive maintenance, have been researched and applied in real-world
applications in recent years. Nevertheless, reports of underlying problems in
both safety and security related domains, for instance adversarial attacks have
unsettled early adopters and are threatening to hinder wider scale adoption of
this technology. The problem for real-world applicability lies in being able to
assess the risk of applying these technologies. In this discussion-paper we
describe the process of arriving at a machine-learnt neural network classifier
pointing out safety and security vulnerabilities in that workflow, citing
relevant research where appropriate.",2207.10809v1,https://arxiv.org/pdf/2207.10809v1
"Computing-In-Memory Neural Network Accelerators for Safety-Critical
  Systems: Can Small Device Variations Be Disastrous?","Zheyu Yan, Xiaobo Sharon Hu, Yiyu Shi","Computing-in-Memory (CiM) architectures based on emerging non-volatile memory
(NVM) devices have demonstrated great potential for deep neural network (DNN)
acceleration thanks to their high energy efficiency. However, NVM devices
suffer from various non-idealities, especially device-to-device variations due
to fabrication defects and cycle-to-cycle variations due to the stochastic
behavior of devices. As such, the DNN weights actually mapped to NVM devices
could deviate significantly from the expected values, leading to large
performance degradation. To address this issue, most existing works focus on
maximizing average performance under device variations. This objective would
work well for general-purpose scenarios. But for safety-critical applications,
the worst-case performance must also be considered. Unfortunately, this has
been rarely explored in the literature. In this work, we formulate the problem
of determining the worst-case performance of CiM DNN accelerators under the
impact of device variations. We further propose a method to effectively find
the specific combination of device variation in the high-dimensional space that
leads to the worst-case performance. We find that even with very small device
variations, the accuracy of a DNN can drop drastically, causing concerns when
deploying CiM accelerators in safety-critical applications. Finally, we show
that surprisingly none of the existing methods used to enhance average DNN
performance in CiM accelerators are very effective when extended to enhance the
worst-case performance, and further research down the road is needed to address
this problem.",2207.07626v1,https://arxiv.org/pdf/2207.07626v1
"Work In Progress: Safety and Robustness Verification of
  Autoencoder-Based Regression Models using the NNV Tool","Neelanjana Pal, Taylor T Johnson","This work in progress paper introduces robustness verification for
autoencoder-based regression neural network (NN) models, following
state-of-the-art approaches for robustness verification of image classification
NNs. Despite the ongoing progress in developing verification methods for safety
and robustness in various deep neural networks (DNNs), robustness checking of
autoencoder models has not yet been considered. We explore this open space of
research and check ways to bridge the gap between existing DNN verification
methods by extending existing robustness analysis methods for such autoencoder
networks. While classification models using autoencoders work more or less
similar to image classification NNs, the functionality of regression models is
distinctly different. We introduce two definitions of robustness evaluation
metrics for autoencoder-based regression models, specifically the percentage
robustness and un-robustness grade. We also modified the existing Imagestar
approach, adjusting the variables to take care of the specific input types for
regression networks. The approach is implemented as an extension of NNV, then
applied and evaluated on a dataset, with a case study experiment shown using
the same dataset. As per the authors' understanding, this work in progress
paper is the first to show possible reachability analysis of autoencoder-based
NNs.",2207.06759v1,https://arxiv.org/pdf/2207.06759v1
"Safe reinforcement learning for multi-energy management systems with
  known constraint functions","Glenn Ceusters, Luis Ramirez Camargo, Rüdiger Franke, Ann Nowé, Maarten Messagie","Reinforcement learning (RL) is a promising optimal control technique for
multi-energy management systems. It does not require a model a priori -
reducing the upfront and ongoing project-specific engineering effort and is
capable of learning better representations of the underlying system dynamics.
However, vanilla RL does not provide constraint satisfaction guarantees -
resulting in various potentially unsafe interactions within its safety-critical
environment. In this paper, we present two novel safe RL methods, namely
SafeFallback and GiveSafe, where the safety constraint formulation is decoupled
from the RL formulation. These provide hard-constraint, rather than soft- and
chance-constraint, satisfaction guarantees both during training a (near)
optimal policy (which involves exploratory and exploitative, i.e. greedy,
steps) as well as during deployment of any policy (e.g. random agents or
offline trained RL agents). This without the need of solving a mathematical
program, resulting in less computational power requirements and a more flexible
constraint function formulation (no derivative information is required). In a
simulated multi-energy systems case study we have shown that both methods start
with a significantly higher utility (i.e. useful policy) compared to a vanilla
RL benchmark and Optlayer benchmark (94,6% and 82,8% compared to 35,5% and
77,8%) and that the proposed SafeFallback method even can outperform the
vanilla RL benchmark (102,9% to 100%). We conclude that both methods are viably
safety constraint handling techniques applicable beyond RL, as demonstrated
with random policies while still providing hard-constraint guarantees.",2207.03830v4,https://arxiv.org/pdf/2207.03830v4
A Safe Semi-supervised Graph Convolution Network,"Zhi Yang, Yadong Yan, Haitao Gan, Jing Zhao, Zhiwei Ye","In the semi-supervised learning field, Graph Convolution Network (GCN), as a
variant model of GNN, has achieved promising results for non-Euclidean data by
introducing convolution into GNN. However, GCN and its variant models fail to
safely use the information of risk unlabeled data, which will degrade the
performance of semi-supervised learning. Therefore, we propose a Safe GCN
framework (Safe-GCN) to improve the learning performance. In the Safe-GCN, we
design an iterative process to label the unlabeled data. In each iteration, a
GCN and its supervised version(S-GCN) are learned to find the unlabeled data
with high confidence. The high-confidence unlabeled data and their pseudo
labels are then added to the label set. Finally, both added unlabeled data and
labeled ones are used to train a S-GCN which can achieve the safe exploration
of the risk unlabeled data and enable safe use of large numbers of unlabeled
data. The performance of Safe-GCN is evaluated on three well-known citation
network datasets and the obtained results demonstrate the effectiveness of the
proposed framework over several graph-based semi-supervised learning methods.",2207.01960v1,https://arxiv.org/pdf/2207.01960v1
Safe Reinforcement Learning via Confidence-Based Filters,"Sebastian Curi, Armin Lederer, Sandra Hirche, Andreas Krause","Ensuring safety is a crucial challenge when deploying reinforcement learning
(RL) to real-world systems. We develop confidence-based safety filters, a
control-theoretic approach for certifying state safety constraints for nominal
policies learned via standard RL techniques, based on probabilistic dynamics
models. Our approach is based on a reformulation of state constraints in terms
of cost functions, reducing safety verification to a standard RL task. By
exploiting the concept of hallucinating inputs, we extend this formulation to
determine a ""backup"" policy that is safe for the unknown system with high
probability. Finally, the nominal policy is minimally adjusted at every time
step during a roll-out towards the backup policy, such that safe recovery can
be guaranteed afterwards. We provide formal safety guarantees, and empirically
demonstrate the effectiveness of our approach.",2207.01337v1,https://arxiv.org/pdf/2207.01337v1
"Safe Decision-making for Lane-change of Autonomous Vehicles via Human
  Demonstration-aided Reinforcement Learning","Jingda Wu, Wenhui Huang, Niels de Boer, Yanghui Mo, Xiangkun He, Chen Lv","Decision-making is critical for lane change in autonomous driving.
Reinforcement learning (RL) algorithms aim to identify the values of behaviors
in various situations and thus they become a promising pathway to address the
decision-making problem. However, poor runtime safety hinders RL-based
decision-making strategies from complex driving tasks in practice. To address
this problem, human demonstrations are incorporated into the RL-based
decision-making strategy in this paper. Decisions made by human subjects in a
driving simulator are treated as safe demonstrations, which are stored into the
replay buffer and then utilized to enhance the training process of RL. A
complex lane change task in an off-ramp scenario is established to examine the
performance of the developed strategy. Simulation results suggest that human
demonstrations can effectively improve the safety of decisions of RL. And the
proposed strategy surpasses other existing learning-based decision-making
strategies with respect to multiple driving performances.",2207.00448v2,https://arxiv.org/pdf/2207.00448v2
Memory Safe Computations with XLA Compiler,"Artem Artemev, Tilman Roeder, Mark van der Wilk","Software packages like TensorFlow and PyTorch are designed to support linear
algebra operations, and their speed and usability determine their success.
However, by prioritising speed, they often neglect memory requirements. As a
consequence, the implementations of memory-intensive algorithms that are
convenient in terms of software design can often not be run for large problems
due to memory overflows. Memory-efficient solutions require complex programming
approaches with significant logic outside the computational framework. This
impairs the adoption and use of such algorithms. To address this, we developed
an XLA compiler extension that adjusts the computational data-flow
representation of an algorithm according to a user-specified memory limit. We
show that k-nearest neighbour and sparse Gaussian process regression methods
can be run at a much larger scale on a single device, where standard
implementations would have failed. Our approach leads to better use of hardware
resources. We believe that further focus on removing memory constraints at a
compiler level will widen the range of machine learning methods that can be
developed in the future.",2206.14148v1,https://arxiv.org/pdf/2206.14148v1
"Safe Exploration Incurs Nearly No Additional Sample Complexity for
  Reward-free RL","Ruiquan Huang, Jing Yang, Yingbin Liang","Reward-free reinforcement learning (RF-RL), a recently introduced RL
paradigm, relies on random action-taking to explore the unknown environment
without any reward feedback information. While the primary goal of the
exploration phase in RF-RL is to reduce the uncertainty in the estimated model
with minimum number of trajectories, in practice, the agent often needs to
abide by certain safety constraint at the same time. It remains unclear how
such safe exploration requirement would affect the corresponding sample
complexity in order to achieve the desired optimality of the obtained policy in
planning. In this work, we make a first attempt to answer this question. In
particular, we consider the scenario where a safe baseline policy is known
beforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET)
framework. We then particularize the SWEET framework to the tabular and the
low-rank MDP settings, and develop algorithms coined Tabular-SWEET and
Low-rank-SWEET, respectively. Both algorithms leverage the concavity and
continuity of the newly introduced truncated value functions, and are
guaranteed to achieve zero constraint violation during exploration with high
probability. Furthermore, both algorithms can provably find a near-optimal
policy subject to any constraint in the planning phase. Remarkably, the sample
complexities under both algorithms match or even outperform the state of the
art in their constraint-free counterparts up to some constant factors, proving
that safety constraint hardly increases the sample complexity for RF-RL.",2206.14057v3,https://arxiv.org/pdf/2206.14057v3
Active Learning with Safety Constraints,"Romain Camilleri, Andrew Wagenmaker, Jamie Morgenstern, Lalit Jain, Kevin Jamieson","Active learning methods have shown great promise in reducing the number of
samples necessary for learning. As automated learning systems are adopted into
real-time, real-world decision-making pipelines, it is increasingly important
that such algorithms are designed with safety in mind. In this work we
investigate the complexity of learning the best safe decision in interactive
environments. We reduce this problem to a constrained linear bandits problem,
where our goal is to find the best arm satisfying certain (unknown) safety
constraints. We propose an adaptive experimental design-based algorithm, which
we show efficiently trades off between the difficulty of showing an arm is
unsafe vs suboptimal. To our knowledge, our results are the first on best-arm
identification in linear bandits with safety constraints. In practice, we
demonstrate that this approach performs well on synthetic and real world
datasets.",2206.11183v1,https://arxiv.org/pdf/2206.11183v1
"Safe and Psychologically Pleasant Traffic Signal Control with
  Reinforcement Learning using Action Masking","Arthur Müller, Matthia Sabatelli","Reinforcement learning (RL) for traffic signal control (TSC) has shown better
performance in simulation for controlling the traffic flow of intersections
than conventional approaches. However, due to several challenges, no RL-based
TSC has been deployed in the field yet. One major challenge for real-world
deployment is to ensure that all safety requirements are met at all times
during operation. We present an approach to ensure safety in a real-world
intersection by using an action space that is safe by design. The action space
encompasses traffic phases, which represent the combination of non-conflicting
signal colors of the intersection. Additionally, an action masking mechanism
makes sure that only appropriate phase transitions are carried out. Another
challenge for real-world deployment is to ensure a control behavior that avoids
stress for road users. We demonstrate how to achieve this by incorporating
domain knowledge through extending the action masking mechanism. We test and
verify our approach in a realistic simulation scenario. By ensuring safety and
psychologically pleasant control behavior, our approach drives development
towards real-world deployment of RL for TSC.",2206.10122v1,https://arxiv.org/pdf/2206.10122v1
"Guided Safe Shooting: model based reinforcement learning with safety
  constraints","Giuseppe Paolo, Jonas Gonzalez-Billandon, Albert Thomas, Balázs Kégl","In the last decade, reinforcement learning successfully solved complex
control tasks and decision-making problems, like the Go board game. Yet, there
are few success stories when it comes to deploying those algorithms to
real-world scenarios. One of the reasons is the lack of guarantees when dealing
with and avoiding unsafe states, a fundamental requirement in critical control
engineering systems. In this paper, we introduce Guided Safe Shooting (GuSS), a
model-based RL approach that can learn to control systems with minimal
violations of the safety constraints. The model is learned on the data
collected during the operation of the system in an iterated batch fashion, and
is then used to plan for the best action to perform at each time step. We
propose three different safe planners, one based on a simple random shooting
strategy and two based on MAP-Elites, a more advanced divergent-search
algorithm. Experiments show that these planners help the learning agent avoid
unsafe situations while maximally exploring the state space, a necessary aspect
when learning an accurate model of the system. Furthermore, compared to
model-free approaches, learning a model allows GuSS reducing the number of
interactions with the real-system while still reaching high rewards, a
fundamental requirement when handling engineering systems.",2206.09743v1,https://arxiv.org/pdf/2206.09743v1
"SafeRL-Kit: Evaluating Efficient Reinforcement Learning Methods for Safe
  Autonomous Driving","Linrui Zhang, Qin Zhang, Li Shen, Bo Yuan, Xueqian Wang","Safe reinforcement learning (RL) has achieved significant success on
risk-sensitive tasks and shown promise in autonomous driving (AD) as well.
Considering the distinctiveness of this community, efficient and reproducible
baselines are still lacking for safe AD. In this paper, we release SafeRL-Kit
to benchmark safe RL methods for AD-oriented tasks. Concretely, SafeRL-Kit
contains several latest algorithms specific to zero-constraint-violation tasks,
including Safety Layer, Recovery RL, off-policy Lagrangian method, and Feasible
Actor-Critic. In addition to existing approaches, we propose a novel
first-order method named Exact Penalty Optimization (EPO) and sufficiently
demonstrate its capability in safe AD. All algorithms in SafeRL-Kit are
implemented (i) under the off-policy setting, which improves sample efficiency
and can better leverage past logs; (ii) with a unified learning framework,
providing off-the-shelf interfaces for researchers to incorporate their
domain-specific knowledge into fundamental safe RL methods. Conclusively, we
conduct a comparative evaluation of the above algorithms in SafeRL-Kit and shed
light on their efficacy for safe autonomous driving. The source code is
available at \href{ https://github.com/zlr20/saferl_kit}{this https URL}.",2206.08528v1,https://arxiv.org/pdf/2206.08528v1
"Barrier Certified Safety Learning Control: When Sum-of-Square
  Programming Meets Reinforcement Learning","Hejun Huang, Zhenglong Li, Dongkun Han","Safety guarantee is essential in many engineering implementations.
Reinforcement learning provides a useful way to strengthen safety. However,
reinforcement learning algorithms cannot completely guarantee safety over
realistic operations. To address this issue, this work adopts control barrier
functions over reinforcement learning, and proposes a compensated algorithm to
completely maintain safety. Specifically, a sum-of-squares programming has been
exploited to search for the optimal controller, and tune the learning
hyperparameters simultaneously. Thus, the control actions are pledged to be
always within the safe region. The effectiveness of proposed method is
demonstrated via an inverted pendulum model. Compared to quadratic programming
based reinforcement learning methods, our sum-of-squares programming based
reinforcement learning has shown its superiority.",2206.07915v2,https://arxiv.org/pdf/2206.07915v2
"Machine vision for vial positioning detection toward the safe automation
  of material synthesis","Leslie Ching Ow Tiong, Hyuk Jun Yoo, Na Yeon Kim, Kwan-Young Lee, Sang Soo Han, Donghun Kim","Although robot-based automation in chemistry laboratories can accelerate the
material development process, surveillance-free environments may lead to
dangerous accidents primarily due to machine control errors. Object detection
techniques can play vital roles in addressing these safety issues; however,
state-of-the-art detectors, including single-shot detector (SSD) models, suffer
from insufficient accuracy in environments involving complex and noisy scenes.
With the aim of improving safety in a surveillance-free laboratory, we report a
novel deep learning (DL)-based object detector, namely, DenseSSD. For the
foremost and frequent problem of detecting vial positions, DenseSSD achieved a
mean average precision (mAP) over 95% based on a complex dataset involving both
empty and solution-filled vials, greatly exceeding those of conventional
detectors; such high precision is critical to minimizing failure-induced
accidents. Additionally, DenseSSD was observed to be highly insensitive to the
environmental changes, maintaining its high precision under the variations of
solution colors or testing view angles. The robustness of DenseSSD would allow
the utilized equipment settings to be more flexible. This work demonstrates
that DenseSSD is useful for enhancing safety in an automated material synthesis
environment, and it can be extended to various applications where high
detection accuracy and speed are both needed.",2206.07272v1,https://arxiv.org/pdf/2206.07272v1
"Architectural patterns for handling runtime uncertainty of data-driven
  models in safety-critical perception","Janek Groß, Rasmus Adler, Michael Kläs, Jan Reich, Lisa Jöckel, Roman Gansch","Data-driven models (DDM) based on machine learning and other AI techniques
play an important role in the perception of increasingly autonomous systems.
Due to the merely implicit definition of their behavior mainly based on the
data used for training, DDM outputs are subject to uncertainty. This poses a
challenge with respect to the realization of safety-critical perception tasks
by means of DDMs. A promising approach to tackling this challenge is to
estimate the uncertainty in the current situation during operation and adapt
the system behavior accordingly. In previous work, we focused on runtime
estimation of uncertainty and discussed approaches for handling uncertainty
estimations. In this paper, we present additional architectural patterns for
handling uncertainty. Furthermore, we evaluate the four patterns qualitatively
and quantitatively with respect to safety and performance gains. For the
quantitative evaluation, we consider a distance controller for vehicle
platooning where performance gains are measured by considering how much the
distance can be reduced in different operational situations. We conclude that
the consideration of context information of the driving situation makes it
possible to accept more or less uncertainty depending on the inherent risk of
the situation, which results in performance gains.",2206.06838v1,https://arxiv.org/pdf/2206.06838v1
"Safe Output Feedback Motion Planning from Images via Learned Perception
  Modules and Contraction Theory","Glen Chou, Necmiye Ozay, Dmitry Berenson","We present a motion planning algorithm for a class of uncertain
control-affine nonlinear systems which guarantees runtime safety and goal
reachability when using high-dimensional sensor measurements (e.g., RGB-D
images) and a learned perception module in the feedback control loop. First,
given a dataset of states and observations, we train a perception system that
seeks to invert a subset of the state from an observation, and estimate an
upper bound on the perception error which is valid with high probability in a
trusted domain near the data. Next, we use contraction theory to design a
stabilizing state feedback controller and a convergent dynamic state observer
which uses the learned perception system to update its state estimate. We
derive a bound on the trajectory tracking error when this controller is
subjected to errors in the dynamics and incorrect state estimates. Finally, we
integrate this bound into a sampling-based motion planner, guiding it to return
trajectories that can be safely tracked at runtime using sensor data. We
demonstrate our approach in simulation on a 4D car, a 6D planar quadrotor, and
a 17D manipulation task with RGB(-D) sensor measurements, demonstrating that
our method safely and reliably steers the system to the goal, while baselines
that fail to consider the trusted domain or state estimation errors can be
unsafe.",2206.06553v2,https://arxiv.org/pdf/2206.06553v2
"Safe-FinRL: A Low Bias and Variance Deep Reinforcement Learning
  Implementation for High-Freq Stock Trading","Zitao Song, Xuyang Jin, Chenliang Li","In recent years, many practitioners in quantitative finance have attempted to
use Deep Reinforcement Learning (DRL) to build better quantitative trading (QT)
strategies. Nevertheless, many existing studies fail to address several serious
challenges, such as the non-stationary financial environment and the bias and
variance trade-off when applying DRL in the real financial market. In this
work, we proposed Safe-FinRL, a novel DRL-based high-freq stock trading
strategy enhanced by the near-stationary financial environment and low bias and
variance estimation. Our main contributions are twofold: firstly, we separate
the long financial time series into the near-stationary short environment;
secondly, we implement Trace-SAC in the near-stationary financial environment
by incorporating the general retrace operator into the Soft Actor-Critic.
Extensive experiments on the cryptocurrency market have demonstrated that
Safe-FinRL has provided a stable value estimation and a steady policy
improvement and reduced bias and variance significantly in the near-stationary
financial environment.",2206.05910v1,https://arxiv.org/pdf/2206.05910v1
"Process Knowledge-Infused AI: Towards User-level Explainability,
  Interpretability, and Safety","Amit Sheth, Manas Gaur, Kaushik Roy, Revathy Venkataraman, Vedant Khandelwal","AI systems have been widely adopted across various domains in the real world.
However, in high-value, sensitive, or safety-critical applications such as
self-management for personalized health or food recommendation with a specific
purpose (e.g., allergy-aware recipe recommendations), their adoption is
unlikely. Firstly, the AI system needs to follow guidelines or well-defined
processes set by experts; the data alone will not be adequate. For example, to
diagnose the severity of depression, mental healthcare providers use Patient
Health Questionnaire (PHQ-9). So if an AI system were to be used for diagnosis,
the medical guideline implied by the PHQ-9 needs to be used. Likewise, a
nutritionist's knowledge and steps would need to be used for an AI system that
guides a diabetic patient in developing a food plan. Second, the BlackBox
nature typical of many current AI systems will not work; the user of an AI
system will need to be able to give user-understandable explanations,
explanations constructed using concepts that humans can understand and are
familiar with. This is the key to eliciting confidence and trust in the AI
system. For such applications, in addition to data and domain knowledge, the AI
systems need to have access to and use the Process Knowledge, an ordered set of
steps that the AI system needs to use or adhere to.",2206.13349v1,https://arxiv.org/pdf/2206.13349v1
"Towards Safe Reinforcement Learning via Constraining Conditional
  Value-at-Risk","Chengyang Ying, Xinning Zhou, Hang Su, Dong Yan, Ning Chen, Jun Zhu","Though deep reinforcement learning (DRL) has obtained substantial success, it
may encounter catastrophic failures due to the intrinsic uncertainty of both
transition and observation. Most of the existing methods for safe reinforcement
learning can only handle transition disturbance or observation disturbance
since these two kinds of disturbance affect different parts of the agent;
besides, the popular worst-case return may lead to overly pessimistic policies.
To address these issues, we first theoretically prove that the performance
degradation under transition disturbance and observation disturbance depends on
a novel metric of Value Function Range (VFR), which corresponds to the gap in
the value function between the best state and the worst state. Based on the
analysis, we adopt conditional value-at-risk (CVaR) as an assessment of risk
and propose a novel reinforcement learning algorithm of
CVaR-Proximal-Policy-Optimization (CPPO) which formalizes the risk-sensitive
constrained optimization problem by keeping its CVaR under a given threshold.
Experimental results show that CPPO achieves a higher cumulative reward and is
more robust against both observation and transition disturbances on a series of
continuous control tasks in MuJoCo.",2206.04436v2,https://arxiv.org/pdf/2206.04436v2
"Data Stealing Attack on Medical Images: Is it Safe to Export Networks
  from Data Lakes?","Huiyu Li, Nicholas Ayache, Hervé Delingette","In privacy-preserving machine learning, it is common that the owner of the
learned model does not have any physical access to the data. Instead, only a
secured remote access to a data lake is granted to the model owner without any
ability to retrieve data from the data lake. Yet, the model owner may want to
export the trained model periodically from the remote repository and a question
arises whether this may cause is a risk of data leakage. In this paper, we
introduce the concept of data stealing attack during the export of neural
networks. It consists in hiding some information in the exported network that
allows the reconstruction outside the data lake of images initially stored in
that data lake. More precisely, we show that it is possible to train a network
that can perform lossy image compression and at the same time solve some
utility tasks such as image segmentation. The attack then proceeds by exporting
the compression decoder network together with some image codes that leads to
the image reconstruction outside the data lake. We explore the feasibility of
such attacks on databases of CT and MR images, showing that it is possible to
obtain perceptually meaningful reconstructions of the target dataset, and that
the stolen dataset can be used in turns to solve a broad range of tasks.
Comprehensive experiments and analyses show that data stealing attacks should
be considered as a threat for sensitive imaging data sources.",2206.03391v1,https://arxiv.org/pdf/2206.03391v1
"CAISAR: A platform for Characterizing Artificial Intelligence Safety and
  Robustness","Julien Girard-Satabin, Michele Alberti, François Bobot, Zakaria Chihani, Augustin Lemesle","We present CAISAR, an open-source platform under active development for the
characterization of AI systems' robustness and safety. CAISAR provides a
unified entry point for defining verification problems by using WhyML, the
mature and expressive language of the Why3 verification platform. Moreover,
CAISAR orchestrates and composes state-of-the-art machine learning verification
tools which, individually, are not able to efficiently handle all problems but,
collectively, can cover a growing number of properties. Our aim is to assist,
on the one hand, the V\&V process by reducing the burden of choosing the
methodology tailored to a given verification problem, and on the other hand the
tools developers by factorizing useful features-visualization, report
generation, property description-in one platform. CAISAR will soon be available
at https://git.frama-c.com/pub/caisar.",2206.03044v2,https://arxiv.org/pdf/2206.03044v2
"A Simple and Optimal Policy Design with Safety against Heavy-Tailed Risk
  for Stochastic Bandits","David Simchi-Levi, Zeyu Zheng, Feng Zhu","We study the stochastic multi-armed bandit problem and design new policies
that enjoy both worst-case optimality for expected regret and light-tailed risk
for regret distribution. Specifically, our policy design (i) enjoys the
worst-case optimality for the expected regret at order $O(\sqrt{KT\ln T})$ and
(ii) has the worst-case tail probability of incurring a regret larger than any
$x>0$ being upper bounded by $\exp(-\Omega(x/\sqrt{KT}))$, a rate that we prove
to be best achievable with respect to $T$ for all worst-case optimal policies.
Our proposed policy achieves a delicate balance between doing more exploration
at the beginning of the time horizon and doing more exploitation when
approaching the end, compared to standard confidence-bound-based policies. We
also enhance the policy design to accommodate the ""any-time"" setting where $T$
is unknown a priori, and prove equivalently desired policy performances as
compared to the ""fixed-time"" setting with known $T$. Numerical experiments are
conducted to illustrate the theoretical findings. We find that from a
managerial perspective, our new policy design yields better tail distributions
and is preferable than celebrated policies especially when (i) there is a risk
of under-estimating the volatility profile, or (ii) there is a challenge of
tuning policy hyper-parameters. We conclude by extending our proposed policy
design to the stochastic linear bandit setting that leads to both worst-case
optimality in terms of expected regret and light-tailed risk on the regret
distribution.",2206.02969v6,https://arxiv.org/pdf/2206.02969v6
Effects of Safety State Augmentation on Safe Exploration,"Aivar Sootla, Alexander I. Cowen-Rivers, Jun Wang, Haitham Bou Ammar","Safe exploration is a challenging and important problem in model-free
reinforcement learning (RL). Often the safety cost is sparse and unknown, which
unavoidably leads to constraint violations -- a phenomenon ideally to be
avoided in safety-critical applications. We tackle this problem by augmenting
the state-space with a safety state, which is nonnegative if and only if the
constraint is satisfied. The value of this state also serves as a distance
toward constraint violation, while its initial value indicates the available
safety budget. This idea allows us to derive policies for scheduling the safety
budget during training. We call our approach Simmer (Safe policy IMproveMEnt
for RL) to reflect the careful nature of these schedules. We apply this idea to
two safe RL problems: RL with constraints imposed on an average cost, and RL
with constraints imposed on a cost with probability one. Our experiments
suggest that ""simmering, a safe algorithm can improve safety during training
for both settings. We further show that Simmer can stabilize training and
improve the performance of safe RL with average constraints.",2206.02675v2,https://arxiv.org/pdf/2206.02675v2
"Product safety idioms: a method for building causal Bayesian networks
  for product safety and risk assessment","Joshua Hunte, Martin Neil, Norman Fenton","Idioms are small, reusable Bayesian network (BN) fragments that represent
generic types of uncertain reasoning. This paper shows how idioms can be used
to build causal BNs for product safety and risk assessment that use a
combination of data and knowledge. We show that the specific product safety
idioms that we introduce are sufficient to build full BN models to evaluate
safety and risk for a wide range of products. The resulting models can be used
by safety regulators and product manufacturers even when there are limited (or
no) product testing data.",2206.02144v2,https://arxiv.org/pdf/2206.02144v2
Safety Certification for Stochastic Systems via Neural Barrier Functions,"Frederik Baymler Mathiesen, Simeon Calvert, Luca Laurenti","Providing non-trivial certificates of safety for non-linear stochastic
systems is an important open problem that limits the wider adoption of
autonomous systems in safety-critical applications. One promising solution to
address this problem is barrier functions. The composition of a barrier
function with a stochastic system forms a supermartingale, thus enabling the
computation of the probability that the system stays in a safe set over a
finite time horizon via martingale inequalities. However, existing approaches
to find barrier functions for stochastic systems generally rely on convex
optimization programs that restrict the search of a barrier to a small class of
functions such as low degree SoS polynomials and can be computationally
expensive. In this paper, we parameterize a barrier function as a neural
network and show that techniques for robust training of neural networks can be
successfully employed to find neural barrier functions. Specifically, we
leverage bound propagation techniques to certify that a neural network
satisfies the conditions to be a barrier function via linear programming and
then employ the resulting bounds at training time to enforce the satisfaction
of these conditions. We also present a branch-and-bound scheme that makes the
certification framework scalable. We show that our approach outperforms
existing methods in several case studies and often returns certificates of
safety that are orders of magnitude larger.",2206.01463v1,https://arxiv.org/pdf/2206.01463v1
"Watch Out for the Safety-Threatening Actors: Proactively Mitigating
  Safety Hazards","Saurabh Jha, Shengkun Cui, Zbigniew Kalbarczyk, Ravishankar K. Iyer","Despite the successful demonstration of autonomous vehicles (AVs), such as
self-driving cars, ensuring AV safety remains a challenging task. Although some
actors influence an AV's driving decisions more than others, current approaches
pay equal attention to each actor on the road. An actor's influence on the AV's
decision can be characterized in terms of its ability to decrease the number of
safe navigational choices for the AV. In this work, we propose a safety threat
indicator (STI) using counterfactual reasoning to estimate the importance of
each actor on the road with respect to its influence on the AV's safety. We use
this indicator to (i) characterize the existing real-world datasets to identify
rare hazardous scenarios as well as the poor performance of existing
controllers in such scenarios; and (ii) design an RL based safety mitigation
controller to proactively mitigate the safety hazards those actors pose to the
AV. Our approach reduces the accident rate for the state-of-the-art AV agent(s)
in rare hazardous scenarios by more than 70%.",2206.00886v1,https://arxiv.org/pdf/2206.00886v1
"Knowledge Graph - Deep Learning: A Case Study in Question Answering in
  Aviation Safety Domain","Ankush Agarwal, Raj Gite, Shreya Laddha, Pushpak Bhattacharyya, Satyanarayan Kar, Asif Ekbal, Prabhjit Thind, Rajesh Zele, Ravi Shankar","In the commercial aviation domain, there are a large number of documents,
like, accident reports (NTSB, ASRS) and regulatory directives (ADs). There is a
need for a system to access these diverse repositories efficiently in order to
service needs in the aviation industry, like maintenance, compliance, and
safety. In this paper, we propose a Knowledge Graph (KG) guided Deep Learning
(DL) based Question Answering (QA) system for aviation safety. We construct a
Knowledge Graph from Aircraft Accident reports and contribute this resource to
the community of researchers. The efficacy of this resource is tested and
proved by the aforesaid QA system. Natural Language Queries constructed from
the documents mentioned above are converted into SPARQL (the interface language
of the RDF graph database) queries and answered. On the DL side, we have two
different QA models: (i) BERT QA which is a pipeline of Passage Retrieval
(Sentence-BERT based) and Question Answering (BERT based), and (ii) the
recently released GPT-3. We evaluate our system on a set of queries created
from the accident reports. Our combined QA system achieves 9.3% increase in
accuracy over GPT-3 and 40.3% increase over BERT QA. Thus, we infer that KG-DL
performs better than either singly.",2205.15952v2,https://arxiv.org/pdf/2205.15952v2
"On the Robustness of Safe Reinforcement Learning under Observational
  Perturbations","Zuxin Liu, Zijian Guo, Zhepeng Cen, Huan Zhang, Jie Tan, Bo Li, Ding Zhao","Safe reinforcement learning (RL) trains a policy to maximize the task reward
while satisfying safety constraints. While prior works focus on the performance
optimality, we find that the optimal solutions of many safe RL problems are not
robust and safe against carefully designed observational perturbations. We
formally analyze the unique properties of designing effective observational
adversarial attackers in the safe RL setting. We show that baseline adversarial
attack techniques for standard RL tasks are not always effective for safe RL
and propose two new approaches - one maximizes the cost and the other maximizes
the reward. One interesting and counter-intuitive finding is that the maximum
reward attack is strong, as it can both induce unsafe behaviors and make the
attack stealthy by maintaining the reward. We further propose a robust training
framework for safe RL and evaluate it via comprehensive experiments. This paper
provides a pioneer work to investigate the safety and robustness of RL under
observational attacks for future safe RL studies. Code is available at:
\url{https://github.com/liuzuxin/safe-rl-robustness}",2205.14691v3,https://arxiv.org/pdf/2205.14691v3
Safety Aware Changepoint Detection for Piecewise i.i.d. Bandits,Subhojyoti Mukherjee,"In this paper, we consider the setting of piecewise i.i.d. bandits under a
safety constraint. In this piecewise i.i.d. setting, there exists a finite
number of changepoints where the mean of some or all arms change
simultaneously. We introduce the safety constraint studied in
\citet{wu2016conservative} to this setting such that at any round the
cumulative reward is above a constant factor of the default action reward. We
propose two actively adaptive algorithms for this setting that satisfy the
safety constraint, detect changepoints, and restart without the knowledge of
the number of changepoints or their locations. We provide regret bounds for our
algorithms and show that the bounds are comparable to their counterparts from
the safe bandit and piecewise i.i.d. bandit literature. We also provide the
first matching lower bounds for this setting. Empirically, we show that our
safety-aware algorithms perform similarly to the state-of-the-art actively
adaptive algorithms that do not satisfy the safety constraint.",2205.13689v1,https://arxiv.org/pdf/2205.13689v1
Penalized Proximal Policy Optimization for Safe Reinforcement Learning,"Linrui Zhang, Li Shen, Long Yang, Shixiang Chen, Bo Yuan, Xueqian Wang, Dacheng Tao","Safe reinforcement learning aims to learn the optimal policy while satisfying
safety constraints, which is essential in real-world applications. However,
current algorithms still struggle for efficient policy updates with hard
constraint satisfaction. In this paper, we propose Penalized Proximal Policy
Optimization (P3O), which solves the cumbersome constrained policy iteration
via a single minimization of an equivalent unconstrained problem. Specifically,
P3O utilizes a simple-yet-effective penalty function to eliminate cost
constraints and removes the trust-region constraint by the clipped surrogate
objective. We theoretically prove the exactness of the proposed method with a
finite penalty factor and provide a worst-case analysis for approximate error
when evaluated on sample trajectories. Moreover, we extend P3O to more
challenging multi-constraint and multi-agent scenarios which are less studied
in previous work. Extensive experiments show that P3O outperforms
state-of-the-art algorithms with respect to both reward improvement and
constraint satisfaction on a set of constrained locomotive tasks.",2205.11814v2,https://arxiv.org/pdf/2205.11814v2
"Transformer-based out-of-distribution detection for clinically safe
  segmentation","Mark S Graham, Petru-Daniel Tudosiu, Paul Wright, Walter Hugo Lopez Pinaya, U Jean-Marie, Yee Mah, James Teo, Rolf H Jäger, David Werring, Parashkev Nachev, Sebastien Ourselin, M Jorge Cardoso","In a clinical setting it is essential that deployed image processing systems
are robust to the full range of inputs they might encounter and, in particular,
do not make confidently wrong predictions. The most popular approach to safe
processing is to train networks that can provide a measure of their
uncertainty, but these tend to fail for inputs that are far outside the
training data distribution. Recently, generative modelling approaches have been
proposed as an alternative; these can quantify the likelihood of a data sample
explicitly, filtering out any out-of-distribution (OOD) samples before further
processing is performed. In this work, we focus on image segmentation and
evaluate several approaches to network uncertainty in the far-OOD and near-OOD
cases for the task of segmenting haemorrhages in head CTs. We find all of these
approaches are unsuitable for safe segmentation as they provide confidently
wrong predictions when operating OOD. We propose performing full 3D OOD
detection using a VQ-GAN to provide a compressed latent representation of the
image and a transformer to estimate the data likelihood. Our approach
successfully identifies images in both the far- and near-OOD cases. We find a
strong relationship between image likelihood and the quality of a model's
segmentation, making this approach viable for filtering images unsuitable for
segmentation. To our knowledge, this is the first time transformers have been
applied to perform OOD detection on 3D image data. Code is available at
github.com/marksgraham/transformer-ood.",2205.10650v2,https://arxiv.org/pdf/2205.10650v2
"A Review of Safe Reinforcement Learning: Methods, Theory and
  Applications","Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Alois Knoll","Reinforcement Learning (RL) has achieved tremendous success in many complex
decision-making tasks. However, safety concerns are raised during deploying RL
in real-world applications, leading to a growing demand for safe RL algorithms,
such as in autonomous driving and robotics scenarios. While safe control has a
long history, the study of safe RL algorithms is still in the early stages. To
establish a good foundation for future safe RL research, in this paper, we
provide a review of safe RL from the perspectives of methods, theories, and
applications. Firstly, we review the progress of safe RL from five dimensions
and come up with five crucial problems for safe RL being deployed in real-world
applications, coined as ""2H3W"". Secondly, we analyze the algorithm and theory
progress from the perspectives of answering the ""2H3W"" problems. Particularly,
the sample complexity of safe RL algorithms is reviewed and discussed, followed
by an introduction to the applications and benchmarks of safe RL algorithms.
Finally, we open the discussion of the challenging problems in safe RL, hoping
to inspire future research on this thread. To advance the study of safe RL
algorithms, we release an open-sourced repository containing the
implementations of major safe RL algorithms at the link:
https://github.com/chauncygu/Safe-Reinforcement-Learning-Baselines.git.",2205.10330v5,https://arxiv.org/pdf/2205.10330v5
"SafeNet: The Unreasonable Effectiveness of Ensembles in Private
  Collaborative Learning","Harsh Chaudhari, Matthew Jagielski, Alina Oprea","Secure multiparty computation (MPC) has been proposed to allow multiple
mutually distrustful data owners to jointly train machine learning (ML) models
on their combined data. However, by design, MPC protocols faithfully compute
the training functionality, which the adversarial ML community has shown to
leak private information and can be tampered with in poisoning attacks. In this
work, we argue that model ensembles, implemented in our framework called
SafeNet, are a highly MPC-amenable way to avoid many adversarial ML attacks.
The natural partitioning of data amongst owners in MPC training allows this
approach to be highly scalable at training time, provide provable protection
from poisoning attacks, and provably defense against a number of privacy
attacks. We demonstrate SafeNet's efficiency, accuracy, and resilience to
poisoning on several machine learning datasets and models trained in end-to-end
and transfer learning scenarios. For instance, SafeNet reduces backdoor attack
success significantly, while achieving $39\times$ faster training and $36
\times$ less communication than the four-party MPC framework of Dalskov et al.
Our experiments show that ensembling retains these benefits even in many
non-iid settings. The simplicity, cheap setup, and robustness properties of
ensembling make it a strong first choice for training ML models privately in
MPC.",2205.09986v2,https://arxiv.org/pdf/2205.09986v2
Real Time Multi-Object Detection for Helmet Safety,"Mrinal Mathur, Archana Benkkallpalli Chandrashekhar, Venkata Krishna Chaithanya Nuthalapati","The National Football League and Amazon Web Services teamed up to develop the
best sports injury surveillance and mitigation program via the Kaggle
competition. Through which the NFL wants to assign specific players to each
helmet, which would help accurately identify each player's ""exposures""
throughout a football play. We are trying to implement a computer vision based
ML algorithms capable of assigning detected helmet impacts to correct players
via tracking information. Our paper will explain the approach to automatically
track player helmets and their collisions. This will also allow them to review
previous plays and explore the trends in exposure over time.",2205.09878v1,https://arxiv.org/pdf/2205.09878v1
"A Comprehensive Study on Artificial Intelligence Algorithms to Implement
  Safety Using Communication Technologies","Rafia Inam, Alberto Yukinobu Hata, Vlasjov Prifti, Sara Abbaspour Asadollah","The recent development of artificial intelligence (AI) has increased the
interest of researchers and practitioners towards applying its techniques into
multiple domains like automotive, health care and air space to achieve
automation. Combined to these applications, the attempt to use AI techniques
into carrying out safety issues is momentarily at a progressive state. As AI
problems are getting even more complex, large processing power is demanded for
safety-critical systems to fulfill real-time requirements. These challenges can
be solved through edge or cloud computing, which makes the communication an
integral part of the solution. This study aims at providing a comprehensive
picture of the state of the art AI based safety solutions that uses different
communication technologies in diverse application domains. To achieve this, a
systematic mapping study is conducted and 565 relevant papers are shortlisted
through a multistage selection process, which are then analyzed according to a
systematically defined classification framework. The results of the study are
based on these main objectives: to clarify current research gaps in the field,
to identify the possibility of increased usage of cellular communication in
multiple domains, to identify the mostly used AI algorithms and to summarize
the emerging future research trends on the topic. The results demonstrate that
automotive domain is the one applying AI and communication the most to
implement safety and the most used AI in this domain is neural networks,
clustering and computer vision; applying cellular communication to automotive
domain is highest; the use of non-cellular communication technologies is
dominant however a clear trend of a rapid increase in the use of cellular
communication is observed specially from 2020 with the roll-out of 5G
technology.",2205.08404v1,https://arxiv.org/pdf/2205.08404v1
"Provably Safe Reinforcement Learning: Conceptual Analysis, Survey, and
  Benchmarking","Hanna Krasowski, Jakob Thumm, Marlon Müller, Lukas Schäfer, Xiao Wang, Matthias Althoff","Ensuring the safety of reinforcement learning (RL) algorithms is crucial to
unlock their potential for many real-world tasks. However, vanilla RL and most
safe RL approaches do not guarantee safety. In recent years, several methods
have been proposed to provide hard safety guarantees for RL, which is essential
for applications where unsafe actions could have disastrous consequences.
Nevertheless, there is no comprehensive comparison of these provably safe RL
methods. Therefore, we introduce a categorization of existing provably safe RL
methods, present the conceptual foundations for both continuous and discrete
action spaces, and empirically benchmark existing methods. We categorize the
methods based on how they adapt the action: action replacement, action
projection, and action masking. Our experiments on an inverted pendulum and a
quadrotor stabilization task indicate that action replacement is the
best-performing approach for these applications despite its comparatively
simple realization. Furthermore, adding a reward penalty, every time the safety
verification is engaged, improved training performance in our experiments.
Finally, we provide practical guidance on selecting provably safe RL approaches
depending on the safety specification, RL algorithm, and type of action space.",2205.06750v3,https://arxiv.org/pdf/2205.06750v3
"Provably Safe Deep Reinforcement Learning for Robotic Manipulation in
  Human Environments","Jakob Thumm, Matthias Althoff","Deep reinforcement learning (RL) has shown promising results in the motion
planning of manipulators. However, no method guarantees the safety of highly
dynamic obstacles, such as humans, in RL-based manipulator control. This lack
of formal safety assurances prevents the application of RL for manipulators in
real-world human environments. Therefore, we propose a shielding mechanism that
ensures ISO-verified human safety while training and deploying RL algorithms on
manipulators. We utilize a fast reachability analysis of humans and
manipulators to guarantee that the manipulator comes to a complete stop before
a human is within its range. Our proposed method guarantees safety and
significantly improves the RL performance by preventing episode-ending
collisions. We demonstrate the performance of our proposed method in simulation
using human motion capture data.",2205.06311v1,https://arxiv.org/pdf/2205.06311v1
"Contingency-constrained economic dispatch with safe reinforcement
  learning","Michael Eichelbeck, Hannah Markgraf, Matthias Althoff","Future power systems will rely heavily on micro grids with a high share of
decentralised renewable energy sources and energy storage systems. The high
complexity and uncertainty in this context might make conventional power
dispatch strategies infeasible. Reinforcement-learning based (RL) controllers
can address this challenge, however, cannot themselves provide safety
guarantees, preventing their deployment in practice. To overcome this
limitation, we propose a formally validated RL controller for economic
dispatch. We extend conventional constraints by a time-dependent constraint
encoding the islanding contingency. The contingency constraint is computed
using set-based backwards reachability analysis and actions of the RL agent are
verified through a safety layer. Unsafe actions are projected into the safe
action space while leveraging constrained zonotope set representations for
computational efficiency. The developed approach is demonstrated on a
residential use case using real-world measurements.",2205.06212v3,https://arxiv.org/pdf/2205.06212v3
"Bridging Model-based Safety and Model-free Reinforcement Learning
  through System Identification of Low Dimensional Linear Models","Zhongyu Li, Jun Zeng, Akshay Thirugnanam, Koushil Sreenath","Bridging model-based safety and model-free reinforcement learning (RL) for
dynamic robots is appealing since model-based methods are able to provide
formal safety guarantees, while RL-based methods are able to exploit the robot
agility by learning from the full-order system dynamics. However, current
approaches to tackle this problem are mostly restricted to simple systems. In
this paper, we propose a new method to combine model-based safety with
model-free reinforcement learning by explicitly finding a low-dimensional model
of the system controlled by a RL policy and applying stability and safety
guarantees on that simple model. We use a complex bipedal robot Cassie, which
is a high dimensional nonlinear system with hybrid dynamics and underactuation,
and its RL-based walking controller as an example. We show that a
low-dimensional dynamical model is sufficient to capture the dynamics of the
closed-loop system. We demonstrate that this model is linear, asymptotically
stable, and is decoupled across control input in all dimensions. We further
exemplify that such linearity exists even when using different RL control
policies. Such results point out an interesting direction to understand the
relationship between RL and optimal control: whether RL tends to linearize the
nonlinear system during training in some cases. Furthermore, we illustrate that
the found linear model is able to provide guarantees by safety-critical optimal
control framework, e.g., Model Predictive Control with Control Barrier
Functions, on an example of autonomous navigation using Cassie while taking
advantage of the agility provided by the RL-based controller.",2205.05787v1,https://arxiv.org/pdf/2205.05787v1
A Safety Assurable Human-Inspired Perception Architecture,"Rick Salay, Krzysztof Czarnecki","Although artificial intelligence-based perception (AIP) using deep neural
networks (DNN) has achieved near human level performance, its well-known
limitations are obstacles to the safety assurance needed in autonomous
applications. These include vulnerability to adversarial inputs, inability to
handle novel inputs and non-interpretability. While research in addressing
these limitations is active, in this paper, we argue that a fundamentally
different approach is needed to address them. Inspired by dual process models
of human cognition, where Type 1 thinking is fast and non-conscious while Type
2 thinking is slow and based on conscious reasoning, we propose a dual process
architecture for safe AIP. We review research on how humans address the
simplest non-trivial perception problem, image classification, and sketch a
corresponding AIP architecture for this task. We argue that this architecture
can provide a systematic way of addressing the limitations of AIP using DNNs
and an approach to assurance of human-level performance and beyond. We conclude
by discussing what components of the architecture may already be addressed by
existing work and what remains future work.",2205.07862v2,https://arxiv.org/pdf/2205.07862v2
"A Verification Framework for Certifying Learning-Based Safety-Critical
  Aviation Systems","Ali Baheri, Hao Ren, Benjamin Johnson, Pouria Razzaghi, Peng Wei","We present a safety verification framework for design-time and run-time
assurance of learning-based components in aviation systems. Our proposed
framework integrates two novel methodologies. From the design-time assurance
perspective, we propose offline mixed-fidelity verification tools that
incorporate knowledge from different levels of granularity in simulated
environments. From the run-time assurance perspective, we propose reachability-
and statistics-based online monitoring and safety guards for a learning-based
decision-making model to complement the offline verification methods. This
framework is designed to be loosely coupled among modules, allowing the
individual modules to be developed using independent methodologies and
techniques, under varying circumstances and with different tool access. The
proposed framework offers feasible solutions for meeting system safety
requirements at different stages throughout the system development and
deployment cycle, enabling the continuous learning and assessment of the system
product.",2205.04590v2,https://arxiv.org/pdf/2205.04590v2
"Zhuyi: Perception Processing Rate Estimation for Safety in Autonomous
  Vehicles","Yu-Shun Hsiao, Siva Kumar Sastry Hari, Michał Filipiuk, Timothy Tsai, Michael B. Sullivan, Vijay Janapa Reddi, Vasu Singh, Stephen W. Keckler","The processing requirement of autonomous vehicles (AVs) for high-accuracy
perception in complex scenarios can exceed the resources offered by the
in-vehicle computer, degrading safety and comfort. This paper proposes a sensor
frame processing rate (FPR) estimation model, Zhuyi, that quantifies the
minimum safe FPR continuously in a driving scenario. Zhuyi can be employed
post-deployment as an online safety check and to prioritize work. Experiments
conducted using a multi-camera state-of-the-art industry AV system show that
Zhuyi's estimated FPRs are conservative, yet the system can maintain safety by
processing only 36% or fewer frames compared to a default 30-FPR system in the
tested scenarios.",2205.03347v1,https://arxiv.org/pdf/2205.03347v1
"Learn-to-Race Challenge 2022: Benchmarking Safe Learning and
  Cross-domain Generalisation in Autonomous Racing","Jonathan Francis, Bingqing Chen, Siddha Ganju, Sidharth Kathpal, Jyotish Poonganam, Ayush Shivani, Vrushank Vyas, Sahika Genc, Ivan Zhukov, Max Kumskoy, Anirudh Koul, Jean Oh, Eric Nyberg","We present the results of our autonomous racing virtual challenge, based on
the newly-released Learn-to-Race (L2R) simulation framework, which seeks to
encourage interdisciplinary research in autonomous driving and to help advance
the state of the art on a realistic benchmark. Analogous to racing being used
to test cutting-edge vehicles, we envision autonomous racing to serve as a
particularly challenging proving ground for autonomous agents as: (i) they need
to make sub-second, safety-critical decisions in a complex, fast-changing
environment; and (ii) both perception and control must be robust to
distribution shifts, novel road features, and unseen obstacles. Thus, the main
goal of the challenge is to evaluate the joint safety, performance, and
generalisation capabilities of reinforcement learning agents on multi-modal
perception, through a two-stage process. In the first stage of the challenge,
we evaluate an autonomous agent's ability to drive as fast as possible, while
adhering to safety constraints. In the second stage, we additionally require
the agent to adapt to an unseen racetrack through safe exploration. In this
paper, we describe the new L2R Task 2.0 benchmark, with refined metrics and
baseline approaches. We also provide an overview of deployment, evaluation, and
rankings for the inaugural instance of the L2R Autonomous Racing Virtual
Challenge (supported by Carnegie Mellon University, Arrival Ltd., AICrowd,
Amazon Web Services, and Honda Research), which officially used the new L2R
Task 2.0 benchmark and received over 20,100 views, 437 active participants, 46
teams, and 733 model submissions -- from 88+ unique institutions, in 58+
different countries. Finally, we release leaderboard results from the challenge
and provide description of the two top-ranking approaches in cross-domain model
transfer, across multiple sensor configurations and simulated races.",2205.02953v2,https://arxiv.org/pdf/2205.02953v2
"An Empirical Analysis of the Use of Real-Time Reachability for the
  Safety Assurance of Autonomous Vehicles","Patrick Musau, Nathaniel Hamilton, Diego Manzanas Lopez, Preston Robinette, Taylor T. Johnson","Recent advances in machine learning technologies and sensing have paved the
way for the belief that safe, accessible, and convenient autonomous vehicles
may be realized in the near future. Despite tremendous advances within this
context, fundamental challenges around safety and reliability are limiting
their arrival and comprehensive adoption. Autonomous vehicles are often tasked
with operating in dynamic and uncertain environments. As a result, they often
make use of highly complex components, such as machine learning approaches, to
handle the nuances of sensing, actuation, and control. While these methods are
highly effective, they are notoriously difficult to assure. Moreover, within
uncertain and dynamic environments, design time assurance analyses may not be
sufficient to guarantee safety. Thus, it is critical to monitor the correctness
of these systems at runtime. One approach for providing runtime assurance of
systems with components that may not be amenable to formal analysis is the
simplex architecture, where an unverified component is wrapped with a safety
controller and a switching logic designed to prevent dangerous behavior. In
this paper, we propose using a real-time reachability algorithm for the
implementation of the simplex architecture to assure the safety of a 1/10 scale
open source autonomous vehicle platform known as F1/10. The reachability
algorithm that we leverage (a) provides provable guarantees of safety, and (b)
is used to detect potentially unsafe scenarios. In our approach, the need to
analyze an underlying controller is abstracted away, instead focusing on the
effects of the controller's decisions on the system's future states. We
demonstrate the efficacy of our architecture through a vast set of experiments
conducted both in simulation and on an embedded hardware platform.",2205.01419v1,https://arxiv.org/pdf/2205.01419v1
"Modeling and mitigation of occupational safety risks in dynamic
  industrial environments","Ashutosh Tewari, Antonio R. Paiva","Identifying and mitigating safety risks is paramount in a number of
industries. In addition to guidelines and best practices, many industries
already have safety management systems (SMSs) designed to monitor and reinforce
good safety behaviors. The analytic capabilities to analyze the data acquired
through such systems, however, are still lacking in terms of their ability to
robustly quantify risks posed by various occupational hazards. Moreover, best
practices and modern SMSs are unable to account for dynamically evolving
environments/behavioral characteristics commonly found in many industrial
settings. This article proposes a method to address these issues by enabling
continuous and quantitative assessment of safety risks in a data-driven manner.
The backbone of our method is an intuitive hierarchical probabilistic model
that explains sparse and noisy safety data collected by a typical SMS. A fully
Bayesian approach is developed to calibrate this model from safety data in an
online fashion. Thereafter, the calibrated model holds necessary information
that serves to characterize risk posed by different safety hazards.
Additionally, the proposed model can be leveraged for automated decision
making, for instance solving resource allocation problems -- targeted towards
risk mitigation -- that are often encountered in resource-constrained
industrial environments. The methodology is rigorously validated on a simulated
test-bed and its scalability is demonstrated on real data from large
maintenance projects at a petrochemical plant.",2205.00894v1,https://arxiv.org/pdf/2205.00894v1
"KING: Generating Safety-Critical Driving Scenarios for Robust Imitation
  via Kinematics Gradients","Niklas Hanselmann, Katrin Renz, Kashyap Chitta, Apratim Bhattacharyya, Andreas Geiger","Simulators offer the possibility of safe, low-cost development of
self-driving systems. However, current driving simulators exhibit na\""ive
behavior models for background traffic. Hand-tuned scenarios are typically
added during simulation to induce safety-critical situations. An alternative
approach is to adversarially perturb the background traffic trajectories. In
this paper, we study this approach to safety-critical driving scenario
generation using the CARLA simulator. We use a kinematic bicycle model as a
proxy to the simulator's true dynamics and observe that gradients through this
proxy model are sufficient for optimizing the background traffic trajectories.
Based on this finding, we propose KING, which generates safety-critical driving
scenarios with a 20% higher success rate than black-box optimization. By
solving the scenarios generated by KING using a privileged rule-based expert
algorithm, we obtain training data for an imitation learning policy. After
fine-tuning on this new data, we show that the policy becomes better at
avoiding collisions. Importantly, our generated data leads to reduced
collisions on both held-out scenarios generated via KING as well as traditional
hand-crafted scenarios, demonstrating improved robustness.",2204.13683v1,https://arxiv.org/pdf/2204.13683v1
IRC-safe Graph Autoencoder for unsupervised anomaly detection,"Oliver Atkinson, Akanksha Bhardwaj, Christoph Englert, Partha Konar, Vishal S. Ngairangbam, Michael Spannowsky","Anomaly detection through employing machine learning techniques has emerged
as a novel powerful tool in the search for new physics beyond the Standard
Model. Historically similar to the development of jet observables, theoretical
consistency has not always assumed a central role in the fast development of
algorithms and neural network architectures. In this work, we construct an
infrared and collinear safe autoencoder based on graph neural networks by
employing energy-weighted message passing. We demonstrate that whilst this
approach has theoretically favourable properties, it also exhibits formidable
sensitivity to non-QCD structures.",2204.12231v2,https://arxiv.org/pdf/2204.12231v2
Distributed Dynamic Safe Screening Algorithms for Sparse Regularization,"Runxue Bao, Xidong Wu, Wenhan Xian, Heng Huang","Distributed optimization has been widely used as one of the most efficient
approaches for model training with massive samples. However, large-scale
learning problems with both massive samples and high-dimensional features
widely exist in the era of big data. Safe screening is a popular technique to
speed up high-dimensional models by discarding the inactive features with zero
coefficients. Nevertheless, existing safe screening methods are limited to the
sequential setting. In this paper, we propose a new distributed dynamic safe
screening (DDSS) method for sparsity regularized models and apply it on
shared-memory and distributed-memory architecture respectively, which can
achieve significant speedup without any loss of accuracy by simultaneously
enjoying the sparsity of the model and dataset. To the best of our knowledge,
this is the first work of distributed safe dynamic screening method.
Theoretically, we prove that the proposed method achieves the linear
convergence rate with lower overall complexity and can eliminate almost all the
inactive features in a finite number of iterations almost surely. Finally,
extensive experimental results on benchmark datasets confirm the superiority of
our proposed method.",2204.10981v1,https://arxiv.org/pdf/2204.10981v1
SCOPE: Safe Exploration for Dynamic Computer Systems Optimization,"Hyunji Kim, Ahsan Pervaiz, Henry Hoffmann, Michael Carbin, Yi Ding","Modern computer systems need to execute under strict safety constraints
(e.g., a power limit), but doing so often conflicts with their ability to
deliver high performance (i.e. minimal latency). Prior work uses machine
learning to automatically tune hardware resources such that the system
execution meets safety constraints optimally. Such solutions monitor past
system executions to learn the system's behavior under different hardware
resource allocations before dynamically tuning resources to optimize the
application execution. However, system behavior can change significantly
between different applications and even different inputs of the same
applications. Hence, the models learned using data collected a priori are often
suboptimal and violate safety constraints when used with new applications and
inputs. To address this limitation, we introduce the concept of an execution
space, which is the cross product of hardware resources, input features, and
applications. To dynamically and safely allocate hardware resources from the
execution space, we present SCOPE, a resource manager that leverages a novel
safe exploration framework. We evaluate SCOPE's ability to deliver improved
latency while minimizing power constraint violations by dynamically configuring
hardware while running a variety of Apache Spark applications. Compared to
prior approaches that minimize power constraint violations, SCOPE consumes
comparable power while improving latency by up to 9.5X. Compared to prior
approaches that minimize latency, SCOPE achieves similar latency but reduces
power constraint violation rates by up to 45.88X, achieving almost zero safety
constraint violations across all applications.",2204.10451v1,https://arxiv.org/pdf/2204.10451v1
Path-Specific Objectives for Safer Agent Incentives,"Sebastian Farquhar, Ryan Carey, Tom Everitt","We present a general framework for training safe agents whose naive
incentives are unsafe. As an example, manipulative or deceptive behaviour can
improve rewards but should be avoided. Most approaches fail here: agents
maximize expected return by any means necessary. We formally describe settings
with 'delicate' parts of the state which should not be used as a means to an
end. We then train agents to maximize the causal effect of actions on the
expected return which is not mediated by the delicate parts of state, using
Causal Influence Diagram analysis. The resulting agents have no incentive to
control the delicate state. We further show how our framework unifies and
generalizes existing proposals.",2204.10018v1,https://arxiv.org/pdf/2204.10018v1
"SAAC: Safe Reinforcement Learning as an Adversarial Game of
  Actor-Critics","Yannis Flet-Berliac, Debabrota Basu","Although Reinforcement Learning (RL) is effective for sequential
decision-making problems under uncertainty, it still fails to thrive in
real-world systems where risk or safety is a binding constraint. In this paper,
we formulate the RL problem with safety constraints as a non-zero-sum game.
While deployed with maximum entropy RL, this formulation leads to a safe
adversarially guided soft actor-critic framework, called SAAC. In SAAC, the
adversary aims to break the safety constraint while the RL agent aims to
maximize the constrained value function given the adversary's policy. The
safety constraint on the agent's value function manifests only as a repulsion
term between the agent's and the adversary's policies. Unlike previous
approaches, SAAC can address different safety criteria such as safe
exploration, mean-variance risk sensitivity, and CVaR-like coherent risk
sensitivity. We illustrate the design of the adversary for these constraints.
Then, in each of these variations, we show the agent differentiates itself from
the adversary's unsafe actions in addition to learning to solve the task.
Finally, for challenging continuous control tasks, we demonstrate that SAAC
achieves faster convergence, better efficiency, and fewer failures to satisfy
the safety constraints than risk-averse distributional RL and risk-neutral soft
actor-critic algorithms.",2204.09424v2,https://arxiv.org/pdf/2204.09424v2
"Learning Forward Dynamics Model and Informed Trajectory Sampler for Safe
  Quadruped Navigation","Yunho Kim, Chanyoung Kim, Jemin Hwangbo","For autonomous quadruped robot navigation in various complex environments, a
typical SOTA system is composed of four main modules -- mapper, global planner,
local planner, and command-tracking controller -- in a hierarchical manner. In
this paper, we build a robust and safe local planner which is designed to
generate a velocity plan to track a coarsely planned path from the global
planner. Previous works used waypoint-based methods (e.g.
Proportional-Differential control and pure pursuit) which simplify the path
tracking problem to local point-goal navigation. However, they suffer from
frequent collisions in geometrically complex and narrow environments because of
two reasons; the global planner uses a coarse and inaccurate model and the
local planner is unable to track the global plan sufficiently well. Currently,
deep learning methods are an appealing alternative because they can learn
safety and path feasibility from experience more accurately. However, existing
deep learning methods are not capable of planning for a long horizon. In this
work, we propose a learning-based fully autonomous navigation framework
composed of three innovative elements: a learned forward dynamics model (FDM),
an online sampling-based model-predictive controller, and an informed
trajectory sampler (ITS). Using our framework, a quadruped robot can
autonomously navigate in various complex environments without a collision and
generate a smoother command plan compared to the baseline method. Furthermore,
our method can reactively handle unexpected obstacles on the planned path and
avoid them. Project page
https://awesomericky.github.io/projects/FDM_ITS_navigation/.",2204.08647v3,https://arxiv.org/pdf/2204.08647v3
"A Pre-study on Data Processing Pipelines for Roadside Object Detection
  Systems Towards Safer Road Infrastructure","Yinan Yu, Samuel Scheidegger, John-Fredrik Grönvall, Magnus Palm, Erik Svanberg, Johan Amoruso Wennerby, Jörg Bakker","Single-vehicle accidents are the most common type of fatal accidents in
Sweden, where a car drives off the road and runs into hazardous roadside
objects. Proper installation and maintenance of protective objects, such as
crash cushions and guard rails, may reduce the chance and severity of such
accidents. Moreover, efficient detection and management of hazardous roadside
objects also plays an important role in improving road safety. To better
understand the state-of-the-art and system requirements, in this pre-study, we
investigate the feasibility, implementation, limitations and scaling up of data
processing pipelines for roadside object detection. In particular, we divide
our investigation into three parts: the target of interest, the sensors of
choice and the algorithm design. The data sources we consider in this study
cover two common setups: 1) road surveying fleet - annual scans conducted by
Trafikverket, the Swedish Transport Administration, and 2) consumer vehicle -
data collected using a research vehicle from the laboratory of Resource for
vehicle research at Chalmers (REVERE). The goal of this report is to
investigate how to implement a scalable roadside object detection system
towards safe road infrastructure and Sweden's Vision Zero.",2205.01783v1,https://arxiv.org/pdf/2205.01783v1
"Ergo, SMIRK is Safe: A Safety Case for a Machine Learning Component in a
  Pedestrian Automatic Emergency Brake System","Markus Borg, Jens Henriksson, Kasper Socha, Olof Lennartsson, Elias Sonnsjö Lönegren, Thanh Bui, Piotr Tomaszewski, Sankar Raman Sathyamoorthy, Sebastian Brink, Mahshid Helali Moghadam","Integration of Machine Learning (ML) components in critical applications
introduces novel challenges for software certification and verification. New
safety standards and technical guidelines are under development to support the
safety of ML-based systems, e.g., ISO 21448 SOTIF for the automotive domain and
the Assurance of Machine Learning for use in Autonomous Systems (AMLAS)
framework. SOTIF and AMLAS provide high-level guidance but the details must be
chiseled out for each specific case. We initiated a research project with the
goal to demonstrate a complete safety case for an ML component in an open
automotive system. This paper reports results from an industry-academia
collaboration on safety assurance of SMIRK, an ML-based pedestrian automatic
emergency braking demonstrator running in an industry-grade simulator. We
demonstrate an application of AMLAS on SMIRK for a minimalistic operational
design domain, i.e., we share a complete safety case for its integrated
ML-based component. Finally, we report lessons learned and provide both SMIRK
and the safety case under an open-source licence for the research community to
reuse.",2204.07874v3,https://arxiv.org/pdf/2204.07874v3
Safe Self-Refinement for Transformer-based Domain Adaptation,"Tao Sun, Cheng Lu, Tianshuo Zhang, Haibin Ling","Unsupervised Domain Adaptation (UDA) aims to leverage a label-rich source
domain to solve tasks on a related unlabeled target domain. It is a challenging
problem especially when a large domain gap lies between the source and target
domains. In this paper we propose a novel solution named SSRT (Safe
Self-Refinement for Transformer-based domain adaptation), which brings
improvement from two aspects. First, encouraged by the success of vision
transformers in various vision tasks, we arm SSRT with a transformer backbone.
We find that the combination of vision transformer with simple adversarial
adaptation surpasses best reported Convolutional Neural Network (CNN)-based
results on the challenging DomainNet benchmark, showing its strong transferable
feature representation. Second, to reduce the risk of model collapse and
improve the effectiveness of knowledge transfer between domains with large
gaps, we propose a Safe Self-Refinement strategy. Specifically, SSRT utilizes
predictions of perturbed target domain data to refine the model. Since the
model capacity of vision transformer is large and predictions in such
challenging tasks can be noisy, a safe training mechanism is designed to
adaptively adjust learning configuration. Extensive evaluations are conducted
on several widely tested UDA benchmarks and SSRT achieves consistently the best
performances, including 85.43% on Office-Home, 88.76% on VisDA-2017 and 45.2%
on DomainNet.",2204.07683v1,https://arxiv.org/pdf/2204.07683v1
Safe Reinforcement Learning Using Black-Box Reachability Analysis,"Mahmoud Selim, Amr Alanwar, Shreyas Kousik, Grace Gao, Marco Pavone, Karl H. Johansson","Reinforcement learning (RL) is capable of sophisticated motion planning and
control for robots in uncertain environments. However, state-of-the-art deep RL
approaches typically lack safety guarantees, especially when the robot and
environment models are unknown. To justify widespread deployment, robots must
respect safety constraints without sacrificing performance. Thus, we propose a
Black-box Reachability-based Safety Layer (BRSL) with three main components:
(1) data-driven reachability analysis for a black-box robot model, (2) a
trajectory rollout planner that predicts future actions and observations using
an ensemble of neural networks trained online, and (3) a differentiable
polytope collision check between the reachable set and obstacles that enables
correcting unsafe actions. In simulation, BRSL outperforms other
state-of-the-art safe RL methods on a Turtlebot 3, a quadrotor, a
trajectory-tracking point mass, and a hexarotor in wind with an unsafe set
adjacent to the area of highest reward.",2204.07417v2,https://arxiv.org/pdf/2204.07417v2
"Safer Autonomous Driving in a Stochastic, Partially-Observable
  Environment by Hierarchical Contingency Planning","Ugo Lecerf, Christelle Yemdji-Tchassi, Pietro Michiardi","When learning to act in a stochastic, partially observable environment, an
intelligent agent should be prepared to anticipate a change in its belief of
the environment state, and be capable of adapting its actions on-the-fly to
changing conditions. As humans, we are able to form contingency plans when
learning a task with the explicit aim of being able to correct errors in the
initial control, and hence prove useful if ever there is a sudden change in our
perception of the environment which requires immediate corrective action. This
is especially the case for autonomous vehicles (AVs) navigating real-world
situations where safety is paramount, and a strong ability to react to a
changing belief about the environment is truly needed.
  In this paper we explore an end-to-end approach, from training to execution,
for learning robust contingency plans and combining them with a hierarchical
planner to obtain a robust agent policy in an autonomous navigation task where
other vehicles' behaviours are unknown, and the agent's belief about these
behaviours is subject to sudden, last-second change. We show that our approach
results in robust, safe behaviour in a partially observable, stochastic
environment, generalizing well over environment dynamics not seen during
training.",2204.06509v1,https://arxiv.org/pdf/2204.06509v1
"Automatically Learning Fallback Strategies with Model-Free Reinforcement
  Learning in Safety-Critical Driving Scenarios","Ugo Lecerf, Christelle Yemdji-Tchassi, Sébastien Aubert, Pietro Michiardi","When learning to behave in a stochastic environment where safety is critical,
such as driving a vehicle in traffic, it is natural for human drivers to plan
fallback strategies as a backup to use if ever there is an unexpected change in
the environment. Knowing to expect the unexpected, and planning for such
outcomes, increases our capability for being robust to unseen scenarios and may
help prevent catastrophic failures. Control of Autonomous Vehicles (AVs) has a
particular interest in knowing when and how to use fallback strategies in the
interest of safety. Due to imperfect information available to an AV about its
environment, it is important to have alternate strategies at the ready which
might not have been deduced from the original training data distribution.
  In this paper we present a principled approach for a model-free Reinforcement
Learning (RL) agent to capture multiple modes of behaviour in an environment.
We introduce an extra pseudo-reward term to the reward model, to encourage
exploration to areas of state-space different from areas privileged by the
optimal policy. We base this reward term on a distance metric between the
trajectories of agents, in order to force policies to focus on different areas
of state-space than the initial exploring agent. Throughout the paper, we refer
to this particular training paradigm as learning fallback strategies.
  We apply this method to an autonomous driving scenario, and show that we are
able to learn useful policies that would have otherwise been missed out on
during training, and unavailable to use when executing the control algorithm.",2204.05196v1,https://arxiv.org/pdf/2204.05196v1
"Offline Reinforcement Learning for Safer Blood Glucose Control in People
  with Type 1 Diabetes","Harry Emerson, Matthew Guy, Ryan McConville","The widespread adoption of effective hybrid closed loop systems would
represent an important milestone of care for people living with type 1 diabetes
(T1D). These devices typically utilise simple control algorithms to select the
optimal insulin dose for maintaining blood glucose levels within a healthy
range. Online reinforcement learning (RL) has been utilised as a method for
further enhancing glucose control in these devices. Previous approaches have
been shown to reduce patient risk and improve time spent in the target range
when compared to classical control algorithms, but are prone to instability in
the learning process, often resulting in the selection of unsafe actions. This
work presents an evaluation of offline RL for developing effective dosing
policies without the need for potentially dangerous patient interaction during
training. This paper examines the utility of BCQ, CQL and TD3-BC in managing
the blood glucose of the 30 virtual patients available within the FDA-approved
UVA/Padova glucose dynamics simulator. When trained on less than a tenth of the
total training samples required by online RL to achieve stable performance,
this work shows that offline RL can significantly increase time in the healthy
blood glucose range from 61.6 +\- 0.3% to 65.3 +/- 0.5% when compared to the
strongest state-of-art baseline (p < 0.001). This is achieved without any
associated increase in low blood glucose events. Offline RL is also shown to be
able to correct for common and challenging control scenarios such as incorrect
bolus dosing, irregular meal timings and compression errors.",2204.03376v2,https://arxiv.org/pdf/2204.03376v2
Safe Reinforcement Learning via Shielding under Partial Observability,"Steven Carr, Nils Jansen, Sebastian Junges, Ufuk Topcu","Safe exploration is a common problem in reinforcement learning (RL) that aims
to prevent agents from making disastrous decisions while exploring their
environment. A family of approaches to this problem assume domain knowledge in
the form of a (partial) model of this environment to decide upon the safety of
an action. A so-called shield forces the RL agent to select only safe actions.
However, for adoption in various applications, one must look beyond enforcing
safety and also ensure the applicability of RL with good performance. We extend
the applicability of shields via tight integration with state-of-the-art deep
RL, and provide an extensive, empirical study in challenging, sparse-reward
environments under partial observability. We show that a carefully integrated
shield ensures safety and can improve the convergence rate and final
performance of RL agents. We furthermore show that a shield can be used to
bootstrap state-of-the-art RL agents: they remain safe after initial learning
in a shielded setting, allowing us to disable a potentially too conservative
shield eventually.",2204.00755v2,https://arxiv.org/pdf/2204.00755v2
Strategies for Safe Multi-Armed Bandits with Logarithmic Regret and Risk,"Tianrui Chen, Aditya Gangrade, Venkatesh Saligrama","We investigate a natural but surprisingly unstudied approach to the
multi-armed bandit problem under safety risk constraints. Each arm is
associated with an unknown law on safety risks and rewards, and the learner's
goal is to maximise reward whilst not playing unsafe arms, as determined by a
given threshold on the mean risk.
  We formulate a pseudo-regret for this setting that enforces this safety
constraint in a per-round way by softly penalising any violation, regardless of
the gain in reward due to the same. This has practical relevance to scenarios
such as clinical trials, where one must maintain safety for each round rather
than in an aggregated sense.
  We describe doubly optimistic strategies for this scenario, which maintain
optimistic indices for both safety risk and reward. We show that schema based
on both frequentist and Bayesian indices satisfy tight gap-dependent
logarithmic regret bounds, and further that these play unsafe arms only
logarithmically many times in total. This theoretical analysis is complemented
by simulation studies demonstrating the effectiveness of the proposed schema,
and probing the domains in which their use is appropriate.",2204.00706v1,https://arxiv.org/pdf/2204.00706v1
"Simulator-based explanation and debugging of hazard-triggering events in
  DNN-based safety-critical systems","Hazem Fahmy, Fabrizio Pastore, Lionel Briand, Thomas Stifter","When Deep Neural Networks (DNNs) are used in safety-critical systems,
engineers should determine the safety risks associated with failures (i.e.,
erroneous outputs) observed during testing. For DNNs processing images,
engineers visually inspect all failure-inducing images to determine common
characteristics among them. Such characteristics correspond to
hazard-triggering events (e.g., low illumination) that are essential inputs for
safety analysis. Though informative, such activity is expensive and
error-prone.
  To support such safety analysis practices, we propose SEDE, a technique that
generates readable descriptions for commonalities in failure-inducing,
real-world images and improves the DNN through effective retraining. SEDE
leverages the availability of simulators, which are commonly used for
cyber-physical systems. It relies on genetic algorithms to drive simulators
towards the generation of images that are similar to failure-inducing,
real-world images in the test set; it then employs rule learning algorithms to
derive expressions that capture commonalities in terms of simulator parameter
values. The derived expressions are then used to generate additional images to
retrain and improve the DNN.
  With DNNs performing in-car sensing tasks, SEDE successfully characterized
hazard-triggering events leading to a DNN accuracy drop. Also, SEDE enabled
retraining leading to significant improvements in DNN accuracy, up to 18
percentage points.",2204.00480v4,https://arxiv.org/pdf/2204.00480v4
Gaussian Control Barrier Functions : A Non-Parametric Paradigm to Safety,"Mouhyemen Khan, Tatsuya Ibuki, Abhijit Chatterjee","Inspired by the success of control barrier functions (CBFs) in addressing
safety, and the rise of data-driven techniques for modeling functions, we
propose a non-parametric approach for online synthesis of CBFs using Gaussian
Processes (GPs). Mathematical constructs such as CBFs have achieved safety by
designing a candidate function a priori. However, designing such a candidate
function can be challenging. A practical example of such a setting would be to
design a CBF in a disaster recovery scenario where safe and navigable regions
need to be determined. The decision boundary for safety in such an example is
unknown and cannot be designed a priori. In our approach, we work with safety
samples or observations to construct the CBF online by assuming a flexible GP
prior on these samples, and term our formulation as a Gaussian CBF. GPs have
favorable properties, in addition to being non-parametric, such as analytical
tractability and robust uncertainty estimation. This allows realizing the
posterior components with high safety guarantees by incorporating variance
estimation, while also computing associated partial derivatives in closed-form
to achieve safe control. Moreover, the synthesized safety function from our
approach allows changing the corresponding safe set arbitrarily based on the
data, thus allowing non-convex safe sets. We validate our approach
experimentally on a quadrotor by demonstrating safe control for fixed but
arbitrary safe sets and collision avoidance where the safe set is constructed
online. Finally, we juxtapose Gaussian CBFs with regular CBFs in the presence
of noisy states to highlight its flexibility and robustness to noise. The
experiment video can be seen at: https://youtu.be/HX6uokvCiGk.",2203.15474v2,https://arxiv.org/pdf/2203.15474v2
Safe Active Learning for Multi-Output Gaussian Processes,"Cen-You Li, Barbara Rakitsch, Christoph Zimmer","Multi-output regression problems are commonly encountered in science and
engineering. In particular, multi-output Gaussian processes have been emerged
as a promising tool for modeling these complex systems since they can exploit
the inherent correlations and provide reliable uncertainty estimates. In many
applications, however, acquiring the data is expensive and safety concerns
might arise (e.g. robotics, engineering). We propose a safe active learning
approach for multi-output Gaussian process regression. This approach queries
the most informative data or output taking the relatedness between the
regressors and safety constraints into account. We prove the effectiveness of
our approach by providing theoretical analysis and by demonstrating empirical
results on simulated datasets and on a real-world engineering dataset. On all
datasets, our approach shows improved convergence compared to its competitors.",2203.14849v1,https://arxiv.org/pdf/2203.14849v1
"Tuning Particle Accelerators with Safety Constraints using Bayesian
  Optimization","Johannes Kirschner, Mojmir Mutný, Andreas Krause, Jaime Coello de Portugal, Nicole Hiller, Jochem Snuverink","Tuning machine parameters of particle accelerators is a repetitive and
time-consuming task that is challenging to automate. While many off-the-shelf
optimization algorithms are available, in practice their use is limited because
most methods do not account for safety-critical constraints in each iteration,
such as loss signals or step-size limitations. One notable exception is safe
Bayesian optimization, which is a data-driven tuning approach for global
optimization with noisy feedback. We propose and evaluate a step-size limited
variant of safe Bayesian optimization on two research facilities of the Paul
Scherrer Institut (PSI): a) the Swiss Free Electron Laser (SwissFEL) and b) the
High-Intensity Proton Accelerator (HIPA). We report promising experimental
results on both machines, tuning up to 16 parameters subject to 224
constraints.",2203.13968v3,https://arxiv.org/pdf/2203.13968v3
"SERA: Safe and Efficient Reactive Obstacle Avoidance for Collaborative
  Robotic Planning in Unstructured Environments","Apan Dastider, Mingjie Lin","Safe and efficient collaboration among multiple robots in unstructured
environments is increasingly critical in the era of Industry 4.0. However,
achieving robust and autonomous collaboration among humans and other robots
requires modern robotic systems to have effective proximity perception and
reactive obstacle avoidance. In this paper, we propose a novel methodology for
reactive whole-body obstacle avoidance that ensures conflict-free robot-robot
interactions even in dynamic environment. Unlike existing approaches based on
Jacobian-type, sampling based or geometric techniques, our methodology
leverages the latest deep learning advances and topological manifold learning,
enabling it to be readily generalized to other problem settings with high
computing efficiency and fast graph traversal techniques. Our approach allows a
robotic arm to proactively avoid obstacles of arbitrary 3D shapes without
direct contact, a significant improvement over traditional industrial cobot
settings. To validate our approach, we implement it on a robotic platform
consisting of dual 6-DoF robotic arms with optimized proximity sensor
placement, capable of working collaboratively with varying levels of
interference. Specifically, one arm performs reactive whole-body obstacle
avoidance while achieving its pre-determined objective, while the other arm
emulates the presence of a human collaborator with independent and potentially
adversarial movements. Our methodology provides a robust and effective solution
for safe human-robot collaboration in non-stationary environments.",2203.13821v2,https://arxiv.org/pdf/2203.13821v2
Are Evolutionary Algorithms Safe Optimizers?,"Youngmin Kim, Richard Allmendinger, Manuel López-Ibáñez","We consider a type of constrained optimization problem, where the violation
of a constraint leads to an irrevocable loss, such as breakage of a valuable
experimental resource/platform or loss of human life. Such problems are
referred to as safe optimization problems (SafeOPs). While SafeOPs have
received attention in the machine learning community in recent years, there was
little interest in the evolutionary computation (EC) community despite some
early attempts between 2009 and 2011. Moreover, there is a lack of acceptable
guidelines on how to benchmark different algorithms for SafeOPs, an area where
the EC community has significant experience in. Driven by the need for more
efficient algorithms and benchmark guidelines for SafeOPs, the objective of
this paper is to reignite the interest of this problem class in the EC
community. To achieve this we (i) provide a formal definition of SafeOPs and
contrast it to other types of optimization problems that the EC community is
familiar with, (ii) investigate the impact of key SafeOP parameters on the
performance of selected safe optimization algorithms, (iii) benchmark EC
against state-of-the-art safe optimization algorithms from the machine learning
community, and (iv) provide an open-source Python framework to replicate and
extend our work.",2203.12622v2,https://arxiv.org/pdf/2203.12622v2
Verification of safety critical control policies using kernel methods,"Nikolaus Vertovec, Sina Ober-Blöbaum, Kostas Margellos","Hamilton-Jacobi reachability methods for safety-critical control have been
well studied, but the safety guarantees derived rely on the accuracy of the
numerical computation. Thus, it is crucial to understand and account for any
inaccuracies that occur due to uncertainty in the underlying dynamics and
environment as well as the induced numerical errors. To this end, we propose a
framework for modeling the error of the value function inherent in
Hamilton-Jacobi reachability using a Gaussian process. The derived safety
controller can be used in conjuncture with arbitrary controllers to provide a
safe hybrid control law. The marginal likelihood of the Gaussian process then
provides a confidence metric used to determine switches between a least
restrictive controller and a safety controller. We test both the prediction as
well as the correction capabilities of the presented method in a classical
pursuit-evasion example.",2203.12407v1,https://arxiv.org/pdf/2203.12407v1
"How to Learn from Risk: Explicit Risk-Utility Reinforcement Learning for
  Efficient and Safe Driving Strategies","Lukas M. Schmidt, Sebastian Rietsch, Axel Plinge, Bjoern M. Eskofier, Christopher Mutschler","Autonomous driving has the potential to revolutionize mobility and is hence
an active area of research. In practice, the behavior of autonomous vehicles
must be acceptable, i.e., efficient, safe, and interpretable. While vanilla
reinforcement learning (RL) finds performant behavioral strategies, they are
often unsafe and uninterpretable. Safety is introduced through Safe RL
approaches, but they still mostly remain uninterpretable as the learned
behaviour is jointly optimized for safety and performance without modeling them
separately. Interpretable machine learning is rarely applied to RL. This paper
proposes SafeDQN, which allows to make the behavior of autonomous vehicles safe
and interpretable while still being efficient. SafeDQN offers an
understandable, semantic trade-off between the expected risk and the utility of
actions while being algorithmically transparent. We show that SafeDQN finds
interpretable and safe driving policies for a variety of scenarios and
demonstrate how state-of-the-art saliency techniques can help to assess both
risk and utility.",2203.08409v2,https://arxiv.org/pdf/2203.08409v2
Safe Neurosymbolic Learning with Differentiable Symbolic Execution,"Chenxi Yang, Swarat Chaudhuri","We study the problem of learning worst-case-safe parameters for programs that
use neural networks as well as symbolic, human-written code. Such neurosymbolic
programs arise in many safety-critical domains. However, because they can use
nondifferentiable operations, it is hard to learn their parameters using
existing gradient-based approaches to safe learning. Our approach to this
problem, Differentiable Symbolic Execution (DSE), samples control flow paths in
a program, symbolically constructs worst-case ""safety losses"" along these
paths, and backpropagates the gradients of these losses through program
operations using a generalization of the REINFORCE estimator. We evaluate the
method on a mix of synthetic tasks and real-world benchmarks. Our experiments
show that DSE significantly outperforms the state-of-the-art DiffAI method on
these tasks.",2203.07671v1,https://arxiv.org/pdf/2203.07671v1
Safe adaptation in multiagent competition,"Macheng Shen, Jonathan P. How","Achieving the capability of adapting to ever-changing environments is a
critical step towards building fully autonomous robots that operate safely in
complicated scenarios. In multiagent competitive scenarios, agents may have to
adapt to new opponents with previously unseen behaviors by learning from the
interaction experiences between the ego-agent and the opponent. However, this
adaptation is susceptible to opponent exploitation. As the ego-agent updates
its own behavior to exploit the opponent, its own behavior could become more
exploitable as a result of overfitting to this specific opponent's behavior. To
overcome this difficulty, we developed a safe adaptation approach in which the
ego-agent is trained against a regularized opponent model, which effectively
avoids overfitting and consequently improves the robustness of the ego-agent's
policy. We evaluated our approach in the Mujoco domain with two competing
agents. The experiment results suggest that our approach effectively achieves
both adaptation to the specific opponent that the ego-agent is interacting with
and maintaining low exploitability to other possible opponent exploitation.",2203.07562v1,https://arxiv.org/pdf/2203.07562v1
"Don't fear the unlabelled: safe semi-supervised learning via simple
  debiasing","Hugo Schmutz, Olivier Humbert, Pierre-Alexandre Mattei","Semi-supervised learning (SSL) provides an effective means of leveraging
unlabelled data to improve a model performance. Even though the domain has
received a considerable amount of attention in the past years, most methods
present the common drawback of lacking theoretical guarantees. Our starting
point is to notice that the estimate of the risk that most discriminative SSL
methods minimise is biased, even asymptotically. This bias impedes the use of
standard statistical learning theory and can hurt empirical performance. We
propose a simple way of removing the bias. Our debiasing approach is
straightforward to implement and applicable to most deep SSL methods. We
provide simple theoretical guarantees on the trustworthiness of these modified
methods, without having to rely on the strong assumptions on the data
distribution that SSL theory usually requires. In particular, we provide
generalisation error bounds for the proposed methods. We evaluate debiased
versions of different existing SSL methods, such as the Pseudo-label method and
Fixmatch, and show that debiasing can compete with classic deep SSL techniques
in various settings by providing better calibrated models. Additionally, we
provide a theoretical explanation of the intuition of the popular SSL methods.",2203.07512v3,https://arxiv.org/pdf/2203.07512v3
MLNav: Learning to Safely Navigate on Martian Terrains,"Shreyansh Daftry, Neil Abcouwer, Tyler Del Sesto, Siddarth Venkatraman, Jialin Song, Lucas Igel, Amos Byon, Ugo Rosolia, Yisong Yue, Masahiro Ono","We present MLNav, a learning-enhanced path planning framework for
safety-critical and resource-limited systems operating in complex environments,
such as rovers navigating on Mars. MLNav makes judicious use of machine
learning to enhance the efficiency of path planning while fully respecting
safety constraints. In particular, the dominant computational cost in such
safety-critical settings is running a model-based safety checker on the
proposed paths. Our learned search heuristic can simultaneously predict the
feasibility for all path options in a single run, and the model-based safety
checker is only invoked on the top-scoring paths. We validate in high-fidelity
simulations using both real Martian terrain data collected by the Perseverance
rover, as well as a suite of challenging synthetic terrains. Our experiments
show that: (i) compared to the baseline ENav path planner on board the
Perserverance rover, MLNav can provide a significant improvement in multiple
key metrics, such as a 10x reduction in collision checks when navigating real
Martian terrains, despite being trained with synthetic terrains; and (ii) MLNav
can successfully navigate highly challenging terrains where the baseline ENav
fails to find a feasible path before timing out.",2203.04563v1,https://arxiv.org/pdf/2203.04563v1
Deep Learning Serves Traffic Safety Analysis: A Forward-looking Review,"Abolfazl Razi, Xiwen Chen, Huayu Li, Hao Wang, Brendan Russo, Yan Chen, Hongbin Yu","This paper explores Deep Learning (DL) methods that are used or have the
potential to be used for traffic video analysis, emphasizing driving safety for
both Autonomous Vehicles (AVs) and human-operated vehicles. We present a
typical processing pipeline, which can be used to understand and interpret
traffic videos by extracting operational safety metrics and providing general
hints and guidelines to improve traffic safety. This processing framework
includes several steps, including video enhancement, video stabilization,
semantic and incident segmentation, object detection and classification,
trajectory extraction, speed estimation, event analysis, modeling and anomaly
detection. Our main goal is to guide traffic analysts to develop their own
custom-built processing frameworks by selecting the best choices for each step
and offering new designs for the lacking modules by providing a comparative
analysis of the most successful conventional and DL-based algorithms proposed
for each step. We also review existing open-source tools and public datasets
that can help train DL models. To be more specific, we review exemplary traffic
problems and mentioned requires steps for each problem. Besides, we investigate
connections to the closely related research areas of drivers' cognition
evaluation, Crowd-sourcing-based monitoring systems, Edge Computing in roadside
infrastructures, Automated Driving Systems (ADS)-equipped vehicles, and
highlight the missing gaps. Finally, we review commercial implementations of
traffic monitoring systems, their future outlook, and open problems and
remaining challenges for widespread use of such systems.",2203.10939v2,https://arxiv.org/pdf/2203.10939v2
Safe Reinforcement Learning for Legged Locomotion,"Tsung-Yen Yang, Tingnan Zhang, Linda Luu, Sehoon Ha, Jie Tan, Wenhao Yu","Designing control policies for legged locomotion is complex due to the
under-actuated and non-continuous robot dynamics. Model-free reinforcement
learning provides promising tools to tackle this challenge. However, a major
bottleneck of applying model-free reinforcement learning in real world is
safety. In this paper, we propose a safe reinforcement learning framework that
switches between a safe recovery policy that prevents the robot from entering
unsafe states, and a learner policy that is optimized to complete the task. The
safe recovery policy takes over the control when the learner policy violates
safety constraints, and hands over the control back when there are no future
safety violations. We design the safe recovery policy so that it ensures safety
of legged locomotion while minimally intervening in the learning process.
Furthermore, we theoretically analyze the proposed framework and provide an
upper bound on the task performance. We verify the proposed framework in four
locomotion tasks on a simulated and real quadrupedal robot: efficient gait,
catwalk, two-leg balance, and pacing. On average, our method achieves 48.6%
fewer falls and comparable or better rewards than the baseline methods in
simulation. When deployed it on real-world quadruped robot, our training
pipeline enables 34% improvement in energy efficiency for the efficient gait,
40.9% narrower of the feet placement in the catwalk, and two times more jumping
duration in the two-leg balance. Our method achieves less than five falls over
the duration of 115 minutes of hardware time.",2203.02638v1,https://arxiv.org/pdf/2203.02638v1
"Bayesian Optimization Meets Hybrid Zero Dynamics: Safe Parameter
  Learning for Bipedal Locomotion Control","Lizhi Yang, Zhongyu Li, Jun Zeng, Koushil Sreenath","In this paper, we propose a multi-domain control parameter learning framework
that combines Bayesian Optimization (BO) and Hybrid Zero Dynamics (HZD) for
locomotion control of bipedal robots. We leverage BO to learn the control
parameters used in the HZD-based controller. The learning process is firstly
deployed in simulation to optimize different control parameters for a large
repertoire of gaits. Next, to tackle the discrepancy between the simulation and
the real world, the learning process is applied on the physical robot to learn
for corrections to the control parameters learned in simulation while also
respecting a safety constraint for gait stability. This method empowers an
efficient sim-to-real transition with a small number of samples in the real
world, and does not require a valid controller to initialize the training in
simulation. Our proposed learning framework is experimentally deployed and
validated on a bipedal robot Cassie to perform versatile locomotion skills with
improved performance on smoothness of walking gaits and reduction of
steady-state tracking errors.",2203.02570v1,https://arxiv.org/pdf/2203.02570v1
"AutoMO-Mixer: An automated multi-objective Mixer model for balanced,
  safe and robust prediction in medicine","Xi Chen, Jiahuan Lv, Dehua Feng, Xuanqin Mou, Ling Bai, Shu Zhang, Zhiguo Zhou","Accurately identifying patient's status through medical images plays an
important role in diagnosis and treatment. Artificial intelligence (AI),
especially the deep learning, has achieved great success in many fields.
However, more reliable AI model is needed in image guided diagnosis and
therapy. To achieve this goal, developing a balanced, safe and robust model
with a unified framework is desirable. In this study, a new unified model
termed as automated multi-objective Mixer (AutoMO-Mixer) model was developed,
which utilized a recent developed multiple layer perceptron Mixer (MLP-Mixer)
as base. To build a balanced model, sensitivity and specificity were considered
as the objective functions simultaneously in training stage. Meanwhile, a new
evidential reasoning based on entropy was developed to achieve a safe and
robust model in testing stage. The experiment on an optical coherence
tomography dataset demonstrated that AutoMO-Mixer can obtain safer, more
balanced, and robust results compared with MLP-Mixer and other available
models.",2203.02384v1,https://arxiv.org/pdf/2203.02384v1
"Evaluating Object (mis)Detection from a Safety and Reliability
  Perspective: Discussion and Measures","Andrea Ceccarelli, Leonardo Montecchi","We argue that object detectors in the safety critical domain should
prioritize detection of objects that are most likely to interfere with the
actions of the autonomous actor. Especially, this applies to objects that can
impact the actor's safety and reliability. To quantify the impact of object
(mis)detection on safety and reliability in the context of autonomous driving,
we propose new object detection measures that reward the correct identification
of objects that are most dangerous and most likely to affect driving decisions.
To achieve this, we build an object criticality model to reward the detection
of the objects based on proximity, orientation, and relative velocity with
respect to the subject vehicle. Then, we apply our model on the recent
autonomous driving dataset nuScenes, and we compare nine object detectors.
Results show that, in several settings, object detectors that perform best
according to the nuScenes ranking are not the preferable ones when the focus is
shifted on safety and reliability.",2203.02205v3,https://arxiv.org/pdf/2203.02205v3
Fail-Safe Adversarial Generative Imitation Learning,"Philipp Geiger, Christoph-Nikolas Straehle","For flexible yet safe imitation learning (IL), we propose theory and a
modular method, with a safety layer that enables a closed-form probability
density/gradient of the safe generative continuous policy, end-to-end
generative adversarial training, and worst-case safety guarantees. The safety
layer maps all actions into a set of safe actions, and uses the
change-of-variables formula plus additivity of measures for the density. The
set of safe actions is inferred by first checking safety of a finite sample of
actions via adversarial reachability analysis of fallback maneuvers, and then
concluding on the safety of these actions' neighborhoods using, e.g., Lipschitz
continuity. We provide theoretical analysis showing the robustness advantage of
using the safety layer already during training (imitation error linear in the
horizon) compared to only using it at test time (up to quadratic error). In an
experiment on real-world driver interaction data, we empirically demonstrate
tractability, safety and imitation performance of our approach.",2203.01696v2,https://arxiv.org/pdf/2203.01696v2
Model-free Neural Lyapunov Control for Safe Robot Navigation,"Zikang Xiong, Joe Eappen, Ahmed H. Qureshi, Suresh Jagannathan","Model-free Deep Reinforcement Learning (DRL) controllers have demonstrated
promising results on various challenging non-linear control tasks. While a
model-free DRL algorithm can solve unknown dynamics and high-dimensional
problems, it lacks safety assurance. Although safety constraints can be encoded
as part of a reward function, there still exists a large gap between an RL
controller trained with this modified reward and a safe controller. In
contrast, instead of implicitly encoding safety constraints with rewards, we
explicitly co-learn a Twin Neural Lyapunov Function (TNLF) with the control
policy in the DRL training loop and use the learned TNLF to build a runtime
monitor. Combined with the path generated from a planner, the monitor chooses
appropriate waypoints that guide the learned controller to provide
collision-free control trajectories. Our approach inherits the scalability
advantages from DRL while enhancing safety guarantees. Our experimental
evaluation demonstrates the effectiveness of our approach compared to DRL with
augmented rewards and constrained DRL methods over a range of high-dimensional
safety-sensitive navigation tasks.",2203.01190v1,https://arxiv.org/pdf/2203.01190v1
Safe Exploration for Efficient Policy Evaluation and Comparison,"Runzhe Wan, Branislav Kveton, Rui Song","High-quality data plays a central role in ensuring the accuracy of policy
evaluation. This paper initiates the study of efficient and safe data
collection for bandit policy evaluation. We formulate the problem and
investigate its several representative variants. For each variant, we analyze
its statistical properties, derive the corresponding exploration policy, and
design an efficient algorithm for computing it. Both theoretical analysis and
experiments support the usefulness of the proposed methods.",2202.13234v2,https://arxiv.org/pdf/2202.13234v2
"Towards Safe, Real-Time Systems: Stereo vs Images and LiDAR for 3D
  Object Detection",Matthew Levine,"As object detectors rapidly improve, attention has expanded past image-only
networks to include a range of 3D and multimodal frameworks, especially ones
that incorporate LiDAR. However, due to cost, logistics, and even some safety
considerations, stereo can be an appealing alternative. Towards understanding
the efficacy of stereo as a replacement for monocular input or LiDAR in object
detectors, we show that multimodal learning with traditional disparity
algorithms can improve image-based results without increasing the number of
parameters, and that learning over stereo error can impart similar 3D
localization power to LiDAR in certain contexts. Furthermore, doing so also has
calibration benefits with respect to image-only methods. We benchmark on the
public dataset KITTI, and in doing so, reveal a few small but common
algorithmic mistakes currently used in computing metrics on that set, and offer
efficient, provably correct alternatives.",2202.12773v1,https://arxiv.org/pdf/2202.12773v1
Finding Safe Zones of policies Markov Decision Processes,"Lee Cohen, Yishay Mansour, Michal Moshkovitz","Given a policy of a Markov Decision Process, we define a SafeZone as a subset
of states, such that most of the policy's trajectories are confined to this
subset. The quality of a SafeZone is parameterized by the number of states and
the escape probability, i.e., the probability that a random trajectory will
leave the subset. SafeZones are especially interesting when they have a small
number of states and low escape probability. We study the complexity of finding
optimal SafeZones, and show that in general, the problem is computationally
hard. Our main result is a bi-criteria approximation learning algorithm with a
factor of almost $2$ approximation for both the escape probability and SafeZone
size, using a polynomial size sample complexity.",2202.11593v2,https://arxiv.org/pdf/2202.11593v2
"Networked Online Learning for Control of Safety-Critical
  Resource-Constrained Systems based on Gaussian Processes","Armin Lederer, Mingmin Zhang, Samuel Tesfazgi, Sandra Hirche","Safety-critical technical systems operating in unknown environments require
the ability to quickly adapt their behavior, which can be achieved in control
by inferring a model online from the data stream generated during operation.
Gaussian process-based learning is particularly well suited for safety-critical
applications as it ensures bounded prediction errors. While there exist
computationally efficient approximations for online inference, these approaches
lack guarantees for the prediction error and have high memory requirements, and
are therefore not applicable to safety-critical systems with tight memory
constraints. In this work, we propose a novel networked online learning
approach based on Gaussian process regression, which addresses the issue of
limited local resources by employing remote data management in the cloud. Our
approach formally guarantees a bounded tracking error with high probability,
which is exploited to identify the most relevant data to achieve a certain
control performance. We further propose an effective data transmission scheme
between the local system and the cloud taking bandwidth limitations and time
delay of the transmission channel into account. The effectiveness of the
proposed method is successfully demonstrated in a simulation.",2202.11491v1,https://arxiv.org/pdf/2202.11491v1
Margin-distancing for safe model explanation,"Tom Yan, Chicheng Zhang","The growing use of machine learning models in consequential settings has
highlighted an important and seemingly irreconcilable tension between
transparency and vulnerability to gaming. While this has sparked sizable debate
in legal literature, there has been comparatively less technical study of this
contention. In this work, we propose a clean-cut formulation of this tension
and a way to make the tradeoff between transparency and gaming. We identify the
source of gaming as being points close to the \emph{decision boundary} of the
model. And we initiate an investigation on how to provide example-based
explanations that are expansive and yet consistent with a version space that is
sufficiently uncertain with respect to the boundary points' labels. Finally, we
furnish our theoretical results with empirical investigations of this tradeoff
on real-world datasets.",2202.11266v1,https://arxiv.org/pdf/2202.11266v1
"Decentralized Safe Multi-agent Stochastic Optimal Control using Deep
  FBSDEs and ADMM","Marcus A. Pereira, Augustinos D. Saravanos, Oswin So, Evangelos A. Theodorou","In this work, we propose a novel safe and scalable decentralized solution for
multi-agent control in the presence of stochastic disturbances. Safety is
mathematically encoded using stochastic control barrier functions and safe
controls are computed by solving quadratic programs. Decentralization is
achieved by augmenting to each agent's optimization variables, copy variables,
for its neighbors. This allows us to decouple the centralized multi-agent
optimization problem. However, to ensure safety, neighboring agents must agree
on ""what is safe for both of us"" and this creates a need for consensus. To
enable safe consensus solutions, we incorporate an ADMM-based approach.
Specifically, we propose a Merged CADMM-OSQP implicit neural network layer,
that solves a mini-batch of both, local quadratic programs as well as the
overall consensus problem, as a single optimization problem. This layer is
embedded within a Deep FBSDEs network architecture at every time step, to
facilitate end-to-end differentiable, safe and decentralized stochastic optimal
control. The efficacy of the proposed approach is demonstrated on several
challenging multi-robot tasks in simulation. By imposing requirements on safety
specified by collision avoidance constraints, the safe operation of all agents
is ensured during the entire training process. We also demonstrate superior
scalability in terms of computational and memory savings as compared to a
centralized approach.",2202.10658v2,https://arxiv.org/pdf/2202.10658v2
"Multi-task Safe Reinforcement Learning for Navigating Intersections in
  Dense Traffic","Yuqi Liu, Qichao Zhang, Dongbin Zhao","Multi-task intersection navigation including the unprotected turning left,
turning right, and going straight in dense traffic is still a challenging task
for autonomous driving. For the human driver, the negotiation skill with other
interactive vehicles is the key to guarantee safety and efficiency. However, it
is hard to balance the safety and efficiency of the autonomous vehicle for
multi-task intersection navigation. In this paper, we formulate a multi-task
safe reinforcement learning with social attention to improve the safety and
efficiency when interacting with other traffic participants. Specifically, the
social attention module is used to focus on the states of negotiation vehicles.
In addition, a safety layer is added to the multi-task reinforcement learning
framework to guarantee safe negotiation. We compare the experiments in the
simulator SUMO with abundant traffic flows and CARLA with high-fidelity vehicle
models, which both show that the proposed algorithm can improve safety with
consistent traffic efficiency for multi-task intersection navigation.",2202.09644v1,https://arxiv.org/pdf/2202.09644v1
System Safety and Artificial Intelligence,Roel I. J. Dobbe,"This chapter formulates seven lessons for preventing harm in artificial
intelligence (AI) systems based on insights from the field of system safety for
software-based automation in safety-critical domains. New applications of AI
across societal domains and public organizations and infrastructures come with
new hazards, which lead to new forms of harm, both grave and pernicious. The
text addresses the lack of consensus for diagnosing and eliminating new AI
system hazards. For decades, the field of system safety has dealt with
accidents and harm in safety-critical systems governed by varying degrees of
software-based automation and decision-making. This field embraces the core
assumption of systems and control that AI systems cannot be safeguarded by
technical design choices on the model or algorithm alone, instead requiring an
end-to-end hazard analysis and design frame that includes the context of use,
impacted stakeholders and the formal and informal institutional environment in
which the system operates. Safety and other values are then inherently
socio-technical and emergent system properties that require design and control
measures to instantiate these across the technical, social and institutional
components of a system. This chapter honors system safety pioneer Nancy
Leveson, by situating her core lessons for today's AI system safety challenges.
For every lesson, concrete tools are offered for rethinking and reorganizing
the safety management of AI systems, both in design and governance. This
history tells us that effective AI safety management requires transdisciplinary
approaches and a shared language that allows involvement of all levels of
society.",2202.09292v1,https://arxiv.org/pdf/2202.09292v1
"Efficient Learning of Safe Driving Policy via Human-AI Copilot
  Optimization","Quanyi Li, Zhenghao Peng, Bolei Zhou","Human intervention is an effective way to inject human knowledge into the
training loop of reinforcement learning, which can bring fast learning and
ensured training safety. Given the very limited budget of human intervention,
it remains challenging to design when and how human expert interacts with the
learning agent in the training. In this work, we develop a novel
human-in-the-loop learning method called Human-AI Copilot Optimization
(HACO).To allow the agent's sufficient exploration in the risky environments
while ensuring the training safety, the human expert can take over the control
and demonstrate how to avoid probably dangerous situations or trivial
behaviors. The proposed HACO then effectively utilizes the data both from the
trial-and-error exploration and human's partial demonstration to train a
high-performing agent. HACO extracts proxy state-action values from partial
human demonstration and optimizes the agent to improve the proxy values
meanwhile reduce the human interventions. The experiments show that HACO
achieves a substantially high sample efficiency in the safe driving benchmark.
HACO can train agents to drive in unseen traffic scenarios with a handful of
human intervention budget and achieve high safety and generalizability,
outperforming both reinforcement learning and imitation learning baselines with
a large margin. Code and demo videos are available at:
https://decisionforce.github.io/HACO/.",2202.10341v1,https://arxiv.org/pdf/2202.10341v1
Safe Reinforcement Learning by Imagining the Near Future,"Garrett Thomas, Yuping Luo, Tengyu Ma","Safe reinforcement learning is a promising path toward applying reinforcement
learning algorithms to real-world problems, where suboptimal behaviors may lead
to actual negative consequences. In this work, we focus on the setting where
unsafe states can be avoided by planning ahead a short time into the future. In
this setting, a model-based agent with a sufficiently accurate model can avoid
unsafe states. We devise a model-based algorithm that heavily penalizes unsafe
trajectories, and derive guarantees that our algorithm can avoid unsafe states
under certain assumptions. Experiments demonstrate that our algorithm can
achieve competitive rewards with fewer safety violations in several continuous
control tasks.",2202.07789v1,https://arxiv.org/pdf/2202.07789v1
"CUP: A Conservative Update Policy Algorithm for Safe Reinforcement
  Learning","Long Yang, Jiaming Ji, Juntao Dai, Yu Zhang, Pengfei Li, Gang Pan","Safe reinforcement learning (RL) is still very challenging since it requires
the agent to consider both return maximization and safe exploration. In this
paper, we propose CUP, a Conservative Update Policy algorithm with a
theoretical safety guarantee. We derive the CUP based on the new proposed
performance bounds and surrogate functions. Although using bounds as surrogate
functions to design safe RL algorithms have appeared in some existing works, we
develop them at least three aspects: (i) We provide a rigorous theoretical
analysis to extend the surrogate functions to generalized advantage estimator
(GAE). GAE significantly reduces variance empirically while maintaining a
tolerable level of bias, which is an efficient step for us to design CUP; (ii)
The proposed bounds are tighter than existing works, i.e., using the proposed
bounds as surrogate functions are better local approximations to the objective
and safety constraints. (iii) The proposed CUP provides a non-convex
implementation via first-order optimizers, which does not depend on any convex
approximation. Finally, extensive experiments show the effectiveness of CUP
where the agent satisfies safe constraints. We have opened the source code of
CUP at https://github.com/RL-boxes/Safe-RL.",2202.07565v1,https://arxiv.org/pdf/2202.07565v1
"Accelerating Non-Negative and Bounded-Variable Linear Regression
  Algorithms with Safe Screening","Cassio F. Dantas, Emmanuel Soubies, Cédric Févotte","Non-negative and bounded-variable linear regression problems arise in a
variety of applications in machine learning and signal processing. In this
paper, we propose a technique to accelerate existing solvers for these problems
by identifying saturated coordinates in the course of iterations. This is akin
to safe screening techniques previously proposed for sparsity-regularized
regression problems. The proposed strategy is provably safe as it provides
theoretical guarantees that the identified coordinates are indeed saturated in
the optimal solution. Experimental results on synthetic and real data show
compelling accelerations for both non-negative and bounded-variable problems.",2202.07258v2,https://arxiv.org/pdf/2202.07258v2
"Saute RL: Almost Surely Safe Reinforcement Learning Using State
  Augmentation","Aivar Sootla, Alexander I. Cowen-Rivers, Taher Jafferjee, Ziyan Wang, David Mguni, Jun Wang, Haitham Bou-Ammar","Satisfying safety constraints almost surely (or with probability one) can be
critical for the deployment of Reinforcement Learning (RL) in real-life
applications. For example, plane landing and take-off should ideally occur with
probability one. We address the problem by introducing Safety Augmented (Saute)
Markov Decision Processes (MDPs), where the safety constraints are eliminated
by augmenting them into the state-space and reshaping the objective. We show
that Saute MDP satisfies the Bellman equation and moves us closer to solving
Safe RL with constraints satisfied almost surely. We argue that Saute MDP
allows viewing the Safe RL problem from a different perspective enabling new
features. For instance, our approach has a plug-and-play nature, i.e., any RL
algorithm can be ""Sauteed"". Additionally, state augmentation allows for policy
generalization across safety constraints. We finally show that Saute RL
algorithms can outperform their state-of-the-art counterparts when constraint
satisfaction is of high importance.",2202.06558v3,https://arxiv.org/pdf/2202.06558v3
SafePicking: Learning Safe Object Extraction via Object-Level Mapping,"Kentaro Wada, Stephen James, Andrew J. Davison","Robots need object-level scene understanding to manipulate objects while
reasoning about contact, support, and occlusion among objects. Given a pile of
objects, object recognition and reconstruction can identify the boundary of
object instances, giving important cues as to how the objects form and support
the pile. In this work, we present a system, SafePicking, that integrates
object-level mapping and learning-based motion planning to generate a motion
that safely extracts occluded target objects from a pile. Planning is done by
learning a deep Q-network that receives observations of predicted poses and a
depth-based heightmap to output a motion trajectory, trained to maximize a
safety metric reward. Our results show that the observation fusion of poses and
depth-sensing gives both better performance and robustness to the model. We
evaluate our methods using the YCB objects in both simulation and the real
world, achieving safe object extraction from piles.",2202.05832v2,https://arxiv.org/pdf/2202.05832v2
"Integrating Testing and Operation-related Quantitative Evidences in
  Assurance Cases to Argue Safety of Data-Driven AI/ML Components","Michael Kläs, Lisa Jöckel, Rasmus Adler, Jan Reich","In the future, AI will increasingly find its way into systems that can
potentially cause physical harm to humans. For such safety-critical systems, it
must be demonstrated that their residual risk does not exceed what is
acceptable. This includes, in particular, the AI components that are part of
such systems' safety-related functions. Assurance cases are an intensively
discussed option today for specifying a sound and comprehensive safety argument
to demonstrate a system's safety. In previous work, it has been suggested to
argue safety for AI components by structuring assurance cases based on two
complementary risk acceptance criteria. One of these criteria is used to derive
quantitative targets regarding the AI. The argumentation structures commonly
proposed to show the achievement of such quantitative targets, however, focus
on failure rates from statistical testing. Further important aspects are only
considered in a qualitative manner -- if at all. In contrast, this paper
proposes a more holistic argumentation structure for having achieved the
target, namely a structure that integrates test results with runtime aspects
and the impact of scope compliance and test data quality in a quantitative
manner. We elaborate different argumentation options, present the underlying
mathematical considerations, and discuss resulting implications for their
practical application. Using the proposed argumentation structure might not
only increase the integrity of assurance cases but may also allow claims on
quantitative targets that would not be justifiable otherwise.",2202.05313v1,https://arxiv.org/pdf/2202.05313v1
"Unaligned but Safe -- Formally Compensating Performance Limitations for
  Imprecise 2D Object Detection","Tobias Schuster, Emmanouil Seferis, Simon Burton, Chih-Hong Cheng","In this paper, we consider the imperfection within machine learning-based 2D
object detection and its impact on safety. We address a special sub-type of
performance limitations: the prediction bounding box cannot be perfectly
aligned with the ground truth, but the computed Intersection-over-Union metric
is always larger than a given threshold. Under such type of performance
limitation, we formally prove the minimum required bounding box enlargement
factor to cover the ground truth. We then demonstrate that the factor can be
mathematically adjusted to a smaller value, provided that the motion planner
takes a fixed-length buffer in making its decisions. Finally, observing the
difference between an empirically measured enlargement factor and our formally
derived worst-case enlargement factor offers an interesting connection between
the quantitative evidence (demonstrated by statistics) and the qualitative
evidence (demonstrated by worst-case analysis).",2202.05123v1,https://arxiv.org/pdf/2202.05123v1
"SAFER: Data-Efficient and Safe Reinforcement Learning via Skill
  Acquisition","Dylan Slack, Yinlam Chow, Bo Dai, Nevan Wichers","Methods that extract policy primitives from offline demonstrations using deep
generative models have shown promise at accelerating reinforcement learning(RL)
for new tasks. Intuitively, these methods should also help to trainsafeRLagents
because they enforce useful skills. However, we identify these techniques are
not well equipped for safe policy learning because they ignore negative
experiences(e.g., unsafe or unsuccessful), focusing only on positive
experiences, which harms their ability to generalize to new tasks safely.
Rather, we model the latentsafetycontextusing principled contrastive training
on an offline dataset of demonstrations from many tasks, including both
negative and positive experiences. Using this late variable, our RL framework,
SAFEty skill pRiors (SAFER) extracts task-specific safe primitive skills to
safely and successfully generalize to new tasks. In the inference stage,
policies trained with SAFER learn to compose safe skills into successful
policies. We theoretically characterize why SAFER can enforce safe policy
learning and demonstrate its effectiveness on several complex safety-critical
robotic grasping tasks inspired by the game Operation, in which
SAFERoutperforms state-of-the-art primitive learning methods in success and
safety.",2202.04849v2,https://arxiv.org/pdf/2202.04849v2
"Smartphone-based Hard-braking Event Detection at Scale for Road Safety
  Services","Luyang Liu, David Racz, Kara Vaillancourt, Julie Michelman, Matt Barnes, Stefan Mellem, Paul Eastham, Bradley Green, Charles Armstrong, Rishi Bal, Shawn O'Banion, Feng Guo","Road crashes are the sixth leading cause of lost disability-adjusted
life-years (DALYs) worldwide. One major challenge in traffic safety research is
the sparsity of crashes, which makes it difficult to achieve a fine-grain
understanding of crash causations and predict future crash risk in a timely
manner. Hard-braking events have been widely used as a safety surrogate due to
their relatively high prevalence and ease of detection with embedded vehicle
sensors. As an alternative to using sensors fixed in vehicles, this paper
presents a scalable approach for detecting hard-braking events using the
kinematics data collected from smartphone sensors. We train a Transformer-based
machine learning model for hard-braking event detection using concurrent sensor
readings from smartphones and vehicle sensors from drivers who connect their
phone to the vehicle while navigating in Google Maps. The detection model shows
superior performance with a $0.83$ Area under the Precision-Recall Curve
(PR-AUC), which is $3.8\times$better than a GPS speed-based heuristic model,
and $166.6\times$better than an accelerometer-based heuristic model. The
detected hard-braking events are strongly correlated with crashes from publicly
available datasets, supporting their use as a safety surrogate. In addition, we
conduct model fairness and selection bias evaluation to ensure that the safety
benefits are equally shared. The developed methodology can benefit many safety
applications such as identifying safety hot spots at road network level,
evaluating the safety of new user interfaces, as well as using routing to
improve traffic safety.",2202.01934v1,https://arxiv.org/pdf/2202.01934v1
Predicting the impact of urban change in pedestrian and road safety,"Cristina Bustos, Daniel Rhoads, Agata Lapedriza, Javier Borge-Holthoefer, Albert Solé-Ribalta","Increased interaction between and among pedestrians and vehicles in the
crowded urban environments of today gives rise to a negative side-effect: a
growth in traffic accidents, with pedestrians being the most vulnerable
elements. Recent work has shown that Convolutional Neural Networks are able to
accurately predict accident rates exploiting Street View imagery along urban
roads. The promising results point to the plausibility of aided design of safe
urban landscapes, for both pedestrians and vehicles. In this paper, by
considering historical accident data and Street View images, we detail how to
automatically predict the impact (increase or decrease) of urban interventions
on accident incidence. The results are positive, rendering an accuracies
ranging from 60 to 80%. We additionally provide an interpretability analysis to
unveil which specific categories of urban features impact accident rates
positively or negatively. Considering the transportation network substrates
(sidewalk and road networks) and their demand, we integrate these results to a
complex network framework, to estimate the effective impact of urban change on
the safety of pedestrians and vehicles. Results show that public authorities
may leverage on machine learning tools to prioritize targeted interventions,
since our analysis show that limited improvement is obtained with current
tools. Further, our findings have a wider application range such as the design
of safe urban routes for pedestrians or to the field of driver-assistance
technologies.",2202.01781v1,https://arxiv.org/pdf/2202.01781v1
"Safe Screening for Logistic Regression with $\ell_0$-$\ell_2$
  Regularization","Anna Deza, Alper Atamturk","In logistic regression, it is often desirable to utilize regularization to
promote sparse solutions, particularly for problems with a large number of
features compared to available labels. In this paper, we present screening
rules that safely remove features from logistic regression with $\ell_0-\ell_2$
regularization before solving the problem. The proposed safe screening rules
are based on lower bounds from the Fenchel dual of strong conic relaxations of
the logistic regression problem. Numerical experiments with real and synthetic
data suggest that a high percentage of the features can be effectively and
safely removed apriori, leading to substantial speed-up in the computations.",2202.00467v1,https://arxiv.org/pdf/2202.00467v1
"AI-based Approach for Safety Signals Detection from Social Networks:
  Application to the Levothyrox Scandal in 2017 on Doctissimo Forum","Valentin Roche, Jean-Philippe Robert, Hanan Salam","Social media can be an important source of information facilitating the
detection of new safety signals in pharmacovigilance. Various approaches have
investigated the analysis of social media data using AI such as NLP techniques
for detecting adverse drug events. Existing approaches have focused on the
extraction and identification of Adverse Drug Reactions, Drug-Drug Interactions
and drug misuse. However, non of the works tackled the detection of potential
safety signals by taking into account the evolution in time of relevant
indicators. Moreover, despite the success of deep learning in various
healthcare applications, it was not explored for this task. We propose an
AI-based approach for the detection of potential pharmaceutical safety signals
from patients' reviews that can be used as part of the pharmacovigilance
surveillance process to flag the necessity of an in-depth pharmacovigilance
investigation. We focus on the Levothyrox case in France which triggered huge
attention from the media following the change of the medication formula,
leading to an increase in the frequency of adverse drug reactions normally
reported by patients. Our approach is two-fold. (1) We investigate various
NLP-based indicators extracted from patients' reviews including words and
n-grams frequency, semantic similarity, Adverse Drug Reactions mentions, and
sentiment analysis. (2) We propose a deep learning architecture, named Word
Cloud Convolutional Neural Network (WC-CNN) which trains a CNN on word clouds
extracted from the patients comments. We study the effect of different time
resolutions and different NLP pre-processing techniques on the model
performance. Our results show that the proposed indicators could be used in the
future to effectively detect new safety signals. The WC-CNN model trained on
word clouds extracted at monthly resolution outperforms the others with an
accuracy of 75%.",2203.03538v1,https://arxiv.org/pdf/2203.03538v1
"A Safety-Critical Decision Making and Control Framework Combining
  Machine Learning and Rule-based Algorithms","Andrei Aksjonov, Ville Kyrki","While artificial-intelligence-based methods suffer from lack of transparency,
rule-based methods dominate in safety-critical systems. Yet, the latter cannot
compete with the first ones in robustness to multiple requirements, for
instance, simultaneously addressing safety, comfort, and efficiency. Hence, to
benefit from both methods they must be joined in a single system. This paper
proposes a decision making and control framework, which profits from advantages
of both the rule- and machine-learning-based techniques while compensating for
their disadvantages. The proposed method embodies two controllers operating in
parallel, called Safety and Learned. A rule-based switching logic selects one
of the actions transmitted from both controllers. The Safety controller is
prioritized every time, when the Learned one does not meet the safety
constraint, and also directly participates in the safe Learned controller
training. Decision making and control in autonomous driving is chosen as the
system case study, where an autonomous vehicle learns a multi-task policy to
safely cross an unprotected intersection. Multiple requirements (i.e., safety,
efficiency, and comfort) are set for vehicle operation. A numerical simulation
is performed for the proposed framework validation, where its ability to
satisfy the requirements and robustness to changing environment is successfully
demonstrated.",2201.12819v1,https://arxiv.org/pdf/2201.12819v1
Towards Safe Reinforcement Learning with a Safety Editor Policy,"Haonan Yu, Wei Xu, Haichao Zhang","We consider the safe reinforcement learning (RL) problem of maximizing
utility with extremely low constraint violation rates. Assuming no prior
knowledge or pre-training of the environment safety model given a task, an
agent has to learn, via exploration, which states and actions are safe. A
popular approach in this line of research is to combine a model-free RL
algorithm with the Lagrangian method to adjust the weight of the constraint
reward relative to the utility reward dynamically. It relies on a single policy
to handle the conflict between utility and constraint rewards, which is often
challenging. We present SEditor, a two-policy approach that learns a safety
editor policy transforming potentially unsafe actions proposed by a utility
maximizer policy into safe ones. The safety editor is trained to maximize the
constraint reward while minimizing a hinge loss of the utility state-action
values before and after an action is edited. SEditor extends existing safety
layer designs that assume simplified safety models, to general safe RL
scenarios where the safety model can in theory be arbitrarily complex. As a
first-order method, it is easy to implement and efficient for both inference
and training. On 12 Safety Gym tasks and 2 safe racing tasks, SEditor obtains
much a higher overall safety-weighted-utility (SWU) score than the baselines,
and demonstrates outstanding utility performance with constraint violation
rates as low as once per 2k time steps, even in obstacle-dense environments. On
some tasks, this low violation rate is up to 200 times lower than that of an
unconstrained RL method with similar utility performance. Code is available at
https://github.com/hnyu/seditor.",2201.12427v3,https://arxiv.org/pdf/2201.12427v3
Safe Policy Improvement Approaches on Discrete Markov Decision Processes,"Philipp Scholl, Felix Dietrich, Clemens Otte, Steffen Udluft","Safe Policy Improvement (SPI) aims at provable guarantees that a learned
policy is at least approximately as good as a given baseline policy. Building
on SPI with Soft Baseline Bootstrapping (Soft-SPIBB) by Nadjahi et al., we
identify theoretical issues in their approach, provide a corrected theory, and
derive a new algorithm that is provably safe on finite Markov Decision
Processes (MDP). Additionally, we provide a heuristic algorithm that exhibits
the best performance among many state of the art SPI algorithms on two
different benchmarks. Furthermore, we introduce a taxonomy of SPI algorithms
and empirically show an interesting property of two classes of SPI algorithms:
while the mean performance of algorithms that incorporate the uncertainty as a
penalty on the action-value is higher, actively restricting the set of policies
more consistently produces good policies and is, thus, safer.",2201.12175v1,https://arxiv.org/pdf/2201.12175v1
"Constrained Variational Policy Optimization for Safe Reinforcement
  Learning","Zuxin Liu, Zhepeng Cen, Vladislav Isenbaev, Wei Liu, Zhiwei Steven Wu, Bo Li, Ding Zhao","Safe reinforcement learning (RL) aims to learn policies that satisfy certain
constraints before deploying them to safety-critical applications. Previous
primal-dual style approaches suffer from instability issues and lack optimality
guarantees. This paper overcomes the issues from the perspective of
probabilistic inference. We introduce a novel Expectation-Maximization approach
to naturally incorporate constraints during the policy learning: 1) a provable
optimal non-parametric variational distribution could be computed in closed
form after a convex optimization (E-step); 2) the policy parameter is improved
within the trust region based on the optimal variational distribution (M-step).
The proposed algorithm decomposes the safe RL problem into a convex
optimization phase and a supervised learning phase, which yields a more stable
training performance. A wide range of experiments on continuous robotic tasks
shows that the proposed method achieves significantly better constraint
satisfaction performance and better sample efficiency than baselines. The code
is available at https://github.com/liuzuxin/cvpo-safe-rl.",2201.11927v3,https://arxiv.org/pdf/2201.11927v3
"Network-level Safety Metrics for Overall Traffic Safety Assessment: A
  Case Study","Xiwen Chen, Hao Wang, Abolfazl Razi, Brendan Russo, Jason Pacheco, John Roberts, Jeffrey Wishart, Larry Head, Alonso Granados Baca","Driving safety analysis has recently experienced unprecedented improvements
thanks to technological advances in precise positioning sensors, artificial
intelligence (AI)-based safety features, autonomous driving systems, connected
vehicles, high-throughput computing, and edge computing servers. Particularly,
deep learning (DL) methods empowered volume video processing to extract
safety-related features from massive videos captured by roadside units (RSU).
Safety metrics are commonly used measures to investigate crashes and
near-conflict events. However, these metrics provide limited insight into the
overall network-level traffic management. On the other hand, some safety
assessment efforts are devoted to processing crash reports and identifying
spatial and temporal patterns of crashes that correlate with road geometry,
traffic volume, and weather conditions. This approach relies merely on crash
reports and ignores the rich information of traffic videos that can help
identify the role of safety violations in crashes. To bridge these two
perspectives, we define a new set of network-level safety metrics (NSM) to
assess the overall safety profile of traffic flow by processing imagery taken
by RSU cameras. Our analysis suggests that NSMs show significant statistical
associations with crash rates. This approach is different than simply
generalizing the results of individual crash analyses, since all vehicles
contribute to calculating NSMs, not only the ones involved in crash incidents.
This perspective considers the traffic flow as a complex dynamic system where
actions of some nodes can propagate through the network and influence the crash
risk for other nodes. We also provide a comprehensive review of surrogate
safety metrics (SSM) in the Appendix A.",2201.13229v2,https://arxiv.org/pdf/2201.13229v2
"SafeAPT: Safe Simulation-to-Real Robot Learning using Diverse Policies
  Learned in Simulation","Rituraj Kaushik, Karol Arndt, Ville Kyrki","The framework of Simulation-to-real learning, i.e, learning policies in
simulation and transferring those policies to the real world is one of the most
promising approaches towards data-efficient learning in robotics. However, due
to the inevitable reality gap between the simulation and the real world, a
policy learned in the simulation may not always generate a safe behaviour on
the real robot. As a result, during adaptation of the policy in the real world,
the robot may damage itself or cause harm to its surroundings. In this work, we
introduce a novel learning algorithm called SafeAPT that leverages a diverse
repertoire of policies evolved in the simulation and transfers the most
promising safe policy to the real robot through episodic interaction. To
achieve this, SafeAPT iteratively learns a probabilistic reward model as well
as a safety model using real-world observations combined with simulated
experiences as priors. Then, it performs Bayesian optimization on the
repertoire with the reward model while maintaining the specified safety
constraint using the safety model. SafeAPT allows a robot to adapt to a wide
range of goals safely with the same repertoire of policies evolved in the
simulation. We compare SafeAPT with several baselines, both in simulated and
real robotic experiments and show that SafeAPT finds high-performance policies
within a few minutes in the real world while minimizing safety violations
during the interactions.",2201.13248v1,https://arxiv.org/pdf/2201.13248v1
"DecisionHoldem: Safe Depth-Limited Solving With Diverse Opponents for
  Imperfect-Information Games","Qibin Zhou, Dongdong Bai, Junge Zhang, Fuqing Duan, Kaiqi Huang","An imperfect-information game is a type of game with asymmetric information.
It is more common in life than perfect-information game. Artificial
intelligence (AI) in imperfect-information games, such like poker, has made
considerable progress and success in recent years. The great success of
superhuman poker AI, such as Libratus and Deepstack, attracts researchers to
pay attention to poker research. However, the lack of open-source code limits
the development of Texas hold'em AI to some extent. This article introduces
DecisionHoldem, a high-level AI for heads-up no-limit Texas hold'em with safe
depth-limited subgame solving by considering possible ranges of opponent's
private hands to reduce the exploitability of the strategy. Experimental
results show that DecisionHoldem defeats the strongest openly available agent
in heads-up no-limit Texas hold'em poker, namely Slumbot, and a high-level
reproduction of Deepstack, viz, Openstack, by more than 730 mbb/h
(one-thousandth big blind per round) and 700 mbb/h. Moreover, we release the
source codes and tools of DecisionHoldem to promote AI development in
imperfect-information games.",2201.11580v2,https://arxiv.org/pdf/2201.11580v2
Safe AI -- How is this Possible?,"Harald Rueß, Simon Burton","Ttraditional safety engineering is coming to a turning point moving from
deterministic, non-evolving systems operating in well-defined contexts to
increasingly autonomous and learning-enabled AI systems which are acting in
largely unpredictable operating contexts. We outline some of underlying
challenges of safe AI and suggest a rigorous engineering framework for
minimizing uncertainty, thereby increasing confidence, up to tolerable levels,
in the safe behavior of AI systems.",2201.10436v2,https://arxiv.org/pdf/2201.10436v2
"GoSafeOpt: Scalable Safe Exploration for Global Optimization of
  Dynamical Systems","Bhavya Sukhija, Matteo Turchetta, David Lindner, Andreas Krause, Sebastian Trimpe, Dominik Baumann","Learning optimal control policies directly on physical systems is challenging
since even a single failure can lead to costly hardware damage. Most existing
model-free learning methods that guarantee safety, i.e., no failures, during
exploration are limited to local optima. A notable exception is the GoSafe
algorithm, which, unfortunately, cannot handle high-dimensional systems and
hence cannot be applied to most real-world dynamical systems. This work
proposes GoSafeOpt as the first algorithm that can safely discover globally
optimal policies for high-dimensional systems while giving safety and
optimality guarantees. We demonstrate the superiority of GoSafeOpt over
competing model-free safe learning methods on a robot arm that would be
prohibitive for GoSafe.",2201.09562v5,https://arxiv.org/pdf/2201.09562v5
"Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and
  Generalization Guarantees","Kai-Chieh Hsu, Allen Z. Ren, Duy Phuong Nguyen, Anirudha Majumdar, Jaime F. Fisac","Safety is a critical component of autonomous systems and remains a challenge
for learning-based policies to be utilized in the real world. In particular,
policies learned using reinforcement learning often fail to generalize to novel
environments due to unsafe behavior. In this paper, we propose
Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically
guaranteed safety-aware policy distribution. To improve safety, we apply a dual
policy setup where a performance policy is trained using the cumulative task
reward and a backup (safety) policy is trained by solving the Safety Bellman
Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab
transfer, we apply a supervisory control scheme to shield unsafe actions during
exploration; in Lab-to-Real transfer, we leverage the Probably Approximately
Correct (PAC)-Bayes framework to provide lower bounds on the expected
performance and safety of policies in unseen environments. Additionally,
inheriting from the HJ reachability analysis, the bound accounts for the
expectation over the worst-case safety in each environment. We empirically
study the proposed framework for ego-vision navigation in two types of indoor
environments with varying degrees of photorealism. We also demonstrate strong
generalization performance through hardware experiments in real indoor spaces
with a quadrupedal robot. See
https://sites.google.com/princeton.edu/sim-to-lab-to-real for supplementary
material.",2201.08355v4,https://arxiv.org/pdf/2201.08355v4
"Self-Awareness Safety of Deep Reinforcement Learning in Road Traffic
  Junction Driving","Zehong Cao, Jie Yun","Autonomous driving has been at the forefront of public interest, and a
pivotal debate to widespread concerns is safety in the transportation system.
Deep reinforcement learning (DRL) has been applied to autonomous driving to
provide solutions for obstacle avoidance. However, in a road traffic junction
scenario, the vehicle typically receives partial observations from the
transportation environment, while DRL needs to rely on long-term rewards to
train a reliable model by maximising the cumulative rewards, which may take the
risk when exploring new actions and returning either a positive reward or a
penalty in the case of collisions. Although safety concerns are usually
considered in the design of a reward function, they are not fully considered as
the critical metric to directly evaluate the effectiveness of DRL algorithms in
autonomous driving. In this study, we evaluated the safety performance of three
baseline DRL models (DQN, A2C, and PPO) and proposed a self-awareness module
from an attention mechanism for DRL to improve the safety evaluation for an
anomalous vehicle in a complex road traffic junction environment, such as
intersection and roundabout scenarios, based on four metrics: collision rate,
success rate, freezing rate, and total reward. Our two experimental results in
the training and testing phases revealed the baseline DRL with poor safety
performance, while our proposed self-awareness attention-DQN can significantly
improve the safety performance in intersection and roundabout scenarios.",2201.08116v1,https://arxiv.org/pdf/2201.08116v1
Safety-Aware Multi-Agent Apprenticeship Learning,Junchen Zhao,"Our objective of this project is to make the extension based on the technique
mentioned in the paper ""Safety-Aware Apprenticeship Learning"" to improve the
utility and the efficiency of the existing Reinforcement Learning model from a
Single-Agent Learning framework to a Multi-Agent Learning framework. Our
contributions to the project are presented in the following bullet points: 1.
Regarding the fact that we will add an extension to the Inverse Reinforcement
Learning model from a Single-Agent scenario to a Multi-Agentscenario. Our first
contribution to this project is considering the case of extracting safe reward
functions from expert behaviors in a Multi-Agent scenario instead of being from
the Single-Agent scenario. 2. Our second contribution is extending the
Single-Agent Learning Framework to a Multi-Agent Learning framework and
designing a novel Learning Framework based on the extension in the end. 3. Our
final contribution to this project is evaluating empirically the performance of
my extension to the Single-Agent Inverse Reinforcement Learning framework.",2201.08111v2,https://arxiv.org/pdf/2201.08111v2
Safe Deep RL in 3D Environments using Human Feedback,"Matthew Rahtz, Vikrant Varma, Ramana Kumar, Zachary Kenton, Shane Legg, Jan Leike","Agents should avoid unsafe behaviour during both training and deployment.
This typically requires a simulator and a procedural specification of unsafe
behaviour. Unfortunately, a simulator is not always available, and procedurally
specifying constraints can be difficult or impossible for many real-world
tasks. A recently introduced technique, ReQueST, aims to solve this problem by
learning a neural simulator of the environment from safe human trajectories,
then using the learned simulator to efficiently learn a reward model from human
feedback. However, it is yet unknown whether this approach is feasible in
complex 3D environments with feedback obtained from real humans - whether
sufficient pixel-based neural simulator quality can be achieved, and whether
the human data requirements are viable in terms of both quantity and quality.
In this paper we answer this question in the affirmative, using ReQueST to
train an agent to perform a 3D first-person object collection task using data
entirely from human contractors. We show that the resulting agent exhibits an
order of magnitude reduction in unsafe behaviour compared to standard
reinforcement learning.",2201.08102v2,https://arxiv.org/pdf/2201.08102v2
"Conservative Distributional Reinforcement Learning with Safety
  Constraints","Hengrui Zhang, Youfang Lin, Sheng Han, Shuo Wang, Kai Lv","Safety exploration can be regarded as a constrained Markov decision problem
where the expected long-term cost is constrained. Previous off-policy
algorithms convert the constrained optimization problem into the corresponding
unconstrained dual problem by introducing the Lagrangian relaxation technique.
However, the cost function of the above algorithms provides inaccurate
estimations and causes the instability of the Lagrange multiplier learning. In
this paper, we present a novel off-policy reinforcement learning algorithm
called Conservative Distributional Maximum a Posteriori Policy Optimization
(CDMPO). At first, to accurately judge whether the current situation satisfies
the constraints, CDMPO adapts distributional reinforcement learning method to
estimate the Q-function and C-function. Then, CDMPO uses a conservative value
function loss to reduce the number of violations of constraints during the
exploration process. In addition, we utilize Weighted Average Proportional
Integral Derivative (WAPID) to update the Lagrange multiplier stably. Empirical
results show that the proposed method has fewer violations of constraints in
the early exploration process. The final test results also illustrate that our
method has better risk control.",2201.07286v2,https://arxiv.org/pdf/2201.07286v2
"Safe Online Bid Optimization with Return-On-Investment and Budget
  Constraints subject to Uncertainty","Matteo Castiglioni, Alessandro Nuara, Giulia Romano, Giorgio Spadaro, Francesco Trovò, Nicola Gatti","In online marketing, the advertisers' goal is usually a tradeoff between
achieving high volumes and high profitability. The companies' business units
customarily address this tradeoff by maximizing the volumes while guaranteeing
a lower bound to the Return On Investment (ROI). This paper investigates
combinatorial bandit algorithms for the bid optimization of advertising
campaigns subject to uncertain budget and ROI constraints. We study the nature
of both the optimization and learning problems. In particular, when focusing on
the optimization problem without uncertainty, we show that it is inapproximable
within any factor unless P=NP, and we provide a pseudo-polynomial-time
algorithm that achieves an optimal solution. When considering uncertainty, we
prove that no online learning algorithm can violate the (ROI or budget)
constraints during the learning process a sublinear number of times while
guaranteeing a sublinear pseudo-regret. Thus, we provide an algorithm, namely
GCB, guaranteeing sublinear regret at the cost of a potentially linear number
of constraints violations. We also design its safe version, namely GCB_{safe},
guaranteeing w.h.p. a constant upper bound on the number of constraints
violations at the cost of a linear pseudo-regret. More interestingly, we
provide an algorithm, namely GCB_{safe}(\psi,\phi), guaranteeing both sublinear
pseudo-regret and safety w.h.p. at the cost of accepting tolerances \psi and
\phi in the satisfaction of the ROI and budget constraints, respectively. This
algorithm actually mitigates the risks due to the constraints violations
without precluding the convergence to the optimal solution. Finally, we
experimentally compare our algorithms in terms of
pseudo-regret/constraint-violation tradeoff in settings generated from
real-world data, showing the importance of adopting safety constraints in
practice and the effectiveness of our algorithms.",2201.07139v1,https://arxiv.org/pdf/2201.07139v1
A causal model of safety assurance for machine learning,Simon Burton,"This paper proposes a framework based on a causal model of safety upon which
effective safety assurance cases for ML-based applications can be built. In
doing so, we build upon established principles of safety engineering as well as
previous work on structuring assurance arguments for ML. The paper defines four
categories of safety case evidence and a structured analysis approach within
which these evidences can be effectively combined. Where appropriate, abstract
formalisations of these contributions are used to illustrate the causalities
they evaluate, their contributions to the safety argument and desirable
properties of the evidences. Based on the proposed framework, progress in this
area is re-evaluated and a set of future research directions proposed in order
for tangible progress in this field to be made.",2201.05451v3,https://arxiv.org/pdf/2201.05451v3
Structured access: an emerging paradigm for safe AI deployment,Toby Shevlane,"Structured access is an emerging paradigm for the safe deployment of
artificial intelligence (AI). Instead of openly disseminating AI systems,
developers facilitate controlled, arm's length interactions with their AI
systems. The aim is to prevent dangerous AI capabilities from being widely
accessible, whilst preserving access to AI capabilities that can be used
safely. The developer must both restrict how the AI system can be used, and
prevent the user from circumventing these restrictions through modification or
reverse engineering of the AI system. Structured access is most effective when
implemented through cloud-based AI services, rather than disseminating AI
software that runs locally on users' hardware. Cloud-based interfaces provide
the AI developer greater scope for controlling how the AI system is used, and
for protecting against unauthorized modifications to the system's design. This
chapter expands the discussion of ""publication norms"" in the AI community,
which to date has focused on the question of how the informational content of
AI research projects should be disseminated (e.g., code and models). Although
this is an important question, there are limits to what can be achieved through
the control of information flows. Structured access views AI software not only
as information that can be shared but also as a tool with which users can have
arm's length interactions. There are early examples of structured access being
practiced by AI developers, but there is much room for further development,
both in the functionality of cloud-based interfaces and in the wider
institutional framework.",2201.05159v2,https://arxiv.org/pdf/2201.05159v2
"Black-box Safety Analysis and Retraining of DNNs based on Feature
  Extraction and Clustering","Mohammed Oualid Attaoui, Hazem Fahmy, Fabrizio Pastore, Lionel Briand","Deep neural networks (DNNs) have demonstrated superior performance over
classical machine learning to support many features in safety-critical systems.
Although DNNs are now widely used in such systems (e.g., self driving cars),
there is limited progress regarding automated support for functional safety
analysis in DNN-based systems. For example, the identification of root causes
of errors, to enable both risk analysis and DNN retraining, remains an open
problem. In this paper, we propose SAFE, a black-box approach to automatically
characterize the root causes of DNN errors. SAFE relies on a transfer learning
model pre-trained on ImageNet to extract the features from error-inducing
images. It then applies a density-based clustering algorithm to detect
arbitrary shaped clusters of images modeling plausible causes of error. Last,
clusters are used to effectively retrain and improve the DNN. The black-box
nature of SAFE is motivated by our objective not to require changes or even
access to the DNN internals to facilitate adoption. Experimental results show
the superior ability of SAFE in identifying different root causes of DNN errors
based on case studies in the automotive domain. It also yields significant
improvements in DNN accuracy after retraining, while saving significant
execution time and memory when compared to alternatives.",2201.05077v4,https://arxiv.org/pdf/2201.05077v4
The Concept of Criticality in AI Safety,"Yitzhak Spielberg, Amos Azaria","When AI agents don't align their actions with human values they may cause
serious harm. One way to solve the value alignment problem is by including a
human operator who monitors all of the agent's actions. Despite the fact, that
this solution guarantees maximal safety, it is very inefficient, since it
requires the human operator to dedicate all of his attention to the agent. In
this paper, we propose a much more efficient solution that allows an operator
to be engaged in other activities without neglecting his monitoring task. In
our approach the AI agent requests permission from the operator only for
critical actions, that is, potentially harmful actions. We introduce the
concept of critical actions with respect to AI safety and discuss how to build
a model that measures action criticality. We also discuss how the operator's
feedback could be used to make the agent smarter.",2201.04632v2,https://arxiv.org/pdf/2201.04632v2
Safe Equilibrium,Sam Ganzfried,"The standard game-theoretic solution concept, Nash equilibrium, assumes that
all players behave rationally. If we follow a Nash equilibrium and opponents
are irrational (or follow strategies from a different Nash equilibrium), then
we may obtain an extremely low payoff. On the other hand, a maximin strategy
assumes that all opposing agents are playing to minimize our payoff (even if it
is not in their best interest), and ensures the maximal possible worst-case
payoff, but results in exceedingly conservative play. We propose a new solution
concept called safe equilibrium that models opponents as behaving rationally
with a specified probability and behaving potentially arbitrarily with the
remaining probability. We prove that a safe equilibrium exists in all
strategic-form games (for all possible values of the rationality parameters),
and prove that its computation is PPAD-hard. We present exact algorithms for
computing a safe equilibrium in both 2 and $n$-player games, as well as
scalable approximation algorithms.",2201.04266v10,https://arxiv.org/pdf/2201.04266v10
"Arguments about Highly Reliable Agent Designs as a Useful Path to
  Artificial Intelligence Safety","Issa Rice, David Manheim","Several different approaches exist for ensuring the safety of future
Transformative Artificial Intelligence (TAI) or Artificial Superintelligence
(ASI) systems, and proponents of different approaches have made different and
debated claims about the importance or usefulness of their work in the near
term, and for future systems. Highly Reliable Agent Designs (HRAD) is one of
the most controversial and ambitious approaches, championed by the Machine
Intelligence Research Institute, among others, and various arguments have been
made about whether and how it reduces risks from future AI systems. In order to
reduce confusion in the debate about AI safety, here we build on a previous
discussion by Rice which collects and presents four central arguments which are
used to justify HRAD as a path towards safety of AI systems.
  We have titled the arguments (1) incidental utility,(2) deconfusion, (3)
precise specification, and (4) prediction. Each of these makes different,
partly conflicting claims about how future AI systems can be risky. We have
explained the assumptions and claims based on a review of published and
informal literature, along with consultation with experts who have stated
positions on the topic. Finally, we have briefly outlined arguments against
each approach and against the agenda overall.",2201.02950v1,https://arxiv.org/pdf/2201.02950v1
SABLAS: Learning Safe Control for Black-box Dynamical Systems,"Zengyi Qin, Dawei Sun, Chuchu Fan","Control certificates based on barrier functions have been a powerful tool to
generate probably safe control policies for dynamical systems. However,
existing methods based on barrier certificates are normally for white-box
systems with differentiable dynamics, which makes them inapplicable to many
practical applications where the system is a black-box and cannot be accurately
modeled. On the other side, model-free reinforcement learning (RL) methods for
black-box systems suffer from lack of safety guarantees and low sampling
efficiency. In this paper, we propose a novel method that can learn safe
control policies and barrier certificates for black-box dynamical systems,
without requiring for an accurate system model. Our method re-designs the loss
function to back-propagate gradient to the control policy even when the
black-box dynamical system is non-differentiable, and we show that the safety
certificates hold on the black-box system. Empirical results in simulation show
that our method can significantly improve the performance of the learned
policies by achieving nearly 100% safety and goal reaching rates using much
fewer training samples, compared to state-of-the-art black-box safe control
methods. Our learned agents can also generalize to unseen scenarios while
keeping the original performance. The source code can be found at
https://github.com/Zengyi-Qin/bcbf.",2201.01918v2,https://arxiv.org/pdf/2201.01918v2
"Learning Control Policies for Fall prevention and safety in bipedal
  locomotion",Visak Kumar,"The ability to recover from an unexpected external perturbation is a
fundamental motor skill in bipedal locomotion. An effective response includes
the ability to not just recover balance and maintain stability but also to fall
in a safe manner when balance recovery is physically infeasible. For robots
associated with bipedal locomotion, such as humanoid robots and assistive
robotic devices that aid humans in walking, designing controllers which can
provide this stability and safety can prevent damage to robots or prevent
injury related medical costs. This is a challenging task because it involves
generating highly dynamic motion for a high-dimensional, non-linear and
under-actuated system with contacts. Despite prior advancements in using
model-based and optimization methods, challenges such as requirement of
extensive domain knowledge, relatively large computational time and limited
robustness to changes in dynamics still make this an open problem. In this
thesis, to address these issues we develop learning-based algorithms capable of
synthesizing push recovery control policies for two different kinds of robots :
Humanoid robots and assistive robotic devices that assist in bipedal
locomotion. Our work can be branched into two closely related directions : 1)
Learning safe falling and fall prevention strategies for humanoid robots and 2)
Learning fall prevention strategies for humans using a robotic assistive
devices. To achieve this, we introduce a set of Deep Reinforcement Learning
(DRL) algorithms to learn control policies that improve safety while using
these robots.",2201.01361v1,https://arxiv.org/pdf/2201.01361v1
"Learning Differentiable Safety-Critical Control using Control Barrier
  Functions for Generalization to Novel Environments","Hengbo Ma, Bike Zhang, Masayoshi Tomizuka, Koushil Sreenath","Control barrier functions (CBFs) have become a popular tool to enforce safety
of a control system. CBFs are commonly utilized in a quadratic program
formulation (CBF-QP) as safety-critical constraints. A class $\mathcal{K}$
function in CBFs usually needs to be tuned manually in order to balance the
trade-off between performance and safety for each environment. However, this
process is often heuristic and can become intractable for high relative-degree
systems. Moreover, it prevents the CBF-QP from generalizing to different
environments in the real world. By embedding the optimization procedure of the
exponential control barrier function based quadratic program (ECBF-QP) as a
differentiable layer within a deep learning architecture, we propose a
differentiable safety-critical control framework that enables generalization to
new environments for high relative-degree systems with forward invariance
guarantees. Finally, we validate the proposed control design with 2D double and
quadruple integrator systems in various environments.",2201.01347v3,https://arxiv.org/pdf/2201.01347v3
"Interactive Attention AI to translate low light photos to captions for
  night scene understanding in women safety","Rajagopal A, Nirmala V, Arun Muthuraj Vedamanickam","There is amazing progress in Deep Learning based models for Image captioning
and Low Light image enhancement. For the first time in literature, this paper
develops a Deep Learning model that translates night scenes to sentences,
opening new possibilities for AI applications in the safety of visually
impaired women. Inspired by Image Captioning and Visual Question Answering, a
novel Interactive Image Captioning is developed. A user can make the AI focus
on any chosen person of interest by influencing the attention scoring.
Attention context vectors are computed from CNN feature vectors and
user-provided start word. The Encoder-Attention-Decoder neural network learns
to produce captions from low brightness images. This paper demonstrates how
women safety can be enabled by researching a novel AI capability in the
Interactive Vision-Language model for perception of the environment in the
night.",2201.00969v1,https://arxiv.org/pdf/2201.00969v1
