import pandas as pdimport requestsimport xml.etree.ElementTree as ETfrom datetime import datetimeimport timedef get_arxiv_date(arxiv_id):    base_url = "http://export.arxiv.org/api/query"    params = {        "id_list": arxiv_id,        "max_results": 1    }        try:        response = requests.get(base_url, params=params)        response.raise_for_status()                root = ET.fromstring(response.content)        published_date = root.find(".//{http://www.w3.org/2005/Atom}published")                if published_date is not None:            date_str = published_date.text            date_obj = datetime.strptime(date_str, "%Y-%m-%dT%H:%M:%SZ")            return date_obj.strftime("%d-%b-%Y")        else:            print(f"Warning: Missing date for paper with arXiv ID: {arxiv_id}")            return "[Missing data]"        except requests.RequestException as e:        print(f"Error fetching data for arXiv ID {arxiv_id}: {str(e)}")        return "[Missing data]"# Read the CSV filedf = pd.read_csv('combined.csv')# Create dataframes for each companyanthropic_df = df[df['Institution'].str.contains('Anthropic', case=False, na=False)].copy()openai_df = df[df['Institution'].str.contains('OpenAI', case=False, na=False)].copy()gdm_df = df[df['Institution'].str.contains('Google|DeepMind', case=False, na=False)].copy()# Function to process dataframedef process_df(df):    df['Company'] = df['Institution']  # Keep the exact Institution value    df['Date'] = df['arXiv ID'].apply(get_arxiv_date)    df['URL'] = df['PDF_Link']    df['Safety_category'] = ''    df = df[['Company', 'Title', 'Date', 'URL', 'Safety_category', 'Abstract']]    return df# Process each dataframeanthropic_df = process_df(anthropic_df)openai_df = process_df(openai_df)gdm_df = process_df(gdm_df)# Check for overlapping papersall_dfs = [('Anthropic', anthropic_df), ('OpenAI', openai_df), ('GDM', gdm_df)]for i, (name1, df1) in enumerate(all_dfs):    for name2, df2 in all_dfs[i+1:]:        overlap = pd.merge(df1, df2, on='Title')        if not overlap.empty:            print(f"Warning: The following papers appear in both {name1} and {name2} dataframes:")            for _, row in overlap.iterrows():                print(f"  - {row['Title']}")                print(f"    Companies: {row['Company_x']} and {row['Company_y']}")# Export to CSVanthropic_df.to_csv('Anthropic.csv', index=False)openai_df.to_csv('OpenAI.csv', index=False)gdm_df.to_csv('GDM.csv', index=False)print("Processing complete. CSV files have been created.")